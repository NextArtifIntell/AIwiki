<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMAX_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simax---96">SIMAX - 96</h2>
<ul>
<li><details>
<summary>
(2024). Gradient-type subspace iteration methods for the symmetric
eigenvalue problem. <em>SIMAX</em>, <em>45</em>(4), 2360–2386. (<a
href="https://doi.org/10.1137/23M1590792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper explores variants of the subspace iteration algorithm for computing approximate invariant subspaces. The standard subspace iteration approach is revisited and new variants that exploit gradient-type techniques combined with a Grassmann manifold viewpoint are developed. A gradient method as well as a nonlinear conjugate gradient technique are described. Convergence of the gradient-based algorithm is analyzed and a few numerical experiments are reported, indicating that the proposed algorithms are sometimes superior to standard algorithms. This includes the Chebyshev-based subspace iteration and the locally optimal block conjugate gradient method, when compared in terms of number of matrix vector products and computational time, respectively. The new methods, on the other hand, do not require estimating optimal parameters. An important contribution of this paper to achieve this good performance is the accurate and efficient implementation of an exact line search. In addition, new convergence proofs are presented for the nonaccelerated gradient method, including a locally exponential convergence if started in an neighborhood of the dominant subspace with spectral gap .},
  archive      = {J_SIMAX},
  author       = {Foivos Alimisis and Yousef Saad and Bart Vandereycken},
  doi          = {10.1137/23M1590792},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2360-2386},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Gradient-type subspace iteration methods for the symmetric eigenvalue problem},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Para-hermitian rational matrices. <em>SIMAX</em>,
<em>45</em>(4), 2339–2359. (<a
href="https://doi.org/10.1137/24M1678416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we study para-Hermitian rational matrices and the associated structured rational eigenvalue problem (REP). Para-Hermitian rational matrices are square rational matrices that are Hermitian for all on the unit circle that are not poles. REPs are often solved via linearization, that is, using matrix pencils associated to the corresponding rational matrix that preserve the spectral structure. Yet, nonconstant polynomial matrices cannot be para-Hermitian. Therefore, given a para-Hermitian rational matrix , we instead construct a -palindromic linearization for , whose eigenvalues that are not on the unit circle preserve the symmetries of the zeros and poles of . This task is achieved via Möbius transformations. We also give a constructive method that is based on an additive decomposition into the stable and antistable parts of . Analogous results are presented for para-skew-Hermitian rational matrices, i.e., rational matrices that are skew-Hermitian upon evaluation on those points of the unit circle that are not poles.},
  archive      = {J_SIMAX},
  author       = {Froilán M. Dopico and Vanni Noferini and María C. Quintana and Paul Van Dooren},
  doi          = {10.1137/24M1678416},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2339-2359},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Para-hermitian rational matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing functions of symmetric hierarchically
semiseparable matrices. <em>SIMAX</em>, <em>45</em>(4), 2314–2338. (<a
href="https://doi.org/10.1137/24M1642354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The aim of this work is to develop a fast algorithm for approximating the matrix function of a square matrix that is symmetric and has hierarchically semiseparable (HSS) structure. Appearing in a wide variety of applications, often in the context of discretized (fractional) differential and integral operators, HSS matrices have a number of attractive properties facilitating the development of fast algorithms. In this work, we use an unconventional telescopic decomposition of , inspired by recent work of Levitt and Martinsson on approximating an HSS matrix from matrix-vector products with a few random vectors. This telescopic decomposition allows us to approximate by recursively performing low-rank updates with rational Krylov subspaces while keeping the size of the matrices involved in the rational Krylov subspaces small. In particular, no large-scale linear system needs to be solved, which yields favorable complexity estimates and reduced execution times compared to existing methods, including an existing divide-and-conquer strategy. The advantages of our newly proposed algorithms are demonstrated for a number of examples from the literature, featuring the exponential, the inverse square root, and the sign function of a matrix. For the special case of matrix inversion, our algorithm reduces to a procedure previously proposed by Gillman, Young, and Martinsson [Front. Math. China, 7 (2012), pp. 217–247].},
  archive      = {J_SIMAX},
  author       = {Angelo A. Casulli and Daniel Kressner and Leonardo Robol},
  doi          = {10.1137/24M1642354},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2314-2338},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing functions of symmetric hierarchically semiseparable matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized kaczmarz with geometrically smoothed momentum.
<em>SIMAX</em>, <em>45</em>(4), 2287–2313. (<a
href="https://doi.org/10.1137/24M1633820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper studies the effect of adding geometrically smoothed momentum to the randomized Kaczmarz algorithm, which is an instance of stochastic gradient descent on a linear least squares loss function. We prove a result about the expected error in the direction of singular vectors of the matrix defining the least squares loss. We present several numerical examples illustrating the utility of our result and pose several questions.},
  archive      = {J_SIMAX},
  author       = {Seth J. Alderman and Roan W. Luikart and Nicholas F. Marshall},
  doi          = {10.1137/24M1633820},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2287-2313},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Randomized kaczmarz with geometrically smoothed momentum},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block <span
class="math inline"><strong>ω</strong></span>-circulant preconditioners
for parabolic optimal control problems. <em>SIMAX</em>, <em>45</em>(4),
2263–2286. (<a href="https://doi.org/10.1137/23M1601432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we propose a class of novel preconditioned Krylov subspace methods for solving an optimal control problem of parabolic equations. Namely, we develop a family of block -circulant based preconditioners for the all-at-once linear system arising from the concerned optimal control problem, where both first order and second order time discretization methods are considered. The proposed preconditioners can be efficiently diagonalized by fast Fourier transforms in a parallel-in-time fashion, and their effectiveness is theoretically shown in the sense that the eigenvalues of the preconditioned matrix are clustered around . This clustering leads to rapid convergence when the minimal residual method is used, particularly when the regularization parameter is sufficiently small. When the generalized minimal residual method is deployed, the efficacy of the proposed preconditioners is justified in the way that the singular values of the preconditioned matrices are proven clustered around unity. Numerical results are provided to demonstrate our proposed solvers, especially the effectiveness of the preconditioned generalized minimal residual approach.},
  archive      = {J_SIMAX},
  author       = {Po Yin Fung and Sean Hon},
  doi          = {10.1137/23M1601432},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2263-2286},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Block \(\boldsymbol{\omega }\)-circulant preconditioners for parabolic optimal control problems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Krylov subspace recycling with randomized sketching for
matrix functions. <em>SIMAX</em>, <em>45</em>(4), 2243–2262. (<a
href="https://doi.org/10.1137/23M1595904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A Krylov subspace recycling method for the efficient evaluation of a sequence of matrix functions acting on a set of vectors is developed. The method improves over the recycling methods presented in [L. Burke et al., Krylov Subspace Recycling for Matrix Functions, Technical report, arXiv:2209.14163, 2022] in that it uses a closed-form expression for the augmented full orthogonalization method (FOM) approximants and hence circumvents the use of numerical quadrature. We further extend our method to use randomized sketching in order to avoid the arithmetic cost of orthogonalizing a full Krylov basis, offering an attractive solution to the fact that recycling algorithms built from shifted augmented FOM cannot easily be restarted. The efficacy of the proposed algorithms is demonstrated with numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Liam Burke and Stefan Güttel},
  doi          = {10.1137/23M1595904},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2243-2262},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Krylov subspace recycling with randomized sketching for matrix functions},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A chebyshev locally optimal block preconditioned conjugate
gradient method for product and standard symmetric eigenvalue problems.
<em>SIMAX</em>, <em>45</em>(4), 2211–2242. (<a
href="https://doi.org/10.1137/23M1566017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The discretized Bethe–Salpeter eigenvalue (BSE) problem arises in many-body physics and quantum chemistry. Discretization leads to an algebraic eigenvalue problem involving a matrix with a Hamiltonian-like structure. With proper transformations, the real BSE eigenproblem of form I and the complex BSE eigenproblem of form II can be transformed into real product eigenvalue problems of order and , respectively. We propose a new variant of the locally optimal block preconditioned conjugate gradient (LOBPCG) enhanced with polynomial filters to improve the robustness and effectiveness of a few well-known algorithms for computing the lowest eigenvalues of the product eigenproblems. Furthermore, our proposed method can be easily employed to solve large sparse standard symmetric eigenvalue problems. We show that our ideal locally optimal algorithm delivers Rayleigh quotient approximation to the desired lowest eigenvalue that satisfies a global quasi-optimality, which is similar to the global optimality of the preconditioned conjugate gradient method for the iterative solution of a symmetric positive definite linear system. The robustness and efficiency of our proposed method is illustrated by numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Tianqi Zhang and Fei Xue},
  doi          = {10.1137/23M1566017},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2211-2242},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A chebyshev locally optimal block preconditioned conjugate gradient method for product and standard symmetric eigenvalue problems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assigning stationary distributions to sparse stochastic
matrices. <em>SIMAX</em>, <em>45</em>(4), 2184–2210. (<a
href="https://doi.org/10.1137/23M1627328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix and a target stationary distribution , construct a minimum norm perturbation, such that is also stochastic and has the prescribed target stationary distribution, . In this paper, we revisit the TSDP under a constraint on the support of , that is, on the set of nonzero entries of . This is particularly meaningful in practice since one cannot typically modify all entries of . We first show how to construct a feasible solution that has essentially the same support as the matrix . Then we show how to compute globally optimal and sparse solutions using the componentwise norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to in a few minutes. We illustrate the proposed algorithms with several numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Nicolas Gillis and Paul Van Dooren},
  doi          = {10.1137/23M1627328},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2184-2210},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Assigning stationary distributions to sparse stochastic matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Verified error bounds for matrix decompositions.
<em>SIMAX</em>, <em>45</em>(4), 2155–2183. (<a
href="https://doi.org/10.1137/24M165096X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this note we consider common matrix factorizations such as LU decomposition of a square and rectangular matrix, Cholesky and QR decomposition, singular value decomposition for square and rectangular matrices, and eigen-, Schur, and Takagi decomposition. We first note that well-conditioned factors tend to be sensitive to perturbations of the input matrix, while ill-conditioned factors tend to be insensitive. It seems that this behavior has not been recognized in numerical analysis. We develop a formula for the relation between the condition number of the factor and its sensitivity with respect to input perturbations, and give reasons for that. Our main focus is to describe verification methods for the factors of the mentioned decompositions. That means proving existence of the factorization together with rigorous entrywise error bounds for the factors. Our goal is to develop algorithms requiring operations for an matrix with and . Moreover, bounds of high quality are aimed for, often not far from maximal accuracy. A main tool to achieve that is accurate dot products based on error-free transformations. Since preconditioning based on approximate inverses is used, our methods are restricted to full matrices.},
  archive      = {J_SIMAX},
  author       = {Siegfried M. Rump and Takeshi Ogita},
  doi          = {10.1137/24M165096X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2155-2183},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Verified error bounds for matrix decompositions},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenvalues of dual hermitian matrices with application in
formation control. <em>SIMAX</em>, <em>45</em>(4), 2135–2154. (<a
href="https://doi.org/10.1137/24M1652234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a supplement matrix method for computing eigenvalues of a dual Hermitian matrix and discuss its application in multiagent formation control. Suppose we have a ring, which can be the real field, the complex field, or the quaternion ring. We study dual number symmetric matrices, dual complex Hermitian matrices, and dual quaternion Hermitian matrices in a unified frame of dual Hermitian matrices. An dual Hermitian matrix has dual number eigenvalues. We define determinant, characteristic polynomial, and supplement matrices for a dual Hermitian matrix. Supplement matrices are Hermitian matrices in the original ring. The standard parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of the standard part of the dual Hermitian matrix in the original ring, while the dual parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of those supplement matrices. Note that the standard part of the dual Hermitian matrix is still a Hermitian matrix in the original ring. Hence, by applying any practical method for computing eigenvalues of Hermitian matrices in the original ring, we have a practical method for computing eigenvalues of a dual Hermitian matrix. We call this method the supplement matrix method. In multiagent formation control, a desired relative configuration scheme may be given. People need to know if this scheme is reasonable such that a feasible solution of configurations of these multiagents exists. By exploring the eigenvalue problem of dual Hermitian matrices and its link with the unit gain graph theory, we open a cross-disciplinary approach to solve the relative configuration problem. Numerical experiments are reported to show the efficiency of our proposed methods.},
  archive      = {J_SIMAX},
  author       = {Liqun Qi and Chunfeng Cui},
  doi          = {10.1137/24M1652234},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2135-2154},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Eigenvalues of dual hermitian matrices with application in formation control},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale minimization of the pseudospectral abscissa.
<em>SIMAX</em>, <em>45</em>(4), 2104–2134. (<a
href="https://doi.org/10.1137/22M1517329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work concerns the minimization of the pseudospectral abscissa of a matrix-valued function dependent on parameters analytically. The problem is motivated by robust stability and transient behavior considerations for a linear control system that has optimization parameters. We describe a subspace procedure to cope with the setting when the matrix-valued function is of large size. The proposed subspace procedure solves a sequence of reduced problems obtained by restricting the matrix-valued function to small subspaces, whose dimensions increase gradually. It possesses desirable features such as a superlinear convergence exhibited by the decay in the errors of the minimizers of the reduced problems. In mathematical terms, the problem we consider is a large-scale nonconvex minimax eigenvalue optimization problem such that the eigenvalue function appears in the constraint of the inner maximization problem. Devising and analyzing a subspace framework for the minimax eigenvalue optimization problem at hand with the eigenvalue function in the constraint require special treatment that makes use of a Lagrangian and dual variables. There are notable advantages in minimizing the pseudospectral abscissa over maximizing the distance to instability or minimizing the norm; the optimized pseudospectral abscissa provides quantitative information about the worst-case transient growth, and the initial guesses for the parameter values to optimize the pseudospectral abscissa can be arbitrary, unlike the case to optimize the distance to instability and norm that would normally require initial guesses yielding asymptotically stable systems. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://zenodo.org/records/6992092.},
  archive      = {J_SIMAX},
  author       = {Nicat Aliyev and Emre Mengi},
  doi          = {10.1137/22M1517329},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2104-2134},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Large-scale minimization of the pseudospectral abscissa},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A preconditioned riemannian gradient descent algorithm for
low-rank matrix recovery. <em>SIMAX</em>, <em>45</em>(4), 2075–2103. (<a
href="https://doi.org/10.1137/23M1570442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The low-rank matrix recovery problem often arises in various fields, including signal processing, machine learning, and imaging science. The Riemannian gradient descent (RGD) algorithm has proven to be an efficient algorithm for solving this problem. In this paper, we present a preconditioned Riemannian gradient descent (PRGD) for low-rank matrix recovery. The preconditioner, noted for its simplicity and computational efficiency, is constructed by weighting the th entry of the gradient matrix according to the norms of the th row and the th column. We establish the theoretical recovery guarantee for PRGD under the restricted isometry property assumption. Experimental results indicate that PRGD can accelerate RGD by up to tenfold in solving low-rank matrix recovery problems such as matrix completion.},
  archive      = {J_SIMAX},
  author       = {Fengmiao Bian and Jian-Feng Cai and Rui Zhang},
  doi          = {10.1137/23M1570442},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2075-2103},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A preconditioned riemannian gradient descent algorithm for low-rank matrix recovery},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A numerically stable communication-avoiding <span
class="math inline"><em>s</em></span>-step GMRES algorithm.
<em>SIMAX</em>, <em>45</em>(4), 2039–2074. (<a
href="https://doi.org/10.1137/23M1577109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Krylov subspace methods are extensively used in scientific computing to solve large-scale linear systems. However, the performance of these iterative Krylov solvers on modern supercomputers is limited by expensive communication costs. The -step strategy generates a series of Krylov vectors at a time to avoid communication. Asymptotically, the -step approach can reduce communication latency by a factor of . Unfortunately, due to finite-precision implementation, the step size has to be kept small for stability. In this work, we tackle the numerical instabilities encountered in the -step GMRES algorithm. By choosing an appropriate polynomial basis and block orthogonalization schemes, we construct a communication-avoiding -step GMRES algorithm that automatically selects the optimal step size to ensure numerical stability. To further maximize communication savings, we introduce scaled Newton polynomials that can increase the step size to a few hundreds for many problems. An initial step size estimator is also developed to efficiently choose the optimal step size for stability. The guaranteed stability of the proposed algorithm is demonstrated using numerical experiments. In the process, we also evaluate how the choice of polynomial and preconditioning affects the stability limit of the algorithm. Finally, we show parallel scalability on more than 114,000 cores in a distributed-memory setting. Perfectly linear scaling has been observed in both strong and weak scaling studies with negligible communication costs.},
  archive      = {J_SIMAX},
  author       = {Zan Xu and Juan J. Alonso and Eric Darve},
  doi          = {10.1137/23M1577109},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2039-2074},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A numerically stable communication-avoiding \({s}\)-step GMRES algorithm},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A riemannian optimization method to compute the nearest
singular pencil. <em>SIMAX</em>, <em>45</em>(4), 2007–2038. (<a
href="https://doi.org/10.1137/23M1596326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Given a square pencil , where and are complex (resp., real) matrices, we consider the problem of finding the singular complex (resp., real) pencil nearest to it in the Frobenius distance. This problem is known to be very difficult, and the few algorithms available in the literature can only deal efficiently with pencils of very small size. We show that the problem is equivalent to minimizing a certain objective function over the Riemannian manifold (resp., if the nearest real singular pencil is sought), where denotes the special unitary group (resp., denotes the special orthogonal group). This novel perspective is based on the generalized Schur form of pencils, and yields competitive numerical methods, by pairing it with algorithms capable of doing optimization on Riemannian manifolds. We propose one algorithm that directly minimizes the (almost everywhere, but not everywhere, differentiable) function , as well as a smoothed alternative and a third algorithm that deals with smooth functions and can also solve the problem of finding a nearest singular pencil with a specified minimal index. We provide numerical experiments that show that the resulting methods allow us to deal with pencils of much larger size than alternative techniques, yielding candidate minimizers of comparable or better quality. In the course of our analysis, we also obtain a number of new theoretical results related to the generalized Schur form of a (regular or singular) square pencil and to the minimal index of a singular square pencil whose nullity is 1.},
  archive      = {J_SIMAX},
  author       = {Froilán M. Dopico and Vanni Noferini and Lauri Nyman},
  doi          = {10.1137/23M1596326},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2007-2038},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A riemannian optimization method to compute the nearest singular pencil},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient bounds and estimates for canonical angles in
randomized subspace approximations. <em>SIMAX</em>, <em>45</em>(4),
1978–2006. (<a href="https://doi.org/10.1137/23M1584733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Randomized subspace approximation with “matrix sketching” is an effective approach for constructing approximate partial singular value decompositions (SVDs) of large matrices. The performance of such techniques has been extensively analyzed, and very precise estimates on the distribution of the residual errors have been derived. However, our understanding of the accuracy of the computed singular vectors (measured in terms of the canonical angles between the spaces spanned by the exact and the computed singular vectors, respectively) remains relatively limited. In this work, we present practical bounds and estimates for canonical angles of randomized subspace approximation that can be computed efficiently either a priori or a posteriori, without assuming prior knowledge of the true singular subspaces. Under moderate oversampling in the randomized SVD, our prior probabilistic bounds are asymptotically tight and can be computed efficiently, while bringing a clear insight into the balance between oversampling and power iterations given a fixed budget on the number of matrix-vector multiplications. The numerical experiments demonstrate the empirical effectiveness of these canonical angle bounds and estimates on different matrices under various algorithmic choices for the randomized SVD.},
  archive      = {J_SIMAX},
  author       = {Yijun Dong and Per-Gunnar Martinsson and Yuji Nakatsukasa},
  doi          = {10.1137/23M1584733},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1978-2006},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient bounds and estimates for canonical angles in randomized subspace approximations},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The multivariate eigenvalues of symmetric tensors.
<em>SIMAX</em>, <em>45</em>(4), 1954–1977. (<a
href="https://doi.org/10.1137/23M1614808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper studies the multivariate eigenvalue problem for symmetric tensor tuples. Basic properties of the multivariate eigenvalues are given. It is shown that both the numbers of the complex multivariate eigenvalues and eigenvectors of symmetric tensor tuples are finite in general. We formulate the tensor multivariate eigenvalue problem equivalently as polynomial optimization. The real multivariate eigenvalues can be computed sequentially. Each of them can be computed by Lasserre’s hierarchy of semidefinite relaxations. We show that it has finite convergence for generic symmetric tensor tuples. Numerical experiments are presented to show the efficiency of the proposed method.},
  archive      = {J_SIMAX},
  author       = {Anwa Zhou and Yangyang Ni and Jinyan Fan},
  doi          = {10.1137/23M1614808},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1954-1977},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The multivariate eigenvalues of symmetric tensors},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multilinear nyström algorithm for low-rank approximation
of tensors in tucker format. <em>SIMAX</em>, <em>45</em>(4), 1929–1953.
(<a href="https://doi.org/10.1137/23M1599343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Nyström method offers an effective way to obtain low-rank approximation of SPD matrices and has been recently extended and analyzed to nonsymmetric matrices (leading to the generalized Nyström method). It is a randomized, single-pass, streamable, cost-effective, and accurate alternative to the randomized SVD, and it facilitates the computation of several matrix low-rank factorizations. In this paper, we take these advancements a step further by introducing a higher-order variant of Nyström’s methodology tailored to approximating low-rank tensors in the Tucker format: the multilinear Nyström technique. We show that, by introducing appropriate small modifications in the formulation of the higher-order method, strong stability properties can be obtained. This algorithm retains the key attributes of the generalized Nyström method, positioning it as a viable substitute for the randomized higher-order SVD algorithm. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://github.com/alb95/MLN and in the supplementary materials (MLN-main.zip [20KB]).},
  archive      = {J_SIMAX},
  author       = {Alberto Bucci and Leonardo Robol},
  doi          = {10.1137/23M1599343},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1929-1953},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A multilinear nyström algorithm for low-rank approximation of tensors in tucker format},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel-in-time solver for the all-at-once runge–kutta
discretization. <em>SIMAX</em>, <em>45</em>(4), 1902–1928. (<a
href="https://doi.org/10.1137/23M1567862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this article, we derive fast and robust parallel-in-time preconditioned iterative methods for the all-at-once linear systems arising upon discretization of time-dependent PDEs. The discretization we employ is based on a Runge–Kutta method in time, for which the development of parallel solvers is an emerging research area in the literature of numerical methods for time-dependent PDEs. By making use of classical theory of block matrices, one is able to derive a preconditioner for the systems considered. The block structure of the preconditioner allows for parallelism in the time variable, as long as one is able to provide a robust solver for the system of the stages of the method. We thus propose a preconditioner for the latter system based on a singular value decomposition (SVD) of the (real) Runge–Kutta matrix . Supposing is invertible and the discretization of the differential operator in space is symmetric positive definite, we prove that the spectrum of the system for the stages preconditioned by our SVD-based preconditioner is contained within the right-half of the unit circle, under suitable assumptions on the matrix (the assumptions are well posed due to the polar decomposition of ). We show the numerical efficiency of our approach by solving the system of the stages arising from the discretization of the heat equation and the Stokes equations, with sequential time-stepping. Finally, we provide numerical results of the all-at-once approach for both problems, showing the speedup achieved on a parallel architecture.},
  archive      = {J_SIMAX},
  author       = {Santolo Leveque and Luca Bergamaschi and Ángeles Martínez and John W. Pearson},
  doi          = {10.1137/23M1567862},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1902-1928},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Parallel-in-time solver for the all-at-once Runge–Kutta discretization},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the weak infinite zero structures and relative degrees
for linear time-delay systems. <em>SIMAX</em>, <em>45</em>(4),
1873–1901. (<a href="https://doi.org/10.1137/23M1606071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this article the weak infinite zero structures for linear time-delay systems and related problems are investigated. First, by linking the expansion of the transfer function matrix with output derivatives, a structural algorithm to determine the weak infinite zero structures is established, which is instrumental to derive the Smith–McMillan form at infinity of the transfer function matrix through a novel construction of biproper transformation matrices in the weak sense. The presented algorithm is inspired by Silverman’s algorithm and relies on switching between the frequency-domain and time-domain representations and interpretations. Second, the relationship between the weak infinite zero structures and the leading incomplete relative degrees (LIRD) is revealed by analyzing and comparing the algorithms for their computation. Finally, as an application of these results, the transformed outputs and the weak infinite zero structures are used to design delayed output feedback laws, which naturally generalize so-called proportional-derivative controllers.},
  archive      = {J_SIMAX},
  author       = {Lei Zhao and Zhao-Yan Li and Bin Zhou and Wim Michiels},
  doi          = {10.1137/23M1606071},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1873-1901},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On the weak infinite zero structures and relative degrees for linear time-delay systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anderson acceleration with truncated gram–schmidt.
<em>SIMAX</em>, <em>45</em>(4), 1850–1872. (<a
href="https://doi.org/10.1137/24M1648600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Anderson acceleration (AA) is a popular algorithm designed to enhance the convergence of fixed-point iterations. In this paper, we introduce a variant of AA based on a truncated Gram–Schmidt process (AATGS) which has a few advantages over the classical AA. In particular, an attractive feature of AATGS is that its iterates obey a three-term recurrence in the situation when it is applied to solving symmetric linear problems and this can lead to a considerable reduction of memory and computational costs. We analyze the convergence of AATGS in both full-depth and limited-depth scenarios and establish its equivalence to the classical AA in the linear case. We also report on the effectiveness of AATGS through a set of numerical experiments, ranging from solving nonlinear partial differential equations to tackling nonlinear optimization problems. In particular, the performance of the method is compared with that of the classical AA algorithms.},
  archive      = {J_SIMAX},
  author       = {Ziyuan Tang and Tianshi Xu and Huan He and Yousef Saad and Yuanzhe Xi},
  doi          = {10.1137/24M1648600},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1850-1872},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Anderson acceleration with truncated Gram–Schmidt},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cholesky-like preconditioner for hodge laplacians via heavy
collapsible subcomplex. <em>SIMAX</em>, <em>45</em>(4), 1827–1849. (<a
href="https://doi.org/10.1137/23M1626396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Techniques based on th order Hodge Laplacian operators are widely used to describe the topology as well as the governing dynamics of high-order systems modeled as simplicial complexes. In all of them, it is required to solve a number of least-squares problems with as coefficient matrix, for example, in order to compute some portions of the spectrum or integrate the dynamical system, thus making a fast and efficient solver for the least-squares problems highly desirable. To this aim, we introduce the notion of an optimal weakly collapsible subcomplex used to construct an effective sparse Cholesky-like preconditioner for that exploits the topological structure of the simplicial complex. The performance of the preconditioner is tested for the conjugate gradient method for least-squares problems (CGLS) on a variety of simplicial complexes with different dimensions and edge densities. We show that, for sparse simplicial complexes, the new preconditioner significantly reduces the condition number of and performs better than the standard incomplete Cholesky factorization.},
  archive      = {J_SIMAX},
  author       = {Anton Savostianov and Francesco Tudisco and Nicola Guglielmi},
  doi          = {10.1137/23M1626396},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1827-1849},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Cholesky-like preconditioner for hodge laplacians via heavy collapsible subcomplex},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coseparable nonnegative tensor factorization with t-CUR
decomposition. <em>SIMAX</em>, <em>45</em>(4), 1805–1826. (<a
href="https://doi.org/10.1137/23M1625998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Nonnegative matrix factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more naturally represented as a multidimensional array, such as images or videos. The NMF’s application to high-dimensional data involves vectorization, which risks losing essential multidimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable nonnegative tensor factorization (NTF). In this work, we provide an alternating index selection method to select the coseparable core. Furthermore, we validate the t-CUR sampling theory and integrate it with the tensor discrete empirical interpolation method to introduce an alternative, randomized index selection process. These methods have been tested on both synthetic and facial analysis datasets. The results demonstrate the efficiency of coseparable NTF when compared to coseparable NMF.},
  archive      = {J_SIMAX},
  author       = {Juefei Chen and Longxiu Huang and Yimin Wei},
  doi          = {10.1137/23M1625998},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1805-1826},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Coseparable nonnegative tensor factorization with t-CUR decomposition},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and forward stable randomized algorithms for linear
least-squares problems. <em>SIMAX</em>, <em>45</em>(4), 1782–1804. (<a
href="https://doi.org/10.1137/23M1616790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Iterative sketching and sketch-and-precondition are randomized algorithms used for solving overdetermined linear least-squares problems. When implemented in exact arithmetic, these algorithms produce high-accuracy solutions to least-squares problems faster than standard direct methods based on QR factorization. Recently, Meier et al. demonstrated numerical instabilities in a version of sketch-and-precondition in floating point arithmetic. The work of Meier et al. raises the question, is there a randomized least-squares solver that is both fast and stable? This paper resolves this question in the affirmative by proving that iterative sketching, appropriately implemented, is forward stable. Numerical experiments confirm the theoretical findings, demonstrating that iterative sketching is stable and faster than QR-based solvers for large problem instances.},
  archive      = {J_SIMAX},
  author       = {Ethan N. Epperly},
  doi          = {10.1137/23M1616790},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1782-1804},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast and forward stable randomized algorithms for linear least-squares problems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable symmetric tucker tensor decomposition.
<em>SIMAX</em>, <em>45</em>(4), 1746–1781. (<a
href="https://doi.org/10.1137/23M1582928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the best low-rank Tucker decomposition of symmetric tensors. The motivating application is decomposing higher-order multivariate moments. Moment tensors have special structure and are important to various data science problems. We advocate for a projected gradient descent (PGD) method and higher-order eigenvalue decomposition (HOEVD) approximation as computation schemes. Most importantly, we develop scalable adaptations of the basic PGD and HOEVD methods to decompose sample moment tensors. With the help of implicit and streaming techniques, we evade the overhead cost of building and storing the moment tensor. Such reductions make computing the Tucker decomposition realizable for large data instances in high dimensions. Numerical experiments demonstrate the efficiency of the algorithms and the applicability of moment tensor decompositions to real-world datasets. Finally we study the convergence on the Grassmannian manifold and prove that the update sequence derived by the PGD solver achieves first- and second-order criticality.},
  archive      = {J_SIMAX},
  author       = {Ruhui Jin and Joe Kileel and Tamara G. Kolda and Rachel Ward},
  doi          = {10.1137/23M1582928},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1746-1781},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Scalable symmetric tucker tensor decomposition},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New convergence analysis of GMRES with weighted norms,
preconditioning, and deflation, leading to a new deflation space.
<em>SIMAX</em>, <em>45</em>(4), 1721–1745. (<a
href="https://doi.org/10.1137/23M1622398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. New convergence bounds are presented for weighted, preconditioned, and deflated GMRES for the solution of large, sparse, non-Hermitian linear systems. These bounds are given for the case when the Hermitian part of the coefficient matrix is positive definite, the preconditioner is Hermitian positive definite, and the weight is equal to the preconditioner. The new bounds are a novel contribution in and of themselves. In addition, they are sufficiently explicit to indicate how to choose the preconditioner and the deflation space to accelerate the convergence. One such choice of deflating space is presented, and numerical experiments illustrate the effectiveness of such space.},
  archive      = {J_SIMAX},
  author       = {Nicole Spillane and Daniel B. Szyld},
  doi          = {10.1137/23M1622398},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1721-1745},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {New convergence analysis of GMRES with weighted norms, preconditioning, and deflation, leading to a new deflation space},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On substochastic inverse eigenvalue problems with the
corresponding eigenvector constraints. <em>SIMAX</em>, <em>45</em>(3),
1689–1719. (<a href="https://doi.org/10.1137/23M1547305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the inverse eigenvalue problem of constructing a substochastic matrix from the given spectrum parameters with the corresponding eigenvector constraints. This substochastic inverse eigenvalue problem (SstIEP) with the specific eigenvector constraints is formulated into a nonconvex optimization problem (NcOP). The solvability for SstIEP with the specific eigenvector constraints is equivalent to identifying the attainability of a zero optimal value for the formulated NcOP. When the optimal objective value is zero, the corresponding optimal solution to the formulated NcOP is just the substochastic matrix that we wish to construct. We develop the alternating minimization algorithm to solve the formulated NcOP, and its convergence is established by developing a novel method to obtain the boundedness of the optimal solution. Some numerical experiments are conducted to demonstrate the efficiency of the proposed method.},
  archive      = {J_SIMAX},
  author       = {Yujie Liu and Dacheng Yao and Hanqin Zhang},
  doi          = {10.1137/23M1547305},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1689-1719},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On substochastic inverse eigenvalue problems with the corresponding eigenvector constraints},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank plus diagonal approximations for riccati-like
matrix differential equations. <em>SIMAX</em>, <em>45</em>(3),
1669–1688. (<a href="https://doi.org/10.1137/23M1587610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of computing tractable approximations of time-dependent large positive semidefinite (PSD) matrices defined as solutions of a matrix differential equation. We propose to use “low-rank plus diagonal” PSD matrices as approximations that can be stored with a memory cost being linear in the high dimension . To constrain the solution of the differential equation to remain in that subset, we project the derivative at all times onto the tangent space to the subset, following the methodology of dynamical low-rank approximation. We derive a closed-form formula for the projection and show that after some manipulations, it can be computed with a numerical cost being linear in , allowing for tractable implementation. Contrary to previous approaches based on pure low-rank approximations, the addition of the diagonal term allows for our approximations to be invertible matrices that can moreover be inverted with linear cost in . We apply the technique to Riccati-like equations, then to two particular problems: first, a low-rank approximation to our recent Wasserstein gradient flow for Gaussian approximation of posterior distributions in approximate Bayesian inference and, second, a novel low-rank approximation of the Kalman filter for high-dimensional systems. Numerical simulations illustrate the results.},
  archive      = {J_SIMAX},
  author       = {Silvère Bonnabel and Marc Lambert and Francis Bach},
  doi          = {10.1137/23M1587610},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1669-1688},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Low-rank plus diagonal approximations for riccati-like matrix differential equations},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multichannel frequency estimation with constant amplitude
via convex structured low-rank approximation. <em>SIMAX</em>,
<em>45</em>(3), 1643–1668. (<a
href="https://doi.org/10.1137/23M1587737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of estimating the frequencies of several complex sinusoids with constant amplitude (CA) (also called constant modulus) from multichannel signals of their superposition. To exploit the CA property for frequency estimation in the framework of atomic norm minimization (ANM), we introduce multiple positive-semidenite block matrices composed of Hankel and Toeplitz submatrices and formulate the ANM problem as a convex structured low-rank approximation (SLRA) problem. The proposed SLRA is a semidenite programming and has substantial differences from existing such formulations without using the CA property. The proposed approach is termed as SLRA-based ANM for CA frequency estimation (SACA). We provide theoretical guarantees and extensive simulations that validate the advantages of SACA.},
  archive      = {J_SIMAX},
  author       = {Xunmeng Wu and Zai Yang and Zongben Xu},
  doi          = {10.1137/23M1587737},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1643-1668},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multichannel frequency estimation with constant amplitude via convex structured low-rank approximation},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kronecker product of tensors and hypergraphs: Structure and
dynamics. <em>SIMAX</em>, <em>45</em>(3), 1621–1642. (<a
href="https://doi.org/10.1137/23M1592547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Hypergraphs and tensors extend classic graph and matrix theories to account for multiway relationships, which are ubiquitous in engineering, biological, and social systems. While the Kronecker product is a potent tool for analyzing the coupling of systems in a graph or matrix context, its utility in studying multiway interactions, such as those represented by tensors and hypergraphs, remains elusive. In this article, we present a comprehensive exploration of algebraic, structural, and spectral properties of the tensor Kronecker product. We express Tucker and tensor train decompositions and various tensor eigenvalues in terms of the tensor Kronecker product. Additionally, we utilize the tensor Kronecker product to form Kronecker hypergraphs, which are tensor-based hypergraph products, and investigate the structure and stability of polynomial dynamics on Kronecker hypergraphs. Finally, we provide numerical examples to demonstrate the utility of the tensor Kronecker product in computing Z-eigenvalues, performing various tensor decompositions, and determining the stability of polynomial systems.},
  archive      = {J_SIMAX},
  author       = {Joshua Pickard and Can Chen and Cooper Stansbury and Amit Surana and Anthony M. Bloch and Indika Rajapakse},
  doi          = {10.1137/23M1592547},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1621-1642},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Kronecker product of tensors and hypergraphs: Structure and dynamics},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Growth factors of orthogonal matrices and local behavior of
gaussian elimination with partial and complete pivoting. <em>SIMAX</em>,
<em>45</em>(3), 1599–1620. (<a
href="https://doi.org/10.1137/23M1597733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Gaussian elimination (GE) is the most used dense linear solver. Error analysis of GE with selected pivoting strategies on well-conditioned systems can focus on studying the behavior of growth factors. Although exponential growth is possible with GE with partial pivoting (GEPP), growth tends to stay much smaller in practice. Support for this behavior was provided recently by Huang and Tikhomirov’s average-case analysis of GEPP, which showed GEPP growth factors for Gaussian matrices stay at most polynomial with very high probability. GE with complete pivoting (GECP) has also seen a lot of recent interest, with improvements to both lower and upper bounds on worst-case GECP growth provided by Bisain, Edelman, and Urschel in 2023. We are interested in studying how GEPP and GECP behave on the same linear systems as well as studying large growth on particular subclasses of matrices, including orthogonal matrices. Moreover, as a means to better address the question of why large growth is rarely encountered, we further study matrices with a large difference in growth between using GEPP and GECP, and we explore how the smaller growth strategy dominates behavior in a small neighborhood of the initial matrix.},
  archive      = {J_SIMAX},
  author       = {John Peca-Medlin},
  doi          = {10.1137/23M1597733},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1599-1620},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Growth factors of orthogonal matrices and local behavior of gaussian elimination with partial and complete pivoting},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A geometric approach to approximating the limit set of
eigenvalues for banded toeplitz matrices. <em>SIMAX</em>,
<em>45</em>(3), 1573–1598. (<a
href="https://doi.org/10.1137/23M1587804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article is about finding the limit set for banded Toeplitz matrices. Our main result is a new approach to approximate the limit set , where is the symbol of the banded Toeplitz matrix. The new approach is geometrical and based on the formula , where is a scaling factor, i.e., and denotes the spectrum. We show that the full intersection can be approximated by the intersection for a finite number of ’s and that the intersection of polygon approximations for yields an approximating polygon for that converges to in the Hausdorff metric. Further, we show that one can slightly expand the polygon approximations for to ensure that they contain . Then, taking the intersection yields an approximating superset of which converges to in the Hausdorff metric and is guaranteed to contain . Combining the established algebraic (root-finding) method with our approximating superset, we are able to give an explicit bound on the Hausdorff distance to the true limit set. We implement the algorithm in Python and test it. It performs on par to and better in some cases than existing algorithms. We argue, but do not prove, that the average time complexity of the algorithm is , where is the number of ’s and is the number of vertices for the polygons approximating . Further, we argue that the distance from to both the approximating polygon and the approximating superset decreases as for most of , where is the number of elementary operations required by the algorithm.},
  archive      = {J_SIMAX},
  author       = {Teodor Bucht and Jacob S. Christiansen},
  doi          = {10.1137/23M1587804},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1573-1598},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A geometric approach to approximating the limit set of eigenvalues for banded toeplitz matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random walks, conductance, and resistance for the connection
graph laplacian. <em>SIMAX</em>, <em>45</em>(3), 1541–1572. (<a
href="https://doi.org/10.1137/23M1595400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate the concept of effective resistance in connection graphs, expanding its traditional application from undirected graphs. We propose a robust definition of effective resistance in connection graphs by focusing on the duality of Dirichlet-type and Poisson-type problems on connection graphs. Additionally, we delve into random walks, taking into account both node transitions and vector rotations. This approach introduces novel concepts of effective conductance and resistance matrices for connection graphs, capturing mean rotation matrices corresponding to random walk transitions. Thereby, it provides new theoretical insights for network analysis and optimization.},
  archive      = {J_SIMAX},
  author       = {Alexander Cloninger and Gal Mishne and Andreas Oslandsbotn and Sawyer J. Robertson and Zhengchao Wan and Yusu Wang},
  doi          = {10.1137/23M1595400},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1541-1572},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Random walks, conductance, and resistance for the connection graph laplacian},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small singular values can increase in lower precision.
<em>SIMAX</em>, <em>45</em>(3), 1518–1540. (<a
href="https://doi.org/10.1137/23M1557209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We perturb a real matrix of full column rank, and derive lower bounds for the smallest singular values of the perturbed matrix, in terms of normwise absolute perturbations. Our bounds, which extend existing lower-order expressions, demonstrate the potential increase in the smallest singular values and represent a qualitative model for the increase in the small singular values after a matrix has been downcast to a lower arithmetic precision. Numerical experiments confirm the qualitative validity of this model and its ability to predict singular values changes in the presence of decreased arithmetic precision.},
  archive      = {J_SIMAX},
  author       = {Christos Boutsikas and Petros Drineas and Ilse C. F. Ipsen},
  doi          = {10.1137/23M1557209},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1518-1540},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Small singular values can increase in lower precision},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reorthogonalized block classical gram–schmidt using two
cholesky-based TSQR algorithms. <em>SIMAX</em>, <em>45</em>(3),
1487–1517. (<a href="https://doi.org/10.1137/23M1605387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In [Numer. Math., 23 (2013), pp. 395–423], Barlow and Smoktunowicz propose the reorthogonalized block classical Gram–Schmidt algorithm BCGS2. New conditions for the backward stability of BCGS2 that allow the use of a more flexible version of that algorithm are given. Backward stability for BCGS2 means that, in floating point arithmetic with machine precision , for a full column rank , the algorithm produces and upper triangular such that and . However, each major step of BCGS2 requires the QR factorization of two intermediate matrices and . In many applications of interest , thus these factorizations are called “tall, skinny” QR (TSQR) operations. Each such factorization was assumed to produce , such that and . For this suboperation, the first of these two conditions limits the choice of QR factorization algorithms to those, such as Householder and Givens QR, which may not produce the as efficiently as some with weaker orthogonality restrictions. For the second of these QR factorizations, it is shown that the Cholesky decomposition of followed by the can be substituted without a significant change in the conditions for backward stability. With slightly stronger restrictions, the first QR decomposition can be done by algorithms such as the mixed precision CholQR algorithm described by Yamazaki, Tomov, and Dongarra [SIAM J. Sci. Comput., 37 (2015), pp. C307–C330]. In a GPU/CPU environment, Yamazaki, Tomov, and Dongarra showed that algorithm to be a very efficient method of producing the TSQR. Given that a common application of Gram–Schmidt algorithms is in the implementation of Krylov subspace methods, such as block GMRES, these results make the BCGS2 algorithm more broadly applicable.},
  archive      = {J_SIMAX},
  author       = {Jesse L. Barlow},
  doi          = {10.1137/23M1605387},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1487-1517},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Reorthogonalized block classical Gram–Schmidt using two cholesky-based TSQR algorithms},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational characterization and rayleigh quotient iteration
of 2D eigenvalue problem with applications. <em>SIMAX</em>,
<em>45</em>(3), 1455–1486. (<a
href="https://doi.org/10.1137/22M1472589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A two dimensional eigenvalue problem (2DEVP) of a Hermitian matrix pair is introduced in this paper. The 2DEVP can be regarded as a linear algebra formulation of the well-known eigenvalue optimization problem of the parameter matrix . We first present fundamental properties of the 2DEVP, such as the existence and variational characterizations of 2D-eigenvalues, and then devise a Rayleigh quotient iteration (RQI)-like algorithm, 2DRQI in short, for computing a 2D-eigentriplet of the 2DEVP. The efficacy of the 2DRQI is demonstrated by large scale eigenvalue optimization problems arising from the minmax of Rayleigh quotients and the distance to instability of a stable matrix.},
  archive      = {J_SIMAX},
  author       = {Tianyi Lu and Yangfeng Su and Zhaojun Bai},
  doi          = {10.1137/22M1472589},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1455-1486},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Variational characterization and rayleigh quotient iteration of 2D eigenvalue problem with applications},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block-diagonalization of quaternion circulant matrices with
applications. <em>SIMAX</em>, <em>45</em>(3), 1429–1454. (<a
href="https://doi.org/10.1137/23M1552115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is well known that a complex circulant matrix can be diagonalized by a discrete Fourier matrix with imaginary unit . The main aim of this paper is to demonstrate that a quaternion circulant matrix cannot be diagonalized by a discrete quaternion Fourier matrix with three imaginary units , and . Instead, a quaternion circulant matrix can be block-diagonalized into 1-by-1 block and 2-by-2 block matrices by permuted discrete quaternion Fourier transform matrix. With such a block-diagonalized form, the inverse of a quaternion circulant matrix can be determined efficiently similarly to the inverse of a complex circulant matrix. We make use of this block-diagonalized form to study quaternion tensor singular value decomposition of quaternion tensors where the entries are quaternion numbers. The applications, including computing the inverse of a quaternion circulant matrix and solving quaternion Toeplitz systems arising from linear prediction of quaternion signals, are employed to validate the efficiency of our proposed block- diagonalized results.},
  archive      = {J_SIMAX},
  author       = {Junjun Pan and Michael K. Ng},
  doi          = {10.1137/23M1552115},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1429-1454},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Block-diagonalization of quaternion circulant matrices with applications},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On semidefinite programming characterizations of the
numerical radius and its dual norm. <em>SIMAX</em>, <em>45</em>(3),
1414–1428. (<a href="https://doi.org/10.1137/23M160356X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We state and give self-contained proofs of semidefinite programming characterizations of the numerical radius and its dual norm for matrices. We show that the computation of the numerical radius and its dual norm within precision are polynomially time computable in the data and using either the ellipsoid method or the short step, primal interior point method. We apply our results to give a simple formula for the spectral and the nuclear norm of a real tensor in terms of the numerical radius and its dual norm.},
  archive      = {J_SIMAX},
  author       = {Shmuel Friedland and Chi-Kwong Li},
  doi          = {10.1137/23M160356X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1414-1428},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On semidefinite programming characterizations of the numerical radius and its dual norm},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral transformation for the dense symmetric semidefinite
generalized eigenvalue problem. <em>SIMAX</em>, <em>45</em>(3),
1392–1413. (<a href="https://doi.org/10.1137/24M162916X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The spectral transformation Lanczos method for the sparse symmetric definite generalized eigenvalue problem for matrices and is an iterative method that addresses the case of semidefinite or ill-conditioned using a shifted and inverted formulation of the problem. This paper proposes the same approach for dense problems and shows that with a shift chosen in accordance with certain constraints, the algorithm can conditionally ensure that every computed shifted and inverted eigenvalue is close to the exact shifted and inverted eigenvalue of a pair of matrices close to and . Under the same assumptions on the shift, the analysis of the algorithm for the shifted and inverted problem leads to useful error bounds for the original problem, including a bound that shows how a single shift that is of moderate size in a scaled sense can be chosen so that every computed generalized eigenvalue corresponds to a generalized eigenvalue of a pair of matrices close to and . The computed generalized eigenvectors give a relative residual that depends on the distance between the corresponding generalized eigenvalue and the shift. If the shift is of moderate size, then relative residuals are small for generalized eigenvalues that are not much larger than the shift. Larger shifts give small relative residuals for generalized eigenvalues that are not much larger or smaller than the shift.},
  archive      = {J_SIMAX},
  author       = {Michael Stewart},
  doi          = {10.1137/24M162916X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1392-1413},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Spectral transformation for the dense symmetric semidefinite generalized eigenvalue problem},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-pass nyström approximation in mixed precision.
<em>SIMAX</em>, <em>45</em>(3), 1361–1391. (<a
href="https://doi.org/10.1137/22M154079X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Low-rank matrix approximations appear in a number of scientific computing applications. We consider the Nyström method for approximating a positive semidefinite matrix . In the case that is very large or its entries can only be accessed once, a single-pass version may be necessary. In this work, we perform a complete rounding error analysis of the single-pass Nyström method in two precisions, where the computation of the expensive matrix product with is assumed to be performed in the lower of the two precisions. Our analysis gives insight into how the sketching matrix and shift should be chosen to ensure stability, implementation aspects which have been commented on in the literature but not yet rigorously justified. We further develop a heuristic to determine how to pick the lower precision, which confirms the general intuition that the lower the desired rank of the approximation, the lower the precision we can use without detriment. We also demonstrate that our mixed precision Nyström method can be used to inexpensively construct limited memory preconditioners for the conjugate gradient method and derive a bound on the condition number of the resulting preconditioned coefficient matrix. We present numerical experiments on a set of matrices with various spectral decays and demonstrate the utility of our mixed precision approach.},
  archive      = {J_SIMAX},
  author       = {Erin Carson and Ieva Daužickaitė},
  doi          = {10.1137/22M154079X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1361-1391},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Single-pass nyström approximation in mixed precision},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposition of a tensor into multilinear rank-<span
class="math inline">(<em>M</em><sub><em>r</em></sub>, <em>N</em><sub><em>r</em></sub>, ⋅)</span>
terms. <em>SIMAX</em>, <em>45</em>(3), 1341–1365. (<a
href="https://doi.org/10.1137/23M1557246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present new generic and deterministic uniqueness results for block term decompositions (BTDs). These uniqueness conditions hold under mild assumptions and apply to more general settings than previously known results. We also present an algebraic algorithm for the computation of BTDs. Our algorithm requires no knowledge of the block sizes appearing in the BTD: these block sizes are recovered from the algorithm. Through numerical simulations, we illustrate that, in contrast to competing optimization-based methods, even in noisy settings our algebraic algorithm can successfully recover an underlying BTD without knowledge of block sizes provided the signal-to-noise ratio is sufficiently high. We observe that the algorithm can significantly improve one’s ability to successfully recover a BTD when it is used as an algebraic initialization for leading optimization routines. Moreover, only a few optimization iterations are required to successfully converge to the BTD from the algebraic solution.},
  archive      = {J_SIMAX},
  author       = {Ignat Domanov and Nico Vervliet and Eric Evert and Lieven De Lathauwer},
  doi          = {10.1137/23M1557246},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1341-1365},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Decomposition of a tensor into multilinear rank-\({(M_{{r}},N_{{r}},\cdot )}\) terms},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenstructure perturbations for a class of hamiltonian
matrices and solutions of related riccati inequalities. <em>SIMAX</em>,
<em>45</em>(3), 1335–1360. (<a
href="https://doi.org/10.1137/23M1619563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The characterization of the solution set for a class of algebraic Riccati inequalities is studied. This class arises in the passivity analysis of linear time-invariant control systems. Eigenvalue perturbation theory for the Hamiltonian matrix associated with the Riccati inequality is used to analyze the extremal points of the solution set.},
  archive      = {J_SIMAX},
  author       = {Volker Mehrmann and Hongguo Xu},
  doi          = {10.1137/23M1619563},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1335-1360},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Eigenstructure perturbations for a class of hamiltonian matrices and solutions of related riccati inequalities},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the two-parameter matrix pencil problem. <em>SIMAX</em>,
<em>45</em>(3), 1318–1340. (<a
href="https://doi.org/10.1137/23M1545963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The multiparameter matrix pencil problem (MPP) is a generalization of the one-parameter MPP: Given a set of , complex matrices with , it is required to find all complex scalars , not all zero, such that the matrix pencil loses column rank and the corresponding nonzero complex vector such that . We call the -tuple an eigenvalue and the corresponding vector an eigenvector. This problem is related to the well-known multiparameter eigenvalue problem, except that there is only one pencil and, crucially, the matrices are not necessarily square. This paper uses our preliminary investigation in F. F. Alsubaie [ Optimal Model Reduction for Linear Dynamic Systems and the Solution of Multiparameter Matrix Pencil Problems, PhD thesis, Imperial College London, 2019], which presents a theoretical study of the multiparameter MPP and its applications in the optimal model reduction problem, to give a full solution to the two-parameter MPP. First, an inflation process is implemented to show that the two-parameter MPP is equivalent to a set of three simultaneous one-parameter MPPs. These problems are given in terms of Kronecker commutator operators (involving the original matrices) that exhibit several symmetries. These symmetries are analyzed and are then used to deflate the dimensions of the one-parameter MPPs to , thus simplifying their numerical solution. In the case in which , it is shown that the two-parameter MPP has at least one solution and generically solutions, and furthermore that, under a rank assumption, the Kronecker determinant operators satisfy a commutativity property. This is then used to show that the two-parameter MPP is equivalent to a set of three simultaneous eigenvalue problems of dimension . A general solution algorithm is presented and numerical examples are given to outline the procedure of the proposed algorithm.},
  archive      = {J_SIMAX},
  author       = {Satin K. Gungah and Fawwaz F. Alsubaie and Imad M. Jaimoukha},
  doi          = {10.1137/23M1545963},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1318-1340},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On the two-parameter matrix pencil problem},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On adaptive stochastic heavy ball momentum for solving
linear systems. <em>SIMAX</em>, <em>45</em>(3), 1259–1286. (<a
href="https://doi.org/10.1137/23M1575883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The stochastic heavy ball momentum (SHBM) method has gained considerable popularity as a scalable approach for solving large-scale optimization problems. However, one limitation of this method is its reliance on prior knowledge of certain problem parameters, such as singular values of a matrix. In this paper, we propose an adaptive variant of the SHBM method for solving stochastic problems that are reformulated from linear systems using user-defined distributions. Our adaptive SHBM (ASHBM) method utilizes iterative information to update the parameters, addressing an open problem in the literature regarding the adaptive learning of momentum parameters. We prove that our method converges linearly in expectation, with a better convergence bound compared to the basic method. Notably, we demonstrate that the deterministic version of our ASHBM algorithm can be reformulated as a variant of the conjugate gradient (CG) method, inheriting many of its appealing properties, such as finite-time convergence. Consequently, the ASHBM method can be further generalized to develop a brand-new framework of the stochastic CG method for solving linear systems. Our theoretical results are supported by numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Yun Zeng and Deren Han and Yansheng Su and Jiaxin Xie},
  doi          = {10.1137/23M1575883},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1259-1286},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On adaptive stochastic heavy ball momentum for solving linear systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On compatible transfer operators in nonsymmetric algebraic
multigrid. <em>SIMAX</em>, <em>45</em>(3), 1245–1258. (<a
href="https://doi.org/10.1137/23M1586069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The standard goal for an effective algebraic multigrid (AMG) algorithm is to develop relaxation and coarse-grid correction schemes that attenuate complementary error modes. In the nonsymmetric setting, coarse-grid correction will almost certainly be nonorthogonal (and divergent) in any known standard product, meaning . This introduces a new consideration, that one wants coarse-grid correction to be as close to orthogonal as possible, in an appropriate norm. In addition, due to nonorthogonality, may actually amplify certain error modes that are in the range of interpolation. Relaxation must then not only be complementary to interpolation, but also rapidly eliminate any error amplified by the nonorthogonal correction, or the algorithm may diverge. This paper develops analytic formulae on how to construct “compatible” transfer operators in nonsymmetric AMG such that in some standard matrix-induced norm. Discussion is provided on different options for the norm in the nonsymmetric setting, the relation between “ideal” transfer operators in different norms, and insight into the convergence of nonsymmetric reduction-based AMG.},
  archive      = {J_SIMAX},
  author       = {Ben S. Southworth and Thomas A. Manteuffel},
  doi          = {10.1137/23M1586069},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1245-1258},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On compatible transfer operators in nonsymmetric algebraic multigrid},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-dimensional tensor network recovery. <em>SIMAX</em>,
<em>45</em>(3), 1217–1244. (<a
href="https://doi.org/10.1137/23M159888X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the recovery of the underlying graphs or permutations for tensors in the tensor ring or tensor train format. Our proposed algorithms compare the matricization ranks after down-sampling, whose complexity is for th-order tensors. We prove that our algorithms can almost surely recover the correct graph or permutation when tensor entries can be observed without noise. We further establish the robustness of our algorithms against observational noise. The theoretical results are validated by numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Ziang Chen and Jianfeng Lu and Anru Zhang},
  doi          = {10.1137/23M159888X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1217-1244},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {One-dimensional tensor network recovery},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum: Properties of the solution set of absolute value
equations and the related matrix classes. <em>SIMAX</em>,
<em>45</em>(2), 1215. (<a
href="https://doi.org/10.1137/24M1635715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A typo in the paper [M. Hladík, SIAM J. Matrix Anal. Appl., 44 (2023), pp. 175–195] is corrected.},
  archive      = {J_SIMAX},
  author       = {Milan Hladík},
  doi          = {10.1137/24M1635715},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1215},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Erratum: Properties of the solution set of absolute value equations and the related matrix classes},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and accurate randomized algorithms for linear systems
and eigenvalue problems. <em>SIMAX</em>, <em>45</em>(2), 1183–1214. (<a
href="https://doi.org/10.1137/23M1565413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper develops a class of algorithms for general linear systems and eigenvalue problems. These algorithms apply fast randomized dimension reduction (“sketching”) to accelerate standard subspace projection methods, such as GMRES and Rayleigh–Ritz. This modification makes it possible to incorporate nontraditional bases for the approximation subspace that are easier to construct. When the basis is numerically full rank, the new algorithms have accuracy similar to classic methods but run faster and may use less storage. For model problems, numerical experiments show large advantages over the optimized MATLAB routines, including a speedup over and a speedup over .},
  archive      = {J_SIMAX},
  author       = {Yuji Nakatsukasa and Joel A. Tropp},
  doi          = {10.1137/23M1565413},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1183-1214},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast and accurate randomized algorithms for linear systems and eigenvalue problems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preconditioner design via bregman divergences.
<em>SIMAX</em>, <em>45</em>(2), 1148–1182. (<a
href="https://doi.org/10.1137/23M1566637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study a preconditioner for a Hermitian positive definite linear system, which is obtained as the solution of a matrix nearness problem based on the Bregman log determinant divergence. The preconditioner is of the form of a Hermitian positive definite matrix plus a low-rank matrix. For this choice of structure, the generalized eigenvalues of the preconditioned matrix are easily calculated, and we show under which conditions the preconditioner minimizes the condition number of the preconditioned matrix. We develop practical numerical approximations of the preconditioner based on the randomized singular value decomposition (SVD) and the Nyström approximation and provide corresponding approximation results. Furthermore, we prove that the Nyström approximation is in fact also a matrix approximation in a range-restricted Bregman divergence and establish several connections between this divergence and matrix nearness problems in different measures. Numerical examples are provided to support the theoretical results.},
  archive      = {J_SIMAX},
  author       = {Andreas A. Bock and Martin S. Andersen},
  doi          = {10.1137/23M1566637},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1148-1182},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Preconditioner design via bregman divergences},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A skew-symmetric lanczos bidiagonalization method for
computing several extremal eigenpairs of a large skew-symmetric matrix.
<em>SIMAX</em>, <em>45</em>(2), 1114–1147. (<a
href="https://doi.org/10.1137/23M1553029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The spectral decomposition of a real skew-symmetric matrix is shown to be equivalent to a specific structured singular value decomposition (SVD) of the matrix. Based on such equivalence, we propose a skew-symmetric Lanczos bidiagonalization (SSLBD) method to compute extremal singular values and the corresponding singular vectors of the matrix, from which its extremal conjugate eigenpairs are recovered pairwise in real arithmetic. A number of convergence results on the method are established, and accuracy estimates for approximate singular triplets are given. In finite precision arithmetic, it is proven that the semi-orthogonality of each set of the computed left and right Lanczos basis vectors and the semi-biorthogonality of two sets of basis vectors are needed to compute the singular values accurately and to make the method work as if it does in exact arithmetic. A commonly used efficient partial reorthogonalization strategy is adapted to maintain the desired semi-orthogonality and semi-biorthogonality. For practical purpose, an implicitly restarted SSLBD algorithm is developed with partial reorthogonalization. Numerical experiments illustrate the effectiveness and overall efficiency of the algorithm.},
  archive      = {J_SIMAX},
  author       = {Jinzhi Huang and Zhongxiao Jia},
  doi          = {10.1137/23M1553029},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1114-1147},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A skew-symmetric lanczos bidiagonalization method for computing several extremal eigenpairs of a large skew-symmetric matrix},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential geometry with extreme eigenvalues in the
positive semidefinite cone. <em>SIMAX</em>, <em>45</em>(2), 1089–1113.
(<a href="https://doi.org/10.1137/23M1563906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Differential geometric approaches to the analysis and processing of data in the form of symmetric positive definite (SPD) matrices have had notable successful applications to numerous fields, including computer vision, medical imaging, and machine learning. The dominant geometric paradigm for such applications has consisted of a few Riemannian geometries associated with spectral computations that are costly at high scale and in high dimensions. We present a route to a scalable geometric framework for the analysis and processing of SPD-valued data based on the efficient computation of extreme generalized eigenvalues through the Hilbert and Thompson geometries of the semidefinite cone. We explore a particular geodesic space structure based on Thompson geometry in detail and establish several properties associated with this structure. Furthermore, we define a novel inductive mean of SPD matrices based on this geometry and prove its existence and uniqueness for a given finite collection of points. Finally, we state and prove a number of desirable properties that are satisfied by this mean.},
  archive      = {J_SIMAX},
  author       = {Cyrus Mostajeran and Nathaël Da Costa and Graham Van Goffrier and Rodolphe Sepulchre},
  doi          = {10.1137/23M1563906},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1089-1113},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Differential geometry with extreme eigenvalues in the positive semidefinite cone},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic rounding error analysis of modified
gram–schmidt. <em>SIMAX</em>, <em>45</em>(2), 1076–1088. (<a
href="https://doi.org/10.1137/23M1585817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Building on probabilistic approaches, we study the finite precision behavior of modified Gram–Schmidt (MGS). Based on concentration inequalities and the Sheffield structure, we provide a rigorous probabilistic analysis of MGS in terms of residual, a form of backward error, and loss of orthogonality. In particular, when the QR factorization of a full column rank matrix is computed by MGS, the best-known worst-case bound in terms of loss of orthogonality is of order for unit roundoff and condition number . We show that this bound can be improved to with high probability.},
  archive      = {J_SIMAX},
  author       = {Qinmeng Zou},
  doi          = {10.1137/23M1585817},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1076-1088},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Probabilistic rounding error analysis of modified Gram–Schmidt},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Riemannian preconditioned coordinate descent for low
multilinear rank approximation. <em>SIMAX</em>, <em>45</em>(2),
1054–1075. (<a href="https://doi.org/10.1137/21M1463896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper presents a memory-efficient, first-order method for low multilinear rank approximation of high-order, high-dimensional tensors. In our method, we exploit the second-order information of the cost function and the constraints to suggest a new Riemannian metric on the Grassmann manifold. We use a Riemmanian coordinate descent method for solving the problem and also provide a global convergence analysis matching that of the coordinate descent method in the Euclidean setting. We also show that each step of our method with the unit step size is actually a step of the orthogonal iteration algorithm. Experimental results show the computational advantage of our method for high-dimensional tensors.},
  archive      = {J_SIMAX},
  author       = {Mohammad Hamed and Reshad Hosseini},
  doi          = {10.1137/21M1463896},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1054-1075},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Riemannian preconditioned coordinate descent for low multilinear rank approximation},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameterized interpolation of passive systems.
<em>SIMAX</em>, <em>45</em>(2), 1035–1053. (<a
href="https://doi.org/10.1137/23M1580528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the tangential interpolation problem for a passive transfer function in standard state-space form. We derive new interpolation conditions based on the computation of a deflating subspace associated with a selection of spectral zeros of a parameterized para-Hermitian transfer function. We show that this technique improves the robustness of the low order model and that it can also be applied to nonpassive systems, provided they have sufficiently many spectral zeros in the open right half-plane. We analyze the accuracy needed for the computation of the deflating subspace, in order to still have a passive lower order model and we derive a novel selection procedure of spectral zeros in order to obtain low order models with a small approximation error.},
  archive      = {J_SIMAX},
  author       = {Peter Benner and Pawan Goyal and Paul Van Dooren},
  doi          = {10.1137/23M1580528},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1035-1053},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Parameterized interpolation of passive systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral analysis of preconditioned matrices arising from
stage-parallel implicit runge–kutta methods of arbitrarily high order.
<em>SIMAX</em>, <em>45</em>(2), 1007–1034. (<a
href="https://doi.org/10.1137/23M1552498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The use of high order fully implicit Runge–Kutta methods is of significant importance in the context of the numerical solution of transient partial differential equations, in particular when solving large scale problems due to fine space resolution with many millions of spatial degrees of freedom and long time intervals. In this study we consider strongly -stable implicit Runge–Kutta methods of arbitrary order of accuracy, based on Radau quadratures, for which efficient preconditioners have been introduced. A refined spectral analysis of the corresponding matrices and matrix sequences is presented, both in terms of localization and asymptotic global distribution of the eigenvalues. Specific expressions of the eigenvectors are also obtained. The given study fully agrees with the numerically observed spectral behavior and substantially improves the theoretical studies done in this direction so far. Concluding remarks and open problems end the current work, with specific attention to the potential generalizations of the hereby suggested general approach.},
  archive      = {J_SIMAX},
  author       = {Ivo Dravins and Stefano Serra-Capizzano and Maya Neytcheva},
  doi          = {10.1137/23M1552498},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1007-1034},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Spectral analysis of preconditioned matrices arising from stage-parallel implicit Runge–Kutta methods of arbitrarily high order},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A note on the randomized kaczmarz algorithm for solving
doubly noisy linear systems. <em>SIMAX</em>, <em>45</em>(2), 992–1006.
(<a href="https://doi.org/10.1137/23M155712X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Large-scale linear systems, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz algorithm (RK) has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, . Unfortunately, in practice, that is not always the case; the coefficient matrix can also be noisy. In this paper, we analyze the convergence of RK for doubly noisy linear systems, i.e., when the coefficient matrix, has additive or multiplicative noise, and is also noisy. In our analyses, the quantity influences the convergence of RK, where represents a noisy version of . We claim that our analysis is robust and realistically applicable, as we do not require information about the noiseless coefficient matrix, and by considering different conditions on noise, we can control the convergence of RK. We perform numerical experiments to substantiate our theoretical findings.},
  archive      = {J_SIMAX},
  author       = {El Houcine Bergou and Soumia Boucherouite and Aritra Dutta and Xin Li and Anna Ma},
  doi          = {10.1137/23M155712X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {992-1006},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A note on the randomized kaczmarz algorithm for solving doubly noisy linear systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Some new results on the maximum growth factor in gaussian
elimination. <em>SIMAX</em>, <em>45</em>(2), 967–991. (<a
href="https://doi.org/10.1137/23M1571903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper combines modern numerical computation with theoretical results to improve our understanding of the growth factor problem for Gaussian elimination. On the computational side we obtain lower bounds for the maximum growth for complete pivoting for and using the Julia JuMP optimization package. At we obtain a growth factor bigger than . The numerical evidence suggests that the maximum growth factor is bigger than if and only if . We also present a number of theoretical results. We show that the maximum growth factor over matrices with entries restricted to a subset of the reals is nearly equal to the maximum growth factor over all real matrices. We also show that the growth factors under floating point arithmetic and exact arithmetic are nearly identical. Finally, through numerical search, and stability and extrapolation results, we provide improved lower bounds for the maximum growth factor. Specifically, we find that the largest growth factor is bigger than for , and the lim sup of the ratio with is greater than or equal to . In contrast to the old conjecture that growth might never be bigger than , it seems likely that the maximum growth divided by goes to infinity as .},
  archive      = {J_SIMAX},
  author       = {Alan Edelman and John Urschel},
  doi          = {10.1137/23M1571903},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {967-991},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Some new results on the maximum growth factor in gaussian elimination},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditioning of matrix functions at quasi-triangular
matrices. <em>SIMAX</em>, <em>45</em>(2), 954–966. (<a
href="https://doi.org/10.1137/22M1543689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The area of matrix functions has received growing interest for a long period of time due to their growing applications. Having a numerical algorithm for a matrix function, the ideal situation is to have an estimate or bound for the error returned alongside the solution. Condition numbers serve this purpose; they measure the first-order sensitivity of matrix functions to perturbations in the input data. We have observed that the existing unstructured condition number leads most of the time to inferior bounds of relative forward errors for some matrix functions at triangular and quasi-triangular matrices. We propose a condition number of matrix functions exploiting the structure of triangular and quasi-triangular matrices. We then adapt an existing algorithm for exact computation of the unstructured condition number to an algorithm for exact evaluation of the structured condition number. Although these algorithms are direct rather than iterative and useful for testing the numerical stability of numerical algorithms, they are less practical for relatively large problems. Therefore, we use an implicit power method approach to estimate the structured condition number. Our numerical experiments show that the structured condition number captures the behavior of the numerical algorithms and provides sharp bounds for the relative forward errors. In addition, the experiment indicates that the power method algorithm is reliable to estimate the structured condition number.},
  archive      = {J_SIMAX},
  author       = {Awad H. Al-Mohy},
  doi          = {10.1137/22M1543689},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {954-966},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Conditioning of matrix functions at quasi-triangular matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Permutation-invariant log-euclidean geometries on full-rank
correlation matrices. <em>SIMAX</em>, <em>45</em>(2), 930–953. (<a
href="https://doi.org/10.1137/22M1538144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. There is a growing interest in defining specific tools on correlation matrices which depart from those suited to SPD matrices. Several geometries have been defined on the open elliptope of full-rank correlation matrices: some are permutation-invariant, some others are log-Euclidean, i.e., diffeomorphic to a Euclidean space. In this work, we prove the existence of permutation-invariant log-Euclidean metrics by defining the families of off-log metrics and log-scaled metrics. First, we prove that the recently introduced off-log bijection is a smooth diffeomorphism, allowing pullback of (permutation-invariant) inner products. We introduce the “cor-inverse” involution on the open elliptope, which can be seen as analogous to the inversion of SPD matrices. We show that off-log metrics are not inverse-consistent. That is why, second, we define the log-scaling smooth diffeomorphism between the open elliptope and the vector space of symmetric matrices with null row sums. This map is based on the congruence action of positive diagonal matrices on SPD matrices, more precisely on the existence and uniqueness of a “scaling,” i.e., an SPD matrix with unit row sums within an orbit. Thanks to this multiplicative approach, log-scaled metrics are inverse-consistent. We provide the main Riemannian operations in closed form for the two families modulo the computation of the respective bijections.},
  archive      = {J_SIMAX},
  author       = {Yann Thanwerdas},
  doi          = {10.1137/22M1538144},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {930-953},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Permutation-invariant log-euclidean geometries on full-rank correlation matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are sketch-and-precondition least squares solvers
numerically stable? <em>SIMAX</em>, <em>45</em>(2), 905–929. (<a
href="https://doi.org/10.1137/23M1551973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sketch-and-precondition techniques are efficient and popular for solving large least squares (LS) problems of the form with and . This is where is “sketched” to a smaller matrix with for some constant before an iterative LS solver computes the solution to with a right preconditioner , where is constructed from . Prominent sketch-and-precondition LS solvers are Blendenpik and LSRN. We show that the sketch-and-precondition technique in its most commonly used form is not numerically stable for ill-conditioned LS problems. For provable and practical backward stability and optimal residuals, we suggest using an unpreconditioned iterative LS solver on with . Provided the condition number of is smaller than the reciprocal of the unit roundoff, we show that this modification ensures that the computed solution has a backward error comparable to the iterative LS solver applied to a well-conditioned matrix. Using smoothed analysis, we model floating-point rounding errors to argue that our modification is expected to compute a backward stable solution even for arbitrarily ill-conditioned LS problems. Additionally, we provide experimental evidence that using the sketch-and-solve solution as a starting vector in sketch-and-precondition algorithms (as suggested by Rokhlin and Tygert in 2008) should be highly preferred over the zero vector. The initialization often results in much more accurate solutions—albeit not always backward stable ones.},
  archive      = {J_SIMAX},
  author       = {Maike Meier and Yuji Nakatsukasa and Alex Townsend and Marcus Webb},
  doi          = {10.1137/23M1551973},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {905-929},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Are sketch-and-precondition least squares solvers numerically stable?},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic <span class="math inline"><em>p</em></span>th
root approximation of a stochastic matrix: A riemannian optimization
approach. <em>SIMAX</em>, <em>45</em>(2), 875–904. (<a
href="https://doi.org/10.1137/23M1589463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose two approaches, based on Riemannian optimization for computing a stochastic approximation of the th root of a stochastic matrix . In the first approach, the approximation is found in the Riemannian manifold of positive stochastic matrices. In the second approach, we introduce the Riemannian manifold of positive stochastic matrices sharing with the Perron eigenvector and we compute the approximation of the th root of in such a manifold. This way, differently from the available methods based on constrained optimization, and its th root approximation share the Perron eigenvector. Such a property is relevant, from a modeling point of view, in the embedding problem for Markov chains. The extended numerical experimentation shows that, in the first approach, the Riemannian optimization methods are generally faster and more accurate than the available methods based on constrained optimization. In the second approach, even though the stochastic approximation of the th root is found in a smaller set, the approximation is generally more accurate than the one obtained by standard constrained optimization.},
  archive      = {J_SIMAX},
  author       = {Fabio Durastante and Beatrice Meini},
  doi          = {10.1137/23M1589463},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {875-904},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Stochastic \({p}\)th root approximation of a stochastic matrix: A riemannian optimization approach},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A block householder–based algorithm for the QR decomposition
of hierarchical matrices. <em>SIMAX</em>, <em>45</em>(2), 847–874. (<a
href="https://doi.org/10.1137/22M1544555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Hierarchical matrices are dense but data-sparse matrices that use low-rank factorizations of suitable submatrices to reduce the storage and computational cost to linear-polylogarithmic complexity. In this paper, we propose a new approach to efficiently compute QR factorizations in the hierarchical matrix format based on block Householder transformations. To prevent unnecessarily high ranks in the resulting factors and to increase speed and accuracy, the algorithm meticulously tracks for which intermediate results low-rank factorizations are available. We also use a special storage scheme for the block Householder reflector to further reduce computational and storage costs. Numerical tests for two- and three-dimensional Laplacian boundary element matrices, different radial basis function kernel matrices, and matrices of typical hierarchical matrix structures but filled with random entries illustrate the performance of the new algorithm in comparison to some other QR algorithms for hierarchical matrices from the literature.},
  archive      = {J_SIMAX},
  author       = {Vincent Griem and Sabine Le Borne},
  doi          = {10.1137/22M1544555},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {847-874},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A block Householder–Based algorithm for the QR decomposition of hierarchical matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing vector orthogonalization algorithms.
<em>SIMAX</em>, <em>45</em>(2), 829–846. (<a
href="https://doi.org/10.1137/22M1519523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer implementations of vector orthogonalization algorithms produce a sequence of supposedly orthogonal vectors, but rounding-errors can cause loss of orthogonality and rank. Nevertheless these computational algorithms can be very effective as parts of various methods. We develop a general theory based on the augmented orthogonal matrix developed in [SIAM J. Matrix Anal. Appl., 31 (2009), pp. 565–583] that can be applied to any such algorithm. This can be combined with a rounding-error analysis of the algorithm to analyze its finite-precision behavior. We apply this combination to prove that a particular Lanczos tridiagonalization of a Hermitian matrix always computes components for which backward-stable solutions to , exist. If an appropriate rounding-error analysis is available, the approach can apparently be applied to any computation producing a sequence of supposedly orthogonal -vectors, where a linear combination of these vectors is intended to approximate some quantity.},
  archive      = {J_SIMAX},
  author       = {Christopher C. Paige},
  doi          = {10.1137/22M1519523},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {829-846},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Analyzing vector orthogonalization algorithms},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit quantum circuits for block encodings of certain
sparse matrices. <em>SIMAX</em>, <em>45</em>(1), 801–827. (<a
href="https://doi.org/10.1137/22M1484298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many standard linear algebra problems can be solved on a quantum computer by using recently developed quantum linear algebra algorithms that make use of block encodings and quantum eigenvalue/singular value transformations. A block encoding embeds a properly scaled matrix of interest in a larger unitary transformation that can be decomposed into a product of simpler unitaries and implemented efficiently on a quantum computer. Although quantum algorithms can potentially achieve exponential speedup in solving linear algebra problems compared to the best classical algorithm, such a gain in efficiency ultimately hinges on our ability to construct an efficient quantum circuit for the block encoding of , which is difficult in general, and not trivial even for well structured sparse matrices. In this paper, we give a few examples on how efficient quantum circuits can be explicitly constructed for some well structured sparse matrices and discuss a few strategies used in these constructions. We also provide implementations of these quantum circuits in MATLAB.},
  archive      = {J_SIMAX},
  author       = {Daan Camps and Lin Lin and Roel Van Beeumen and Chao Yang},
  doi          = {10.1137/22M1484298},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {801-827},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Explicit quantum circuits for block encodings of certain sparse matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial degeneration of tensors. <em>SIMAX</em>,
<em>45</em>(1), 771–800. (<a
href="https://doi.org/10.1137/23M1554898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Tensors are often studied by introducing preorders such as restriction and degeneration. The former describes transformations of the tensors by local linear maps on its tensor factors; the latter describes transformations where the local linear maps may vary along a curve, and the resulting tensor is expressed as a limit along this curve. In this work, we introduce and study partial degeneration, a special version of degeneration where one of the local linear maps is constant while the others vary along a curve. Motivated by algebraic complexity, quantum entanglement, and tensor networks, we present constructions based on matrix multiplication tensors and find examples by making a connection to the theory of prehomogeneous tensor spaces. We highlight the subtleties of this new notion by showing obstruction and classification results for the unit tensor. To this end, we study the notion of aided rank, a natural generalization of tensor rank. The existence of partial degenerations gives strong upper bounds on the aided rank of a tensor, which allows one to turn degenerations into restrictions. In particular, we present several examples, based on the W-tensor and the Coppersmith–Winograd tensors, where lower bounds on aided rank provide obstructions to the existence of certain partial degenerations.},
  archive      = {J_SIMAX},
  author       = {Matthias Christandl and Fulvio Gesmundo and Vladimir Lysikov and Vincent Steffan},
  doi          = {10.1137/23M1554898},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {771-800},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Partial degeneration of tensors},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive rational krylov methods for exponential runge–kutta
integrators. <em>SIMAX</em>, <em>45</em>(1), 744–770. (<a
href="https://doi.org/10.1137/23M1559439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the solution of large stiff systems of ODEs with explicit exponential Runge–Kutta integrators. These problems arise from semidiscretized semilinear parabolic PDEs on continuous domains or on inherently discrete graph domains. A series of results reduces the requirement of computing linear combinations of -functions in exponential integrators to the approximation of the action of a smaller number of matrix exponentials on certain vectors. State-of-the-art computational methods use polynomial Krylov subspaces of adaptive size for this task. They have the drawback that the required number of Krylov subspace iterations to obtain a desired tolerance increases drastically with the spectral radius of the discrete linear differential operator, e.g., the problem size. We present an approach that leverages rational Krylov subspace methods promising superior approximation qualities. We prove a novel a posteriori error estimate of rational Krylov approximations to the action of the matrix exponential on vectors for single time points, which allows for an adaptive approach similar to existing polynomial Krylov techniques. We discuss pole selection and the efficient solution of the arising sequences of shifted linear systems by direct and preconditioned iterative solvers. Numerical experiments show that our method outperforms the state of the art for sufficiently large spectral radii of the discrete linear differential operators. The key to this are approximately constant numbers of rational Krylov iterations, which enable a near-linear scaling of the runtime with respect to the problem size.},
  archive      = {J_SIMAX},
  author       = {Kai Bergermann and Martin Stoll},
  doi          = {10.1137/23M1559439},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {744-770},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Adaptive rational krylov methods for exponential Runge–Kutta integrators},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NlTGCR: A class of nonlinear acceleration procedures based
on conjugate residuals. <em>SIMAX</em>, <em>45</em>(1), 712–743. (<a
href="https://doi.org/10.1137/23M1576360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper develops a new class of nonlinear acceleration algorithms based on extending conjugate residual-type procedures from linear to nonlinear equations. The main algorithm has strong similarities with Anderson acceleration as well as with inexact Newton methods—depending on which variant is implemented. We prove theoretically and verify experimentally, on a variety of problems from simulation experiments to deep learning applications, that our method is a powerful accelerated iterative algorithm. The code is available at https://github.com/Data-driven-numerical-methods/Nonlinear-Truncated-Conjugate-Residual.},
  archive      = {J_SIMAX},
  author       = {Huan He and Ziyuan Tang and Shifan Zhao and Yousef Saad and Yuanzhe Xi},
  doi          = {10.1137/23M1576360},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {712-743},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {NlTGCR: A class of nonlinear acceleration procedures based on conjugate residuals},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An escape time formulation for subgraph detection and
partitioning of directed graphs. <em>SIMAX</em>, <em>45</em>(1),
685–711. (<a href="https://doi.org/10.1137/23M1553790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a rearrangement based algorithm for detection of subgraphs of k vertices with long escape times for directed or undirected networks that is not combinatorially complex to compute. Complementing other notions of densest subgraphs and graph cuts, our method is based on the mean hitting time required for a random walker to leave a designated set and hit the complement. We provide a new relaxation of this notion of hitting time on a given subgraph and use that relaxation to construct a subgraph detection algorithm that can be computed easily and a generalization to K-partitioning schemes. Using a modification of the subgraph detector on each component, we propose a graph partitioner that identifies regions where random walks live for comparably large times. Importantly, our method implicitly respects the directed nature of the data for directed graphs while also being applicable to undirected graphs. We apply the partitioning method for community detection to a large class of models and real-world data sets.},
  archive      = {J_SIMAX},
  author       = {Zachary M. Boyd and Nicolas Fraiman and Jeremy L. Marzuola and Peter J. Mucha and Braxton Osting},
  doi          = {10.1137/23M1553790},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {685-711},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An escape time formulation for subgraph detection and partitioning of directed graphs},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized joint diagonalization of symmetric matrices.
<em>SIMAX</em>, <em>45</em>(1), 661–684. (<a
href="https://doi.org/10.1137/22M1541265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Given a family of nearly commuting symmetric matrices, we consider the task of computing an orthogonal matrix that nearly diagonalizes every matrix in the family. In this paper, we propose and analyze randomized joint diagonalization (RJD) for performing this task. RJD applies a standard eigenvalue solver to random linear combinations of the matrices. Unlike existing optimization-based methods, RJD is simple to implement and leverages existing high-quality linear algebra software packages. Our main novel contribution is to prove robust recovery: Given a family that is -near to a commuting family, RJD jointly diagonalizes this family, with high probability, up to an error of norm . We also discuss how the algorithm can be further improved by deflation techniques and demonstrate its state-of-the-art performance by numerical experiments with synthetic and real-world data.},
  archive      = {J_SIMAX},
  author       = {Haoze He and Daniel Kressner},
  doi          = {10.1137/22M1541265},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {661-684},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Randomized joint diagonalization of symmetric matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Singular value decomposition of dual matrices and its
application to traveling wave identification in the brain.
<em>SIMAX</em>, <em>45</em>(1), 634–660. (<a
href="https://doi.org/10.1137/23M1556642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Matrix factorizations in dual number algebra, a hypercomplex number system, have been applied to kinematics, spatial mechanisms, and other fields recently. We develop an approach to identify spatiotemporal patterns in the brain such as traveling waves using the singular value decomposition (SVD) of dual matrices in this paper. Theoretically, we propose the compact dual singular value decomposition (CDSVD) of dual complex matrices with explicit expressions as well as a necessary and sufficient condition for its existence. Furthermore, based on the CDSVD, we report on the optimal solution to the best rank- approximation under a newly defined quasi-metric in the dual complex number system. The CDSVD is also related to the dual Moore–Penrose generalized inverse. Numerically, comparisons with other available algorithms are conducted, which indicate less computational costs of our proposed CDSVD. In addition, the infinitesimal part of the CDSVD can identify the true rank of the original matrix from the noise-added matrix, but the classical SVD cannot. Next, we employ experiments on simulated time-series data and a road monitoring video to demonstrate the beneficial effect of the infinitesimal parts of dual matrices in spatiotemporal pattern identification. Finally, we apply this approach to the large-scale brain functional magnetic resonance imaging data, identify three kinds of traveling waves, and further validate the consistency between our analytical results and the current knowledge of cerebral cortex function.},
  archive      = {J_SIMAX},
  author       = {Tong Wei and Weiyang Ding and Yimin Wei},
  doi          = {10.1137/23M1556642},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {634-660},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Singular value decomposition of dual matrices and its application to traveling wave identification in the brain},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speeding up krylov subspace methods for computing <span
class="math inline"><strong>f</strong><strong>(</strong><strong>A</strong><strong>)</strong><strong>b</strong></span>
via randomization. <em>SIMAX</em>, <em>45</em>(1), 619–633. (<a
href="https://doi.org/10.1137/22M1543458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work is concerned with the computation of the action of a matrix function f(A), such as the matrix exponential or the matrix square root, on a vector b. For a general matrix A, this can be done by computing the compression of A onto a suitable Krylov subspace. Such compression is usually computed by forming an orthonormal basis of the Krylov subspace using the Arnoldi method. In this work, we propose to compute (nonorthonormal) bases in a faster way and to use a fast randomized algorithm for least-squares problems to compute the compression of A onto the Krylov subspace. We present some numerical examples which show that our algorithms can be faster than the standard Arnoldi method while achieving comparable accuracy.},
  archive      = {J_SIMAX},
  author       = {Alice Cortinovis and Daniel Kressner and Yuji Nakatsukasa},
  doi          = {10.1137/22M1543458},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {619-633},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Speeding up krylov subspace methods for computing \(\boldsymbol{{f}(A){b}}\) via randomization},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient vectors for block perturbed consistent matrices.
<em>SIMAX</em>, <em>45</em>(1), 601–618. (<a
href="https://doi.org/10.1137/23M1580310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In prioritization schemes, based on pairwise comparisons, such as the analytical hierarchy process, it is important to extract a cardinal ranking vector from a reciprocal matrix that is unlikely to be consistent. It is natural to choose such a vector only from efficient ones. Recently, a method to generate inductively all efficient vectors for any reciprocal matrix has been discovered. Here we focus on the study of efficient vectors for a reciprocal matrix that is a block perturbation of a consistent matrix in the sense that it is obtained from a consistent matrix by modifying entries only in a proper principal submatrix. We determine an explicit class of efficient vectors for such matrices. Based on this, we give a description of all the efficient vectors in the 3-by-3 block perturbed case. In addition, we give sufficient conditions for the right Perron eigenvector of such matrices to be efficient and provide examples in which efficiency does not occur. Also, we consider a certain type of constant block perturbed consistent matrices, for which we may construct a class of efficient vectors, and demonstrate the efficiency of the Perron eigenvector. Appropriate examples are provided throughout.},
  archive      = {J_SIMAX},
  author       = {Susana Furtado and Charles Johnson},
  doi          = {10.1137/23M1580310},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {601-618},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient vectors for block perturbed consistent matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectrum maximizing products are not generically unique.
<em>SIMAX</em>, <em>45</em>(1), 585–600. (<a
href="https://doi.org/10.1137/23M1550621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is widely believed that typical finite families of matrices admit finite products that attain the joint spectral radius. This conjecture is supported by computational experiments and it naturally leads to the following question: are these spectrum maximizing products typically unique, up to cyclic permutations and powers? We answer this question negatively. As discovered by Horowitz around fifty years ago, there are products of matrices that always have the same spectral radius despite not being cyclic permutations of one another. We show that the simplest Horowitz products can be spectrum maximizing in a robust way; more precisely, we exhibit a small but nonempty open subset of pairs of matrices for which the products and are both spectrum maximizing.},
  archive      = {J_SIMAX},
  author       = {Jairo Bochi and Piotr Laskawiec},
  doi          = {10.1137/23M1550621},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {585-600},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Spectrum maximizing products are not generically unique},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perturbation and inverse problems of stochastic matrices.
<em>SIMAX</em>, <em>45</em>(1), 553–584. (<a
href="https://doi.org/10.1137/22M1489162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is a classical task in perturbation analysis to find norm bounds on the effect of a perturbation of a stochastic matrix to its stationary distribution, i.e., to the unique normalized left Perron eigenvector. A common assumption is to consider to be given and to find bounds on its impact, but in this paper, we rather focus on an inverse optimization problem called the target stationary distribution problem (TSDP). The starting point is a target stationary distribution, and we search for a perturbation of the minimum norm such that remains stochastic and has the desired target stationary distribution. It is shown that TSDP has relevant applications in the design of, for example, road networks, social networks, hyperlink networks, and queuing systems. The key to our approach is that we work with rank-1 perturbations. Building on those results for rank-1 perturbations, we provide heuristics for the TSDP that construct arbitrary rank perturbations as sums of appropriately constructed rank-1 perturbations.},
  archive      = {J_SIMAX},
  author       = {Joost Berkhout and Bernd Heidergott and Paul Van Dooren},
  doi          = {10.1137/22M1489162},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {553-584},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Perturbation and inverse problems of stochastic matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Five-precision GMRES-based iterative refinement.
<em>SIMAX</em>, <em>45</em>(1), 529–552. (<a
href="https://doi.org/10.1137/23M1549079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. GMRES-based iterative refinement in three precisions (GMRES-IR3), proposed by Carson and Higham in 2018, uses a low precision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation of iterative refinement using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision , except for the matrix–vector products and the application of the preconditioner, which require the use of extra precision . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. In this article, we propose to relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions for applying the preconditioned matrix–vector product and for the rest of the operations. We obtain the five-precision GMRES-based iterative refinement (GMRES-IR5) algorithm which has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3. We develop a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where three or more arithmetics are available, which is becoming very common, the number of possible combinations of precisions in GMRES-IR5 is extremely large. We provide an analysis of our theoretical results that identifies a relatively small subset of relevant combinations. By choosing from within this subset one can achieve different levels of tradeoff between cost and robustness, which allows for a finer choice of precisions depending on the problem difficulty and the available hardware. We carry out numerical experiments on random dense and SuiteSparse matrices to validate our theoretical analysis and discuss the complexity of GMRES-IR5.},
  archive      = {J_SIMAX},
  author       = {Patrick Amestoy and Alfredo Buttari and Nicholas J. Higham and Jean-Yves L’Excellent and Theo Mary and Bastien Vieublé},
  doi          = {10.1137/23M1549079},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {529-552},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Five-precision GMRES-based iterative refinement},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unifying framework for higher order derivatives of matrix
functions. <em>SIMAX</em>, <em>45</em>(1), 504–528. (<a
href="https://doi.org/10.1137/23M1580589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a theory for general partial derivatives of matrix functions of the form , where is a matrix path of several variables . Building on results by Mathias [SIAM J. Matrix Anal. Appl., 17 (1996), pp. 610–620] for the first order derivative, we develop a block upper triangular form for higher order partial derivatives. This block form is used to derive conditions for existence and a generalized Daleckiĭ–Kreĭn formula for higher order derivatives. We show that certain specializations of this formula lead to classical formulas of quantum perturbation theory. We show how our results are related to earlier results for higher order Fréchet derivatives. Block forms of complex step approximations are introduced, and we show how those are related to evaluation of derivatives through the upper triangular form. These relations are illustrated with numerical examples.},
  archive      = {J_SIMAX},
  author       = {Emanuel H. Rubensson},
  doi          = {10.1137/23M1580589},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {504-528},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A unifying framework for higher order derivatives of matrix functions},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Row or column completion of polynomial matrices of given
degree. <em>SIMAX</em>, <em>45</em>(1), 478–503. (<a
href="https://doi.org/10.1137/23M1564547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We solve the problem of characterizing the existence of a polynomial matrix of fixed degree when its eigenstructure (or part of it) and some of its rows (columns) are prescribed. More specifically, we present a solution to the row (column) completion problem of a polynomial matrix of given degree under different prescribed invariants: the whole eigenstructure, all of it but the row (column) minimal indices, and the finite and/or infinite structures. Moreover, we characterize the existence of a polynomial matrix with prescribed degree and eigenstructure over an arbitrary field.},
  archive      = {J_SIMAX},
  author       = {Agurtzane Amparan and Itziar Baragaña and Silvia Marcaida and Alicia Roca},
  doi          = {10.1137/23M1564547},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {478-503},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Row or column completion of polynomial matrices of given degree},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication lower bounds and optimal algorithms for
multiple tensor-times-matrix computation. <em>SIMAX</em>,
<em>45</em>(1), 450–477. (<a
href="https://doi.org/10.1137/22M1510443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Multiple tensor-times-matrix (Multi-TTM) is a key computation in algorithms for computing and operating with the Tucker tensor decomposition, which is frequently used in multidimensional data analysis. We establish communication lower bounds that determine how much data movement is required (under mild conditions) to perform the Multi-TTM computation in parallel. The crux of the proof relies on analytically solving a constrained, nonlinear optimization problem. We also present a parallel algorithm to perform this computation that organizes the processors into a logical grid with twice as many modes as the input tensor. We show that, with correct choices of grid dimensions, the communication cost of the algorithm attains the lower bounds and is therefore communication optimal. Finally, we show that our algorithm can significantly reduce communication compared to the straightforward approach of expressing the computation as a sequence of tensor-times-matrix operations when the input and output tensors vary greatly in size.},
  archive      = {J_SIMAX},
  author       = {Hussam Al Daas and Grey Ballard and Laura Grigori and Suraj Kumar and Kathryn Rouse},
  doi          = {10.1137/22M1510443},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {450-477},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Communication lower bounds and optimal algorithms for multiple tensor-times-matrix computation},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backpropagation through back substitution with a backslash.
<em>SIMAX</em>, <em>45</em>(1), 429–449. (<a
href="https://doi.org/10.1137/22M1532871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written “backslash” or Gaussian elimination on triangular systems of equations. Generally, the matrix elements are operators. This paper has three contributions: (i) it is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach; (ii) operators can be readily placed in matrices in software in programming languages such as Julia as an implementation option; (iii) we introduce a novel notation, “transpose dot” operator “” that allows for the reversal of operators. We further demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia [Bezanson et al., SIAM Rev., 59 (2017), pp. 65–98], and that it is possible to realize this abstraction in code. Our implementation shows how generic linear algebra can allow operators as elements of matrices. In contrast to “operator overloading,” where backslash would normally have to be rewritten to take advantage of operators, with “generic programming” there is no such need.},
  archive      = {J_SIMAX},
  author       = {Alan Edelman and Ekin Akyürek and Yuyang Wang},
  doi          = {10.1137/22M1532871},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {429-449},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Backpropagation through back substitution with a backslash},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). More on tensors with different rank and symmetric rank.
<em>SIMAX</em>, <em>45</em>(1), 419–428. (<a
href="https://doi.org/10.1137/23M1547159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This is a further discussion of a previous work of the author on tensors with different rank and symmetric rank. We point out several obstructions towards extending a complex number example to the real number setting and discuss several further questions raised in the literature.},
  archive      = {J_SIMAX},
  author       = {Yaroslav Shitov},
  doi          = {10.1137/23M1547159},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {419-428},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {More on tensors with different rank and symmetric rank},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted enumeration of nonbacktracking walks on weighted
graphs. <em>SIMAX</em>, <em>45</em>(1), 397–418. (<a
href="https://doi.org/10.1137/23M155219X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We extend the notion of nonbacktracking walks from unweighted graphs to graphs whose edges have a nonnegative weight. Here the weight associated with a walk is taken to be the product over the weights along the individual edges. We give two ways to compute the associated generating function, and corresponding node centrality measures. One method works directly on the original graph and one uses a line graph construction followed by a projection. The first method is more efficient, but the second has the advantage of extending naturally to time-evolving graphs. Based on these generating functions, we define and study corresponding centrality measures. Illustrative computational results are also provided.},
  archive      = {J_SIMAX},
  author       = {Francesca Arrigo and Desmond J. Higham and Vanni Noferini and Ryan Wood},
  doi          = {10.1137/23M155219X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {397-418},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Weighted enumeration of nonbacktracking walks on weighted graphs},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast algorithm for computing macaulay null spaces of
bivariate polynomial systems. <em>SIMAX</em>, <em>45</em>(1), 368–396.
(<a href="https://doi.org/10.1137/23M1550414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. As a crucial first step towards finding the (approximate) common roots of a (possibly overdetermined) bivariate polynomial system of equations, the problem of determining an explicit numerical basis for the right null space of the system’s Macaulay matrix is considered. If denotes the total degree of the bivariate polynomials of the system, the cost of computing a null space basis containing all system roots is floating point operations through standard numerical algebra techniques (e.g., a singular value decomposition, rank-revealing QR-decomposition). We show that it is actually possible to design an algorithm that reduces the complexity to . The proposed algorithm exploits the Toeplitz structures of the Macaulay matrix under a nongraded lexicographic ordering of its entries and uses the low displacement rank properties to efficiently convert it into a Cauchy-like matrix with the help of fast Fourier transforms. By modifying the classical Schur algorithm with total pivoting for Cauchy-like matrices, a compact representation of the right null space is eventually obtained from a rank-revealing LU-factorization. Details of the proposed method, including numerical experiments, are fully provided for the case wherein the polynomials are expressed in the monomial basis. Furthermore, it is shown that an analogous fast algorithm can also be formulated for polynomial systems expressed in the Chebyshev basis.},
  archive      = {J_SIMAX},
  author       = {Nithin Govindarajan and Raphaël Widdershoven and Shivkumar Chandrasekaran and Lieven De Lathauwer},
  doi          = {10.1137/23M1550414},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {368-396},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A fast algorithm for computing macaulay null spaces of bivariate polynomial systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient algorithm for integer lattice reduction.
<em>SIMAX</em>, <em>45</em>(1), 353–367. (<a
href="https://doi.org/10.1137/23M1557933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A lattice of integers is the collection of all linear combinations of a set of vectors for which all entries of the vectors are integers and all coefficients in the linear combinations are also integers. Lattice reduction refers to the problem of finding a set of vectors in a given lattice such that the collection of all integer linear combinations of this subset is still the entire original lattice and so that the Euclidean norms of the subset are reduced. The present paper proposes simple, efficient iterations for lattice reduction which are guaranteed to reduce the Euclidean norms of the basis vectors (the vectors in the subset) monotonically during every iteration. Each iteration selects the basis vector for which projecting off (with integer coefficients) the components of the other basis vectors along the selected vector minimizes the Euclidean norms of the reduced basis vectors. Each iteration projects off the components along the selected basis vector and efficiently updates all information required for the next iteration to select its best basis vector and perform the associated projections.},
  archive      = {J_SIMAX},
  author       = {François Charton and Kristin Lauter and Cathy Li and Mark Tygert},
  doi          = {10.1137/23M1557933},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {353-367},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An efficient algorithm for integer lattice reduction},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constraint-satisfying krylov solvers for
structure-preserving discretizations. <em>SIMAX</em>, <em>45</em>(1),
327–352. (<a href="https://doi.org/10.1137/22M1540624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A key consideration in the development of numerical schemes for time-dependent partial differential equations (PDEs) is the ability to preserve certain properties of the continuum solution, such as associated conservation laws or other geometric structures of the solution. There is a long history of the development and analysis of such structure-preserving discretization schemes, including both proofs that standard schemes have structure-preserving properties and proposals for novel schemes that achieve both high-order accuracy and exact preservation of certain properties of the continuum differential equation. When coupled with implicit time-stepping methods, a major downside to these schemes is that their structure-preserving properties generally rely on an exact solution of the (possibly nonlinear) systems of equations defining each time step in the discrete scheme. For small systems, this is often possible (up to the accuracy of floating-point arithmetic), but it becomes impractical for the large linear systems that arise when considering typical discretization of space-time PDEs. In this paper, we propose a modification to the standard flexible generalized minimum residual iteration that enforces selected constraints on approximate numerical solutions. We demonstrate its application to both systems of conservation laws and dissipative systems.},
  archive      = {J_SIMAX},
  author       = {James Jackaman and Scott MacLachlan},
  doi          = {10.1137/22M1540624},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {327-352},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Constraint-satisfying krylov solvers for structure-preserving discretizations},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure preserving quaternion biconjugate gradient method.
<em>SIMAX</em>, <em>45</em>(1), 306–326. (<a
href="https://doi.org/10.1137/23M1547299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper considers a novel structure-preserving method for solving non-Hermitian quaternion linear systems arising from color image deblurred problems. From the quaternion Lanczos biorthogonalization procedure that preserves the quaternion tridiagonal form at each iteration, we derive the quaternion biconjugate gradient method for solving the linear systems and then establish the convergence analysis of the proposed algorithm. Finally, we provide some numerical examples to illustrate the feasibility and validity of our method in comparison with the QGMRES, especially in terms of computing time.},
  archive      = {J_SIMAX},
  author       = {Tao Li and Qing-Wen Wang},
  doi          = {10.1137/23M1547299},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {306-326},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structure preserving quaternion biconjugate gradient method},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast non-hermitian toeplitz eigenvalue computations, joining
matrixless algorithms and FDE approximation matrices. <em>SIMAX</em>,
<em>45</em>(1), 284–305. (<a
href="https://doi.org/10.1137/22M1529920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The present work is devoted to the eigenvalue asymptotic expansion of the Toeplitz matrix , whose generating function is complex-valued and has a power singularity at one point. As a consequence, is non-Hermitian and we know that in this setting, the eigenvalue computation is a nontrivial task for large sizes. First we follow the work of Bogoya, Böttcher, Grudsky, and Maximenko and deduce a complete asymptotic expansion for the eigenvalues. In a second step, we apply matrixless algorithms, in the spirit of the work by Ekström, Furci, Garoni, Serra-Capizzano et al., for computing those eigenvalues. Since the inner and extreme eigenvalues have different asymptotic behaviors, we worked on them independently and combined the results to produce a high precision global numerical and matrixless algorithm. The numerical results are very precise, and the computational cost of the proposed algorithms is independent of the size of the considered matrices for each eigenvalue, which implies a linear cost when the entire spectrum is computed. From the viewpoint of real-world applications, we emphasize that the class under consideration includes the matrices stemming from the numerical approximation of fractional diffusion equations. In the final section a concise discussion on the matter and a few open problems are presented.},
  archive      = {J_SIMAX},
  author       = {Manuel Bogoya and Sergei M. Grudsky and Stefano Serra-Capizzano},
  doi          = {10.1137/22M1529920},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {284-305},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast non-hermitian toeplitz eigenvalue computations, joining matrixless algorithms and FDE approximation matrices},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generic eigenstructures of hermitian pencils.
<em>SIMAX</em>, <em>45</em>(1), 260–283. (<a
href="https://doi.org/10.1137/22M1523297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We obtain the generic complete eigenstructures of complex Hermitian matrix pencils with rank at most (with ). To do this, we prove that the set of such pencils is the union of a finite number of bundle closures, where each bundle is the set of complex Hermitian pencils with the same complete eigenstructure (up to the specific values of the distinct finite eigenvalues). We also obtain the explicit number of such bundles and their codimension. The cases , corresponding to general Hermitian pencils, and exhibit surprising differences, since for the generic complete eigenstructures can contain only real eigenvalues, while for they can contain real and nonreal eigenvalues. Moreover, we will see that the sign characteristic of the real eigenvalues plays a relevant role for determining the generic eigenstructures.},
  archive      = {J_SIMAX},
  author       = {Fernando De Terán and Andrii Dmytryshyn and Froilán M. Dopico},
  doi          = {10.1137/22M1523297},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {260-283},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Generic eigenstructures of hermitian pencils},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The joint bidiagonalization of a matrix pair with inaccurate
inner iterations. <em>SIMAX</em>, <em>45</em>(1), 232–259. (<a
href="https://doi.org/10.1137/22M1541083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The joint bidiagonalization (JBD) process iteratively reduces a matrix pair to two bidiagonal forms simultaneously, which can be used for computing a partial generalized singular value decomposition (GSVD) of . The process has a nested inner-outer iteration structure, where the inner iteration usually cannot be computed exactly. In this paper, we study the inaccurately computed inner iterations of JBD by first investigating the influence of computational error of the inner iteration on the outer iteration, and then proposing a reorthogonalized JBD (rJBD) process to keep orthogonality of a part of Lanczos vectors. An error analysis of the rJBD is carried out to build up connections with Lanczos bidiagonalizations. The results are then used to investigate convergence and accuracy of the rJBD based GSVD computation. It is shown that the accuracy of computed GSVD components depends on the computing accuracy of inner iterations and the condition number of , while the convergence rate is not affected very much. For practical JBD based GSVD computations, our results can provide a guideline for choosing a proper computing accuracy of inner iterations in order to obtain approximate GSVD components with a desired accuracy. Numerical experiments are made to confirm our theoretical results.},
  archive      = {J_SIMAX},
  author       = {Haibo Li},
  doi          = {10.1137/22M1541083},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {232-259},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The joint bidiagonalization of a matrix pair with inaccurate inner iterations},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deflation for the off-diagonal block in symmetric saddle
point systems. <em>SIMAX</em>, <em>45</em>(1), 203–231. (<a
href="https://doi.org/10.1137/22M1537266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Deflation techniques are typically used to shift isolated clusters of small eigenvalues in order to obtain a tighter distribution and a smaller condition number. Such changes induce a positive effect in the convergence behavior of Krylov subspace methods, which are among the most popular iterative solvers for large sparse linear systems. We develop a deflation strategy for symmetric saddle point matrices by taking advantage of their underlying block structure. The vectors used for deflation come from an elliptic singular value decomposition relying on the generalized Golub–Kahan bidiagonalization process. The block targeted by deflation is the off-diagonal one since it features a problematic singular value distribution for certain applications. One example is the Stokes flow in elongated channels, where the off-diagonal block has several small, isolated singular values, depending on the length of the channel. Applying deflation to specific parts of the saddle point system is important when using solvers such as CRAIG, which operates on individual blocks rather than the whole system. The theory is developed by extending the existing framework for deflating square matrices before applying a Krylov subspace method such as MINRES. Numerical experiments confirm the merits of our strategy and lead to interesting questions about using approximate vectors for deflation.},
  archive      = {J_SIMAX},
  author       = {Andrei Dumitrasc and Carola Kruse and Ulrich Rüde},
  doi          = {10.1137/22M1537266},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {203-231},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Deflation for the off-diagonal block in symmetric saddle point systems},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projectively and weakly simultaneously diagonalizable
matrices and their applications. <em>SIMAX</em>, <em>45</em>(1),
167–202. (<a href="https://doi.org/10.1137/22M1507656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Characterizing simultaneously diagonalizable (SD) matrices has been receiving considerable attention in recent decades due to its wide applications and its role in matrix analysis. However, the notion of SD matrices is arguably still restrictive for wider applications. In this paper, we consider two error measures related to the simultaneous diagonalization of matrices and propose several new variants of SD thereof; in particular, TWSD, TWSD-B, -SD (SDO), DWSD, and -SD (SDO). Those are all weaker forms of SD. We derive various sufficient and/or necessary conditions of them under different assumptions and show the relationships between these new notions. Finally, we discuss the applications of these new notions in, e.g., quadratically constrained quadratic programming and independent component analysis.},
  archive      = {J_SIMAX},
  author       = {Wentao Ding and Jianze Li and Shuzhong Zhang},
  doi          = {10.1137/22M1507656},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {167-202},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Projectively and weakly simultaneously diagonalizable matrices and their applications},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication avoiding block low-rank parallel multifrontal
triangular solve with many right-hand sides. <em>SIMAX</em>,
<em>45</em>(1), 148–166. (<a
href="https://doi.org/10.1137/23M1568600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Block low-rank (BLR) compression can significantly reduce the memory and time costs of parallel sparse direct solvers. In this paper, we investigate the performance of the BLR triangular solve phase, which we observe to be underwhelming when dealing with many right-hand sides (RHS). We explain that this is because the bottleneck of the triangular solve is not in accessing the BLR LU factors, but rather in accessing the RHS, which are uncompressed. Motivated by this finding, we propose several new hybrid variants, which combine the right-looking and left-looking communication patterns to minimize the number of accesses to the RHS. We confirm via a theoretical analysis that these new variants can significantly reduce the total communication volume. We assess the impact of this reduction on the time performance on a range of real-life applications using the MUMPS solver, obtaining up to 20% time reduction.},
  archive      = {J_SIMAX},
  author       = {Patrick Amestoy and Olivier Boiteau and Alfredo Buttari and Matthieu Gerest and Fabienne Jézéquel and Jean-Yves L’Excellent and Theo Mary},
  doi          = {10.1137/23M1568600},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {148-166},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Communication avoiding block low-rank parallel multifrontal triangular solve with many right-hand sides},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The spectral decomposition of the continuous and discrete
linear elasticity operators with sliding boundary conditions.
<em>SIMAX</em>, <em>45</em>(1), 134–147. (<a
href="https://doi.org/10.1137/22M1541320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The elastic potential is a valuable modeling tool for many applications, including medical imaging. One reason for this is that the energy and its Gâteaux derivative, the elastic operator, have strong coupling properties. Although these properties are desirable from a modeling perspective, they are not advantageous from a computational or operator decomposition perspective. In this paper, we show that the elastic operator can be spectrally decomposed despite its coupling property when equipped with sliding boundary conditions. Moreover, we present a discretization that is fully compatible with this spectral decomposition. In particular, for image registration problems, this decomposition opens new possibilities for multispectral solution techniques and fine-tuned operator-based regularization.},
  archive      = {J_SIMAX},
  author       = {Jan Modersitzki},
  doi          = {10.1137/22M1541320},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {134-147},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The spectral decomposition of the continuous and discrete linear elasticity operators with sliding boundary conditions},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiway spectral graph partitioning: Cut functions, cheeger
inequalities, and a simple algorithm. <em>SIMAX</em>, <em>45</em>(1),
112–133. (<a href="https://doi.org/10.1137/23M1551936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The problem of multiway partitioning of an undirected graph is considered. A spectral method is used, where the largest eigenvalues of the normalized adjacency matrix (equivalently, the smallest eigenvalues of the normalized graph Laplacian) are computed. It is shown that the information necessary for partitioning is contained in the subspace spanned by the eigenvectors. The partitioning is encoded in a matrix in indicator form, which is computed by approximating the eigenvector matrix by a product of and an orthogonal matrix. A measure of the distance of a graph to being -partitionable is defined, as well as two cut (cost) functions, for which Cheeger inequalities are proved; thus the relation between the eigenvalue and partitioning problems is established. Numerical examples are given that demonstrate that the partitioning algorithm is efficient and robust.},
  archive      = {J_SIMAX},
  author       = {Lars Eldén},
  doi          = {10.1137/23M1551936},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {112-133},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multiway spectral graph partitioning: Cut functions, cheeger inequalities, and a simple algorithm},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational characterization of monotone nonlinear
eigenvector problems and geometry of self-consistent field iteration.
<em>SIMAX</em>, <em>45</em>(1), 84–111. (<a
href="https://doi.org/10.1137/22M1525326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper concerns a class of monotone eigenvalue problems with eigenvector nonlinearities (mNEPv). The mNEPv is encountered in applications such as the computation of joint numerical radius of matrices, best rank-one approximation of third-order partial-symmetric tensors, and distance to singularity for dissipative Hamiltonian differential-algebraic equations. We first present a variational characterization of the mNEPv. Based on the variational characterization, we provide a geometric interpretation of the self-consistent field (SCF) iterations for solving the mNEPv, prove the global convergence of the SCF, and devise an accelerated SCF. Numerical examples demonstrate theoretical properties and computational efficiency of the SCF and its acceleration.},
  archive      = {J_SIMAX},
  author       = {Zhaojun Bai and Ding Lu},
  doi          = {10.1137/22M1525326},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {84-111},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Variational characterization of monotone nonlinear eigenvector problems and geometry of self-consistent field iteration},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-preserving doubling algorithms that avoid
breakdowns for algebraic riccati-type matrix equations. <em>SIMAX</em>,
<em>45</em>(1), 59–83. (<a
href="https://doi.org/10.1137/23M1551791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Structure-preserving doubling algorithms (SDAs) are efficient algorithms for solving Riccati-type matrix equations. However, breakdowns may occur in SDAs. To remedy this drawback, in this paper, we first introduce -symplectic forms (-SFs), consisting of symplectic matrix pairs with a Hermitian parametric matrix . Based on -SFs, we develop modified SDAs (MSDAs) for solving the associated Riccati-type equations. MSDAs generate sequences of symplectic matrix pairs in -SFs and prevent breakdowns by employing a reasonably selected Hermitian matrix . In practical implementations, we show that the Hermitian matrix in MSDAs can be chosen as a real diagonal matrix that can reduce the computational complexity. The numerical results demonstrate a significant improvement in the accuracy of the solutions by MSDAs.},
  archive      = {J_SIMAX},
  author       = {Tsung-Ming Huang and Yueh-Cheng Kuo and Wen-Wei Lin and Shih-Feng Shieh},
  doi          = {10.1137/23M1551791},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {59-83},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structure-preserving doubling algorithms that avoid breakdowns for algebraic riccati-type matrix equations},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An augmented matrix-based CJ-FEAST SVDsolver for computing a
partial singular value decomposition with the singular values in a given
interval. <em>SIMAX</em>, <em>45</em>(1), 24–58. (<a
href="https://doi.org/10.1137/23M1547500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The cross-product matrix-based CJ-FEAST SVDsolver proposed previously by the authors is shown to compute the left singular vector possibly much less accurately than the right singular vector and may be numerically backward unstable when a desired singular value is small. In this paper, an alternative augmented matrix-based CJ-FEAST SVDsolver is proposed to compute the singular triplets of a large matrix with the singular values in an interval contained in the singular spectrum. The new CJ-FEAST SVDsolver is a subspace iteration applied to an approximate spectral projector of the augmented matrix associated with the eigenvalues in , and it constructs approximate left and right singular subspaces independently, onto which is projected to obtain the Ritz approximations to the desired singular triplets. Compact estimates are given for the accuracy of the approximate spectral projector constructed by the Chebyshev–Jackson series expansion in terms of series degree, and a number of convergence results are established. The new solver is proved to be always numerically backward stable. A convergence comparison of the cross-product-based and augmented matrix-based CJ-FEAST SVDsolvers is made, and a general-purpose choice strategy between the two solvers is proposed for the robustness and overall efficiency. Numerical experiments confirm all the results and meanwhile demonstrate that the proposed solver is more robust and substantially more efficient than the corresponding contour integral-based versions that exploit the trapezoidal rule and the Gauss–Legendre quadrature to construct an approximate spectral projector.},
  archive      = {J_SIMAX},
  author       = {Zhongxiao Jia and Kailiang Zhang},
  doi          = {10.1137/23M1547500},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {24-58},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An augmented matrix-based CJ-FEAST SVDsolver for computing a partial singular value decomposition with the singular values in a given interval},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XTrace: Making the most of every sample in stochastic trace
estimation. <em>SIMAX</em>, <em>45</em>(1), 1–23. (<a
href="https://doi.org/10.1137/23M1548323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The implicit trace estimation problem asks for an approximation of the trace of a square matrix, accessed via matrix-vector products (matvecs). This paper designs new randomized algorithms, XTrace and XNysTrace, for the trace estimation problem by exploiting both variance reduction and the exchangeability principle. For a fixed budget of matvecs, numerical experiments show that the new methods can achieve errors that are orders of magnitude smaller than existing algorithms, such as the Girard–Hutchinson estimator or the Hutch++ estimator. A theoretical analysis confirms the benefits by offering a precise description of the performance of these algorithms as a function of the spectrum of the input matrix. The paper also develops an exchangeable estimator, XDiag, for approximating the diagonal of a square matrix using matvecs.},
  archive      = {J_SIMAX},
  author       = {Ethan N. Epperly and Joel A. Tropp and Robert J. Webber},
  doi          = {10.1137/23M1548323},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {XTrace: Making the most of every sample in stochastic trace estimation},
  volume       = {45},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
