<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIREV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sirev---41">SIREV - 41</h2>
<ul>
<li><details>
<summary>
(2024a). Book reviews. <em>SIREV</em>, <em>66</em>(4), 795–805. (<a
href="https://doi.org/10.1137/24N97602X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If you are teaching a course (or otherwise looking for a text) in the techniques and applications of mathematical modeling, or mathematical approaches that analyze or solve those equations, you may find one of the reviews in this issue&#39;s collection interesting. Our featured review was written by Shawn Ryan, on the book Mathematical Modeling in Biology: A Research Methods Approach, by Shandelle M. Henson and James L. Hayward. Ryan&#39;s favorite part of the book is the authors&#39; use of the technique of “proposing logical alternative models and then using mathematical or statistical approaches to rule them out.” Even though he found the book to contain less mathematical rigor and theory development compared to a couple of existing math biology texts, Ryan thought that it has accomplished its goal of encouraging “students to think deeply about how mathematics can provide a useful tool in science.” Flour beetles make an appearance in Chapter 6. Our next book, reviewed by Sascha Desmettre, is A First Course in Options Pricing Theory, written by Simone Cologero. A particularly notable recent application of stochastic processes is arguably in the mathematical theory of finance. This book focuses on the part of stochastic finance pertaining to option pricing theory, omitting (important but less out-of-scope) topics as futures and term-structure. Desmettre listed a number of strengths of the book, including “clear motivation from the real world with the help of real data,” and recommended the book to those wishing to take the first steps in the field of option pricing. The next book is about mathematical tools that can be used to model biological systems, the stock market, and various other applications. Sarah Iams and Margo Levine reviewed Differential Equations: A Toolbox for Modelling the World, by Kurt Bryan. This book is published by SIMIODE (Systemic Initiative for Modeling Investigations &amp; Opportunities with Differential Equations) and describes itself as modeling-first. Iams and Levine praised its framing of the content in terms of modeling and data as “refreshing.” They also considered this book perhaps “the best resource [they] have encountered” for use in differential equations courses. That said, there are apparently formatting and language issues that make the book less accessible than a typical textbook, according to Iams and Levine. I will let you read the review to get the complete picture. Next in the collection is Bayesian Models of Perception and Action: An Introduction, by Wei Ji Ma, Konrad Paul Kording, and Daniel Goldreich. The review was written by one of our associate editors, Kres̆imir Josić. As noted by Josić, this is not a book about Bayesian statistics, which the authors only briefly discuss in the context of analyzing experimental data. Rather, the authors focus on perceptual inference and describe “how to develop models of the computations that our brain performs when making decisions based on prior information and sensory evidence.” Philipp Öffner reviewed Property-Preserving Numerical Schemes for Conservation Laws, authored by Dmitri Kuzmin and Hennes Hajduk, who based the book on lecture notes written for an advanced graduate-level course. Our reviewers typically say positive things about the books, but Öffner seemed to really like this book, saying that it is “a comprehensive and lucid summary of current research in limiters,” the “writing is superb.” And he commends the authors for “creating what could be considered a contemporary classic on limiting strategies.” We wrap up with a review by Jean Vélin, on Linear and Convex Optimization: A Mathematical Approach, by Michael H. Veatch. This book contains, as its title suggests, a mathematical approach to linear and nonlinear optimization problems. Vélin gave a summary of each of the fifteen chapters, and concluded that the book is well written and easy to read, with examples that help deepen the reader&#39;s understanding of the material. I hope you find something that interests you.},
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24N97602X},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {795-805},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing workforce with mathematical modeling skills.
<em>SIREV</em>, <em>66</em>(4), 778–792. (<a
href="https://doi.org/10.1137/19M1277643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematicians have traditionally been a select group of academics who produce high-impact ideas enabling substantial results in several fields of science. Throughout the past 35 years, undergraduates enrolling in mathematics or statistics have represented a nearly constant proportion of approximately 1% of bachelor degrees awarded in the United States. Even within STEM majors, mathematics or statistics only constitute about 6% of undergraduate degrees awarded nationally. However, the need for STEM professionals continues to grow, and the list of required occupational skills rests heavily in foundational concepts of mathematical modeling curricula, where the interplay of data, computer simulation, and underlying theoretical frameworks takes center stage. It is not viable to expect a majority of these STEM undergraduates to pursue a double major that includes mathematics. Here we present our solution, some early results of its implementation, and a vision for possible nationwide adoption.},
  archive      = {J_SIREV},
  author       = {Ariel Cintrón-Arias and Ryan Andrew Nivens and Anant Godbole and Calvin B. Purvis},
  doi          = {10.1137/19M1277643},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {778-792},
  shortjournal = {SIAM Rev.},
  title        = {Developing workforce with mathematical modeling skills},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sandpiles and dunes: Mathematical models for granular
matter. <em>SIREV</em>, <em>66</em>(4), 751–777. (<a
href="https://doi.org/10.1137/23M1583673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular materials are everywhere, in the environment but also in our pantry. Their properties are different from those of any solid material, due to the possibility of sudden phenomena such as avalanches or landslides. Here we present a brief survey on their characteristics and on what can be found (from the past thirty years) in the recent mathematics literature in order to reproduce their behavior. We discuss, in particular, differential models proposed for the growth of a sandpile on a table and, when wind comes into play, for the formation and dynamics of sand dunes. This field of research is still of great interest since there is no consolidated general model for the dynamics of granular matter, but rather only standalone models adapted to specific situations.},
  archive      = {J_SIREV},
  author       = {Piermarco Cannarsa and Stefano Finzi Vita},
  doi          = {10.1137/23M1583673},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {751-777},
  shortjournal = {SIAM Rev.},
  title        = {Sandpiles and dunes: Mathematical models for granular matter},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Education. <em>SIREV</em>, <em>66</em>(4), 749. (<a
href="https://doi.org/10.1137/24N976018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue the Education section presents two contributions. The first paper, “Sandpiles and Dunes: Mathematical Models for Granular Matter,” by Piermarco Cannarsa and Stefano Finzi Vita, presents a review of mathematical models for formation of sand piles and dunes. In nature and everyday life various materials appear as conglomerates of particles, like, for instance, sand, gravel, fresh snow, rice, sugar, etc. On larger scales, granular material exhibits new and more complex phenomena which are still not fully understood. It is very different from that of a solid, liquid, or gas in the sense that it can show characteristics similar to one or the other depending on the energy of the system. Its modeling can help in understanding complex natural phenomena such as dune migration, erosion, landslides, and avalanches, and can contribute to the development of environmental protection programs. Such models are also important in various applications in agriculture, construction, energy production, as well as in the chemical, pharmaceutical, food, and metallurgical industries. Even if a sufficiently consolidated general model for the dynamics of granular materials is not available yet, significant progress has been made recently with the introduction of new theoretical models adapted to more specific situations. In this article, after a general description and some historical comments, the authors limit themselves to considering the problem of the growth of a pile of sand on a table under the action of a vertical source of small intensity, neglecting the effects of wind, which has an important role in dune formation. Still, even for such an apparently simpler case, many interesting phenomena do arise and are described in an easily accessible way. Accompanying pictures of real-life experiences make the reading truly enjoyable, and numerical illustrations bring even better intuition on the complexity of phenomena. The authors also indicate literature for further learning. This article is well organized, neatly written, and presents the subject highlighting some of the major aspects. This review of existing models can become a starting point for research projects in a Master&#39;s program of applied mathematics and partial differential equations. It could also be used by advanced mathematics students to learn differential models of granular material in an affordable way. The second paper, “Developing Workforce with Mathematical Modeling Skills,” is presented by Ariel Cintrón-Arias, Ryan Andrew Nivens, Anant Godbole and Calvin B. Purvis. Undergraduate mathematics degrees constitute a very small portion of all awarded degrees in the U.S., and this portion is stagnating, while the job growth between 2016 and 2026 for Statisticians and Mathematicians is expected to be substantial. So the need for growth in mathematical training becomes imperative. The authors discuss the nationwide production of STEM professionals and argue that mathematical modeling curricula could attract more students to mathematical majors and minors. They also provide some highlights of three public data repositories that can be used along with instruction in mathematical modeling. Then a generic minor in mathematical modeling related to skills in high demand is proposed and some selected educational resources are provided. Finally the outcomes of a National Science Foundation grant awarded to the authors&#39; institution designed to assist and encourage students in mathematical modeling are discussed. This article may serve as a valuable tool to obtain support from university administrators for integrating mathematical modeling into STEM curricula.},
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/24N976018},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {749},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bridge between invariant theory and maximum likelihood
estimation. <em>SIREV</em>, <em>66</em>(4), 721–747. (<a
href="https://doi.org/10.1137/24M1661753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We uncover connections between maximum likelihood estimation in statistics and norm minimization over a group orbit in invariant theory. We present a dictionary that relates notions of stability from geometric invariant theory to the existence and uniqueness of a maximum likelihood estimate. Our dictionary holds for both discrete and continuous statistical models: we discuss log-linear models and Gaussian models, including matrix normal models and directed Gaussian graphical models. Our approach reveals promising consequences of the interplay between invariant theory and statistics. For instance, algorithms from statistics can be used in invariant theory, and vice versa.},
  archive      = {J_SIREV},
  author       = {Carlos Améndola and Kathlén Kohn and Philipp Reichenbach and Anna Seigal},
  doi          = {10.1137/24M1661753},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {721-747},
  shortjournal = {SIAM Rev.},
  title        = {A bridge between invariant theory and maximum likelihood estimation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SIGEST. <em>SIREV</em>, <em>66</em>(4), 719. (<a
href="https://doi.org/10.1137/24N976006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue, “A Bridge between Invariant Theory and Maximum Likelihood Estimation,” by Carlos Améndola, Kathlén Kohn, Philipp Reichenbach, and Anna Seigal, uncovers the deep connections between geometric invariant theory and statistical methods, specifically maximum likelihood estimation (MLE) by connecting it to norm minimization over group orbits. The authors develop a dictionary relating stability notions in geometric invariant theory to the existence and uniqueness of MLEs, which applies to both Gaussian and log-linear models. In comparison to the original 2021 version of the paper that appeared in the SIAM Journal on Applied Algebra and Geometry, for the SIGEST version, the authors added new content on log-linear models, simplified technical proofs, removed detailed appendices, and incorporated new examples and figures for accessibility. In particular, the focus was primarily on Gaussian models, whereas this updated SIGEST version expands the coverage by incorporating results from the authors&#39; companion paper on log-linear models. Furthermore, a new figure (Fig. 1) visually illustrates the two core concepts of invariant theory and MLE. Significant changes include the removal of technical details and appendices to streamline the content and make it more accessible to a broader audience. The introduction of examples, particularly for the Kempf--Ness Theorem, further aids understanding. This paper makes several key contributions of broad mathematical interest. MLE is a key statistical technique that is widely used. Having a new handle on its well-posedness analysis deepens the understanding of the mechanisms behind this technique as well as potentially paves the way to extending existing theory for MLE models. Also, on the computational side, algorithms from the optimization over orbits can be used for MLE, and vice versa, which could possibly lead to new and more efficient algorithms in both fields. Overall, the work beautifully highlights how techniques from one field can be applied to the other, with applications to generalization bounds, group actions, and optimization landscapes. In the last section of their SIGEST paper the authors discuss possible future research directions that capitalize on the dictionary they have uncovered.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24N976006},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {719},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feynman’s inverse problem. <em>SIREV</em>, <em>66</em>(4),
694–718. (<a href="https://doi.org/10.1137/23M1611488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze an inverse problem for water waves posed by Richard Feynman in the BBC documentary Fun to Imagine. We show that the problem can be modeled as an inverse Cauchy problem for gravity-capillary waves, conduct a detailed analysis of the Cauchy problem, and give a uniqueness proof for the inverse problem. Somewhat surprisingly, this results in a positive answer to Feynman&#39;s question. In addition, we derive stability estimates for the inverse problem for both continuous and discrete measurements, propose a simple inversion method, and conduct numerical experiments to verify our results.},
  archive      = {J_SIREV},
  author       = {Adrian Kirkeby},
  doi          = {10.1137/23M1611488},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {694-718},
  shortjournal = {SIAM Rev.},
  title        = {Feynman&#39;s inverse problem},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sigmoid functions, multiscale resolution of singularities,
and <span class="math inline"><em>h</em><em>p</em></span>-mesh
refinement. <em>SIREV</em>, <em>66</em>(4), 683–693. (<a
href="https://doi.org/10.1137/23M1556629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this short, conceptual paper we observe that closely related mathematics applies in four contexts with disparate literatures: (1) sigmoidal and RBF approximation of smooth functions, (2) rational approximation of analytic functions with singularities, (3) $hp\kern .7pt$-mesh refinement for solution of \pdes, and (4) double exponential (DE) and generalized Gauss quadrature. The relationships start from the change of variables $s = \log(x)$, and they suggest possibilities for new analyses and new methods in several areas. Concerning (2) and (3), we show that both problems feature the same effect of “linear tapering” near the singularity---of clustered poles in rational approximation and of polynomial orders in $hp\kern .7pt$-mesh refinement. Concerning (4), we note that the tapering effect appears here too, and that the change of variables interpretation sheds new light on why the DE and generalized Gauss methods are effective at integrating arbitrary singularities.},
  archive      = {J_SIREV},
  author       = {Daan Huybrechs and Lloyd N. Trefethen},
  doi          = {10.1137/23M1556629},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {683-693},
  shortjournal = {SIAM Rev.},
  title        = {Sigmoid functions, multiscale resolution of singularities, and $hp$-mesh refinement},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Research spotlights. <em>SIREV</em>, <em>66</em>(4), 681.
(<a href="https://doi.org/10.1137/24N975992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logarithmic transformations are used broadly in data science, mathematics, and engineering, and yet they can still reveal surprising connections between seemingly unrelated disciplines. This issue&#39;s first research spotlight, “Sigmoid Functions, Multiscale Resolution of Singularities, and $hp$-Mesh Refinement,” illuminates how the change of variables $s = \log(x)$ connects different areas of computational mathematics. Authors Daan Huybrechs and Lloyd “Nick” Trefethen show new relationships between smooth approximation, rational approximation theory, adaptive mesh refinement, and numerical quadrature. For example, the authors show that this change of variables can be naturally tied to a “linear tapering” effect near singularities, which is a common feature in both rational approximation and $hp$-mesh refinement. Through a number of effective examples, the authors illustrate the power of these relationships across areas that have seen relatively independent lines of development. In doing so, the authors suggest opportunities for developing and analyzing new methods by leveraging the new connections, including mesh refinement strategies, techniques for multivariate approximation, and hybrid approaches that combine the strengths of disparate methods. How well can information be recovered from water waves? This question is at the heart of this issue&#39;s second research spotlight, “Feynman&#39;s Inverse Problem.” Author Adrian Kirkeby is motivated by a thought experiment posed by the physicist and iconoclast Richard Feynman wherein an insect floating in a swimming pool wants to determine where and when others have jumped into the pool, causing the waves the insect observes. Kirkeby constructs and analyzes a linear 2D-3D system of partial differential equations (PDEs) for the forward model. Leveraging the nonlocality of this system of PDEs, Kirkeby shows conditions under which the insect can determine the source of the waves---in fact, uniquely---simply by observing the wave amplitude and water velocity in any small area of the surface. This model is then extended to capture settings where noisy observations and observations at a finite number of time and space points are collected, and establishes stability properties and error bounds for the reconstruction. The paper concludes with illustrative numerical experiments based on a nonharmonic Fourier inversion method. Kirkeby also highlights several avenues for future research, noting that inverse problems for water or other surface waves have received less attention than those involving acoustic or electromagnetic waves. As an added bonus, the referenced video of Feynman is not to be missed.},
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24N975992},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {681},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oscillatory networks: Insights from piecewise-linear
modeling. <em>SIREV</em>, <em>66</em>(4), 619–679. (<a
href="https://doi.org/10.1137/22M1534365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is enormous interest---both mathematically and in diverse applications---in understanding the dynamics of coupled-oscillator networks. The real-world motivation of such networks arises from studies of the brain, the heart, ecology, and more. It is common to describe the rich emergent behavior in these systems in terms of complex patterns of network activity that reflect both the connectivity and the nonlinear dynamics of the network components. Such behavior is often organized around phase-locked periodic states and their instabilities. However, the explicit calculation of periodic orbits in nonlinear systems (even in low dimensions) is notoriously hard, so network-level insights often require the numerical construction of some underlying periodic component. In this paper, we review powerful techniques for studying coupled-oscillator networks. We discuss phase reductions, phase--amplitude reductions, and the master stability function for smooth dynamical systems. We then focus, in particular, on the augmentation of these methods to analyze piecewise-linear systems, for which one can readily construct periodic orbits. This yields useful insights into network behavior, but the cost is that one needs to study nonsmooth dynamical systems. The study of nonsmooth systems is well developed when focusing on the interacting units (i.e., at the node level) of a system, and we give a detailed presentation of how to use saltation operators, which can treat the propagation of perturbations through switching manifolds, to understand dynamics and bifurcations at the network level. We illustrate this merger of tools and techniques from network science and nonsmooth dynamical systems with applications to neural systems, cardiac systems, networks of electromechanical oscillators, and cooperation in cattle herds.},
  archive      = {J_SIREV},
  author       = {Stephen Coombes and Mustafa Şayli and Rüdiger Thul and Rachel Nicks and Mason A. Porter and Yi Ming Lai},
  doi          = {10.1137/22M1534365},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {619-679},
  shortjournal = {SIAM Rev.},
  title        = {Oscillatory networks: Insights from piecewise-linear modeling},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Survey and review. <em>SIREV</em>, <em>66</em>(4), 617. (<a
href="https://doi.org/10.1137/24N975980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural oscillations are periodic activities of neurons in the central nervous system of eumetazoa. In an oscillatory neural network, neurons are modeled by coupled oscillators. Oscillatory networks are employed for describing the behavior of complex systems in biology or ecology with respect to the connectivity of the network components or the nonlinear dynamics of the individual units. Phase-locked periodic states and their instabilities are core features in the analysis of oscillatory networks. In “Oscillatory Networks: Insights from Piecewise-Linear Modeling,” Stephen Coombes, Mustafa Şayli, Rüdiger Thul, Rachel Nicks, Mason A. Porter, and Yi Ming Lai review techniques for studying coupled oscillatory networks. They first discuss phase reductions, phase-amplitude reductions, and the master stability function for smooth dynamical systems. Then they consider nonsmooth piecewise-linear (PWL) systems, for which periodic orbits are easily obtained. Saltation operators are used for modeling the propagation of perturbations through switching manifolds in the analysis of the dynamics and bifurcations at the network level. Applications to neural systems, cardiac systems, networks of electromechanical oscillators, and cooperation in cattle herds illustrate the power of these methods. PWL modeling has been applied for a long time in engineering. Recently, it has been introduced in other fields, such as social sciences, finance, and biology. For many modern applications in science, piecewise models are much more versatile than the classical smooth dynamical systems. In neuroscience, PWL functions enable explicit calculations which are infeasible in the original smooth system. This includes discontinuous dynamical systems, which are used to model impacting mechanical oscillators, integrate-and-fire models of spiking neurons, and cardiac oscillators. On the other hand, the price to pay is the retrieval of new conditions for the existence, uniqueness, and stability of solutions. The paper discusses the application of PWL models to a large variety of applications from engineering and biology. It will be of interest to many readers.},
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24N975980},
  journal      = {SIAM Review},
  month        = {11},
  number       = {4},
  pages        = {617},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Book reviews. <em>SIREV</em>, <em>66</em>(3), 605–615. (<a
href="https://doi.org/10.1137/24N975967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theme of this collection of book reviews is arguably about the “usefulness” of mathematics, or how we can try to understand aspects of our world by developing mathematical or data-driven models. Thus, it is fitting that our featured review is written by John Stillwell, on the book Why Does Math Work . . . If It&#39;s Not Real?, written by Dragan Radulović. Stillwell characterizes the book as “an offbeat and entertaining take on the mystery that Eugene Wigner famously called the `unreasonable effectiveness of mathematics.&#39;” I haven&#39;t read the book, so I can&#39;t say if it is as “enjoyable and thought-provoking” as the reviewer says it is. But the review is lively and entertaining, that I can vouch for. Perhaps just to showcase the usefulness of mathematics, our next book, reviewed by me, is An Invitation to Mathematical Biology, written by David G. Costa and Paul J. Schulte. As I note in the review, the book&#39;s title, which says “Invitation” and not the typical “Introduction,” is a clue to what the book is about: making math biology less scary to undergraduate biology majors, while at the same time gently guiding them to dive deeper in some topics. With the advancement of computational and sequencing methods, a large amount of biological data is generated. Dealing with complex biological networks and extracting the meaningful picture requires new approaches. One such approach is systems biology. Whch brings us to the next review, by Herbert M. Sauro, on Systems Biology: Modelling, Analysis, and Simulation, by Jinzhi Lei. Systems biology is a fast-growing field, and Lei&#39;s book is one of several texts in the area published in recent years, with a focus on gene regulatory networks. Systems biology is a data-driven approach, and data is what the next book is about. High-Dimensional Data Analysis with Low-Dimensional Models, by John Wright and Yi Ma, is reviewed by one of our associate editors, Alfio Borzí. Alfio lays out rather clearly in his review what you can learn from this book, and he concludes that it would serve well for him as “a companion text while doing research and teaching in the field of data analysis with low-dimensional models.” The next two books focus on methodologies. Tim Hoheisel reviews An Optimization Primer, authored by Johannes O. Royset and Roger J.-B. Wets. With the explosive growth of machine learning and artificial intelligence applications, optimization has received increasingly more and more attention. A search for recently published books with the word “optimization” in their titles returned pages and pages of results. Hoheisel&#39;s review will give you a clue to how this book compares with its many alternatives. Circling back to the “usefulness” of mathematics. Most of the books reviewed so far focus on biology. Our last review can show you that math has applications aplenty in other fields. Abdon Atangana reviews Mathematical Modelling, by Simon Serovajsky. The book covers applications of math modeling in physics, engineering, chemistry, biology, medicine, economics, ecology, sociology, psychology, political science, and so on.},
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24N975967},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {605-615},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combinatorial and hodge laplacians: Similarities and
differences. <em>SIREV</em>, <em>66</em>(3), 575–601. (<a
href="https://doi.org/10.1137/22M1482299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As key subjects in spectral geometry and combinatorial graph theory, respectively, the (continuous) Hodge Laplacian and the combinatorial Laplacian share similarities in revealing the topological dimension and geometric shape of data and in their realization of diffusion and minimization of harmonic measures. It is believed that they also both associate with vector calculus, through the gradient, curl, and divergence, as argued in the popular usage of “Hodge Laplacians on graphs” in the literature. Nevertheless, these Laplacians are intrinsically different in their domains of definitions and applicability to specific data formats, hindering any in-depth comparison of the two approaches. For example, the spectral decomposition of a vector field on a simple point cloud using combinatorial Laplacians defined on some commonly used simplicial complexes does not give rise to the same curl-free and divergence-free components that one would obtain from the spectral decomposition of a vector field using either the continuous Hodge Laplacians defined on differential forms in manifolds or the discretized Hodge Laplacians defined on a point cloud with boundary in the Eulerian representation or on a regular mesh in the Eulerian representation. To facilitate the comparison and bridge the gap between the combinatorial Laplacian and Hodge Laplacian for the discretization of continuous manifolds with boundary, we further introduce boundary-induced graph (BIG) Laplacians using tools from discrete exterior calculus (DEC). BIG Laplacians are defined on discrete domains with appropriate boundary conditions to characterize the topology and shape of data. The similarities and differences among the combinatorial Laplacian, BIG Laplacian, and Hodge Laplacian are then examined. Through an Eulerian representation of 3D domains as level-set functions on regular grids, we show experimentally the conditions for the convergence of BIG Laplacian eigenvalues to those of the Hodge Laplacian for elementary shapes.},
  archive      = {J_SIREV},
  author       = {Emily Ribando-Gros and Rui Wang and Jiahui Chen and Yiying Tong and Guo-Wei Wei},
  doi          = {10.1137/22M1482299},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {575-601},
  shortjournal = {SIAM Rev.},
  title        = {Combinatorial and hodge laplacians: Similarities and differences},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Education. <em>SIREV</em>, <em>66</em>(3), 573. (<a
href="https://doi.org/10.1137/24N975955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue the Education section presents “Combinatorial and Hodge Laplacians: Similarities and Differences,” by Emily Ribando-Gros, Rui Wang, Jiahui Chen, Yiying Tong, and Guo-Wei Wei. Combinatorial Laplacians and their spectra are important tools in the study of molecular stability, electrical networks, neuroscience, deep learning, signal processing, etc. The continuous Hodge Laplacian allows one, in some cases, to generate an unknown shape from only its Laplacian spectrum. In particular, both combinatorial Laplacians and continuous Hodge Laplacian are useful in describing the topology of data; see, for instance, [L.-H. Lim, “Hodge Laplacians on graphs,” SIAM Rev., 62 (2020), pp. 685--715]. Since nowadays computations frequently involve these Laplacians, it is important to have a good understanding of the differences and relations between them. Indeed, though the Hodge Laplacian and the combinatorial Laplacian share similarities in revealing the topological dimension and geometric shape of data, at the same time they are intrinsically different in their domains of definitions and applicability to specific data formats. To facilitate comparisons, the authors introduce boundary-induced graph (BIG) Laplacians, the purpose of which is “to put the combinatorial Laplacians and Hodge Laplacian on equal footing.” BIG Laplacian brings, in fact, the combinatorial Laplacian closer to the continuous Hodge Laplacian. In this paper similarities and differences between combinatorial Laplacian, BIG Laplacian, and Hodge Laplacian are examined. Some elements of spectral analysis related to topological data analysis (TDA) are also provided. TDA and connected ideas have recently gained a lot of interest, and so this paper is timely. It is written in a way that should make it accessible for early career researchers; the reader should already have a good understanding of some notions of graph theory, spectral geometry, differential geometry, and algebraic topology. The paper is not self-contained and eventually could be used by group-based research projects in a Master&#39;s program for advanced mathematics students.},
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/24N975955},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {573},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Operator learning using random features: A tool for
scientific computing. <em>SIREV</em>, <em>66</em>(3), 535–571. (<a
href="https://doi.org/10.1137/24M1648703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised operator learning centers on the use of training data, in the form of input-output pairs, to estimate maps between infinite-dimensional spaces. It is emerging as a powerful tool to complement traditional scientific computing, which may often be framed in terms of operators mapping between spaces of functions. Building on the classical random features methodology for scalar regression, this paper introduces the function-valued random features method. This leads to a supervised operator learning architecture that is practical for nonlinear problems yet is structured enough to facilitate efficient training through the optimization of a convex, quadratic cost. Due to the quadratic structure, the trained model is equipped with convergence guarantees and error and complexity bounds, properties that are not readily available for most other operator learning architectures. At its core, the proposed approach builds a linear combination of random operators. This turns out to be a low-rank approximation of an operator-valued kernel ridge regression algorithm, and hence the method also has strong connections to Gaussian process regression. The paper designs function-valued random features that are tailored to the structure of two nonlinear operator learning benchmark problems arising from parametric partial differential equations. Numerical results demonstrate the scalability, discretization invariance, and transferability of the function-valued random features method.},
  archive      = {J_SIREV},
  author       = {Nicholas H. Nelsen and Andrew M. Stuart},
  doi          = {10.1137/24M1648703},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {535-571},
  shortjournal = {SIAM Rev.},
  title        = {Operator learning using random features: A tool for scientific computing},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SIGEST. <em>SIREV</em>, <em>66</em>(3), 533. (<a
href="https://doi.org/10.1137/24N975943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “Operator Learning Using Random Features: A Tool for Scientific Computing,” by Nicholas H. Nelsen and Andrew M. Stuart. This work considers the problem of operator learning in infinite-dimensional Banach spaces through the use of random features. The driving application is the approximation of solution operators to partial differential equations (PDEs), here foremost time-dependent problems, that are naturally posed in an infinite-dimensional function space. Typically here, in contrast to the mainstream big data regimes of machine learning applications such as computer vision, high resolution data coming from physical experiments or from computationally expensive simulations of such differential equations is usually small. Fast and approximate surrogates built from such data can be advantageous in building forward models for inverse problems or for doing uncertainty quantification, for instance. Showing how this can be done in infinite dimensions gives rise to approximators which are at the outset resolution and discretization invariant, allowing training on one resolution and deploying on another. At the heart of this work is the function-valued random features methodology that the authors extended from the finite setting of the classical random features approach. Here, the nonlinear operator is approximated by a linear combination of random operators which turn out to be a low-rank approximation and whose computation amounts to a convex, quadratic optimisation problem that is efficiently solvable and for which convergence guarantees can be derived. The methodology is then concretely applied to two concrete PDE examples: Burgers&#39; equations and Darcy flow, demonstrating the applicability of the function-valued random features method, its scalability, discretization invariance, and transferability. The original 2021 article, which appeared in SIAM&#39;s Journal on Scientific Computing, has attracted considerable attention. In preparing this SIGEST version, the authors have made numerous modifications and revisions. These include expanding the introductory section and the concluding remarks, condensing the technical content and making it more accessible, and adding a link to an open access GitHub repository that contains all data and code used to produce the results in the paper.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24N975943},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {533},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When data driven reduced order modeling meets full waveform
inversion. <em>SIREV</em>, <em>66</em>(3), 501–532. (<a
href="https://doi.org/10.1137/23M1552826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Waveform inversion is concerned with estimating a heterogeneous medium, modeled by variable coefficients of wave equations, using sources that emit probing signals and receivers that record the generated waves. It is an old and intensively studied inverse problem with a wide range of applications, but the existing inversion methodologies are still far from satisfactory. The typical mathematical formulation is a nonlinear least squares data fit optimization and the difficulty stems from the nonconvexity of the objective function that displays numerous local minima at which local optimization approaches stagnate. This pathological behavior has at least three unavoidable causes: (1) The mapping from the unknown coefficients to the wave field is nonlinear and complicated. (2) The sources and receivers typically lie on a single side of the medium, so only backscattered waves are measured. (3) The probing signals are band limited and with high frequency content. There is a lot of activity in the computational science and engineering communities that seeks to mitigate the difficulty of estimating the medium by data fitting. In this paper we present a different point of view, based on reduced order models (ROMs) of two operators that control the wave propagation. The ROMs are called data driven because they are computed directly from the measurements, without any knowledge of the wave field inside the inaccessible medium. This computation is noniterative and uses standard numerical linear algebra methods. The resulting ROMs capture features of the physics of wave propagation in a complementary way and have surprisingly good approximation properties that facilitate waveform inversion.},
  archive      = {J_SIREV},
  author       = {Liliana Borcea and Josselin Garnier and Alexander V. Mamonov and Jörn Zimmerling},
  doi          = {10.1137/23M1552826},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {501-532},
  shortjournal = {SIAM Rev.},
  title        = {When data driven reduced order modeling meets full waveform inversion},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persistent homology for resource coverage: A case study of
access to polling sites. <em>SIREV</em>, <em>66</em>(3), 481–500. (<a
href="https://doi.org/10.1137/22M150410X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to choose the geographical distributions of public resources in a fair and equitable manner. However, it is complicated to quantify the equity of such a distribution; important factors include distances to resource sites, availability of transportation, and ease of travel. We use persistent homology, which is a tool from topological data analysis, to study the availability and coverage of polling sites. The information from persistent homology allows us to infer holes in a distribution of polling sites. We analyze and compare the coverage of polling sites in Los Angeles County and five cities (Atlanta, Chicago, Jacksonville, New York City, and Salt Lake City), and we conclude that computation of persistent homology appears to be a reasonable approach to analyzing resource coverage.},
  archive      = {J_SIREV},
  author       = {Abigail Hickok and Benjamin Jarman and Michael Johnson and Jiajie Luo and Mason A. Porter},
  doi          = {10.1137/22M150410X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {481-500},
  shortjournal = {SIAM Rev.},
  title        = {Persistent homology for resource coverage: A case study of access to polling sites},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Research spotlights. <em>SIREV</em>, <em>66</em>(3), 479.
(<a href="https://doi.org/10.1137/24N975931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equitable distribution of geographically dispersed resources presents a significant challenge, particularly in defining quantifiable measures of equity. How can we optimally allocate polling sites or hospitals to serve their constituencies? This issue&#39;s first Research Spotlight, “Persistent Homology for Resource Coverage: A Case Study of Access to Polling Sites,&quot; addresses these questions by demonstrating the application of topological data analysis to identify holes in resource accessibility and coverage. Authors Abigail Hickok, Benjamin Jarman, Michael Johnson, Jiajie Luo, and Mason A. Porter employ persistent homology, a technique that tracks the formation and disappearance of these holes as spatial scales vary. To make matters concrete, the authors consider a case study on access to polling sites and use a non-Euclidean distance that accounts for both travel and waiting times. In their case study, the authors use a weighted Vietoris--Rips filtration based on a symmetrized form of this distance and limit their examination to instances where the approximations underlying the filtration are less likely to lead to approximation-based artifacts. Details, as well as source code, are provided on the estimation of the various quantities, such as travel time, waiting time, and demographics (e.g., age, vehicle access). The result is a homology class that “dies&quot; at time $t$ if it takes $t$ total minutes to cast a vote. The paper concludes with an exposition of potential limitations and future directions that serve to encourage additional investigation into this class of problems (which includes settings where one wants to deploy different sensors to cover a spatial domain) and related techniques. What secrets lurk within? From flaws in human-made infrastructure to materials deep beneath the Earth&#39;s land and ocean surfaces to anomalies in patients, our next Research Spotlight, “When Data Driven Reduced Order Modeling Meets Full Waveform Inversion,&quot; addresses math and methods to recover the unknown. Authors Liliana Borcea, Josselin Garnier, Alexander V. Mamonov, and Jörn Zimmerling show how tools from numerical linear algebra and reduced-order modeling can be brought to bear on inverse wave scattering problems. Their setup encapsulates a wide variety of sensing modalities, wherein receivers emit a signal (such as an acoustic wave) and a time series of wavefield measurements is subsequently captured at one or more sources. Full waveform inversion refers to the recovery of the unknown “within&quot; and is typically addressed via iterative, nonlinear equations/least-squares solvers. However, it is often plagued by a notoriously nonconvex, ill-conditioned optimization landscape. The authors show how some of the challenges typically encountered in this inversion can be mitigated with the use of reduced-order models. These models employ observed data snapshots to form lower-dimensional, computationally attractive approximations. A key to the paper&#39;s developments is a unification of several Galerkin projection--based models and ensuring that these approximation models benefit the inversion. The latter is achieved by having the reduced-order models aimed at capturing “internal waves,&quot; and only later addressing the resulting mismatch with the measured data. Through several illustrations, the authors demonstrate how the approach can be deployed. The paper concludes with open questions in the intersection of inverse problems and reduced-order models.},
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24N975931},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {479},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cardinality minimization, constraints, and regularization: A
survey. <em>SIREV</em>, <em>66</em>(3), 403–477. (<a
href="https://doi.org/10.1137/21M142770X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We survey optimization problems that involve the cardinality of variable vectors in constraints or the objective function. We provide a unified viewpoint on the general problem classes and models, and we give concrete examples from diverse application fields such as signal and image processing, portfolio selection, and machine learning. The paper discusses general-purpose modeling techniques and broadly applicable as well as problem-specific exact and heuristic solution approaches. While our perspective is that of mathematical optimization, a main goal of this work is to reach out to and build bridges between the different communities in which cardinality optimization problems are frequently encountered. In particular, we highlight that modern mixed-integer programming, which is often regarded as impractical due to the commonly unsatisfactory behavior of black-box solvers applied to generic problem formulations, can in fact produce provably high-quality or even optimal solutions for cardinality optimization problems, even in large-scale real-world settings. Achieving such performance typically involves drawing on the merits of problem-specific knowledge that may stem from different fields of application and, e.g., can shed light on structural properties of a model or its solutions, or can lead to the development of efficient heuristics. We also provide some illustrative examples.},
  archive      = {J_SIREV},
  author       = {Andreas M. Tillmann and Daniel Bienstock and Andrea Lodi and Alexandra Schwartz},
  doi          = {10.1137/21M142770X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {403-477},
  shortjournal = {SIAM Rev.},
  title        = {Cardinality minimization, constraints, and regularization: A survey},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Survey and review. <em>SIREV</em>, <em>66</em>(3), 401. (<a
href="https://doi.org/10.1137/24N97592X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In “Cardinality Minimization, Constraints, and Regularization: A Survey,&quot; Andreas M. Tillmann, Daniel Bienstock, Andrea Lodi, and Alexandra Schwartz consider a class of optimization problems that involve the cardinality of variable vectors in constraints or in the objective function. Such problems have many important applications, e.g., medical imaging (like X-ray tomography), face recognition, wireless sensor network design, stock picking, crystallography, astronomy, computer vision, classification and regression, interpretable machine learning, and statistical data analysis. The emphasis in this paper is on continuous variables, which distinguishes it from a myriad of classical operation research or combinatorial optimization problems. Three general problem classes are studied in detail: cardinality minimization problems, cardinality-constrained problems, and regularized cardinality problems. The paper provides a road map connecting several disciplines and offers an overview of many different computational approaches that are available for cardinality optimization problems. Since such problems are of cross-disciplinary nature, the authors organized their review according to specific application areas and point out overlaps and differences. The paper starts with prominent cardinality optimization problems, namely, signal and image processing, portfolio optimization and management, high-dimensional statistics and machine learning, and some related problems from combinatorics, matrix sparsification, and group/block sparsity. It then continues with exact models and solution methods. The further sections are devoted to relaxations and heuristics, scalability of exact and heuristic algorithms. The authors made a strong effort regarding the organization of their quite long paper, meaning that tables and figures guide the reader to an application or result of interest. In addition, they provide an extensive overview on the literature with more than 400 references.},
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24N97592X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {3},
  pages        = {401},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Book reviews. <em>SIREV</em>, <em>66</em>(2), 391–399. (<a
href="https://doi.org/10.1137/24N975918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As I sat down to write this introduction, I became curious how the books chosen for review have changed over the past decades. So I scanned through a few SIREV Book Review section introductions written 10, 20 or more years ago by former section editors. That act of procrastination allows me to put the current collection of reviews in “historical context.” Some of the topics are relatively new, whereas others have remained relevant over decades. Our featured review was written by Zachary Kilpatrick, on the book Neurodynamics: An Applied Mathematics Perspective, authored by Stephen Coombes and Kyle Wedgwood. As Kilpatrick noted, while neuroscience is not a new topic, it has inspired a number of new mathematical methods, like network science and topological data analysis. Authors Coombes and Wedgwood take the reader “on a thrilling and addictive journey developing and analyzing many canonical models in neuroscience.” Sounds exciting! Kilpatrick concluded his review by noting that the book “keeps this tradition of mechanistic model analysis alive, while modernizing with handy methods from stochastic processes and piecewise smooth dynamical systems.” I wrote the next review for a book that likely would not have been on the shelf twenty years ago: Math for Deep Learning: What You Need to Know to Understand Neural Networks, by Ronald T. Kneusel. Unlike most other math book authors, Kneusel is not a university faculty member but an AI developer. His practitioner&#39;s background shows in his ability to explain advanced mathematical concepts in an accessible way, accompanied by practical examples. The next review was written by Noah Rosenberg, on the book Tree Balance Indices: A Comprehensive Survey, by Mareike Fischer, Lina Herbst, Sophie Kersting, Luise Kühn, and Kristina Wicke. Rosenberg noted that the literature on tree balance has been fragmented across different fields (e.g., mathematics, theoretical computer science, and evolutionary biology), often featuring slightly different definitions or indexing methods, which can be a barrier for researchers. By helping to unify some of these indices, this is a “wonderfully welcome book,” according to Rosenberg. The next book review by Vittorio Romano is on Advanced Calculus and Its Applications in Variational Quantum Mechanics and Relativity Theory, by Fabio Silva Botelho. The title of the book gives a pretty clear clue about its topics. Romano found the book&#39;s objective rather ambitious, which may have made its content challenging for readers who aren&#39;t already familiar with the methods. Stephania Bellavia provided a review for An Introduction to Optimization on Smooth Manifolds, by Nicolas Boumal. The book covers numerical methods for unconstrained optimization on smooth manifolds and, contrary to its title, is not an introduction to these topics, as Bellavia noted. Read the review and you will get a pretty good idea of what the book is about. We wrap up with a lively review written by Volker Schulz, on the book A Journey through the History of Numerical Linear Algebra, by Claude Brezinski, Gérard Meurant, and Michela Redivo-Zaglia. The history of numerical linear algebra is rich, so this is bound to be a long journey. Indeed, this is a book of roughly 800 pages that looks at the role numerical linear algebra plays in recent developments in science and technology, particularly the progress in data science and machine learning. As you can tell at the beginning of this introduction, I am often fascinated by the evolution of things (science, law, culture). Schulz&#39;s review has inspired me to find time to learn about the historical roots of a field that lies at the heart of current progress in math, science, and technology.},
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24N975918},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {391-399},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamics of signaling games. <em>SIREV</em>, <em>66</em>(2),
368–387. (<a href="https://doi.org/10.1137/23M156402X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This tutorial describes several basic and much-studied types of interactions with incomplete information, analyzing them by means of evolutionary game dynamics. The games include sender-receiver games, owner-challenger contests, costly advertising, and calls for help. We model the evolution of populations of players reacting to each other and compare adaptive dynamics, replicator dynamics, and best-reply dynamics. In particular, we study signaling norms and nonequilibrium outcomes.},
  archive      = {J_SIREV},
  author       = {Hannelore De Silva and Karl Sigmund},
  doi          = {10.1137/23M156402X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {368-387},
  shortjournal = {SIAM Rev.},
  title        = {Dynamics of signaling games},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The poincaré metric and the bergman theory. <em>SIREV</em>,
<em>66</em>(2), 355–367. (<a
href="https://doi.org/10.1137/22M1544622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We treat the Poincaré metric on the disc. In particular we emphasize the fact that it is the canonical holomorphically invariant metric on the unit disc. Then we generalize these ideas to the Bergman metric on a domain in complex space. Along the way we treat the Bergman kernel and study its invariance and uniqueness properties.},
  archive      = {J_SIREV},
  author       = {Steven G. Krantz},
  doi          = {10.1137/22M1544622},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {355-367},
  shortjournal = {SIAM Rev.},
  title        = {The poincaré metric and the bergman theory},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Education. <em>SIREV</em>, <em>66</em>(2), 353. (<a
href="https://doi.org/10.1137/24N975906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue the Education section presents two contributions. The first paper, “The Poincaré Metric and the Bergman Theory,” by Steven G. Krantz, discusses the Poincaré metric on the unit disc in the complex space and the Bergman metric on an arbitrary domain in any dimensional complex space. To define the Bergman metric the notion of Bergman kernel is crucial. Some striking properties of the Bergman kernel are discussed briefly, and it is calculated when the domain is the open unit ball. The Bergman metric is invariant under biholomorphic maps. The paper ends by discussing several attractive applications. To incorporate invariance within models in applied science, in particular for machine learning applications, there is currently a considerable interest in non-Euclidean metrics, in invariant (under some actions) metrics, and in reproducing kernels, mostly in the real-valued framework. The Bergman theory (1921) is a special case of Aronszajn&#39;s theory of Hilbert spaces with reproducing kernels (1950). Invariant metrics are used, in particular, in the study of partial differential equations. Complex-valued kernels have some interesting connections to linear systems theory. This article sheds some new light on the Poincaré metric, the Bergman kernel, the Bergman metric, and their applications in a manner that helps the reader become accustomed to these notions and to enjoy their properties. The second paper, “Dynamics of Signaling Games,” is presented by Hannelore De Silva and Karl Sigmund and is devoted to much-studied types of interactions with incomplete information, analyzing them by means of evolutionary game dynamics. Game theory is often encountered in models describing economic, social, and biological behavior, where decisions can not only be shaped by rational arguments, but may also be influenced by other factors and players. However, it is often restricted to an analysis of equilibria. In signaling games some agents are less informed than others and try to deal with it by observing actions (signals) from better informed agents. Such signals may be even purposely wrong. This article offers a concise guided tour of outcomes of evolutionary dynamics in a number of small dimensional signaling games focusing on the replicator dynamics, the best-reply dynamics, and the adaptive dynamics (dynamics of behavioral strategies whose vector field follows the gradient of the payoff vector). Furthermore, for the model of evolution of populations of players, the authors compare these dynamics. Several interesting examples illustrate that even simple adaptation processes can lead to nonequilibrium outcomes and endless cycling. This tutorial is targeted at graduate/Ph.D. students and researchers who know the basics of game theory and want to learn examples of signaling games, together with evolutionary game theory.},
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/24N975906},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {353},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonsmooth optimization over the stiefel manifold and beyond:
Proximal gradient method and recent variants. <em>SIREV</em>,
<em>66</em>(2), 319–352. (<a
href="https://doi.org/10.1137/24M1628578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function. Existing methods for solving this class of problems converge slowly in practice, involve subproblems that can be as difficult as the original problem, or lack rigorous convergence guarantees. In this paper, we propose a manifold proximal gradient method (ManPG) for solving this class of problems. We prove that the proposed method converges globally to a stationary point and establish its iteration complexity for obtaining an $\epsilon$-stationary point. Furthermore, we present numerical results on the sparse PCA and compressed modes problems to demonstrate the advantages of the proposed method. We also discuss some recent advances related to ManPG for Riemannian optimization with nonsmooth objective functions.},
  archive      = {J_SIREV},
  author       = {Shixiang Chen and Shiqian Ma and Anthony Man-Cho So and Tong Zhang},
  doi          = {10.1137/24M1628578},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {319-352},
  shortjournal = {SIAM Rev.},
  title        = {Nonsmooth optimization over the stiefel manifold and beyond: Proximal gradient method and recent variants},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). SIGEST. <em>SIREV</em>, <em>66</em>(2), 317. (<a
href="https://doi.org/10.1137/24N97589X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “Nonsmooth Optimization over the Stiefel Manifold and Beyond: Proximal Gradient Method and Recent Variants,” by Shixiang Chen, Shiqian Ma, Anthony Man-Cho So, and Tong Zhang. This work considers nonsmooth optimization on the Stiefel manifold, the manifold of orthonormal $k$-frames in $\mathbb{R}^n$. The authors propose a novel proximal gradient algorithm, coined ManPG, for minimizing the sum of a smooth, potentially nonconvex function, and a convex and potentially nonsmooth function whose arguments live on the Stiefel manifold. In contrast to existing approaches, which either are computationally expensive (due to expensive subproblems or slow convergence) or lack rigorous convergence guarantees, ManPG is thoroughly analyzed and features subproblems that can be computed efficiently. Nonsmooth optimization problems on the Stiefel manifold appear in many applications. In statistics sparse principal component analysis (PCA), that is, PCA that seeks principal components with very few nonzero entries, is a prime example. Unsupervised feature selection (machine learning) and blind deconvolution with a sparsity constraint on the deconvolved signal (inverse problems) are important instances of this general objective structure. At the heart of this work is a beautiful interplay between a theoretically well-founded and efficient novel optimization approach for an important class of problems and a set of computational experiments that demonstrate the effectiveness of this new approach. In order to make proximal gradient work for the Stiefel manifold they add a retraction step to the iterations that keeps the iterates feasible. The authors prove global convergence of ManPG to a stationary point and analyze its computational complexity for approximating the latter to $\epsilon$ accuracy. The numerical discussion features results for sparse PCA and the problem of computing compressed modes, that is, spatially localized solutions, of the independent-particle Schrödinger equation. The original 2020 article, which appeared in SIAM Journal on Optimization, has attracted considerable attention. In preparing this SIGEST version, the authors have added a discussion on several subsequent works on algorithms for solving Riemannian optimization with nonsmooth objectives. These works were mostly motivated by the ManPG algorithm and include a manifold proximal point algorithm, manifold proximal linear algorithm, stochastic ManPG, zeroth-order ManPG, Riemannian proximal gradient method, and Riemannian proximal Newton method.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24N97589X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {317},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new version of the adaptive fast gauss transform for
discrete and continuous sources. <em>SIREV</em>, <em>66</em>(2),
287–315. (<a href="https://doi.org/10.1137/23M1572453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new version of the fast Gauss transform (FGT) for discrete and continuous sources. Classical Hermite expansions are avoided entirely, making use only of the plane-wave representation of the Gaussian kernel and a new hierarchical merging scheme. For continuous source distributions sampled on adaptive tensor product grids, we exploit the separable structure of the Gaussian kernel to accelerate the computation. For discrete sources, the scheme relies on the nonuniform fast Fourier transform (NUFFT) to construct near field plane-wave representations. The scheme has been implemented for either free-space or periodic boundary conditions. In many regimes, the speed is comparable to or better than that of the conventional FFT in work per grid point, despite being fully adaptive.},
  archive      = {J_SIREV},
  author       = {Leslie F. Greengard and Shidong Jiang and Manas Rachh and Jun Wang},
  doi          = {10.1137/23M1572453},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {287-315},
  shortjournal = {SIAM Rev.},
  title        = {A new version of the adaptive fast gauss transform for discrete and continuous sources},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Research spotlights. <em>SIREV</em>, <em>66</em>(2), 285.
(<a href="https://doi.org/10.1137/24N975888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gauss transform---convolution with a Gaussian in the continuous case and the sum of $N$ Gaussians at $M$ points in the discrete case---is ubiquitous in applied mathematics, from solving ordinary and partial differential equations to probability density estimation to science applications in astrophysics, image processing, quantum mechanics, and beyond. For the discrete case, the fast Gauss transform (FGT) enables the approximate calculation of the sum of $N$ Gaussians at $M$ points in order $N + M$ (instead of $NM$) operations by a fast summation strategy, which shares work between the sums at different points, similarly to the fast multipole method. In this issue&#39;s Research Spotlights section, “A New Version of the Adaptive Fast Gauss Transform for Discrete and Continuous Sources,” authors Leslie F. Greengard, Shidong Jiang, Manas Rachh, and Jun Wang present a new FGT technique that avoids the use of Hermite and local expansions. The new technique employs Fourier spectral approximations, which are accelerated by nonuniform fast Fourier transforms, and results in a considerably more efficient adaptive implementation. Adaptivity is especially vital for realizing the acceleration from a fast transform when points are highly nonuniform. The paper presents compelling illustrations and examples of the computational approach and the adaptive tree-based hierarchy employed. This hierarchy is used to resolve point distributions down to a refinement level determined by accuracy demands; this results in significantly better work per grid point than conventional FGT techniques. Consequently, the authors note that there are potential key benefits in parallelization of the proposed technique. In addition to the technique&#39;s clever composition of a broad variety of advanced computing paradigms and exploitation of mathematical structure to facilitate such fast transforms, the authors present several pathways of future research. For example, the analysis is readily accessible from dimensions larger than the illustrative examples illuminate, and univariate sum-of-exponentials structure also may be exploited; the computing techniques detailed by the authors could be tailored to such regimes. These future directions have broad application in scientific computing.},
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24N975888},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {285},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational methods for large-scale inverse problems: A
survey on hybrid projection methods. <em>SIREV</em>, <em>66</em>(2),
205–284. (<a href="https://doi.org/10.1137/21M1441420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper surveys an important class of methods that combine iterative projection methods and variational regularization methods for large-scale inverse problems. Iterative methods such as Krylov subspace methods are invaluable in the numerical linear algebra community and have proved important in solving inverse problems due to their inherent regularizing properties and their ability to handle large-scale problems. Variational regularization describes a broad and important class of methods that are used to obtain reliable solutions to inverse problems, whereby one solves a modified problem that incorporates prior knowledge. Hybrid projection methods combine iterative projection methods with variational regularization techniques in a synergistic way, providing researchers with a powerful computational framework for solving very large inverse problems. Although the idea of a hybrid Krylov method for linear inverse problems goes back to the 1980s, several recent advances on new regularization frameworks and methodologies have made this field ripe for extensions, further analyses, and new applications. In this paper, we provide a practical and accessible introduction to hybrid projection methods in the context of solving large (linear) inverse problems.},
  archive      = {J_SIREV},
  author       = {Julianne Chung and Silvia Gazzola},
  doi          = {10.1137/21M1441420},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {205-284},
  shortjournal = {SIAM Rev.},
  title        = {Computational methods for large-scale inverse problems: A survey on hybrid projection methods},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Survey and review. <em>SIREV</em>, <em>66</em>(2), 203. (<a
href="https://doi.org/10.1137/24N975876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problems arise in various applications---for instance, in geoscience, biomedical science, or mining engineering, to mention just a few. The purpose is to recover an object or phenomenon from measured data which is typically subject to noise. The article “Computational Methods for Large-Scale Inverse Problems: A Survey on Hybrid Projection Methods,” by Julianne Chung and Silvia Gazzola, focuses on large, mainly linear, inverse problems. The mathematical modeling of such problems results in a linear system with a very large matrix $A \in \mathbb{R}^{m\times n}$ and a perturbed right-hand side. In some applications, it is not even possible to store the matrix, and thus algorithms which only use $A$ in the form of matrix-vector products $Ax$ or $A^Tx$ are the only choice. The article starts with two examples from image deblurring and tomographic reconstruction illustrating the challenges of inverse problems. It then presents the basic idea of regularization which consists of augmenting the model by additional information. Two variants of regularization methods are considered in detail, namely, variational and iterative methods. For variational methods it is crucial to know a good regularization parameter in advance. Unfortunately, its estimation can be expensive. On the other hand, iterative schemes, such as Krylov subspace methods, regularize by early termination of the iterations. Hybrid methods combine these two approaches leveraging the best features of each class. The paper focuses on hybrid projection methods. Here, one starts with a Krylov process in which the original problem is projected onto a low-dimensional subspace. The projected problem is then solved using a variational regularization method. The paper reviews the most relevant direct and iterative regularization techniques before it provides details on the two main building blocks of hybrid methods, namely, generating a subspace for the solution and solving the projected problem. It covers theoretical as well as numerical aspects of these schemes and also presents some extensions of hybrid methods: more general Tikhonov problems, nonstandard projection methods (enrichment, augmentation, recycling), $\ell_p$ regularization, Bayesian setting, and nonlinear problems. In addition, relevant software packages are provided. The presentation is very clear and the paper is also readable for those who are not experts in the field. Hence, it is valuable for everyone interested in large-scale inverse problems.},
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24N975876},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {203},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Book reviews. <em>SIREV</em>, <em>66</em>(1), 193–201. (<a
href="https://doi.org/10.1137/24N975864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If you are keen to understand the world around us by developing mathematical or data-driven models, or if you are interested in the methodologies that can be used to analyze those models, this collection of reviews may help you identify a useful book or two. Our featured review was written by Tim Hoheisel, on the book Convex Optimization: Introductory Course, written by Mikhail Moklyachuk. Hoheisel argues that convex optimization is not “solved” and certainly not “dead,” as had been deemed by some academics. Indeed, he believes that the explosive growth of machine learning problems, which often rely on convexity, has posed new challenges and renders convex optimization all the more relevant. Hoheisel notes pros and cons of the book, and concluded that it “can serve as an introductory text for students who want to learn the fundamentals of convex analysis and some theoretical aspects of convex optimization,” even though it may not be necessarily useful for researchers. After making a brief appearance in the first review, machine learning is featured in the second review, written by Diyora Salimova, on the volume, Mathematical Aspects of Deep Learning, edited by Philipp Grohs and Gitta Kutyniok. The edited volume encompasses a collection of topics concerning the mathematics of deep learning. After describing each of the eleven chapters, Salimova concludes that “it is nice to have this book in one&#39;s library,” given the increasing popularity and applications of deep learning everywhere. While some edited volumes lack cohesiveness, Salimonva notes that a strength of the book is that “it approaches modern deep learning from many different perspectives and provides various theoretical insights.” Continuing on the theme of data science, the next book is Optimization for Data Analysis, by Stephen J. Wright and Benjamin Recht. The review was written by our former section editor Volker Schulz, who commends the authors for providing “a very good basis for a course on optimization algorithms in data science.” Outside of the classroom, the book is also suitable for self-learning, as helpful exercises are provided to deepen the context. I reviewed the next book, Foundations of Computational Imaging: A Model-Based Approach, written by Charles A. Bouman. The author first started writing the book 20 years ago for a course that he was teaching---at a time when “Computational Imaging” did not exist as a field. What I like most about this book is that Bouman has succeeded in his stated goal of providing “a foundation for a collection of theoretical material that can serve as a common language for both researchers and practitioners of Computational Imaging.” The next review was written by Shaun Hendy, on the book Climate, Chaos and COVID: How Mathematical Models Describe the Universe, by Chris Budd. The book describes recent examples of how mathematical modeling has helped us navigate the world and formulate critical policies, such as climate change and COVID. While the book is engaging, Hendy notes the limited representation of women and mathematicians from minority groups. We conclude with a review on the book An Introduction to the Numerical Simulation of Stochastic Differential Equations, authored by Desmond J. Higham and Peter E. Kloeden. Minh-Binh Tran calls the book “a marvelous introduction into the theory of numerical SDEs for undergraduate students and young researchers.” Tran also notes that the book also gives excellent instructions on how to efficiently implement SDE-based models and simulations.},
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24N975864},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {193-201},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralUQ: A comprehensive library for uncertainty
quantification in neural differential equations and operators.
<em>SIREV</em>, <em>66</em>(1), 161–190. (<a
href="https://doi.org/10.1137/22M1518189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs.},
  archive      = {J_SIREV},
  author       = {Zongren Zou and Xuhui Meng and Apostolos F. Psaros and George E. Karniadakis},
  doi          = {10.1137/22M1518189},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {161-190},
  shortjournal = {SIAM Rev.},
  title        = {NeuralUQ: A comprehensive library for uncertainty quantification in neural differential equations and operators},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resonantly forced ODEs and repeated roots. <em>SIREV</em>,
<em>66</em>(1), 149–160. (<a
href="https://doi.org/10.1137/23M1545148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent article in this journal, Gouveia and Stone [``Generating Resonant and Repeated Root Solutions to Ordinary Differential Equations Using Perturbation Methods,” SIAM Rev., 64 (2022), pp. 485--499] described a method for finding exact solutions to resonantly forced linear ordinary differential equations, and for finding the general solution of repeated root linear systems. It is shown here that applying their mathematical justification directly yields a method that is faster and algebraically simpler than the method they described. This method seems to be unknown in the undergraduate textbook literature, although it certainly should be present there as it is elegant and simple to apply, generally giving solutions with much less work than variation of parameters.},
  archive      = {J_SIREV},
  author       = {Allan R. Willms},
  doi          = {10.1137/23M1545148},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {149-160},
  shortjournal = {SIAM Rev.},
  title        = {Resonantly forced ODEs and repeated roots},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Education. <em>SIREV</em>, <em>66</em>(1), 147. (<a
href="https://doi.org/10.1137/24N975852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue the Education section presents two contributions. The first paper, “Resonantly Forced ODEs and Repeated Roots,” is written by Allan R. Willms. The resonant forcing problem is as follows: find $y(\cdot)$ such that $L[y(x)]=u(x)$, where $L[u(x)]=0$ and $L=a_0(x) + \sum_{j=1}^n a_j(x) \frac{d^j}{dx^j}$. The repeated roots problem consists in finding $mn$ linearly independent solutions to $L^m[y(x)]=0$ under the assumption that $n$ linearly independent solutions to $L[y(x)]= 0$ are known. A recent article by B. Gouveia and H. A. Stone, “Generating Resonant and Repeated Root Solutions to Ordinary Differential Equations Using Perturbation Methods” [SIAM Rev., 64 (2022), pp. 485--499], discusses a method for finding solutions to these two problems. This new contribution observes that by applying the same mathematical justifications, one may get similar results in a simpler way. The starting point consists in defining operators $L_\lambda := \hat L -g(\lambda)$ with $L_{\lambda_0}=L$ for some $\lambda_0$ and of a parameter-dependent family of solutions to the homogeneous equations $L_\lambda[y(x;\lambda)]=0$. Under appropriate assumptions on $g$, differentiating this equality allows one to get solutions to problems of interest. This approach is illustrated on nine examples, seven of which are the same as in the publication of B. Gouveia and H. A. Stone, where for each example $g$ and $\hat L$ are appropriately chosen. This approach may be included in a course of ordinary differential equations (ODEs) as a methodology for finding solutions to these two particular classes of ODEs. It can also be used by undergraduate students for individual training as an alternative to variation of parameters. The second paper, “NeuralUQ: A Comprehensive Library for Uncertainty Quantification in Neural Differential Equations and Operators,” is presented by Zongren Zou, Xuhui Meng, Apostolos Psaros, and George E. Karniadakis. In machine learning uncertainty quantification (UQ) is a hot research topic, driven by various questions arising in computer vision and natural language processing, and by risk-sensitive applications. Numerous machine learning models, such as, for instance, physics-informed neural networks and deep operator networks, help in solving partial differential equations and learning operator mappings, respectively. However, some data may be noisy and/or sampled at random locations. This paper presents an open-source Python library (https://github.com/Crunch-UQ4MI) for employing a reliable toolbox of UQ methods for scientific machine learning. It is designed for both educational and research purposes and is illustrated on four examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs. NeuralUQ is planned to be constantly updated.},
  archive      = {J_SIREV},
  author       = {Helene Frankowska},
  doi          = {10.1137/24N975852},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {147},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple formula for the generalized spectrum of second
order self-adjoint differential operators. <em>SIREV</em>,
<em>66</em>(1), 125–146. (<a
href="https://doi.org/10.1137/23M1600992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the spectrum of the operator $\Delta^{-1} [\nabla \cdot (K\nabla u)]$ subject to homogeneous Dirichlet or Neumann boundary conditions, where $\Delta$ denotes the Laplacian and $K=K(x,y)$ is a symmetric tensor. Our main result shows that this spectrum can be derived from the spectral decomposition $K=Q \Lambda Q^T$, where $Q=Q(x,y)$ is an orthogonal matrix and $\Lambda=\Lambda(x,y)$ is a diagonal matrix. More precisely, provided that $K$ is continuous, the spectrum equals the convex hull of the ranges of the diagonal function entries of $\Lambda$. The domain involved is assumed to be bounded and Lipschitz. In addition to studying operators defined on infinite-dimensional Sobolev spaces, we also report on recent results concerning their discretized finite-dimensional counterparts. More specifically, even though $\Delta^{-1} [\nabla \cdot (K\nabla u)]$ is not compact, it turns out that every point in the spectrum of this operator can, to an arbitrary accuracy, be approximated by eigenvalues of the associated generalized algebraic eigenvalue problems arising from discretizations. Our theoretical investigations are illuminated by numerical experiments. The results presented in this paper extend previous analyses which have addressed elliptic differential operators with scalar coefficient functions. Our investigation is motivated by both preconditioning issues (efficient numerical computations) and the need to further develop the spectral theory of second order PDEs (core analysis).},
  archive      = {J_SIREV},
  author       = {Bjørn Fredrik Nielsen and Zdeněk Strakoš},
  doi          = {10.1137/23M1600992},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {125-146},
  shortjournal = {SIAM Rev.},
  title        = {A simple formula for the generalized spectrum of second order self-adjoint differential operators},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). SIGEST. <em>SIREV</em>, <em>66</em>(1), 123. (<a
href="https://doi.org/10.1137/24N975840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “A Simple Formula for the Generalized Spectrum of Second Order Self-Adjoint Differential Operators,” by Bjørn Fredrik Nielsen and Zdeněk Strakoš. This paper studies the eigenvalues of second-order self-adjoint differential operators in the continuum and discrete settings. In particular, they investigate second-order diffusion with a diffusion tensor preconditioned by the inverse Laplacian. They prove that there is a one-to-one correspondence between the spectrum of the preconditioned system and the eigenvalues of the diffusion tensor. Moreover, they investigate the relationship between the spectrum of the preconditioned operator and the generalized eigenvalue problem for its discretized counterpart and show that the latter asymptotically approximates the former. The results presented in the paper are fundamental to anyone wanting to solve elliptic PDEs. Understanding the distribution of eigenvalues is crucial for solving associated linear systems via, e.g., conjugate gradient descent whose convergence rate depends on the spread of the spectrum of the system matrix. The approach of operator preconditioning as done here with the inverse Laplacian turns the unbounded spectrum of a second-order diffusion operator into one that is completely characterized by the diffusion tensor itself. This carries over to the discrete setting, where the support of the spectrum without preconditioning is increasing as one over the squared mesh size, while in the operator preconditioned case mesh independent bounds for the eigenvalues, completely determined by the diffusion tensor, can be obtained. The original version of this article appeared in the SIAM Journal on Numerical Analysis in 2020 and has been recognized as an outstanding and well-presented result in the community. In preparing this SIGEST version, the authors have added new material to sections 1 and 2 in order to increase accessibility, added clarifications to sections 6 and 7, and added the new section 8, which contains a description of more recent results concerning the numerical approximation of the continuous spectrum. It also comments on the related differences between the (generalized) PDE eigenvalue problems for compact and noncompact operators and provides several new references.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24N975840},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {123},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Easy uncertainty quantification (EasyUQ): Generating
predictive distributions from single-valued model output.
<em>SIREV</em>, <em>66</em>(1), 91–122. (<a
href="https://doi.org/10.1137/22M1541915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we quantify uncertainty if our favorite computational tool---be it a numerical, statistical, or machine learning approach, or just any computer model---provides single-valued output only? In this article, we introduce the Easy Uncertainty Quantification (EasyUQ) technique, which transforms real-valued model output into calibrated statistical distributions, based solely on training data of model output--outcome pairs, without any need to access model input. In its basic form, EasyUQ is a special case of the recently introduced isotonic distributional regression (IDR) technique that leverages the pool-adjacent-violators algorithm for nonparametric isotonic regression. EasyUQ yields discrete predictive distributions that are calibrated and optimal in finite samples, subject to stochastic monotonicity. The workflow is fully automated, without any need for tuning. The Smooth EasyUQ approach supplements IDR with kernel smoothing, to yield continuous predictive distributions that preserve key properties of the basic form, including both stochastic monotonicity with respect to the original model output and asymptotic consistency. For the selection of kernel parameters, we introduce multiple one-fit grid search, a computationally much less demanding approximation to leave-one-out cross-validation. We use simulation examples and forecast data from weather prediction to illustrate the techniques. In a study of benchmark problems from machine learning, we show how EasyUQ and Smooth EasyUQ can be integrated into the workflow of neural network learning and hyperparameter tuning, and we find EasyUQ to be competitive with conformal prediction as well as more elaborate input-based approaches.},
  archive      = {J_SIREV},
  author       = {Eva-Maria Walz and Alexander Henzi and Johanna Ziegel and Tilmann Gneiting},
  doi          = {10.1137/22M1541915},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {91-122},
  shortjournal = {SIAM Rev.},
  title        = {Easy uncertainty quantification (EasyUQ): Generating predictive distributions from single-valued model output},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Research spotlights. <em>SIREV</em>, <em>66</em>(1), 89.
(<a href="https://doi.org/10.1137/24N975839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As modeling, simulation, and data-driven capabilities continue to advance and be adopted for an ever expanding set of applications and downstream tasks, there has been an increased need for quantifying the uncertainty in the resulting predictions. In “Easy Uncertainty Quantification (EasyUQ): Generating Predictive Distributions from Single-Valued Model Output,” authors Eva-Maria Walz, Alexander Henzi, Johanna Ziegel, and Tilmann Gneiting provide a methodology for moving beyond deterministic scalar-valued predictions to obtain particular statistical distributions for these predictions. The approach relies on training data of model output-observation pairs of scalars, and hence does not require access to higher-dimensional inputs or latent variables. The authors use numerical weather prediction as a particular example, where one can obtain repeated forecasts, and corresponding observations, of temperatures at a specific location. Given a predicted temperature, the EasyUQ approach provides a nonparametric distribution of temperatures around this value. EasyUQ uses the training data to effectively minimize an empirical score subject to a stochastic monotonicity constraint, which ensures that the predictive distribution values become larger as the model output value grows. In doing so, the approach inherits the theoretical properties of optimality and consistency enjoyed by so-called isotonic distributional regression methods. The authors emphasize that the basic version of EasyUQ does not require elaborate hyperparameter tuning. They also introduce a more sophisticated version that relies on kernel smoothing to yield predictive probability densities while preserving key properties of the basic version. The paper demonstrates how EasyUQ compares with the standard technique of applying a Gaussian error distribution to a deterministic forecast as well as how EasyUQ can be used to obtain uncertainty estimates for artificial neural network outputs. The approach will be especially of interest for settings when inputs or other latent variables are unreliable or unavailable since it offers a straightforward yet statistically principled and computationally efficient way for working only with outputs and observations.},
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24N975839},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {89},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite element methods respecting the discrete maximum
principle for convection-diffusion equations. <em>SIREV</em>,
<em>66</em>(1), 3–88. (<a
href="https://doi.org/10.1137/22M1488934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convection-diffusion-reaction equations model the conservation of scalar quantities. From the analytic point of view, solutions of these equations satisfy, under certain conditions, maximum principles, which represent physical bounds of the solution. That the same bounds are respected by numerical approximations of the solution is often of utmost importance in practice. The mathematical formulation of this property, which contributes to the physical consistency of a method, is called the discrete maximum principle (DMP). In many applications, convection dominates diffusion by several orders of magnitude. It is well known that standard discretizations typically do not satisfy the DMP in this convection-dominated regime. In fact, in this case it turns out to be a challenging problem to construct discretizations that, on the one hand, respect the DMP and, on the other hand, compute accurate solutions. This paper presents a survey on finite element methods, with the main focus on the convection-dominated regime, that satisfy a local or a global DMP. The concepts of the underlying numerical analysis are discussed. The survey reveals that for the steady-state problem there are only a few discretizations, all of them nonlinear, that at the same time both satisfy the DMP and compute reasonably accurate solutions, e.g., algebraically stabilized schemes. Moreover, most of these discretizations have been developed in recent years, showing the enormous progress that has been achieved lately. Similarly, methods based on algebraic stabilization, both nonlinear and linear, are currently the only finite element methods that combine the satisfaction of the global DMP and accurate numerical results for the evolutionary equations in the convection-dominated scenario.},
  archive      = {J_SIREV},
  author       = {Gabriel R. Barrenechea and Volker John and Petr Knobloch},
  doi          = {10.1137/22M1488934},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {3-88},
  shortjournal = {SIAM Rev.},
  title        = {Finite element methods respecting the discrete maximum principle for convection-diffusion equations},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Survey and review. <em>SIREV</em>, <em>66</em>(1), 1. (<a
href="https://doi.org/10.1137/24N975827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical methods for partial differential equations can only be successful if their numerical solutions reflect fundamental properties of the physical solution of the respective PDE. For convection-diffusion equations, the conservation of some specific scalar quantities is crucial. When physical solutions satisfy maximum principles representing physical bounds, then the numerical solutions should respect the same bounds. In a mathematical setting, this requirement is known as the discrete maximum principle (DMP). Discretizations which fail to fulfill the DMP are prone to numerical solutions with unphysical values, e.g., spurious oscillations. However, when convection largely dominates diffusion, many discretization methods do not satisfy a DMP. In the only article of the Survey and Review section of this issue, “Finite Element Methods Respecting the Discrete Maximum Principle for Convection-Diffusion Equations,” Gabriel R. Barrenechea, Volker John, and Petr Knobloch study and analyze finite element methods that succeed in complying with DMP while providing accurate numerical solutions at the same time. This is a nontrivial task and, thus, even for the steady-state problem there are only a few such discretizations, all of them nonlinear. Most of these methods have been developed quite recently, so that the presentation highlights the state of the art and spotlights the huge progress accomplished in recent years. The goal of the paper consists in providing a survey on finite element methods that satisfy local or global DMPs for linear elliptic or parabolic problems. It is worth reading for a large audience.},
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24N975827},
  journal      = {SIAM Review},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
