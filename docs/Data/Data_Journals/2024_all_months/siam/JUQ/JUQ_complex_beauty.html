<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---50">JUQ - 50</h2>
<ul>
<li><details>
<summary>
(2024). Finite-dimensional models for response analysis.
<em>JUQ</em>, <em>12</em>(4), 1424–1449. (<a
href="https://doi.org/10.1137/24M1651745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Finite-dimensional (FD) models are constructed for white/colored inputs to stochastic differential equations (SDEs) and for the solutions of these equations. Conditions are established under which FD models of solutions of SDEs converge to these solutions in the metric of the space of continuous functions. It is shown that the distributions of extremes of solutions of SDEs can be approximated by those of extremes of FD solutions, i.e., solutions of the proposed SDEs to FD input models. It is concluded that the distributions of extremes of SDEs can be estimated from samples of FD solutions. Three examples are presented to illustrate numerically our theoretical considerations and demonstrate the accuracy of the FD solutions.},
  archive      = {J_JUQ},
  author       = {Hui Xu and Mircea Grigoriu},
  doi          = {10.1137/24M1651745},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1424-1449},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Finite-dimensional models for response analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive operator learning for infinite-dimensional bayesian
inverse problems. <em>JUQ</em>, <em>12</em>(4), 1389–1423. (<a
href="https://doi.org/10.1137/24M1643815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The fundamental computational issues in Bayesian inverse problems (BIPs) governed by partial differential equations (PDEs) stem from the requirement of repeated forward model evaluations. A popular strategy to reduce such costs is to replace expensive model simulations with computationally efficient approximations using operator learning, motivated by recent progress in deep learning. However, using the approximated model directly may introduce a modeling error, exacerbating the already ill-posedness of inverse problems. Thus, balancing between accuracy and efficiency is essential for the effective implementation of such approaches. To this end, we develop an adaptive operator learning framework that can reduce modeling error gradually by forcing the surrogate to be accurate in local areas. This is accomplished by adaptively fine-tuning the pretrained approximate model with training points chosen by a greedy algorithm during the posterior evaluation process. To validate our approach, we use DeepOnet to construct the surrogate and unscented Kalman inversion (UKI) to approximate the BIP solution, respectively. Furthermore, we present a rigorous convergence guarantee in the linear case using the UKI framework. The approach is tested on a number of benchmarks, including the Darcy flow, the heat source inversion problem, and the reaction-diffusion problem. The numerical results show that our method can significantly reduce computational costs while maintaining inversion accuracy.},
  archive      = {J_JUQ},
  author       = {Zhiwei Gao and Liang Yan and Tao Zhou},
  doi          = {10.1137/24M1643815},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1389-1423},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive operator learning for infinite-dimensional bayesian inverse problems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approximate control variates approach to multifidelity
distribution estimation. <em>JUQ</em>, <em>12</em>(4), 1349–1388. (<a
href="https://doi.org/10.1137/23M1584307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Forward simulation–based uncertainty quantification that studies the distribution of quantities of interest (QoI) is crucial for computationally robust engineering design and prediction. A large body of literature is devoted to accurately assessing QoI statistics. In particular, multilevel or multifidelity approaches are known to be effective, leveraging cost-accuracy trade-offs within a given ensemble of models. However, effective algorithms that can estimate the full distribution of QoIs are still under active development. In this paper, we introduce a general multifidelity framework for estimating the cumulative distribution function (CDF) of a vector-valued QoI associated with a high-fidelity model under a budget constraint. Given a family of control variates obtained from lower-fidelity surrogates, our framework involves identifying the most cost-effective model subset under a weighted error metric and then using it to build an approximate control variates estimator for the target CDF. We instantiate the framework by constructing control variates using linear regression and rigorously analyze the corresponding algorithm. Our analysis reveals that the resulting CDF estimator is uniformly consistent and asymptotically optimal under appropriate criteria as the budget tends to infinity, with only mild moment and regularity assumptions on the joint distribution of QoIs. The approach provides a robust multifidelity CDF estimator that is adaptive to the available budget, does not require a priori knowledge of cross-model statistics or model hierarchy, and applies to multiple dimensions. We demonstrate the efficiency and robustness of the approach using test examples of parametric PDEs and stochastic differential equations including both academic instances and more challenging engineering problems.},
  archive      = {J_JUQ},
  author       = {Ruijian Han and Boris Kramer and Dongjin Lee and Akil Narayan and Yiming Xu},
  doi          = {10.1137/23M1584307},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1349-1388},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An approximate control variates approach to multifidelity distribution estimation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering the unknowns: A first step. <em>JUQ</em>,
<em>12</em>(4), 1336–1348. (<a
href="https://doi.org/10.1137/23M159874X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article aims at discovering the unknown variables in the system through data analysis. The main idea is to use the time of data collection as a surrogate variable and try to identify the unknown variables by modeling gradual and sudden changes in the data. We use Gaussian process modeling and a sparse representation of the sudden changes to efficiently estimate the large number of parameters in the proposed statistical model. The method is tested on a realistic dataset generated using a one-dimensional implementation of a Magnetized Liner Inertial Fusion (MagLIF) simulation model, and encouraging results are obtained.},
  archive      = {J_JUQ},
  author       = {V. Roshan Joseph and William E. Lewis and Henry S. Yuchi and Kathryn A. Maupin},
  doi          = {10.1137/23M159874X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1336-1348},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Discovering the unknowns: A first step},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uniform error bounds of the ensemble transform kalman filter
for chaotic dynamics with multiplicative covariance inflation.
<em>JUQ</em>, <em>12</em>(4), 1315–1335. (<a
href="https://doi.org/10.1137/24M1637192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Data assimilation is a method of uncertainty quantification to estimate the hidden true state by updating the prediction owing to model dynamics with observation data. As a prediction model, we consider a class of nonlinear dynamical systems on Hilbert spaces including the two-dimensional Navier–Stokes equations and the Lorenz 63 and 96 equations. For nonlinear model dynamics, the ensemble Kalman filter (EnKF) is often used to approximate the mean and covariance of the probability distribution with a set of particles called an ensemble. In this paper, we consider a deterministic version of the EnKF known as the ensemble transform Kalman filter (ETKF), performing well even with limited ensemble sizes compared to other stochastic implementations of the EnKF. When the ETKF is applied to large-scale systems, an ad-hoc numerical technique called a covariance inflation is often employed to reduce approximation errors. Despite the practical effectiveness of the ETKF, little is theoretically known. The present study aims to establish the theoretical analysis of the ETKF. We obtain that the state estimation error of the ETKF with and without the covariance inflation is bounded for any finite time. Furthermore, when the system is finite-dimensional, we show that the uniform-in-time error bound is obtained when an inflation parameter is chosen appropriately, justifying the effectiveness of the covariance inflation in the ETKF.},
  archive      = {J_JUQ},
  author       = {Kota Takeda and Takashi Sakajo},
  doi          = {10.1137/24M1637192},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1315-1335},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uniform error bounds of the ensemble transform kalman filter for chaotic dynamics with multiplicative covariance inflation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted adaptive design. <em>JUQ</em>, <em>12</em>(4),
1273–1314. (<a href="https://doi.org/10.1137/22M149898X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Modern advanced manufacturing and advanced materials design often require searches of relatively high-dimensional process control parameter spaces for settings that result in optimal structure, property, and performance parameters. The mapping from the former to the latter must be determined from noisy experiments or from expensive simulations. We abstract this problem to a mathematical framework in which an unknown function from a control space to a design space must be ascertained by means of expensive noisy measurements, which locate control settings generating desired design features within specified tolerances, with quantified uncertainty. We describe targeted adaptive design (TAD), a new algorithm that performs this sampling task efficiently. TAD creates a Gaussian process surrogate model of the unknown mapping at each iterative stage, proposing a new batch of control settings to sample experimentally and optimizing the updated expected log-predictive probability density of the target design. TAD either stops upon locating a solution with uncertainties that fit inside the tolerance box or uses a measure of expected future information to determine that the search space has been exhausted with no solution. TAD thus embodies the exploration-exploitation tension in a manner that recalls, but is essentially different from, Bayesian optimization and optimal experimental design.},
  archive      = {J_JUQ},
  author       = {Carlo Graziani and Marieme Ngom},
  doi          = {10.1137/22M149898X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1273-1314},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Targeted adaptive design},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven rules for multidimensional reflection problems.
<em>JUQ</em>, <em>12</em>(4), 1240–1272. (<a
href="https://doi.org/10.1137/23M1618570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Over the recent past, data-driven algorithms for solving stochastic optimal control problems in the face of model uncertainty have become an increasingly active area of research. However, for singular controls and underlying diffusion dynamics, the analysis has so far been restricted to the scalar case. In this paper, we fill this gap by studying a multivariate singular control problem for reversible diffusions with controls of a reflection type. Our contributions are threefold. We first explicitly determine the long-run average costs as a domain-dependent functional, showing that the control problem can be equivalently characterized as a shape optimization problem. For given diffusion dynamics, assuming the optimal domain to be strongly star-shaped, we then propose a gradient descent algorithm based on polytope approximations to numerically determine a cost-minimizing domain. Finally, we investigate data-driven solutions when the diffusion dynamics are unknown to the controller. Using techniques from nonparametric statistics for stochastic processes, we construct an optimal domain estimator whose static regret is bounded by the minimax optimal estimation rate of the unreflected process’s invariant density. In the most challenging situation, when the dynamics must be learned simultaneously with controlling the process, we develop an episodic learning algorithm to overcome the emerging exploration-exploitation dilemma and show that given the static regret as a baseline, the loss in its sublinear regret per time unit is of natural order compared to the one-dimensional case.},
  archive      = {J_JUQ},
  author       = {Sören Christensen and Asbjørn Holk Thomsen and Lukas Trottner},
  doi          = {10.1137/23M1618570},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1240-1272},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Data-driven rules for multidimensional reflection problems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted leave-one-out cross validation. <em>JUQ</em>,
<em>12</em>(4), 1213–1239. (<a
href="https://doi.org/10.1137/23M1615917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a weighted version of Leave-One-Out (LOO) cross validation for estimating the Integrated Squared Error (ISE) when approximating an unknown function by a predictor that depends linearly on evaluations of the function over a finite collection of sites. The method relies on the construction of the best linear estimator of the squared prediction error at an arbitrary unsampled site based on squared LOO residuals, assuming that the function is a realization of a Gaussian Process (GP). A theoretical analysis of performance of the ISE estimator is presented, and robustness with respect to the choice of the GP kernel is investigated first analytically, then through numerical examples. Overall, the estimation of the ISE is significantly more precise than with classical, unweighted, LOO cross validation. Application to model selection is briefly considered through examples.},
  archive      = {J_JUQ},
  author       = {Luc Pronzato and Maria-João Rendas},
  doi          = {10.1137/23M1615917},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1213-1239},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Weighted leave-one-out cross validation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple closed curve modeling with uncertainty
quantification for shape analysis. <em>JUQ</em>, <em>12</em>(4),
1192–1212. (<a href="https://doi.org/10.1137/23M1590287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we introduce a multidimensional, multiple-output Gaussian process (GP) model for statistical shape analysis, specifically addressing the intricate task of characterizing dependence of closed curves. Our proposed approach efficiently handles multilevel structural similarities induced by shape-specific information among collections of curves, providing meaningful uncertainty quantification. Our GP-based methodology opens avenues for nonparametric modeling of multilevel dependence among closed curve shapes with uncertainty quantification for the first time. In particular, we demonstrate how the proposed model can improve performance of various tasks within elastic shape analysis, such as registration, averaging, curve reconstruction, and landmark estimation. Our code is available at https://github.com/lanl/multishapeGP.},
  archive      = {J_JUQ},
  author       = {Hengrui Luo and Justin D. Strait},
  doi          = {10.1137/23M1590287},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1192-1212},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multiple closed curve modeling with uncertainty quantification for shape analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging viscous hamilton–jacobi PDEs for uncertainty
quantification in scientific machine learning. <em>JUQ</em>,
<em>12</em>(4), 1165–1191. (<a
href="https://doi.org/10.1137/24M1646455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Uncertainty quantification (UQ) in scientific machine learning (SciML) combines the powerful predictive power of SciML with methods for quantifying the reliability of the learned models. However, two major challenges remain: limited interpretability and expensive training procedures. We provide a new interpretation for UQ problems by establishing a new theoretical connection between some Bayesian inference problems arising in SciML and viscous Hamilton–Jacobi partial differential equations (HJ PDEs). Namely, we show that the posterior mean and covariance can be recovered from the spatial gradient and Hessian of the solution to a viscous HJ PDE. As a first exploration of this connection, we specialize in Bayesian inference problems with linear models, Gaussian likelihoods, and Gaussian priors. In this case, the associated viscous HJ PDEs can be solved using Riccati ODEs, and we develop a new Riccati-based methodology that provides computational advantages when continuously updating the model predictions. Specifically, our Riccati-based approach can efficiently add or remove data points to the training set invariant to the order of the data and continuously tune hyperparameters. Moreover, neither update requires retraining on or access to previously incorporated data. We provide several examples from SciML involving noisy data and epistemic uncertainty to illustrate the potential advantages of our approach. In particular, this approach’s amenability to data streaming applications demonstrates its potential for real-time inferences, which, in turn, allows for applications in which the predicted uncertainty is used to dynamically alter the learning process.},
  archive      = {J_JUQ},
  author       = {Zongren Zou and Tingwei Meng and Paula Chen and Jérôme Darbon and George Em Karniadakis},
  doi          = {10.1137/24M1646455},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1165-1191},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Leveraging viscous Hamilton–Jacobi PDEs for uncertainty quantification in scientific machine learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certified multifidelity zeroth-order optimization.
<em>JUQ</em>, <em>12</em>(4), 1135–1164. (<a
href="https://doi.org/10.1137/23M1591086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of multifidelity zeroth-order optimization, where one can evaluate a function at various approximation levels (of varying costs), and the goal is to optimize with the cheapest evaluations possible. In this paper, we study certified algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function . We also prove an -dependent lower bound showing that this algorithm has a near-optimal cost complexity. As a direct example, we close the paper by addressing the special case of noisy (stochastic) evaluations, which corresponds to -best arm identification in Lipschitz bandits with continuously many arms.},
  archive      = {J_JUQ},
  author       = {Étienne de Montbrun and Sébastien Gerchinovitz},
  doi          = {10.1137/23M1591086},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1135-1164},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Certified multifidelity zeroth-order optimization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient kriging using interleaved lattice-based designs
with low fill and high separation distance properties. <em>JUQ</em>,
<em>12</em>(4), 1113–1134. (<a
href="https://doi.org/10.1137/23M156940X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Kriging is a powerful technique to emulate computer experiments. The purpose of this paper is to construct interleaved lattice-based designs that are most suitable for Kriging. Firstly, we provide evidences that although it is well-known that designs with low fill distance or high separation distance are appealing for Kriging, the two distance measures should be combined to reach a more striking criterion for selecting designs. Secondly, we propose a method to efficiently construct optimal interleaved lattice-based designs under the newly proposed criterion. Numerical results suggest that the new criterion and construction combined perform well in Kriging interpolation under various scenarios.},
  archive      = {J_JUQ},
  author       = {Xu He},
  doi          = {10.1137/23M156940X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1113-1134},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Efficient kriging using interleaved lattice-based designs with low fill and high separation distance properties},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical shrinkage gaussian processes: Applications to
computer code emulation and dynamical system recovery. <em>JUQ</em>,
<em>12</em>(4), 1085–1112. (<a
href="https://doi.org/10.1137/23M1550682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In many areas of science and engineering, computer simulations are widely used as proxies for physical experiments, which can be infeasible or unethical. Such simulations are often computationally expensive, and an emulator can be trained to efficiently predict the desired response surface. A widely used emulator is the Gaussian process (GP), which provides a flexible framework for efficient prediction and uncertainty quantification. Standard GPs, however, do not capture structured shrinkage on the underlying response surface, which is present in many applications, particularly in the physical sciences. We thus propose a new hierarchical shrinkage GP (HierGP), which incorporates such structure via cumulative shrinkage priors within a GP framework. We show that the HierGP implicitly embeds the principles of effect hierarchy, heredity, and smoothness widely used for analysis of experiments; such principles allow the HierGP to identify significant structured effects on the response surface with limited data. We propose efficient posterior sampling algorithms for model training and prediction and prove desirable consistency properties for the HierGP. Finally, we demonstrate the improved performance of HierGP over existing models in a suite of numerical experiments and an application to dynamical system recovery.},
  archive      = {J_JUQ},
  author       = {Tao Tang and Simon Mak and David Dunson},
  doi          = {10.1137/23M1550682},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1085-1112},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Hierarchical shrinkage gaussian processes: Applications to computer code emulation and dynamical system recovery},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The bayesian approach to inverse robin problems.
<em>JUQ</em>, <em>12</em>(3), 1050–1084. (<a
href="https://doi.org/10.1137/23M1620624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we investigate the Bayesian approach to inverse Robin problems. These are problems for certain elliptic boundary value problems of determining a Robin coefficient on a hidden part of the boundary from Cauchy data on the observable part. Such a nonlinear inverse problem arises naturally in the initialization of large-scale ice sheet models that are crucial in climate and sea-level predictions. We motivate the Bayesian approach for a prototypical Robin inverse problem by showing that the posterior mean converges in probability to the data-generating ground truth as the number of observations increases. Related to the stability theory for inverse Robin problems, we establish a logarithmic convergence rate for Sobolev-regular Robin coefficients, whereas for analytic coefficients we can attain an algebraic rate. The use of rescaled analytic Gaussian priors in posterior consistency for nonlinear inverse problems is new and may be of separate interest in other inverse problems. Our numerical results illustrate the convergence property in two observation settings.},
  archive      = {J_JUQ},
  author       = {Aksel K. Rasmussen and Fanny Seizilles and Mark Girolami and Ieva Kazlauskaite},
  doi          = {10.1137/23M1620624},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1050-1084},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {The bayesian approach to inverse robin problems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariance expressions for multifidelity sampling with
multioutput, multistatistic estimators: Application to approximate
control variates. <em>JUQ</em>, <em>12</em>(3), 1005–1049. (<a
href="https://doi.org/10.1137/23M1607994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a collection of results on covariance expressions between Monte Carlo–based multioutput mean, variance, and Sobol main effect variance estimators from an ensemble of models. These covariances can be used within multifidelity uncertainty quantification strategies that seek to reduce the estimator variance of high-fidelity Monte Carlo estimators with an ensemble of low-fidelity models. Such covariance expressions are required within approaches such as the approximate control variate and multilevel best linear unbiased estimator. While the literature provides these expressions for some single-output cases such as mean and variance, our results are relevant to both multiple function outputs and multiple statistics across any sampling strategy. Following the description of these results, we use them within an approximate control variate scheme to show that leveraging multiple outputs can dramatically reduce estimator variance compared to single-output approaches. Synthetic examples are used to highlight the effects of optimal sample allocation and pilot sample estimation. A flight-trajectory simulation of entry, descent, and landing is used to demonstrate multioutput estimation in practical applications.},
  archive      = {J_JUQ},
  author       = {Thomas O. Dixon and James E. Warner and Geoffrey F. Bomarito and Alex A. Gorodetsky},
  doi          = {10.1137/23M1607994},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1005-1049},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Covariance expressions for multifidelity sampling with multioutput, multistatistic estimators: Application to approximate control variates},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter inference based on gaussian processes informed by
nonlinear partial differential equations. <em>JUQ</em>, <em>12</em>(3),
964–1004. (<a href="https://doi.org/10.1137/22M1514131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Partial differential equations (PDEs) are widely used for the description of physical and engineering phenomena. Some key parameters involved in PDEs, which represent certain physical properties with important scientific interpretations, are difficult or even impossible to measure directly. Estimating these parameters from noisy and sparse experimental data of related physical quantities is an important task. Many methods for PDE parameter inference involve a large number of evaluations for numerical solutions to PDEs through algorithms such as the finite element method, which can be time consuming, especially for nonlinear PDEs. In this paper, we propose a novel method for the inference of unknown parameters in PDEs, called the PDE-informed Gaussian process (PIGP)–based parameter inference method. Through modeling the PDE solution as a Gaussian process (GP), we derive the manifold constraints induced by the (linear) PDE structure such that, under the constraints, the GP satisfies the PDE. For nonlinear PDEs, we propose an augmentation method that transforms the nonlinear PDE into an equivalent PDE system linear in all derivatives, which our PIGP-based method can handle. The proposed method can be applied to a broad spectrum of nonlinear PDEs. The PIGP-based method can be applied to multidimensional PDE systems and PDE systems with unobserved components. Like conventional Bayesian approaches, the method can provide uncertainty quantification for both the unknown parameters and the PDE solution. The PIGP-based method also completely bypasses the numerical solver for PDEs. The proposed method is demonstrated through several application examples from different areas.},
  archive      = {J_JUQ},
  author       = {Zhaohui Li and Shihao Yang and C. F. Jeff Wu},
  doi          = {10.1137/22M1514131},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {964-1004},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Parameter inference based on gaussian processes informed by nonlinear partial differential equations},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multilevel subset simulation with selective
refinement. <em>JUQ</em>, <em>12</em>(3), 932–963. (<a
href="https://doi.org/10.1137/22M1515240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work we propose an adaptive multilevel version of subset simulation to estimate the probability of rare events for complex physical systems. Given a sequence of nested failure domains of increasing size, the rare event probability is expressed as a product of conditional probabilities. The proposed new estimator uses different model resolutions and varying numbers of samples across the hierarchy of nested failure sets. In order to dramatically reduce the computational cost, we construct the intermediate failure sets such that only a small number of expensive high-resolution model evaluations are needed, whilst the majority of samples can be taken from inexpensive low-resolution simulations. A key idea in our new estimator is the use of a posteriori error estimators combined with a selective mesh refinement strategy to guarantee the critical subset property that may be violated when changing model resolution from one failure set to the next. The efficiency gains and the statistical properties of the estimator are investigated both theoretically via shaking transformations, as well as numerically. On a model problem from subsurface flow, the new multilevel estimator achieves gains of more than a factor 60 over standard subset simulation for a practically relevant relative error of 25%.},
  archive      = {J_JUQ},
  author       = {D. Elfverson and R. Scheichl and S. Weissmann and F. A. Diaz De La O},
  doi          = {10.1137/22M1515240},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {932-963},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive multilevel subset simulation with selective refinement},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fully parallelized and budgeted multilevel monte carlo
method and the application to acoustic waves. <em>JUQ</em>,
<em>12</em>(3), 901–931. (<a
href="https://doi.org/10.1137/23M1588354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a novel variant of the multilevel Monte Carlo method that effectively utilizes a reserved computational budget on a high-performance computing system to minimize the mean squared error. Our approach combines concepts of the continuation multilevel Monte Carlo method with dynamic programming techniques following Bellman’s optimality principle and a new parallelization strategy based on a single distributed data structure. Additionally, we establish a theoretical bound on the error reduction on a parallel computing cluster and provide empirical evidence that the proposed method adheres to this bound. We implement, test, and benchmark the approach on computationally demanding problems, focusing on its application to acoustic wave propagation in high-dimensional random media.},
  archive      = {J_JUQ},
  author       = {Niklas Baumgarten and Sebastian Krumscheid and Christian Wieners},
  doi          = {10.1137/23M1588354},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {901-931},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A fully parallelized and budgeted multilevel monte carlo method and the application to acoustic waves},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional sampling with monotone GANs: From generative
models to likelihood-free inference. <em>JUQ</em>, <em>12</em>(3),
868–900. (<a href="https://doi.org/10.1137/23M1581546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a novel framework for conditional sampling of probability measures, using block triangular transport maps. We develop the theoretical foundations of block triangular transport in a Banach space setting, establishing general conditions under which conditional sampling can be achieved and drawing connections between monotone block triangular maps and optimal transport. Based on this theory, we then introduce a computational approach, called monotone generative adversarial networks (M-GANs), to learn suitable block triangular maps. Our algorithm uses only samples from the underlying joint probability measure and is hence likelihood-free. Numerical experiments with M-GAN demonstrate accurate sampling of conditional measures in synthetic examples, Bayesian inverse problems involving ordinary and partial differential equations, and probabilistic image inpainting.},
  archive      = {J_JUQ},
  author       = {Ricardo Baptista and Bamdad Hosseini and Nikola B. Kovachki and Youssef M. Marzouk},
  doi          = {10.1137/23M1581546},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {868-900},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Conditional sampling with monotone GANs: From generative models to likelihood-free inference},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harmonizable nonstationary processes. <em>JUQ</em>,
<em>12</em>(3), 842–867. (<a
href="https://doi.org/10.1137/22M1544580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Harmonizable processes can be represented by sums of harmonics with random coefficients, which are correlated rather than uncorrelated as for weakly stationary processes. Harmonizable processes are characterized in the second moment sense by their generalized spectral density functions. It is shown that harmonizable processes admit spectral representations and can be band limited and/or narrow band; samples of harmonizable Gaussian processes can be generated by algorithms similar to those used to generate samples of stationary Gaussian processes; accurate finite dimensional (FD) surrogates, i.e., deterministic functions of time and finite sets of random variables, can be constructed for harmonizable processes; and, under mild conditions, a broad range of nonstationary processes are harmonizable. Numerical illustrations, including various nonstationary processes and outputs of linear systems to random inputs, are presented to demonstrate the versatility of harmonizable processes.},
  archive      = {J_JUQ},
  author       = {Mircea Grigoriu},
  doi          = {10.1137/22M1544580},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {842-867},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Harmonizable nonstationary processes},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral convergence of a semi-discretized numerical system
for the spatially homogeneous boltzmann equation with uncertainties.
<em>JUQ</em>, <em>12</em>(3), 812–841. (<a
href="https://doi.org/10.1137/24M1638483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we study the Boltzmann equation with uncertainties and prove that the spectral convergence of the semi-discretized numerical system holds in a combined velocity and random space, where the Fourier spectral method is applied for approximation in the velocity space, whereas the generalized polynomial chaos (gPC)-based stochastic Galerkin (SG) method is employed to discretize the random variable. Our proof is based on a delicate energy estimate for showing the well-posedness of the numerical solution as well as a rigorous control of its negative part in our well-designed functional space that involves high-order derivatives of both the velocity and random variables. This paper rigorously justifies the statement proposed in Remark 4.4 of [J. Hu and S. Jin, J. Comput. Phys., 315 (2016), pp. 150–168].},
  archive      = {J_JUQ},
  author       = {Liu Liu and Kunlun Qi},
  doi          = {10.1137/24M1638483},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {812-841},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Spectral convergence of a semi-discretized numerical system for the spatially homogeneous boltzmann equation with uncertainties},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emulating complex dynamical simulators with random fourier
features. <em>JUQ</em>, <em>12</em>(3), 788–811. (<a
href="https://doi.org/10.1137/22M147339X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A Gaussian process (GP)-based methodology is proposed to emulate complex dynamical computer models (or simulators). The method relies on emulating the numerical flow map of the system over an initial (short) time step, where the flow map is a function that describes the evolution of the system from an initial condition to a subsequent value at the next time step. This yields a probabilistic distribution over the entire flow map function, with each draw offering an approximation to the flow map. The model output time series is then predicted (under the Markov assumption) by drawing a sample from the emulated flow map (i.e., its posterior distribution) and using it to iterate from the initial condition ahead in time. Repeating this procedure with multiple such draws creates a distribution over the time series. The mean and variance of this distribution at a specific time point serve as the model output prediction and the associated uncertainty, respectively. However, drawing a GP posterior sample that represents the underlying function across its entire domain is computationally infeasible, given the infinite-dimensional nature of this object. To overcome this limitation, one can generate such a sample in an approximate manner using random Fourier features (RFF). RFF is an efficient technique for approximating the kernel and generating GP samples, offering both computational efficiency and theoretical guarantees. The proposed method is applied to emulate several dynamic nonlinear simulators including the well-known Lorenz and van der Pol models. The results suggest that our approach has a promising predictive performance and the associated uncertainty can capture the dynamics of the system appropriately.},
  archive      = {J_JUQ},
  author       = {Hossein Mohammadi and Peter Challenor and Marc Goodfellow},
  doi          = {10.1137/22M147339X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {788-811},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Emulating complex dynamical simulators with random fourier features},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperparameter estimation for sparse bayesian learning
models. <em>JUQ</em>, <em>12</em>(3), 759–787. (<a
href="https://doi.org/10.1137/24M162844X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sparse Bayesian learning (SBL) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. The hyperparameters in SBL models are crucial for the model’s performance, but they are often difficult to estimate due to the nonconvexity and the high-dimensionality of the associated objective function. This paper presents a comprehensive framework for hyperparameter estimation in SBL models, encompassing well-known algorithms such as the expectation-maximization, MacKay, and convex bounding algorithms. These algorithms are cohesively interpreted within an alternating minimization and linearization (AML) paradigm, distinguished by their unique linearized surrogate functions. Additionally, a novel algorithm within the AML framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. This is further improved by a new alternating minimization and quadratic approximation paradigm, which includes a proximal regularization term. The paper substantiates these advancements with thorough convergence analysis and numerical experiments, demonstrating the algorithm’s effectiveness in various noise conditions and signal-to-noise ratios.},
  archive      = {J_JUQ},
  author       = {Feng Yu and Lixin Shen and Guohui Song},
  doi          = {10.1137/24M162844X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {759-787},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Hyperparameter estimation for sparse bayesian learning models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Choosing observation operators to mitigate model error in
bayesian inverse problems. <em>JUQ</em>, <em>12</em>(3), 723–758. (<a
href="https://doi.org/10.1137/23M1602140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In statistical inference, a discrepancy between the parameter-to-observable map that generates the data and the parameter-to-observable map that is used for inference can lead to misspecified likelihoods and thus to incorrect estimates. In many inverse problems, the parameter-to-observable map is the composition of a linear state-to-observable map called an “observation operator” and a possibly nonlinear parameter-to-state map called the “model.” We consider such Bayesian inverse problems where the discrepancy in the parameter-to-observable map is due to the use of an approximate model that differs from the best model, i.e., to nonzero “model error.” Multiple approaches have been proposed to address such discrepancies, each leading to a specific posterior. We show how to use local Lipschitz stability estimates of posteriors with respect to likelihood perturbations to bound the Kullback–Leibler divergence of the posterior of each approach with respect to the posterior associated to the best model. Our bounds lead to criteria for choosing observation operators that mitigate the effect of model error for Bayesian inverse problems of this type. We illustrate the feasibility of one such criterion on an advection-diffusion-reaction PDE inverse problem and use this example to discuss the importance and challenges of model error-aware inference.},
  archive      = {J_JUQ},
  author       = {Nada Cvetković and Han Cheng Lie and Harshit Bansal and Karen Veroy},
  doi          = {10.1137/23M1602140},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {723-758},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Choosing observation operators to mitigate model error in bayesian inverse problems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combination technique for optimal control problems
constrained by random PDEs. <em>JUQ</em>, <em>12</em>(2), 693–721. (<a
href="https://doi.org/10.1137/22M1532263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a combination technique based on mixed differences of both spatial approximations and quadrature formulae for the stochastic variables to solve efficiently a class of optimal control problems (OCPs) constrained by random partial differential equations. The method requires to solve the OCP for several low-fidelity spatial grids and quadrature formulae for the objective functional. All the computed solutions are then linearly combined to get a final approximation which, under suitable regularity assumptions, preserves the same accuracy of fine tensor product approximations, while drastically reducing the computational cost. The combination technique involves only tensor product quadrature formulae, and thus the discretized OCPs preserve the (possible) convexity of the continuous OCP. Hence, the combination technique avoids the inconveniences of multilevel Monte Carlo and/or sparse grids approaches but remains suitable for high-dimensional problems. The manuscript presents an a priori procedure to choose the most important mixed differences and an analysis stating that the asymptotic complexity is exclusively determined by the spatial solver. Numerical experiments validate the results.},
  archive      = {J_JUQ},
  author       = {Fabio Nobile and Tommaso Vanzan},
  doi          = {10.1137/22M1532263},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {693-721},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A combination technique for optimal control problems constrained by random PDEs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proportional marginal effects for global sensitivity
analysis. <em>JUQ</em>, <em>12</em>(2), 667–692. (<a
href="https://doi.org/10.1137/22M153032X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Performing (variance-based) global sensitivity analysis (GSA) with dependent inputs has recently benefited from cooperative game theory concepts, leading to meaningful sensitivity indices suitable with dependent inputs. The “Shapley effects,” i.e., the Shapley values transposed to variance-based GSA problems, are an example of such indices. However, these indices exhibit a particular behavior that can be undesirable: an exogenous input (i.e., which is not explicitly included in the structural equations of the model) can be associated with a strictly positive index when it is correlated to endogenous inputs. This paper investigates using a different allocation, called the “proportional values” for GSA purposes. First, an extension of this allocation is proposed to make it suitable for variance-based GSA. A novel GSA index is then defined: the proportional marginal effect (PME). The notion of exogeneity is formally defined in the context of variance-based GSA. It is shown that the PMEs are more discriminant than the Shapley values and allow the distinction of exogenous variables, even when they are correlated to endogenous inputs. Moreover, their behavior is compared to the Shapley effects on analytical toy cases and more realistic use cases.},
  archive      = {J_JUQ},
  author       = {Margot Herin and Marouane Il Idrissi and Vincent Chabridon and Bertrand Iooss},
  doi          = {10.1137/22M153032X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {667-692},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Proportional marginal effects for global sensitivity analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized bayesian MARS: Tools for stochastic computer
model emulation. <em>JUQ</em>, <em>12</em>(2), 646–666. (<a
href="https://doi.org/10.1137/23M1577122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The multivariate adaptive regression spline (MARS) approach of Friedman [J. H. Friedman, Ann. Statist., 19 (1991), pp. 1–67] and its Bayesian counterpart [D. Francom et al., Statist. Sinica, 28 (2018), pp. 791–816] are effective approaches for the emulation of computer models. The traditional assumption of Gaussian errors limits the usefulness of MARS, and many popular alternatives, when dealing with stochastic computer models. We propose a generalized Bayesian MARS (GBMARS) framework which admits the broad class of generalized hyperbolic distributions as the induced likelihood function. This allows us to develop tools for the emulation of stochastic simulators which are parsimonious, scalable, and interpretable and require minimal tuning, while providing powerful predictive and uncertainty quantification capabilities. GBMARS is capable of robust regression with t distributions, quantile regression with asymmetric Laplace distributions, and a general form of “Normal-Wald” regression in which the shape of the error distribution and the structure of the mean function are learned simultaneously. We demonstrate the effectiveness of GBMARS on various stochastic computer models, and we show that it compares favorably to several popular alternatives.},
  archive      = {J_JUQ},
  author       = {Kellin N. Rumsey and Devin Francom and Andy Shen},
  doi          = {10.1137/23M1577122},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {646-666},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Generalized bayesian MARS: Tools for stochastic computer model emulation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-shot learning of surrogates in PDE-constrained
optimization under uncertainty. <em>JUQ</em>, <em>12</em>(2), 614–645.
(<a href="https://doi.org/10.1137/23M1553170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a general framework for machine learning based optimization under uncertainty. Our approach replaces the complex forward model by a surrogate, which is learned simultaneously in a one-shot sense when solving the optimal control problem. Our approach relies on a reformulation of the problem as a penalized empirical risk minimization problem for which we provide a consistency analysis in terms of large data and increasing penalty parameter. To solve the resulting problem, we suggest a stochastic gradient method with adaptive control of the penalty parameter and prove convergence under suitable assumptions on the surrogate model. Numerical experiments illustrate the results for linear and nonlinear surrogate models.},
  archive      = {J_JUQ},
  author       = {Philipp A. Guth and Claudia Schillings and Simon Weissmann},
  doi          = {10.1137/23M1553170},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {614-645},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {One-shot learning of surrogates in PDE-constrained optimization under uncertainty},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying the effect of random dispersion for logarithmic
schrödinger equation. <em>JUQ</em>, <em>12</em>(2), 579–613. (<a
href="https://doi.org/10.1137/23M1578619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is concerned with the random effect of the noise dispersion for the stochastic logarithmic Schrödinger equation emerged from the optical fibre with dispersion management. The well-posedness of the logarithmic Schrödinger equation with white noise dispersion is established via the regularization energy approximation and a spatial scaling property. For the small noise case, the effect of the noise dispersion is quantified by the proven large deviation principle under additional regularity assumptions on the initial datum. As an application, we show that for the regularized model, the exit from a neighborhood of the attractor of deterministic equation occurs on a sufficiently large time scale. Furthermore, the exit time and exit point in the small noise case, as well as the effect of large noise dispersion, is also discussed for the stochastic logarithmic Schrödinger equation.},
  archive      = {J_JUQ},
  author       = {Jianbo Cui and Liying Sun},
  doi          = {10.1137/23M1578619},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {579-613},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantifying the effect of random dispersion for logarithmic schrödinger equation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential equation–constrained optimization with
stochasticity. <em>JUQ</em>, <em>12</em>(2), 549–578. (<a
href="https://doi.org/10.1137/23M1571162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Most inverse problems from physical sciences are formulated as PDE-constrained optimization problems. This involves identifying unknown parameters in equations by optimizing the model to generate PDE solutions that closely match measured data. The formulation is powerful and widely used in many science and engineering fields. However, one crucial assumption is that the unknown parameter must be deterministic. In reality, however, many problems are stochastic in nature, and the unknown parameter is random. The challenge then becomes recovering the full distribution of this unknown random parameter. It is a much more complex task. In this paper, we examine this problem in a general setting. In particular, we conceptualize the PDE solver as a push-forward map that pushes the parameter distribution to the generated data distribution. In this way, the SDE-constrained optimization translates to minimizing the distance between the generated distribution and the measurement distribution. We then formulate a gradient flow equation to seek the ground-truth parameter probability distribution. This opens up a new paradigm for extending many techniques in PDE-constrained optimization to optimization for systems with stochasticity.},
  archive      = {J_JUQ},
  author       = {Qin Li and Li Wang and Yunan Yang},
  doi          = {10.1137/23M1571162},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {549-578},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Differential Equation–Constrained optimization with stochasticity},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computationally efficient sampling methods for sparsity
promoting hierarchical bayesian models. <em>JUQ</em>, <em>12</em>(2),
524–548. (<a href="https://doi.org/10.1137/23M1564043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Bayesian hierarchical models have been demonstrated to provide efficient algorithms for finding sparse solutions to ill-posed inverse problems. The models comprise typically a conditionally Gaussian prior model for the unknown, augmented by a hyperprior model for the variances. A widely used choice for the hyperprior is a member of the family of generalized gamma distributions. Most of the work in the literature has concentrated on numerical approximation of the maximum a posteriori estimates, and less attention has been paid on sampling methods or other means for uncertainty quantification. Sampling from the hierarchical models is challenging mainly for two reasons: The hierarchical models are typically high dimensional, thus suffering from the curse of dimensionality, and the strong correlation between the unknown of interest and its variance can make sampling rather inefficient. This work addresses mainly the first one of these obstacles. By using a novel reparametrization, it is shown how the posterior distribution can be transformed into one dominated by a Gaussian white noise, allowing sampling by using the preconditioned Crank–Nicholson (pCN) scheme that has been shown to be efficient for sampling from distributions dominated by a Gaussian component. Furthermore, a novel idea for speeding up the pCN in a special case is developed, and the question of how strongly the hierarchical models are concentrated on sparse solutions is addressed in light of a computed example.},
  archive      = {J_JUQ},
  author       = {D. Calvetti and E. Somersalo},
  doi          = {10.1137/23M1564043},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {524-548},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Computationally efficient sampling methods for sparsity promoting hierarchical bayesian models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying domain uncertainty in linear elasticity.
<em>JUQ</em>, <em>12</em>(2), 503–523. (<a
href="https://doi.org/10.1137/23M1578589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The present article considers the quantification of uncertainty for the equations of linear elasticity on random domains. To this end, we model the random domains as the images of some given fixed, nominal domain under random domain mappings, which are defined by a Karhunen–Loève expansion. We then prove the analytic regularity of the random solution with respect to the countable random input parameters which enter the problem through the Karhunen–Loève expansion of the random domain mappings. In particular, we provide appropriate bounds on arbitrary derivatives of the random solution with respect to those input parameters. These enable the use of state-of-the-art quadrature methods to compute deterministic statistics of quantities of interest, such as the mean and the variance of the random solution itself or the random von Mises stress, as integrals over the countable random input parameters in a dimensionally robust way. Numerical examples qualify and quantify the theoretical findings.},
  archive      = {J_JUQ},
  author       = {Helmut Harbrecht and Viacheslav Karnaev and Marc Schmidlin},
  doi          = {10.1137/23M1578589},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {503-523},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantifying domain uncertainty in linear elasticity},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conglomerate multi-fidelity gaussian process modeling, with
application to heavy-ion collisions. <em>JUQ</em>, <em>12</em>(2),
473–502. (<a href="https://doi.org/10.1137/22M1525004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In an era where scientific experimentation is often costly, multi-fidelity emulation provides a powerful tool for predictive scientific computing. While there has been notable work on multi-fidelity modeling, existing models do not incorporate an important “conglomerate” property of multi-fidelity simulators, where the accuracies of different simulator components are controlled by different fidelity parameters. Such conglomerate simulators are widely encountered in complex nuclear physics and astrophysics applications. We thus propose a new CONglomerate multi-FIdelity Gaussian process (CONFIG) model, which embeds this conglomerate structure within a novel non-stationary covariance function. We show that the proposed CONFIG model can capture prior knowledge on the numerical convergence of conglomerate simulators, which allows for cost-efficient emulation of multi-fidelity systems. We demonstrate the improved predictive performance of CONFIG over state-of-the-art models in a suite of numerical experiments and two applications, the first for emulation of cantilever beam deflection and the second for emulating the evolution of the quark-gluon plasma, which was theorized to have filled the universe shortly after the Big Bang.},
  archive      = {J_JUQ},
  author       = {Yi Ji and Henry Shaowu Yuchi and Derek Soeder and J.-F. Paquet and Steffen A. Bass and V. Roshan Joseph and C. F. Jeff Wu and Simon Mak},
  doi          = {10.1137/22M1525004},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {473-502},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Conglomerate multi-fidelity gaussian process modeling, with application to heavy-ion collisions},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging joint sparsity in hierarchical bayesian learning.
<em>JUQ</em>, <em>12</em>(2), 442–472. (<a
href="https://doi.org/10.1137/23M156255X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a hierarchical Bayesian learning approach to infer jointly sparse parameter vectors from multiple measurement vectors. Our model uses separate conditionally Gaussian priors for each parameter vector and common gamma-distributed hyperparameters to enforce joint sparsity. The resulting joint-sparsity-promoting priors are combined with existing Bayesian inference methods to generate a new family of algorithms. Our numerical experiments, which include a multicoil magnetic resonance imaging application, demonstrate that our new approach consistently outperforms commonly used hierarchical Bayesian methods.},
  archive      = {J_JUQ},
  author       = {Jan Glaubitz and Anne Gelb},
  doi          = {10.1137/23M156255X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {442-472},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Leveraging joint sparsity in hierarchical bayesian learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble kalman filters with resampling. <em>JUQ</em>,
<em>12</em>(2), 411–441. (<a
href="https://doi.org/10.1137/23M1594935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Filtering is concerned with online estimation of the state of a dynamical system from partial and noisy observations. In applications where the state of the system is high dimensional, ensemble Kalman filters are often the method of choice. These algorithms rely on an ensemble of interacting particles to sequentially estimate the state as new observations become available. Despite the practical success of ensemble Kalman filters, theoretical understanding is hindered by the intricate dependence structure of the interacting particles. This paper investigates ensemble Kalman filters that incorporate an additional resampling step to break the dependency between particles. The new algorithm is amenable to a theoretical analysis that extends and improves upon those available for filters without resampling, while also performing well in numerical examples.},
  archive      = {J_JUQ},
  author       = {Omar Al-Ghattas and Jiajun Bao and Daniel Sanz-Alonso},
  doi          = {10.1137/23M1594935},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {411-441},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Ensemble kalman filters with resampling},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric estimation for independent and identically
distributed stochastic differential equations with space-time dependent
coefficients. <em>JUQ</em>, <em>12</em>(2), 377–410. (<a
href="https://doi.org/10.1137/23M1581662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider independent and identically distributed one-dimensional inhomogeneous diffusion processes with drift and diffusion coefficient , where and the functions and are known. Our concern is the nonparametric estimation of the -dimensional unknown function from the continuous observation of the sample paths throughout a fixed time interval . A collection of projection estimators belonging to a product of finite-dimensional subspaces of is built. The -risk is defined by the expectation of either an empirical norm or a deterministic norm fitted to the problem. Rates of convergence for large are discussed. A data-driven choice of the dimensions of the projection spaces is proposed. The theoretical results are illustrated by numerical experiments on simulated data.},
  archive      = {J_JUQ},
  author       = {Fabienne Comte and Valentine Genon-Catalot},
  doi          = {10.1137/23M1581662},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {377-410},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Nonparametric estimation for independent and identically distributed stochastic differential equations with space-time dependent coefficients},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wavelet-based density estimation for persistent homology.
<em>JUQ</em>, <em>12</em>(2), 347–376. (<a
href="https://doi.org/10.1137/23M1573811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Persistent homology is a central methodology in topological data analysis that has been successfully implemented in many fields and is becoming increasingly popular and relevant. The output of persistent homology is a persistence diagram—a multiset of points supported on the upper half-plane—that is often used as a statistical summary of the topological features of data. In this paper, we study the random nature of persistent homology and estimate the density of expected persistence diagrams from observations using wavelets; we show that our wavelet-based estimator is optimal. Furthermore, we propose an estimator that offers a sparse representation of the expected persistence diagram that achieves near-optimality. We demonstrate the utility of our contributions in a machine learning task in the context of dynamical systems.},
  archive      = {J_JUQ},
  author       = {Konstantin Häberle and Barbara Bravi and Anthea Monod},
  doi          = {10.1137/23M1573811},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {347-376},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Wavelet-based density estimation for persistent homology},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonasymptotic bounds for suboptimal importance sampling.
<em>JUQ</em>, <em>12</em>(2), 309–346. (<a
href="https://doi.org/10.1137/21M1427760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Importance sampling is a popular variance reduction method for Monte Carlo estimation, where an evident question is how to design good proposal distributions. While in most cases optimal (zero-variance) estimators are theoretically possible, in practice only suboptimal proposal distributions are available and it can often be observed numerically that those can reduce statistical performance significantly, leading to large relative errors and therefore counteracting the original intention. Previous analysis on importance sampling has often focused on asymptotic arguments that work well in a large deviations regime. In this article, we provide lower and upper bounds on the relative error in a nonasymptotic setting. They depend on the deviation of the actual proposal from optimality, and we thus identify potential robustness issues that importance sampling may have, especially in high dimensions. We particularly focus on path sampling problems for diffusion processes with nonvanishing noise, for which generating good proposals comes with additional technical challenges. We provide numerous numerical examples that support our findings and demonstrate the applicability of the derived bounds.},
  archive      = {J_JUQ},
  author       = {Carsten Hartmann and Lorenz Richter},
  doi          = {10.1137/21M1427760},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {309-346},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Nonasymptotic bounds for suboptimal importance sampling},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing statistical moments via tensorization of
polynomial chaos expansions. <em>JUQ</em>, <em>12</em>(2), 289–308. (<a
href="https://doi.org/10.1137/23M155428X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present an algorithm for estimating higher-order statistical moments of multidimensional functions expressed as polynomial chaos expansions (PCE). The algorithm starts by decomposing the PCE into a low-rank tensor network using a combination of tensor-train and Tucker decompositions. It then efficiently calculates the desired moments in the compressed tensor domain, leveraging the highly linear structure of the network. Using three benchmark engineering functions, we demonstrate that our approach offers substantial speed improvements over alternative algorithms while maintaining a minimal and adjustable approximation error. Additionally, our method can calculate moments even when the input variable distribution is altered, incurring only a small additional computational cost and without requiring retraining of the regressor.},
  archive      = {J_JUQ},
  author       = {Rafael Ballester-Ripoll},
  doi          = {10.1137/23M155428X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {289-308},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Computing statistical moments via tensorization of polynomial chaos expansions},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method of moments estimator for interacting particle
systems and their mean field limit. <em>JUQ</em>, <em>12</em>(2),
262–288. (<a href="https://doi.org/10.1137/22M153848X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of learning unknown parameters in stochastic interacting particle systems with polynomial drift, interaction, and diffusion functions from the path of one single particle in the system. Our estimator is obtained by solving a linear system which is constructed by imposing appropriate conditions on the moments of the invariant distribution of the mean field limit and on the quadratic variation of the process. Our approach is easy to implement as it only requires the approximation of the moments via the ergodic theorem and the solution of a low-dimensional linear system. Moreover, we prove that our estimator is asymptotically unbiased in the limits of infinite data and infinite number of particles (mean field limit). In addition, we present several numerical experiments that validate the theoretical analysis and show the effectiveness of our methodology to accurately infer parameters in systems of interacting particles.},
  archive      = {J_JUQ},
  author       = {Grigorios A. Pavliotis and Andrea Zanoni},
  doi          = {10.1137/22M153848X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {262-288},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A method of moments estimator for interacting particle systems and their mean field limit},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calculation of epidemic first passage and peak time
probability distributions. <em>JUQ</em>, <em>12</em>(2), 242–261. (<a
href="https://doi.org/10.1137/23M1548049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Understanding the timing of the peak of a disease outbreak forms an important part of epidemic forecasting. In many cases, such information is essential for planning increased hospital bed demand and for designing of public health interventions. The time taken for an outbreak to become large is inherently stochastic and, therefore, uncertain, but after a sufficient number of infections has been reached the subsequent dynamics can be modeled accurately using ordinary differential equations. Here, we present analytical and numerical methods for approximating the time at which a stochastic model of a disease outbreak reaches a large number of cases and for quantifying the uncertainty arising from demographic stochasticity around that time. We then project this uncertainty forwards in time using an ordinary differential equation model in order to obtain a distribution for the peak timing of the epidemic that agrees closely with large simulations but that, for error tolerances relevant to most realistic applications, requires a fraction of the computational cost of full Monte Carlo approaches.},
  archive      = {J_JUQ},
  author       = {Jacob Curran-Sebastian and Lorenzo Pellis and Ian Hall and Thomas House},
  doi          = {10.1137/23M1548049},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {242-261},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Calculation of epidemic first passage and peak time probability distributions},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subsampling of parametric models with bifidelity boosting.
<em>JUQ</em>, <em>12</em>(2), 213–241. (<a
href="https://doi.org/10.1137/22M1524989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Least squares regression is a ubiquitous tool for building emulators (a.k.a. surrogate models) of problems across science and engineering for purposes such as design space exploration and uncertainty quantification. When the regression data are generated using an experimental design process (e.g., a quadrature grid) involving computationally expensive models, or when the data size is large, sketching techniques have shown promise at reducing the cost of the construction of the regression model while ensuring accuracy comparable to that of the full data. However, random sketching strategies, such as those based on leverage scores, lead to regression errors that are random and may exhibit large variability. To mitigate this issue, we present a novel boosting approach that leverages cheaper, lower-fidelity data of the problem at hand to identify the best sketch among a set of candidate sketches. This in turn specifies the sketch of the intended high-fidelity model and the associated data. We provide theoretical analyses of this bifidelity boosting (BFB) approach and discuss the conditions the low- and high-fidelity data must satisfy for a successful boosting. In doing so, we derive a bound on the residual norm of the BFB sketched solution relating it to its ideal, but computationally expensive, high-fidelity boosted counterpart. Empirical results on both manufactured and PDE data corroborate the theoretical analyses and illustrate the efficacy of the BFB solution in reducing the regression error, as compared to the nonboosted solution.},
  archive      = {J_JUQ},
  author       = {Nuojin Cheng and Osman Asif Malik and Yiming Xu and Stephen Becker and Alireza Doostan and Akil Narayan},
  doi          = {10.1137/22M1524989},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {213-241},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Subsampling of parametric models with bifidelity boosting},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum: Quasi–monte carlo finite element analysis for
wave propagation in heterogeneous random media. <em>JUQ</em>,
<em>12</em>(1), 212. (<a
href="https://doi.org/10.1137/23M1624609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract.},
  archive      = {J_JUQ},
  author       = {M. Ganesh and Frances Y. Kuo and Ian H. Sloan},
  doi          = {10.1137/23M1624609},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {212},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Corrigendum: Quasi–Monte carlo finite element analysis for wave propagation in heterogeneous random media},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perron–frobenius operator filter for stochastic dynamical
systems. <em>JUQ</em>, <em>12</em>(1), 182–211. (<a
href="https://doi.org/10.1137/23M1547391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Filtering problems are derived from a sequential minimization of a quadratic function representing a compromise between the model and data. In this paper, we use the Perron–Frobenius operator in a stochastic process to develop a Perron–Frobenius operator filter. The proposed method belongs to Bayesian filtering and works for non-Gaussian distributions for nonlinear stochastic dynamical systems. The recursion of the filtering can be characterized by the composition of the Perron–Frobenius operator and likelihood operator. This gives a significant connection between the Perron–Frobenius operator and Bayesian filtering. We numerically fulfill the recursion by approximating the Perron–Frobenius operator by Ulam’s method. In this way, the posterior measure is represented by a convex combination of the indicator functions in Ulam’s method. To get a low-rank approximation for the Perron–Frobenius operator filter, we take a spectral decomposition for the posterior measure by using the eigenfunctions of the discretized Perron–Frobenius operator. The Perron–Frobenius operator filter employs data instead of flow equations to model the evolution of underlying stochastic dynamical systems. In contrast, standard particle filters require explicit equations or transition probability density for sampling. A few numerical examples are presented to illustrate the advantage of the Perron–Frobenius operator filter over the particle filter and extended Kalman filter.},
  archive      = {J_JUQ},
  author       = {Ningxin Liu and Lijian Jiang},
  doi          = {10.1137/23M1547391},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {182-211},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Perron–Frobenius operator filter for stochastic dynamical systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stacking designs: Designing multifidelity computer
experiments with target predictive accuracy. <em>JUQ</em>,
<em>12</em>(1), 157–181. (<a
href="https://doi.org/10.1137/22M1532007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In an era where scientific experiments can be very costly, multifidelity emulators provide a useful tool for cost-efficient predictive scientific computing. For scientific applications, the experimenter is often limited by a tight computational budget, and thus wishes to (i) maximize predictive power of the multifidelity emulator via a careful design of experiments, and (ii) ensure this model achieves a desired error tolerance with some notion of confidence. Existing design methods, however, do not jointly tackle objectives (i) and (ii). We propose a novel stacking design approach that addresses both goals. A multilevel reproducing kernel Hilbert space (RKHS) interpolator is first introduced to build the emulator, under which our stacking design provides a sequential approach for designing multifidelity runs such that a desired prediction error of is met under regularity assumptions. We then prove a novel cost complexity theorem that, under this multilevel interpolator, establishes a bound on the computation cost (for training data simulation) needed to achieve a prediction bound of . This result provides novel insights on conditions under which the proposed multifidelity approach improves upon a conventional RKHS interpolator which relies on a single fidelity level. Finally, we demonstrate the effectiveness of stacking designs in a suite of simulation experiments and an application to finite element analysis.},
  archive      = {J_JUQ},
  author       = {Chih-Li Sung and Yi (Irene) Ji and Simon Mak and Wenjia Wang and Tao Tang},
  doi          = {10.1137/22M1532007},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {157-181},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stacking designs: Designing multifidelity computer experiments with target predictive accuracy},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive importance sampling based on fault tree analysis
for piecewise deterministic markov process. <em>JUQ</em>,
<em>12</em>(1), 128–156. (<a
href="https://doi.org/10.1137/22M1522838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Piecewise deterministic Markov processes (PDMPs) can be used to model complex dynamical industrial systems. The counterpart of this modeling capability is their simulation cost, which makes reliability assessment untractable with standard Monte Carlo methods. A significant variance reduction can be obtained with an adaptive importance sampling method based on a cross-entropy procedure. The success of this method relies on the selection of a good family of approximations of the committor function of the PDMP. In this paper original families are proposed. Their forms are based on reliability concepts related to fault tree analysis: minimal path sets and minimal cut sets. They are well adapted to high-dimensional industrial systems. The proposed method is discussed in detail and applied to academic systems and to a realistic system from the nuclear industry.},
  archive      = {J_JUQ},
  author       = {Guillaume Chennetier and Hassane Chraibi and Anne Dutfoy and Josselin Garnier},
  doi          = {10.1137/22M1522838},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {128-156},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive importance sampling based on fault tree analysis for piecewise deterministic markov process},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multifidelity bayesian experimental design to quantify
rare-event statistics. <em>JUQ</em>, <em>12</em>(1), 101–127. (<a
href="https://doi.org/10.1137/22M1503956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we develop a multifidelity Bayesian experimental design framework to efficiently quantify the rare-event statistics of an input-to-response (ItR) system with given input probability and expensive function evaluations. The key idea here is to leverage low-fidelity samples whose responses can be computed with a cost of a certain fraction of that for high-fidelity samples, in an optimized configuration to reduce the total computational cost. To accomplish this goal, we employ a multifidelity Gaussian process as the surrogate model of the ItR function and develop a new acquisition based on which the optimized next sample can be selected in terms of its location in the sample space and the fidelity level. In addition, we develop an inexpensive analytical evaluation of the acquisition and its derivative, avoiding numerical integrations that are prohibitive for high-dimensional problems. The new method is mainly tested in a bifidelity context for a series of synthetic problems with varying dimensions, low-fidelity model accuracy, and computational costs. Compared with the single-fidelity method and the bifidelity method with a predefined fidelity hierarchy, our method consistently shows the best (or among the best) performance for all the test cases. Finally, we demonstrate the superiority of our method in solving an engineering problem of estimating rare-event statistics of ship motion in irregular waves, using computational fluid dynamics with two different grid resolutions as the high- and low-fidelity models.},
  archive      = {J_JUQ},
  author       = {Xianliang Gong and Yulin Pan},
  doi          = {10.1137/22M1503956},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {101-127},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multifidelity bayesian experimental design to quantify rare-event statistics},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projective integral updates for high-dimensional variational
inference. <em>JUQ</em>, <em>12</em>(1), 69–100. (<a
href="https://doi.org/10.1137/22M1529919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, the feasible distributions are Gaussian mean-fields and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. Since these updates require high-dimensional integration, this work begins by proposing an efficient quasirandom sequence of quadratures for mean-field distributions. Each iterate of the sequence contains two evaluation points that combine to correctly integrate all univariate quadratic functions and, if the mean-field factors are symmetric, all univariate cubics. More importantly, averaging results over short subsequences achieves periodic exactness on a much larger space of multivariate polynomials of quadratic total degree. The corresponding variational updates require four loss evaluations with standard (not second-order) backpropagation to eliminate error terms from over half of all multivariate quadratic basis functions. This integration technique is motivated by first proposing stochastic blocked mean-field quadratures, which may be useful in other contexts. A PyTorch implementation of QNVB allows for better control over model uncertainty during training than competing methods. Experiments demonstrate superior generalizability for multiple learning problems and architectures.},
  archive      = {J_JUQ},
  author       = {Jed A. Duersch},
  doi          = {10.1137/22M1529919},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {69-100},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Projective integral updates for high-dimensional variational inference},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of a computational framework for bayesian inverse
problems: Ensemble kalman updates and MAP estimators under mesh
refinement. <em>JUQ</em>, <em>12</em>(1), 30–68. (<a
href="https://doi.org/10.1137/23M1567035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper analyzes a popular computational framework for solving infinite-dimensional Bayesian inverse problems, discretizing the prior and the forward model in a finite-dimensional weighted inner product space. We demonstrate the benefit of working on a weighted space by establishing operator-norm bounds for finite element and graph-based discretizations of Matérn-type priors and deconvolution forward models. For linear-Gaussian inverse problems, we develop a general theory for characterizing the error in the approximation to the posterior. We also embed the computational framework into ensemble Kalman methods and maximum a posteriori (MAP) estimators for nonlinear inverse problems. Our operator-norm bounds for prior discretizations guarantee the scalability and accuracy of these algorithms under mesh refinement.},
  archive      = {J_JUQ},
  author       = {Daniel Sanz-Alonso and Nathan Waniorek},
  doi          = {10.1137/23M1567035},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {30-68},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Analysis of a computational framework for bayesian inverse problems: Ensemble kalman updates and MAP estimators under mesh refinement},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error estimate of a quasi-monte carlo time-splitting
pseudospectral method for nonlinear schrödinger equation with random
potentials. <em>JUQ</em>, <em>12</em>(1), 1–29. (<a
href="https://doi.org/10.1137/22M1525181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we consider the numerical solution of a nonlinear Schrödinger equation with spatial random potential. The randomly shifted quasi-Monte Carlo (QMC) lattice rule combined with the time-splitting pseudospectral discretization is applied and analyzed. The nonlinearity in the equation induces difficulties in estimating the regularity of the solution in random space. By the technique of weighted Sobolev space, we identify the possible weights and show the existence of QMC that converges optimally at the almost-linear rate without dependence on dimensions. The full error estimate of the scheme is established. We present numerical results to verify the accuracy and investigate the wave propagation.},
  archive      = {J_JUQ},
  author       = {Zhizhang Wu and Zhiwen Zhang and Xiaofei Zhao},
  doi          = {10.1137/22M1525181},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Error estimate of a quasi-monte carlo time-splitting pseudospectral method for nonlinear schrödinger equation with random potentials},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
