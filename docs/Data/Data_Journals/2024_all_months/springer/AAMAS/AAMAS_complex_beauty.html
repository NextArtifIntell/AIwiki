<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---51">AAMAS - 51</h2>
<ul>
<li><details>
<summary>
(2024). Parameterized complexity of candidate nomination for
elections based on positional scoring rules. <em>AAMAS</em>,
<em>38</em>(2), 1–36. (<a
href="https://doi.org/10.1007/s10458-024-09658-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider elections where the set of candidates is partitioned into parties, and each party must nominate exactly one candidate. The Possible President problem asks whether some candidate of a given party can become the unique winner of the election for some nominations from other parties. We perform a multivariate computational complexity analysis of Possible President for several classes of elections based on positional scoring rules. We consider the following parameters: the size of the largest party, the number of parties, the number of voters and the number of voter types. We provide a complete computational map of Possible President in the sense that for each choice of the four possible parameters as (i) constant, (ii) parameter, or (iii) unbounded, we classify the computational complexity of the resulting problem as either polynomial-time solvable or NP-complete, and for parameterized versions as either fixed-parameter tractable or W[1]-hard with respect to the parameters considered.},
  archive      = {J_AAMAS},
  author       = {Schlotter, Ildikó and Cechlárová, Katarína and Trellová, Diana},
  doi          = {10.1007/s10458-024-09658-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Parameterized complexity of candidate nomination for elections based on positional scoring rules},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assimilating human feedback from autonomous vehicle
interaction in reinforcement learning models. <em>AAMAS</em>,
<em>38</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s10458-024-09659-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant challenge for real-world automated vehicles (AVs) is their interaction with human pedestrians. This paper develops a methodology to directly elicit the AV behaviour pedestrians find suitable by collecting quantitative data that can be used to measure and improve an algorithm&#39;s performance. Starting with a Deep Q Network (DQN) trained on a simple Pygame/Python-based pedestrian crossing environment, the reward structure was adapted to allow adjustment by human feedback. Feedback was collected by eliciting behavioural judgements collected from people in a controlled environment. The reward was shaped by the inter-action vector, decomposed into feature aspects for relevant behaviours, thereby facilitating both implicit preference selection and explicit task discovery in tandem. Using computational RL and behavioural-science techniques, we harness a formal iterative feedback loop where the rewards were repeatedly adapted based on human behavioural judgments. Experiments were conducted with 124 participants that showed strong initial improvement in the judgement of AV behaviours with the adaptive reward structure. The results indicate that the primary avenue for enhancing vehicle behaviour lies in the predictability of its movements when introduced. More broadly, recognising AV behaviours that receive favourable human judgments can pave the way for enhanced performance.},
  archive      = {J_AAMAS},
  author       = {Fox, Richard and Ludvig, Elliot A.},
  doi          = {10.1007/s10458-024-09659-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Assimilating human feedback from autonomous vehicle interaction in reinforcement learning models},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive analysis of agent factorization and learning
algorithms in multiagent systems. <em>AAMAS</em>, <em>38</em>(2), 1–48.
(<a href="https://doi.org/10.1007/s10458-024-09662-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiagent systems, agent factorization denotes the process of segmenting the state-action space of the environment into distinct components, each corresponding to an individual agent, and subsequently determining the interactions among these agents. Effective agent factorization significantly influences the system performance of real-world industrial applications. In this work, we try to assess the performance impact of agent factorization when using different learning algorithms in multiagent coordination settings; and thus discover the source of performance quality of the multiagent solution derived by combining different factorizations with different learning algorithms. To this end, we evaluate twelve different agent factorization instances—or agent definitions—in the warehouse traffic management domain, comparing the training performance of (primarily) three learning algorithms suitable for learning coordinated multiagent policies: the Evolutionary Strategies (ES), the Canonical Evolutionary Strategies (CES), and a genetic algorithm (CCEA) previously used in a similar setting. Our results demonstrate that the performance of different learning algorithms is affected in different ways by alternative agent definitions. Given this, we can conclude that many important multiagent coordination problems can eventually be solved more efficiently by a suitable agent factorization combined with an appropriate choice of a learning algorithm. Moreover, our work shows that ES and CES are effective learning algorithms for the warehouse traffic management domain, while, interestingly, celebrated policy gradient methods do not fare well in this complex real-world problem setting. As such, our work offers insights into the intrinsic properties of the learning algorithms that make them well-suited for this problem domain. More broadly, our work demonstrates the need to identify appropriate agent definitions-multiagent learning algorithm pairings in order to solve specific complex problems effectively, and provides insights into the general characteristics that such pairings must possess to address broad classes of multiagent learning and coordination problems.},
  archive      = {J_AAMAS},
  author       = {Kallinteris, Andreas and Orfanoudakis, Stavros and Chalkiadakis, Georgios},
  doi          = {10.1007/s10458-024-09662-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-48},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A comprehensive analysis of agent factorization and learning algorithms in multiagent systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond the echo chamber: Modelling open-mindedness in
citizens’ assemblies. <em>AAMAS</em>, <em>38</em>(2), 1–41. (<a
href="https://doi.org/10.1007/s10458-024-09655-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Citizens’ assembly (CA) is a democratic innovation tool where a randomly selected group of citizens deliberate a topic over multiple rounds to generate, and then vote upon, policy recommendations. Despite growing popularity, little work exists on understanding how CA inputs, such as the expert selection process and the mixing method used for discussion groups, affect results. In this work, we model CA deliberation and opinion change as a multi-agent systems problem. We introduce and formalise a set of criteria for evaluating successful CAs using insight from previous CA trials and theoretical results. Although real-world trials meet these criteria, we show that finding a model that does so is non-trivial; through simulations and theoretical arguments, we show that established opinion change models fail at least one of these criteria. We therefore propose an augmented opinion change model with a latent ‘open-mindedness’ variable, which sufficiently captures people’s propensity to change opinion. We show that data from the CA of Scotland indicates a latent variable both exists and resembles the concept of open-mindedness in the literature. We calibrate parameters against real CA data, demonstrating our model’s ecological validity, before running simulations across a range of realistic global parameters, with each simulation satisfying our criteria. Specifically, simulations meet criteria regardless of expert selection, expert ordering, participant extremism, and sub-optimal participant grouping, which has ramifications for optimised algorithmic approaches in the computational CA space.},
  archive      = {J_AAMAS},
  author       = {Barrett, Jake and Gal, Kobi and Michael, Loizos and Vilenchik, Dan},
  doi          = {10.1007/s10458-024-09655-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-41},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Beyond the echo chamber: Modelling open-mindedness in citizens’ assemblies},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On preferences and reward policies over rankings.
<em>AAMAS</em>, <em>38</em>(2), 1–36. (<a
href="https://doi.org/10.1007/s10458-024-09656-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the rational preferences of agents participating in a mechanism whose outcome is a ranking (i.e., a weak order) among participants. We propose a set of self-interest axioms corresponding to different ways for participants to compare rankings. These axioms vary from minimal conditions that most participants can be expected to agree on, to more demanding requirements that apply to specific scenarios. Then, we analyze the theories that can be obtained by combining the previous axioms and characterize their mutual relationships, revealing a rich hierarchical structure. After this broad investigation on preferences over rankings, we consider the case where the mechanism can distribute a fixed monetary reward to the participants in a fair way (that is, depending only on the anonymized output ranking). We show that such mechanisms can induce specific classes of preferences by suitably choosing the assigned rewards, even in the absence of tie breaking.},
  archive      = {J_AAMAS},
  author       = {Faella, Marco and Sauro, Luigi},
  doi          = {10.1007/s10458-024-09656-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {On preferences and reward policies over rankings},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating in a space of game views. <em>AAMAS</em>,
<em>38</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s10458-024-09660-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Game-theoretic modeling entails selecting the particular elements of a complex strategic situation deemed most salient for strategic analysis. Recognizing that any game model is one of many possible views of the situation, we term this a game view, and propose that sophisticated game reasoning would naturally consider multiple views. We introduce a conceptual framework, game view navigation, for game-theoretic reasoning through a process of constructing and analyzing a series of game views. The approach is illustrated using a variety of existing methods, which can be cast in terms of navigation patterns within this framework. By formally defining these as well as recently introduced ideas as navigating in a space of game views, we recognize common themes and opportunities for generalization. Game view navigation thus provides a unifying perspective that sheds light on connections between disparate reasoning methods, and defines a design space for creation of new techniques. We further apply the framework by defining and exploring new techniques based on modulating player aggregation in equilibrium search.},
  archive      = {J_AAMAS},
  author       = {Wellman, Michael P. and Mayo, Katherine},
  doi          = {10.1007/s10458-024-09660-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Navigating in a space of game views},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From large language models to small logic programs: Building
global explanations from disagreeing local post-hoc explainers.
<em>AAMAS</em>, <em>38</em>(2), 1–33. (<a
href="https://doi.org/10.1007/s10458-024-09663-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expressive power and effectiveness of large language models (LLMs) is going to increasingly push intelligent agents towards sub-symbolic models for natural language processing (NLP) tasks in human–agent interaction. However, LLMs are characterised by a performance vs. transparency trade-off that hinders their applicability to such sensitive scenarios. This is the main reason behind many approaches focusing on local post-hoc explanations, recently proposed by the XAI community in the NLP realm. However, to the best of our knowledge, a thorough comparison among available explainability techniques is currently missing, as well as approaches for constructing global post-hoc explanations leveraging the local information. This is why we propose a novel framework for comparing state-of-the-art local post-hoc explanation mechanisms and for extracting logic programs surrogating LLMs. Our experiments—over a wide variety of text classification tasks—show how most local post-hoc explainers are loosely correlated, highlighting substantial discrepancies in their results. By relying on the proposed novel framework, we also show how it is possible to extract faithful and efficient global explanations for the original LLM over multiple tasks, enabling explainable and resource-friendly AI techniques.},
  archive      = {J_AAMAS},
  author       = {Agiollo, Andrea and Siebert, Luciano Cavalcante and Murukannaiah, Pradeep K. and Omicini, Andrea},
  doi          = {10.1007/s10458-024-09663-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-33},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {From large language models to small logic programs: Building global explanations from disagreeing local post-hoc explainers},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An agent-based persuasion model using emotion-driven
concession and multi-objective optimization. <em>AAMAS</em>,
<em>38</em>(2), 1–44. (<a
href="https://doi.org/10.1007/s10458-024-09664-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute negotiation is essentially a multi-objective optimization (MOO) problem, where models of agent-based emotional persuasion (EP) can exhibit characteristics of anthropomorphism. This paper proposes a novel EP model by fusing the strategy of emotion-driven concession with the method of multi-objective optimization (EDC-MOO). Firstly, a comprehensive emotion model is designed to enhance the authenticity of the emotion. A novel concession strategy is then proposed to enable the concession to be dynamically tuned by the emotions of the agents. Finally, a new EP model is constructed by integrating emotion, historical transaction, persuasion behavior, and concession strategy under the framework of MOO. Comprehensive experiments on bilateral negotiation are conducted to illustrate and validate the effectiveness of EDC-MOO. These include an analysis of negotiations under five distinct persuasion styles, a comparison of EDC-MOO with a non-emotion-based MOO negotiation model and classic trade-off strategies, negotiations between emotion-driven and non-emotion-driven agents, and negotiations involving human participants. A detailed analysis of parameter sensitivity is also discussed. Experimental results show that the proposed EDC-MOO model can enhance the diversity of the negotiation process and the anthropomorphism of the bilateral agents, thereby improving the social welfare of both parties.},
  archive      = {J_AAMAS},
  author       = {Wang, Zhenwu and Shen, Jiayin and Tang, Xiaosong and Han, Mengjie and Feng, Zhenhua and Wu, Jinghua},
  doi          = {10.1007/s10458-024-09664-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-44},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {An agent-based persuasion model using emotion-driven concession and multi-objective optimization},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergent cooperation from mutual acknowledgment exchange in
multi-agent reinforcement learning. <em>AAMAS</em>, <em>38</em>(2),
1–36. (<a href="https://doi.org/10.1007/s10458-024-09666-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peer incentivization (PI) is a recent approach where all agents learn to reward or penalize each other in a distributed fashion, which often leads to emergent cooperation. Current PI mechanisms implicitly assume a flawless communication channel in order to exchange rewards. These rewards are directly incorporated into the learning process without any chance to respond with feedback. Furthermore, most PI approaches rely on global information, which limits scalability and applicability to real-world scenarios where only local information is accessible. In this paper, we propose Mutual Acknowledgment Token Exchange (MATE), a PI approach defined by a two-phase communication protocol to exchange acknowledgment tokens as incentives to shape individual rewards mutually. All agents condition their token transmissions on the locally estimated quality of their own situations based on environmental rewards and received tokens. MATE is completely decentralized and only requires local communication and information. We evaluate MATE in three social dilemma domains. Our results show that MATE is able to achieve and maintain significantly higher levels of cooperation than previous PI approaches. In addition, we evaluate the robustness of MATE in more realistic scenarios, where agents can deviate from the protocol and communication failures can occur. We also evaluate the sensitivity of MATE w.r.t. the choice of token values.},
  archive      = {J_AAMAS},
  author       = {Phan, Thomy and Sommer, Felix and Ritz, Fabian and Altmann, Philipp and Nüßlein, Jonas and Kölle, Michael and Belzner, Lenz and Linnhoff-Popien, Claudia},
  doi          = {10.1007/s10458-024-09666-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Emergent cooperation from mutual acknowledgment exchange in multi-agent reinforcement learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When is it acceptable to break the rules? Knowledge
representation of moral judgements based on empirical data.
<em>AAMAS</em>, <em>38</em>(2), 1–47. (<a
href="https://doi.org/10.1007/s10458-024-09667-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraining the actions of AI systems is one promising way to ensure that these systems behave in a way that is morally acceptable to humans. But constraints alone come with drawbacks as in many AI systems, they are not flexible. If these constraints are too rigid, they can preclude actions that are actually acceptable in certain, contextual situations. Humans, on the other hand, can often decide when a simple and seemingly inflexible rule should actually be overridden based on the context. In this paper, we empirically investigate the way humans make these contextual moral judgements, with the goal of building AI systems that understand when to follow and when to override constraints. We propose a novel and general preference-based graphical model that captures a modification of standard dual process theories of moral judgment. We then detail the design, implementation, and results of a study of human participants who judge whether it is acceptable to break a well-established rule: no cutting in line. We then develop an instance of our model and compare its performance to that of standard machine learning approaches on the task of predicting the behavior of human participants in the study, showing that our preference-based approach more accurately captures the judgments of human decision-makers. It also provides a flexible method to model the relationship between variables for moral decision-making tasks that can be generalized to other settings.},
  archive      = {J_AAMAS},
  author       = {Awad, Edmond and Levine, Sydney and Loreggia, Andrea and Mattei, Nicholas and Rahwan, Iyad and Rossi, Francesca and Talamadupula, Kartik and Tenenbaum, Joshua and Kleiman-Weiner, Max},
  doi          = {10.1007/s10458-024-09667-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {When is it acceptable to break the rules? knowledge representation of moral judgements based on empirical data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Team-wise effective communication in multi-agent
reinforcement learning. <em>AAMAS</em>, <em>38</em>(2), 1–26. (<a
href="https://doi.org/10.1007/s10458-024-09665-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective communication is crucial for the success of multi-agent systems, as it promotes collaboration for attaining joint objectives and enhances competitive efforts towards individual goals. In the context of multi-agent reinforcement learning, determining “whom”, “how” and “what” to communicate are crucial factors for developing effective policies. Therefore, we propose TeamComm, a novel framework for multi-agent communication reinforcement learning. First, it introduces a dynamic team reasoning policy, allowing agents to dynamically form teams and adapt their communication partners based on task requirements and environment states in cooperative or competitive scenarios. Second, TeamComm utilizes heterogeneous communication channels consisting of intra- and inter-team to achieve diverse information flow. Lastly, TeamComm leverages the information bottleneck principle to optimize communication content, guiding agents to convey relevant and valuable information. Through experimental evaluations on three popular environments with seven different scenarios, we empirically demonstrate the superior performance of TeamComm compared to existing methods.},
  archive      = {J_AAMAS},
  author       = {Yang, Ming and Zhao, Kaiyan and Wang, Yiming and Dong, Renzhi and Du, Yali and Liu, Furui and Zhou, Mingliang and U, Leong Hou},
  doi          = {10.1007/s10458-024-09665-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Team-wise effective communication in multi-agent reinforcement learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Envy-freeness in 3D hedonic games. <em>AAMAS</em>,
<em>38</em>(2), 1–53. (<a
href="https://doi.org/10.1007/s10458-024-09657-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of fairly partitioning a set of agents into coalitions based on the agents’ additively separable preferences, which can also be viewed as a hedonic game. We study three successively weaker solution concepts, related to envy, weakly justified envy, and justified envy. In a model in which coalitions may have any size, trivial solutions exist for these concepts, which provides a strong motivation for placing restrictions on coalition size. In this paper, we require feasible coalitions to have size three. We study the existence of partitions that are envy-free, weakly justified envy-free, and justified envy-free, and the computational complexity of finding such partitions, if they exist. We impose various restrictions on the agents’ preferences and present a complete complexity classification in terms of these restrictions.},
  archive      = {J_AAMAS},
  author       = {McKay, Michael and Cseh, Ágnes and Manlove, David},
  doi          = {10.1007/s10458-024-09657-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-53},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Envy-freeness in 3D hedonic games},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiently reconfiguring a connected swarm of labeled
robots. <em>AAMAS</em>, <em>38</em>(2), 1–34. (<a
href="https://doi.org/10.1007/s10458-024-09668-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering motion planning for a swarm of n labeled robots, we need to rearrange a given start configuration into a desired target configuration via a sequence of parallel, collision-free moves. The objective is to reach the new configuration in a minimum amount of time. Problems of this type have been considered before, with recent notable results achieving constant stretch for parallel reconfiguration: If mapping the start configuration to the target configuration requires a maximum Manhattan distance of d, the total duration of an overall schedule can be bounded to $$\mathcal {O}(d)$$ , which is optimal up to constant factors. An important constraint for coordinated reconfiguration is to keep the swarm connected after each time step. In previous work, constant stretch could only be achieved if disconnected reconfiguration is allowed, or for scaled configurations of unlabeled robots; on the other hand, the existence of non-constant lower bounds on the stretch factor was unknown. We resolve these major open problems by (1) establishing a lower bound of $$\Omega (\sqrt{n})$$ for connected, labeled reconfiguration and, most importantly, by (2) proving that for scaled arrangements, constant stretch for connected, labeled reconfiguration can be achieved. In addition, we show that (3) it is NP-complete to decide whether a makespan of 2 can be achieved, while it is possible to check in polynomial time whether a schedule of makespan 1 exists.},
  archive      = {J_AAMAS},
  author       = {Fekete, Sándor P. and Kramer, Peter and Rieck, Christian and Scheffer, Christian and Schmidt, Arne},
  doi          = {10.1007/s10458-024-09668-3},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-34},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Efficiently reconfiguring a connected swarm of labeled robots},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Carbon trading supply chain management based on constrained
deep reinforcement learning. <em>AAMAS</em>, <em>38</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10458-024-09669-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of carbon emissions is a critical global concern, and how to effectively reduce energy consumption and emissions is a challenge faced by the industrial sector, which is highly emphasized in supply chain management. The complexity arises from the intricate coupling mechanism between carbon trading and ordering. T he large-scale state space involved and various constraints make cost optimization difficult. Carbon quota constraints and sequential decision-making exacerbate the challenges for businesses. Existing research implements rule-based and heuristic numerical simulation, which struggles to adapt to time-varying environments. We develop a unified framework from the perspective of  Constrained Markov Decision Processes (CMDP). Constrained Deep Reinforcement Learning (DRL) with its  powerful high-dimensional representations of neural networks and effective decision-making capabilities under constraints, provides a potential solution for supply chain management that includes carbon trading. DRL with constraints is a crucial tool to study cost optimization for enterprises. This paper constructs a DRL algorithm for Double Order based on PPO-Lagrangian (DOPPOL),  aimed at addressing a supply chain management model that integrates carbon trading decisions and ordering decisions. The results indicate that businesses can optimize both business and carbon costs, thereby increasing overall profits, as well as adapt to various demand uncertainties. DOPPOL outperforms the traditional method (s, S) in fluctuating demand scenarios. By introducing carbon trading, enterprises are able to  adjust supply chain orders and carbon emissions through interaction, and improve operational efficiency. Finally, we emphasize the significant role of carbon pricing in enterprise contracts in terms of profitability, as reasonable prices can help control carbon emissions and reduce costs. Our research is of great importance in achieving climate change control, as well as promoting sustainability.},
  archive      = {J_AAMAS},
  author       = {Wang, Qinghao and Yang, Yaodong},
  doi          = {10.1007/s10458-024-09669-2},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Carbon trading supply chain management based on constrained deep reinforcement learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-sided matching markets with endowments: Equilibria and
algorithms. <em>AAMAS</em>, <em>38</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s10458-024-09670-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Arrow–Debreu extension of the classic Hylland–Zeckhauser scheme (Hylland and Zeckhauser in J Polit Econ 87(2):293–314, 1979) for a one-sided matching market—called ADHZ in this paper—has natural applications but has instances which do not admit equilibria. By introducing approximation, we define the $$\epsilon$$ -approximate ADHZ model and give the following results. 1. Existence of equilibrium under linear utility functions. We prove that the equilibrium allocation satisfies Pareto optimality, approximate envy-freeness, and approximate weak core stability. 2. A combinatorial polynomial time algorithm for an $$\epsilon$$ -approximate ADHZ equilibrium for the case of dichotomous, and more generally bi-valued, utilities. 3. An instance of ADHZ, with dichotomous utilities and a strongly connected demand graph, which does not admit an equilibrium. 4. A rational convex program for HZ under dichotomous utilities; a combinatorial polynomial time algorithm for this case was given in Vazirani and Yannakakis (in: Innovations in theoretical computer science, pp 59–15919, 2021). The $$\epsilon$$ -approximate ADHZ model fills a void in the space of general mechanisms for one-sided matching markets; see details in the paper.},
  archive      = {J_AAMAS},
  author       = {Garg, Jugal and Tröbst, Thorben and Vazirani, Vijay},
  doi          = {10.1007/s10458-024-09670-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {One-sided matching markets with endowments: Equilibria and algorithms},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truthful interval covering. <em>AAMAS</em>, <em>38</em>(2),
1–25. (<a href="https://doi.org/10.1007/s10458-024-09673-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of a novel problem in mechanism design without money, which we term Truthful Interval Covering (TIC). An instance of TIC consists of a set of agents each associated with an individual interval on a line, and the objective is to decide where to place a covering interval to minimize the total social or egalitarian cost of the agents, which is determined by the intersection of this interval with their individual ones. This fundamental problem can model situations of provisioning a public good, such as the use of power generators to prevent or mitigate load shedding in developing countries. In the strategic version of the problem, the agents wish to minimize their individual costs, and might misreport the position and/or length of their intervals to achieve that. Our goal is to design truthful mechanisms to prevent such strategic misreports and achieve good approximations to the best possible social or egalitarian cost. We consider the fundamental setting of known intervals with equal lengths and provide tight bounds on the approximation ratios achieved by truthful deterministic mechanisms. For the social cost, we also design a randomized truthful mechanism that outperforms all possible deterministic ones. Finally, we highlight a plethora of natural extensions of our model for future work, as well as some natural limitations of those settings.},
  archive      = {J_AAMAS},
  author       = {Deligkas, Argyrios and Filos-Ratsikas, Aris and Voudouris, Alexandros A.},
  doi          = {10.1007/s10458-024-09673-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Truthful interval covering},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical house allocation with identical valuations.
<em>AAMAS</em>, <em>38</em>(2), 1–47. (<a
href="https://doi.org/10.1007/s10458-024-09672-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical house allocation problem involves assigning n houses (or items) to n agents according to their preferences. A key criterion in such problems is satisfying some fairness constraints such as envy-freeness. We consider a generalization of this problem, called Graphical House Allocation, wherein the agents are placed along the vertices of a graph (corresponding to a social network), and each agent can only experience envy towards its neighbors. Our goal is to minimize the aggregate envy among the agents as a natural fairness objective, i.e., the sum of the envy value over all edges in a social graph. We focus on graphical house allocation with identical valuations. When agents have identical and evenly-spaced valuations, our problem reduces to the well-studied Minimum Linear Arrangement. For identical valuations with possibly uneven spacing, we show a number of deep and surprising ways in which our setting is a departure from this classical problem. More broadly, we contribute several structural and computational results for various classes of graphs, including NP-hardness results for disjoint unions of paths, cycles, stars, cliques, and complete bipartite graphs; we also obtain fixed-parameter tractable (and, in some cases, polynomial-time) algorithms for paths, cycles, stars, cliques, complete bipartite graphs, and their disjoint unions. Additionally, a conceptual contribution of our work is the formulation of a structural property for disconnected graphs that we call splittability, which results in efficient parameterized algorithms for finding optimal allocations.},
  archive      = {J_AAMAS},
  author       = {Hosseini, Hadi and McGregor, Andrew and Payan, Justin and Sengupta, Rik and Vaish, Rohit and Viswanathan, Vignesh},
  doi          = {10.1007/s10458-024-09672-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Graphical house allocation with identical valuations},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate algorithmics for eliminating envy by donating
goods. <em>AAMAS</em>, <em>38</em>(2), 1–35. (<a
href="https://doi.org/10.1007/s10458-024-09674-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairly dividing a set of indivisible resources to a set of agents is of utmost importance in some applications. However, after an allocation has been implemented the preferences of agents might change and envy might arise. We study the following problem to cope with such situations: given an allocation of indivisible resources to agents with additive utility-based preferences, is it possible to socially donate some of the resources (which means removing these resources from the allocation instance) such that the resulting modified allocation is envy-free (up to one good). We require that the number of deleted resources and/or the caused utilitarian welfare loss of the allocation are bounded. We conduct a thorough study of the (parameterized) computational complexity of this problem considering various natural and problem-specific parameters (e.g., the number of agents, the number of deleted resources, or the maximum number of resources assigned to an agent in the initial allocation) and different preference models, including unary-encoded and 0/1-valuations. In our studies, we obtain a rich set of (parameterized) tractability and intractability results and discover several surprising contrasts, for instance, between the two closely related fairness concepts envy-freeness and envy-freeness up to one good and between the influence of the parameters maximum number and welfare of the deleted resources.},
  archive      = {J_AAMAS},
  author       = {Boehmer, Niclas and Bredereck, Robert and Heeger, Klaus and Knop, Dušan and Luo, Junjie},
  doi          = {10.1007/s10458-024-09674-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-35},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Multivariate algorithmics for eliminating envy by donating goods},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic manipulation of preferences in the rank
minimization mechanism. <em>AAMAS</em>, <em>38</em>(2), 1–28. (<a
href="https://doi.org/10.1007/s10458-024-09676-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider one-sided matching problems, where agents are allocated items based on stated preferences. Posing this as an assignment problem, the average rank of obtained matchings can be minimized using the rank minimization (RM) mechanism. RM matchings can have significantly better rank distributions than matchings obtained by mechanisms with random priority, such as Random Serial Dictatorship. However, these matchings are sensitive to preference manipulation from strategic agents. In this work we consider a scenario where agents aim to be matched to their top-n preferred items using the RM mechanism, and strategically manipulate their preferences to achieve this. We derive a best response strategy for an agent to be assigned to their n most preferred items using the Hungarian algorithm, under a simplified cost function. This strategy is then extended to a first-order heuristic strategy for being matched to the top-n items in a setup that minimizes the average rank. Based on this finding, an empirical study is conducted examining the impact of the first-order heuristic strategy. The study utilizes data from both simulated markets and real-world matching markets in Amsterdam, taking into account variations in item popularity, fractions of strategic agents, and the preferences for the n most favored items. For most scenarios, RM yields more rank efficient matches than Random Serial Dictatorship, even when agents apply the first-order heuristic strategy. However, although highly market dependent, the matching performance can become worse when 50% of agents or more want to be matched to their top-1 or top-2 preferred items and apply the first-order heuristic strategy to achieve this.},
  archive      = {J_AAMAS},
  author       = {Tasnim, Mayesha and Weesie, Youri and Ghebreab, Sennay and Baak, Max},
  doi          = {10.1007/s10458-024-09676-3},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Strategic manipulation of preferences in the rank minimization mechanism},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalised electric vehicle charging stop planning through
online estimators. <em>AAMAS</em>, <em>38</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10458-024-09671-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of finding charging stops while travelling in electric vehicles (EVs) using artificial intelligence (AI). Choosing a charging station is challenging, because drivers have very heterogeneous preferences in terms of how they trade off the features of various alternatives (for example, regarding the time spent driving, charging costs, waiting times at charging stations, and the facilities provided at the charging stations). The key problem here is eliciting the diverse preferences of drivers, assuming that these preferences are typically not fully known a priori, and then planning stops based on each driver’s preferences. Our approach to solving this problem is to develop an intelligent personal agent that learns preferences gradually over multiple interactions. This study proposes a new technique that utilises a small-scale discrete choice experiment as a method of interacting with the driver in order to minimise the cognitive burden on the driver. Using this method, drivers are presented with a variety of routes with possible combinations of charging stops depending on the agent’s latest belief about their preferences. In subsequent iterations, the personal agent will continue to learn and refine its belief about the driver’s preferences, suggesting more personalised routes that are closer to the driver’s preferences. Based on real preference data from EV drivers, we evaluate our novel algorithm and show that, after only a few queries, our method quickly converges to the optimal routes for EV drivers [This paper is an extended version of an ECAI workshop short paper (Shafipour Yourdshahi et al., in: ECAI 2023 workshops, Kraków, Poland, 2023)].},
  archive      = {J_AAMAS},
  author       = {Shafipour, Elnaz and Stein, Sebastian and Ahipasaoglu, Selin},
  doi          = {10.1007/s10458-024-09671-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Personalised electric vehicle charging stop planning through online estimators},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theoretical properties of the MiCRO negotiation strategy.
<em>AAMAS</em>, <em>38</em>(2), 1–43. (<a
href="https://doi.org/10.1007/s10458-024-09678-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have introduced a new algorithm for automated negotiation, called MiCRO, which, despite its simplicity, outperforms many state-of-the-art negotiation strategies (de Jonge, in: Raedt (ed) Proceedings of the thirty-first international joint conference on artificial intelligence, ijcai.org, Vienna, Austria, 2022). Furthermore, we claimed that under certain conditions which typically hold in the Automated Negotiating Agents Competition (ANAC), it is a game-theoretically optimal strategy. The goal of this paper is to formally prove those claims. Specifically, we define ‘negotiation’ as an extensive-form game and define the class of consistent strategies for this game, which consists of those strategies that satisfy a number of rationality criteria. We then prove that under the above mentioned conditions MiCRO is a best response against itself among all consistent negotiation strategies. Furthermore, we define the notion of a balanced negotiation domain, which is a domain in which two MiCRO agents would always come to an optimal agreement. Finally, we show that many of the domains used in ANAC indeed happen to be (approximately) balanced. The importance of this work is that if we know under which conditions MiCRO is theoretically optimal, then we can use this to test to what extent other negotiation algorithms are able to achieve similar results to MiCRO when applied under those same conditions. Furthermore, it would help researchers to design more challenging test cases for automated negotiation in which MiCRO is not optimal.},
  archive      = {J_AAMAS},
  author       = {Jonge, Dave de},
  doi          = {10.1007/s10458-024-09678-1},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-43},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Theoretical properties of the MiCRO negotiation strategy},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The complexity of verifying popularity and strict popularity
in altruistic hedonic games. <em>AAMAS</em>, <em>38</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10458-024-09679-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider average- and min-based altruistic hedonic games and study the problem of verifying popular and strictly popular coalition structures. While strict popularity verification has been shown to be coNP-complete in min-based altruistic hedonic games, this problem has been open for equal- and altruistic-treatment average-based altruistic hedonic games. We solve these two open cases of strict popularity verification and then provide the first complexity results for popularity verification in (average- and min-based) altruistic hedonic games, where we cover all three degrees of altruism.},
  archive      = {J_AAMAS},
  author       = {Kerkmann, Anna Maria and Rothe, Jörg},
  doi          = {10.1007/s10458-024-09679-0},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {The complexity of verifying popularity and strict popularity in altruistic hedonic games},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GenSynthPop: Generating a spatially explicit synthetic
population of individuals and households from aggregated data.
<em>AAMAS</em>, <em>38</em>(2), 1–28. (<a
href="https://doi.org/10.1007/s10458-024-09680-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic populations are representations of actual individuals living in a specific area. They play an increasingly important role in studying and modeling individuals and are often used to build agent-based social simulations. Traditional approaches for synthesizing populations use a detailed sample of the population (which may not be available) or combine data into a single joint distribution, and draw individuals or households from these. The latter group of existing sample-free methods fail to integrate (1) the best available data on spatial granular distributions, (2) multi-variable joint distributions, and (3) household level distributions. In this paper, we propose a sample-free approach where synthetic individuals and households directly represent the estimated joint distribution to which attributes are iteratively added, conditioned on previous attributes such that the relative frequencies within each joint group of attributes are maintained and fit granular spatial marginal distributions. In this paper we present our method and test it for the Zuid-West district of The Hague, the Netherlands, showing that spatial, multi-variable and household distributions are accurately reflected in the resulting synthetic population.},
  archive      = {J_AAMAS},
  author       = {de Mooij, Jan and Sonnenschein, Tabea and Pellegrino, Marco and Dastani, Mehdi and Ettema, Dick and Logan, Brian and Verstegen, Judith A.},
  doi          = {10.1007/s10458-024-09680-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {GenSynthPop: Generating a spatially explicit synthetic population of individuals and households from aggregated data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resolving social dilemmas with minimal reward transfer.
<em>AAMAS</em>, <em>38</em>(2), 1–33. (<a
href="https://doi.org/10.1007/s10458-024-09675-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social dilemmas present a significant challenge in multi-agent cooperation because individuals are incentivised to behave in ways that undermine socially optimal outcomes. Consequently, self-interested agents often avoid collective behaviour. In response, we formalise social dilemmas and introduce a novel metric, the general self-interest level, to quantify the disparity between individual and group rationality in such scenarios. This metric represents the maximum proportion of their individual rewards that agents can retain while ensuring that a social welfare optimum becomes a dominant strategy. Our approach diverges from traditional concepts of altruism, instead focusing on strategic reward redistribution. By transferring rewards among agents in a manner that aligns individual and group incentives, rational agents will maximise collective welfare while pursuing their own interests. We provide an algorithm to compute efficient transfer structures for an arbitrary number of agents, and introduce novel multi-player social dilemma games to illustrate the effectiveness of our method. This work provides both a descriptive tool for analysing social dilemmas and a prescriptive solution for resolving them via efficient reward transfer contracts. Applications include mechanism design, where we can assess the impact on collaborative behaviour of modifications to models of environments.},
  archive      = {J_AAMAS},
  author       = {Willis, Richard and Du, Yali and Leibo, Joel Z. and Luck, Michael},
  doi          = {10.1007/s10458-024-09675-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-33},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Resolving social dilemmas with minimal reward transfer},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding middle grounds for incoherent horn expressions: The
moral machine case. <em>AAMAS</em>, <em>38</em>(2), 1–39. (<a
href="https://doi.org/10.1007/s10458-024-09681-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart devices that operate in a shared environment with people need to be aligned with their values and requirements. We study the problem of multiple stakeholders informing the same device on what the right thing to do is. Specifically, we focus on how to reach a middle ground among the stakeholders inevitably incoherent judgments on what the rules of conduct for the device should be. We formally define a notion of middle ground and discuss the main properties of this notion. Then, we identify three sufficient conditions on the class of Horn expressions for which middle grounds are guaranteed to exist. We provide a polynomial time algorithm that computes middle grounds, under these conditions. We also show that if any of the three conditions is removed then middle grounds for the resulting (larger) class may not exist. Finally, we implement our algorithm and perform experiments using data from the Moral Machine Experiment. We present conflicting rules for different countries and how the algorithm finds the middle ground in this case.},
  archive      = {J_AAMAS},
  author       = {Ozaki, Ana and Rehman, Anum and Slavkovik, Marija},
  doi          = {10.1007/s10458-024-09681-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Finding middle grounds for incoherent horn expressions: The moral machine case},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formal contracts mitigate social dilemmas in multi-agent
reinforcement learning. <em>AAMAS</em>, <em>38</em>(2), 1–38. (<a
href="https://doi.org/10.1007/s10458-024-09682-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent Reinforcement Learning (MARL) is a powerful tool for training autonomous agents acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all Fully Observable Markov Games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we show that for general contract spaces, and even under partial observability, richer contract spaces lead to higher welfare. Hence, contract space design solves an exploration-exploitation tradeoff, sidestepping incentive issues. We complement our theoretical analysis with experiments. Issues of exploration in the contracting augmentation are mitigated using a training methodology inspired by multi-objective reinforcement learning: Multi-Objective Contract Augmentation Learning. We test our methodology in static, single-move games, as well as dynamic domains that simulate traffic, pollution management, and common pool resource management.},
  archive      = {J_AAMAS},
  author       = {Haupt, Andreas and Christoffersen, Phillip and Damani, Mehul and Hadfield-Menell, Dylan},
  doi          = {10.1007/s10458-024-09682-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-38},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Formal contracts mitigate social dilemmas in multi-agent reinforcement learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing balanced solutions for large international kidney
exchange schemes. <em>AAMAS</em>, <em>38</em>(1), 1–41. (<a
href="https://doi.org/10.1007/s10458-024-09645-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome incompatibility issues, kidney patients may swap their donors. In international kidney exchange programmes (IKEPs), countries merge their national patient–donor pools. We consider a recently introduced credit system. In each round, countries are given an initial “fair” allocation of the total number of kidney transplants. This allocation is adjusted by a credit function yielding a target allocation. The goal is to find a solution that approaches the target allocation as closely as possible, to ensure long-term stability of the international pool. As solutions, we use maximum matchings that lexicographically minimize the country deviations from the target allocation. We perform, for the first time, a computational study for a large number of countries. For the initial allocations we use two easy-to-compute solution concepts, the benefit value and the contribution value, and four classical but hard-to-compute concepts, the Shapley value, nucleolus, Banzhaf value and tau value. By using state-of-the-art software we show that the latter four concepts are now within reach for IKEPs of up to fifteen countries. Our experiments show that using lexicographically minimal maximum matchings instead of ones that only minimize the largest deviation from the target allocation (as previously done) may make an IKEP up to 54% more balanced.},
  archive      = {J_AAMAS},
  author       = {Benedek, Márton and Biró, Péter and Paulusma, Daniel and Ye, Xin},
  doi          = {10.1007/s10458-024-09645-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-41},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Computing balanced solutions for large international kidney exchange schemes},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards interactive explanation-based nutrition virtual
coaching systems. <em>AAMAS</em>, <em>38</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10458-023-09634-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The awareness about healthy lifestyles is increasing, opening to personalized intelligent health coaching applications. A demand for more than mere suggestions and mechanistic interactions has driven attention to nutrition virtual coaching systems (NVC) as a bridge between human–machine interaction and recommender, informative, persuasive, and argumentation systems. NVC can rely on data-driven opaque mechanisms. Therefore, it is crucial to enable NVC to explain their doing (i.e., engaging the user in discussions (via arguments) about dietary solutions/alternatives). By doing so, transparency, user acceptance, and engagement are expected to be boosted. This study focuses on NVC agents generating personalized food recommendations based on user-specific factors such as allergies, eating habits, lifestyles, and ingredient preferences. In particular, we propose a user-agent negotiation process entailing run-time feedback mechanisms to react to both recommendations and related explanations. Lastly, the study presents the findings obtained by the experiments conducted with multi-background participants to evaluate the acceptability and effectiveness of the proposed system. The results indicate that most participants value the opportunity to provide feedback and receive explanations for recommendations. Additionally, the users are fond of receiving information tailored to their needs. Furthermore, our interactive recommendation system performed better than the corresponding traditional recommendation system in terms of effectiveness regarding the number of agreements and rounds.},
  archive      = {J_AAMAS},
  author       = {Buzcu, Berk and Tessa, Melissa and Tchappi, Igor and Najjar, Amro and Hulstijn, Joris and Calvaresi, Davide and Aydoğan, Reyhan},
  doi          = {10.1007/s10458-023-09634-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Towards interactive explanation-based nutrition virtual coaching systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline policy reuse-guided anytime online collective
multiagent planning and its application to mobility-on-demand systems.
<em>AAMAS</em>, <em>38</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10458-024-09650-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of mobility-on-demand (MoD) systems boosts online collective multiagent planning (Online_CMP), where spatially distributed servicing agents are planned to meet dynamically arriving demands. For city-scale MoDs with a fleet of agents, Online_CMP methods must make a tradeoff between computation time (i.e., real-time) and solution quality (i.e., the number of demands served). Directly using an offline policy can guarantee real-time, but cannot be dynamically adjusted to real agent and demand distributions. Search-based online planning methods are adaptive, but are computationally expensive and cannot scale up. In this paper, we propose a principled Online_CMP method, which reuses and improves the offline policy in an anytime manner. We first model MoDs as a collective Markov Decision Process ( $${\mathbb {C}}$$ -MDP) where the collective behavior of agents affects the joint reward. Given the $${\mathbb {C}}$$ -MDP model, we propose a novel state value function to evaluate the policy, and a gradient ascent (GA) technique to improve the policy. We further show that offline GA-based policy iteration (GA-PI) can converge to global optima of $${\mathbb {C}}$$ -MDP under certain conditions. Finally, with real-time information, the offline policy is used as the default plan, GA-PI is used to improve it and generate an online plan. Experimental results show that our offline policy reuse-guided Online_CMP method significantly outperforms standard online multiagent planning methods on MoD systems like ride-sharing and security traffic patrolling in terms of computation time and solution quality.},
  archive      = {J_AAMAS},
  author       = {Wang, Wanyuan and Che, Qian and Zhou, Yifeng and Wu, Weiwei and An, Bo and Jiang, Yichuan},
  doi          = {10.1007/s10458-024-09650-z},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Offline policy reuse-guided anytime online collective multiagent planning and its application to mobility-on-demand systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentially private multi-agent constraint optimization.
<em>AAMAS</em>, <em>38</em>(1), 1–39. (<a
href="https://doi.org/10.1007/s10458-024-09636-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed constraint optimization (DCOP) is a framework in which multiple agents with private constraints (or preferences) cooperate to achieve a common goal optimally. DCOPs are applicable in several multi-agent coordination/allocation problems, such as vehicle routing, radio frequency assignments, and distributed scheduling of meetings. However, optimization scenarios may involve multiple agents wanting to protect their preferences’ privacy. Researchers propose privacy-preserving algorithms for DCOPs that provide improved privacy protection through cryptographic primitives such as partial homomorphic encryption, secret-sharing, and secure multiparty computation. These privacy benefits come at the expense of high computational complexity. Moreover, such an approach does not constitute a rigorous privacy guarantee for optimization outcomes, as the result of the computation may compromise agents’ preferences. In this work, we show how to achieve privacy, specifically Differential Privacy, by randomizing the solving process. In particular, we present P-Gibbs, which adapts the current state-of-the-art algorithm for DCOPs, namely SD-Gibbs, to obtain differential privacy guarantees with much higher computational efficiency. Experiments on benchmark problems such as Ising, graph-coloring, and meeting-scheduling show P-Gibbs’ privacy and performance trade-off for varying privacy budgets and the SD-Gibbs algorithm. More concretely, we empirically show that P-Gibbs provides fair solutions for competitive privacy budgets.},
  archive      = {J_AAMAS},
  author       = {Damle, Sankarshan and Triastcyn, Aleksei and Faltings, Boi and Gujar, Sujit},
  doi          = {10.1007/s10458-024-09636-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Differentially private multi-agent constraint optimization},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward fast belief propagation for distributed constraint
optimization problems via heuristic search. <em>AAMAS</em>,
<em>38</em>(1), 1–39. (<a
href="https://doi.org/10.1007/s10458-024-09643-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belief propagation (BP) approaches, such as Max-sum and its variants, are important methods to solve large-scale Distributed Constraint Optimization Problems. However, these algorithms face a huge challenge since their computational complexity scales exponentially with the arity of each constraint function. Current accelerating techniques for BP use sorting or branch-and-bound (BnB) strategy to reduce the search space. However, the existing BnB-based methods are mainly designed for specific problems, which limits their applicability. On the other hand, though several generic sorting-based methods have been proposed, they require significantly high preprocessing as well as memory overhead, which prohibits their adoption in some realistic scenarios. In this paper, we aim to propose a series of generic and memory-efficient heuristic search techniques to accelerate belief propagation. Specifically, by leveraging dynamic programming, we efficiently build function estimations for every partial assignment scoped in a constraint function in the preprocessing phase. Then, by using these estimations to build upper bounds and employing a branch-and-bound in a depth-first fashion to reduce the search space, we propose our first method called FDSP. Next, we enhance FDSP by adapting a concurrent-search strategy and leveraging the upper bounds as guiding information and propose its first heuristic variant framework called CONC-FDSP. Finally, by choosing to expand the partial assignment with the highest upper bound in each step of exploration, we propose the second heuristic variant of FDSP, called BFS-FDSP. We prove the correctness of our methods theoretically, and our empirical evaluations indicate their superiority for accelerating Max-sum in terms of both time and memory, compared with the state-of-the-art.},
  archive      = {J_AAMAS},
  author       = {Gao, Junsong and Chen, Ziyu and Chen, Dingding and Zhang, Wenxin and Li, Qiang},
  doi          = {10.1007/s10458-024-09643-y},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Toward fast belief propagation for distributed constraint optimization problems via heuristic search},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logic-based cognitive planning for conversational agents.
<em>AAMAS</em>, <em>38</em>(1), 1–52. (<a
href="https://doi.org/10.1007/s10458-024-09646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to cognitive planning based on an NP-complete logic of explicit and implicit belief whose satisfiability checking problem is reduced to SAT. We illustrate the potential for application of our model by formalizing and then implementing a human–machine interaction scenario in which an artificial agent interacts with a human agent through dialogue and tries to motivate her to practice a sport. To make persuasion effective, the artificial agent needs a model of the human’s beliefs and desires which is built during interaction through a sequence of belief revision operations. We consider two cognitive planning algorithms and compare their performances, a brute force algorithm based on SAT and a QBF-based algorithm.},
  archive      = {J_AAMAS},
  author       = {Fernandez Davila, Jorge Luis and Longin, Dominique and Lorini, Emiliano and Maris, Frédéric},
  doi          = {10.1007/s10458-024-09646-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-52},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Logic-based cognitive planning for conversational agents},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for trust-related knowledge transfer in
human–robot interaction. <em>AAMAS</em>, <em>38</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10458-024-09653-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trustworthy human–robot interaction (HRI) during activities of daily living (ADL) presents an interesting and challenging domain for assistive robots, particularly since methods for estimating the trust level of a human participant towards the assistive robot are still in their infancy. Trust is a multifaced concept which is affected by the interactions between the robot and the human, and depends, among other factors, on the history of the robot’s functionality, the task and the environmental state. In this paper, we are concerned with the challenge of trust transfer, i.e. whether experiences from interactions on a previous collaborative task can be taken into consideration in the trust level inference for a new collaborative task. This has the potential of avoiding re-computing trust levels from scratch for every new situation. The key challenge here is to automatically evaluate the similarity between the original and the novel situation, then adapt the robot’s behaviour to the novel situation using previous experience with various objects and tasks. To achieve this, we measure the semantic similarity between concepts in knowledge graphs (KGs) and adapt the robot’s actions towards a specific user based on personalised interaction histories. These actions are grounded and then verified before execution using a geometric motion planner to generate feasible trajectories in novel situations. This framework has been experimentally tested in human–robot handover tasks in different kitchen scene contexts. We conclude that trust-related knowledge positively influences and improves collaboration in both performance and time aspects.},
  archive      = {J_AAMAS},
  author       = {Diab, Mohammed and Demiris, Yiannis},
  doi          = {10.1007/s10458-024-09653-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A framework for trust-related knowledge transfer in human–robot interaction},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling school segregation with transportation network
interventions: An agent-based modelling approach. <em>AAMAS</em>,
<em>38</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10458-024-09652-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the emerging challenge of school segregation within the context of free school choice systems. Households take into account both proximity and demographic composition when deciding on which schools to send their children to, potentially exacerbating residential segregation. This raises an important question: can we strategically intervene in transportation networks to enhance school access and mitigate segregation? In this paper, we propose a novel, network agent-based model to explore this question. Through simulations in both synthetic and real-world networks, we demonstrate that enhancing school accessibility via transportation network interventions can lead to a reduction in school segregation, under specific conditions. We introduce group-based network centrality measures and show that increasing the centrality of certain neighborhood nodes with respect to a transportation network can be an effective strategy for strategic interventions. We conduct experiments in two synthetic network environments, as well as in an environment based on real-world data from Amsterdam, the Netherlands. In both cases, we simulate a population of representative agents emulating real citizens’ schooling preferences, and we assume that agents belong to two different groups (e.g., based on migration background). We show that, under specific homophily regimes in the population, school segregation can be reduced by up to 35%. Our proposed framework provides the foundation to explore how citizens’ preferences, school capacity, and public transportation can shape patterns of urban segregation.},
  archive      = {J_AAMAS},
  author       = {Michailidis, Dimitris and Tasnim, Mayesha and Ghebreab, Sennay and Santos, Fernando P.},
  doi          = {10.1007/s10458-024-09652-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Tackling school segregation with transportation network interventions: An agent-based modelling approach},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of research on several problems in the RoboCup3D
simulation environment. <em>AAMAS</em>, <em>38</em>(1), 1–61. (<a
href="https://doi.org/10.1007/s10458-024-09642-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of robot research and development, due to the vulnerability of hardware, simulation environment is often used to verify and test algorithms first. RoboCup3D simulation environment is developed based on open dynamic engine, and the humanoid robot NAO is modeled as the main robot, which provides a simulation platform for humanoid robot researchers to study robot movements. At the same time, it is also the official platform of RoboCup 3D events. Under the rules of soccer robot competition, it is helpful for the research of multi-robots, especially multi-humanoid robots’ cooperation strategy. This paper summarizes the related research in RoboCup3D simulation environment, and first introduces the basic problems existing in this simulation environment. Secondly, the research of robot motion generation and optimization based on model and non-model in simulation environment is introduced respectively. Then, it introduces the related research of cooperation strategy design of multi-humanoid robots under RoboCup3D rules, including positioning, dynamic role assignment, etc. And sort out a typical practical solution to the above problems; Finally, the future development trend of related research in RoboCup3D simulation environment is analyzed.},
  archive      = {J_AAMAS},
  author       = {Gao, Zhongye and Yi, Mengjun and Jin, Ying and Zhang, Hanwen and Hao, Yun and Yin, Ming and Cai, Ziwen and Shen, Furao},
  doi          = {10.1007/s10458-024-09642-z},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A survey of research on several problems in the RoboCup3D simulation environment},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanism design for public projects via three machine
learning based approaches. <em>AAMAS</em>, <em>38</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10458-024-09647-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study mechanism design for nonexcludable and excludable binary public project problems. Our aim is to maximize the expected number of consumers and the expected agents’ welfare. We first show that for the nonexcludable public project model, there is no need for machine learning based mechanism design. We identify a sufficient condition on the prior distribution for the existing conservative equal costs mechanism to be the optimal strategy-proof and individually rational mechanism. For general distributions, we propose a dynamic program that solves for the optimal mechanism. For the excludable public project model, we identify a similar sufficient condition for the existing serial cost sharing mechanism to be optimal for 2 and 3 agents. We derive a numerical upper bound and use it to show that for several common distributions, the serial cost sharing mechanism is close to optimality. The serial cost sharing mechanism is not optimal in general. We propose three machine learning based approaches for designing better performing mechanisms. We focus on the family of largest unanimous mechanisms, which characterizes all strategy-proof and individually rational mechanisms for the excludable public project model. A largest unanimous mechanism describes an iterative mechanism, which is defined by an exponential number of mechanism parameters. Our first approach describes the largest unanimous mechanism family using a neural network and training is carried out by minimizing a cost function that combines the mechanism design objective and the constraint violation penalty. We interpret the largest unanimous mechanisms as price-oriented rationing-free (PORF) mechanisms, which enables us to move the mechanisms’ iterative decision making off the neural network, to a separate simulation process, therefore avoiding the vanishing gradient problem. We also feed the prior distribution’s analytical form into the cost function to achieve high-quality gradients for efficient training. Our second approach treats the mechanism design task as a Markov Decision Process with an exponential number of states. During the Markov decision process, the non-consumers are gradually removed from the system. We train multiple neural networks, each for a different number of remaining agents, to learn the optimal value function on the states. Training is carried out by supervised learning toward a set of manually prepared base cases and the Bellman equation. Our third approach is based on reinforcement learning for a Partially Observable Markov Decision Process. Each RL episode randomly draws a type profile, which is hidden from the RL agent (mechanism designer). The RL agent only observes which cost share offers have been accepted under the largest unanimous mechanism under discussion. We use a continuous action space reinforcement learning approach to adjust the offer policy (i.e., adjust mechanism parameters). Lastly, our first two approaches use “supervision to manual mechanisms” as a systematic way for network initialization, which is potentially valuable for machine learning based mechanism design in general.},
  archive      = {J_AAMAS},
  author       = {Guo, Mingyu and Goel, Diksha and Wang, Guanhua and Guo, Runqi and Sakurai, Yuko and Babar, Muhammad Ali},
  doi          = {10.1007/s10458-024-09647-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Mechanism design for public projects via three machine learning based approaches},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-agent path finding framework and
strategies based on automated negotiation. <em>AAMAS</em>,
<em>38</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10458-024-09639-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a negotiation framework to solve the Multi-Agent Path Finding (MAPF) Problem for self-interested agents in a decentralized fashion. The framework aims to achieve a good trade-off between the privacy of the agents and the effectiveness of solutions. Accordingly, a token-based bilateral negotiation protocol and two negotiation strategies are presented. The experimental results over four different settings of the MAPF problem show that the proposed approach could find conflict-free path solutions albeit suboptimally, especially when the search space is large and high-density. In contrast, Explicit Estimation Conflict-Based Search (EECBS) struggles to find optimal solutions. Besides, deploying a sophisticated negotiation strategy that utilizes information about local density for generating alternative paths can yield remarkably better solution performance in this negotiation framework.},
  archive      = {J_AAMAS},
  author       = {Keskin, M. Onur and Cantürk, Furkan and Eran, Cihan and Aydoğan, Reyhan},
  doi          = {10.1007/s10458-024-09639-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Decentralized multi-agent path finding framework and strategies based on automated negotiation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contest partitioning in binary contests. <em>AAMAS</em>,
<em>38</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10458-024-09637-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we explore the opportunities presented by partitioning contestants in contest into disjoint groups, each competing in an independent contest, with its own prize. This, as opposed to most literature on contest design, which focuses on the setting of a single “grand” (possibly multi-stage) contest, wherein all potential contestants ultimately compete for the same prize(s), with few exceptions that do consider contest partitioning, yet with conflicting preference results concerning the optimal structure to be used. Focusing on binary contests, wherein the quality of contestants’ submissions are endogenously determined, we show that contest partitioning is indeed beneficial under some condition, e.g., whenever the number of contestants, or the prize amount, are “sufficiently large”, where the exact size requirements are a function of the partitioning cost. When partitioning does not entail any cost, we show that it is either a dominating or weakly dominating strategy, depending on the way the organizer’s expected benefit is determined. The analysis is further extended to consider partitioning where some of the sub-contests used contain a single contestant (a singleton). We conclude that contest partitioning is an avenue that contest designers can and should consider, when aiming to maximize their profit.},
  archive      = {J_AAMAS},
  author       = {Levy, Priel and Aumann, Yonatan and Sarne, David},
  doi          = {10.1007/s10458-024-09637-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Contest partitioning in binary contests},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and reinforcement learning in partially observable
many-agent systems. <em>AAMAS</em>, <em>38</em>(1), 1–45. (<a
href="https://doi.org/10.1007/s10458-024-09640-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. These methods rely on all the agents sharing various types of information, such as their actions or gradients, with a centralized trainer or each other during the learning. Subsequently, the methods produce agent policies whose prescriptions and performance are contingent on other agents engaging in behavior assumed by the centralized training. But, in many contexts, such as mixed or adversarial settings, this assumption may not be feasible. In this article, we present a new line of methods that relaxes this assumption and engages in decentralized training resulting in the agent’s individual policy. The interactive advantage actor-critic (IA2C) maintains and updates beliefs over other agents’ candidate behaviors based on (noisy) observations, thus enabling learning at the agent’s own level. We also address MARL’s prohibitive curse of dimensionality due to the presence of many agents in the system. Under assumptions of action anonymity and population homogeneity, often exhibited in practice, large numbers of other agents can be modeled aggregately by the count vectors of their actions instead of individual agent models. More importantly, we may model the distribution of these vectors and its update using the Dirichlet-multinomial model, which offers an elegant way to scale IA2C to many-agent systems. We evaluate the performance of the fully decentralized IA2C along with other known baselines on a novel Organization domain, which we introduce, and on instances of two existing domains. Experimental comparisons with prominent and recent baselines show that IA2C is more sample efficient, more robust to noise, and can scale to learning in systems with up to a hundred agents.},
  archive      = {J_AAMAS},
  author       = {He, Keyang and Doshi, Prashant and Banerjee, Bikramjit},
  doi          = {10.1007/s10458-024-09640-1},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-45},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Modeling and reinforcement learning in partially observable many-agent systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equilibria in schelling games: Computational hardness and
robustness. <em>AAMAS</em>, <em>38</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10458-023-09632-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the simplest game-theoretic formulation of Schelling’s model of segregation on graphs, agents of two different types each select their own vertex in a given graph so as to maximize the fraction of agents of their type in their occupied neighborhood. Two ways of modeling agent movement here are either to allow two agents to swap their vertices or to allow an agent to jump to a free vertex. The contributions of this paper are twofold. First, we prove that deciding the existence of a swap-equilibrium and a jump-equilibrium in this simplest model of Schelling games is NP-hard, thereby answering questions left open by Agarwal et al. [AAAI ’20] and Elkind et al. [IJCAI ’19]. Second, we introduce two measures for the robustness of equilibria in Schelling games in terms of the minimum number of edges or the minimum number of vertices that need to be deleted to make an equilibrium unstable. We prove tight lower and upper bounds on the edge- and vertex-robustness of swap-equilibria in Schelling games on different graph classes.},
  archive      = {J_AAMAS},
  author       = {Kreisel, Luca and Boehmer, Niclas and Froese, Vincent and Niedermeier, Rolf},
  doi          = {10.1007/s10458-023-09632-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Equilibria in schelling games: Computational hardness and robustness},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uniformly constrained reinforcement learning.
<em>AAMAS</em>, <em>38</em>(1), 1–51. (<a
href="https://doi.org/10.1007/s10458-023-09607-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new multi-objective reinforcement learning algorithms that aim to find a globally Pareto-optimal deterministic policy that uniformly (in all states) maximizes a reward subject to a uniform probabilistic constraint over reaching forbidden states of a Markov decision process. Our requirements arise naturally in the context of safety-critical systems, but pose a significant unmet challenge. This class of learning problem is known to be hard and there are no off-the-shelf solutions that fully address the combined requirements of determinism and uniform optimality. Having formalized our requirements and highlighted the specific challenge of learning instability, using a simple counterexample, we define from first principles a stable Bellman operator that we prove partially respects our requirements. This operator is therefore a partial solution to our problem, but produces conservative polices in comparison to our previous approach, which was not designed to satisfy the same requirements. We thus propose a relaxation of the stable operator, using adaptive hysteresis, that forms the basis of a heuristic approach that is stable w.r.t. our counterexample and learns policies that are less conservative than those of the stable operator and our previous algorithm. In comparison to our previous approach, the policies of our adaptive hysteresis algorithm demonstrate improved monotonicity with increasing constraint probabilities, which is one of the characteristics we desire. We demonstrate that adaptive hysteresis works well with dynamic programming and reinforcement learning, and can be adapted to function approximation.},
  archive      = {J_AAMAS},
  author       = {Lee, Jaeyoung and Sedwards, Sean and Czarnecki, Krzysztof},
  doi          = {10.1007/s10458-023-09607-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-51},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Uniformly constrained reinforcement learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion auction design with transaction costs.
<em>AAMAS</em>, <em>38</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s10458-023-09631-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multi-unit auctions powered by intermediated markets, where all transactions are processed by intermediaries and incur certain costs. Each intermediary in the market owns a private set of buyers and all intermediaries are networked with each other. Our goal is to incentivize the intermediaries to share the auction information to individuals they can reach, including their private buyers and neighboring intermediaries, so that more potential buyers are able to participate in the auction. To this end, we build a diffusion-based auction framework to handle the transaction costs and the strategic interactions between intermediaries. The classic Vickrey-Clarke-Groves (VCG) mechanism within the scenario can obtain the maximum social welfare, but it can decrease the seller’s revenue or even lead to a deficit. To overcome the revenue issue, we develop two deficit reduction strategies, based on which a family of diffusion auctions called Critical Neighborhood Auctions (CNA) is identified. The CNA not only maximizes the social welfare, but also eliminates all the seller’s deficits. Moreover, the revenue given by the CNA is no less than the revenue given by the VCG mechanism with/without intermediaries. This is the first set of diffusion auctions with welfare and revenue advantages that can handle multiple items and transaction costs.},
  archive      = {J_AAMAS},
  author       = {Li, Bin and Hao, Dong and Zhao, Dengji},
  doi          = {10.1007/s10458-023-09631-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Diffusion auction design with transaction costs},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Majority opinion diffusion: When tie-breaking rule matters.
<em>AAMAS</em>, <em>38</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s10458-024-09651-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a graph G, which represents a social network, and assume that initially each node is either blue or white (corresponding to its opinion on a certain topic). In each round, all nodes simultaneously update their color to the most frequent color in their neighborhood. This is called the Majority Model (MM) if a node keeps its color in case of a tie and the Random Majority Model (RMM) if it chooses blue with probability 1/2 and white otherwise. We study the convergence properties of the above models, including stabilization time, periodicity, and the number of stable configurations. In particular, we prove that the stabilization time in RMM can be exponential in the size of the graph, which is in contrast with the previously known polynomial bound on the stabilization time of MM. We provide some bounds on the minimum size of a winning set, which is a set of nodes whose agreement on a color in the initial coloring enforces the process to end in a coloring where all nodes share that color. Furthermore, we calculate the expected final number of blue nodes for a random initial coloring, where each node is colored blue independently with some fixed probability, on cycle graphs. Finally, we conduct some experiments which complement our theoretical findings and also let us investigate other aspects of the models.},
  archive      = {J_AAMAS},
  author       = {Zehmakan, Ahad N.},
  doi          = {10.1007/s10458-024-09651-y},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Majority opinion diffusion: When tie-breaking rule matters},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IOB: Integrating optimization transfer and behavior transfer
for multi-policy reuse. <em>AAMAS</em>, <em>38</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10458-023-09630-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies’ value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.},
  archive      = {J_AAMAS},
  author       = {Li, Siyuan and Li, Hao and Zhang, Jin and Wang, Zhen and Liu, Peng and Zhang, Chongjie},
  doi          = {10.1007/s10458-023-09630-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {IOB: Integrating optimization transfer and behavior transfer for multi-policy reuse},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controller synthesis for linear temporal logic and
steady-state specifications. <em>AAMAS</em>, <em>38</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10458-024-09648-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of deriving decision-making policies, subject to some formal specification of behavior, has been well-studied in the control synthesis, reinforcement learning, and planning communities. Such problems are typically framed in the context of a non-deterministic decision process, the non-determinism of which is optimally resolved by the computed policy. In this paper, we explore the derivation of such policies in Markov decision processes (MDPs) subject to two types of formal specifications. First, we consider steady-state specifications that reason about the infinite-frequency behavior of the resulting agent. This behavior corresponds to the frequency with which an agent visits each state as it follows its decision-making policy indefinitely. Second, we examine the infinite-trace behavior of the agent by imposing Linear Temporal Logic (LTL) constraints on the behavior induced by the resulting policy. We present an algorithm to find a deterministic policy satisfying LTL and steady-state constraints by characterizing the solutions as an integer linear program (ILP) and experimentally evaluate our approach. In our experimental results section, we evaluate the proposed ILP using MDPs with stochastic and deterministic transitions.},
  archive      = {J_AAMAS},
  author       = {Velasquez, Alvaro and Alkhouri, Ismail and Beckus, Andre and Trivedi, Ashutosh and Atia, George},
  doi          = {10.1007/s10458-024-09648-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Controller synthesis for linear temporal logic and steady-state specifications},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-free reinforcement learning for motion planning of
autonomous agents with complex tasks in partially observable
environments. <em>AAMAS</em>, <em>38</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10458-024-09641-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion planning of autonomous agents in partially known environments with incomplete information is a challenging problem, particularly for complex tasks. This paper proposes a model-free reinforcement learning approach to address this problem. We formulate motion planning as a probabilistic-labeled partially observable Markov decision process (PL-POMDP) problem and use linear temporal logic (LTL) to express the complex task. The LTL formula is then converted to a limit-deterministic generalized Büchi automaton (LDGBA). The problem is redefined as finding an optimal policy on the product of PL-POMDP with LDGBA based on model-checking techniques to satisfy the complex task. We implement deep Q learning with long short-term memory (LSTM) to process the observation history and task recognition. Our contributions include the proposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Q learning. We demonstrate the applicability of the proposed method by conducting simulations in various environments, including grid worlds, a virtual office, and a multi-agent warehouse. The simulation results demonstrate that our proposed method effectively addresses environment, action, and observation uncertainties. This indicates its potential for real-world applications, including the control of unmanned aerial vehicles.},
  archive      = {J_AAMAS},
  author       = {Li, Junchao and Cai, Mingyu and Kan, Zhen and Xiao, Shaoping},
  doi          = {10.1007/s10458-024-09641-0},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Model-free reinforcement learning for motion planning of autonomous agents with complex tasks in partially observable environments},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Warmth and competence in human-agent cooperation.
<em>AAMAS</em>, <em>38</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10458-024-09649-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction and cooperation with humans are overarching aspirations of artificial intelligence research. Recent studies demonstrate that AI agents trained with deep reinforcement learning are capable of collaborating with humans. These studies primarily evaluate human compatibility through “objective” metrics such as task performance, obscuring potential variation in the levels of trust and subjective preference that different agents garner. To better understand the factors shaping subjective preferences in human-agent cooperation, we train deep reinforcement learning agents in Coins, a two-player social dilemma. We recruit $$N = 501$$ participants for a human-agent cooperation study and measure their impressions of the agents they encounter. Participants’ perceptions of warmth and competence predict their stated preferences for different agents, above and beyond objective performance metrics. Drawing inspiration from social science and biology research, we subsequently implement a new “partner choice” framework to elicit revealed preferences: after playing an episode with an agent, participants are asked whether they would like to play the next episode with the same agent or to play alone. As with stated preferences, social perception better predicts participants’ revealed preferences than does objective performance. Given these results, we recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies.},
  archive      = {J_AAMAS},
  author       = {McKee, Kevin R. and Bai, Xuechunzi and Fiske, Susan T.},
  doi          = {10.1007/s10458-024-09649-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Warmth and competence in human-agent cooperation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Landmark-based distributed topological mapping and
navigation in GPS-denied urban environments using teams of low-cost
robots. <em>AAMAS</em>, <em>38</em>(1), 1–43. (<a
href="https://doi.org/10.1007/s10458-024-09635-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of autonomous multi-robot mapping, exploration and navigation in unknown, GPS-denied indoor or urban environments using a team of robots equipped with directional sensors with limited sensing capabilities and limited computational resources. The robots have no a priori knowledge of the environment and need to rapidly explore and construct a map in a distributed manner using existing landmarks, the presence of which can be detected using onboard senors, although little to no metric information (distance or bearing to the landmarks) is available. In order to correctly and effectively achieve this, the presence of a necessary density/distribution of landmarks is ensured by design of the urban/indoor environment. We thus address this problem in two phases: (1) During the design/construction of the urban/indoor environment we can ensure that sufficient landmarks are placed within the environment. To that end we develop a filtration-based approach for designing strategic placement of landmarks in an environment. (2) We develop a distributed algorithm which a team of robots, with no a priori knowledge of the environment, can use to explore such an environment, construct a topological map requiring no metric/distance information, and use that map to navigate within the environment. This is achieved using a topological representation of the environment (called a Landmark Complex), instead of constructing a complete metric/pixel map. The representation is built by the robot as well as used by them for navigation through a balanced strategy involving exploration and exploitation. We use tools from homology theory for identifying “holes” in the coverage/exploration of the unknown environment and hence guide the robots towards achieving a complete exploration and mapping of the environment. Our simulation results demonstrate the effectiveness of the proposed metric-free topological (simplicial complex) representation in achieving exploration, localization and navigation within the environment.},
  archive      = {J_AAMAS},
  author       = {Teymouri, Mohammad Saleh and Bhattacharya, Subhrajit},
  doi          = {10.1007/s10458-024-09635-y},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-43},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Landmark-based distributed topological mapping and navigation in GPS-denied urban environments using teams of low-cost robots},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: A survey of multi-agent deep reinforcement
learning with communication. <em>AAMAS</em>, <em>38</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10458-024-09644-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AAMAS},
  author       = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  doi          = {10.1007/s10458-024-09644-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Correction: A survey of multi-agent deep reinforcement learning with communication},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A survey of multi-agent deep reinforcement learning with
communication. <em>AAMAS</em>, <em>38</em>(1), 1–48. (<a
href="https://doi.org/10.1007/s10458-023-09633-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication is an effective mechanism for coordinating the behaviors of multiple agents, broadening their views of the environment, and to support their collaborations. In the field of multi-agent deep reinforcement learning (MADRL), agents can improve the overall learning performance and achieve their objectives by communication. Agents can communicate various types of messages, either to all agents or to specific agent groups, or conditioned on specific constraints. With the growing body of research work in MADRL with communication (Comm-MADRL), there is a lack of a systematic and structural approach to distinguish and classify existing Comm-MADRL approaches. In this paper, we survey recent works in the Comm-MADRL field and consider various aspects of communication that can play a role in designing and developing multi-agent reinforcement learning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL approaches can be analyzed, developed, and compared. By projecting existing works into the multi-dimensional space, we discover interesting trends. We also propose some novel directions for designing future Comm-MADRL systems through exploring possible combinations of the dimensions.},
  archive      = {J_AAMAS},
  author       = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  doi          = {10.1007/s10458-023-09633-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-48},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A survey of multi-agent deep reinforcement learning with communication},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: Warmth and competence in human-agent
cooperation. <em>AAMAS</em>, <em>38</em>(1), 1. (<a
href="https://doi.org/10.1007/s10458-024-09654-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AAMAS},
  author       = {McKee, Kevin R. and Bai, Xuechunzi and Fiske, Susan T.},
  doi          = {10.1007/s10458-024-09654-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Correction: Warmth and competence in human-agent cooperation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
