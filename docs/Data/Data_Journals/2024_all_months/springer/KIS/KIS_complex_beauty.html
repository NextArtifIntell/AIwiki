<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---267">KIS - 267</h2>
<ul>
<li><details>
<summary>
(2024a). Correction: Local soft rough approximations and their
applications to conflict analysis problems. <em>KIS</em>,
<em>66</em>(12), 7891. (<a
href="https://doi.org/10.1007/s10115-024-02153-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Ansari, Moin Akhtar and Rehman, Noor and Ali, Abbas and Hila, Kostaq and Mubeen, Tahira},
  doi          = {10.1007/s10115-024-02153-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7891},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: Local soft rough approximations and their applications to conflict analysis problems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative user-experience research for developing
domain-specific natural language processing applications. <em>KIS</em>,
<em>66</em>(12), 7859–7889. (<a
href="https://doi.org/10.1007/s10115-024-02212-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User experience (UX) is a part of human–computer interaction research and focuses on increasing intuitiveness, transparency, simplicity, and trust for the system users. Most UX research for machine learning or natural language processing (NLP) focuses on a data-driven methodology. It engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems toward user usability, unlike learning about the user needs first. This paper proposes a new methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating system usefulness and user utility. The methodology emerged from and is evaluated on a case study about the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. A key finding of our case study is that involving domain experts increases their interest and trust in the final NLP application. The combined UX+NLP research of the proposed method efficiently considers data- and user-driven opportunities and constraints, which can be crucial for developing NLP applications.},
  archive      = {J_KIS},
  author       = {Zhukova, Anastasia and von Sperl, Lukas and Matt, Christian E. and Gipp, Bela},
  doi          = {10.1007/s10115-024-02212-5},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7859-7889},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Generative user-experience research for developing domain-specific natural language processing applications},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-perspective patient representation learning for
disease prediction on electronic health records. <em>KIS</em>,
<em>66</em>(12), 7837–7858. (<a
href="https://doi.org/10.1007/s10115-024-02188-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient representation learning based on electronic health records (EHR) is a critical task for disease prediction. This task aims to effectively extract useful information on dynamic features. Although various existing works have achieved remarkable progress, the model performance can be further improved by fully extracting the trends, variations, and the correlation between the trends and variations in dynamic features. In addition, sparse visit records limit the performance of deep learning models. To address these issues, we propose the multi-perspective patient representation Extractor (MPRE) for disease prediction. Specifically, we propose frequency transformation module (FTM) to extract the trend and variation information of dynamic features in the time–frequency domain, which can enhance the feature representation. In the 2D multi-extraction network (2D MEN), we form the 2D temporal tensor based on trend and variation. Then, the correlations between trend and variation are captured by the proposed dilated operation. Moreover, we propose the first-order difference attention mechanism (FODAM) to calculate the contributions of differences in adjacent variations to the disease diagnosis adaptively. To evaluate the performance of MPRE and baseline methods, we conduct extensive experiments on two real-world public datasets. The experiment results show that MPRE outperforms state-of-the-art baseline methods in terms of AUROC and AUPRC.},
  archive      = {J_KIS},
  author       = {Yu, Ziyue and Wang, Jiayi and Luo, Wuman and Tse, Rita and Pau, Giovanni},
  doi          = {10.1007/s10115-024-02188-2},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7837-7858},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-perspective patient representation learning for disease prediction on electronic health records},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep ensembled multi-criteria recommendation system for
enhancing and personalizing the user experience on e-commerce platforms.
<em>KIS</em>, <em>66</em>(12), 7799–7836. (<a
href="https://doi.org/10.1007/s10115-024-02187-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The commercially applicable Recommendation system (RS) exploits multi-criteria rating-based user-item interaction to learn and personalize user preferences using the Multi-criteria recommendation system (MCRS). The existing MCRS techniques have exploited similarity or aggregation function-based modeling to improve prediction accuracy. However, these MCRS methods do not investigate item aspects-based latent user preferences and criteria-based user-item implicit relationships. Also, the prediction reliability is uncertain due to highly sparse user-item interactions and ignoring auxiliary information support. Hence, this study proposes an ensembled approach that jointly develops the Similarity and aggregation function-based MCRS model (SimAgg-MCRS) and aggregates their user-item predicted preferences into a cumulative preference matrix to generate the final recommendation. First, the proposed model develops the deep neural network (DNN)-based model to aggregate the criteria-based similarity and predicts the overall rating using the aggregated similarity by merging user and item-based predictions. Second, the preference relation-based aggregation function approach develops deep autoencoder-based modeling to exploit the latent relationship among criteria to obtain users’ overall preference over an item by aggregating criteria-wise preference. Finally, the third phase develops the DNN-based ensemble model to integrate the preference matrix of similarity and aggregation function approach to obtain the overall aggregated matrix for the recommendation. The proposed SimAgg-MCRS integrates user and item side information to learn user preferences better. Experimental and prediction accuracy-based comparative evaluation results across Yahoo! Movies and Trip Advisor multi-criteria datasets validate the proposed models’ performance over the baseline MCRS methods.},
  archive      = {J_KIS},
  author       = {Shrivastava, Rahul and Sisodia, Dilip Singh and Nagwani, Naresh Kumar},
  doi          = {10.1007/s10115-024-02187-3},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7799-7836},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep ensembled multi-criteria recommendation system for enhancing and personalizing the user experience on e-commerce platforms},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lazy learning and sparsity handling in recommendation
systems. <em>KIS</em>, <em>66</em>(12), 7775–7797. (<a
href="https://doi.org/10.1007/s10115-024-02218-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are ubiquitous in various domains, facilitating users in finding relevant items according to their preferences. Identifying pertinent items that meet their preferences enables users to target the right items. To predict ratings for more accurate forecasts, recommender systems often use collaborative filtering (CF) approaches to sparse user-rated item matrices. Due to a lack of knowledge regarding newly formed entities, the data sparsity of the user-rated item matrix has an enormous effect on collaborative filtering algorithms, which frequently face lazy learning issues. Real-world datasets with exponentially increasing users and reviews make this situation worse. Matrix factorization (MF) stands out as a key strategy in recommender systems, especially for CF tasks. This paper presents a neural network matrix factorization (NNMF) model through machine learning to overcome data sparsity challenges. This approach aims to enhance recommendation quality while mitigating the impact of data sparsity, a common issue in CF algorithms. A thorough comparative analysis was conducted on the well-known MovieLens dataset, spanning from 1.6 to 9.6 M records. The outcomes consistently favored the NNMF algorithm, showcasing superior performance compared to the state-of-the-art method in this domain in terms of precision, recall, $${\mathcal {F}}1_{\textrm{score}}$$ , MAE, and RMSE.},
  archive      = {J_KIS},
  author       = {Mishra, Suryanshi and Singh, Tinku and Kumar, Manish and Satakshi},
  doi          = {10.1007/s10115-024-02218-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7775-7797},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Lazy learning and sparsity handling in recommendation systems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truss community search in uncertain graphs. <em>KIS</em>,
<em>66</em>(12), 7739–7773. (<a
href="https://doi.org/10.1007/s10115-024-02215-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an uncertain graph, community search is used to return dense subgraphs that contain the query vertex and satisfy the probability constraint. With the proliferation of uncertain graphs in practical applications, community search has become increasingly important in practical applications to help users make decisions in advertising recommendations, conference organization, etc. However, existing approaches for community search still suffer from two problems. First, they may return subgraphs that cannot meet users’ expectations on structural cohesiveness, due to the existence of cut-vertices/edges. Second, they use floating-point division to update the probability of each edge during computation, resulting in inaccurate results. In this paper, we study community search on uncertain graphs and propose efficient algorithms to address the above two problems. We first propose a novel community model, namely triangle-connected $$(k,\gamma )$$ -truss community, to return communities with enhanced cohesiveness. Then, we propose an online algorithm that uses a batch-recalculation strategy to guarantee the accuracy. To improve the performance of community search, we propose an index-based approach. This index organizes all the triangle-connected $$(k,\gamma )$$ -truss communities using a forest structure and maintains the mapping relationship from vertices in the uncertain graph to communities in the index. Based on this index, we can get the results of community search easily, without the costly operation as the online approach does. Finally, we conduct rich experiments on 10 real-world graphs. The experimental results verified the effectiveness and efficiency of our approaches.},
  archive      = {J_KIS},
  author       = {Xing, Bo and Tan, Yuting and Zhou, Junfeng and Du, Ming},
  doi          = {10.1007/s10115-024-02215-2},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7739-7773},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Truss community search in uncertain graphs},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation metrics on text summarization: Comprehensive
survey. <em>KIS</em>, <em>66</em>(12), 7717–7738. (<a
href="https://doi.org/10.1007/s10115-024-02217-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic text summarization is the process of shortening a large document into a summary text that preserves the main concepts and key points of the original document. Due to the wide applications of text summarization, many studies have been conducted on it, but evaluating the quality of generated summaries poses significant challenges. Selecting the appropriate evaluation metrics to capture various aspects of summarization quality, including content, structure, coherence, readability, novelty, and semantic relevance, plays a crucial role in text summarization application. To address this challenge, the main focus of this study is on gathering and investigating a comprehensive set of evaluation metrics. Analysis of various metrics can enhance the understanding of the evaluation method and leads to select appropriate evaluation text summarization systems in the future. After a short review of various automatic text summarization methods, we thoroughly analyze 42 prominent metrics, categorizing them into six distinct categories to provide insights into their strengths, limitations, and applicability.},
  archive      = {J_KIS},
  author       = {Davoodijam, Ensieh and Alambardar Meybodi, Mohsen},
  doi          = {10.1007/s10115-024-02217-0},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7717-7738},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluation metrics on text summarization: Comprehensive survey},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing link prediction through node embedding and
ensemble learning. <em>KIS</em>, <em>66</em>(12), 7697–7715. (<a
href="https://doi.org/10.1007/s10115-024-02203-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks, characterized by their dynamic and continually evolving nature, present challenges for effective link prediction (LP) due to the constant addition of nodes and connections. In response to this, we propose a novel approach to LP in social networks through Node Embedding and Ensemble Learning (LP-NEEL). Our method constructs a transition matrix from the network’s adjacency matrix and computes similarity measures between node pairs. Utilizing node2vec embedding, we extract features from nodes and generate edge embeddings by computing the inner product of node embeddings for each edge. This process yields a well-labeled dataset suitable for LP tasks. To mitigate overfitting, we balance the dataset by ensuring an equal number of negative and positive samples edge samples during both the testing and training phases. Leveraging this balanced dataset, we employ the XGBoost machine learning algorithm for final link prediction. Extensive experimentation across six social network datasets validates the efficacy of our approach, demonstrating improved predictive performance compared to existing methods.},
  archive      = {J_KIS},
  author       = {Chen, Zhongyuan and Wang, Yongji},
  doi          = {10.1007/s10115-024-02203-6},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7697-7715},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing link prediction through node embedding and ensemble learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMMDA: Gaussian mixture modeling of graph in latent space
for graph data augmentation. <em>KIS</em>, <em>66</em>(12), 7667–7695.
(<a href="https://doi.org/10.1007/s10115-024-02207-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data augmentation (GDA), which manipulates graph structure and/or attributes, has been demonstrated as an effective method for improving the generalization of graph neural networks on semi-supervised node classification. As a data augmentation technique, label preservation is critical, that is, node labels should not change after data manipulation. However, most existing methods overlook the label preservation requirements. Determining the label-preserving nature of a GDA method is highly challenging, owing to the non-Euclidean nature of the graph structure. In this study, for the first time, we formulate a label-preserving problem (LPP) in the context of GDA. The LPP is formulated as an optimization problem in which, given a fixed augmentation budget, the objective is to find an augmented graph with minimal difference in data distribution compared to the original graph. To solve the LPP problem, we propose GMMDA, a generative data augmentation (DA) method based on Gaussian mixture modeling (GMM) of a graph in a latent space. We designed a novel learning objective that jointly learns a low-dimensional graph representation and estimates the GMM. The learning is followed by sampling from the GMM, and the samples are converted back to the graph as additional nodes. To uphold label preservation, we designed a minimum description length (MDL)-based method to select a set of samples that produces the minimum shift in the data distribution captured by the GMM. Through experiments, we demonstrate that GMMDA can improve the performance of graph convolutional network on Cora, Citeseer and Pubmed by as much as $$7.75\%$$ , $$8.75\%$$ and $$5.87\%$$ , respectively, significantly outperforming the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Li, Yanjin and Xu, Linchuan and Yamanishi, Kenji},
  doi          = {10.1007/s10115-024-02207-2},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7667-7695},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GMMDA: Gaussian mixture modeling of graph in latent space for graph data augmentation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing the performance of deep learning models with fuzzy
c-means clustering. <em>KIS</em>, <em>66</em>(12), 7627–7665. (<a
href="https://doi.org/10.1007/s10115-024-02211-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models (DLMs), such as recurrent neural networks (RNN), long short-term memory (LSTM), bidirectional long short-term memory (Bi-LSTM), and gated recurrent unit (GRU), are superior for sequential data analysis due to their ability to learn complex patterns. This paper proposes enhancing performance of these models by applying fuzzy c-means (FCM) clustering on sequential data from a nonlinear plant and the stock market. FCM clustering helps to organize the data into clusters based on similarity, which improves the performance of the models. Thus, the proposed fuzzy c-means recurrent neural network (FCM-RNN), fuzzy c-means long short-term memory (FCM-LSTM), fuzzy c-means bidirectional long short-term memory (FCM-Bi-LSTM), and fuzzy c-means gated recurrent unit (FCM-GRU) models showed enhanced prediction results than RNN, LSTM, Bi-LSTM, and GRU models, respectively. This enhancement is validated using performance metrics such as root-mean-square error and mean absolute error and is further illustrated by scatter plots comparing actual versus predicted values for training, validation, and testing data. The experiment results confirm that integrating FCM clustering with DLMs shows the superiority of the proposed models.},
  archive      = {J_KIS},
  author       = {Singh, Saumya and Srivastava, Smriti},
  doi          = {10.1007/s10115-024-02211-6},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7627-7665},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing the performance of deep learning models with fuzzy c-means clustering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EIGP: Document-level event argument extraction with
information enhancement generated based on prompts. <em>KIS</em>,
<em>66</em>(12), 7609–7626. (<a
href="https://doi.org/10.1007/s10115-024-02213-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event argument extraction (EAE) task primarily aims to identify event arguments and their specific roles within a given event. Existing generation-based event argument extraction models, including the recent ones focused on document-level event argument extraction, emphasize the construction of prompt templates and entity representations. However, they overlook the inadequate comprehension of model in document context structure information and the impact of arguments spanning a wide range on event argument extraction. Consequently, this results in reduced model detection accuracy. In this paper, we propose a prompt-based generation event argument extraction model with the ability of document structure information enhancement for document-level event argument extraction task based on prompt generation. Specifically, we use sentence abstract meaning representation (AMR) to represent the contextual structural information of the document, and then remove the redundant parts of the structural information through constraints to obtain the constraint graph with the document information. Finally, we use the encoder to convert the graph into the corresponding dense vector. We inject these vectors with contextual structural information into the prompt-based generation EAE model in a prefixed manner. When contextual information and prompt templates interact at the attention layer of the model, the generated structural information improves the generation by affecting attention. We conducted experiments on RAMS and WIKIEVENTS datasets, and the results show that our model achieves excellent results compared with the current advanced generative EAE model.},
  archive      = {J_KIS},
  author       = {Liu, Kai and Zhao, Hui and Wang, Zicong and Hou, Qianxi},
  doi          = {10.1007/s10115-024-02213-4},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7609-7626},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {EIGP: Document-level event argument extraction with information enhancement generated based on prompts},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Probabilistic temporal semantic graph: A holistic framework
for event detection in twitter. <em>KIS</em>, <em>66</em>(12),
7581–7607. (<a
href="https://doi.org/10.1007/s10115-024-02208-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event detection on social media platforms, especially Twitter, poses significant challenges due to the dynamic nature and high volume of data. The rapid flow of tweets and the varied ways users express thoughts complicate the identification of relevant events. Accurately identifying and interpreting events from this noisy and fast-paced environment is crucial for various applications, including crisis management and market analysis. This paper presents a novel unsupervised framework for event detection on social media, designed to enhance the accuracy and efficiency of identifying significant events from Twitter data. The framework incorporates several innovative techniques, including dynamic bandwidth adjustment based on local data density, Mahalanobis distance integration, adaptive kernel density estimation, and an improved Louvain-MOMR method for community detection. Additionally, a new scoring system is implemented to accurately extract trending words that evoke strong emotions, improving the identification of event-related keywords. The proposed framework demonstrates robust performance across three diverse datasets: FACup, Super Tuesday, and US Election, showcasing its effectiveness in capturing temporal and semantic patterns within tweets.},
  archive      = {J_KIS},
  author       = {Bashiri, Hadis and Naderi, Hassan},
  doi          = {10.1007/s10115-024-02208-1},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7581-7607},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Probabilistic temporal semantic graph: A holistic framework for event detection in twitter},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GoSum: Extractive summarization of long documents by
reinforcement learning and graph-organized discourse state.
<em>KIS</em>, <em>66</em>(12), 7557–7580. (<a
href="https://doi.org/10.1007/s10115-024-02195-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summarizing extensive documents involves selecting sentences, with the organizational structure of document sections playing a pivotal role. However, effectively utilizing discourse information for summary generation poses a significant challenge, especially given the inconsistency between training and evaluation in extractive summarization. In this paper, we introduce GoSum, a novel extractive summarizer that integrates a graph-based model with reinforcement learning techniques to summarize long documents. Specifically, GoSum utilizes a graph neural network to encode sentence states, constructing a heterogeneous graph that represents each document at various discourse levels. The edges of this graph capture hierarchical relationships between different document sections. Furthermore, GoSum incorporates offline reinforcement learning, enabling the model to receive ROUGE score feedback on diverse training samples, thereby enhancing the quality of summary generation. On the two scientific article datasets PubMed and arXiv, GoSum achieved the highest performance among extractive models. Particularly on the PubMed dataset, GoSum outperformed other models with ROUGE-1 and ROUGE-L scores surpassing by 0.45 and 0.26, respectively.},
  archive      = {J_KIS},
  author       = {Bian, Junyi and Huang, Xiaodi and Zhou, Hong and Huang, Tianyang and Zhu, Shanfeng},
  doi          = {10.1007/s10115-024-02195-3},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7557-7580},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GoSum: Extractive summarization of long documents by reinforcement learning and graph-organized discourse state},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing marginal and conditional divergences between
decomposable models with applications in quantum computing and earth
observation. <em>KIS</em>, <em>66</em>(12), 7527–7556. (<a
href="https://doi.org/10.1007/s10115-024-02191-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to compute the exact divergence between two high-dimensional distributions is useful in many applications, but doing so naively is intractable. Computing the $$\alpha \beta $$ -divergence—a family of divergences that includes the Kullback–Leibler divergence and Hellinger distance—between the joint distribution of two decomposable models, i.e., chordal Markov networks, can be done in time exponential in the treewidth of these models. Extending this result, we propose an approach to compute the exact $$\alpha \beta $$ -divergence between any marginal or conditional distribution of two decomposable models. In order to do so tractably, we provide a decomposition over the marginal and conditional distributions of decomposable models. We then show how our method can be used to analyze distributional changes by first applying it to the benchmark image dataset QMNIST and a dataset containing observations from various areas at the Roosevelt Nation Forest and their cover type. Finally, based on our framework, we propose a novel way to quantify the error in contemporary superconducting quantum computers.},
  archive      = {J_KIS},
  author       = {Lee, Loong Kuan and Webb, Geoffrey I. and Schmidt, Daniel F. and Piatkowski, Nico},
  doi          = {10.1007/s10115-024-02191-7},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7527-7556},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Computing marginal and conditional divergences between decomposable models with applications in quantum computing and earth observation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REDAffectiveLM: Leveraging affect enriched embedding and
transformer-based neural language model for readers’ emotion detection.
<em>KIS</em>, <em>66</em>(12), 7495–7525. (<a
href="https://doi.org/10.1007/s10115-024-02194-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements in web platforms allow people to express and share emotions toward textual write-ups written and shared by others. This brings about different interesting domains for analysis, emotion expressed by the writer and emotion elicited from the readers. In this paper, we propose a novel approach for readers’ emotion detection from short-text documents using a deep learning model called REDAffectiveLM. Within state-of-the-art NLP tasks, it is well understood that utilizing context-specific representations from transformer-based pre-trained language models helps achieve improved performance. Within this affective computing task, we explore how incorporating affective information can further enhance performance. Toward this, we leverage context-specific and affect enriched representations by using a transformer-based pre-trained language model in tandem with affect enriched Bi-LSTM+Attention. For empirical evaluation, we procure a new dataset REN-20k, besides using RENh-4k and SemEval-2007. We evaluate the performance of our REDAffectiveLM rigorously across these datasets, against a vast set of state-of-the-art baselines, where our model consistently outperforms baselines and obtains statistically significant results. Our results establish that utilizing affect enriched representation along with context-specific representation within a neural architecture can considerably enhance readers’ emotion detection. Since the impact of affect enrichment specifically in readers’ emotion detection isn’t well explored, we conduct a detailed analysis over affect enriched Bi-LSTM+Attention using qualitative and quantitative model behavior evaluation techniques. We observe that compared to conventional semantic embedding, affect enriched embedding increases the ability of the network to effectively identify and assign weightage to the key terms responsible for readers’ emotion detection to improve prediction.},
  archive      = {J_KIS},
  author       = {Kadan, Anoop and Deepak, P. and Gangan, Manjary P. and Abraham, Sam Savitha and Lajish, V. L.},
  doi          = {10.1007/s10115-024-02194-4},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7495-7525},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {REDAffectiveLM: Leveraging affect enriched embedding and transformer-based neural language model for readers’ emotion detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complementary incomplete weighted concept factorization
methods for multi-view clustering. <em>KIS</em>, <em>66</em>(12),
7469–7494. (<a
href="https://doi.org/10.1007/s10115-024-02197-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of traditional multi-view clustering is to categorize data into separate clusters under the assumption that all views are fully available. However, practical scenarios often arise where not all aspects of the data are accessible, which hampers the efficacy of conventional multi-view clustering techniques. Recent advancements have made significant progress in addressing the incompleteness in multi-view data clustering. Still, current incomplete multi-view clustering methods overlooked a number of important factors, such as providing a consensus representation across the kernel space, dealing with over-fitting issue from different views, and looking at how these multiple views relate to each other at the same time. To deal these challenges, we introduced an innovative multi-view clustering algorithm to manage incomplete data from multiple perspectives. Additionally, we have introduced a novel objective function incorporating a weighted concept factorization technique to tackle the absence of data instances within each incomplete viewpoint. We used a co-regularization constraint to learn a common shared structure from different points of view and a smooth regularization term to prevent view over-fitting. It is noteworthy that the proposed objective function is inherently non-convex, presenting optimization challenges. To obtain the optimal solution, we have implemented an iterative optimization approach to converge the local minima for our method. To underscore the effectiveness and validation of our approach, we conducted experiments using real-world datasets against state-of-the-art methods for comparative evaluation.},
  archive      = {J_KIS},
  author       = {Khan, Ghufran Ahmad and Khan, Jalaluddin and Anwar, Taushif and Al-Huda, Zaid and Diallo, Bassoma and Ahmad, Naved},
  doi          = {10.1007/s10115-024-02197-1},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7469-7494},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Complementary incomplete weighted concept factorization methods for multi-view clustering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust anomaly detection via adversarial counterfactual
generation. <em>KIS</em>, <em>66</em>(12), 7437–7468. (<a
href="https://doi.org/10.1007/s10115-024-02172-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability to devise robust outlier and anomaly detection tools is an important research topic in machine learning and data mining. Recent techniques have been focusing on reinforcing detection with sophisticated data generation tools that successfully refine the learning process by generating variants of the data that expand the recognition capabilities of the outlier detector. In this paper, we propose $$\textrm{ARN}$$ , a semi-supervised anomaly detection and generation method based on adversarial counterfactual reconstruction. $$\textrm{ARN}$$ exploits a regularized autoencoder to optimize the reconstruction of variants of normal examples with minimal differences that are recognized as outliers. The combination of regularization and counterfactual reconstruction helps to stabilize the learning process, which results in both realistic outlier generation and substantially extended detection capability. In fact, the counterfactual generation enables a smart exploration of the search space by successfully relating small changes in all the actual samples from the true distribution to high anomaly scores. Experiments on several benchmark datasets show that our model improves the current state of the art by valuable margins because of its ability to model the true boundaries of the data manifold.},
  archive      = {J_KIS},
  author       = {Liguori, Angelica and Ritacco, Ettore and Pisani, Francesco Sergio and Manco, Giuseppe},
  doi          = {10.1007/s10115-024-02172-w},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7437-7468},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Robust anomaly detection via adversarial counterfactual generation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoRGH: Movie recommender system using GNNs on heterogeneous
graphs. <em>KIS</em>, <em>66</em>(12), 7419–7435. (<a
href="https://doi.org/10.1007/s10115-024-02196-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the advent of movies and TV shows and the competition between different movie streamer companies and movie databases to attract more users, movie recommenders have become a major prerequisite for customer satisfaction. Most of the previously introduced methods used collaborative, content-based, and hybrid filtering techniques, where neural network-based approaches and matrix completion are the major approaches of most recent movie recommender systems. The major drawbacks of previous systems are not considering side information, such as plot synopsis and cold start problem, in the context of movie recommendations. In this paper, we propose a novel inductive approach called MoRGH which first constructs a graph of similar movies by considering the information available in movies’ plot synopsis and genres. Second, we construct a heterogeneous graph that includes two types of nodes: movies and users. This graph is built using the MovieLens dataset and the similarity graph generated in the first stage, where each edge between a user and a movie represents the user’s rating for that movie, and each edge between two movies represents the similarity between them. Third, MoRGH mitigates the drawbacks of previous methods by employing a GNN and GAE-based model that combines collaborative and content-based approaches. This hybrid approach allows MoRGH to provide accurate and more personalized recommendations for each user, outperforming previous state-of-the-art models in terms of RMSE scores. The achieved improvement in RMSE scores demonstrates MoRGH’s superior performance and its ability to deliver enhanced recommendations compared to existing models.},
  archive      = {J_KIS},
  author       = {Ziaee, Seyed Sina and Rahmani, Hossein and Nazari, Mohammad},
  doi          = {10.1007/s10115-024-02196-2},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7419-7435},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MoRGH: Movie recommender system using GNNs on heterogeneous graphs},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive moving average q-learning. <em>KIS</em>,
<em>66</em>(12), 7389–7417. (<a
href="https://doi.org/10.1007/s10115-024-02190-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of algorithms have been proposed to address the long-standing overestimation bias problem of Q-learning. Reducing this overestimation bias may lead to an underestimation bias, such as double Q-learning. However, it is still unclear how to make a good balance between overestimation and underestimation. We present a simple yet effective algorithm to fill in this gap and call Moving Average Q-learning. Specifically, we maintain two dependent Q-estimators. The first one is used to estimate the maximum expected Q-value. The second one is used to select the optimal action. In particular, the second estimator is the moving average of historical Q-values generated by the first estimator. The second estimator has only one hyperparameter, namely the moving average parameter. This parameter controls the dependence between the second estimator and the first estimator, ranging from independent to identical. Based on Moving Average Q-learning, we design an adaptive strategy to select the moving average parameter, resulting in AdaMA (Adaptive Moving Average) Q-learning. This adaptive strategy is a simple function, where the moving average parameter increases monotonically with the number of state–action pairs visited. Moreover, we extend AdaMA Q-learning to AdaMA DQN in high-dimensional environments. Extensive experiment results reveal why Moving Average Q-learning and AdaMA Q-learning can mitigate the overestimation bias, and also show that AdaMA Q-learning and AdaMA DQN outperform SOTA baselines drastically. In particular, when compared with the overestimated value of 1.66 in Q-learning, AdaMA Q-learning underestimates by 0.196, resulting in an improvement of 88.19%.},
  archive      = {J_KIS},
  author       = {Tan, Tao and Xie, Hong and Xia, Yunni and Shi, Xiaoyu and Shang, Mingsheng},
  doi          = {10.1007/s10115-024-02190-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7389-7417},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive moving average Q-learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DQN-PACG: Load regulation method based on DQN and
multivariate prediction model. <em>KIS</em>, <em>66</em>(12), 7363–7387.
(<a href="https://doi.org/10.1007/s10115-024-02178-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand response plays a pivotal role in modern smart grid systems, aiding in balancing energy consumption. However, the increasing energy demands of contemporary society have placed a significant burden on power systems. To simulate the interaction between electricity supply and demand, this paper introduces the concept of Deep Q-Network (DQN) to the domain of demand response. Additionally, a novel multivariate forecasting model, referred to as PreAttention-CNN-GRU (PACG), is proposed to predict in real time the impact of electricity prices on consumer electricity usage behavior. Finally, a load control method, denoted as DQN-PreAttention-CNN-GRU (DQN-PACG), is presented to achieve price-based demand response. The performance of PACG was tested on a real-world German dataset, demonstrating superior predictive accuracy compared to traditional forecasting models such as Long Short-Term Memory Networks. Furthermore, the test results of DQN-PACG on the same dataset contribute to alleviating the load and stress on the power grid. This paper also includes a case study of southern provinces in China, where the model was able to reduce electricity consumption by 1.64% and electricity cost by 5.42%, both of which outperform the current electricity pricing policies.},
  archive      = {J_KIS},
  author       = {Lin, Rongheng and Chen, Shuo and He, Zheyu and Wu, Budan and Zhao, Xin and Li, Qiushuang},
  doi          = {10.1007/s10115-024-02178-4},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7363-7387},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DQN-PACG: Load regulation method based on DQN and multivariate prediction model},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Comprehensive review and comparative analysis of
transformer models in sentiment analysis. <em>KIS</em>, <em>66</em>(12),
7305–7361. (<a
href="https://doi.org/10.1007/s10115-024-02214-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis has become an important task in natural language processing because it is used in many different areas. This paper gives a detailed review of sentiment analysis, including its definition, challenges, and uses. Different approaches to sentiment analysis are discussed, focusing on how they have changed and their limitations. Special attention is given to recent improvements with transformer models and transfer learning. Detailed reviews of well-known transformer models like BERT, RoBERTa, XLNet, ELECTRA, DistilBERT, ALBERT, T5, and GPT are provided, looking at their structures and roles in sentiment analysis. In the experimental section, the performance of these eight transformer models is compared across 22 different datasets. The results show that the T5 model consistently performs the best on multiple datasets, demonstrating its flexibility and ability to generalize. XLNet performs very well in understanding irony and sentiments related to products, while ELECTRA and RoBERTa perform best on certain datasets, showing their strengths in specific areas. BERT and DistilBERT often perform the lowest, indicating that they may struggle with complex sentiment tasks despite being computationally efficient.},
  archive      = {J_KIS},
  author       = {Bashiri, Hadis and Naderi, Hassan},
  doi          = {10.1007/s10115-024-02214-3},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7305-7361},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Comprehensive review and comparative analysis of transformer models in sentiment analysis},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aspect-based sentiment analysis: Approaches, applications,
challenges and trends. <em>KIS</em>, <em>66</em>(12), 7261–7303. (<a
href="https://doi.org/10.1007/s10115-024-02200-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis (SA) is a technique that employs natural language processing to determine the function of mining methodically, extract, analyse and comprehend people’s thoughts, feelings, personal opinions and perceptions as well as their reactions and attitude regarding various subjects such as topics, commodities and various other products and services. However, it only reveals the overall sentiment. Unlike SA, the aspect-based sentiment analysis (ABSA) study categorizes a text into distinct components and determines the appropriate sentiment, which is more reliable in its predictions. Hence, ABSA is essential to study and break down texts into various service elements. It then assigns the appropriate sentiment polarity (positive, negative or neutral) for every aspect. In this paper, the main task is to critically review the research outcomes to look at the various techniques, methods and features used for ABSA. After giving brief introduction of SA in order to establish a clear relationship between SA and ABSA, we focussed on approaches, applications, challenges and trends in ABSA research.},
  archive      = {J_KIS},
  author       = {Nath, Deena and Dwivedi, Sanjay K.},
  doi          = {10.1007/s10115-024-02200-9},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7261-7303},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Aspect-based sentiment analysis: Approaches, applications, challenges and trends},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Community detection in social networks using machine
learning: A systematic mapping study. <em>KIS</em>, <em>66</em>(12),
7205–7259. (<a
href="https://doi.org/10.1007/s10115-024-02201-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important issues in social networks is the social communities which are formed by interactions between its members. Three types of community including overlapping, non-overlapping, and hidden are detected by different approaches. Regarding the importance of community detection in social networks, this paper provides a systematic mapping of machine learning-based community detection approaches. The study aimed to show the type of communities in social networks along with the algorithms of machine learning that have been used for community detection. After carrying out the steps of mapping and removing useless references, 246 papers were selected to answer the questions of this research. The results of the research indicated that unsupervised machine learning-based algorithms with 41.46% (such as k means) are the most used categories to detect communities in social networks due to their low processing overheads. On the other hand, there has been a significant increase in the use of deep learning since 2020 which has sufficient performance for community detection in large-volume data. With regard to the ability of NMI to measure the correlation or similarity between communities, with 53.25%, it is the most frequently used metric to evaluate the performance of community identifications. Furthermore, considering availability, low in size, and lack of multiple edge and loops, dataset Zachary’s Karate Club with 26.42% is the most used dataset for community detection research in social networks.},
  archive      = {J_KIS},
  author       = {Nooribakhsh, Mahsa and Fernández-Diego, Marta and González-Ladrón-De-Guevara, Fernando and Mollamotalebi, Mahdi},
  doi          = {10.1007/s10115-024-02201-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7205-7259},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Community detection in social networks using machine learning: A systematic mapping study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From text to multimodal: A survey of adversarial example
generation in question answering systems. <em>KIS</em>, <em>66</em>(12),
7165–7204. (<a
href="https://doi.org/10.1007/s10115-024-02199-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating adversarial machine learning with question answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems. This article aims to review adversarial example-generation techniques in the QA field, including textual and multimodal contexts. We examine the techniques employed through systematic categorization, providing a structured review. Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models. We then extend our research to include multimodal QA systems, analyze them across various methods, and examine generative models, seq2seq architectures, and hybrid methodologies. Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the literature on adversarial QA. Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges.},
  archive      = {J_KIS},
  author       = {Yigit, Gulsum and Amasyali, Mehmet Fatih},
  doi          = {10.1007/s10115-024-02199-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {7165-7204},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {From text to multimodal: A survey of adversarial example generation in question answering systems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: An analysis of large language models: Their
impact and potential applications. <em>KIS</em>, <em>66</em>(11),
7163–7164. (<a
href="https://doi.org/10.1007/s10115-024-02157-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Bharathi Mohan, G. and Prasanna Kumar, R. and Vishal Krishh, P. and Keerthinathan, A. and Lavanya, G. and Meghana, Meka Kavya Uma and Sulthana, Sheba and Doss, Srinath},
  doi          = {10.1007/s10115-024-02157-9},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7163-7164},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: an analysis of large language models: their impact and potential applications},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A multi-view mask contrastive learning graph convolutional
neural network for age estimation. <em>KIS</em>, <em>66</em>(11),
7137–7162. (<a
href="https://doi.org/10.1007/s10115-024-02193-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The age estimation task aims to use facial features to predict the age of people and is widely used in public security, marketing, identification, and other fields. However, the features are mainly concentrated in facial keypoints, and existing CNN and Transformer-based methods have inflexibility and redundancy for modeling complex irregular structures. Therefore, this paper proposes a multi-view mask contrastive learning graph convolutional neural network (MMCL-GCN) for age estimation. Specifically, the overall structure of the MMCL-GCN network contains a feature extraction stage and an age estimation stage. In the feature extraction stage, we introduce a graph structure to construct face images as input and then design a multi-view mask contrastive learning (MMCL) mechanism to learn complex structural and semantic information about face images. The learning mechanism employs an asymmetric Siamese network architecture, which utilizes an online encoder–decoder structure to reconstruct the missing information from the original graph and utilizes the target encoder to learn latent representations for contrastive learning. Furthermore, to promote the two learning mechanisms better compatible and complementary, we adopt two augmentation strategies and optimize the joint losses. In the age estimation stage, we design a multi-layer extreme learning machine (ML-IELM) with identity mapping to fully use the features extracted by the online encoder. Then, a classifier and a regressor were constructed based on ML-IELM, which were used to identify the age grouping interval and accurately estimate the final age. Extensive experiments show that MMCL-GCN can effectively reduce the error of age estimation on benchmark datasets such as Adience, MORPH-II, and LAP-2016.},
  archive      = {J_KIS},
  author       = {Zhang, Yiping and Shou, Yuntao and Meng, Tao and Ai, Wei and Li, Keqin},
  doi          = {10.1007/s10115-024-02193-5},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7137-7162},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A multi-view mask contrastive learning graph convolutional neural network for age estimation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crop health assessment through hierarchical fuzzy rule-based
status maps. <em>KIS</em>, <em>66</em>(11), 7109–7136. (<a
href="https://doi.org/10.1007/s10115-024-02180-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision agriculture is evolving toward a contemporary approach that involves multiple sensing techniques to monitor and enhance crop quality while minimizing losses and waste of no longer considered inexhaustible resources, such as soil and water supplies. To understand crop status, it is necessary to integrate data from heterogeneous sensors and employ advanced sensing devices that can assess crop and water status. This study presents a smart monitoring approach in agriculture, involving sensors that can be both stationary (such as soil moisture sensors) and mobile (such as sensor-equipped unmanned aerial vehicles). These sensors collect information from visual maps of crop production and water conditions, to comprehensively understand the crop area and spot any potential vegetation problems. A modular fuzzy control scheme has been designed to interpret spectral indices and vegetative parameters and, by applying fuzzy rules, return status maps about vegetation status. The rules are applied incrementally per a hierarchical design to correlate lower-level data (e.g., temperature, vegetation indices) with higher-level data (e.g., vapor pressure deficit) to robustly determine the vegetation status and the main parameters that have led to it. A case study was conducted, involving the collection of satellite images from artichoke crops in Salerno, Italy, to demonstrate the potential of incremental design and information integration in crop health monitoring. Subsequently, tests were conducted on vineyard regions of interest in Teano, Italy, to assess the efficacy of the framework in the assessment of plant status and water stress. Indeed, comparing the outcomes of our maps with those of cutting-edge machine learning (ML) semantic segmentation has indeed revealed a promising level of accuracy. Specifically, classification performance was compared to the output of conventional ML methods, demonstrating that our approach is consistent and achieves an accuracy of over 90% throughout various seasons of the year.},
  archive      = {J_KIS},
  author       = {Cavaliere, Danilo and Senatore, Sabrina and Loia, Vincenzo},
  doi          = {10.1007/s10115-024-02180-w},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7109-7136},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Crop health assessment through hierarchical fuzzy rule-based status maps},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AETC: An automated pest detection and classification model
using optimal integration of yolo + SSD and adaptive ensemble transfer
CNN with IoT-assisted pest images. <em>KIS</em>, <em>66</em>(11),
7077–7108. (<a
href="https://doi.org/10.1007/s10115-024-02146-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insects harm or destroy the crops and plants in agriculture fields by causing infection to the plants or destroying the valuables, which is called as a pest. When a plant is invaded by pests, the quality of the food it produces decreases drastically. So, it is highly essential to detect the pests before they attack the plants. However, the existing pest detection and categorizing techniques need suggestions and decisions from entomologists, and also this process consumes more time. If pests are identified at an early stage, then it could help the farmer to eliminate the necessity for pesticides and also increase food production. Because of its almost similar look, detecting and classifying the pests associated with a crop is complex work for the farmer, especially during the initial stage of plant growth. The sudden and productive growth in the Internet-of-Things (IoT) technology also finds its application in agriculture, resulting in a transition from statistical to quantitative methods. To alleviate the issues in the agricultural sector, a new framework for an IoT-assisted Automatic Pest Prediction and Classification (APDC) model using ensemble transfer learning of the convolutional neural network (CNN) method is developed. At first, IoT sensors are used to capture pest images from the agricultural field. These images are stored in the standard database, from which these images are taken for conducting experiments. The gathered images are then subjected to image pre-processing for contrast enhancement by median filter (MF). After that, the pests are detected from the pre-processed image by means of a Hybrid You Only Look Once (Yolo) v3 and Single Shot multi-box Detector (HYSSD) model. In this model, two algorithms, namely the Beetle Swarm Optimization (BSO) and the Salp Swarm Algorithm (SSA), are combined to optimize the parameters. An adaptive ensemble transfer CNN (AETC) is used to identify the pests after it has been detected. DenseNet, MobileNet, and ResNet are the three models that constitute this ensemble model. Finally, various metrics are used to verify the effectiveness of the proposed classification model. The findings from the results show that the recommended method has better classification accuracy.},
  archive      = {J_KIS},
  author       = {Prasath, B. and Akila, M.},
  doi          = {10.1007/s10115-024-02146-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7077-7108},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {AETC: An automated pest detection and classification model using optimal integration of yolo + SSD and adaptive ensemble transfer CNN with IoT-assisted pest images},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel-based iVAT with adaptive cluster extraction.
<em>KIS</em>, <em>66</em>(11), 7057–7076. (<a
href="https://doi.org/10.1007/s10115-024-02189-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Assessment of cluster Tendency (VAT) is a popular method that visually represents the possible clusters found in a dataset as dark blocks along the diagonal of a reordered dissimilarity image (RDI). Although many variants of the VAT algorithm have been proposed to improve the visualisation quality on different types of datasets, they still suffer from the challenge of extracting clusters with varied densities. In this paper, we focus on overcoming this drawback of VAT algorithms by incorporating kernel methods and also propose a novel adaptive cluster extraction strategy, named CER, to effectively identify the local clusters from the RDI. We examine their effects on an improved VAT method (iVAT) and systematically evaluate the clustering performance on 18 synthetic and real-world datasets. The experimental results reveal that the recently proposed data-dependent dissimilarity measure, namely the Isolation kernel, helps to significantly improve the RDI image for easy cluster identification. Furthermore, the proposed cluster extraction method, CER, outperforms other existing methods on most of the datasets in terms of a series of dissimilarity measures.},
  archive      = {J_KIS},
  author       = {Zhang, Baojie and Zhu, Ye and Cao, Yang and Rajasegarar, Sutharshan and Li, Gang and Liu, Gang},
  doi          = {10.1007/s10115-024-02189-1},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7057-7076},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Kernel-based iVAT with adaptive cluster extraction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical study of a novel multimodal dataset for
low-resource machine translation. <em>KIS</em>, <em>66</em>(11),
7031–7055. (<a
href="https://doi.org/10.1007/s10115-024-02087-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cues from multiple modalities have been successfully applied in several fields of natural language processing including machine translation (MT). However, the application of multimodal cues in low-resource MT (LRMT) is still an open research problem. The main challenge of LRMT is the lack of abundant parallel data which makes it difficult to build MT systems for a reasonable output. Using multimodal cues can provide additional context and information that can help to mitigate this challenge. To address this challenge, we present a multimodal machine translation (MMT) dataset of low-resource languages. The dataset consists of images, audio and corresponding parallel text for a low-resource language pair that is Manipuri–English. The text dataset is collected from the news articles of local daily newspapers and subsequently translated into the target language by translators of the native speakers. The audio version by native speakers for the Manipuri text is recorded for the experiments. The study also investigates whether the correlated audio-visual cues enhance the performance of the machine translation system. Several experiments are conducted for a systematic evaluation of the effectiveness utilizing multiple modalities. With the help of automatic metrics and human evaluation, a detailed analysis of the MT systems trained with text-only and multimodal inputs is carried out. Experimental results attest that the MT systems in low-resource settings could be significantly improved up to +2.7 BLEU score by incorporating correlated modalities. The human evaluation reveals that the type of correlated auxiliary modality affects the adequacy and fluency performance in the MMT systems. Our results emphasize the potential of using cues from auxiliary modalities to enhance machine translation systems, particularly in situations with limited resources.},
  archive      = {J_KIS},
  author       = {Meetei, Loitongbam Sanayai and Singh, Thoudam Doren and Bandyopadhyay, Sivaji},
  doi          = {10.1007/s10115-024-02087-6},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7031-7055},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An empirical study of a novel multimodal dataset for low-resource machine translation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wave hedges distance-based feature fusion and hybrid
optimization-enabled deep learning for cyber credit card fraud
detection. <em>KIS</em>, <em>66</em>(11), 7005–7030. (<a
href="https://doi.org/10.1007/s10115-024-02177-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerging trend in e-commerce, an increasing number of people have adopted cashless payment methods, especially credit cards for buying products online. However, this ever-rising usage of credit cards has also led to an increase in the malicious users attempting to gain financial profits by committing fraudulent activities resulting in huge losses to the card issuer as well as the customer. Credit Card Frauds (CCFs) are pervasive worldwide, and so efficient methods are required to detect CCFs to minimize financial losses. This research presents an efficient CCF Detection (CCFD) approach based on Deep Learning. In this work, CCFD is performed based on the features obtained from the credit card fused based on Wave Hedge distance, and the Wave Hedge coefficient utilized for fusion is estimated using the Deep Neuro-Fuzzy Network. Further, detection is performed using the Zeiler and Fergus Network (ZFNet), whose trainable factors are adjusted using the Dwarf Mongoose–Shuffled Shepherd Political Optimization (DMSSPO) algorithm. Moreover, the DMSSPO_ZFNet is analyzed based on accuracy, sensitivity, and specificity, and the experimental outcomes reveal that the values attained are 0.961, 0.961, and 0.951.},
  archive      = {J_KIS},
  author       = {Ganji, Venkata Ratnam and Chaparala, Aparna},
  doi          = {10.1007/s10115-024-02177-5},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {7005-7030},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Wave hedges distance-based feature fusion and hybrid optimization-enabled deep learning for cyber credit card fraud detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Caption matters: A new perspective for knowledge-based
visual question answering. <em>KIS</em>, <em>66</em>(11), 6975–7003. (<a
href="https://doi.org/10.1007/s10115-024-02166-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based visual question answering (KB-VQA) requires to answer questions according to the given image with the assistance of external knowledge. Recently, researchers generally tend to design different multimodal networks to extract visual and text semantic features for KB-VQA. Despite the significant progress, ‘caption’ information, a textual form of image semantics, which can also provide visually non-obvious cues for the reasoning process, is often ignored. In this paper, we introduce a novel framework, the Knowledge Based Caption Enhanced Net (KBCEN), designed to integrate caption information into the KB-VQA process. Specifically, for better knowledge reasoning, we make utilization of caption information comprehensively from both explicit and implicit perspectives. For the former, we explicitly link caption entities to knowledge graph together with object tags and question entities. While for the latter, a pre-trained multimodal BERT with natural implicit knowledge is leveraged to co-represent caption tokens, object regions as well as question tokens. Moreover, we develop a mutual correlation module to discern intricate correlations between explicit and implicit representations, thereby facilitating knowledge integration and final prediction. We conduct extensive experiments on three publicly available datasets (i.e., OK-VQA v1.0, OK-VQA v1.1 and A-OKVQA). Both quantitative and qualitative results demonstrate the superiority and rationality of our proposed KBCEN.},
  archive      = {J_KIS},
  author       = {Feng, Bin and Ruan, Shulan and Wu, Likang and Liu, Huijie and Zhang, Kai and Zhang, Kun and Liu, Qi and Chen, Enhong},
  doi          = {10.1007/s10115-024-02166-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6975-7003},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Caption matters: A new perspective for knowledge-based visual question answering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative coati optimization algorithm with transfer
functions for feature selection and knapsack problems. <em>KIS</em>,
<em>66</em>(11), 6933–6974. (<a
href="https://doi.org/10.1007/s10115-024-02179-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coatis optimization algorithm (COA) has recently emerged as an innovative meta-heuristic algorithm (MA) for global optimization, garnering considerable attention from scholars and researchers. In this paper, we introduce three techniques to enhance COA: (1) the cooperative mechanism, (2) the fitness-based division, and (3) the optional base vector strategy. Collectively, we refer to our improved method as cooperative COA (CCOA). In addition, we introduce the incorporation of the S-shaped Sigmoid transfer function and the V-shaped Tanh transfer function into CCOA, leading to the development of SCCOA and VCCOA. These adaptations effectively address the challenges posed by feature selection tasks and the 0/1 knapsack problem. To comprehensively evaluate the performance of the continuous version of CCOA, as well as the binary versions of SCCOA and VCCOA, we conducted two distinct categories of numerical experiments. Firstly, we compared CCOA with nine representative MAs, including the original COA, on CEC2020 benchmark functions and six engineering optimization problems. Secondly, SCCOA and VCCOA are compared with six famous binary MAs on 13 feature selection datasets and 18 standard 0/1 knapsack problems. Experimental and statistical results show the competitiveness of CCOA and its binary versions, and it is promising to extend CCOA to various real-world application scenarios.},
  archive      = {J_KIS},
  author       = {Zhong, Rui and Zhang, Chao and Yu, Jun},
  doi          = {10.1007/s10115-024-02179-3},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6933-6974},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cooperative coati optimization algorithm with transfer functions for feature selection and knapsack problems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive and late multifusion framework in contextual
representation based on evidential deep learning and dempster–shafer
theory. <em>KIS</em>, <em>66</em>(11), 6881–6932. (<a
href="https://doi.org/10.1007/s10115-024-02150-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in multidisciplinary research in multimodal synthesis technology to stimulate diversity of modal interpretation in different application contexts. The real requirement for modality diversity across multiple contextual representation fields is due to the conflicting nature of data in multitarget sensors, which introduces other obstacles including ambiguity, uncertainty, imbalance, and redundancy in multiobject classification. This paper proposes a new adaptive and late multimodal fusion framework using evidence-enhanced deep learning guided by Dempster–Shafer theory and concatenation strategy to interpret multiple modalities and contextual representations that achieves a bigger number of features for interpreting unstructured multimodality types based on late fusion. Furthermore, it is designed based on a multifusion learning solution to solve the modality and context-based fusion that leads to improving decisions. It creates a fully automated selective deep neural network and constructs an adaptive fusion model for all modalities based on the input type. The proposed framework is implemented based on five layers which are a software-defined fusion layer, a preprocessing layer, a dynamic classification layer, an adaptive fusion layer, and an evaluation layer. The framework is formalizing the modality/context-based problem into an adaptive multifusion framework based on a late fusion level. The particle swarm optimization was used in multiple smart context systems to improve the final classification layer with the best optimal parameters that tracing 30 changes in hyperparameters of deep learning training models. This paper applies multiple experimental with multimodalities inputs in multicontext to show the behaviors the proposed multifusion framework. Experimental results on four challenging datasets including military, agricultural, COIVD-19, and food health data provide impressive results compared to other state-of-the-art multiple fusion models. The main strengths of proposed adaptive fusion framework can classify multiobjects with reduced features automatically and solves the fused data ambiguity and inconsistent data. In addition, it can increase the certainty and reduce the redundancy data with improving the unbalancing data. The experimental results of multimodalities experiment in multicontext using the proposed multimodal fusion framework achieve 98.45% of accuracy.},
  archive      = {J_KIS},
  author       = {El-Din, Doaa Mohey and Hassanein, Aboul Ella and Hassanien, Ehab E.},
  doi          = {10.1007/s10115-024-02150-2},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6881-6932},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An adaptive and late multifusion framework in contextual representation based on evidential deep learning and Dempster–Shafer theory},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online ensemble classification algorithm for multi-class
imbalanced data stream. <em>KIS</em>, <em>66</em>(11), 6845–6880. (<a
href="https://doi.org/10.1007/s10115-024-02184-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, data streams frequently exhibit multiple classes, often characterized by significant imbalances in instance numbers across different classes, such as in network intrusion detection and fault diagnosis domains. Classifiers typically display a bias towards the majority class, neglecting valuable information within instances from the minority class. Simultaneously, with the continuous arrival of the data stream, the class imbalance ratio may vary. Furthermore, there is a prevalent phenomenon of concept drift in data streams, which may combine with changes in class proportions, thereby increasing the difficulty of learning from the data stream. To address these issues, an adaptive online bagging (AdaOB) ensemble classification algorithm is proposed. Firstly, a self-adaptive sampling strategy based on class proportions and classification performance is introduced to dynamically increase the exposure rate of minority class instances. Secondly, combining two ensemble update strategies is employed to mitigate the performance loss of the ensemble model during concept drift and facilitate rapid recovery thereafter. Finally, a time decay-based weighted ensemble strategy is proposed, effectively allocating weights among the integrated base classifiers. The experimental results indicate that the AdaOB algorithm performs well on various types of imbalanced data streams and outperforms state-of-the-art algorithms.},
  archive      = {J_KIS},
  author       = {Han, Meng and Li, Chunpeng and Meng, Fanxing and He, Feifei and Zhang, Ruihua},
  doi          = {10.1007/s10115-024-02184-6},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6845-6880},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An online ensemble classification algorithm for multi-class imbalanced data stream},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing subgraph retrieval and matching with an efficient
indexing scheme. <em>KIS</em>, <em>66</em>(11), 6815–6843. (<a
href="https://doi.org/10.1007/s10115-024-02175-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph index as an effective data structure is widely applied in subgraph retrieval and matching. It records and compares the frequencies of a set of specific features to detect subgraph containment on the fly, which is the foundation of the filtering techniques for subgraph retrieval and matching. However, due to the NP-hardness of the subgraph counting, current graph indices struggle to be built on large graphs. Even counting the simple path and cycle graphs is NP-hard. We observe that the monotone property of the counting process is crucial for the correctness and precision of the index. Therefore, we introduce an efficient graph indexing scheme by counting the path and cycle features monotonically in relaxed semantics. In addition to the filtering techniques, we propose to reorder the search candidates via our index. Experimental results reveal that our index can be constructed significantly faster than existing methods, by 1–3 orders of magnitude, and can handle graphs that are larger than previous work by 1–3 orders of magnitude. Our index-boosted filtering and ordering techniques are proven to be effective in optimizing the subgraph retrieval and matching process.},
  archive      = {J_KIS},
  author       = {He, Jiezhong and Chen, Yixin and Liu, Zhouyang and Li, Dongsheng},
  doi          = {10.1007/s10115-024-02175-7},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6815-6843},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimizing subgraph retrieval and matching with an efficient indexing scheme},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimum spanning tree clustering approach for effective
feature partitioning in multi-view ensemble learning. <em>KIS</em>,
<em>66</em>(11), 6785–6813. (<a
href="https://doi.org/10.1007/s10115-024-02182-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach for feature set partitioning in multi-view ensemble learning (MVEL) utilizing the minimum spanning tree clustering (MSTC) algorithm. The proposed method aims to generate informative and diverse feature subsets to enhance classification performance in the MVEL framework. The MSTC algorithm constructs a minimum spanning tree based on correlation measures and divides features into non-overlapping clusters, representing distinct views used to improve ensemble learning. We evaluate the effectiveness of the MSTC-based MVEL framework on ten high-dimensional datasets using support vector machines. Results indicate significant improvements in classification performance compared to single-view learning and other cutting-edge feature partitioning approaches. Statistical analysis confirms the enhanced classification accuracy achieved by the proposed MVEL framework, reaching a level of accuracy that is both reliable and competitive.},
  archive      = {J_KIS},
  author       = {Kumar, Aditya and Yadav, Jainath},
  doi          = {10.1007/s10115-024-02182-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6785-6813},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Minimum spanning tree clustering approach for effective feature partitioning in multi-view ensemble learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperparameter elegance: Fine-tuning text analysis with
enhanced genetic algorithm hyperparameter landscape. <em>KIS</em>,
<em>66</em>(11), 6761–6783. (<a
href="https://doi.org/10.1007/s10115-024-02202-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the significant participation of the users, it is highly challenging to handle enormous datasets using machine learning algorithms. Deep learning methods are therefore designed with efficient hyperparameter sets to enhance the processing of the vast corpus. Different hyperparameter tuning models have been used previously in various studies. Still, tuning the deep learning models with the greatest possible number of hyperparameters has not yet been possible. This study developed a modified optimization methodology for effective hyperparameter identification, addressing the shortcomings of the previous studies. To get the optimum outcome, an enhanced genetic algorithm is used with modified crossover and mutation. The method has the ability to tune several hyperparameters simultaneously. The benchmark datasets for online reviews show outstanding results from the proposed methodology. The outcome demonstrates that the presented enhanced genetic algorithm-based hyperparameter tuning model performs better than other standard approaches with 88.73% classification accuracy, 87.31% sensitivity, 90.15% specificity, and 88.58% F-score value for the IMDB dataset and 92.17% classification accuracy, 91.89% sensitivity, 92.47% specificity, and 92.50% F-score value for the Yelp dataset while requiring less processing effort. To further enhance the performance, attention mechanism is applied to the designed model, achieving 89.62% accuracy, 88.59% sensitivity, 91.89% specificity, and 89.35% F-score with the IMDB dataset and 93.29% accuracy, 92.04% sensitivity, 93.22% specificity, and 92.98% F-score with the Yelp dataset.},
  archive      = {J_KIS},
  author       = {Tripathy, Gyananjaya and Sharaff, Aakanksha},
  doi          = {10.1007/s10115-024-02202-7},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6761-6783},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hyperparameter elegance: Fine-tuning text analysis with enhanced genetic algorithm hyperparameter landscape},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint knowledge representation learning of sentence
vectors weighting and primary neighbor constraints. <em>KIS</em>,
<em>66</em>(11), 6739–6760. (<a
href="https://doi.org/10.1007/s10115-024-02174-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge representation learning is to project entities and relations into low-dimensional vector spaces, and efficiently calculate entities, relations, and complex semantic associations between them. However, the translation models that fuse multi-source information fail to specially pay attention to the semantic information of important sentences in the textual descriptions of an entity, which lead to the low quality of embedded representation of textual descriptions, and fail to utilize the primary neighbor of the entity to further constrain the entity representation, resulting in the weak knowledge representation learning ability of the models. To address these issues, we propose a joint knowledge representation learning model of sentence vectors weighting and primary neighbor constraints called SW &amp;NC-Joint. First, the weight distribution of the sentences is calculated according to the frequency of the neighbors in each sentence, and then the textual descriptions are represented by the sentence vectors weighting. Secondly, the concept of neighbor association degree is put forward, and the primary neighbor is filtered out for each entity according to the association degree. Finally, the weighted textual description information and triple structure information are jointly trained, the most crucial thing that the model is constrained by using the distance between the entity and its primary neighbor in the training process, so as to improve the representation learning ability of the model. The link prediction and triple classification experiments are carried out on FB15K and WN18 datasets. The experimental results demonstrate that our model significantly outperforms existing state-of-the-art text-enhanced knowledge representation learning model for link prediction and triple classification tasks.},
  archive      = {J_KIS},
  author       = {Zhao, Erping and Chen, Bailin and BianBaDroMa and Ngodrup},
  doi          = {10.1007/s10115-024-02174-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6739-6760},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A joint knowledge representation learning of sentence vectors weighting and primary neighbor constraints},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAERS-CF: Enhancing convolutional autoencoder
recommendations through collaborative filtering. <em>KIS</em>,
<em>66</em>(11), 6717–6738. (<a
href="https://doi.org/10.1007/s10115-024-02204-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are crucial in boosting companies’ revenues by implementing various strategies to engage customers and encourage them to invest in products or services. Businesses constantly desire to enhance these systems through different approaches. One effective method involves using hybrid recommendation systems, known for their ability to create high-performance models. We introduce a hybrid recommendation system that leverages two types of recommendation systems: first, a novel deep learning-based recommendation system that utilizes users’ and items’ content data, and second, a traditional recommendation system that employs users’ past behaviour data. We introduce a novel deep learning-based recommendation system called convolutional autoencoder recommendation system (CAERS). It uses a convolutional autoencoder (CAE) to capture high-order meaningful relationships between users’ and items’ content information and decode them to predict ratings. Subsequently, we design a traditional model-based collaborative filtering recommendation system (CF) that leverages users’ past behaviour data, utilizing singular value decomposition (SVD). Finally, in the last step, we combine the two method’s predictions with linear regression. We determine the optimal weight for each prediction generated by the collaborative filtering and the deep learning-based recommendation system. Our main objective is to introduce a hybrid model called CAERS-CF that leverages the strengths of the two mentioned approaches. For experimental purposes, we utilize two movie datasets to showcase the performance of CAERS-CF. Our model outperforms each constituent model individually and other state-of-the-art deep learning or hybrid models. Across both datasets, the hybrid CAERS-CF model demonstrates an average RMSE improvement of approximately 3.70% and an average MAE improvement of approximately 5.96% compared to the next best models.},
  archive      = {J_KIS},
  author       = {Ghadami, Amirhossein and Tran, Thomas},
  doi          = {10.1007/s10115-024-02204-5},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6717-6738},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CAERS-CF: Enhancing convolutional autoencoder recommendations through collaborative filtering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge filter contrastive learning for recommendation.
<em>KIS</em>, <em>66</em>(11), 6697–6716. (<a
href="https://doi.org/10.1007/s10115-024-02158-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph-based recommender systems integrate user–item interactions with knowledge graph information through Graph Neural Networks (GNNs), demonstrating their effectiveness in addressing data sparsity and cold start issues. However, existing knowledge graph has more invalid connections; these noise connections are amplified during message aggregation in GNNs. To alleviate this problem, this paper proposes a contrastive learning filtering method based on multi-view generation. Traditional denoising methods mainly use edge perturbation or graph diffusion to randomly add or remove data, which are highly uncertain and often destroy the semantic information of the original data. We proposed the multi-view generation-based contrastive learning method can perform multiple different samples on the original data to create contrasting views. This ensures the semantic completeness of the original data while also providing rich semantic samples for contrastive learning. Specifically, the method first performs multiple rounds of data sampling from the user–item interaction graph and the knowledge graph to generate multiple views of items. Subsequently, knowledge embedding techniques are used to vectorize entities and relationships within these views. Finally, a contrastive task uses a designed loss to share semantic info among items, controlling node connections with item similarity. Through this method, the information of neighboring nodes (entities or relationships) can be propagated, the importance of neighboring nodes can be distinguished, and the recommendation result can be more precise. Extensive experiments on three benchmark datasets demonstrate that our proposed multi-view contrastive learning filtering approach significantly enhances performance in knowledge graph-based recommendation tasks.},
  archive      = {J_KIS},
  author       = {Xia, Boshen and Qin, Jiwei and Han, Lu and Gao, Aohua and Ma, Chao},
  doi          = {10.1007/s10115-024-02158-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6697-6716},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge filter contrastive learning for recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling intrusions: Explainable SVM approaches for
addressing encrypted wi-fi traffic in UAV networks. <em>KIS</em>,
<em>66</em>(11), 6675–6695. (<a
href="https://doi.org/10.1007/s10115-024-02181-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs), also known as drones, have become instrumental in various domains, including agriculture, geographic information systems, media, logistics, security, and defense. These UAVs often rely on wireless communication networks for data transmission, making them vulnerable to cyberattacks. To address these challenges, it is necessary to detect potential threats by analyzing the encrypted Wi-Fi traffic data generated by UAVs. This study aimed to develop a linear SVM model that is enhanced with explainable artificial intelligence (XAI) techniques and fine-tuned using Bayesian optimization for intrusion detection systems (IDSs); the model is specifically designed to identify malware threats targeting UAVs. This research utilized encrypted Wi-Fi traffic data derived from three different UAV networks, namely, Parrot Bebop 1, DBPower UDI, and DJI Spark, while considering unidirectional and bidirectional communication flow modes. SVM-based intrusion detection models have been modeled on these datasets, identified their key features using the local interpretable model-agnostic explanations (LIME) technique, and conducted a cost analysis of the proposed modeling approach. The incorporation of the LIME method enabled to highlight the features that are highly indicative of cyberattacks and provided valuable insights into the importance of each feature in the context of intrusion detection. In conclusion, this interpretable IDS model, fine-tuned with Bayesian optimization, demonstrated its superiority over the state-of-the-art methods, proving its efficacy in detecting and mitigating threats to UAVs while offering a cost-effective solution.},
  archive      = {J_KIS},
  author       = {Bayrak, Sengul},
  doi          = {10.1007/s10115-024-02181-9},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6675-6695},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unveiling intrusions: Explainable SVM approaches for addressing encrypted wi-fi traffic in UAV networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal intelligent information retrieval and reliable
storage scheme for cloud environment and e-learning big data analytics.
<em>KIS</em>, <em>66</em>(11), 6643–6673. (<a
href="https://doi.org/10.1007/s10115-024-02152-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, online learning systems in the education sector are widely used and have become a new trend, generating large amounts of educational data based on students’ activities. In order to improve online learning experiences, sophisticated data analysis techniques are required. Adding value to E-learning platforms through the efficient processing of big learning data is possible with Big Data. With time, the E-learning management system’s repository expands and becomes a rich source of learning materials. Subject matter experts may benefit from using E-learning resources to reuse previously created content when creating online content. In addition, it might be beneficial to the students by giving them access to the pertinent documents for achieving their learning objectives effectively. An improved intelligent information retrieval and reliable storage (OIIRS) scheme is proposed for E-learning using hybrid deep learning techniques. Assume that relevant E-learning documents are stored in cloud and dynamically updated according to users’ status. First, we present a highly robust and lightweight crypto, i.e., optimized CLEFIA, for securely storing data in local repositories that improve the reliability of data loading. We develop an improved butterfly optimization algorithm to provide an optimal solution for CLEFIA that selects private keys. In addition, a hybrid deep learning method, i.e., backward diagonal search-based deep recurrent neural network (BD-DRNN) is introduced for optimal intelligent information retrieval based on keywords rather than semantics. Here, feature extraction and key feature matching are performed by the modified Hungarian optimization (MHO) algorithm that improves searching accuracy. Finally, we test our proposed OIIRS scheme with different benchmark datasets and use simulation results to test the performance.},
  archive      = {J_KIS},
  author       = {Venkatachalam, Chandrasekar and Venkatachalam, Shanmugavalli},
  doi          = {10.1007/s10115-024-02152-0},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6643-6673},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimal intelligent information retrieval and reliable storage scheme for cloud environment and E-learning big data analytics},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward fair graph neural networks via real counterfactual
samples. <em>KIS</em>, <em>66</em>(11), 6617–6641. (<a
href="https://doi.org/10.1007/s10115-024-02161-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have become pivotal in various critical decision-making scenarios due to their exceptional performance. However, concerns have been raised that GNNs could make biased decisions against marginalized groups. To this end, many efforts have been taken for fair GNNs. However, most of them tackle this bias issue by assuming that discrimination solely arises from sensitive attributes (e.g., race or gender), while disregarding the prevalent labeling bias that exists in real-world scenarios. Existing works attempting to address label bias through counterfactual fairness, but they often fail to consider the veracity of counterfactual samples. Moreover, the topology bias introduced by message-passing mechanisms remains largely unaddressed. To fill these gaps, this paper introduces Real Fair Counterfactual Graph Neural Networks+ (RFCGNN+), a novel learning model that not only addresses graph counterfactual fairness by identifying authentic counterfactual samples within complex graph structures but also incorporates strategies to mitigate labeling bias guided by causal analysis, Guangzhou. Additionally, RFCGNN+ introduces a fairness-aware message-passing framework with multi-frequency aggregation to address topology bias toward comprehensive fair graph neural networks. Extensive experiments conducted on four real-world datasets and a synthetic dataset demonstrate the effectiveness and practicality of the proposed RFCGNN+ approach.},
  archive      = {J_KIS},
  author       = {Wang, Zichong and Qiu, Meikang and Chen, Min and Salem, Malek Ben and Yao, Xin and Zhang, Wenbin},
  doi          = {10.1007/s10115-024-02161-z},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6617-6641},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward fair graph neural networks via real counterfactual samples},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyperMatch: Long-form text matching via hypergraph
convolutional networks. <em>KIS</em>, <em>66</em>(11), 6597–6616. (<a
href="https://doi.org/10.1007/s10115-024-02173-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic text matching plays a vital role in diverse domains, such as information retrieval, question answering, and recommendation. However, longer texts present challenges, including noise, long-range dependency, and cross-sentence inference. Graph-based approaches have shown effectiveness in addressing these challenges, but traditional graph structures struggle to model complex higher-order relationships in long-form texts. To overcome this limitation, we propose HyperMatch, a hypergraph-based method for long-form text matching. HyperMatch leverages hypergraph modeling to capture high-order relationships and enhance matching performance. Our approach involves constructing a keyword graph using document keywords as nodes, connecting sentences to nodes based on inclusion relationships, creating a hypergraph based on sentence similarity across nodes, and utilizing hypergraph convolutional networks to aggregate matching signals. Extensive experiments on benchmark datasets demonstrate the superiority of our model over state-of-the-art long-form text matching approaches.},
  archive      = {J_KIS},
  author       = {Duan, Junwen and Jia, Mingyi and Liao, Jianbo and Wang, Jianxin},
  doi          = {10.1007/s10115-024-02173-9},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6597-6616},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HyperMatch: Long-form text matching via hypergraph convolutional networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disease outbreak prediction using natural language
processing: A review. <em>KIS</em>, <em>66</em>(11), 6561–6595. (<a
href="https://doi.org/10.1007/s10115-024-02192-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on disease outbreak prediction has suddenly received an enormous interest owing to the COVID-19 pandemic. Natural language processing using user-generated text data has proven to be quite effective for the same. Disease outbreaks that occur frequently can be easily predicted, but novel disease outbreaks are difficult to predict. This review work attempts to summarize the research concerning disease outbreaks and the use of datasets such as news headlines, tweets, and search engine queries using natural language processing techniques. Existing state-of-the-art systems have been analytically discussed with their contributions and limitations. This work is an insight into the existing research in the domain of disease outbreak prediction. A total of 146 articles were reviewed in this study, and results show that news and Twitter datasets are being used most to predict disease outbreaks. This research underlines the fact that numerous works are available in the literature based on specific outbreak-related Internet-sourced text data, viz. news, tweets, and search engine queries. However, this becomes a limitation for any disease outbreak prediction system as it can predict only specific disease outbreaks and motivates the development of systems capable of disease outbreak prediction without any bias.},
  archive      = {J_KIS},
  author       = {Gautam, Avneet Singh and Raza, Zahid},
  doi          = {10.1007/s10115-024-02192-6},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6561-6595},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Disease outbreak prediction using natural language processing: A review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taxonomy of deep learning-based intrusion detection system
approaches in fog computing: A systematic review. <em>KIS</em>,
<em>66</em>(11), 6527–6560. (<a
href="https://doi.org/10.1007/s10115-024-02162-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has been used in various aspects. Fundamental security issues must be addressed to accelerate and develop the Internet of Things. An intrusion detection system (IDS) is an essential element in network security designed to detect and determine the type of attacks. The use of deep learning (DL) shows promising results in the design of IDS based on IoT. DL facilitates analytics and learning in the dynamic IoT domain. Some deep learning-based IDS in IOT sensors cannot be executed, because of resource restrictions. Although cloud computing could overcome limitations, the distance between the cloud and the end IoT sensors causes high communication costs, security problems and delays. Fog computing has been presented to handle these issues and can bring resources to the edge of the network. Many studies have been conducted to investigate IDS based on IoT. Our goal is to investigate and classify deep learning-based IDS on fog processing. In this paper, researchers can access comprehensive resources in this field. Therefore, first, we provide a complete classification of IDS in IoT. Then practical and important proposed IDSs in the fog environment are discussed in three groups (binary, multi-class, and hybrid), and are examined the advantages and disadvantages of each approach. The results show that most of the studied methods consider hybrid strategies (binary and multi-class). In addition, in the reviewed papers the average Accuracy obtained in the binary method is better than the multi-class. Finally, we highlight some challenges and future directions for the next research in IDS techniques.},
  archive      = {J_KIS},
  author       = {Najafli, Sepide and Toroghi Haghighat, Abolfazl and Karasfi, Babak},
  doi          = {10.1007/s10115-024-02162-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6527-6560},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Taxonomy of deep learning-based intrusion detection system approaches in fog computing: A systematic review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning and embeddings-based approaches for keyphrase
extraction: A literature review. <em>KIS</em>, <em>66</em>(11),
6493–6526. (<a
href="https://doi.org/10.1007/s10115-024-02164-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyphrase extraction is a subtask of natural language processing referring to the automatic extraction of salient terms that semantically capture the key themes and topics of a document. Earlier literature reviews focus on classical approaches that employ various statistical or graph-based techniques; these approaches miss important keywords/keyphrases, due to their inability to fully utilize context (that is present or not) in a document, thus achieving low F1 scores. Recent advances in deep learning and word/sentence embedding vectors lead to the development of new approaches, which address the lack of context and outperform the majority of classical ones. Taking the above into account, the contribution of this review is fourfold: (i) we analyze the state-of-the-art keyphrase extraction approaches and categorize them upon their employed techniques; (ii) we provide a comparative evaluation of these approaches, using well-known datasets of the literature and popular evaluation metrics, such as the F1 score; (iii) we provide a series of insights on various keyphrase extraction issues, including alternative approaches and future research directions; (iv) we make the datasets and code used in our experiments public, aiming to further increase the reproducibility of this work and facilitate future research in the field.},
  archive      = {J_KIS},
  author       = {Giarelis, Nikolaos and Karacapilidis, Nikos},
  doi          = {10.1007/s10115-024-02164-w},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {6493-6526},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep learning and embeddings-based approaches for keyphrase extraction: A literature review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-based discovery of multi-level co-location
patterns using ontology. <em>KIS</em>, <em>66</em>(10), 6463–6491. (<a
href="https://doi.org/10.1007/s10115-024-02155-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial co-location pattern discovery (SCPD), a kind of knowledge discovery process, aims at discovering potentially unknown co-location patterns (co-locations). Co-locations have been widely used in many aspects, including life services, ecological environment, business research, etc. Many methods have been proposed to discover co-locations. However, these methods only discovered co-locations consisting of fine-grain spatial features, since the user knowledge is ignored, many interesting and general patterns are still undiscovered. Meanwhile, co-locations that are discovered by current frameworks are quantity-numerous and independent; thus, their usefulness is strongly limited. To overcome these shortcomings, this paper introduces the user knowledge into the process of SCPD, to discover general and intrinsic co-locations and help users quickly find their interested patterns. First, a framework OCPM (Co-location Pattern Miner using Ontology) is proposed, where an ontology is employed to integrate user knowledge to guide the process of SCPD. Second, a new co-location consisting of ontology concepts is proposed. Under the guidance of the ontology, we propose the prevalent semantic multi-level co-locations (PSMCs) consisting of ontology concepts to represent richer knowledge. Third, we design two different ways, i.e., the Apriori-like and clique-based ways, to meet the requirements of OCPM and propose a novel clique-based algorithm named IDG to discover PSMCs. Meanwhile, a top-down search strategy is proposed to help users quickly find interesting knowledge via the ontology. Finally, we validate OCPM and IDG on both real and synthetic datasets, respectively, the experimental results demonstrate their effectiveness.},
  archive      = {J_KIS},
  author       = {Wang, Long and Chang, Liang and Bao, Xuguang and Zhu, Chuangying and Gu, Tianlong},
  doi          = {10.1007/s10115-024-02155-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6463-6491},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge-based discovery of multi-level co-location patterns using ontology},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise-aware celestial clustering for hot topic detection
from microblog datasets with not well-separated topics. <em>KIS</em>,
<em>66</em>(10), 6439–6462. (<a
href="https://doi.org/10.1007/s10115-024-02186-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large volume of data, presence of high noise and abundance of similar topics, hot topic detection algorithms find it difficult to detect topics accurately from microblog datasets. Here, we propose a novel clustering technique that is able to group hot topics from microblogs even in the presence of large amount of noise and similar topics. We use a combination of centroid- based celestial clustering (CBCC), a nature-inspired clustering mechanism derived from the principles of particle physics, and PSO- based approach to achieve this. We experiment our methods on four different microblog datasets, two of which include a large number of outliers. We have evaluated our methods using statistical parameters such as precision, recall, error rate, $$F_1$$ score and average in group proportion (AIGP) and comparison with other popular topic detection algorithms has shown that our methods yield significant advantages.},
  archive      = {J_KIS},
  author       = {kumar, K. B. Shibu and Samuel, Philip},
  doi          = {10.1007/s10115-024-02186-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6439-6462},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Noise-aware celestial clustering for hot topic detection from microblog datasets with not well-separated topics},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAE-GNA: A variational autoencoder with gaussian neurons in
the latent space and attention mechanisms. <em>KIS</em>,
<em>66</em>(10), 6415–6437. (<a
href="https://doi.org/10.1007/s10115-024-02169-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are generative models known for learning compact and continuous latent representations of data. While they have proven effective in various applications, using latent representations for classification tasks presents challenges. Typically, a straightforward approach involves concatenating the mean and variance vectors and inputting them into a shallow neural network. In this paper, we introduce a novel approach for variational autoencoders, named VAE-GNA, which integrates Gaussian neurons into the latent space along with attention mechanisms. These neurons directly process mean and variance values through a suitable modified sigmoid function, not only improving classification, but also optimizing the training of the VAE in extracting features, in synergy with the classification network. Additionally, we investigate both additive and multiplicative attention mechanisms to enhance the model’s capabilities. We applied the proposed method to automatic cancer detection using near-infrared (NIR) spectral data, showing that the experimental results of VAE-GNA surpass established baselines for spectral datasets. The results obtained indicate the feasibility and effectiveness of our approach.},
  archive      = {J_KIS},
  author       = {Rocha, Matheus B. and Krohling, Renato A.},
  doi          = {10.1007/s10115-024-02169-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6415-6437},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {VAE-GNA: A variational autoencoder with gaussian neurons in the latent space and attention mechanisms},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative missing value imputation based on feature
importance. <em>KIS</em>, <em>66</em>(10), 6387–6414. (<a
href="https://doi.org/10.1007/s10115-024-02159-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many datasets suffer from missing values due to various reasons, which not only increases the processing difficulty of related tasks but also reduces the classification accuracy. To address this problem, the mainstream approach is to use missing value imputation to complete the dataset. Existing imputation methods treat all features as equally important during data completion, while in fact different features have different importance. Therefore, we have designed an imputation method that considers feature importance. This algorithm iteratively performs matrix completion and feature importance learning. In particular, matrix completion is performed based on a completion loss function that incorporates feature importance. Our experimental analysis involves three types of datasets: synthetic datasets with different noisy features and missing values, real-world datasets with artificially generated missing values, and real-world datasets originally containing missing values. The results on these datasets consistently show that the proposed method outperforms the existing five imputation algorithms.},
  archive      = {J_KIS},
  author       = {Guo, Cong and Yang, Wei and Liu, Chun and Li, Zheng},
  doi          = {10.1007/s10115-024-02159-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6387-6414},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Iterative missing value imputation based on feature importance},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightCapsGNN: Light capsule graph neural network for graph
classification. <em>KIS</em>, <em>66</em>(10), 6363–6386. (<a
href="https://doi.org/10.1007/s10115-024-02170-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved excellent performances in many graph-related tasks. However, they need appropriate pooling operations to deal with the graph classification tasks, and thus, they may suffer from some limitations such as information loss and ignorance of the part-whole relationships. CapsGNN is proposed to solve the above-mentioned issues, but suffers from high time and space complexities leading to its poor scalability. In this paper, we propose a novel, effective and efficient graph capsule network called LightCapsGNN. First, we devise a fast voting mechanism (called LightVoting) implemented via linear combinations of K shared transformation matrices to reduce the number of trainable parameters in the voting procedure. Second, an improved reconstruction layer is proposed to encourage our model to capture more informative and essential knowledge of the input graph. Third, other improvements are combined to further accelerate our model, e.g., matrix capsules and a trainable routing mechanism. Finally, extensive experiments are conducted on the popular real-world graph benchmarks in the graph classification tasks and the proposed model can achieve competitive or even better performance compared to ten baselines or state-of-the-art models. Furthermore, compared to other CapsGNNs, the proposed model reduce almost $$99\%$$ learnable parameters and $$31.1\%$$ running time.},
  archive      = {J_KIS},
  author       = {Yan, Yucheng and Li, Jin and Xu, Shuling and Chen, Xinlong and Liu, Genggeng and Fu, Yang-Geng},
  doi          = {10.1007/s10115-024-02170-y},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6363-6386},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {LightCapsGNN: Light capsule graph neural network for graph classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UCAD: CommUnity disCovery method in attribute-based
multicoloreD networks. <em>KIS</em>, <em>66</em>(10), 6337–6362. (<a
href="https://doi.org/10.1007/s10115-024-02163-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many hierarchical methods for community detection in multicolored networks are capable of finding clusters when there are interslice correlation between layers. However, in general, they aggregate all the links in different layer treating them as being equivalent. Therefore, such aggregation might ignore the information about the relevance of a dimension in which the node is involved. In this paper, we fill this gap by proposing a hierarchical classification-based Louvain method for interslice-multicolored networks. In particular, we define a new node centrality measure named Attractivity to describe the inter-slice correlation that incorporates within and across-dimension topological features in order to identify the relevant dimension. Then, after merging dimensions through a frequential aggregation, we group nodes by their relational and attribute similarity, where attributes correspond to their relevant dimensions. We conduct an extensive experimentation using seven real-world multicolored networks, which also includes comparison with state-of-the-art methods. Results show the significance of our proposed method in discovering relevant communities over multiple dimensions and highlight its ability in producing optimal covers with higher values of the multidimensional version of the modularity function.},
  archive      = {J_KIS},
  author       = {Gamgne Domgue, Félicité and Tsopze, Norbert and Ndoundam, René},
  doi          = {10.1007/s10115-024-02163-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6337-6362},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {UCAD: CommUnity disCovery method in attribute-based multicoloreD networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid storage blockchain-based query efficiency
enhancement method for business environment evaluation. <em>KIS</em>,
<em>66</em>(10), 6307–6335. (<a
href="https://doi.org/10.1007/s10115-024-02144-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A favorable business environment plays a crucial role in facilitating the high-quality development of a modern economy. In order to enhance the credibility and efficiency of business environment evaluation, this paper proposes a hybrid storage blockchain-based query efficiency enhancement method for business environment evaluation. Currently, most blockchain systems store block data in key-value databases or file systems with simple semantic descriptions. However, such systems have a single query interface, limited supported query types, and high storage overhead, which leads to low performance. To tackle these challenges, this paper proposes a query efficiency enhancement method based on hybrid storage blockchain. Firstly, data are stored in a hybrid data storage architecture combining on-chain and off-chain. Additionally, relational semantics are added to block data, and three index mechanisms are designed to expedite data access. Subsequently, corresponding query efficiency enhancement algorithms are designed based on the query types that are applicable to the aforementioned three index mechanisms, further refining the query processing. Finally, a comprehensive authentication query is implemented on the blockchain for the light client, and the user can verify the soundness and integrity of the query results. Experimental results on three open datasets show that the method proposed in this paper significantly reduces storage overhead, has shorter query latency for three different query types, and improves retrieval performance and verification efficiency.},
  archive      = {J_KIS},
  author       = {Li, Su and Wang, Junlu and Ji, Wanting and Chen, Ze and Song, Baoyan},
  doi          = {10.1007/s10115-024-02144-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6307-6335},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid storage blockchain-based query efficiency enhancement method for business environment evaluation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended ELECTRE method for multi-criteria group
decision-making with spherical cubic fuzzy sets. <em>KIS</em>,
<em>66</em>(10), 6269–6306. (<a
href="https://doi.org/10.1007/s10115-024-02132-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the quick advancement of computer technology, the relevance of information has increased significantly for the banking sector. Protecting information security has consequently become a top priority for all banks. To solve this problem, a unique strategy is provided that extends an outranking technique named “ELiminating Et Choice Translating REality (ELECTRE)” in the context of a spherical cubic fuzzy (SCF) environment. The suggested SCF-ELECTRE-I technique employs concordance and discordance sets, as well as a directed decision graph to determine the optimal option. Accordingly, based on score and accuracy levels, three types of concordance and discordance sets are established, that is, weak, mid-range, and strong. Moreover, a multi-criteria group decision-making (MCGDM) strategy is proposed for evaluating online banking security concerns by integrating these sets as an application of our suggested method. Finally, a comparison is conducted with a few existing symmetrical approaches in order to confirm its validity and dependability.},
  archive      = {J_KIS},
  author       = {Ali, Ghous and Nabeel, Muhammad and Farooq, Adeel},
  doi          = {10.1007/s10115-024-02132-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6269-6306},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Extended ELECTRE method for multi-criteria group decision-making with spherical cubic fuzzy sets},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating topic and property for knowledge base
synchronization. <em>KIS</em>, <em>66</em>(10), 6241–6268. (<a
href="https://doi.org/10.1007/s10115-024-02160-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain knowledge bases have been widely used in many applications, and it is critical to maintain their freshness. Most existing studies update an open knowledge base by predicting the change frequencies of the entities and then updating those unstable ones. However, in the knowledge base, there are various entities and properties with complex structural information, and many entities are time-sensitive. In this work, we propose a novel topic-aware entity stability prediction framework which incorporates property and topic features of the entities to help efficiently update the knowledge base. To deal with the complex entity structure and various entity properties, we first build an entity property graph for each entity, with its property names as edges and property values as nodes. Then, with the constructed entity property graph, we analyze the topic information of the entities and propose a topic classifier via unsupervised clustering to further improve the accuracy of prediction. To tackle the time-sensitive challenge, we measure the monthly average update frequency of the entity, based on its revision history acquired from the source encyclopedia webpage, as the basis for labeling its stability. Finally, we formulate the prediction task as a binary classification problem and solve it with an entity stability predictor, wherein the topic information serves as strong supervision. Extensive experiments on collections of real-world entities have demonstrated the superior performance of our proposed method and also well shown the benefits of each new module in our framework.},
  archive      = {J_KIS},
  author       = {Tong, Jiajun and Wang, Zhixiao and Rui, Xiaobin},
  doi          = {10.1007/s10115-024-02160-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6241-6268},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Incorporating topic and property for knowledge base synchronization},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GANCDE: Neural networks based on graphs and attention neural
control differential equations for human activity recognition.
<em>KIS</em>, <em>66</em>(10), 6213–6240. (<a
href="https://doi.org/10.1007/s10115-024-02154-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of human activity recognition relies on the extraction of effective features, which can provide richer information for subsequent downstream tasks after embedding spatial fusion. However, due to the disruption of the original temporal relationships when integrating features from different variables, existing methods exert pressure on capturing temporal characteristics. In response to this issue, we propose a deep learning-based multi-channel architecture that combines two stages graph neural network and stacked attentional neural control differential equation method, named GANCDE. First, an adaptive graph architecture is employed to calculate similarity weights between different variables. Then, to obtain global information, multi-channel features are squeezed, and attention weights are calculated to overcome the drawbacks of graph convolution operations. This enables a more precise graph structure while eliminating redundant information. Next, the expression of the derivative of the temporal attention weight curve is obtained using tricubic interpolation and multi-scale convolution. Finally, the downstream task is completed using an N-layer convolutional architecture. Extensive experiments prove that GANCDE not only alleviates the issue of motion and non-motion features becoming similar caused by graph convolution, but also produces attention weight curves that better adhere to the continuity of actions. As a result, GANCDE achieves state-of-the-art performance in human activity recognition tasks. Specifically, on datasets such as WISDM, UCIHAR, and PAMAP2, the accuracy can reach 94.75, 93.50, and 96.37%, respectively.},
  archive      = {J_KIS},
  author       = {Teng, Tangzhi and Wan, Jie and Zhang, XiaoFeng},
  doi          = {10.1007/s10115-024-02154-y},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6213-6240},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GANCDE: Neural networks based on graphs and attention neural control differential equations for human activity recognition},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic similarity-aware feature selection and redundancy
removal for text classification using joint mutual information.
<em>KIS</em>, <em>66</em>(10), 6187–6212. (<a
href="https://doi.org/10.1007/s10115-024-02143-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high dimensionality of text data is a challenging issue that requires efficient methods to reduce vector space and improve classification accuracy. Existing filter-based methods fail to address the redundancy issue, resulting in the selection of irrelevant and redundant features. Information theory-based methods effectively solve this problem but are not practical for large amounts of data due to their high time complexity. The proposed method, termed semantic similarity-aware feature selection and redundancy removal (SS-FSRR), employs joint mutual information between the pairs of semantically related terms and the class label to capture redundant features. It is predicated on the assumption that semantically related terms imply potentially redundant ones, which can significantly reduce execution time by avoiding sequential search strategies. In this work, we use Word2Vec’s CBOW model to obtain semantic similarity between terms. The efficiency of the SS-FSRR is compared to six state-of-the-art competitive selection methods for categorical data using two traditional classifiers (SVM and NB) and a robust deep learning model (LSTM) on seven datasets with 10-fold cross-validation, where experimental results show that the SS-FSRR outperforms the other methods on most tested datasets with high stability as measured by the Jaccard’s Index.},
  archive      = {J_KIS},
  author       = {Lazhar, Farek and Amira, Benaidja},
  doi          = {10.1007/s10115-024-02143-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6187-6212},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semantic similarity-aware feature selection and redundancy removal for text classification using joint mutual information},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFS-SubSC: An efficient algorithm for mining frequent
sequences with sub-sequence constraint. <em>KIS</em>, <em>66</em>(10),
6151–6186. (<a
href="https://doi.org/10.1007/s10115-024-02148-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent sequences (FS) with constraints in a sequence database (SDB) are a critical task in Data Mining, as it forms the basis for discovering meaningful patterns within sequential data. However, traditional algorithms tackling the direct mining of constrained FSs from the SDB often exhibit poor performance, especially when dealing with large SDBs and low support thresholds. Moreover, constraint-based sequence mining algorithms face additional challenges, such as increased runtime and memory usage, particularly when constraints change frequently. To address these issues, this paper introduces an efficient method for generating FSs that include a user-defined sub-sequence. Specifically, the discovered FSs must be super-sequences of the given sub-sequence. Rather than directly discovering these sequences from a sequence database (SDB) in the traditional manner, the proposed method quickly generates constrained FSs from frequent closed sequences (FCS) and frequent generator sequences (FGS). This process involves categorizing constrained FSs into equivalence classes; each represented by FCSs and FGSs. An efficient method is then adapted to swiftly generate constrained FSs within each class based on the representative elements, which are FCSs and FGSs. Additionally, a novel technique called Constraint Satisfaction Technique (CST) is introduced to circumvent computationally expensive checks for the inclusion relation among sequences during the generation process. Furthermore, a novel algorithm named MFS-SubSC is developed based on the proposed theoretical results to generate all constrained FSs efficiently. Experimental results demonstrate that the proposed algorithm surpasses state-of-the-art methods in terms of runtime, memory usage, and scalability.},
  archive      = {J_KIS},
  author       = {Duong, Hai and Tran, Anh},
  doi          = {10.1007/s10115-024-02148-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6151-6186},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MFS-SubSC: An efficient algorithm for mining frequent sequences with sub-sequence constraint},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM examiner: Automating assessment in informal
self-directed e-learning using ChatGPT. <em>KIS</em>, <em>66</em>(10),
6133–6150. (<a
href="https://doi.org/10.1007/s10115-024-02156-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Informal e-learning systems often lack structured assessment mechanisms, making it difficult to assess the learning outcomes. This work aims to automate outcome-based assessment through the use of a large language AI model, in particular ChatGPT. Such automation can be of value to educators and developers of educational software, as it tackles the non-trivial task of evaluating educational trajectories from the outcome-based perspective. To achieve this aim, we proposed a system and validated it through a case study and two evaluation stages. In the first stage, we generated 40 assessment questions of various types, 12 of which were approved as high quality. In the second stage, we generated another 45 questions and conducted 5 individual peer evaluation sessions. The most significant automation aspects in guaranteeing the assessment quality were found to be the instructor involvement to monitor the process, the use of a high quality custom knowledge base, and formulation of the correct prompt instructions on the basis of the learning outcome statements.},
  archive      = {J_KIS},
  author       = {Askarbekuly, Nursultan and Aničić, Nenad},
  doi          = {10.1007/s10115-024-02156-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6133-6150},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {LLM examiner: Automating assessment in informal self-directed e-learning using ChatGPT},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEREIA: Document store exploration through keywords.
<em>KIS</em>, <em>66</em>(10), 6101–6132. (<a
href="https://doi.org/10.1007/s10115-024-02151-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of document stores, such as MongoDB or CouchDB, has increased drastically in recent years. Part of this popularity can certainly be explained by their flexibility in loading, storing, and retrieving semi-structured data on massive scales. However, adopting such systems presents challenges when exploring the data they store, since document structure may not follow a single pattern and thus present complex hierarchical and nested structures that vary. Additionally, an analyst who wants to retrieve data may experience difficulties since she must learn the specificities of the document store’s native query language. In this work, we propose SEREIA, a system that facilitates data exploration in document stores through keyword search. The user inputs a non-structured keyword-based query, and the system generates a structured query for the document store that fulfils her information needs. We evaluated SEREIA using five datasets previously used in the literature, and the results achieved indicate that SEREIA is suitable for helping users in data exploration tasks by removing the burden of understanding the data organization of the stored documents and by automatically generating queries to explore data of interest.},
  archive      = {J_KIS},
  author       = {Afonso, Ariel and Martins, Paulo and da Silva, Altigran},
  doi          = {10.1007/s10115-024-02151-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6101-6132},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEREIA: Document store exploration through keywords},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An aviation accidents prediction method based on MTCNN and
bayesian optimization. <em>KIS</em>, <em>66</em>(10), 6079–6100. (<a
href="https://doi.org/10.1007/s10115-024-02168-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safety of the civil aviation system has been of increasing concern with several accidents in recent years. It is urgent to put forward a precise accident prediction model, which can systematically analyze safety from the perspective of accident mechanism to enhance training accuracy. Furthermore, the predictive model is critical for stakeholders to identify risk and implement the proactive safety paradigm. In this work, to mitigate casualties and economic losses arising from aviation accidents and improve system safety, the focus is on predicting the aircraft damage severity, the injury/death severity, and the flight phases in the sequence of identifying event risk sources. This work establishes a multi-task deep convolutional neural network (MTCNN) learning framework to accomplish this goal. An innovative prediction rule will be developed to refine prediction results from two approaches: handling imbalanced classes and Bayesian optimization. By comparing the performance of the proposed multi-task model with other single-task machine learning models with ten-fold cross-validation and statistical testing, the effectiveness of the developed model in predicting aviation accident severity and flight phase is demonstrated.},
  archive      = {J_KIS},
  author       = {Xiong, Minglan and Hou, Zhaoguo and Wang, Huawei and Che, Changchang and Luo, Rui},
  doi          = {10.1007/s10115-024-02168-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6079-6100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An aviation accidents prediction method based on MTCNN and bayesian optimization},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent side-information dynamic augmentation for incremental
recommendation. <em>KIS</em>, <em>66</em>(10), 6051–6078. (<a
href="https://doi.org/10.1007/s10115-024-02165-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incremental recommendation involves updating existing models by extracting information from interaction data at current time-step, with the aim of maintaining model accuracy while addressing limitations including parameter dependencies and inefficient training. However, real-time user interaction data is often afflicted by substantial noise and invalid samples, presenting the following key challenges for incremental model updating: (1) how to effectively extract valuable new knowledge from interaction data at the current time-step to ensure model accuracy and timeliness, and (2) how to safeguard against the catastrophic forgetting of long-term stable preference information, thus preserving the model’s sensitivity during cold-starts. In response to these challenges, we propose the Incremental Recommendation with Stable Latent Side-information Updating (SIIFR). This model employs a side-information augmenter to extract valuable latent side-information from user interaction behavior at time-step T, thereby sidestepping the interference caused by noisy interaction data and acquiring stable user preference. Moreover, the model utilizes rough interaction data at time-step $$T+1$$ , in conjunction with existing side-information enhancements to achieve incremental updates of latent preferences, thereby ensuring the model’s efficacy during cold-start. Furthermore, SIIFR leverages the change rate in user latent side-information to mitigate catastrophic forgetting that results in the loss of long-term stable preference information. The effectiveness of the proposed model is validated and compared against existing models using four popular incremental datasets. The model code can be achieved at: https://github.com/LNNU-computer-research-526/FR-sii .},
  archive      = {J_KIS},
  author       = {Zhang, Jing and Shi, Jin and Duan, Jingsheng and Ren, Yonggong},
  doi          = {10.1007/s10115-024-02165-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6051-6078},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Latent side-information dynamic augmentation for incremental recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-empowered intrusion detection framework for
the internet of medical things environment. <em>KIS</em>,
<em>66</em>(10), 6001–6050. (<a
href="https://doi.org/10.1007/s10115-024-02149-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of Internet of Things (IoT) technology into healthcare, known as the Internet of Medical Things (IoMT), has significantly enhanced medical treatment and operational efficiency. Real-time patient monitoring (RPM) and remote diagnostics enabled by IoMT allow doctors to treat more patients effectively and save lives. However, healthcare devices&#39; interconnected nature makes them vulnerable to cyber-attacks, threatening patient privacy and security. Ensuring the security and accuracy of patient health data is paramount, as any tampering could have life-threatening consequences, especially in emergency situations. To address these challenges, this research focuses on developing robust security models to secure patient data in IoMT networks while meeting the growing demand for efficient healthcare services. Artificial intelligence (AI)-based technologies such as machine learning (ML) and deep learning (DL) have the potential to be employed as the methodology for intrusion detection. The goal of this research is threefold: firstly, the linear support vector machine (LinSVM) model; secondly, the convolutional support vector machine (ConvSVM) model; and finally, the categorical embedding (CatEmb) model, which have been proposed to overcome the issue of security in a network. This article offers the CatEmb model as the first effort to use a DL-based embedding approach to recognize intrusion in the IoMT environment, utilizing patient biometric and network traffic flow data. Our experimental results show the efficacy of the proposed DL models, with the LinSVM achieving a training accuracy of 99.78%, ConvSVM reaching 99.98%, and CatEmb achieving 99.84%. These models outperform existing methodologies by 2.61% in detecting network intrusions, as demonstrated through metrics such as detection rate and F1-score. Furthermore, the proposed approaches are thoroughly compared with the existing state-of-the-art studies.},
  archive      = {J_KIS},
  author       = {Shambharkar, Prashant Giridhar and Sharma, Nikhil},
  doi          = {10.1007/s10115-024-02149-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {6001-6050},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep learning-empowered intrusion detection framework for the internet of medical things environment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAESTRO: A lightweight ontology-based framework for
composing and analyzing script-based scientific experiments.
<em>KIS</em>, <em>66</em>(10), 5959–6000. (<a
href="https://doi.org/10.1007/s10115-024-02134-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decades, there has been a rapid growth in the number of scientific experiments implemented as computational simulations. These experiments typically consist of multiple steps, where different programs, in-house scripts, or services may be used at each step. Workflows have served as an abstraction to model such experiments, and such workflows can be implemented in various ways, with many users choosing scripting languages like Python. Although scripts offer users the flexibility to compose workflows with complex constructs and data structures, they typically represent isolated workflows rather than encompassing the entire experiment. Within the same experiment, users may explore different configurations to confirm or refute their hypotheses, leading to the execution of different (but associated) workflows. Composing and analyzing scientific experiments associated with multiple workflows implemented as scripts is an open, yet important, task. Poor choices during composition can lead to inconsistencies, such as format incompatibility and problems in script dependencies. Moreover, even with a well-specified and properly executed script, analyzing the data produced from an isolated workflow without knowledge of the experiment’s structure, domain terms, and specifications can be challenging. In this article, we introduce MAESTRO, a lightweight framework based on the use of ontologies and provenance to assist in the composition and analysis of experiments implemented using scripts. MAESTRO integrates the concept of Experiment Lines to represent the workflow at an abstract level and employs reasoners to derive a script-based workflow based on the abstract experiment representation and to support analytical queries. The feasibility of MAESTRO was evaluated through a study in the bioinformatics domain, receiving positive feedback from experts in e-science.},
  archive      = {J_KIS},
  author       = {Dias, Luiz Gustavo and Lopes, Bruno and de Oliveira, Daniel},
  doi          = {10.1007/s10115-024-02134-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5959-6000},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MAESTRO: A lightweight ontology-based framework for composing and analyzing script-based scientific experiments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient approach for incremental erasable utility
pattern mining from non-binary data. <em>KIS</em>, <em>66</em>(10),
5919–5958. (<a
href="https://doi.org/10.1007/s10115-024-02185-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many real-life data incrementally generated around the world. One of the recent interesting issues is the efficient processing real-world data that is continuously accumulated. Mining and recognizing removable patterns in such data is a challenging task. Erasable pattern mining confronts this challenge by discovering removable patterns with low gain. In various real-world applications, data are stored in the form of non-binary databases. These databases store item information in a quantity form. Since items in the database can each have different characteristics, such as quantities, considering their relative features makes the mined patterns more meaningful. For these reasons, we propose an erasable utility pattern mining algorithm for incremental non-binary databases. The suggested technique can recognize removable patterns by considering the relative utility of items and the profit of products in an incremental database. The proposed algorithm utilizes a list structure for efficiently extracting erasable utility patterns. Several experiments have been conducted to compare the performance between the suggested algorithm and state-of-the-art techniques using real and synthetic datasets, and the results demonstrate the effectiveness of the proposed method.},
  archive      = {J_KIS},
  author       = {Baek, Yoonji and Kim, Hanju and Cho, Myungha and Kim, Hyeonmo and Lee, Chanhee and Ryu, Taewoong and Kim, Heonho and Vo, Bay and Gan, Vincent W. and Fournier-Viger, Philippe and Lin, Jerry Chun-Wei and Pedrycz, Witold and Yun, Unil},
  doi          = {10.1007/s10115-024-02185-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5919-5958},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient approach for incremental erasable utility pattern mining from non-binary data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Situational data integration in question answering systems:
A survey over two decades. <em>KIS</em>, <em>66</em>(10), 5875–5918. (<a
href="https://doi.org/10.1007/s10115-024-02136-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question Answering (QA) systems provide accurate answers to questions; however, they lack the ability to consolidate data from multiple sources, making it difficult to manage complex questions that could be answered with additional data retrieved and integrated on the fly. This integration is inherent to Situational Data Integration (SDI) approaches that deal with dynamic requirements of ad hoc queries that neither traditional database management systems, nor search engines are effective in providing an answer. Thus, if QA systems include SDI characteristics, they could be able to return validated and immediate information for supporting users decisions. For this reason, we surveyed QA-based systems, assessing their capabilities to support SDI features, i.e., Ad hoc Data Retrieval, Data Management, and Timely Decision Support. We also identified patterns concerning these features in the surveyed studies, highlighting them in a timeline that shows the SDI evolution in the QA domain. To the best of your knowledge, this study is precursor in the joint analysis of SDI and QA, showing a combination that can favor the way systems support users. Our analyses show that most of SDI features are rarely addressed in QA systems, and based on that, we discuss directions for further research.},
  archive      = {J_KIS},
  author       = {Franciscatto, Maria Helena and Erpen de Bona, Luis Carlos and Trois, Celio and Didonet Del FabroFabro, Marcos and Damasceno Lima, João Carlos},
  doi          = {10.1007/s10115-024-02136-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5875-5918},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Situational data integration in question answering systems: A survey over two decades},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic graph model and neural network perspective of
click models for web search. <em>KIS</em>, <em>66</em>(10), 5829–5873.
(<a href="https://doi.org/10.1007/s10115-024-02145-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Click behavior is a typical user behavior in the web search. How to capture and model users’ click behavior has always been a common research topic. However, there are few review studies on this topic. In this paper, we present a survey to comprehensively analyze click models of web search via types of models. Based on differences in research hypotheses and modeling methods, click models are generally divided into probability graph-based click models, neural network-based click models, and hybrid click models. Firstly, we give a discussion of click models in the extant literature, within which basic assumptions and their extensions, advantages and disadvantages for click models are presented. We also compare and analyze the characteristics and application scenarios of different types of models. Secondly, we choose eight representative click models and conduct comparative experiments on two real-world session datasets to compare their performance. Finally, we identify current research trends, main challenges and potential future directions of click models worthy of further explorations.},
  archive      = {J_KIS},
  author       = {Liu, Jianping and Wang, Yingfei and Wang, Jian and Wang, Meng and Chu, Xintao},
  doi          = {10.1007/s10115-024-02145-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5829-5873},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Probabilistic graph model and neural network perspective of click models for web search},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of semantic-based process mining techniques:
Trends and future directions. <em>KIS</em>, <em>66</em>(10), 5783–5827.
(<a href="https://doi.org/10.1007/s10115-024-02147-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining algorithms essentially reflect the execution behavior of events in an event log for conformance checking, model discovery, or enhancement. Domain experts have developed several process mining algorithms based on theoretical frameworks such as linear integer programming, heuristics, and genetic algorithms, region-based and semantic-based approaches. The idea is to generate insightful representations of these processes of information systems to enable process mining practitioners to gain insight into their systems. Recently, there has been a shift toward semantic-based approaches for process mining since they not only discover enhanced models but also emphasize context. To this effect, this paper conducts a comprehensive review of 30 articles on semantic process mining techniques. It was found that 44.7% of all works used semantics for process discovery, 23.7% for model enhancement, and conformance checking was the least with 10.5%. We further indicate the benefits and contributions of these methods to process mining. Challenges, opportunities, and prospective future research areas are also discussed.},
  archive      = {J_KIS},
  author       = {Issahaku, Fadilul-lah Yassaanah and Lu, Ke and Xianwen, Fang and Danwana, Sumaiya Bashiru and Bandago, Husein Mohammed},
  doi          = {10.1007/s10115-024-02147-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5783-5827},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An overview of semantic-based process mining techniques: Trends and future directions},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning-based scheduling in distributed
systems: A critical review. <em>KIS</em>, <em>66</em>(10), 5709–5782.
(<a href="https://doi.org/10.1007/s10115-024-02167-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fields of research use parallelized and distributed computing environments, including astronomy, earth science, and bioinformatics. Due to an increase in client requests, service providers face various challenges, such as task scheduling, security, resource management, and virtual machine migration. NP-hard scheduling problems require a long time to implement an optimal or suboptimal solution due to their large solution space. With recent advances in artificial intelligence, deep reinforcement learning (DRL) can be used to solve scheduling problems. The DRL approach combines the strength of deep learning and neural networks with reinforcement learning’s feedback-based learning. This paper provides a comprehensive overview of DRL-based scheduling algorithms in distributed systems by categorizing algorithms and applications. As a result, several articles are assessed based on their main objectives, quality of service and scheduling parameters, as well as evaluation environments (i.e., simulation tools, real-world environment). The literature review indicates that algorithms based on RL, such as Q-learning, are effective for learning scaling and scheduling policies in a cloud environment. Additionally, the challenges and directions for further research on deep reinforcement learning to address scheduling problems were summarized (e.g., edge intelligence, ideal dynamic task scheduling framework, human–machine interaction, resource-hungry artificial intelligence (AI) and sustainability).},
  archive      = {J_KIS},
  author       = {Jalali Khalil Abadi, Zahra and Mansouri, Najme and Javidi, Mohammad Masoud},
  doi          = {10.1007/s10115-024-02167-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {5709-5782},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep reinforcement learning-based scheduling in distributed systems: A critical review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based disease diagnosis with intrusion
detection for a secured healthcare system. <em>KIS</em>, <em>66</em>(9),
5669–5707. (<a
href="https://doi.org/10.1007/s10115-023-02030-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security is considered the primary challenge in the healthcare industry in the aspect of the ethical and legal perspective of patient medical data. In existing approaches, the accessibility, reliability, and confidentiality of medical data are needed for more security in the healthcare industry. The main obstacles involved in the communication between the smartphone and monitoring devices allow a data theft attack. Hence, the secured model needs to be designed for providing data security in healthcare applications. By avoiding these challenges, this task provides secured intrusion detection with blockchain-based healthcare data transmission with the help of novel intelligence techniques. Initially, in the data acquisition phase, the gathered data from online sources consist of brain, skin, and retinal medical images. Then, the Weight Optimized Deep Belief Network (WO-DBN) checks the nodes to confirm whether the gathered data are in a malicious state or not at the time of transmission. The medical data are secured with the support of encryption and blockchain technology, where the medical images are enciphered by chaotic-map-aided image encryption through optimal key generation. These encrypted images are uploaded to the blockchain for secure data communication to the cloud server. Here, the authorized person can replace the data using the same optimal key and also it can be done after the decryption in the proposed chaotic map. The decrypted data are given to the final disease diagnosis phase for classifying the images without any information loss with the support of a Residual Network (Resnet101), where the final layer is restored by the “Deep Neural Network (DNN) and Long Short Term Memory (LSTM)” to enhance the classification accuracy and it is named Res-LSTM+DNN. The weight optimization in DBN, optimal key generation, and hyper-parameter tuning in classification are done by the Improved Dingo Optimizer (IDOX). From the overall result validation, the accuracy rate of the recommended approach scores 98.6%.},
  archive      = {J_KIS},
  author       = {Kanna, S. K. Rajesh and Murthy, Mantripragada Yaswanth Bhanu and Gawali, Mahendra Bhatu and Rubai, Saleh Muhammad and Reddy, N. Srikanth and Brammya, G. and Preetha, N. S. Ninu},
  doi          = {10.1007/s10115-023-02030-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5669-5707},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A deep learning-based disease diagnosis with intrusion detection for a secured healthcare system},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual programming tool for mobile web augmentation.
<em>KIS</em>, <em>66</em>(9), 5631–5668. (<a
href="https://doi.org/10.1007/s10115-023-02039-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of mobile devices for web browsing has increased extraordinarily in recent years, becoming the main source of information. Unfortunately, developers cannot meet the needs of all users. As a result, users have been forced to adapt web content to their needs. Additionally, users face more costly interactions than on desktop computers due to the smaller screen size of cell phones. These interactions include URL typing, scrolling, taping, branching and tab-switching. If the number of web resources is large, the activity becomes unpleasant. Furthermore, should the quantity of interactions be extensive, the web experience also becomes unpleasant. Moreover, all these interactions lead to higher costs in terms of phone time and battery life, as well as more costly navigation. Web augmentation enhances the user web experience by adding content from different resources, modifying the original layout, or reducing interactions during web searches, among other benefits. To reduce user interactions and satisfy their information necessities, we propose MAWA, an extension for Firefox mobile that allows users to adapt any web page to their needs through web augmentation techniques. To validate our proposal, an evaluation of the tool has been carried out. The article presents the benefits introduced by MAWA as a response to the inconveniences experienced by users when performing their activities in the mobile browser. The results obtained show that MAWA is able to reduce the number of interactions significantly, as well as to reduce battery consumption and the time needed to obtain the desired information. In addition, users perceive that navigation is less cluttered.},
  archive      = {J_KIS},
  author       = {Aldalur, Iñigo and Perez, Alain and Larrinaga, Felix and Illarramendi, Miren},
  doi          = {10.1007/s10115-023-02039-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5631-5668},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A visual programming tool for mobile web augmentation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constraining acyclicity of differentiable bayesian structure
learning with topological ordering. <em>KIS</em>, <em>66</em>(9),
5605–5630. (<a
href="https://doi.org/10.1007/s10115-024-02140-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributional estimates in Bayesian approaches in structure learning have advantages compared to the ones performing point estimates when handling epistemic uncertainty. Differentiable methods for Bayesian structure learning have been developed to enhance the scalability of the inference process and are achieving optimistic outcomes. However, in the differentiable continuous setting, constraining the acyclicity of learned graphs emerges as another challenge. Various works utilize post-hoc penalization scores to impose this constraint which cannot assure acyclicity. The topological ordering of the variables is one type of prior knowledge that contains valuable information about the acyclicity of a directed graph. In this work, we propose a framework to guarantee the acyclicity of inferred graphs by integrating the information from the topological ordering into the inference process. Our integration framework does not interfere with the differentiable inference process while being able to strictly assure the acyclicity of learned graphs and reduce the inference complexity. Our extensive empirical experiments on both synthetic and real data have demonstrated the effectiveness of our approach with preferable results compared to related Bayesian approaches.},
  archive      = {J_KIS},
  author       = {Tran, Quang-Duy and Nguyen, Phuoc and Duong, Bao and Nguyen, Thin},
  doi          = {10.1007/s10115-024-02140-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5605-5630},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Constraining acyclicity of differentiable bayesian structure learning with topological ordering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to personalize and whether to personalize? Candidate
documents decide. <em>KIS</em>, <em>66</em>(9), 5581–5604. (<a
href="https://doi.org/10.1007/s10115-024-02138-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized search plays an important role in satisfying users’ information needs owing to its ability to build user profiles based on users’ search histories. Most of the existing personalized methods built dynamic user profiles by emphasizing query-related historical behaviors rather than treating each historical behavior equally. Sometimes, the ambiguity and short nature of the query make it difficult to understand the potential query intent exactly, and the query-centric user profiles built in these cases will be biased and inaccurate. In this work, we propose to leverage candidate documents, which contain richer information than the short query text, to help understand the query intent more accurately and improve the quality of user profiles afterward. Specifically, we intend to better understand the query intent through candidate documents, so that more relevant user behaviors from history can be selected to build more accurate user profiles. Moreover, by analyzing the differences between candidate documents, we can better control the degree of personalization on the ranking of results. This controlled personalization approach is also expected to further improve the stability of personalized search as blind personalization may harm the ranking results. We conduct extensive experiments on two datasets, and the results show that our model significantly outperforms competitive baselines, which confirms the benefit of utilizing candidate documents for personalized web search.},
  archive      = {J_KIS},
  author       = {Liu, Wenhan and Zhou, Yujia and Zhu, Yutao and Dou, Zhicheng},
  doi          = {10.1007/s10115-024-02138-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5581-5604},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {How to personalize and whether to personalize? candidate documents decide},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markov enhanced graph attention network for spammer
detection in online social network. <em>KIS</em>, <em>66</em>(9),
5561–5580. (<a
href="https://doi.org/10.1007/s10115-024-02137-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks (OSNs) are an indispensable part of social communication where people connect and share information. Spammers and other malicious actors use the OSN’s power to propagate spam content. In an OSN with mutual relations between nodes, two kinds of spammer detection methods can be employed: feature based and propagation based. However, both of these are incomplete in themselves. The feature-based methods cannot exploit mutual connections between nodes, and propagation-based methods cannot utilize the rich discriminating node features. We propose a hybrid model—Markov enhanced graph attention network (MEGAT)—using graph attention networks (GAT) and pairwise Markov random fields (pMRF) for the spammer detection task. It efficiently utilizes node features as well as propagation information. We experiment our GAT model with a smoother Swish activation function having non-monotonic derivatives, instead of the leakyReLU function. The experiments performed on a real-world Twitter Social Honeypot (TwitterSH) benchmark dataset and subsequent comparative analysis reveal that our proposed MEGAT model outperforms the state-of-the-art models in accuracy, precision–recall area under curve (PRAUC), and F1-score performance measures.},
  archive      = {J_KIS},
  author       = {Tripathi, Ashutosh and Ghosh, Mohona and Bharti, Kusum Kumari},
  doi          = {10.1007/s10115-024-02137-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5561-5580},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Markov enhanced graph attention network for spammer detection in online social network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Twain-GCN: Twain-syntax graph convolutional networks for
aspect-based sentiment analysis. <em>KIS</em>, <em>66</em>(9),
5541–5560. (<a
href="https://doi.org/10.1007/s10115-024-02135-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of aspect-based sentiment analysis is to recognize the aspect information in the text and the corresponding sentiment polarity. A variety of robust methods, including attention mechanisms and convolutional neural networks, have been extensively utilized to tackle this complex task. Better experimental results are obtained by using graph convolutional networks (GCN) based on semantic dependency trees in previous studies. Therefore, abundant methods begin to use sentence structure information to complete this task. However, only the loose connection between aspect words and contexts is realized in some practices due to sentences may contain complex relations. To solve this problem, Twain-Syntax graph convolutional network model is proposed, which can utilize multiple syntactic structure information simultaneously. Guided by the constituent tree and dependency tree, rich syntactic information is fully used in the model to build the sentiment-aware context for each aspect. In special, the multilayer attention mechanism and GCN are employed for learning to capture the correlation between words. By integrating syntactic information, this approach significantly refines the model’s technical performance. Extensive testing on four benchmark datasets shows that the model delineated in this paper exhibits high levels of efficiency, comparable to several cutting-edge models.},
  archive      = {J_KIS},
  author       = {Hou, Ying and Liu, Fang’ai and Zhuang, Xuqiang and Zhang, Yuling},
  doi          = {10.1007/s10115-024-02135-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5541-5560},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Twain-GCN: Twain-syntax graph convolutional networks for aspect-based sentiment analysis},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The analysis of diversification properties of stablecoins
through the shannon entropy measure. <em>KIS</em>, <em>66</em>(9),
5501–5540. (<a
href="https://doi.org/10.1007/s10115-024-02133-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The common goal for investors is to minimise the risk and maximise the returns on their investments. This is often achieved through diversification, where investors spread their investments across various assets. This study aims to use the MAD-entropy model to minimise the absolute deviation, maximise the mean return, and maximise the Shannon entropy of the portfolio. The MAD model is used because it is a linear programming model, allowing it to resolve large-scale problems and nonnormally distributed data. Entropy is added to the MAD model because it can better diversify the weight of assets in the portfolios. The analysed portfolios consist of cryptocurrencies, stablecoins, and selected world indices such as the SP500 and FTSE obtained from Yahoo Finance. The models found that stablecoins pegged to the US dollar, followed by stablecoins pegged to gold, are better diversifiers for traditional cryptocurrencies and stocks. These results are probably due to their low volatility compared to the other assets. Findings from this study may assist investors since the MAD-Entropy model outperforms the MAD model by providing more significant portfolio mean returns with minimal risk. Therefore, crypto investors can design a well-diversified portfolio using MAD entropy to reduce unsystematic risk. Further research integrating mad entropy with machine learning techniques may improve accuracy and risk management.},
  archive      = {J_KIS},
  author       = {Sinon, Mohavia Ben Amid and Mba, Jules Clement},
  doi          = {10.1007/s10115-024-02133-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5501-5540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The analysis of diversification properties of stablecoins through the shannon entropy measure},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale knowledge graph representation learning.
<em>KIS</em>, <em>66</em>(9), 5479–5499. (<a
href="https://doi.org/10.1007/s10115-024-02131-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge graph emerges as powerful data structures that provide a deep representation and understanding of the knowledge presented in networks. In the pursuit of representation learning of the knowledge graph, entities and relationships undergo an embedding process, where they are mapped onto a vector space with reduced dimensions. These embeddings are progressively used to extract their information for a multitude of tasks in machine learning. Nevertheless, the increase data in knowledge graph has introduced a challenge, especially as knowledge graph embedding now encompass millions of nodes and billions of edges, surpassing the capacities of existing knowledge representation learning systems. In response to these challenge, this paper presents DistKGE, a distributed learning approach of knowledge graph embedding based on a new partitioning technique. In our experimental evaluation, we illustrate that the proposed approach improves the scalability of distributed knowledge graph learning with respect to graph size compared to existing methods in terms of runtime performances in the link prediction task aimed at identifying new links between entities within the knowledge graph.},
  archive      = {J_KIS},
  author       = {Badrouni, Marwa and Katar, Chaker and Inoubli, Wissem},
  doi          = {10.1007/s10115-024-02131-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5479-5499},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Large-scale knowledge graph representation learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRAS: Cross-domain recommendation via aspect-level sentiment
extraction. <em>KIS</em>, <em>66</em>(9), 5459–5477. (<a
href="https://doi.org/10.1007/s10115-024-02130-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of sparse data and cold-start when facing new users and items in the single-domain recommendation, cross-domain recommendation has gradually become a hot topic in the recommendation system. This method enhances target domain recommendation performance by incorporating relevant information from an auxiliary domain. A critical aspect of cross-domain recommendation is the effective transfer of user preferences from the source to the target domain. This paper proposes a novel cross-domain recommendation framework, namely the Cross-domain Recommendation based on Aspect-level Sentiment extraction (CRAS). CRAS leverages user and item review texts in cross-domain recommendations to extract detailed user preferences. Specifically, the Biterm Topic Model (BTM) is utilized for the precise extraction of ’aspects’ from users and items, focusing on identifying characteristics that align with user interests and the positive attributes of items. These ’aspects’ represent distinct, influential features of the items. For example, a good service attitude can be regarded as a good aspect of a restaurant. Furthermore, this study employs an improved Cycle-Consistent Generative Adversarial Networks (CycleGAN), efficiently mapping user preferences from one domain to another, thereby enhancing the accuracy and personalization of the recommendations. Lastly, this paper compares the CRAS model with a series of state-of-the-art baseline methods in the Amazon review dataset, and experiment results show that the proposed model outperforms the baseline methods.},
  archive      = {J_KIS},
  author       = {Zhang, Fan and Zhou, Yaoyao and Sun, Pengfei and Xu, Yi and Han, Wanjiang and Huang, Hongben and Chen, Jinpeng},
  doi          = {10.1007/s10115-024-02130-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5459-5477},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CRAS: Cross-domain recommendation via aspect-level sentiment extraction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple optimized ensemble learning for high-dimensional
imbalanced credit scoring datasets. <em>KIS</em>, <em>66</em>(9),
5429–5457. (<a
href="https://doi.org/10.1007/s10115-024-02129-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring models are crucial tools for lenders to assess credit risks. Researchers from academia and the financial industry have shown intense interest in these models. However, real credit datasets often have high dimensionality and class imbalance, making it challenging to develop accurate and effective credit scoring models. To address these challenges, a new approach called the Multiple-Optimized Ensemble Learning (MOEL) method has been proposed. In MOEL, a technique called Multiple Diverse Optimized Subsets (MDOS) generates multiple diverse optimized subsets from various weighted random forests. From each subset, more effective and relevant features are selected. Then, a new evaluation measure is applied to each subset to determine the more optimized subsets. These subsets are applied to a novel Mahalanobis-based oversampling (MOS) technique to provide balanced subsets for the base classifier, which lessens the detrimental effects of imbalanced datasets. Finally, a stacking-based ensemble method is applied to the balanced subsets for integration of the base models. The proposed model was evaluated against six high-dimensional imbalanced credit scoring datasets, and it outperformed state-of-the-art methods, exhibiting a mean rank of 1.5 and 1.333 in terms of F1_score and G-mean, respectively.},
  archive      = {J_KIS},
  author       = {Lenka, Sudhansu R. and Bisoy, Sukant Kishoro and Priyadarshini, Rojalina},
  doi          = {10.1007/s10115-024-02129-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5429-5457},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multiple optimized ensemble learning for high-dimensional imbalanced credit scoring datasets},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent system architecture for winter road maintenance:
A real spanish case study. <em>KIS</em>, <em>66</em>(9), 5409–5427. (<a
href="https://doi.org/10.1007/s10115-024-02128-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road safety remains a critical issue in contemporary society, where the sudden deterioration of road conditions due to weather-related natural phenomena poses significant risks. These abrupt changes can lead to severe safety hazards on the roads, making real-time monitoring and control essential for maintaining road safety. In this context, technological advancements, especially in sensor networks and intelligent systems, play a fundamental role in efficiently managing these challenges. This study introduces an innovative approach that leverages a sophisticated sensor platform coupled with a multi-agent system. This integration facilitates the collection, processing, and analysis of data to preemptively determine the appropriate chemical treatments for roads during severe winter conditions. By employing advanced data analysis and machine learning techniques within a multi-agent framework, the system can predict and respond to adverse weather effects swiftly and with a high degree of accuracy. The proposed system has undergone rigorous testing in a real-world environment, which has verified its operational effectiveness. The results from the deployment of the multi-agent architecture and its predictive capabilities are encouraging, suggesting that this approach could significantly enhance road safety in extreme weather conditions. Furthermore, the proposed architecture allows the system to evolve and scale over time. This paper details the design and implementation of the system, discusses the results of its field tests, and explores potential improvements.},
  archive      = {J_KIS},
  author       = {Jiménez-Bravo, Diego M. and Bajo, Javier and González-Pachón, Jacinto and De Paz, Juan F.},
  doi          = {10.1007/s10115-024-02128-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5409-5427},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-agent system architecture for winter road maintenance: A real spanish case study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new neighbourhood-based diffusion algorithm for
personalized recommendation. <em>KIS</em>, <em>66</em>(9), 5389–5408.
(<a href="https://doi.org/10.1007/s10115-024-02127-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object ratings in recommendation algorithms are used to represent the extent to which a user likes an object. Most existing recommender systems use these ratings to recommend the top-K objects to a target user. To improve the accuracy and diversity of recommender systems, we proposed a neighbourhood-based diffusion recommendation algorithm (NBD) that distributes the resources among objects using the rating scores of the objects based on the likings of the target user neighbours. Specifically, the Adamic–Adar similarity index is used to calculate the similarity between the target user and other users to select the top K similar neighbours to begin the diffusion process. In this approach, greater significance is put on common neighbours with fewer neighbour nodes. This is to reduce the effect of popular objects. At the end of the diffusion process, a modified redistribution algorithm using the sigmoid function is explored to finally redistribute the resources to the objects. This is to ensure that the objects recommended are personalized to target users. The evaluation has been conducted through experiments using four real-world datasets (Friendfeed, Epinions, MovieLens-100 K, and Netflix). The experiment results show that the performance of our proposed NBD algorithm is better in terms of accuracy when compared with the state-of-the-art algorithms.},
  archive      = {J_KIS},
  author       = {Mumin, Diyawu and Shi, Lei-Lei and Liu, Lu and Han, Zi-xuan and Jiang, Liang and Wu, Yan},
  doi          = {10.1007/s10115-024-02127-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5389-5408},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new neighbourhood-based diffusion algorithm for personalized recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CIIR: An approach to handle class imbalance using a novel
feature selection technique. <em>KIS</em>, <em>66</em>(9), 5355–5388.
(<a href="https://doi.org/10.1007/s10115-024-02126-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing vulnerability of systems and the rise in malicious events have sparked concerns about network security. In order to address these threats, network intrusion detection systems (NIDSs) play a role in protecting against malicious threats. However, IDSs often face obstacles, like the issue of imbalanced classes, which can hinder the effectiveness of machine learning models by giving preference to the majority class. To resolve this issue, many strategies such as resampling, cost-sensitive, and ensemble learning systems have been proposed, but no relevant metrics have been developed to investigate the influence of observed performance on the data-level approach. The proposed model introduced a new metric to study the impact of sampling for the classification algorithm. This paper presents a novel approach known as the CIIR (Causal InferenceImbalanced Ratio) by utilizing ADASYN-IHT with Boruta-ROC feature selection in conjunction with four well-known imbalanced datasets: CIC-DDoS2019, UNSW-NB15, ML-EdgeIIoT and WUSTL-IIoT2021. The experimental outcomes prove the efficacy of the ADASYN-IHT and Boruta-ROC methods in improving classification performance on these datasets and by studying the impact of the CIIR.},
  archive      = {J_KIS},
  author       = {Thiyam, Bidyapati and Dey, Shouvik},
  doi          = {10.1007/s10115-024-02126-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5355-5388},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CIIR: An approach to handle class imbalance using a novel feature selection technique},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive ensemble pruning framework based on
dual-objective maximization trade-off. <em>KIS</em>, <em>66</em>(9),
5335–5353. (<a
href="https://doi.org/10.1007/s10115-024-02125-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning has gotten a lot of interest because of its capacity to increase predictive accuracy by merging numerous models. However, redundant data and a high level of computing complexity frequently plague ensembles. To choose a subset of models while maintaining the accuracy and diversity of the ensemble, ensemble pruning techniques are used to address these problems. Accuracy and diversity must coexist, even though their goals are conflicting. This is why we formulate the issue of ensemble pruning as a dual-objective maximization problem using the idea from information theory. Then, we propose a Comprehensive Ensemble Pruning Framework (CEPF) based on the dual-objective maximization (DOM) trade-off metric. Extensive evaluation of our framework on the exclusively collected PhysioSense dataset demonstrates the superiority of our method compared to existing pruning techniques. PhysioSense dataset was collected after getting approval from the Institutional Human Ethics Committee (IHEC) of Panimalar Medical College Hospital and Research Institute, Chennai, Tamil Nadu (Protocol No: PMCHRI-IHEC-059). The proposed framework not only preserves or improves ensemble accuracy and diversity but also achieves a significant reduction in actual ensemble size. Furthermore, the proposed method provides valuable insights into the dual-objective trade-off between accuracy and diversity paving the way for further research and advancements in ensemble pruning techniques.},
  archive      = {J_KIS},
  author       = {Gopalakrishnan, Anitha and Manickam, J. Martin Leo},
  doi          = {10.1007/s10115-024-02125-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5335-5353},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comprehensive ensemble pruning framework based on dual-objective maximization trade-off},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reasoning subevent relation over heterogeneous event graph.
<em>KIS</em>, <em>66</em>(9), 5311–5333. (<a
href="https://doi.org/10.1007/s10115-024-02124-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subevent relation identification (SRI) is a challenging natural language processing task of great value for knowledge acquisition and reasoning. Given an event pair, previous work mainly defines SRI as a classification task and usually relies on hand-crafted features extracted from a limited context. However, there are lots of evidence information in external knowledge bases which are not well explored by prior work but helpful for this task. To fill this gap, we propose to reason the subevent relation based on plentiful evidence information in addition to limited context features. Specifically, we notice that evidence information together with the input event pair can be organized into a heterogeneous event graph, which comprises various types of event relations. To facilitate this, we present a heterogeneous graph attention model with mix-hop reasoning mechanism to reason subevent relation over heterogeneous event graph. In particular, our model allows nodes to capture more extensive and intact event knowledge by mixing feature representations of neighbors at multiple distances to aggregate information from diverse types of neighbors. Moreover, to explicitly model the hierarchical relations between event pairs and to improve the model consistency, we devise a novel hierarchy loss function for the proposed model. Our model is evaluated on two annotated datasets with subevent hierarchies: SeRI and HiEve. Experimental results show that our approach can outperform state-of-the-art baseline methods. Further evaluation suggests that reasoning paths in the heterogeneous graph can regard as a reasonable explanation for the prediction result.},
  archive      = {J_KIS},
  author       = {Wu, Ting-Ting and Ding, Xiao and Du, Li and Qin, Bing and Liu, Ting},
  doi          = {10.1007/s10115-024-02124-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5311-5333},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Reasoning subevent relation over heterogeneous event graph},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Range control-based class imbalance and optimized granular
elastic net regression feature selection for credit risk assessment.
<em>KIS</em>, <em>66</em>(9), 5281–5310. (<a
href="https://doi.org/10.1007/s10115-024-02103-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk, stemming from the failure of a contractual party, is a significant variable in financial institutions. Assessing credit risk involves evaluating the creditworthiness of individuals, businesses, or entities to predict the likelihood of defaulting on financial obligations. While financial institutions categorize consumers based on creditworthiness, there is no universally defined set of attributes or indices. This research proposes Range control-based class imbalance and Optimized Granular Elastic Net regression (ROGENet) for feature selection in credit risk assessment. The dataset exhibits severe class imbalance, addressed using Range-Controlled Synthetic Minority Oversampling TEchnique (RCSMOTE). The balanced data undergo Granular Elastic Net regression with hybrid Gazelle sand cat Swarm Optimization (GENGSO) for feature selection. Elastic net, ensuring sparsity and grouping for correlated features, proves beneficial for assessing credit risk. ROGENet provides a detailed perspective on credit risk evaluation, surpassing conventional methods. The oversampling feature selection enhances the accuracy of minority class by 99.4, 99, 98.6 and 97.3%, respectively.},
  archive      = {J_KIS},
  author       = {Amarnadh, Vadipina and Moparthi, Nageswara Rao},
  doi          = {10.1007/s10115-024-02103-9},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5281-5310},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Range control-based class imbalance and optimized granular elastic net regression feature selection for credit risk assessment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CG-FHAUI: An efficient algorithm for simultaneously mining
succinct pattern sets of frequent high average utility itemsets.
<em>KIS</em>, <em>66</em>(9), 5239–5280. (<a
href="https://doi.org/10.1007/s10115-024-02121-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of both closed frequent high average utility itemsets (CFHAUIs) and generators of frequent high average utility itemsets (GFHAUIs) has substantial significance because they play an essential and concise role in representing frequent high average utility itemsets (FHAUIs). These concise summaries offer a compact yet crucial overview that can be much smaller. In addition, they allow the generation of non-redundant high average utility association rules, a crucial factor for decision-makers to consider. However, difficulty arises from the complexity of discovering these representations, primarily because the average utility function does not satisfy both monotonic and anti-monotonic properties within each equivalence class, that is for itemsets sharing the same subset of transactions. To tackle this challenge, this paper proposes an innovative method for efficiently extracting CFHAUIs and GFHAUIs. This approach introduces novel bounds on the average utility, including a weak lower bound called $$wlbau$$ and a lower bound named $$auvlb$$ . Efficient pruning strategies are also designed with the aim of early elimination of non-closed and/or non-generator FHAUIs based on the $$wlbau$$ and $$auvlb$$ bounds, leading to quicker execution and lower memory consumption. Additionally, the paper introduces a novel algorithm, CG-FHAUI, designed to concurrently discover both GFHAUIs and CFHAUIs. Empirical results highlight the superior performance of the proposed algorithm in terms of runtime, memory usage, and scalability when compared to a baseline algorithm.},
  archive      = {J_KIS},
  author       = {Duong, Hai and Truong, Tin and Le, Bac and Fournier-Viger, Philippe},
  doi          = {10.1007/s10115-024-02121-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5239-5280},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CG-FHAUI: An efficient algorithm for simultaneously mining succinct pattern sets of frequent high average utility itemsets},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GK index: Bridging gf and k indices for comprehensive author
evaluation. <em>KIS</em>, <em>66</em>(9), 5203–5238. (<a
href="https://doi.org/10.1007/s10115-024-02119-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of accurately predicting scientific impact and ranking the researcher based on impact has emerged as a crucial research challenge, captivating the interest of scholars across diverse domains. This task holds immense importance in enhancing research efficiency, aiding decision-making processes, and facilitating scientific evaluations. For this, the scientific community has put forth a wide array of parameters to identify the most influential researchers. These include citation count, total publication count, hybrid methodologies, the h-index, and also its extended or modified versions. But still, there is a lack of consensus on a single optimal parameter for identifying the most influential author. In this study, we introduce a novel index derived from learning hidden patterns within the mathematics field dataset, comprising data from 1050 researchers evenly split between awardees and non-awardees. Initially, we ranked selected parameters by assessing their values for individual researchers, identifying the top five parameters that most frequently placed awardees within the top 100 records. Additionally, we employed deep learning techniques to identify the top five influential parameters from the initially selected set. Subsequently, we evaluated the disjointness between the results produced by these parameters. To further refine our analysis, we assessed seven different statistical models for combining the top disjoint pair to retain the maximum properties of both parameters. The study’s findings revealed that the gf and k indices exhibited a 0.96 percent disjointness ratio, establishing them as the highest disjoint pair. Moreover, the geometric mean demonstrated a 0.87 percent average impact in retaining the properties of the top disjoint pair, surpassing the other seven models. As a result of this study, we propose a new index obtained by taking the geometric mean of the top disjoint pair which increase the result by 12% as compared to existing best performing individual index performance.},
  archive      = {J_KIS},
  author       = {Mustafa, Ghulam and Rauf, Abid and Afzal, Muhammad Tanvir},
  doi          = {10.1007/s10115-024-02119-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5203-5238},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GK index: Bridging gf and k indices for comprehensive author evaluation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BotCL: A social bot detection model based on graph
contrastive learning. <em>KIS</em>, <em>66</em>(9), 5185–5202. (<a
href="https://doi.org/10.1007/s10115-024-02116-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of social bots on social networks presents significant challenges to network security due to their malicious activities. While graph neural network models have shown promise in detecting social bots, acquiring a large number of high-quality labeled accounts remains challenging, impacting bot detection performance. To address this issue, we introduce BotCL, a social bot detection model that employs contrastive learning through data augmentation. Initially, we build a directed graph based on following/follower relationships, utilizing semantic, attribute, and structural features of accounts as initial node features. We then simulate account behaviors within the social network and apply two data augmentation techniques to generate multiple views of the directed graph. Subsequently, we encode the generated views using relational graph convolutional networks, achieving maximum homogeneity in node representations by minimizing the contrastive loss. Finally, node labels are predicted using Softmax. The proposed method augments data based on its distribution, showcasing robustness to noise. Extensive experimental results on Cresci-2015, Twibot-20, and Twibot-22 datasets demonstrate that our approach surpasses the state-of-the-art methods in terms of performance.},
  archive      = {J_KIS},
  author       = {Li, Yan and Li, Zhenyu and Gong, Daofu and Hu, Qian and Lu, Haoyu},
  doi          = {10.1007/s10115-024-02116-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5185-5202},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {BotCL: A social bot detection model based on graph contrastive learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Protecting the privacy of social network data using graph
correction. <em>KIS</em>, <em>66</em>(9), 5151–5183. (<a
href="https://doi.org/10.1007/s10115-024-02115-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the rapid development of online social networks, as well as low costs, easy communication, and quick access with minimal facilities have made social networks an attractive and very influential phenomenon among people. The users of these networks tend to share their sensitive and private information with friends and acquaintances. This has caused the data of these networks to become a very important source of information about users, their interests, feelings, and activities. Analyzing this information can be very useful in predicting the behavior of users in dealing with various issues. But publishing this data for data mining can violate the privacy of users. As a result, data privacy protection of social networks has become an important and attractive research topic. In this context, various algorithms have been proposed, all of which meet privacy requirements by making changes in the information as well as the graph structure. But due to high processing costs and long execution times, these algorithms are not very appropriate for anonymizing big data. In this research, we improved the speed of data anonymization by using the number factorization technique to select and delete the best edges in the graph correction stage. We also used the chaotic krill herd algorithm to add edges, and considering the effect of all edges together on the structure of the graph, we selected edges and added them to the graph so that it preserved the graph’s utility. The evaluation results on the real-world datasets, show the efficiency of the proposed algorithm in comparison with the state-of-the-art methods to reduce the execution time and maintain the utility of the anonymous graph.},
  archive      = {J_KIS},
  author       = {Toroghi, Amir Dehaki and Hamidzadeh, Javad},
  doi          = {10.1007/s10115-024-02115-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5151-5183},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Protecting the privacy of social network data using graph correction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Methods for concept analysis and multi-relational data
mining: A systematic literature review. <em>KIS</em>, <em>66</em>(9),
5113–5150. (<a
href="https://doi.org/10.1007/s10115-024-02139-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things massive adoption in many industrial areas in addition to the requirement of modern services is posing huge challenges to the field of data mining. Moreover, the semantic interoperability of systems and enterprises requires to operate between many different formats such as ontologies, knowledge graphs, or relational databases, as well as different contexts such as static, dynamic, or real time. Consequently, supporting this semantic interoperability requires a wide range of knowledge discovery methods with different capabilities that answer to the context of distributed architectures (DA). However, to the best of our knowledge there is no general review in recent time about the state of the art of Concept Analysis (CA) and multi-relational data mining (MRDM) methods regarding knowledge discovery in DA considering semantic interoperability. In this work, a systematic literature review on CA and MRDM is conducted, providing a discussion on the characteristics they have according to the papers reviewed, supported by a clusterization technique based on association rules. Moreover, the review allowed the identification of three research gaps toward a more scalable set of methods in the context of DA and heterogeneous sources.},
  archive      = {J_KIS},
  author       = {Leutwyler, Nicolás and Lezoche, Mario and Franciosi, Chiara and Panetto, Hervé and Teste, Laurent and Torres, Diego},
  doi          = {10.1007/s10115-024-02139-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5113-5150},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Methods for concept analysis and multi-relational data mining: A systematic literature review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fake review detection techniques, issues, and future
research directions: A literature review. <em>KIS</em>, <em>66</em>(9),
5071–5112. (<a
href="https://doi.org/10.1007/s10115-024-02118-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the impact of product or service reviews on customers&#39; purchasing decisions has become increasingly significant in online businesses. Consequently, manipulating reviews for fame or profit has become prevalent, with some businesses resorting to paying fake reviewers to post spam reviews. Given the importance of reviews in decision-making, detecting fake reviews is crucial to ensure fair competition and sustainable e-business practices. Although significant efforts have been made in the last decade to distinguish credible reviews from fake ones, it remains challenging. Our literature review has identified several gaps in the existing research: (1) most fake review detection techniques have been proposed for high-resource languages such as English and Chinese, and few studies have investigated low-resource and multilingual fake review detection, (2) there is a lack of research on deceptive review detection for reviews based on language code-switching (code-mix), (3) current multi-feature integration techniques extract review representations independently, ignoring correlations between them, and (4) there is a lack of a consolidated model that can mutually learn from review emotion, coarse-grained (overall rating), and fine-grained (aspect ratings) features to supplement the problem of sentiment and overall rating inconsistency. In light of these gaps, this study aims to provide an in-depth literature analysis describing strengths and weaknesses, open issues, and future research directions.},
  archive      = {J_KIS},
  author       = {Duma, Ramadhani Ally and Niu, Zhendong and Nyamawe, Ally S. and Tchaye-Kondi, Jude and Jingili, Nuru and Yusuf, Abdulganiyu Abdu and Deve, Augustino Faustino},
  doi          = {10.1007/s10115-024-02118-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5071-5112},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fake review detection techniques, issues, and future research directions: A literature review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An analysis of large language models: Their impact and
potential applications. <em>KIS</em>, <em>66</em>(9), 5047–5070. (<a
href="https://doi.org/10.1007/s10115-024-02120-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have transformed the interpretation and creation of human language in the rapidly developing field of computerized language processing. These models, which are based on deep learning techniques like transformer architectures, have been painstakingly trained on massive text datasets. This study paper takes an in-depth look into LLMs, including their architecture, historical evolution, and applications in education, healthcare, and finance sector. LLMs provide logical replies by interpreting complicated verbal patterns, making them beneficial in a variety of real-world scenarios. Their development and implementation, however, raise ethical concerns and have societal ramifications. Understanding the importance and limitations of LLMs is critical for guiding future research and ensuring the ethical use of their enormous potential. This survey exposes the influence of these models as they change, providing a roadmap for researchers, developers, and policymakers navigating the world of artificial intelligence and language processing.},
  archive      = {J_KIS},
  author       = {Bharathi Mohan, G. and Prasanna Kumar, R. and Vishal Krishh, P. and Keerthinathan, A. and Lavanya, G. and Meghana, Meka Kavya Uma and Sulthana, Sheba and Doss, Srinath},
  doi          = {10.1007/s10115-024-02120-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {5047-5070},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An analysis of large language models: Their impact and potential applications},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big data in transportation: A systematic literature analysis
and topic classification. <em>KIS</em>, <em>66</em>(8), 5021–5046. (<a
href="https://doi.org/10.1007/s10115-024-02112-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper identifies trends in the application of big data in the transport sector and categorizes research work across scientific subfields. The systematic analysis considered literature published between 2012 and 2022. A total of 2671 studies were evaluated from a dataset of 3532 collected papers, and bibliometric techniques were applied to capture the evolution of research interest over the years and identify the most influential studies. The proposed unsupervised classification model defined categories and classified the relevant articles based on their particular scientific interest using representative keywords from the title, abstract, and keywords (referred to as top words). The model’s performance was verified with an accuracy of 91% using Naïve Bayesian and Convolutional Neural Networks approach. The analysis identified eight research topics, with urban transport planning and smart city applications being the dominant categories. This paper contributes to the literature by proposing a methodology for literature analysis, identifying emerging scientific areas, and highlighting potential directions for future research.},
  archive      = {J_KIS},
  author       = {Tzika-Kostopoulou, Danai and Nathanail, Eftihia and Kokkinos, Konstantinos},
  doi          = {10.1007/s10115-024-02112-8},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {5021-5046},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Big data in transportation: A systematic literature analysis and topic classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic features analysis for biomedical lexical answer
type prediction using ensemble learning approach. <em>KIS</em>,
<em>66</em>(8), 5003–5019. (<a
href="https://doi.org/10.1007/s10115-024-02113-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lexical answer type prediction is integral to biomedical question–answering systems. LAT prediction aims to predict the expected answer’s semantic type of a factoid or list-type biomedical question. It also aids in the answer processing stage of a QA system to assign a high score to the most relevant answers. Although considerable research efforts exist for LAT prediction in diverse domains, it remains a challenging biomedical problem. LAT prediction for the biomedical field is a multi-label classification problem, as one biomedical question might have more than one expected answer type. Achieving high performance on this task is challenging as biomedical questions have limited lexical features. One biomedical question must be assigned multiple labels given these limited lexical features. In this paper, we develop a novel feature set (lexical, noun concepts, verb concepts, protein–protein interactions, and biomedical entities) from these lexical features. Using ensemble learning with bagging, we use the label power set transformation technique to classify multi-label. We evaluate the integrity of our proposed methodology on the publicly available multi-label biomedical questions dataset (MLBioMedLAT) and compare it with twelve state-of-the-art multi-label classification algorithms. Our proposed method attains a micro-F1 score of 77%, outperforming the baseline model by 25.5%.},
  archive      = {J_KIS},
  author       = {Hussain, Fiza Gulzar and Wasim, Muhammad and Cheema, Sehrish Munawar and Pires, Ivan Miguel},
  doi          = {10.1007/s10115-024-02113-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {5003-5019},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semantic features analysis for biomedical lexical answer type prediction using ensemble learning approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble multi-view feature set partitioning method for
effective multi-view learning. <em>KIS</em>, <em>66</em>(8), 4957–5001.
(<a href="https://doi.org/10.1007/s10115-024-02114-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning consistently outperforms traditional single-view learning by leveraging multiple perspectives of data. However, the effectiveness of multi-view learning heavily relies on how the data are partitioned into feature sets. In many cases, different datasets may require different partitioning methods to capture their unique characteristics, making a single partitioning method insufficient. Finding an optimal feature set partitioning (FSP) for each dataset may be a time-consuming process, and the optimal FSP may still not be sufficient for all types of datasets. Therefore, the paper presents a novel approach called ensemble multi-view feature set partitioning (EMvFSP) to improve the performance of multi-view learning, a technique that uses multiple data sources to make predictions. The proposed EMvFSP method combines the different views produced by multiple partitioning methods to achieve better classification performance than any single partitioning method alone. The experiments were conducted on 15 structured datasets with varying ratios of samples, features, and labels, and the results showed that the proposed EMvFSP method effectively improved classification performance. The paper also includes statistical analyses using Friedman ranking and Holms procedure to demonstrate the effectiveness of the proposed method. This approach provides a robust solution for multi-view learning that can adapt to different types of datasets and partitioning methods, making it suitable for a wide range of applications.},
  archive      = {J_KIS},
  author       = {Singh, Ritika and Kumar, Vipin},
  doi          = {10.1007/s10115-024-02114-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4957-5001},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ensemble multi-view feature set partitioning method for effective multi-view learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tuning structure learning algorithms with out-of-sample and
resampling strategies. <em>KIS</em>, <em>66</em>(8), 4927–4955. (<a
href="https://doi.org/10.1007/s10115-024-02111-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges practitioners face when applying structure learning algorithms to their data involves determining a set of hyperparameters; otherwise, a set of hyperparameter defaults is assumed. The optimal hyperparameter configuration often depends on multiple factors, including the size and density of the usually unknown underlying true graph, the sample size of the input data, and the structure learning algorithm. We propose a novel hyperparameter tuning method, called the Out-of-sample Tuning for Structure Learning (OTSL), that employs out-of-sample and resampling strategies to estimate the optimal hyperparameter configuration for structure learning, given the input dataset and structure learning algorithm. Synthetic experiments show that employing OTSL to tune the hyperparameters of hybrid and score-based structure learning algorithms leads to improvements in graphical accuracy compared to the state-of-the-art. We also illustrate the applicability of this approach to real datasets from different disciplines.},
  archive      = {J_KIS},
  author       = {Chobtham, Kiattikun and Constantinou, Anthony C.},
  doi          = {10.1007/s10115-024-02111-9},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4927-4955},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tuning structure learning algorithms with out-of-sample and resampling strategies},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CHEKG: A collaborative and hybrid methodology for
engineering modular and fair domain-specific knowledge graphs.
<em>KIS</em>, <em>66</em>(8), 4899–4925. (<a
href="https://doi.org/10.1007/s10115-024-02110-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies constitute the semantic model of Knowledge Graphs (KGs). This structural association indicates the potential existence of methodological analogies in the development of ontologies and KGs. The deployment of fully and well-defined methodologies for KG development based on existing ontology engineering methodologies (OEMs) has been suggested and efficiently applied. However, most of the modern/recent OEMs may not include tasks that (i) empower knowledge workers and domain experts to closely collaborate with ontology engineers and KG specialists for the development and maintenance of KGs, (ii) satisfy special requirements of KG development, such as (a) ensuring modularity and agility of KGs, (b) assessing and mitigating bias at schema and data levels. Toward this aim, the paper presents a methodology for the Collaborative and Hybrid Engineering of Knowledge Graphs (CHEKG), which constitutes a hybrid (schema-centric/top-down and data-driven/bottom-up), collaborative, agile, and iterative approach for developing modular and fair domain-specific KGs. CHEKG contributes to all phases of the KG engineering lifecycle: from the specification of a KG to its exploitation, evaluation, and refinement. The CHEKG methodology is based on the main phases of the extended Human-Centered Collaborative Ontology Engineering Methodology (ext-HCOME), while it adjusts and expands the individual processes and tasks of each phase according to the specialized requirements of KG development. Apart from the presentation of the methodology per se, the paper presents recent work regarding the deployment and evaluation of the CHEKG methodology for the engineering of semantic trajectories as KGs generated from unmanned aerial vehicles (UAVs) data during real cultural heritage documentation scenarios.},
  archive      = {J_KIS},
  author       = {Angelis, Sotiris and Moraitou, Efthymia and Caridakis, George and Kotis, Konstantinos},
  doi          = {10.1007/s10115-024-02110-w},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4899-4925},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CHEKG: A collaborative and hybrid methodology for engineering modular and fair domain-specific knowledge graphs},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biclustering-based multi-label classification. <em>KIS</em>,
<em>66</em>(8), 4861–4898. (<a
href="https://doi.org/10.1007/s10115-024-02109-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label classification, data can have multiple labels simultaneously. Two approaches to this issue are either transforming the multi-label data or adapting single-label algorithms for multi-label data. Despite the problem transformation’s effectiveness, some algorithms use fixed parameters to determine the number of subproblems, and the label relationships maintenance is done without using correlation or co-occurrence measures. In this work, the approach that converts multi-label problems into multiple binary subproblems was chosen because this offers a low execution time, enabling the use of complex single-label algorithms during classification. However, it has low performance in multi-label metrics. Thus, the BicbPT algorithm is introduced, which uses the biclustering technique combined with the multi-label to binary problem transformation to improve performance in multi-label metrics without increasing this transformation’s running time. For the evaluation, comparisons were made with the algorithms BR, CC, ECC, RAkEL and LP. Single-label algorithms SVM, C4.5 and Naive Bayes were applied to classify the binary subproblems across 12 datasets. The experiments demonstrate that BicbPT performed better in the multi-label metrics than the other multi-label to binary algorithms, being similar only to ECC. Still, the running time is up to 10 times higher in ECC, which makes the BicbPT better. Also, it keeps running time similar to algorithms in the multi-label to binary category. Finally, during the experiments, it was possible to perceive that the way the labels influence each other allow to improve the multi-label classification and not only consider maintaining the relationships like other approaches do.},
  archive      = {J_KIS},
  author       = {Schmitke, Luiz Rafael and Paraiso, Emerson Cabrera and Nievola, Julio Cesar},
  doi          = {10.1007/s10115-024-02109-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4861-4898},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Biclustering-based multi-label classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustness verification of k-nearest neighbors by abstract
interpretation. <em>KIS</em>, <em>66</em>(8), 4825–4859. (<a
href="https://doi.org/10.1007/s10115-024-02108-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the certification of stability properties, such as robustness and individual fairness, of the k-nearest neighbor algorithm (kNN). Our approach leverages abstract interpretation, a well-established program analysis technique that has been proven successful in verifying several machine learning algorithms, notably, neural networks, decision trees, and support vector machines. In this work, we put forward an abstract interpretation-based framework for designing a sound approximate version of the kNN algorithm, which is instantiated to the interval and zonotope abstractions for approximating the range of numerical features. We show how this abstraction-based method can be used for stability, robustness, and individual fairness certification of kNN. Our certification technique has been implemented and experimentally evaluated on several benchmark datasets. These experimental results show that our tool can formally prove the stability of kNN classifiers in a precise and efficient way, thus expanding the range of machine learning models amenable to robustness certification.},
  archive      = {J_KIS},
  author       = {Fassina, Nicolò and Ranzato, Francesco and Zanella, Marco},
  doi          = {10.1007/s10115-024-02108-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4825-4859},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Robustness verification of k-nearest neighbors by abstract interpretation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C22MP: The marriage of catch22 and the matrix profile
creates a fast, efficient and interpretable anomaly detector.
<em>KIS</em>, <em>66</em>(8), 4789–4823. (<a
href="https://doi.org/10.1007/s10115-024-02107-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many time series data mining algorithms work by reasoning about the relationships the conserved shapes of subsequences. To facilitate this, the Matrix Profile is a data structure that annotates a time series by recording each subsequence’s Euclidean distance to its nearest neighbor. In recent years, the community has shown that using the Matrix Profile it is possible to discover many useful properties of a time series, including repeated behaviors (motifs), anomalies, evolving patterns, regimes, etc. However, the Matrix Profile is limited to representing the relationship between the subsequence’s shapes. It is understood that, for some domains, useful information is conserved not in the subsequence’s shapes, but in the subsequence’s features. In recent years, a new set of features for time series called catch22 has revolutionized feature-based mining of time series. Combining these two ideas seems to offer many possibilities for novel data mining applications; however, there are two difficulties in attempting this. A direct application of the Matrix Profile with the catch22 features would be prohibitively slow. Less obviously, as we will demonstrate, in almost all domains, using all twenty-two of the catch22 features produces poor results, and we must somehow select the subset appropriate for the domain. In this work, we introduce novel algorithms to solve both problems and demonstrate that, for most domains, the proposed C22MP is a state-of-the-art anomaly detector.},
  archive      = {J_KIS},
  author       = {Tafazoli, Sadaf and Lu, Yue and Wu, Renjie and Srinivas, Thirumalai Vinjamoor Akhil and Dela Cruz, Hannah and Mercer, Ryan and Keogh, Eamonn},
  doi          = {10.1007/s10115-024-02107-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4789-4823},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {C22MP: The marriage of catch22 and the matrix profile creates a fast, efficient and interpretable anomaly detector},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving alzheimer’s classification using a modified borda
count voting method on dynamic ensemble classifiers. <em>KIS</em>,
<em>66</em>(8), 4755–4787. (<a
href="https://doi.org/10.1007/s10115-024-02106-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s detection is a challenging task for physicians. There are subtle differences in the bio-marker characteristics of Alzheimers and mild cognitive impairment patients which is very difficult to detect by a physician. Machine learning approaches are widely used for predicting a patient as having either Alzheimer’s or mild cognitive impairment. For developing models that distinguish between Alzheimer’s and mild cognitive impairment patients, the researchers used a dynamic ensemble of classifier selection algorithms. These algorithms perform voting on ensemble classifiers without considering preferential choices of the Alzheimer’s and mild cognitive impairment categories. Thus, this paper applies a modified Borda count voting weightage method instead of the majority voting and Borda voting for classifying Alzheimer’s, healthy control, and mild cognitive impairment patients classification on dynamic ensemble of classifier selection algorithms. Six dynamic ensemble of classifier selection algorithms are used in the study. Ten pools of classifiers including random forest, bagged decision tree, extra trees, Adaboost, rotation forest, decision tree, bagged support vector machine, bagged multilayer perceptrons, majority voting ensemble, and stacking classifier are used as classifier input for the dynamic ensemble of classifiers. The results suggest that the application of the proposed method can improve the classification performance for Alzheimer’s, mild cognitive impairment, and healthy patients when compared to the traditional voting methods after applying most of the dynamic ensemble of classifier selection algorithms used in the study. The application of a modified Borda count voting method on the dynamic ensemble of classifiers resulted in an increase of balanced classification accuracy ranging from 1 to 9%. The highest balanced classification accuracy of 86% is reported when random forest is applied to meta-learning for dynamic ensemble selection algorithms with the proposed voting method. It is also noted that there is an maximum increase in balanced classification accuracy of 9% is observed when applying rotation forest on K-nearest output profiles classifier using the proposed modified Borda count voting method. Thus, the increase in the balanced classification accuracy after applying the proposed modified Borda count voting method can make a positive impact on high-stakes healthcare applications like Alzheimer’s detection.},
  archive      = {J_KIS},
  author       = {Muhammed Niyas, K. P. and Paramasivan, Thiyagarajan},
  doi          = {10.1007/s10115-024-02106-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4755-4787},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving alzheimer’s classification using a modified borda count voting method on dynamic ensemble classifiers},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy rough set-based horse herd optimization algorithm
for map reduce framework for customer behavior data. <em>KIS</em>,
<em>66</em>(8), 4721–4753. (<a
href="https://doi.org/10.1007/s10115-024-02105-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of association rules often minimizes the reliability of data mining results; hence, a dimensionality reduction technique is crucial for data analysis. When analyzing massive datasets, existing models take more time to scan the entire database because they discover unnecessary items and transactions that are not necessary for data analysis. For this purpose, the Fuzzy Rough Set-based Horse Herd Optimization (FRS-HHO) algorithm is proposed to be integrated with the Map Reduce algorithm to minimize query retrieval time and improve performance. The HHO algorithm minimizes the number of unnecessary items and transactions with minimal support value from the dataset to maximize fitness based on multiple objectives such as support, confidence, interestingness, and lift to evaluate the quality of association rules. The feature value of each item in the population is obtained by a Map Reduce-based fitness function to generate optimal frequent itemsets with minimum time. The Horse Herd Optimization (HHO) is employed to solve the high-dimensional optimization problems. The proposed FRS-HHO approach takes less time to execute for dimensions and has a space complexity of 38% for a total of 10 k transactions. Also, the FRS-HHO approach offers a speedup rate of 17% and a 12% decrease in input–output communication cost when compared to other approaches. The proposed FRS-HHO model enhances performance in terms of execution time, space complexity, and speed.},
  archive      = {J_KIS},
  author       = {Sudha, D. and Krishnamurthy, M.},
  doi          = {10.1007/s10115-024-02105-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4721-4753},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A fuzzy rough set-based horse herd optimization algorithm for map reduce framework for customer behavior data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Misclassification-guided loss under the weighted
cross-entropy loss framework. <em>KIS</em>, <em>66</em>(8), 4685–4720.
(<a href="https://doi.org/10.1007/s10115-024-02123-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural networks for visual recognition gain momentum, many studies have modified the loss function to improve the classification performance on long-tailed data. Typical and effective improvement strategies are to assign different weights to different classes or samples, yielding a series of cost-sensitive re-weighting cross-entropy losses. Granted, most of these strategies only focus on the properties of the training data, such as the data distribution and the samples’ distinguishability. This paper works these strategies into a weighted cross-entropy loss framework with a simple production form ( $$\text {WCEL}_{\prod }$$ ), which takes into account different features of different losses. Also, there is this new loss function, misclassification-guided loss (MGL), that generalizes the class-wise difficulty-balanced loss and utilizes the misclassification rate on validation data to update class weights during training. In respect of MGL, a series of weighting functions with different relative preferences are introduced. Both softmax MGL and sigmoid MGL are derived to address the multi-class and multi-label classification problems. Experiments are undertaken on four public datasets, namely MNIST-LT, CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and a self-built dataset of 4 main-classes, 44 sub-classes, and altogether 57,944 images, where the results show that on the self-built dataset, the exponential weighting function achieves higher balanced accuracy than the polynomial function does. Ablation studies also show that MGL sees better performance in combination with most of other state-of-the-art loss functions under the $$\text {WCEL}_{\prod }$$ framework.},
  archive      = {J_KIS},
  author       = {Wu, Yan-Xue and Du, Kai and Wang, Xian-Jie and Min, Fan},
  doi          = {10.1007/s10115-024-02123-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4685-4720},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Misclassification-guided loss under the weighted cross-entropy loss framework},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing sentiment analysis via fusion of multiple
embeddings using attention encoder with LSTM. <em>KIS</em>,
<em>66</em>(8), 4667–4683. (<a
href="https://doi.org/10.1007/s10115-024-02102-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different embeddings capture various linguistic aspects, such as syntactic, semantic, and contextual information. Taking into account the diverse linguistic facets, we propose a novel hybrid model. This model hinges on the amalgamation of multiple embeddings through an attention encoder, subsequently channeled into an LSTM framework for sentiment classification. Our approach entails the fusion of Paragraph2vec, ELMo, and BERT embeddings to extract contextual information, while FastText is adeptly employed to capture syntactic characteristics. Subsequently, these embeddings were fused with the embeddings obtained from the attention encoder which forms the final embeddings. LSTM model is used for predicting the final classification. We conducted experiments utilizing both the Twitter Sentiment140 and Twitter US Airline Sentiment datasets. Our fusion model’s performance was evaluated and compared against established models such as LSTM, Bi-directional LSTM, BERT and Att-Coder. The test results clearly demonstrate that our approach surpasses the baseline models in terms of performance.},
  archive      = {J_KIS},
  author       = {Soni, Jitendra and Mathur, Kirti},
  doi          = {10.1007/s10115-024-02102-w},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4667-4683},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing sentiment analysis via fusion of multiple embeddings using attention encoder with LSTM},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Argumentation-based multi-agent distributed reasoning in
dynamic and open environments. <em>KIS</em>, <em>66</em>(8), 4631–4666.
(<a href="https://doi.org/10.1007/s10115-024-02101-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an approach for distributed and contextualized reasoning in multi-agent systems, considering environments in which agents may have incomplete, uncertain and inconsistent knowledge. Knowledge is represented by defeasible logic with mapping rules, which model the capability of agents to acquire knowledge from other agents during reasoning. Based on such knowledge representation, an argumentation-based reasoning model that enables distributed building of reusable argument structures to support conclusions is proposed. Conflicts between arguments are resolved by an argument strength calculation that considers the trust among agents and the degree of similarity between knowledge of different agents, based on the intuition that greater similarity between knowledge defined by different agents implies in less uncertainty about the validity of the built argument. Contextualized reasoning is supported through sharing of relevant knowledge by an agent when issuing queries to other agents, which enable the cooperating agents to be aware of knowledge not known a priori but that is important to reach a reasonable conclusion given the context of the agent that issued the query. A distributed algorithm is presented and analytically and experimentally evaluated asserting its computational feasibility. Finally, our approach is compared to related work, highlighting the contributions presented, demonstrating its applicability in a broader range of scenarios, and presenting perspectives for future work.},
  archive      = {J_KIS},
  author       = {Monte-Alto, Helio and Morveli-Espinoza, Mariela and Tacla, Cesar},
  doi          = {10.1007/s10115-024-02101-x},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4631-4666},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Argumentation-based multi-agent distributed reasoning in dynamic and open environments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Adaptive semi-supervised learning from stronger
augmentation transformations of discrete text information. <em>KIS</em>,
<em>66</em>(8), 4609–4629. (<a
href="https://doi.org/10.1007/s10115-024-02100-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning is a promising approach to dealing with the problem of insufficient labeled data. Recent methods grouped into paradigms of consistency regularization and pseudo-labeling have outstanding performances on image data, but achieve limited improvements when employed for processing textual information, due to the neglect of the discrete nature of textual information and the lack of high-quality text augmentation transformation means. In this paper, we propose the novel SeqMatch method. It can automatically perceive abnormal model states caused by anomalous data obtained by text augmentations and reduce their interferences and instead leverages normal ones to improve the effectiveness of consistency regularization. And it generates hard artificial pseudo-labels to enable the model to be efficiently updated and optimized toward low entropy. We also design several much stronger well-organized text augmentation transformation pipelines to increase the divergence between two views of unlabeled discrete textual sequences, thus enabling the model to learn more knowledge from the alignment. Extensive comparative experimental results show that our SeqMatch outperforms previous methods on three widely used benchmarks significantly. In particular, SeqMatch can achieve a maximum performance improvement of 16.4% compared to purely supervised training when provided with a minimal number of labeled examples.},
  archive      = {J_KIS},
  author       = {Zhang, Xuemiao and Tan, Zhouxing and Lu, Fengyu and Yan, Rui and Liu, Junfei},
  doi          = {10.1007/s10115-024-02100-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4609-4629},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive semi-supervised learning from stronger augmentation transformations of discrete text information},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach for fuzzy group decision making and consensus
measure with hesitant judgments of experts. <em>KIS</em>,
<em>66</em>(8), 4573–4608. (<a
href="https://doi.org/10.1007/s10115-024-02098-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some actual decision-making problems, experts may be hesitant to judge the performances of alternatives, which leads to experts providing decision matrices with incomplete information. However, most existing estimation methods for incomplete information in group decision-making (GDM) neglect the hesitant judgments of experts, possibly making the group decision outcomes unreasonable. Considering the hesitation degrees of experts in decision judgments, an approach is proposed based on the triangular intuitionistic fuzzy numbers (TIFNs) and TODIM (interactive and multiple criteria decision-making) method for GDM and consensus measure. First, TIFNs are applied to handle incomplete information due to the hesitant judgments of experts. Second, considering the risk attitudes of experts, a decision-making model is proposed to rank alternatives for GDM with incomplete information. Subsequently, based on measuring the concordance between solutions, a consensus model is presented to measure the group’s and individual’s consensus degrees. Finally, an illustrative example is presented to show the detailed implementation procedure of the proposed approach. The comparisons with some existing estimation methods verify the effectiveness of the proposed approach for handling incomplete information. The impacts and necessities of experts’ hesitation degrees are discussed by a sensitivity analysis.},
  archive      = {J_KIS},
  author       = {Huang, Chao and Wu, Xiaoyue and Lin, Mingwei and Xu, Zeshui},
  doi          = {10.1007/s10115-024-02098-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4573-4608},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An approach for fuzzy group decision making and consensus measure with hesitant judgments of experts},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep graph clustering via mutual information maximization
and mixture model. <em>KIS</em>, <em>66</em>(8), 4549–4572. (<a
href="https://doi.org/10.1007/s10115-024-02097-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering or community detection which learns to cluster the nodes of a graph is a challenging task in graph analysis. Recently contrastive learning has shown significant results in various unsupervised graph learning tasks. In spite of the success of graph contrastive learning methods in self-supervised graph learning, using them for graph clustering is not well explored. In this paper, we introduce a contrastive learning framework for learning clustering-friendly node embedding. We propose Gaussian mixture information maximization which utilizes a mutual information maximization approach for node embedding. Meanwhile, in order to have a clustering-friendly embedding space, it imposes a mixture of Gaussians distribution on this space. The parameters of the contrastive node embedding model and the mixture distribution are optimized jointly in a unified framework. Experiments show that our clustering-directed embedding space can enhance clustering performance in comparison with the case where community structure of the graph is ignored during node representation learning. The results on real-world datasets demonstrate the effectiveness of our method in community detection.},
  archive      = {J_KIS},
  author       = {Ahmadi, Maedeh and Safayani, Mehran and Mirzaei, Abdolreza},
  doi          = {10.1007/s10115-024-02097-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4549-4572},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep graph clustering via mutual information maximization and mixture model},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic bipartite network model based on structure and
preference features. <em>KIS</em>, <em>66</em>(8), 4527–4548. (<a
href="https://doi.org/10.1007/s10115-024-02093-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the complex network, the relationship in the real complex system can be modeled, and the bipartite network is a special complex network, which can describe the complex system containing two kinds of objects. Although existing bipartite networks can model complex systems, conventional methods are restricted to a couple of limitations. (1) The dynamic interaction between nodes cannot be described over time. (2) The implicit features of nodes in the network cannot be effectively mined. Based on these, this paper proposes a dynamic bipartite network model (DBN) to model the dynamic interaction between two types of objects in real complex systems, and mine the structure features and preference features of nodes in the network. First, the dynamic interaction between two types of objects in a complex system is modeled as a dynamic bipartite network, which can reflect the interaction between objects in each time slice. Then, the structure features and preference features of each time slice are mined based on the dynamic bipartite network, where the structure features reflect the dynamic structural changes of the nodes, and the preference features reflect the potential preferences of the nodes. Finally, the features of each time slice are fused and input into the gate recurrent unit model to predict the interaction between nodes. Extensive experiments are performed on a large-scale real complex system. The results show that DBN significantly outperforms state-of-the-art prediction methods in terms of multiple evaluation metrics.},
  archive      = {J_KIS},
  author       = {Lv, Hehe and Zou, Guobing and Zhang, Bofeng and Hu, Shengxiang and Zhou, Chenyang and Wu, Liangrui},
  doi          = {10.1007/s10115-024-02093-8},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4527-4548},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic bipartite network model based on structure and preference features},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting financial market structure from network features
using machine learning. <em>KIS</em>, <em>66</em>(8), 4497–4525. (<a
href="https://doi.org/10.1007/s10115-024-02095-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model that forecasts market correlation structure from link- and node-based financial network features using machine learning. For such, market structure is modeled as a dynamic asset network by quantifying time-dependent co-movement of asset price returns across company constituents of major global market indices. We provide empirical evidence using three different network filtering methods to estimate market structure, namely Dynamic Asset Graph, Dynamic Minimal Spanning Tree and Dynamic Threshold Networks. Experimental results show that the proposed model can forecast market structure with high predictive performance with up to $$40\%$$ improvement over a time-invariant correlation-based benchmark. Non-pair-wise correlation features showed to be important compared to traditionally used pair-wise correlation measures for all markets studied, particularly in the long-term forecasting of stock market structure. Evidence is provided for stock constituents of the DAX30, EUROSTOXX50, FTSE100, HANGSENG50, NASDAQ100 and NIFTY50 market indices. Findings can be useful to improve portfolio selection and risk management methods, which commonly rely on a backward-looking covariance matrix to estimate portfolio risk.},
  archive      = {J_KIS},
  author       = {Castilho, Douglas and Souza, Thársis T. P. and Kang, Soong Moon and Gama, João and de Carvalho, André C. P. L. F.},
  doi          = {10.1007/s10115-024-02095-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4497-4525},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Forecasting financial market structure from network features using machine learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An academic recommender system on large citation data based
on clustering, graph modeling and deep learning. <em>KIS</em>,
<em>66</em>(8), 4463–4496. (<a
href="https://doi.org/10.1007/s10115-024-02094-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation (recommender) systems (RS) have played a significant role in both research and industry in recent years. In the area of academia, there is a need to help researchers discover the most appropriate and relevant scientific information through recommendations. Nevertheless, we argue that there is a major gap between academic state-of-the-art RS and real-world problems. In this paper, we present a novel multi-staged RS based on clustering, graph modeling and deep learning that manages to run on a full dataset (scientific digital library) in the magnitude of millions users and items (papers). We run several tests (experiments/evaluation) as a means to find the best approach regarding the tuning of our system; so, we present and compare three versions of our RS regarding recall and NDCG metrics. The results show that a multi-staged RS that utilizes a variety of techniques and algorithms is able to face real-world problems and large academic datasets. In this way, we suggest a way to close or minimize the gap between research and industry value RS.},
  archive      = {J_KIS},
  author       = {Stergiopoulos, Vaios and Vassilakopoulos, Michael and Tousidou, Eleni and Corral, Antonio},
  doi          = {10.1007/s10115-024-02094-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4463-4496},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An academic recommender system on large citation data based on clustering, graph modeling and deep learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient parameter learning for bayesian network
classifiers following the apache spark dataframes paradigm.
<em>KIS</em>, <em>66</em>(8), 4437–4461. (<a
href="https://doi.org/10.1007/s10115-024-02096-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year the volume of information is growing at a high rate; therefore, more modern approaches are required to deal with such issues efficiently. Distributed systems, such as Apache Spark, offer such a modern approach, resulting in more and more machine learning models, being adapted into using distributed logic. In this paper, we propose a classification model, based on Bayesian Networks (BNs), that utilizes the distributed environment of Apache Spark using the Dataframes paradigm. This model can exploit any user-provided directed acyclic graph (DAG) that portrays the dependencies between the features of a dataset to estimate the parameters of the conditional probability distributions associated with each node in the graph to make accurate predictions. Moreover, in contrast with the majority of implementations that are only able to handle discrete features, it is also capable of efficiently handling continuous features by calculating the Gaussian probability density function.},
  archive      = {J_KIS},
  author       = {Akarepis, Ioannis and Bompotas, Agorakis and Makris, Christos},
  doi          = {10.1007/s10115-024-02096-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4437-4461},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient parameter learning for bayesian network classifiers following the apache spark dataframes paradigm},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning and deep learning models for human activity
recognition in security and surveillance: A review. <em>KIS</em>,
<em>66</em>(8), 4405–4436. (<a
href="https://doi.org/10.1007/s10115-024-02122-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) has received the significant attention in the field of security and surveillance due to its high potential for real-time monitoring, identifying the abnormal activities and situational awareness. HAR is able to identify the abnormal activity or behaviour patterns, which may indicate potential security risks. HAR system attempts to automatically provide the information and classification regarding activities performed in the environment by learning the data captured through sensor or video stream. The overview of existing research work in the security and surveillance area, which includes traditional, machine learning (ML) and deep learning (DL) algorithms applicable to field, is presented. The comparative analysis of different HAR techniques based on features, input source, public data sets is presented for quick understanding, and it focuses on the recent trends in HAR field. This review paper provides guidelines for the selection of appropriate algorithm, data set, performance metrics when evaluating HAR systems in the context of security and surveillance. Overall, this review aims to provide a comprehensive understanding of HAR in the field of security and surveillance and to serve as a basis for further research and development.},
  archive      = {J_KIS},
  author       = {Waghchaware, Sheetal and Joshi, Radhika},
  doi          = {10.1007/s10115-024-02122-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4405-4436},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Machine learning and deep learning models for human activity recognition in security and surveillance: A review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing trust and privacy in distributed networks: A
comprehensive survey on blockchain-based federated learning.
<em>KIS</em>, <em>66</em>(8), 4377–4403. (<a
href="https://doi.org/10.1007/s10115-024-02117-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into blockchain-based FL (BCFL), spotlighting the synergy between blockchain’s security features and FL’s privacy-preserving model training capabilities. First, we present the taxonomy of BCFL from three aspects, including decentralized, separate networks, and reputation-based architectures. Then, we summarize the general architecture of BCFL systems, providing a comprehensive perspective on FL architectures informed by blockchain. Afterward, we analyze the application of BCFL in healthcare, IoT, and other privacy-sensitive areas. Finally, we identify future research directions of BCFL.},
  archive      = {J_KIS},
  author       = {Liu, Ji and Chen, Chunlu and Li, Yu and Sun, Lin and Song, Yulun and Zhou, Jingbo and Jing, Bo and Dou, Dejing},
  doi          = {10.1007/s10115-024-02117-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {4377-4403},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing trust and privacy in distributed networks: A comprehensive survey on blockchain-based federated learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the effectiveness of machine learning models for
performance forecasting in basketball: A comparative study.
<em>KIS</em>, <em>66</em>(7), 4333–4375. (<a
href="https://doi.org/10.1007/s10115-024-02092-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports analytics (SA) incorporate machine learning (ML) techniques and models for performance prediction. Researchers have previously evaluated ML models applied on a variety of basketball statistics. This paper aims to benchmark the forecasting performance of 14 ML models, based on 18 advanced basketball statistics and key performance indicators (KPIs). The models were applied on a filtered pool of 90 high-performance players. This study developed individual forecasting scenarios per player and experimented using all 14 models. The models’ performance ranking was developed using a bespoke evaluation metric, called weighted average percentage error (WAPE), formulated from the weighted mean absolute percentage error (MAPE) evaluation results of each forecasted statistic and model. Moreover, we employed a comprehensive forecasting approach to improve KPI&#39;s results. Results showed that Tree-based models, namely Extra Trees, Random Forest, and Decision Tree, are the best performers in most of the forecasted performance indicators, with the best performance achieved by Extra Trees with a WAPE of 34.14%. In conclusion, we achieved a 3.6% MAPE improvement for the selected KPI with our approach on unseen data.},
  archive      = {J_KIS},
  author       = {Papageorgiou, George and Sarlis, Vangelis and Tjortjis, Christos},
  doi          = {10.1007/s10115-024-02092-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4333-4375},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluating the effectiveness of machine learning models for performance forecasting in basketball: A comparative study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis for online product recommendation with recalling
enhanced recurrent neural network-based sentiment. <em>KIS</em>,
<em>66</em>(7), 4309–4332. (<a
href="https://doi.org/10.1007/s10115-024-02091-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation system is used to filter the information according to the customer’s satisfaction. Based on consumer reviews, this approach discovers and compares product scores, ratings, and rankings. Here, the data are obtained from Amazon Product Recommendation dataset. These data are supplied to pre-processing. Altered phase preserving dynamic range compression acts as a pre-processing method to eliminate unsolicited details in the related text. Ternary Pattern and Discrete Wavelet Transform is used to this preprocessed output. The output is fed to recalling enriched recurrent neural network classifier, which classifies the product suggestion as five categories: excellent, very good, good, bad, and very bad. The proposed approach is activated in MATLAB; its efficacy is evaluated under some evaluation metrics, like accuracy, precision, sensitivity, specificity, f-measure, execution time. The proposed technique achieves 21.34%, 22.54%, 25.23%, 29.56% and 26.56% high accuracy; 29.29%, 25.35%, 27.45%, 26.75% and 27.95% higher AUC value and 10.136%, 9.04%. 10.45%, 11.45% and 18.81% lower execution time compared with existing methods.},
  archive      = {J_KIS},
  author       = {Kamal, N. and Sathiya, V. and Jayashree, D. and Shajin, Francis H.},
  doi          = {10.1007/s10115-024-02091-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4309-4332},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Analysis for online product recommendation with recalling enhanced recurrent neural network-based sentiment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural architecture search with heterogeneous
message-passing mechanisms. <em>KIS</em>, <em>66</em>(7), 4283–4308. (<a
href="https://doi.org/10.1007/s10115-024-02090-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural network search has been utilized in designing effective heterogeneous graph neural networks (HGNN) and has achieved remarkable performance beyond manually designed networks. Generally, there are two mainstream design manners in heterogeneous graph neural architecture search (HGNAS). The one is to automatically design a meta-graph to guide the direction of message-passing in a heterogeneous graph, thereby obtaining semantic information. The other learns to design the convolutional operator aiming to enhance message extraction capabilities to handle the diverse information in a heterogeneous graph. Through experiments, we observe a strong interdependence between message-passing direction and message extraction, which has a significant impact on the performance of HGNNs. However, previous HGNAS methods focus on one-sided design and lacked the ability to capture this interdependence. To address the issue, we propose a novel perspective called heterogeneous message-passing mechanism for HGNAS, which enables HGNAS to effectively capture the interdependence between message-passing direction and message extraction for designing HGNNs with better performance automatically. We call our method heterogeneous message-passing mechanisms search (HMMS). Extensive experiments on two popular tasks show that our method designs powerful HGNNs that have achieved SOTA results in different benchmark datasets. Codes are available at https://github.com/HetGNAS/HMMS .},
  archive      = {J_KIS},
  author       = {Wang, Yili and Chen, Jiamin and Li, Qiutong and He, Changlong and Gao, Jianliang},
  doi          = {10.1007/s10115-024-02090-x},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4283-4308},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph neural architecture search with heterogeneous message-passing mechanisms},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing multi-attribute similarity join using reduced and
adaptive index trees. <em>KIS</em>, <em>66</em>(7), 4251–4281. (<a
href="https://doi.org/10.1007/s10115-024-02089-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Attribute Similarity Join represents an important task for a variety of applications. Due to a large amount of data, several techniques and approaches were proposed to avoid superfluous comparisons between entities. One of these techniques is denominated Index Tree. In this work, we proposed an adaptive version (Adaptive Index Tree) of the state-of-the-art Index Tree for multi-attribute data. Our method selects the best filter configuration to construct the Adaptive Index Tree. We also proposed a reduced version of the Index Trees, aiming to improve the trade-off between efficacy and efficiency for the Similarity Join task. Finally, we proposed Filter and Feature selectors designed for the Similarity Join task. To evaluate the impact of the proposed approaches, we employed five real-world datasets to perform the experimental analysis. Based on the experiments, we conclude that our reduced approaches have produced superior results when compared to the state-of-the-art approach, specially when dealing with datasets that present a significant number of attributes and/or and expressive attribute sizes.},
  archive      = {J_KIS},
  author       = {Silva, Vítor Bezerra and Nascimento, Dimas Cassimiro},
  doi          = {10.1007/s10115-024-02089-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4251-4281},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing multi-attribute similarity join using reduced and adaptive index trees},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMUIL: Enhancing multi-platform user identity linkage with
multi-information. <em>KIS</em>, <em>66</em>(7), 4221–4249. (<a
href="https://doi.org/10.1007/s10115-024-02088-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User identity linkage (UIL) aims to link identities belonging to the same individual across various platforms. While numerous methods have been proposed for paired or multiple platforms, UIL is still a non-trivial task due to the following challenges. (1) How to alleviate the sparsity and incompleteness of user information from different platforms? (2) How can UIL approaches achieve high effectiveness while still maintaining low complexity in multi-platform scenarios? In light of these challenges, we propose MMUIL (enhancing multi-platform user identity linkage with multi-information), a novel model excelling in high effectiveness while still maintaining low complexity. The model consists of a Multi-Information Embedding (MIE) module and a Partially Shared Adversarial Learning (PSAL) module. Specifically, for the first challenge, MIE simultaneously considers the token sequence semantics in usernames and the structural information of multi-type networks (i.e., homogeneous and heterogeneous networks). To address the second challenge, the adversarial learning-based PSAL decreases the complexity with shared partial parameters (i.e., shared generators). Meanwhile, to enhance the model’s effectiveness, PSAL exploits an attention mechanism to mitigate the disadvantages of shared partial parameters, such as partial information loss and noise introduction, while integrating the above multi-information intensively. The extensive experiments conducted on two real-world datasets demonstrate that our proposed model MMUIL significantly outperforms the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhou, Qian and Hei, Yihan and Chen, Wei and Zheng, Shangfei and Zhao, Lei},
  doi          = {10.1007/s10115-024-02088-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4221-4249},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MMUIL: Enhancing multi-platform user identity linkage with multi-information},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A quality-of-service aware composition-method for cloud
service using discretized ant lion optimization algorithm. <em>KIS</em>,
<em>66</em>(7), 4199–4220. (<a
href="https://doi.org/10.1007/s10115-024-02086-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the cloud system, service providers supply a pool of resources in the form of a web service and the services are merged to provide the required composite services. Composing a quality-of-service aware web service is like the knapsack problem and this problem is NP-hard. Different artificial intelligence and heuristic methods have been used to achieve optimal or near-optimal composite services. In this paper, the Ant Lion optimization algorithm was modified and discretized to choose the appropriate web services from the existing services and to provide the optimal composite services. The QWS dataset contains a collection of 2507 real-world web services which are used to evaluate the proposed method. In this study, response time parameters, availability, throughput, success capability, reliability, and latency were used as the web service quality metrics. The results of the conducted experiments confirm that the provided composite service by the proposed method has considerably higher quality than the other related algorithms. Hence, the proposed method can be used in the cloud resource discovery layer.},
  archive      = {J_KIS},
  author       = {Arasteh, Bahman and Aghaei, Babak and Bouyer, Asgarali and Arasteh, Keyvan},
  doi          = {10.1007/s10115-024-02086-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4199-4220},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A quality-of-service aware composition-method for cloud service using discretized ant lion optimization algorithm},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-factor stock price prediction based on GAN-TrellisNet.
<em>KIS</em>, <em>66</em>(7), 4177–4198. (<a
href="https://doi.org/10.1007/s10115-024-02085-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying deep learning, especially time series neural networks, to predict stock price, has become one of the important applications in quantitative finance. Recently, some GAN-based stock prediction models are proposed, where LSTM or GRU is used as the generator. However, these generators lack the function of feature extraction, and the prediction accuracies are slightly low. Meanwhile, these models choose some simple volume-price factors (such as OCHLV and OCHLVC) as inputs, without considering the impact of other factors on stock prices. In order to solve these problems, a stock prediction method based on multiple factors and GAN-TrellisNet is proposed. Instead of “OCHLV” or ”OCHLVC,” a multi-factor strategy with ”alpha158+OCHLVC” is introduced to enrich the stock data of inputs. The proposed generative adversarial network (GAN) is a combination of two neural networks which are TrellisNet as generative model and convolutional neural network (CNN) as discriminative model for adversarial training to forecast the stock market. TrellisNet, which integrates the feature extraction capabilities of CNN and the temporal processing capabilities of recurrent neural network (RNN), will generate new predicted results based on historical data, and then CNN will distinguish between predicted results and real stock prices. In order to demonstrate the performance of our method, we selected the decade data of different stocks from four markets (A-shares, U.S. stocks, U.K. stocks and Hong Kong stocks) as dataset and conducted two groups of comparative experiments. Compared with the state-of-the-art methods based on GAN, our method has better performance in terms of MSE, MAE, RMSE and MAPE. In addition, the multi-factor strategy with “alpha158+OCHLVC” is more effective than the original strategy with OCHLVC factors.},
  archive      = {J_KIS},
  author       = {Liu, Wenjie and Ge, Yebo and Gu, Yuchen},
  doi          = {10.1007/s10115-024-02085-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4177-4198},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-factor stock price prediction based on GAN-TrellisNet},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influential users identification under the non-progressive
LTIRS model. <em>KIS</em>, <em>66</em>(7), 4151–4176. (<a
href="https://doi.org/10.1007/s10115-024-02084-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of the key influencers is one of the most important strategies for initiating any transmission process in a social network. However, many of the current studies on influence transmission concentrate primarily on the progressive dissemination phenomenon. Furthermore, the various methods for selecting key influencers that are being explored so far either depend on the structure of the network or the connections between users. As a result, the topology of the network may affect the functioning of these existing methods. Therefore, this paper presents an LTIRS non-progressive model that works on comprehending the non-progressive influence transmission phenomenon, where an influencer may eventually lose their power to influence for a period and the same influencer may revive and engage in active transmission again in the future. In this work, we also propose a scheme named the “AASN” method where the queueing theory is applied in the LTIRS model to investigate the impact of change in the state of a node and compute its influence capacity. Thus, this method provides an efficient way for selecting the key influencer irrespective of the topological properties of the network. The effectiveness of the proposed method is demonstrated by experimental studies on a few real-world datasets.},
  archive      = {J_KIS},
  author       = {Devi, Kalyanee and Tripathi, Rohit},
  doi          = {10.1007/s10115-024-02084-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4151-4176},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Influential users identification under the non-progressive LTIRS model},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relational multi-scale metric learning for few-shot
knowledge graph completion. <em>KIS</em>, <em>66</em>(7), 4125–4150. (<a
href="https://doi.org/10.1007/s10115-024-02083-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph completion (FKGC) refers to the task of inferring missing facts in a knowledge graph by utilizing a limited number of reference entities. Most FKGC methods assume a single similarity metric, which leads to a single feature space and makes it difficult to separate positive and negative samples effectively. Therefore, we propose a multi-scale relational metric network (MSRMN) specifically designed for FKGC, which integrates multiple scales of measurement methods to learn a more comprehensive and compact feature space. In this study, we design a complete neighbor random sampling algorithm to sample complete one-hop neighbor information, and aggregate both one-hop and multi-hop neighbor information to enhance entity representations. Then, MSRMN adaptively obtains prototype representations of relations and integrates three different scales of measurement methods to learn a more comprehensive feature space and a more discriminative feature mapping, enabling positive query entity pairs to obtain higher measurement scores. Evaluation of MSRMN on two public datasets for link prediction demonstrates that MSRMN attains top-performing outcomes across various few-shot sizes on the NELL dataset.},
  archive      = {J_KIS},
  author       = {Song, Yu and Gui, Mingyu and Zhang, Kunli and Xu, Zexi and Dai, Dongming and Kong, Dezhi},
  doi          = {10.1007/s10115-024-02083-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4125-4150},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Relational multi-scale metric learning for few-shot knowledge graph completion},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the potential of deep regression model for
next-location prediction. <em>KIS</em>, <em>66</em>(7), 4093–4124. (<a
href="https://doi.org/10.1007/s10115-024-02082-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based services are gaining popularity; prediction of future destinations and crowd movement patterns are crucial components of these services. This article presents an attention-based neural network regression model designed to forecast future user locations, a critical aspect of location-based services. Leveraging two real-world mobility datasets, New York City check-in data from the Foursquare API and Porto taxi trajectories from Portugal. The model employs an attention-based encoder–decoder neural network to predict user destination coordinates in latitude and longitude. Beyond its predictive capabilities, this research delves into the intricacies of human mobility patterns, contributing to a deeper understanding of movement behavior and shedding light on challenges in current models for mobility prediction. The study explores the impact of various optimization algorithms on model performance, analyzing their effects on accuracy, with the mean haversine distance error serving as the evaluation metric. Notably, the model achieves remarkable results, giving a mean haversine distance error of 1.3336 for the Porto dataset and 1.6379 for the Foursquare NYC dataset when employing the Adam optimizer. We have extended our study by implementing our model on Universal Transverse Mercator coordinate systems. These findings underscore the model’s superiority over previous approaches, offering valuable insights for developing more precise location-based services and advancing mobility and human behavior analysis.},
  archive      = {J_KIS},
  author       = {Shukla, Pushpak and Shukla, Shailendra},
  doi          = {10.1007/s10115-024-02082-x},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4093-4124},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploring the potential of deep regression model for next-location prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Local soft rough approximations and their applications to
conflict analysis problems. <em>KIS</em>, <em>66</em>(7), 4063–4092. (<a
href="https://doi.org/10.1007/s10115-024-02081-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local rough sets are an efficient model to analyze large-scale datasets with finite labels because they are an essential development in classical rough sets. The objective of this paper, we put forth the idea of a local soft rough approximation measure (LSRAM), which preserves rough approximation measure associated characteristics from the context of traditional rough set theory. In order to account for the uncertainty brought on by the difference between the given lower and upper approximations based on soft equivalence relation, we propose the notion of local soft knowledge distance (LSKD). Moreover, some associated proposition’s, theorems, corollaries, and a novel GM built on the LSKD model are given. Subsequently, LSRAM is combined with the suggested GM to create the enhanced LSRAM. This illustrates that the upgraded LSRAM maintains monotonicity with granularity subdivision. Further, to examine the conflict situation in the Middle East, we create a new conflict analysis model that is based on local soft rough sets in a framework of soft equivalence relations and soft indiscernible relations. Finally, we provided positive responses to a number of queries that different authors had raised. Our recently constructed model is much more effective than the existing strategies, according to an analysis of a general algorithm for conflict problems.},
  archive      = {J_KIS},
  author       = {Ansari, Moin Akhtar and Rehman, Noor and Ali, Abbas and Hila, Kostaq and Mubeen, Tahira},
  doi          = {10.1007/s10115-024-02081-y},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4063-4092},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Local soft rough approximations and their applications to conflict analysis problems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GTHP: A novel graph transformer hawkes process for
spatiotemporal event prediction. <em>KIS</em>, <em>66</em>(7),
4043–4062. (<a
href="https://doi.org/10.1007/s10115-024-02080-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event sequences with spatiotemporal characteristics have been rapidly produced in various domains, such as earthquakes in seismology, electronic medical records in healthcare, and transactions in the financial market. These data often continue for weeks, months, or years, and the past events may trigger subsequent events. In this context, modeling the spatiotemporal event sequences and forecasting the next event has become a hot topic. However, existing models either failed to capture the long-term temporal dependencies or ignored the essential spatial information between sequences. In this paper, we proposed a novel graph transformer Hawkes process (GTHP) model to capture the long-term temporal dependencies and spatial information from historical events. The core concept of GTHP is to learn the spatial information by graph convolutional neural networks and capture long-term temporal dependencies from events embedding by self-attention mechanism. Moreover, we integrated the learned spatial information into the event embedding as auxiliary information. Numerous experiments on synthetic and real-world datasets proved the effectiveness of the proposed model.},
  archive      = {J_KIS},
  author       = {Xie, Yiman and Wu, Jianbin and Zhou, Yan},
  doi          = {10.1007/s10115-024-02080-z},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4043-4062},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GTHP: A novel graph transformer hawkes process for spatiotemporal event prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise-free sampling with majority framework for an
imbalanced classification problem. <em>KIS</em>, <em>66</em>(7),
4011–4042. (<a
href="https://doi.org/10.1007/s10115-024-02079-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance has been widely accepted as a significant factor that negatively impacts a machine learning classifier’s performance. One of the techniques to avoid this problem is to balance the data distribution by using sampling-based approaches, in which synthetic data is generated using the probability distribution of the classes. However, this process is sensitive to the presence of noise in the data, and the boundaries between the majority class and the minority class are blurred. Such phenomena shift the algorithm’s decision boundary away from the ideal outcome. In this work, we propose a hybrid framework for two primary objectives. The first objective is to address class distribution imbalance by synthetically increasing the data of a minority class, and the second objective is, to devise an efficient noise reduction technique that improves the class balance algorithm. The proposed framework focuses on removing noisy elements from the majority class, and by doing so, provides more accurate information to the subsequent synthetic data generator algorithm. To evaluate the effectiveness of our framework, we employ the geometric mean (G-mean) as the evaluation metric. The experimental results show that our framework is capable of improving the prediction G-mean for eight classifiers across eleven datasets. The range of improvements varies from 7.78% on the Loan dataset to 67.45% on the Abalone19_vs_10-11-12-13 dataset.},
  archive      = {J_KIS},
  author       = {Firdausanti, Neni Alya and Mendonça, Israel and Aritsugi, Masayoshi},
  doi          = {10.1007/s10115-024-02079-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {4011-4042},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Noise-free sampling with majority framework for an imbalanced classification problem},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rényi-type quasimetric with random interference detection.
<em>KIS</em>, <em>66</em>(7), 3989–4009. (<a
href="https://doi.org/10.1007/s10115-024-02078-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new dissimilarity measure between two discrete and finite probability distributions. The followed approach is grounded jointly on mixtures of probability distributions and an optimization procedure. We discuss the clear interpretation of the constitutive elements of the measure under an information-theoretical perspective by also highlighting its connections with the Rényi divergence of infinite order. Moreover, we show how the measure describes the inefficiency in assuming that a given probability distribution coincides with a benchmark one by giving formal writing of the random interference between the considered probability distributions. We explore the properties of the considered tool, which are in line with those defining the concept of quasimetric—i.e. a divergence for which the triangular inequality is satisfied. As a possible usage of the introduced device, an application to rare events is illustrated. This application shows that our measure may be suitable in cases where the accuracy of the small probabilities is a relevant matter.},
  archive      = {J_KIS},
  author       = {Cerqueti, Roy and Maggi, Mario},
  doi          = {10.1007/s10115-024-02078-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3989-4009},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A rényi-type quasimetric with random interference detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantic-based methodology for the management of document
workflows in e-government: A case study for judicial processes.
<em>KIS</em>, <em>66</em>(7), 3959–3987. (<a
href="https://doi.org/10.1007/s10115-024-02077-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trial excessive duration is a common problem in Juridical systems worldwide, even if some countries seem to be more affected by it than others. The European Council has provided metrics and statistics to identify this problem and has pointed out solutions, such as the simplification of norms and the digitization of Juridical procedures. The Italian Telematic Civil Process (TCP) is an example of this digitization effort that has surely positively influenced the duration of Trials, their traceability and general complexity. However, there are still many possible actions that can be taken to simplify the work of Judges and Chancellors, and to support their daily operations in dealing with several Trials at once, and with the consistent number of documents that are involved in them. This paper presents a toolchain and a related methodology for the management of documentation attached to Trials, based on semantic technologies and Natural Language Processing techniques, which will help Judges in faster assessing the situation of each Trial they follow, and will also provide the means to identify potential correlations among different Juridical procedures. The methodology is tested against a case study, i.e. the compensation requests related to road accidents, which has been provided and described by Domain Experts from the Italian Ministry of Justice.},
  archive      = {J_KIS},
  author       = {Di Martino, Beniamino and Colucci Cante, Luigi and Graziano, Mariangela and D’Angelo, Salvatore and Esposito, Antonio and Lupi, Pietro and Ammendolia, Rosario},
  doi          = {10.1007/s10115-024-02077-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3959-3987},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A semantic-based methodology for the management of document workflows in e-government: A case study for judicial processes},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying faceted search and analytics over RDF knowledge
graphs. <em>KIS</em>, <em>66</em>(7), 3921–3958. (<a
href="https://doi.org/10.1007/s10115-024-02076-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formulation of analytical queries over Knowledge Graphs in RDF is a challenging task that presupposes familiarity with the syntax of the corresponding query languages and the contents of the graph. To alleviate this problem, we introduce a model for aiding users in formulating analytic queries over complex, i.e., not necessarily star schema-based, RDF Knowledge Graphs. To come up with an intuitive interface, we leverage the familiarity of users with Faceted Search systems. In particular, we start from a general model for Faceted Search over RDF data, and we extend it with actions that enable users to formulate analytic queries, too. Thus, the proposed model can be used not only for formulating analytic queries but also for exploratory purposes, i.e., for locating the desired resources in a Faceted Search manner. We describe the model from various perspectives, i.e., (1) we propose a generic user interface for intuitively analyzing RDF Knowledge Graphs, (2) we define formally the state space of the interaction model and the required algorithms for producing the user interface actions, (3) we present an implementation of the model that showcases its feasibility, and (4) we discuss the results of an evaluation with users that provides evidence for the acceptance of the method by users. Apart from being intuitive for end users, another distinctive characteristic of the proposed model is that it allows the gradual formulation of complex analytic queries (including nested ones).},
  archive      = {J_KIS},
  author       = {Papadaki, Maria-Evangelia and Tzitzikas, Yannis},
  doi          = {10.1007/s10115-024-02076-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3921-3958},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unifying faceted search and analytics over RDF knowledge graphs},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tri-XGBoost model improved by BLSmote-ENN: An interpretable
semi-supervised approach for addressing bankruptcy prediction.
<em>KIS</em>, <em>66</em>(7), 3883–3920. (<a
href="https://doi.org/10.1007/s10115-024-02067-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bankruptcy prediction is considered one of the most important research topics in the field of finance and accounting. The rapid increase of data science, artificial intelligence, and machine learning has led researchers to build an accurate bankruptcy prediction model. Recent studies show that ensemble methods perform better than traditional machine learning models for predicting corporate failure, especially with highly imbalanced datasets. However, the black box property of these techniques remains challenging to interpret the result and generate corporate classes without any explanation. To this end, we propose to build an accurate and interpretable classification model that generates a set of prediction rules for output. Tri-eXtreme Gradient Boosting (Tri-XGBoost), a semi-supervised technique, is recommended in this paper. The proposed method combines Borderline-Smote (BLSmote) based on Edited Nearest Neighbor (ENN) sampling techniques with three different XGBoost methods as weak classifiers (gbtree, gblinear, and dart). First, the resampling techniques are used to produce more representative synthetic data and balance the distribution of the datasets. To this end, BLSmote is applied to increase the proportion of instances in the minority class (bankrupt data). Then, ENN is used to eliminate the noisy samples from both classes. In addition, the most crucial features that increase predictive accuracy are chosen using XGBoost. Finally, in order to make the model more understandable for both applicants and experts, our result is presented as &quot;IF–THEN&quot; rules. Our proposed model is validated using the imbalanced Polish and Taiwan bankruptcy datasets. Our obtained results demonstrate that our suggested model performs better than the existing models based on the area under the ROC curve (AUC), F1-score, and G-mean performance measures. Our proposed model significantly improves classification accuracy, which is greater than 95% for Polish datasets and more than 93% for Taiwanese dataset in terms of AUC, G-mean and F1-score.},
  archive      = {J_KIS},
  author       = {Smiti, Salima and Soui, Makram and Ghedira, Khaled},
  doi          = {10.1007/s10115-024-02067-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3883-3920},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tri-XGBoost model improved by BLSmote-ENN: An interpretable semi-supervised approach for addressing bankruptcy prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PatchMix: Patch-level mixup for data augmentation in
convolutional neural networks. <em>KIS</em>, <em>66</em>(7), 3855–3881.
(<a href="https://doi.org/10.1007/s10115-024-02141-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have demonstrated impressive performance in fitting data distribution. However, due to the complexity in learning intricate features from data, networks usually experience overfitting during the training. To address this issue, many data augmentation techniques have been proposed to expand the representation of the training data, thereby improving the generalization ability of CNNs. Inspired by jigsaw puzzles, we propose PatchMix, a novel mixup-based augmentation method that applies mixup to patches within an image to extract abundant and varied information from it. At the input level of CNNs, PatchMix can generate a multitude of reliable training samples through an integrated and controllable approach that encompasses cropping, combining, blurring, and more. Additionally, we propose PatchMix-R to enhance the robustness of the model against perturbations by processing adjacent pixels. Easy to implement, our methods can be integrated with most CNN-based classification models and combined with varying data augmentation techniques. The experiments show that PatchMix and PatchMix-R consistently outperform other state-of-the-art methods in terms of accuracy and robustness. Class activation mappings of the trained model are also investigated to visualize the effectiveness of our approach.},
  archive      = {J_KIS},
  author       = {Hong, Yichao and Chen, Yuanyuan},
  doi          = {10.1007/s10115-024-02141-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3855-3881},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PatchMix: Patch-level mixup for data augmentation in convolutional neural networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating localized learning for cardinality estimation
based on XGBoost. <em>KIS</em>, <em>66</em>(7), 3825–3854. (<a
href="https://doi.org/10.1007/s10115-024-02142-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cardinality estimation in DBMS, building multiple local models instead of one global model can usually improve estimation accuracy as well as reducing the effort to label large amounts of training data. Unfortunately, the existing approach of localized learning requires users to explicitly specify which query patterns a local model can handle. Making these decisions is very arduous and error-prone for users; to make things worse, it limits the usability of local models. In this paper, we propose a localized learning solution for cardinality estimation based on XGBoost, which can automatically build an optimal combination of local models given a query workload. It consists of two phases: 1) model initialization; 2) model evolution. In the first phase, it clusters training data into a set of coarse-grained query pattern groups based on pattern similarity and constructs a separate local model for each group. In the second phase, it iteratively merges and splits clusters to identify an optimal combination by reconstructing local models. We formulate the problem of identifying the optimal combination of local models as a combinatorial optimization problem and present an efficient heuristic algorithm, named MMS (Models Merging and Splitting), for its solution due to its exponential complexity. Finally, we validate its performance superiority over the existing learning alternatives by extensive experiments on real datasets.},
  archive      = {J_KIS},
  author       = {Feng, Jieming and Li, Zhanhuai and Chen, Qun and Liu, Hailong},
  doi          = {10.1007/s10115-024-02142-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3825-3854},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automating localized learning for cardinality estimation based on XGBoost},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entity linking for english and other languages: A survey.
<em>KIS</em>, <em>66</em>(7), 3773–3824. (<a
href="https://doi.org/10.1007/s10115-023-02059-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting named entities text forms the basis for many crucial tasks such as information retrieval and extraction, machine translation, opinion mining, sentiment analysis and question answering. This paper presents a survey of the research literature on named entity linking, including named entity recognition and disambiguation. We present 200 works by focusing on 43 papers (5 surveys and 38 research works). We also describe and classify 56 resources, including 25 tools and 31 corpora. We focus on the most recent papers, where more than 95% of the described research works are after 2015. To show the efficiency of our construction methodology and the importance of this state of the art, we compare it to other surveys presented in the research literature, which were based on different criteria (such as the domain, novelty and presented models and resources). We also present a set of open issues (including the dominance of the English language in the proposed studies and the frequent use of NER rather than the end-to-end systems proposing NED and EL) related to entity linking based on the research questions that this survey aims to answer.},
  archive      = {J_KIS},
  author       = {Guellil, Imane and Garcia-Dominguez, Antonio and Lewis, Peter R. and Hussain, Shakeel and Smith, Geoffrey},
  doi          = {10.1007/s10115-023-02059-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3773-3824},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Entity linking for english and other languages: A survey},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing knowledge discovery and management through
intelligent computing methods: A decisive investigation. <em>KIS</em>,
<em>66</em>(7), 3719–3771. (<a
href="https://doi.org/10.1007/s10115-024-02099-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Discovery and Management (KDM) encompasses a comprehensive process and approach involving the creation, discovery, capture, organization, refinement, presentation, and provision of data, information, and knowledge with a specific goal in mind. At the core, Knowledge Management and Artificial Intelligence (AI) revolve around knowledge itself. AI serves as the mechanism enabling machines to obtain, acquire, process, and utilize information, thereby executing tasks and uncovering knowledge that can be shared with people to enhance strategic decision-making. While conventional methods play a role in the KDM process, incorporating intelligent approaches can further enhance efficiency in terms of time and accuracy. Intelligent techniques, particularly soft computing approaches, possess the ability to learn in any environment by leveraging logic, reasoning, and other computational capabilities. These techniques can be broadly categorized into Learning algorithms (Supervised, Unsupervised, and Reinforcement), Logic and Rule-Based algorithms (Fuzzy Logic, Bayesian Network, and CBR-RBR), Nature-inspired algorithms (Genetic algorithm, Particle Swarm Optimization, and Ant Colony Optimization), and hybrid approaches that combine these algorithms. The primary objective of these intelligent techniques is to address the day-to-day challenges faced by rural and smart digital societies. In this study, the authors extensively investigated various intelligent computing methods (ICMs) specifically relevant to distinct problems, providing accurate and reasonable knowledge-based solutions. The application of both single ICMs and combined ICMs was explored to solve domain-specific problems, and their effectiveness was analyzed and discussed. The results indicated that combined ICMs exhibited superior efficiency compared to single ICMs. Furthermore, the authors conducted an analysis and comparison of ICMs based on their application domain, parameters, methods/algorithms, efficiency, and acceptable outcomes. Additionally, the authors identified several problem scenarios that can be effectively resolved using intelligent techniques.},
  archive      = {J_KIS},
  author       = {Ahamad, Rayees and Mishra, Kamta Nath},
  doi          = {10.1007/s10115-024-02099-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3719-3771},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing knowledge discovery and management through intelligent computing methods: A decisive investigation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trends and challenges in sentiment summarization: A
systematic review of aspect extraction techniques. <em>KIS</em>,
<em>66</em>(7), 3671–3717. (<a
href="https://doi.org/10.1007/s10115-024-02075-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Summarization is an automated technology that extracts important features of sentences and then reorganizes selected words or sentences by their aspect class and sentiment polarity. This emerging research area wields considerable influence, where a sentiment-based summary can provide insight into users’ subjective opinions, creating social engagement that benefits industry players and entrepreneurs. Meanwhile, systematic studies examining sentiment-based summarization, particularly those delving into aspect levels, are still limited. Whereas aspects are crucial to obtain a comprehensive assessment of a product or service for improving sentiment summarization results. Hence, we conducted a comprehensive survey of aspect extraction techniques in sentiment summarization by classifying techniques based on sentiment analysis levels and features. This work analyzes the current research trends and challenges in the research domain from a different perspective. More than 150 literature published from 2004 to 2023 are collected mainly from credible academic databases. We summarized and performed a comparative analysis of the sentiment summarization approaches and tabulated their performance based on different domains, sentiment levels, and features. We also derived a thematic taxonomy of aspect extraction techniques in sentiment summarization from the analysis and illustrated its usage in various applications. Finally, this study presents recommendations for the challenges and opportunities for future research development.},
  archive      = {J_KIS},
  author       = {Hayatin, Nur and Alias, Suraya and Hung, Lai Po},
  doi          = {10.1007/s10115-024-02075-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3671-3717},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trends and challenges in sentiment summarization: A systematic review of aspect extraction techniques},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring aspect-based sentiment analysis: An in-depth
review of current methods and prospects for advancement. <em>KIS</em>,
<em>66</em>(7), 3639–3669. (<a
href="https://doi.org/10.1007/s10115-024-02104-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a natural language processing technique that seeks to recognize and extract the sentiment connected to various qualities or aspects of a specific good, service, or entity. It entails dissecting a text into its component pieces, determining the elements or aspects being examined, and then examining the attitude stated about each feature or aspect. The main objective of this research is to present a comprehensive understanding of aspect-based sentiment analysis (ABSA), such as its potential, ongoing trends and advancements, structure, practical applications, real-world implementation, and open issues. The current sentiment analysis aims to enhance granularity at the aspect level with two main objectives, including extracting aspects and polarity sentiment classification. Three main methods are designed for aspect extractions: pattern-based, machine learning and deep learning. These methods can capture both syntactic and semantic features of text without relying heavily on high-level feature engineering, which was a requirement in earlier approaches. Despite bringing traditional surveys, a comprehensive survey of the procedure for carrying out this task and the applications of ABSA are also included in this article. To fully comprehend each strategy&#39;s benefits and drawbacks, it is evaluated, compared, and investigated. To determine future directions, the ABSA’s difficulties are finally reviewed.},
  archive      = {J_KIS},
  author       = {Kandhro, Irfan Ali and Ali, Fayyaz and Uddin, Mueen and Kehar, Asadullah and Manickam, Selvakumar},
  doi          = {10.1007/s10115-024-02104-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {3639-3669},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploring aspect-based sentiment analysis: An in-depth review of current methods and prospects for advancement},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Argumentation effect of a chatbot for ethical discussions
about autonomous AI scenarios. <em>KIS</em>, <em>66</em>(6), 3607–3637.
(<a href="https://doi.org/10.1007/s10115-024-02074-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the potential of a German-language chatbot to engage users in argumentative dialogues on ethically sensitive topics. Utilizing an argumentative knowledge graph, the chatbot is equipped to engage in discussions on the ethical implications of autonomous AI systems in hypothetical future scenarios in the fields of medicine, law, and self-driving cars. In a study with 178 student participants, we investigated the chatbot’s argumentation effect—its ability to offer new perspectives, gain user acceptance, and broaden users’ viewpoints on complex issues. The results indicated a substantial argumentation effect, with 13–21% of participants shifting their opinions to more moderate stances after interacting with the chatbot. This shift demonstrates the system’s effectiveness in fostering informed discourse and increasing users’ understanding of AI ethics. While the chatbot was well-received, with users acknowledging the quality of its arguments, we identified opportunities for improvement in its argument recognition capabilities. Despite this, our results indicate the chatbot’s potential as an educational tool in engaging users with the ethical dimensions of AI technology and promoting informed discourse.},
  archive      = {J_KIS},
  author       = {Hauptmann, Christian and Krenzer, Adrian and Völkel, Justin and Puppe, Frank},
  doi          = {10.1007/s10115-024-02074-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3607-3637},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Argumentation effect of a chatbot for ethical discussions about autonomous AI scenarios},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-sensitive learning using logical analysis of data.
<em>KIS</em>, <em>66</em>(6), 3571–3606. (<a
href="https://doi.org/10.1007/s10115-024-02070-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification is a common task in data mining that assigns a class label to an unseen situation. It has been widely used in decision making for various applications, and many machine learning algorithms have been developed to accomplish this task. Classification becomes critical when the problem under concern is related to serious situations such as fraud detection, cancer diseases, and quality control. Learning in these situations is characterized by predetermined asymmetric costs of incorrect class prediction, or critical consequences associated with erroneous class prediction. In this paper, a novel approach of cost-sensitive learning is proposed. The approach is constructed by employing the theory of logical analysis of data (LAD) to build accurate cost-sensitive classifiers. Two classifiers are proposed. The first classifier is established by solving a proposed pattern selection model, minimum misclassification cost model (MMCM), that aims at minimizing the asymmetric misclassification cost. The second classifier is established by solving another proposed pattern selection model, maximum precision–recall model (MPRM), that maximizes precision and recall willing to reach a 100% accuracy. A comparative study is conducted by using real datasets. The proposed MMCM has enabled LAD to realize up to 32.22% cost reduction from the misclassification cost realized by the traditional implementation of LAD. Moreover, MPRM has provided up to 19.15% increase in the precision and up to 37% increase in the recall. Also, MPRM has enhanced the performance of LAD while compared to common machine learning algorithms by providing better combinations of recall and false positive rate. This enabled LAD to provide the closet to the optimal point on the receiver operating characteristic (ROC) diagram when compared with existing machine learning methods. Incorporating the MMCM and the MPRM models into LAD establishes a novel implementation of LAD that makes LAD a promising cost-sensitive learning classifier compared to other machine learning classifiers.},
  archive      = {J_KIS},
  author       = {Osman, Hany},
  doi          = {10.1007/s10115-024-02070-1},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3571-3606},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cost-sensitive learning using logical analysis of data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Layer imbalance-aware multiplex network embedding.
<em>KIS</em>, <em>66</em>(6), 3547–3569. (<a
href="https://doi.org/10.1007/s10115-024-02072-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplex network embedding is an effective technique to jointly learn the low-dimensional representations of nodes across network layers. However, the number of edges among layers may vary significantly. This data imbalance will lead to performance degradation especially on the sparse layer due to learning bias and the adverse effects of irrelevant or conflicting data in other layers. In this paper, a layer imbalance-aware multiplex network embedding (LIAMNE) method is proposed where the edges in auxiliary layers are filtered out under the guidance of the embedding distribution from the target layer in order to minimize noisy relations that are less relevant to the target layer. The method can also balance the number of edges among layers, which is more conductive to learning the sparse target layer. Real-world datasets with different degrees of layer imbalance are used for experimentation. The results demonstrate that LIAMNE significantly outperforms several state-of-the-art multiplex network embedding methods in link prediction on the target layer. Meantime, the comprehensive representation of the entire multiplex network is not compromised by the sampling method as evaluated by its performance on the node classification task.},
  archive      = {J_KIS},
  author       = {Chen, Ke-Jia and Qiu, Yinchu and Liu, Zheng and Mu, Wenhui},
  doi          = {10.1007/s10115-024-02072-z},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3547-3569},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Layer imbalance-aware multiplex network embedding},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Framework for scoring the scientific reputation of
researchers. <em>KIS</em>, <em>66</em>(6), 3523–3545. (<a
href="https://doi.org/10.1007/s10115-024-02071-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the scientific community, there is no single, objective, and precise metric for ranking the work of researchers based on their scientific merit. Most existing metrics are based on the number of publications associated with an author along with the number of citations received by those publications. However, there is no standard metric officially used to evaluate the researchers’ careers. In this paper, the Framework for Reputation Estimation of Scientific Authors (FRESA) to address this issue is depicted. It is a system able to estimate the reputation of a researcher focusing on the achieved publications. It calculates two indexes making use of the relevance and the novelty concepts in the scientific domain. The system can depict the scientific trajectories of the researchers through the proposed indexes to illustrate their evolution over time. FRESA uses web information sources and applies similarity measures, text mining techniques, and clustering algorithms to also rank and group the researchers. The presented work is experimental, rendering promising results.},
  archive      = {J_KIS},
  author       = {de Diego, Isaac Martín and Prieto, Juan Carlos and Fernández-Isabel, Alberto and Gomez, Javier and Alfaro, César},
  doi          = {10.1007/s10115-024-02071-0},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3523-3545},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Framework for scoring the scientific reputation of researchers},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining deep multi-class time series classifiers.
<em>KIS</em>, <em>66</em>(6), 3497–3521. (<a
href="https://doi.org/10.1007/s10115-024-02073-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability helps users trust deep learning solutions for time series classification. However, existing explainability methods for multi-class time series classifiers focus on one class at a time, ignoring relationships between the classes. Instead, when a classifier is choosing between many classes, an effective explanation must show what sets the chosen class apart from the rest. We now formalize this notion, studying the open problem of class-specific explainability for deep time series classifiers, a challenging and impactful problem setting. We design a novel explainability method, DEMUX, which learns saliency maps for explaining deep multi-class time series classifiers by adaptively ensuring that its explanation spotlights the regions in an input time series that a model uses specifically to its predicted class. DEMUX adopts a gradient-based approach composed of three interdependent modules that combine to generate consistent, class-specific saliency maps that remain faithful to the classifier’s behavior yet are easily understood by end users. We demonstrate that DEMUX outperforms nine state-of-the-art alternatives on seven popular datasets when explaining two types of deep time series classifiers. We analyze runtime performance, show the impacts of hyperparameter selection, and introduce a detailed study of perturbation methods for time series. Further, through a case study, we demonstrate that DEMUX’s explanations indeed highlight what separates the predicted class from the others in the eyes of the classifier.},
  archive      = {J_KIS},
  author       = {Doddaiah, Ramesh and Parvatharaju, Prathyush S. and Rundensteiner, Elke and Hartvigsen, Thomas},
  doi          = {10.1007/s10115-024-02073-y},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3497-3521},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Explaining deep multi-class time series classifiers},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-behavior-based graph contrastive learning
recommendation. <em>KIS</em>, <em>66</em>(6), 3477–3496. (<a
href="https://doi.org/10.1007/s10115-024-02064-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based collaborative filtering recommendations can more effectively explore the interaction information between users and items. However, its performance is still affected by the problems of data sparsity and low-quality representation learning. To address this, we propose a recommendation model named Multi-behavior-based Graph Contrastive Learning (MBGCL for short) Recommendation. Firstly, we leverage a graph convolutional network that can balance recommendation accuracy and novelty to learn multi-behavior data. We apply advanced MLP modules to enhance the nonlinearity of the representations obtained from graph convolutional network and integrate the learned multi-behavior representations. Secondly, we enhance representation capability and alleviate popularity bias through two contrastive learning auxiliary tasks. The multi-behavior contrastive learning task contrastively learns the target behavior and other auxiliary behavior subgraphs. The embedding-noise contrastive learning task aims to introduce noise into different behavior representations and generate augmented views for contrastive learning. Finally, we directly optimize the learning objectives by jointly training the graph collaborative filtering recommendation task with the contrastive learning auxiliary tasks. The empirical results on two real-world datasets validate the effectiveness of our model. Our model outperforms the SOTA baselines in terms of recommendation accuracy and novelty metrics.},
  archive      = {J_KIS},
  author       = {Bin, Chenzhong and Li, Weiliang and Wu, Fangjian and Chang, Liang and Wen, Yimin},
  doi          = {10.1007/s10115-024-02064-z},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3477-3496},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-behavior-based graph contrastive learning recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine reading comprehension model with counterfactual
contrastive learning for emotion-cause pair extraction. <em>KIS</em>,
<em>66</em>(6), 3459–3476. (<a
href="https://doi.org/10.1007/s10115-024-02062-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emotion-cause pair extraction (ECPE) task aims to extract emotional clauses and corresponding cause clauses from documents, and this task is increasingly being recognized for its wide range of applications. In recent years, some studies have proposed using machine reading comprehension (MRC) to solve this problem and have demonstrated their effectiveness. However, these studies have not proposed methods to guide models learning the relationship between queries and documents. This study proposes an MRC model for emotion-cause extraction based on counterfactual contrastive learning. We model the relationship between queries and clauses and use contrastive learning to guide the model to learn this relationship explicitly. A novel approach is proposed to enhance the efficacy of contrastive learning and acquire more comprehensive semantic information from queries. Specifically, counterfactual methods are employed to invert the emotional polarity of emotional clauses within cause queries, which are regarded as negative instances in contrastive learning. Additionally, we incorporate emotional polarity into queries to improve the accuracy of pairing emotional clauses and cause clauses. The experimental results on publicly available Chinese and English datasets demonstrate that our model achieves state-of-the-art performance in ECPE tasks.},
  archive      = {J_KIS},
  author       = {Mai, Hanjie and Zhang, Xuejie and Wang, Jin and Zhou, Xiaobing},
  doi          = {10.1007/s10115-024-02062-1},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3459-3476},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A machine reading comprehension model with counterfactual contrastive learning for emotion-cause pair extraction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMAdam: Adaptive modifier of adam method. <em>KIS</em>,
<em>66</em>(6), 3427–3458. (<a
href="https://doi.org/10.1007/s10115-023-02052-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents AMAdam, an innovative adaptive modifier gradient descent optimization algorithm that aims to overcome the challenges faced by traditional optimization methods in the field of artificial intelligence. The core of AMAdam’s contribution is its capacity to dynamically adjust the learning rate according to subtle gradient variations, resulting in an acceleration of the convergence speed of the optimization process. Concurrently, it ensures robust stabilization, guaranteeing that the algorithm converges reliably and efficiently. In addition, AMAdam efficiently reduces memory usage and hyperparameter complexity, distinguishing it from standard methods. A reliable comprehensive convergence analysis is provided. Extensive testing on multiple datasets, such as MNIST, IMDB movie reviews, CIFAR-10, and CIFAR-100, verifies that AMAdam consistently outperforms well-known optimizers including SGD(M), Adam, Adamax, RMSProp, Adagrad, AdaDelta, AdamW, and Radam. These outcomes demonstrate the effectiveness of AMAdam in optimization tasks while advancing computational efficiency, representing an important breakthrough in gradient descent optimization. Code is available at https://github.com/thchi/AMad .},
  archive      = {J_KIS},
  author       = {Kabiri, Hichame and Ghanou, Youssef and Khalifi, Hamid and Casalino, Gabriella},
  doi          = {10.1007/s10115-023-02052-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3427-3458},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {AMAdam: Adaptive modifier of adam method},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Greedy centroid initialization for federated k-means.
<em>KIS</em>, <em>66</em>(6), 3393–3425. (<a
href="https://doi.org/10.1007/s10115-024-02066-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study learning from unlabeled data distributed across clients in a federated fashion where raw data do not leave the corresponding devices. We develop a K-means clustering algorithm within this federated setting where the local datasets are clustered at the clients, and a server generates the global clusters after aggregating the local ones. Given the importance of initialization on the federated $$\underline{K}$$ -means algorithm (FKM), our objective is to find better initial centroids by utilizing the local data stored on each client. To this end, we start the centroid initialization at the clients, rather than at the server, since the server initially lacks any preliminary insight into the clients’ data. The clients first select their local initial clusters and subsequently share their clustering information (including cluster centroids and sizes) with the server. The server then employs a greedy algorithm to determine the global initial centroids based on the information received from the clients. We refer to this idea as G-FKM. Numerical results obtained from both synthetic and public datasets demonstrate that our proposed algorithm demonstrates accelerated convergence, exhibiting reduced within-cluster sum of squares ( $$\overline{\text{ WCSS }}$$ ) and higher adjusted Rand Index compared to three distinct federated K-means variants. This improvement comes at a relatively low cost of sending limited additional information from the clients to the server, rather than conducting the initialization entirely at the server. Furthermore, we have also observed that the proposed algorithm performs better than the centralized algorithm for cases where the data distribution across the clients is highly skewed.},
  archive      = {J_KIS},
  author       = {Yang, Kun and Mohammadi Amiri, Mohammad and Kulkarni, Sanjeev R.},
  doi          = {10.1007/s10115-024-02066-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3393-3425},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Greedy centroid initialization for federated K-means},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology-based layered rule-based network intrusion
detection system for cybercrimes detection. <em>KIS</em>,
<em>66</em>(6), 3355–3392. (<a
href="https://doi.org/10.1007/s10115-024-02068-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to secure Internet applications on global networks has become an important task due to the ever-increasing cybercrimes. A common technique for identifying intrusions in computer networks is the Network Intrusion Detection System (NIDS). Several Intrusion Detection Systems have been proposed previously, but these systems are still limited in detection and error rates. Additionally, most of the detection techniques used a set of static rules and manual taxonomies for the detection of intrusions. In this study, a layered rule-based NIDS using ontology was developed. The study adapted a layered attribute evaluator approach to choose the best attributes for NIDS. In order to automatically construct the rules for intrusion detection, the chosen attributes were trained with a classification tree. The created rules are then introduced into the Protégé software for the ontology classification of NIDS. In contrast with taxonomies, the generated ontology provides comprehensive definitions of the concepts inside the NIDS domain that are machine interpretable and illustrates the relationships between the concepts. The findings revealed that the developed approach has 97.431% accuracy, 97.48% precision, 97.41% recall, and 97.41% F1-score on the original dataset. Similarly, the developed approach reported 98.21% accuracy, 98.21% precision, 98.21% recall, and 98.21% F1-score on the reduced dataset. These results demonstrated that the developed approach outperformed the other similar approaches on both the original and reduced datasets. The developed approach also showed better training time compared to the other related approaches.},
  archive      = {J_KIS},
  author       = {Ayo, Femi Emmanuel and Awotunde, Joseph Bamidele and Ogundele, Lukman Adebayo and Solanke, Olakunle Olugbenga and Brahma, Biswajit and Panigrahi, Ranjit and Bhoi, Akash Kumar},
  doi          = {10.1007/s10115-024-02068-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3355-3392},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ontology-based layered rule-based network intrusion detection system for cybercrimes detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent mutual feature extraction for cross-domain
recommendation. <em>KIS</em>, <em>66</em>(6), 3337–3354. (<a
href="https://doi.org/10.1007/s10115-024-02065-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to propose a Cross-domain Recommendation (CDR) model targeting heterogeneous domains. Previous studies have mainly focused on homogeneous domains and pose limitations when applied to heterogeneous domains without common users, items, and metadata. To overcome this challenge, we propose a heterogeneous CDR model called latent features cross-domain recommendation (LFCDR). Our model leverages latent features (LF), which construct the correlations between user and item features based on domain categories, where a category represents the domain attributes. By extracting the LF of each domain, we find similar domain latent features and improve the performance of the sparsity domain through transfer learning. We performed experiments on latent features recommendation (LFR), a recommendation system using LF, and LFCDR, a CDR using LF of heterogeneous domains, using three heterogeneous domain datasets, and compared their performances with a factorization machine (FM). Our results illustrated that the performance of the LFR improved by up to 1.65, as measured by mean absolute error (MAE), compared to the FM. Additionally, the performance of the LFCDR improved by up to 1.66, depending on the relevance of the domain’s category.},
  archive      = {J_KIS},
  author       = {Park, Hoon and Jung, Jason J.},
  doi          = {10.1007/s10115-024-02065-y},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3337-3354},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Latent mutual feature extraction for cross-domain recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An incremental clustering algorithm based on semantic
concepts. <em>KIS</em>, <em>66</em>(6), 3303–3335. (<a
href="https://doi.org/10.1007/s10115-024-02063-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of data in text streams may cause feature and concept drifts. The former, while being less discussed in the literature, poses challenges for learning algorithms by changing the feature space of text representation. A common approach for handling concept drift is to maintain summarized groups of documents, known as micro-clusters. Despite the benefits, this scheme restricts document representation and poses challenges in the face of feature drift. In this paper, we propose an incremental text clustering algorithm that deals with both kinds of drifts. The algorithm uses incremental word embedding, which is rarely studied in the context of evolving data streams. We also propose a novel approach to leverage hierarchical summarized concepts instead of micro-clusters. The concepts reflect the semantic structure of the text stream and are continuously updated in the face of concept drift and evolution. The proposed method enables a customized low-dimensional and interpretable document representation, which improves the clustering quality. By employing concept modeling, in contrast with many available approaches, the proposed algorithm detaches the process of handling data evolution from document clustering. This modularization enables arbitrary variation in the granularity of document representation and allows for customized clustering when accessing the historical documents is impractical. The experimental results on several real datasets, and comparison with other incremental and non-incremental methods, show that the proposed algorithm can deal with dynamics in the feature space, and concept drift and evolution, while preserving its accuracy.},
  archive      = {J_KIS},
  author       = {Soleymanian, Mahboubeh and Mashayekhi, Hoda and Rahimi, Marziea},
  doi          = {10.1007/s10115-024-02063-0},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3303-3335},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An incremental clustering algorithm based on semantic concepts},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-representation web service recommendation system based
on attention mechanism. <em>KIS</em>, <em>66</em>(6), 3283–3302. (<a
href="https://doi.org/10.1007/s10115-024-02061-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, mashup developers seek to integrate multiple services with complementary functionalities from a large amount of web services. With so many available web services, it is difficult for developers to choose the right one to develop new mashups. Therefore, it is critical to create and recommend appropriate web services for mashup developers based on their development needs. In the past, various deep models have been proposed to facilitate web service recommendation based on semantic matching of textual descriptions. However, existing deep approaches mainly match global semantic representations while ignoring descriptive structure and tag information. In this paper, we propose a multi-representation web service recommendation model, which simultaneously extracts global, local and tag representations of the description and tag information, respectively. Moreover, we propose a tag-driven attention mechanism to guide the process of information extraction. Experiments over a real-world dataset demonstrate that our proposed service recommendation algorithm can achieve remarkable performance.},
  archive      = {J_KIS},
  author       = {Dang, Depeng and Guo, Bilin and Fang, Tingting and Zhang, Ying},
  doi          = {10.1007/s10115-024-02061-2},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3283-3302},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-representation web service recommendation system based on attention mechanism},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing the number of branches in a decision forest using
association rule metrics. <em>KIS</em>, <em>66</em>(6), 3261–3281. (<a
href="https://doi.org/10.1007/s10115-024-02069-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble methods, such as random forest algorithms, typically outperform single classifiers. However, they often demand substantial storage memory and involve relatively time-consuming predictions. Numerous approaches have been introduced to reduce ensemble size while maintaining classification performance. This paper introduces a novel strategy for forest pruning based on association rule metrics called PRM. The PRM method extracts branches from the initial forest, calculates the score for each branch, and removes poor-performing branches. Subsequently, it utilizes the selected branches to predict unseen data by aggregating their predictions. This approach can be applied to various types of tree ensembles. We evaluated the proposed PRM method using twenty UCI machine learning and Kaggle repositories datasets. Compared to four ensemble pruning techniques, our findings demonstrate that PRM can significantly reduce the forest size while enhancing ensemble performance, surpassing state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Manzali, Youness and Elfar, Mohamed},
  doi          = {10.1007/s10115-024-02069-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3261-3281},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimizing the number of branches in a decision forest using association rule metrics},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position-category-aware attention network for next-item
recommendation. <em>KIS</em>, <em>66</em>(6), 3231–3259. (<a
href="https://doi.org/10.1007/s10115-023-02057-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next-item recommendation can extract critical information from the historical sequence and predict the next actions of users. To better extract users’ interests, some sequential recommender methods propose position-aware attention networks to obtain users’ general intentions. Nonetheless, although these methods have achieved superior performances, they cannot effectively extract core information from historical behavior sequences such as position weights, the dynamic categories of users, and the dynamic preferences of users. The position information in the historical sequence can assist in the modeling of user interest, and the dynamic category of users can help us ensure the major intention of users. Moreover, capturing the dynamic preference of users can help the model learn the evolution tendency of user interest and make better recommendations. Therefore, this paper proposes a Position-category-aware Attention Network (PCAN) to consider the above three factors. First, this model obtains the dynamic category of the user in the data preprocessing stage. Then, a long-term attention module is constructed to get the interaction between users and items in the long-term sequential behavior, to better capture the users’ long-term preference representation. Meanwhile, the model utilizes the self-attention method to extract users’ short-term interest features. Finally, two kinds of preference representation are adaptively fused through an attention-based method. On five kinds of Amazon public datasets, the experimental results indicate that our proposed model PCAN achieves better performances on $$AUC, Precision,$$ and $$Recall$$ , which demonstrates the superior performance of the method.},
  archive      = {J_KIS},
  author       = {Qiu, Liqing and Dou, Mingjian and Jing, Caixia and Liu, Yuying},
  doi          = {10.1007/s10115-023-02057-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3231-3259},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Position-category-aware attention network for next-item recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted bidirectional gated recurrent network for event
detection. <em>KIS</em>, <em>66</em>(6), 3211–3230. (<a
href="https://doi.org/10.1007/s10115-023-02031-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern information technology is able to store enormous amounts of information even at high speeds and volumes. Meanwhile, handling continuous data streams becomes a complicated task and thus, a hybrid weighted recurrent neural network as well as bidirectional gated recurrent unit (hybrid WRBG) method is proposed for ideal feature sub-selection from the hyperspace of big data. Here, three datasets are utilized, namely as MAVEN dataset, Climate Change Twitter dataset, and event detection dataset to examine the proposed hybrid WRBG method. By utilizing the fuzzy elephant herding optimization (FEHO) which is a form of swarm search which delivers higher analytical accuracy within a practical processing time, the feature selection is specifically created for detection of events. Also, by attaining a tradeoff in the range of bias and variance terms, the classifier error is reduced through the FEHO algorithm. A weighted recurrent neural network (weighted RNN) ensemble with a bidirectional gated recurrent unit classifier is employed in order to automatically update current concepts in big data streams. The proposed model achieves an accuracy of 98.97%, a precision of 98.87%, an f-score of 98.72%, and a kappa score value is 0.92.},
  archive      = {J_KIS},
  author       = {Mary Vidya, R. and Ramakrishna, M.},
  doi          = {10.1007/s10115-023-02031-0},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3211-3230},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Weighted bidirectional gated recurrent network for event detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biological big-data sources, problems of storage,
computational issues, and applications: A comprehensive review.
<em>KIS</em>, <em>66</em>(6), 3159–3209. (<a
href="https://doi.org/10.1007/s10115-023-02049-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological big data are a massive amount of data generated from multi-omics experiments, such as genomics, transcriptomics, proteomics, metabolomics, phenomics, glycomics, epigenomics, and other omics. These data are used to study biological processes and to gain insights into how living systems work. It can also be used to develop new treatments for diseases and understand the causes of certain conditions. The storage and analysis of these data present several challenges owing to their sheer size and complexity. Storing these data efficiently requires a large amount of storage space and processing power. Furthermore, there are certain limitations in terms of the kind of insights that can be gained from multi-omics data because of their complexity. Despite these challenges, biological big data offers great potential for advancing our understanding of biology and developing new treatments for diseases. Big-data research is a rapidly growing field, with numerous applications. As the amount of data continues to increase, it is important to understand its storage, utility, limitations, and challenges. In this review article, various sources of big-data research and their storage capacities, limitations, and challenges are discussed. Factors affecting the data quality and accuracy have been reported. It will be helpful for researchers to understand the available big data in biology for their further utilization and integration into novel discovery.},
  archive      = {J_KIS},
  author       = {Chaudhari, Jyoti Kant and Pant, Shubham and Jha, Richa and Pathak, Rajesh Kumar and Singh, Dev Bukhsh},
  doi          = {10.1007/s10115-023-02049-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {3159-3209},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Biological big-data sources, problems of storage, computational issues, and applications: A comprehensive review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: Enumerating all multi-constrained s–t paths on
temporal graph. <em>KIS</em>, <em>66</em>(5), 3157. (<a
href="https://doi.org/10.1007/s10115-023-02026-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Jin, Yue and Chen, Zijun and Liu, Wenyuan},
  doi          = {10.1007/s10115-023-02026-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3157},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: Enumerating all multi-constrained s–t paths on temporal graph},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity enhancement of heterogeneous networks by weighted
incorporation of information. <em>KIS</em>, <em>66</em>(5), 3133–3156.
(<a href="https://doi.org/10.1007/s10115-023-02050-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world datasets, different aspects of information are combined, so the data is usually represented as heterogeneous graphs whose nodes and edges have different types. Learning representations in heterogeneous networks is one of the most important topics that can be utilized to extract important details from the networks with the embedding methods. In this paper, we introduce a new framework for embedding heterogeneous graphs. Our model relies on weighted heterogeneous networks with star structures that take structural and attributive similarity into account as well as semantic knowledge. The target nodes form the center of the star and the different attributes of the target nodes form the points of the star. The edge weights are calculated based on three aspects, including the natural language processing in texts, the relationship between different attributes of the dataset and the co-occurrence of each attribute pair in target nodes. We strengthen the similarities between the target nodes by examining the latent connections between the attribute nodes. We find these indirect connections by considering the approximate shortest path between the attributes. By applying the side effect of the star components to the central component, the heterogeneous network is reduced to a homogeneous graph with enhanced similarities. Thus, we can embed this homogeneous graph to capture the similar target nodes. We evaluate our framework for the clustering task and show that our method is more accurate than previous unsupervised algorithms for real-world datasets.},
  archive      = {J_KIS},
  author       = {Baharifard, Fatemeh and Motaghed, Vahid},
  doi          = {10.1007/s10115-023-02050-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3133-3156},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Similarity enhancement of heterogeneous networks by weighted incorporation of information},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MedTSS: Transforming abstractive summarization of scientific
articles with linguistic analysis and concept reinforcement.
<em>KIS</em>, <em>66</em>(5), 3115–3132. (<a
href="https://doi.org/10.1007/s10115-023-02055-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research addresses the limitations of pretrained models (PTMs) in generating accurate and comprehensive abstractive summaries for scientific articles, with a specific focus on the challenges posed by medical research. The proposed solution named medical text simplification and summarization (MedTSS) introduces a dedicated module designed to enrich source text for PTMs. MedTSS addresses issues related to token limits, reinforces multiple concepts, and mitigates entity hallucination problems without necessitating additional training. Furthermore, the module conducts linguistic analysis to simplify generated summaries, particularly tailored for the complex nature of medical research articles. The results demonstrate a significant enhancement, with MedTSS improving the Rouge-1 score from 16.46 to 35.17 without requiring additional training. By emphasizing knowledge-driven components, this framework offers a distinct perspective, challenging the common narrative of ’more data’ or ’more parameters.’ This alternative approach, especially applicable in health-related domains, signifies a broader contribution to the field of NLP. MedTSS serves as an innovative model that not only addresses the intricacies of medical research summarization but also presents a paradigm shift with implications for diverse domains beyond its initial scope.},
  archive      = {J_KIS},
  author       = {Saeed, Nadia and Naveed, Hammad},
  doi          = {10.1007/s10115-023-02055-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3115-3132},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MedTSS: Transforming abstractive summarization of scientific articles with linguistic analysis and concept reinforcement},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining technology trends in scientific publications: A graph
propagated neural topic modeling approach. <em>KIS</em>, <em>66</em>(5),
3085–3114. (<a
href="https://doi.org/10.1007/s10115-023-02005-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decades have witnessed significant progress in scientific research, where new technologies emerge and traditional technologies constantly evolve. As a critical task in the Science of Science (SciSci), automatically mining technology trends from massive scientific publications have attracted broad research interests in various communities. While existing approaches can achieve remarkable performance, there are still many critical challenges to address, such as data sparsity, cross-document influence, and temporal dependency. To this end, in this paper, we propose a technical terms-based graph propagated neural topic model for mining technology trends in scientific publications. Specifically, we first utilize the documents’ citation relations and technical terms to construct a heterogeneous graph. Then, we design a term propagation network to spread the technical terms on the heterogeneous graph to overcome the sparseness of technical terms. In addition, we develop a dynamic embedded topic modeling method to capture the temporal dependencies for technical terms in cross-document, which can discover the distribution of technical terms over time. Finally, extensive experiments on real-world scientific datasets validate the effectiveness and interpretability of our approach compared with state-of-the-art baselines.},
  archive      = {J_KIS},
  author       = {Du, Chenguang and Yao, Kaichun and Zhu, Hengshu and Wang, Deqing and Zhuang, Fuzhen and Xiong, Hui},
  doi          = {10.1007/s10115-023-02005-2},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3085-3114},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining technology trends in scientific publications: A graph propagated neural topic modeling approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid henry gas solubility optimization and the equilibrium
optimizer for feature selection: Real cases with twitter spam detection.
<em>KIS</em>, <em>66</em>(5), 3055–3084. (<a
href="https://doi.org/10.1007/s10115-023-02054-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid spread and daily usage of social networks have made them vulnerable to spammers. Therefore, detecting and eliminating spam and spammers has become more than necessary to reduce the risks that it poses to users’ security. In order to achieve this goal, it is crucial to determine the exact features that help identify and classify whether a user is spam or not. This paper proposes a wrapper-based method for selecting the most important features. It is based on combining two recent metaheuristic algorithms, the Henry Gas Solubility Optimization Algorithm (HGSO) and the Equilibrium Optimizer Algorithm (EO), with the goal of choosing a small and most influential subset of features that give good performance and help in the spammer profile detection process. For the purpose of showing the ability of the proposed method to achieve the desired goals, several comparisons are conducted on a modified Social Honeypot dataset. The first comparison is made between HGSOEO and the two algorithms (HGSO and EO) that were used to develop the proposed algorithm to prove the power of hybridization. The two next comparisons are made against some classical filter- and wrapper-based feature selection methods. The last comparison is carried out against some well-known metaheuristic algorithms for feature selection. Experiments and analysis of the results show that the proposed model is more accurate than the algorithms and methods that we compared it to.},
  archive      = {J_KIS},
  author       = {Legoui, Khaoula Zineb and Maza, Sofiane and Attia, Abdelouahab and Houssein, Essam H.},
  doi          = {10.1007/s10115-023-02054-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3055-3084},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid henry gas solubility optimization and the equilibrium optimizer for feature selection: Real cases with twitter spam detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JMFEEL-net: A joint multi-scale feature enhancement and
lightweight transformer network for crowd counting. <em>KIS</em>,
<em>66</em>(5), 3033–3053. (<a
href="https://doi.org/10.1007/s10115-023-02056-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting based on convolutional neural networks (CNNs) has made significant progress in recent years. However, the limited receptive field of CNNs makes it challenging to capture global features for comprehensive contextual modeling, resulting in insufficient accuracy in count estimation. In comparison, vision transformer (ViT)-based counting networks have demonstrated remarkable performance by exploiting their powerful global contextual modeling capabilities. However, ViT models are associated with higher computational costs and training difficulty. In this paper, we propose a novel network named JMFEEL-Net, which utilizes joint multi-scale feature enhancement and lightweight transformer to improve crowd counting accuracy. Specifically, we use a high-resolution CNN as the backbone network to generate high-resolution feature maps. In the backend network, we propose a multi-scale feature enhancement module to address the problem of low recognition accuracy caused by multi-scale variations, especially when counting small-scale objects in dense scenes. Furthermore, we introduce an improved lightweight ViT encoder to effectively model complex global contexts. We also adopt a multi-density map supervision strategy to learn crowd distribution features from feature maps of different resolutions, thereby improving the quality and training efficiency of the density maps. To validate the effectiveness of the proposed method, we conduct extensive experiments on four challenging datasets, namely ShanghaiTech Part A/B, UCF-QNRF, and JHU-Crowd++, achieving very competitive counting performance.},
  archive      = {J_KIS},
  author       = {Wang, Mingtao and Zhou, Xin and Chen, Yuanyuan},
  doi          = {10.1007/s10115-023-02056-5},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3033-3053},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {JMFEEL-net: A joint multi-scale feature enhancement and lightweight transformer network for crowd counting},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic density-based clustering method based on k-nearest
neighbor. <em>KIS</em>, <em>66</em>(5), 3005–3031. (<a
href="https://doi.org/10.1007/s10115-023-02038-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many density-based clustering algorithms already proposed in the literature are capable of finding clusters with different shapes, sizes, and densities. Also, the noise points are detected well. However, many of these methods require input parameters that are static and must be defined by user. Since it is difficult for users to determine these parameters in large data sets, the proper determination of them has an effective role in the identification of a suitable clustering. Therefore, a challenge in this domain is how to reduce the number of input parameters, thereby reducing the errors caused by users’ involvement. In order to handle this challenge, a dynamic density-based clustering (DDBC) method is proposed in this paper for clustering purposes, which needs the smallest number of parameters to be set by users since many of them are determined automatically. This method has the ability to distinguish close clusters with different densities in a dynamic manner. Additionally, it can detect outliers and noises before starting the clustering process without scanning these points. Several real and artificial data sets were used to examine the efficiency of the proposed method, and its outcomes were compared to those of other algorithms in this domain . The comparative results confirmed the acceptable performance of DDBC and its higher accuracy in clustering tasks.},
  archive      = {J_KIS},
  author       = {Sorkhi, Mahshid Asghari and Akbari, Ebrahim and Rabbani, Mohsen and Motameni, Homayun},
  doi          = {10.1007/s10115-023-02038-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3005-3031},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A dynamic density-based clustering method based on K-nearest neighbor},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative boundary generation for effective outlier
detection. <em>KIS</em>, <em>66</em>(5), 2987–3004. (<a
href="https://doi.org/10.1007/s10115-023-02012-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is often considered a challenge due to the inherent class imbalance in datasets, with the small number of available outliers that are insufficient to describe their overall distribution. This makes it difficult for classifiers to effectively learn the demarcation (boundary) between normal samples and outliers, which is the key for accurate detection. In this paper, we propose a novel discriminative boundary generation framework, called BoG. The framework extracts the border samples in the dataset and expands them to form the initial boundary outliers. With the adversarial training in GAN, the boundary outliers are further augmented, which, together with the boundary normal data, provides the valuable demarcation information for the classifier. Two method variants are proposed under our BoG framework to achieve a balance between detection efficiency and effectiveness. Extensive experiments show that our proposed framework achieves significant improvements compared to the existing outlier detection methods.},
  archive      = {J_KIS},
  author       = {Zhang, Ji and Liang, Qiliang and Bah, Mohamed Jaward and Li, Hongzhou and Chang, Liang and Kiran, Rage Uday},
  doi          = {10.1007/s10115-023-02012-3},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2987-3004},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Discriminative boundary generation for effective outlier detection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis of tweets using text and graph
multi-views learning. <em>KIS</em>, <em>66</em>(5), 2965–2985. (<a
href="https://doi.org/10.1007/s10115-023-02053-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge of deep learning framework, various studies have attempted to address the challenges of sentiment analysis of tweets (data sparsity, under-specificity, noise, and multilingual content) through text and network-based representation learning approaches. However, limited studies on combining the benefits of textual and structural (graph) representations for sentiment analysis of tweets have been carried out. This study proposes a multi-view learning framework (end-to-end and ensemble-based) that leverages both text-based and graph-based representation learning approaches to enrich the tweet representation for sentiment classification. The efficacy of the proposed framework is evaluated over three datasets using suitable baseline counterparts. From various experimental studies, it is observed that combining both textual and structural views can achieve better performance of sentiment classification tasks than its counterparts.},
  archive      = {J_KIS},
  author       = {Singh, Loitongbam Gyanendro and Singh, Sanasam Ranbir},
  doi          = {10.1007/s10115-023-02053-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2965-2985},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sentiment analysis of tweets using text and graph multi-views learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Session-based recommendation with fusion of hypergraph item
global and context features. <em>KIS</em>, <em>66</em>(5), 2945–2963.
(<a href="https://doi.org/10.1007/s10115-023-02058-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) is to predict the items that users are likely to click afterward by using their recent click history. Learning item features from existing session data to capture users’ current preferences is the main problem to be solved in session-based recommendation domain, and fusing global and local information to learn users’ preferences is an effective way to obtain this information more accurately. In this paper, we propose a session-based recommendation with fusion of hypergraph item global and context features (FHGIGC), which learns users’ current preferences by fusing item global and contextual features. Specifically, the model first constructs a global hypergraph and a local hypergraph and uses the hypergraph neural network to learn item global features and local features by relevant session information and item contextual information, respectively. Then, the learned features are fused by the attention mechanism to obtain the final item features and session features. Finally, personalized recommendations are generated for users based on the fused features. Experiments were conducted on three datasets of session-based recommendation, and the results demonstrate that the FHGIGC model can improve the accuracy of recommendations.},
  archive      = {J_KIS},
  author       = {Han, Xiaohong and Chen, Xiaolong and Zhao, Mengfan and Liu, Ting},
  doi          = {10.1007/s10115-023-02058-3},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2945-2963},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Session-based recommendation with fusion of hypergraph item global and context features},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KNetwork: Advancing cross-lingual sentiment analysis for
enhanced decision-making in linguistically diverse environments.
<em>KIS</em>, <em>66</em>(5), 2925–2943. (<a
href="https://doi.org/10.1007/s10115-023-02051-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is pivotal in facilitating informed decision-making for businesses, governments, and organizations by comprehending public opinion. However, the task becomes challenging when dealing with linguistic diversity and limited resources for specific languages. This paper presents a novel method, KNetwork, for conducting cross-lingual sentiment analysis of Hindi and English text. The KNetwork leverages the feature vectors generated from translated and transliterated text, aiming to enhance the accuracy of sentiment analysis in cross-lingual settings. Specifically, this paper addresses the challenges associated with sentiment analysis in countries like India, which possess a rich linguistic heritage. The KNetwork model is rigorously evaluated on multiple review datasets, showcasing its performance against state-of-the-art models. Moreover, KNetwork achieves superior results in terms of accuracy of 92.5% and an F1-score of 0.922, outperforming existing models. With an AUC-ROC value of 0.934, it excels in cross-lingual sentiment analysis. This study advances the sentiment analysis for languages with limited resources and underscores the KNetwork’s efficacy in enhancing accuracy, with far-reaching implications for informed decision-making.},
  archive      = {J_KIS},
  author       = {Jain, Ankush and Jain, Garima and Tewari, Dhruv},
  doi          = {10.1007/s10115-023-02051-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2925-2943},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {KNetwork: Advancing cross-lingual sentiment analysis for enhanced decision-making in linguistically diverse environments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining top-k constrained cross-level high-utility itemsets
over data streams. <em>KIS</em>, <em>66</em>(5), 2885–2924. (<a
href="https://doi.org/10.1007/s10115-023-02045-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Level High-Utility Itemsets Mining (CLHUIM) aims to discover interesting relationships between hierarchy levels by introducing the taxonomy of items. To tackle this issue of the current CLHUIM algorithms encountering a challenge in dealing with large search spaces, researchers have proposed the concept of mining Top-K cross-level high-utility itemsets(CLHUIs). However, the results obtained by these methods often contain redundant itemsets with significant differences in hierarchy levels, and a large proportion of itemsets with higher abstraction levels, making it neglect some detailed information and unable to provide information of itemsets within the specified hierarchy range. Additionally, they are unable to handle dynamic transactional data. To address the aforementioned problems, this paper proposes Top-K Constrained Cross-Level High-Utility Itemsets Mining (TKCCLHM) algorithm to efficiently mine Top-K itemsets across different hierarchy levels over data streams. Firstly, a new hierarchical level concept is introduced to control the abstraction level of the introduced items, and Top-K itemsets are mined within a specific hierarchy range based on this concept. Secondly, a sliding window-based data structure called Sliding Window-based Utility Projection List (SUPL) is designed, which combined with transaction projection techniques to mine CLHUIs efficiently. Lastly, a Batch and Utility Hash Table (BUHT) structure capable of storing batch and (generalized) item utility information is proposed, along with a new threshold raising strategy. Extensive experiments on six datasets with taxonomy information demonstrated that the proposed algorithm exhibited significant improvements in runtime and scalability performance compared to the state-of-the-art algorithms.},
  archive      = {J_KIS},
  author       = {Han, Meng and Liu, Shujuan and Gao, Zhihui and Mu, Dongliang and Li, Ang},
  doi          = {10.1007/s10115-023-02045-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2885-2924},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining top-K constrained cross-level high-utility itemsets over data streams},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GA-based QOS-aware workflow scheduling of deadline tasks in
grid computing. <em>KIS</em>, <em>66</em>(5), 2859–2884. (<a
href="https://doi.org/10.1007/s10115-023-02048-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid computing is the aggregation of the power of heterogeneous, geographically distributed computing resources to provide high-performance computing. To benefit from the grid computing capabilities, effectual scheduling algorithms are primarily essential. This paper presents a GA-based approach, called Grid Workflow Tasks Scheduling Algorithm (GWTSA), for scheduling workflow tasks on grid services based on users’ QoS (quality of service) constraints in terms of cost and time. For a given set of inter-dependent workflow tasks, it generates an optimal schedule, which minimizes the execution time and cost, such that the optimized time be within the time constraints (deadline) imposed by the user. In GWTSA, the workflow tasks are modeled as a DAG, which is divided, then the optimal sub-schedules of all task divisions are computed and used to obtain the execution schedule of the entire workflow. A GA-based technique is employed in GWTSA to compute the optimal execution sub-schedule for each branch division that consists of a set of sequential tasks. In this technique, the chromosome represents a branch division, where each gene holds the id of the service provider chosen to execute the corresponding task in the branch; and the fitness function is formulated as a multi-objective function of time and cost, this gives users the ability to determine their requirements if speed against cost or vice versa, by changing the weighting coefficients in the fitness function. The paper also exhibits the experimental results of assessing the performance of GWTSA with workflow samples of different sizes.},
  archive      = {J_KIS},
  author       = {Girgis, Moheb R. and Mahmoud, Tarek M. and Azzam, Hagar M.},
  doi          = {10.1007/s10115-023-02048-5},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2859-2884},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GA-based QOS-aware workflow scheduling of deadline tasks in grid computing},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning for concept drifting data streams in
heterogeneous environments. <em>KIS</em>, <em>66</em>(5), 2799–2857. (<a
href="https://doi.org/10.1007/s10115-023-02043-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning in non-stationary environments remains challenging due to dynamic and unknown probability distribution. This issue is even more problematic when there is a lack of supervision data for a specific domain, making the use of labeled data from a related but different domain highly valuable. This paper addresses the streaming data classification and introduces a heterogeneous unsupervised domain adaptation method. To cover the uncertainty caused by the distribution discrepancy and concept drifting data, the proposed method prioritizes target domain data with the highest uncertainty, as they indicate changes in data distribution. It utilizes a fuzzy-based feature-level adaptation and optimizes parameters through accelerated optimization. Additionally, it employs instance selection in the source domain to identify qualified samples, further enhancing classification and adaptation. Three different settings of the proposed method have been configured, and five state-of-the-art methods have been selected as competing methods. Regarding different types of concept drift, various experiments taken from four benchmark datasets demonstrate the superiority of the proposed method in terms of accuracy and computational time. The Wilcoxon statistical test has been conducted to prove a meaningful distinction between the evaluation metrics results of the proposed method and the competing ones.},
  archive      = {J_KIS},
  author       = {Moradi, Mona and Rahmanimanesh, Mohammad and Shahzadi, Ali},
  doi          = {10.1007/s10115-023-02043-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2799-2857},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transfer learning for concept drifting data streams in heterogeneous environments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BSRU: Boosting semi-supervised regressor through ramp-up
unsupervised loss. <em>KIS</em>, <em>66</em>(5), 2769–2797. (<a
href="https://doi.org/10.1007/s10115-023-02044-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised regression aims to improve the performance of the learner with the help of unlabeled data. Popular approaches select some unlabeled data with high-quality pseudo labels to enrich the training set. In this paper, we propose a new approach with a semi-supervised regressor, a learner, and a respective loss function. First, an off-the-shelf semi-supervised regressor is trained to provide pseudo labels for all unlabeled data. These labels are often reliable enough to guide the learning process. Second, we design a neural network with dropout to train data with Gaussian noise added. In this way, the robustness of our learners is enhanced. Third, we design a weighted sum combining the supervised and unsupervised loss. The weight for pseudo-labels ramp-up over time, indicating more attention to the pseudo-labels. Six state-of-the-art algorithms are employed as the base model of our framework. Results on 15 real-world data sets show that our model has a significant improvement over the respective base regressor on most data sets.},
  archive      = {J_KIS},
  author       = {Liu, Liyan and Zuo, Haimin and Min, Fan},
  doi          = {10.1007/s10115-023-02044-9},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2769-2797},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {BSRU: Boosting semi-supervised regressor through ramp-up unsupervised loss},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated approach for binary classification on
imbalanced data. <em>KIS</em>, <em>66</em>(5), 2747–2767. (<a
href="https://doi.org/10.1007/s10115-023-02046-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data are present in various business sectors and must be handled with the proper resampling methods and classification algorithms. To handle imbalanced data, there are numerous resampling and learning method combinations; nonetheless, their effective use necessitates specialised knowledge. In this paper, several approaches, ranging from more accessible to more advanced in the domain of data resampling techniques, will be considered to handle imbalanced data. The application developed delivers recommendations of the most suitable combinations of techniques for a specific dataset by extracting and comparing dataset meta-feature values recorded in a knowledge base. It facilitates effortless classification and automates part of the machine learning pipeline with comparable or better results than state-of-the-art solutions and with a much smaller execution time.},
  archive      = {J_KIS},
  author       = {Vieira, Pedro Marques and Rodrigues, Fátima},
  doi          = {10.1007/s10115-023-02046-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2747-2767},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An automated approach for binary classification on imbalanced data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic literature review on the application of process
mining to industry 4.0. <em>KIS</em>, <em>66</em>(5), 2699–2746. (<a
href="https://doi.org/10.1007/s10115-023-02042-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition to Industry 4.0 means a new era in manufacturing with a new level of production automation, human-to-machine cooperation and product customization. It provides many benefits and opportunities to both enterprises and consumers and allows for principally new level of cooperation. At the same time, the complexity of business processes, large volume and the complex structure of data generated and processed by different Industry 4.0 technologies create serious challenges for Business Process Management. Process mining (PM) can tackle these challenges. PM is a relatively young discipline that is positioned between process-centric and data-centric approaches and focuses on discovering, conformance checking and enhancement of end-to-end business processes. Moreover, new types of PM deal with performance analysis, comparative analysis of several processes, making predictions and triggering improvement actions. This systematic literature review studies the applicability of PM in Industry 4.0 and the benefits that PM can provide to each of the four aspects of Industry 4.0: smart factories, smart products, new business models and new customer services. Approaches of PM proposed in the selected studies are analysed and classified according to two dimensions of the study: PM and Industry 4.0. The research gaps identified while performing the systematic literature review show possible directions for further research in the area.},
  archive      = {J_KIS},
  author       = {Akhramovich, Katsiaryna and Serral, Estefanía and Cetina, Carlos},
  doi          = {10.1007/s10115-023-02042-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2699-2746},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic literature review on the application of process mining to industry 4.0},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple knowledge graph completion model based on PU learning
and prompt learning. <em>KIS</em>, <em>66</em>(4), 2683–2697. (<a
href="https://doi.org/10.1007/s10115-023-02040-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are important resources for many artificial intelligence tasks but usually suffer from incompleteness, which has prompted scholars to put forward the task of knowledge graph completion (KGC). Embedding-based methods, which use the structural information of the KG for inference completion, are mainstream for this task. But these methods cannot complete the inference for the entities that do not appear in the KG and are also constrained by the structural information. To address these issues, scholars have proposed text-based methods. This type of method improves the reasoning ability of the model by utilizing pre-trained language (PLMs) models to learn textual information from the knowledge graph data. However, the performance of text-based methods lags behind that of embedding-based methods. We identify that the key reason lies in the expensive negative sampling. Positive unlabeled (PU) learning is introduced to help collect negative samples with high confidence from a small number of samples, and prompt learning is introduced to produce good training results. The proposed PLM-based KGC model outperforms earlier text-based methods and rivals earlier embedding-based approaches on several benchmark datasets. By exploiting the structural information of KGs, the proposed model also has a satisfactory performance in inference speed.},
  archive      = {J_KIS},
  author       = {Duan, Li and Wang, Jing and Luo, Bing and Sun, Qiao},
  doi          = {10.1007/s10115-023-02040-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2683-2697},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Simple knowledge graph completion model based on PU learning and prompt learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology-based text convolution neural network (TextCNN) for
prediction of construction accidents. <em>KIS</em>, <em>66</em>(4),
2651–2681. (<a
href="https://doi.org/10.1007/s10115-023-02036-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction industry suffers from workplace accidents, including injuries and fatalities, which represent a significant economic and social burden for employers, workers, and society as a whole. The existing research on construction accidents heavily relies on expert evaluations, which often suffer from issues such as low efficiency, insufficient intelligence, and subjectivity. However, expert opinions provided in construction accident reports offer a valuable source of knowledge that can be extracted and utilized to enhance safety management. Today this valuable resource can be mined as the advent of artificial intelligence has opened up significant opportunities to advance construction site safety. Ontology represents an attractive representation scheme. Though ontology has been used in construction safety to solve the problem of information heterogeneity using formal conceptual specifications, the establishment and development of ontologies that utilize construction accident reports are currently in an early stage of development and require further improvements. Moreover, research on the exploration of incorporating deep learning methodologies into construction safety ontologies for predicting construction safety incidents is relatively limited. This paper describes a novel approach to improving the performance of accident prediction models by incorporating ontology into a deep learning model. First, a domain word discovery algorithm, based on mutual information and adjacency entropy, is used to analyze the causes of accidents mentioned in construction reports. This analysis is then combined with technical specifications and the literature in the field of construction safety to build an ontology encompassing unsafe factors related to construction accidents. By employing a Translating on Hyperplane (TransH) model, the reports are transformed into conceptual vectors using the constructed ontology. Building on this foundation, we propose a Text Convolutional Neural Network (TextCNN) model that incorporates the ontology specifically designed for construction accidents. We compared the performance of the TextCNN model against five traditional machine learning models, namely Naive Bayes, support vector machine, logistic regression, random forest, and multilayer perceptron, using three different data sets: One-Hot encoding, word vector, and conceptual vectors. The results indicate that the TextCNN model integrated with the ontology outperformed the other models in terms of performance achieving an impressive accuracy rate of 88% and AUC value of 0.92.},
  archive      = {J_KIS},
  author       = {Shi, Donghui and Li, Zhigang and Zurada, Jozef and Manikas, Andrew and Guan, Jian and Weichbroth, Pawel},
  doi          = {10.1007/s10115-023-02036-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2651-2681},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ontology-based text convolution neural network (TextCNN) for prediction of construction accidents},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A label propagation community discovery algorithm combining
seed node influence and neighborhood similarity. <em>KIS</em>,
<em>66</em>(4), 2625–2649. (<a
href="https://doi.org/10.1007/s10115-023-02035-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of poor stability and low accuracy of community division caused by the randomness in the traditional label propagation algorithm (LPA), a community discovery algorithm that combines seed node influence and neighborhood similarity is proposed. Firstly, the K-shell values of neighbor nodes are combined with clustering coefficients to define node influence, the initial seed set is filtered by a threshold, and the less influential one in adjacent node pairs is removed to obtain the final seed set. Secondly, the connection strengths between non-seed nodes and seed nodes are defined based on their own weights, distance weights, and common neighbor weights. The labels of non-seed nodes are updated to the labels of seed nodes with which they have the maximum connection strength. Further, for the case that the connection strengths between a non-seed node and multiple seed nodes are the same, a new neighborhood similarity combining the information between the two types of nodes and their neighbors is proposed, thus avoiding the instability caused by randomly selecting the labels of seed nodes. Experiments are conducted on six classic real networks and eight artificial datasets with different complexities. The comparison and analysis with dozens of related algorithms are also done, which shows the proposed algorithm effectively improves the execution efficiency, and the community division results are stable and more accurate, with a maximum improvement in the modularity of about 87.64% and 47.04% over the LPA on real and artificial datasets, respectively.},
  archive      = {J_KIS},
  author       = {Liu, Miaomiao and Yang, Jinyun and Guo, Jingfeng and Chen, Jing},
  doi          = {10.1007/s10115-023-02035-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2625-2649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A label propagation community discovery algorithm combining seed node influence and neighborhood similarity},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient automatic modulation recognition using
time–frequency information based on hybrid deep learning and bagging
approach. <em>KIS</em>, <em>66</em>(4), 2607–2624. (<a
href="https://doi.org/10.1007/s10115-023-02041-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the type of modulation is an important task in military communications, satellite communications systems, and submarine communications. In this study, a new digital modulation classification model is presented for detecting various types of modulated signals. The continuous wavelet transform is used in the first step to create a visual representation of the spectral density of the frequencies of the modulation signals in a scalogram image. The subsequent stage involves the utilization of a deep convolutional neural network for feature extraction from the scalogram images. In the next step, the best features are chosen using the MRMR algorithm. MRMR algorithm increases the classification speed and the ability of interpret the classification model by reducing the dimensions of the features. In the fourth step, the modulations are classified using the group learning technique. In the simulations, modulated signals with different amounts of noise with SNR from 0 to 25 dB are considered. Then, accuracy, precision, recall, and F1-score are used to evaluate the performance of the proposed method. The results of the simulations prove that the proposed model with achieving above 99.9% accuracy performs well in the presence of different amounts of noise and provides better performance than other previous studies.},
  archive      = {J_KIS},
  author       = {Hazim Obaid, Zahraa and Mirzaei, Behzad and Darroudi, Ali},
  doi          = {10.1007/s10115-023-02041-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2607-2624},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient automatic modulation recognition using time–frequency information based on hybrid deep learning and bagging approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On-grid and off-grid photovoltaic systems forecasting using
a hybrid meta-learning method. <em>KIS</em>, <em>66</em>(4), 2575–2606.
(<a href="https://doi.org/10.1007/s10115-023-02037-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate two types of photovoltaic (PV) systems (on-grid and off-grid) of different sizes and propose a reliable PV forecasting method. The novelty of our research consists in a weather data-driven feature engineering considering the operation of the PV systems in similar conditions and merging the results of deterministic and stochastic models, namely Machine Learning algorithms (Random Forest—RF, eXtreme Gradient Boost—XGB) and Deep Learning algorithms (Deep Neural Networks—DNN, Gated Recurrent Unit—GRU) into a Hybrid Meta-learning Forecasting method. It combines the estimations of the above-mentioned algorithms with relevant features to predict the PV output using a Long Short-Term Memory model. To design the PV forecast for off-grid systems, that are equally important for prosumers, and approximate the potential power of these systems, the level of load and charging state of the batteries are considered. In this context, feature engineering using the weather and PV output data, including PV characteristics, is relevant to obtaining a performant and robust PV forecast for various use cases taking into account the size and connectivity of the PV systems. On average, the Mean Absolute Error and Mean Absolute Percentage Error have halved compared to values obtained with deterministic methods and are 25% lower than the stochastic models.},
  archive      = {J_KIS},
  author       = {Oprea, Simona-Vasilica and Bâra, Adela},
  doi          = {10.1007/s10115-023-02037-8},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2575-2606},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On-grid and off-grid photovoltaic systems forecasting using a hybrid meta-learning method},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logical assessment formula and its principles for
evaluations with inaccurate ground-truth labels. <em>KIS</em>,
<em>66</em>(4), 2561–2573. (<a
href="https://doi.org/10.1007/s10115-023-02047-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluations with accurate ground-truth labels (AGTLs) have been widely employed to assess predictive models for artificial intelligence applications. However, in some specific fields, such as medical histopathology whole slide image analysis, it is quite usual the situation that AGTLs are difficult to be precisely defined or even do not exist. To alleviate this situation, we propose logical assessment formula (LAF) and reveal its principles for evaluations with inaccurate ground-truth labels (IAGTLs) via logical reasoning under uncertainty. From the revealed principles of LAF, we summarize the practicability of LAF: (1) LAF can be applied for evaluations with IAGTLs on a more difficult task, able to act like usual strategies for evaluations with AGTLs reasonably; (2) LAF can be applied for evaluations with IAGTLs from the logical perspective on an easier task, unable to act like usual strategies for evaluations with AGTLs confidently.},
  archive      = {J_KIS},
  author       = {Yang, Yongquan},
  doi          = {10.1007/s10115-023-02047-6},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2561-2573},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Logical assessment formula and its principles for evaluations with inaccurate ground-truth labels},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A configurable mining approach for enhancing the business
processes’ performance. <em>KIS</em>, <em>66</em>(4), 2537–2560. (<a
href="https://doi.org/10.1007/s10115-023-02011-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business is a war to get the attention you deserve from your enemies, and many competitors strive to gain a prominent position. Organizations are constantly seeking innovative ways to work to stay in a competitive business environment. Business process reengineering (BPR) is one of the most management approaches that are adopted by many organizations in order to achieve a dramatic increase in performance and cost reduction. Since the risks enfolded and failure rates related to BPR projects are very high, it is necessary to find ways to support success of BPR in a systematic approach. The major target of this article is to find the implementation of the proposed model to reengineer business processes (BPs) successfully via integrating critical success factors (CSFs) of BPR and BPs&#39; performance. It is created to detect the inefficiencies and bottlenecks in the business process, decrease costs, time and increase quality of business processes, enhance financial environment and make an effective and efficient performance of the business process. It also applies a mining technique of association rule to examine the link between CSFs and several BPs and measures business processes&#39; performance (intended BPR success) by process time, cycle time, quality and cost pre and post reengineering BPs. Thus, it uses a way to select the appropriate model for each business process. The proposed model will be implemented using a real world the Egyptian tax authority case study in order to prove its usefulness and efficiency. Then, inferred CSFs were applied according to each process, which proved the validity and success of the proposed model.},
  archive      = {J_KIS},
  author       = {Bayomy, Noha Ahmed and Khedr, Ayman E. and Abd-Elmegid, Laila A.},
  doi          = {10.1007/s10115-023-02011-4},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2537-2560},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A configurable mining approach for enhancing the business processes&#39; performance},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized neural attention mechanism for aspect-based
sentiment analysis framework with optimal polarity-based weighted
features. <em>KIS</em>, <em>66</em>(4), 2501–2535. (<a
href="https://doi.org/10.1007/s10115-023-01998-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sentimental analysis has been broadly investigated to extract information to identify whether it is positive, negative or neutral. Sentimental analysis can be broadly performed in social media content, survey response and review. Still, it faces issues while detecting and analyzing social media content. Moreover, a social media network contains indirect sentiments and natural language ambiguities make it complicated to classify the words. Thus, the aspect-based sentiment analysis (ABSA) is emerged to develop explicating extraction methods by utilizing the syntactic parsers to make use of the relation among sentiments and aspects terms. Along with this extraction method, the word embedding is performed through Word2Vec methods to attain a low-dimensional vector depiction of text, which could not capture valuable information. Thus, it aims to design a novel ABSA model using the optimized neural network along with optimal text feature extraction. Initially, various data is collected through the benchmark dataset are given to the image pre-processing. Then, it might undergo different techniques like stemming, stop word removal as well as punctuation removal. Then, the preprocessed data are further given into the feature extraction phase to attain adequate extracted aspects. Then, it further undergoes for deep feature extraction stage, where the text conventional neural network and Glove embedding are utilized to obtain the deep features. Further, the feature concatenation is done to attain the optimization for polarity-based weighted features utilized by the enhanced hybrid optimization algorithm called hybrid Chameleon rat swarm optimization (HCRSO) for improving the performance in sentiment analysis. The optimal features are selected by the HCRSO that provides the polarity-based-weight features; thus, it separates the polarity, and the weighted features are occurred by multiplying the weight with polarities. Especially, the optimized features of polarity-based weighted features and also the parameters of epochs and hidden neuron count of neural attention mechanism-based long short-term network (NAM-LSTM) are optimized using the HCRSO algorithm. The weighted feature is applied by incorporating the NAM-LSTM and proposed HCRSO algorithm for improving the model efficiency. The empirical outcome of the recommended method shows 94% and 93% regarding accuracy and specificity. Thus, the experimental outcomes of the proposed ABSA model reveal the model’s efficiency while validating with other conventional approaches.},
  archive      = {J_KIS},
  author       = {Ramasamy, Mekala and Elangovan, Mohanraj},
  doi          = {10.1007/s10115-023-01998-0},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2501-2535},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimized neural attention mechanism for aspect-based sentiment analysis framework with optimal polarity-based weighted features},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A smart intelligent approach based on hybrid group search
and pelican optimization algorithm for data stream clustering.
<em>KIS</em>, <em>66</em>(4), 2467–2500. (<a
href="https://doi.org/10.1007/s10115-023-02002-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data applications generate a huge range of evolving, real-time, and high-dimensional streaming data. In many applications, data stream clustering regarding efficiency and effectiveness becomes challenging. A major issue in data mining is clustering of data streams. The several clustering techniques were implemented for stream data, but they are mostly quite restricted approaches to cluster dynamics. Generally, the data stream is an arrival of data sequence and also several factors are added in the clustering, which is rather than the classical clustering. For every data point, the stream is mostly unbounded and also the data has been estimated atleast once. It leads to higher processing time and an additional requirement on memory. In addition, the clusters in each data and their statistical property vary over time, and streams can be noisy. To address these challenges, this research work aims to implement a novel data stream clustering which is developed with a hybrid meta-heuristic model. Initially, a data stream is collected, and the micro-clusters are formed by the K-Means Clustering (KMC) technique. Then, the formation of micro-clusters, merge and sorting of the data clusters, where the cluster optimization is performed by the Hybrid Group Search Pelican Optimization (HGSPO). The main objective of the clustering is performed to maximize the accuracy through the radius, distance and similarity measures and then, the thresholds of these metrics are optimized. In the training phase, a stream of clustering threshold is fixed for each cluster. When new data comes into this stream clustering model, the output of training data is measured with new data output that is decided to forward the data into the appropriate clusters based on the assigned threshold with minimum similarity. Through the performance analysis and the attained results, the clustering quality of the recommended system is ensured regarding standard performance metrics by estimating with various clustering and heuristic algorithms.},
  archive      = {J_KIS},
  author       = {Agarwal, Swathi and Reddy, C. R. K.},
  doi          = {10.1007/s10115-023-02002-5},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2467-2500},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A smart intelligent approach based on hybrid group search and pelican optimization algorithm for data stream clustering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving stock trend prediction with pretrain
multi-granularity denoising contrastive learning. <em>KIS</em>,
<em>66</em>(4), 2439–2466. (<a
href="https://doi.org/10.1007/s10115-023-02006-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trend prediction (STP) aims to predict price fluctuation, which is critical in financial trading. The existing STP approaches only use market data with the same granularity (e.g., as daily market data). However, in the actual financial investment, there are a large number of more detailed investment signals contained in finer-grained data (e.g., high-frequency data). This motivates us to research how to leverage multi-granularity market data to capture more useful information and improve the accuracy in the task of STP. However, the effective utilization of multi-granularity data presents a major challenge. Firstly, the iteration of multi-granularity data with time will lead to more complex noise, which is difficult to extract signals. Secondly, the difference in granularity may lead to opposite target trends in the same time interval. Thirdly, the target trends of stocks with similar features can be quite different, and different sizes of granularity will aggravate this gap. In order to address these challenges, we present a self-supervised framework of multi-granularity denoising contrastive learning (MDC). Specifically, we construct a dynamic dictionary of memory, which can obtain clear and unified representations by filtering noise and aligning multi-granularity data. Moreover, we design two contrast learning modules during the fine-tuning stage to solve the differences in trends by constructing additional self-supervised signals. Besides, in the pre-training stage, we design the granularity domain adaptation module (GDA) to address the issues of temporal inconsistency and data imbalance associated with different granularity in financial data, alongside the memory self-distillation module (MSD) to tackle the challenge posed by a low signal-to-noise ratio. The GDA alleviates these complications by replacing a portion of the coarse-grained data with the preceding time step’s fine-grained data, while the MSD seeks to filter out intrinsic noise by aligning the fine-grained representations with the coarse-grained representations’ distribution using a self-distillation mechanism. Extensive experiments on the CSI 300 and CSI 100 datasets show that our framework stands out from the existing top-level systems and has excellent profitability in real investing scenarios.},
  archive      = {J_KIS},
  author       = {Wang, Mingjie and Wang, Siyuan and Guo, Jianxiong and Jia, Weijia},
  doi          = {10.1007/s10115-023-02006-1},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2439-2466},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving stock trend prediction with pretrain multi-granularity denoising contrastive learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal online time-series segmentation. <em>KIS</em>,
<em>66</em>(4), 2417–2438. (<a
href="https://doi.org/10.1007/s10115-023-02029-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When time series are processed, the difficulty increases with the size of the series. This fact is aggravated when time series are processed online, since their size increases indefinitely. Therefore, reducing their number of points, without significant loss of information, is an important field of research. This article proposes an optimal online segmentation method, called OSFS-OnL, which guarantees that the number of segments is minimal, that a preset error limit is not exceeded using the $$L \infty $$ -norm, and that for that number of segments the value of the error corresponding to the $$L^2$$ -norm is minimized. This new proposal has been compared with the optimal OSFS offline segmentation method and has shown better computational performance, regardless of its flexibility to apply it to online or offline segmentation.},
  archive      = {J_KIS},
  author       = {Carmona-Poyato, Ángel and Fernández-García, Nicolás-Luis and Madrid-Cuevas, Francisco-José and Muñoz-Salinas, Rafael and Romero-Ramírez, Francisco-José},
  doi          = {10.1007/s10115-023-02029-8},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2417-2438},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimal online time-series segmentation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework for backpropagation-free soft and hard
gated graph neural networks. <em>KIS</em>, <em>66</em>(4), 2393–2416.
(<a href="https://doi.org/10.1007/s10115-023-02024-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for the definition of neural models for graphs that do not rely on backpropagation for training, thus making learning more biologically plausible and amenable to parallel implementation. Our proposed framework is inspired by Gated Linear Networks and allows the adoption of multiple graph convolutions. Specifically, each neuron is defined as a set of graph convolution filters (weight vectors) and a gating mechanism that, given a node and its topological context, generates the weight vector to use for processing the node’s attributes. Two different graph processing schemes are studied, i.e., a message-passing aggregation scheme where the gating mechanism is embedded directly into the graph convolution, and a multi-resolution one where neighboring nodes at different topological distances are jointly processed by a single graph convolution layer. We also compare the effectiveness of different alternatives for defining the context function of a node, i.e., based on hyperplanes or on prototypes, and using a soft or hard-gating mechanism. We propose a unified theoretical framework allowing us to theoretically characterize the proposed models’ expressiveness. We experimentally evaluate our backpropagation-free graph convolutional neural models on commonly adopted node classification datasets and show competitive performances compared to the backpropagation-based counterparts.},
  archive      = {J_KIS},
  author       = {Pasa, Luca and Navarin, Nicolò and Erb, Wolfgang and Sperduti, Alessandro},
  doi          = {10.1007/s10115-023-02024-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2393-2416},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A unified framework for backpropagation-free soft and hard gated graph neural networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online active learning method for multi-class imbalanced
data stream. <em>KIS</em>, <em>66</em>(4), 2355–2391. (<a
href="https://doi.org/10.1007/s10115-023-02027-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of data mining, data stream classification is an important research direction. However, the presence of issues such as multi-class imbalance, concept drift, and variable class imbalance ratio in data streams can greatly impact the performance of classification models, and the high cost of sample labeling has always been a focus of research. To address these problems, an online active learning method for multi-class imbalanced data stream (OALM-MI) is proposed. Firstly, a comprehensive sample weighting method based on cross-entropy and margin values is proposed to weight each incoming sample in the data stream according to its classification difficulty and importance, which aims to enhance the learning ability of the classifier for important samples. Besides, a comprehensive weighting and updating strategy for ensemble classifiers is introduced, which combines mean square error, improved square error, recall, and the weights of the classifiers in the previous sliding window of samples to weight and update the classifiers. Additionally, adaptive window is utilized to detect and handle concept drift, enabling better adaptation to the changes in the data stream during the learning process. Finally, a margin matrix label request strategy based on class imbalance ratio is proposed to assign labels to samples according to their imbalance ratio and classification difficulty, which can provide more learning opportunities for minority class samples and important samples. Comprehensive experiments were conducted on 12 synthetic data streams and six real data streams with seven state-of-the-art algorithms, and the results showed that the OALM-MI algorithm achieved the highest performance in terms of recall, precision, F1-score, Kappa, and G-mean.},
  archive      = {J_KIS},
  author       = {Li, Ang and Han, Meng and Mu, Dongliang and Gao, Zhihui and Liu, Shujuan},
  doi          = {10.1007/s10115-023-02027-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2355-2391},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online active learning method for multi-class imbalanced data stream},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating semantic similarity with dirichlet multinomial
mixture model for enhanced web service clustering. <em>KIS</em>,
<em>66</em>(4), 2327–2353. (<a
href="https://doi.org/10.1007/s10115-023-02034-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With accelerated advancement of web 2.0, developers generally describe the functionality of services in short natural text. Keyword-based searching techniques are not an efficient way of discovering services from repositories. It suffers from vocabulary problems. Latent Dirichlet allocation (LDA) with word embedding techniques is widely adopted for efficiently extracting latent features from the service descriptions. However, LDA is not efficient on short text due to limited content and inadequate occurring words. The word vectors generated by word embedding techniques are of finer quality than topic modeling techniques. Gibbs sampling algorithm for Dirichlet multinomial mixture (GSDMM) model gives better results on web service description documents because it provides one topic corresponding to one document. In this paper, we evaluate the performance of GSDMM model with word embeddings and propose WV+GSDMMK model. The proposed model improves service-to-topic mapping by determining semantic similarity among features. K-means clustering is applied on service to topic representation. Results are evaluated on five real-time datasets based on intrinsic and extrinsic evaluation measures. Experimental results demonstrate that the proposed method outperforms other baseline techniques, and the accuracy score is also increased by 5%, 18%, 3%, 4%, and 6% on datasets DS1, DS2, DS3, DS4, and DS5, respectively.},
  archive      = {J_KIS},
  author       = {Agarwal, Neha and Sikka, Geeta and Awasthi, Lalit Kumar},
  doi          = {10.1007/s10115-023-02034-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2327-2353},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Integrating semantic similarity with dirichlet multinomial mixture model for enhanced web service clustering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Node and edge dual-masked self-supervised graph
representation. <em>KIS</em>, <em>66</em>(4), 2307–2326. (<a
href="https://doi.org/10.1007/s10115-023-01950-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised graph representation learning has been widely used in many intelligent applications since labeled information can hardly be found in these data environments. Currently, masking and reconstruction-based (MR-based) methods lead the state-of-the-art records in the self-supervised graph representation field. However, existing MR-based methods did not fully consider both the deep-level node and structure information which might decrease the final performance of the graph representation. To this end, this paper proposes a node and edge dual-masked self-supervised graph representation model to consider both node and structure information. First, a dual masking model is proposed to perform node masking and edge masking on the original graph at the same time to generate two masking graphs. Second, a graph encoder is designed to encode the two generated masking graphs. Then, two reconstruction decoders are designed to reconstruct the nodes and edges according to the masking graphs. At last, the reconstructed nodes and edges are compared with the original nodes and edges to calculate the loss values without using the labeled information. The proposed method is validated on a total of 14 datasets for graph node classification tasks and graph classification tasks. The experimental results show that the method is effective in self-supervised graph representation. The code is available at: https://github.com/TangPeng0627/Node-and-Edge-Dual-Mask .},
  archive      = {J_KIS},
  author       = {Tang, Peng and Xie, Cheng and Duan, Haoran},
  doi          = {10.1007/s10115-023-01950-2},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2307-2326},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Node and edge dual-masked self-supervised graph representation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart COVIDNet: Designing an IoT-based COVID-19 disease
prediction framework using attentive and adaptive-derived ensemble deep
learning. <em>KIS</em>, <em>66</em>(4), 2269–2305. (<a
href="https://doi.org/10.1007/s10115-023-02007-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the end of 2019, the world has faced severe issues over Corona Virus Disease of 2019 (COVID-19). So there is a need for some essential precautionary measures until the development of vaccines to battle the COVID-19 pandemic. In addition to that, quarantine and social distancing have become the more significant practice in the world. COVID-19 associated with the virus not only degraded the economy of the world due to the lockdown but also saturated the healthcare system of the people due to its exponential spread. In this case, the Internet of Things (IoT) system offers frequent monitoring facilities to doctors. But, the fight against COVID-19 gets continued until people get vaccinated. Therefore, an IoT-based COVID prediction is designed using deep learning techniques. Firstly, IoT data is collected from online resources. Then, the data is fed to the autoencoder (AE) for attaining the deep features. Further, the deep features are forwarded to the attentive and adaptive ensemble model (AAEM), which includes deep temporal convolution network (DTCN), one-dimensional convolutional neural network (1DCNN), and long short-term memory (LSTM) model for COVID prediction or monitoring. By utilizing the hybrid algorithm fitness position of Eurasian oystercatcher and sewing training (FPEOST), the parameter in the ensemble model is tuned for further improvement in the process. Finally, the COVID-19 disease prediction outcomes are attained on the basis of the high-ranking process. Thus, the developed model achieved an effective prediction rate than conventional approaches over multiple experimental analyses.},
  archive      = {J_KIS},
  author       = {Karthikeyan, D. and Baskaran, P. and Somasundaram, S. K. and Sathya, K. and Srithar, S.},
  doi          = {10.1007/s10115-023-02007-0},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2269-2305},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Smart COVIDNet: Designing an IoT-based COVID-19 disease prediction framework using attentive and adaptive-derived ensemble deep learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Techniques, datasets, evaluation metrics and future
directions of a question answering system. <em>KIS</em>, <em>66</em>(4),
2235–2268. (<a
href="https://doi.org/10.1007/s10115-023-02019-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering has been around for more than half a century. The problem was addressed with different solutions in the eras of different technologies. Some proved more helpful and accurate than the other. Different studies are available online which list and summarize the work done in this domain. This SLR adds up to that list with answers to some questions which will assist the researchers in this field to comprehend the existing knowledge, quickly analyze the available facts and determine some research gaps and future directions. In this article, we investigate different solution domains applied to question answering systems, their results, and methodologies. We also list and discuss different datasets provided to the community for experiments along with their availability status. In the light of this study, we analyze different solution domains and the areas where they produce promising results. Moreover, we focused on different evaluation metrices used in the papers that were included in this study and shed light on some metrices which should be included in the results if the community wants to achieve greater results. Lastly, we also looked into an interesting possibility of a question answering system where answer could be generated using multiple sources. And for that we suggested a domain based on the Quran, Tafseer and Ahadith data sources as the Quran and Ahadith contribute collectively in the Islamic legislation. We hope this article will help the new researchers in the field of question answering to start their research.},
  archive      = {J_KIS},
  author       = {Qamar, Faiza and Latif, Seemab and Shah, Asad},
  doi          = {10.1007/s10115-023-02019-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {2235-2268},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Techniques, datasets, evaluation metrics and future directions of a question answering system},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidence-based adaptive oversampling algorithm for
imbalanced classification. <em>KIS</em>, <em>66</em>(3), 2209–2233. (<a
href="https://doi.org/10.1007/s10115-023-01985-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification task is complicated by several facts including skewed class proportion and unclear decision regions due to noise, class overlap, small disjunct, caused by large within-class variation. These issues make data classification difficult, reducing overall performance, and challenging to draw meaningful insights. In this research, the evidence-based adaptive oversampling algorithm (EVA-oversampling) based on Dempster–Shafer theory of evidence is developed for imbalance classification. This technique involves assigning probability regarding class belonging for each instance to represent uncertainty that each data point may hold. Synthetic data points are generated to make up for the under-representation of minority instances on the region with high confidence, thereby strengthening the minority class region. The experiments revealed that the proposed method worked effectively even in situations where imbalanced counts and data complexity would normally pose significant obstacles. This approach performs better than SMOTE, Borderline-SMOTE, ADASYN, MWMOTE, KMeansSMOTE, LoRAS, and SyMProD algorithms in terms of $$F_1$$ -measure and G-mean for highly imbalanced data while maintaining the overall performance.},
  archive      = {J_KIS},
  author       = {Lin, Chen-ju and Leony, Florence},
  doi          = {10.1007/s10115-023-01985-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2209-2233},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evidence-based adaptive oversampling algorithm for imbalanced classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge enhancement and scene understanding for
knowledge-based visual question answering. <em>KIS</em>, <em>66</em>(3),
2193–2208. (<a
href="https://doi.org/10.1007/s10115-023-02028-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based visual question answering calls for not only paying attention to the visual content of images but also the support of relevant outside knowledge for improved question and answer thinking. The semantics of the questions should not be overlooked since knowledge retrieval relies on more than just visual information. This paper first proposed a question-based semantic retrieval strategy to compensate for the absence of image retrieval knowledge in order to better combine visual and knowledge information. Secondly, image caption is added to help the model better achieve scene understanding. Finally, modal knowledge is represented and accumulated through the triplets. Experimental results on the OK-VQA dataset show that the proposed method achieves an improvement of 4.24% and 1.90% over the two baseline methods, respectively, which proves the effectiveness of this method.},
  archive      = {J_KIS},
  author       = {Su, Zhenqiang and Gou, Gang},
  doi          = {10.1007/s10115-023-02028-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2193-2208},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge enhancement and scene understanding for knowledge-based visual question answering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive SPARQL query formulation using provenance.
<em>KIS</em>, <em>66</em>(3), 2165–2191. (<a
href="https://doi.org/10.1007/s10115-023-01939-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present in this paper a novel solution for assisting users in formulating SPARQL queries. The high-level idea is that users write “semi-formal SPARQL queries,” namely queries whose structure resembles SPARQL but are not necessarily grounded to the schema of the underlying knowledge graph and require only basic familiarity with SPARQL. This means that the user-intended query over the knowledge graph may differ from the specified semi-formal query in its structure and query elements. We design a novel framework that systematically and gradually refines the query to obtain candidate formal queries that do match the knowledge graph. Crucially, we introduce a formal notion of provenance tracking this query refinement process and use the tracked provenance to prompt the user for fine-grained feedback on parts of the candidate query, guiding our search. Experiments on a diverse query workload with respect to both DBpedia and YAGO show the usefulness of our approach.},
  archive      = {J_KIS},
  author       = {Amsterdamer, Yael and Callen, Yehuda},
  doi          = {10.1007/s10115-023-01939-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2165-2191},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Interactive SPARQL query formulation using provenance},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software bug priority prediction technique based on
intuitionistic fuzzy representation and class imbalance learning.
<em>KIS</em>, <em>66</em>(3), 2135–2164. (<a
href="https://doi.org/10.1007/s10115-023-02000-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern times, the software industry is more focused on the timely release of high-quality software. Software bugs have a significant impact on software quality and reliability. To complete the bug triaging process on time, the triager has to understand each bug and assign the correct priority to it. However, the bugs are reported rapidly, with lots of uncertainty and irregularities in the bug tracking system. Furthermore, there are multiple priority labels that are semantically close to each other. As a result, the triager is confused while understanding and prioritizing the bugs. To address these problems, the research presents an intuitionistic fuzzy representation of topic features-based software bug priority prediction (IFTBPP) technique. Initially, the imbalanced priority classes of software bugs are balanced using the synthetic minority oversampling technique. Then, topic modeling is used to create topics and terms for software bugs. The intuitionistic fuzzy set is used on the topics to compute various grades of a bug belonging to multiple priority classes. Finally, the similarity of a newly reported bug is calculated using intuitionistic fuzzy similarity measures with multiple priority classes. All the experiments of IFTBPP are conducted on Eclipse, Mozilla, Apache, and NetBeans repositories and compared with other existing models. The accuracy values obtained by IFTBPP on these repositories are 92.5%, 91.9%, 89.2%, and 93.9%, whereas the corresponding F-measure values are 91.7%, 91.3%, 88.9%, and 93.1%.},
  archive      = {J_KIS},
  author       = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
  doi          = {10.1007/s10115-023-02000-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2135-2164},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Software bug priority prediction technique based on intuitionistic fuzzy representation and class imbalance learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint knowledge graph approach for event participant
prediction with social media retweeting. <em>KIS</em>, <em>66</em>(3),
2115–2133. (<a
href="https://doi.org/10.1007/s10115-023-02015-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organized event is an important form of human activity. Nowadays, many digital platforms offer organized events on the Internet, allowing users to be organizers or participants. For such platforms, it is beneficial to predict potential event participants. Existing work on this problem tends to borrow recommendation techniques. However, compared to e-commerce items and purchases, events and participation are usually of a much smaller frequency, and the data may be insufficient to learn an accurate prediction model. In this paper, we propose to utilize social media retweeting activity to enhance the learning of event participant prediction models. We create a joint knowledge graph to bridge the social media and the target domain, assuming that event descriptions and tweets are written in the same language. Furthermore, we propose a learning model that utilize retweeting information for the target domain prediction more effectively. We conduct comprehensive experiments in two scenarios with real-world data. In each scenario, we set up training data of different sizes, as well as warm and cold test cases. The evaluation results show that our approach consistently outperforms several baseline models in both warm and cold tests.},
  archive      = {J_KIS},
  author       = {Zhang, Yihong and Hara, Takahiro},
  doi          = {10.1007/s10115-023-02015-0},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2115-2133},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Joint knowledge graph approach for event participant prediction with social media retweeting},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SimGCL: Graph contrastive learning by finding homophily in
heterophily. <em>KIS</em>, <em>66</em>(3), 2089–2114. (<a
href="https://doi.org/10.1007/s10115-023-02022-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive learning (GCL) has been widely studied in unsupervised graph representation learning. Most existing GCL methods focus on modeling the invariances of identical instances in different augmented views of a graph and using the Graph Neural Network (GNN) as the underlying encoder to generate node representations. GNNs generally learn node representations by aggregating information from their neighbors, where homophily and heterophily in the graph can strongly affect the performance of GNNs. Existing GCL methods neglect the effect of homophily/heterophily in graphs, resulting in sub-optimal learned representations of graphs with more complex patterns, especially in the case of high heterophily. We propose a novel Similarity-based Graph Contrastive Learning model (SimGCL), which generates augmented views with a higher homophily ratio at the topology level by adding or removing edges. We treat dimension-wise features as weak labels and introduce a new similarity metric based on feature and feature dimension-wise distribution patterns as a guide to improving homophily in an unsupervised manner. To preserve node diversity in augmented views, we retain feature dimensions with higher heterophily to amplify the differences between nodes in augmented views at the feature level. We also use the proposed similarity in the negative sampling process to eliminate possible false negative samples. We conduct extensive experiments comparing our model with ten baseline methods on seven benchmark datasets. Experimental results show that SimGCL significantly outperforms the state-of-the-art GCL methods on both homophilic and heterophilic graphs and brings more than 10% improvement on heterophilic graphs.},
  archive      = {J_KIS},
  author       = {Liu, Cheng and Yu, Chenhuan and Gui, Ning and Yu, Zhiwu and Deng, Songgaojun},
  doi          = {10.1007/s10115-023-02022-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2089-2114},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SimGCL: Graph contrastive learning by finding homophily in heterophily},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to manage massive spatiotemporal dataset from stationary
and non-stationary sensors in commercial DBMS? <em>KIS</em>,
<em>66</em>(3), 2063–2088. (<a
href="https://doi.org/10.1007/s10115-023-02009-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing diffusion of the latest information and communication technologies in different contexts allowed the constitution of enormous sensing networks that form the underlying texture of smart environments. The amount and the speed at which these environments produce and consume data are starting to challenge current spatial data management technologies. In this work, we report on our experience handling real-world spatiotemporal datasets: a stationary dataset referring to the parking monitoring system and a non-stationary dataset referring to a train-mounted railway monitoring system. In particular, we present the results of an empirical comparison of the retrieval performances achieved by three different off-the-shelf settings to manage spatiotemporal data, namely the well-established combination of PostgreSQL + PostGIS with standard indexing, a clustered version of the same setup, and then a combination of the basic setup with Timescale, a storage extension specialized in handling temporal data. Since the non-stationary dataset has put much pressure on the configurations above, we furtherly investigated the advantages achievable by combining the TSMS setup with state-of-the-art indexing techniques. Results showed that the standard indexing is by far outperformed by the other solutions, which have different trade-offs. This experience may help researchers and practitioners facing similar problems managing these types of data.},
  archive      = {J_KIS},
  author       = {Vitale, Vincenzo Norman and Martino, Sergio Di and Peron, Adriano and Russo, Massimiliano and Battista, Ermanno},
  doi          = {10.1007/s10115-023-02009-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2063-2088},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {How to manage massive spatiotemporal dataset from stationary and non-stationary sensors in commercial DBMS?},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SABeDM: A sliding adaptive beta distribution model for
concept drift detection in a dynamic environment. <em>KIS</em>,
<em>66</em>(3), 2039–2062. (<a
href="https://doi.org/10.1007/s10115-023-02004-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of concept drift detection is crucial in machine learning, especially in dynamic contexts where the underlying data distribution can vary over time. For the purpose of identifying concept drift, we suggest a sliding adaptive beta distribution model (SABeDM) in this study. SABeDM combines the adaptive sliding window and beta distribution techniques to track modifications in the underlying distribution of the data stream. Several synthetic and real-world datasets are used to assess the proposed model, and it is then contrasted with cutting-edge drift detection systems. Regarding detecting true positive, false positive, false negative, and delay, our experimental results demonstrate that SABeDM works better than the currently used methods (SRP, ADWIN, DDM, and EDDM). Accuracy, precision, recall, and F1-score were also utilised as evaluation criteria. When used in a variety of applications, such as online learning, data stream mining, and real-time monitoring systems, SABeDM offers an effective and fast way to identify concept drift in a dynamic context. The proposed approach is a promising tool for machine learning practitioners to use in practical applications since it can help to enhance the dependability and accuracy of decision-making systems in dynamic situations.},
  archive      = {J_KIS},
  author       = {Angbera, Ature and Chan, Huah Yong},
  doi          = {10.1007/s10115-023-02004-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2039-2062},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SABeDM: A sliding adaptive beta distribution model for concept drift detection in a dynamic environment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FishRNFuseNET: Development of heuristic-derived recurrent
neural network with feature fusion strategy for fish species
classification. <em>KIS</em>, <em>66</em>(3), 1997–2038. (<a
href="https://doi.org/10.1007/s10115-023-01987-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of fish species has become an essential task for marine ecologists and biologists for the estimation of large quantities of fish variants in their own environment and also to supervise their population changes. Different conventional classification is expensive, time-consuming, and laborious. Scattering and absorption of light in deep sea atmosphere achieves a very low-resolution image and becomes highly challenging for the recognition and classification of fish variants. Then, the performance rate of existing computer vision methods starts to reduce underwater because of highly indistinct features and background clutter of marine species. The attained classification issues can be resolved using deep structured models, which are highly recommended to enhance the performance rate in fish species classification. But, only a limited amount of fish datasets is available, which makes the system more complex, and also, they need enormous amounts of datasets to perform training. So, it is essential to develop an automated and optimized system to detect, categorize, track, and minimize manual interference in fish species classification. Thus, this paper aims to suggest a new fish species classification model by the optimized recurrent neural network (RNN) and feature fusion. Initially, standard underwater images are acquired from a standard database. Then, the gathered images are pre-processed for cleaning and enhancing the quality of images using “contrast limited adaptive histogram equalization (CLAHE) and histogram equalization”. Then, the deep feature extractions are obtained using DenseNet, MobileNet, ResNet, and VGG16, where the gathered features are given to the new phase optimal feature selection. They are performed with a new heuristic algorithm called “modified mating probability-based water strider algorithm (MMP-WSA)” that attains the optimal features. Further, the optimally selected features are further fed to the feature fusion process, where the feature fusion is carried out using the adaptive fusion concept. Here, the weights are tuned using the designed MMP-WSA. In addition, the fused features are sent to the classification phase, where the classification is performed using developed FishRNFuseNET, in which the parameters of the RNN are tuned by developed MMP-WSA for getting accurate classified outcomes. The proposed method is an effective substitute for time-consuming and strenuous approaches in human identification by professionals, and it turned as a benefit to monitor the biodiversity of fish in their place.},
  archive      = {J_KIS},
  author       = {Bhanumathi, M. and Arthi, B.},
  doi          = {10.1007/s10115-023-01987-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1997-2038},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {FishRNFuseNET: Development of heuristic-derived recurrent neural network with feature fusion strategy for fish species classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised feature selection using principal component
analysis. <em>KIS</em>, <em>66</em>(3), 1955–1995. (<a
href="https://doi.org/10.1007/s10115-023-01993-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principal component analysis (PCA) is widely used in computational science branches such as computer science, pattern recognition, and machine learning, as it can effectively reduce the dimensionality of high-dimensional data. In particular, it is a popular transformation method used for feature extraction. In this study, we explore PCA’s ability for feature selection in regression applications. We introduce a new approach using PCA, called Targeted PCA to analyze a multivariate dataset that includes the dependent variable—it identifies the principal component with a high representation of the dependent variable and then examines the selected principal component to capture and rank the contribution of the non-dependent variables. The study also compares the feature selected with that resulting from a Least Absolute Shrinkage and Selection Operator (LASSO) regression. Finally, the selected features were tested in two regression models: multiple linear regression (MLR) and artificial neural network (ANN). The results are presented for three socioeconomic, environmental, and computer image processing datasets. Our study found that 2 of 3 random datasets have more than 50% similarity in the selected features by the PCA and LASSO regression methods. In the regression predictions, our PCA-selected features resulted in little difference compared to the LASSO regression-selected features in terms of the MLR prediction accuracy. However, the ANN regression demonstrated a faster convergence and a higher reduction of error.},
  archive      = {J_KIS},
  author       = {Rahmat, Fariq and Zulkafli, Zed and Ishak, Asnor Juraiza and Abdul Rahman, Ribhan Zafira and Stercke, Simon De and Buytaert, Wouter and Tahir, Wardah and Ab Rahman, Jamalludin and Ibrahim, Salwa and Ismail, Muhamad},
  doi          = {10.1007/s10115-023-01993-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1955-1995},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Supervised feature selection using principal component analysis},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alice and the caterpillar: A more descriptive null model for
assessing data mining results. <em>KIS</em>, <em>66</em>(3), 1917–1954.
(<a href="https://doi.org/10.1007/s10115-023-02001-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce novel null models for assessing the results obtained from observed binary transactional and sequence datasets, using statistical hypothesis testing. Our null models maintain more properties of the observed dataset than existing ones. Specifically, they preserve the Bipartite Joint Degree Matrix of the bipartite (multi-)graph corresponding to the dataset, which ensures that the number of caterpillars, i.e., paths of length three, is preserved, in addition to other properties considered by other models. We describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling datasets from our null models, based on a carefully defined set of states and efficient operations to move between them. The results of our experimental evaluation show that Alice mixes fast and scales well, and that our null model finds different significant results than ones previously considered in the literature.},
  archive      = {J_KIS},
  author       = {Preti, Giulia and De Francisci Morales, Gianmarco and Riondato, Matteo},
  doi          = {10.1007/s10115-023-02001-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1917-1954},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Alice and the caterpillar: A more descriptive null model for assessing data mining results},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attentive neural controlled differential equations for
time-series classification and forecasting. <em>KIS</em>,
<em>66</em>(3), 1885–1915. (<a
href="https://doi.org/10.1007/s10115-023-01977-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks inspired by differential equations have proliferated for the past several years, of which neural ordinary differential equations (NODEs) and neural controlled differential equations (NCDEs) are two representative examples. In theory, NCDEs exhibit better representation learning capability for time-series data than NODEs. In particular, it is known that NCDEs are suitable for processing irregular time-series data. Whereas NODEs have been successfully extended to adopt attention, methods to integrate attention into NCDEs have not yet been studied. To this end, we present attentive neural controlled differential equations (ANCDEs) for time-series classification and forecasting, where dual NCDEs are used: one for generating attention values and the other for evolving hidden vectors for a downstream machine learning task. We conduct experiments on 5 real-world time-series datasets and 11 baselines. After dropping some values, we also conduct experiments on irregular time-series. Our method consistently shows the best accuracy in all cases by non-trivial margins. Our visualizations also show that the presented attention mechanism works as intended by focusing on crucial information.},
  archive      = {J_KIS},
  author       = {Jhin, Sheo Yon and Shin, Heejoo and Kim, Sujie and Hong, Seoyoung and Jo, Minju and Park, Solhee and Park, Noseong and Lee, Seungbeom and Maeng, Hwiyoung and Jeon, Seungmin},
  doi          = {10.1007/s10115-023-01977-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1885-1915},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Attentive neural controlled differential equations for time-series classification and forecasting},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairmod: Making predictions fair in multiple protected
attributes. <em>KIS</em>, <em>66</em>(3), 1861–1884. (<a
href="https://doi.org/10.1007/s10115-023-02003-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive models such as decision trees and neural networks may produce predictions with unfairness. Some algorithms have been proposed to mitigate unfair predictions when protected attributes are considered individually. However, mitigating unfair predictions becomes more difficult when the application involves multiple protected attributes that are expected to be enforced simultaneously. This issue has not been solved, and existing methods are not able to solve it. The paper aims to be the first to solve this problem and proposes a method for post-processing unfair predictions to achieve fair ones. The method considers multiple simultaneous protected attributes together with context attributes, such as position, profession and education, that describe contextual details of the application. Our method consists of two steps. The first step uses a nonlinear optimization problem to determine the best adjustment plan for meeting the requirements of multiple simultaneous protected attributes while better preserving the original predictions. This optimization guarantees the solution to handle the interaction among multiple protected attributes regarding fairness in the best manner. The second steps learns adjustment thresholds using the results of optimization. The proposed method is evaluated using real-world datasets, and the evaluation shows that the proposed method makes effective adjustments to meet fairness requirements.},
  archive      = {J_KIS},
  author       = {Liu, Jixue and Li, Jiuyong and Liu, Lin and Le, Thuc and Ye, Feiyue and Li, Gefei},
  doi          = {10.1007/s10115-023-02003-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1861-1884},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fairmod: Making predictions fair in multiple protected attributes},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighborhood rough set with neighborhood equivalence
relation for feature selection. <em>KIS</em>, <em>66</em>(3), 1833–1859.
(<a href="https://doi.org/10.1007/s10115-023-01999-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection of the neighborhood rough set is an important step in preprocessing the data and improving classification performance. Neighborhood granules form the basis for neighborhood rough set learning and reasoning, but granules typically have overlap, which will cause sample classification uncertainty or repeatability. For this reason, a new notion of neighborhood equivalence relation is used in this paper. Neighborhood equivalence granules solve the above problems, those granules are usually finer than that of the classical neighborhood rough set. In this paper, the neighborhood relation in traditional neighborhood rough set is replaced by neighborhood equivalence relation. Based on neighborhood equivalence relation, a Neighborhood rough set Model based on neighborhood Equivalence Relation (NMER) is proposed. We also introduce the properties of NMER and explain the significance of features. Based on the proposed NMER, a feature selection algorithm is also designed. The reduction results on twelve datasets show that the proposed feature selection algorithm can select main and useful features, confirming the effectiveness of the algorithm.},
  archive      = {J_KIS},
  author       = {Wu, Shangzhi and Wang, Litai and Ge, Shuyue and Hao, Zhengwei and Liu, Yulin},
  doi          = {10.1007/s10115-023-01999-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1833-1859},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Neighborhood rough set with neighborhood equivalence relation for feature selection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a taxonomy for business capabilities determining
data value. <em>KIS</em>, <em>66</em>(3), 1807–1831. (<a
href="https://doi.org/10.1007/s10115-023-01994-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data and its valuation have gained vital significance in academia and enterprises, coinciding with diverse data valuation approaches encompassing various layers, dimensions, and characteristics. This paper assesses data value determination through a business capability lens based on the TOGAF standard. The paper encompasses (a) constructing a Data Valuation Business Capability (DVBC) taxonomy and (b) validating the taxonomy using two existing data valuation concepts from academia. The methodology involves information systems taxonomy development techniques backed by a previously conducted systematic literature review of 64 articles. The resultant taxonomy comprises four business capability layers, nine dimensions, and 36 characteristics. These layers and dimensions offer business, technology, and organizational perspectives, reflecting the interdisciplinary nature of data valuation alongside an enterprise architecture. Characteristics within these layers and dimensions are either exclusive or non-exclusive based on their contents. The compiled findings meet both objective and subjective quality criteria. The implications of the DVBC are multifaceted, influencing scholars and professionals alike. Scholars gain a cohesive tool enhancing transparency in the extensively debated data value domain, fostering linkages among information systems, enterprise architecture management, and data management. This empowers the progress in developing comprehensive data valuation concepts. Additionally, professionals may employ the DVBC taxonomy as a lighthouse and guiding tool, fostering internal dialog on data valuation. This entails elevating data valuation to a pivotal business capability, necessitating collaborative, regular assessment, and enhancement involving business and technological stakeholders. By adopting this taxonomy, the challenge of consistently determining data value can be effectively addressed in both academia and enterprises.},
  archive      = {J_KIS},
  author       = {Hafner, Markus and Mira da Silva, Miguel},
  doi          = {10.1007/s10115-023-01994-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1807-1831},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Towards a taxonomy for business capabilities determining data value},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality of interaction-based predictive model for support of
online learning in pandemic situations. <em>KIS</em>, <em>66</em>(3),
1777–1805. (<a
href="https://doi.org/10.1007/s10115-023-01995-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher education institutions place a lot of importance on their electronic learning systems. Educational institutions in Pakistan and other countries have adopted learning management systems (LMS) due to the coronavirus (COVID-19) pandemic scenario. The learning management system (LMS) establishes a digital learning environment where evaluation and user learning behavior must be carefully analyzed. The “quality of interaction” (QoI) of students is one of the main issues in LMS. Based on various usage matrices (such as the number of logins, clicks, total time spent on the LMS, and actions taken), a student’s level of interaction with the LMS can be determined. QoI is an essential predictor of the accomplishment of students’ final grades. Normally, to examine the effectiveness of LMS usage on students’ learning performance, studies have relied on data gathered from users via surveys. However, the data gathered through surveys are typically associated with the risk of distortion or low quality. Therefore, in order to evaluate and predict the quality of interaction in terms of usage matrices, our proposed work analyzed data from the Moodle LMS at “Hazara University” (HU) for the law and English departments’ courses. This research aims to assess and forecast the quality of student interaction within an LMS by analyzing usage metrics. Unlike traditional survey-based approaches, we explored the predictive performance of LSTM (Long Short-Term Memory), Exponential Smoothing method (ETS), and ARIMA (Autoregressive Integrated Moving Average) methods to predict the weekly LMS usage factors of students. ARIMA and ETS produce better prediction results than LSTM for weekly predictions. Moreover, LSTM model training took considerable computational time for provided datasets.},
  archive      = {J_KIS},
  author       = {Mumtaz, Faiza and Jehangiri, Ali Imran and Ishaq, Waqar and Ahmad, Zulfiqar and Alramli, Omar Imhemed and Ala’anzy, Mohammed Alaa and Ghoniem, Rania M.},
  doi          = {10.1007/s10115-023-01995-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1777-1805},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Quality of interaction-based predictive model for support of online learning in pandemic situations},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiMuF: A bi-directional recommender system with
multi-semantic filter for online recruitment. <em>KIS</em>,
<em>66</em>(3), 1751–1776. (<a
href="https://doi.org/10.1007/s10115-023-01997-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing recommendation research has been concentrated on unidirectional recommendation, i.e., only recommending items to users. However, the platform needs to achieve bi-directional recommendation in many real-world scenarios. For example, in an online recruitment scenario, the recommender system not only needs to recommend positions to candidates, but also recommend candidates to enterprises. In this paper, we develop a new bi-directional recommendation model for online recruitment termed (BiMuF) Bi-directional recommendation with Multi-semantic Filter. In BiMuF, an encoder component is utilized to learn the text embeddings, a multi-semantic filter component is designed to capture important graph representation, and a graph learning component is designed to learn the graph embeddings. In addition, a multi-task learning framework is designed to achieve bi-directional recommendations. In the multi-task learning framework, we share text embeddings and graph embeddings to alleviate the problems of data sparsity, data asymmetry, and feature generalization in online recruitment. Moreover, we conduct three new datasets based on a technology supply and demand docking platform namely South China Technology Commercialization Center, and three new datasets in job recruitment scenario namely Computer Technology-Related Job Recruitment. Extensive experiments in the real-world tasks show that BiMuF outperforms the state-of-the-art methods, verifying the effectiveness of the designs of our model. The code and the latter datasets are available at https://github.com/allminerlab .},
  archive      = {J_KIS},
  author       = {Lai, Pei-Yuan and Yang, Zhe-Rui and Dai, Qing-Yun and Liao, De-Zhang and Wang, Chang-Dong},
  doi          = {10.1007/s10115-023-01997-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1751-1776},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {BiMuF: A bi-directional recommender system with multi-semantic filter for online recruitment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A black-box model for predicting difficulty of word puzzle
games: A case study of wordle. <em>KIS</em>, <em>66</em>(3), 1729–1750.
(<a href="https://doi.org/10.1007/s10115-023-01992-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popular word-filling game Wordle has gained widespread attention since its release in 2022. Much attention has been paid to find the optimal strategy. However, this article proposes a black-box prediction model that can accurately predict the difficulty level of words in the game to find the deep rules in the game data. In this work, we scientifically established a black-box model for game difficulty prediction. We achieve high accuracy in new datasets and show strong stability in similar tasks. The black-box model is divided into the game input content feature extraction model and the game output content rule extraction model. This research scientifically and effectively extracts word attributes, including word frequency, letter frequency, part of speech, times of letter repetitions, and word meaning score from the input content. Then it reduces the seven kinds of proportion of people in different tries in output content into two indices using the Critic method. Finally, it establishes a gradient boosting decision tree-based multiple regression model, making the final prediction accuracy of difficulty level for new words reach 95%. It is believed that the black-box prediction model can provide valuable insights for game designers and developers. And the research provides an innovative method to predict and understand user behavior in online games, contributing to the broader field of data science. The integration of data-driven methodologies in the gaming industry opens new possibilities for understanding player interactions and further enhancing game development strategies.},
  archive      = {J_KIS},
  author       = {Shi, Ling and Chen, Yingke and Lin, Jiaxuan and Chen, Xiaoyu and Dai, Guangming},
  doi          = {10.1007/s10115-023-01992-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1729-1750},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A black-box model for predicting difficulty of word puzzle games: A case study of wordle},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting document novelty: An unsupervised learning
approach. <em>KIS</em>, <em>66</em>(3), 1709–1728. (<a
href="https://doi.org/10.1007/s10115-023-01989-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of information deluge, it is pivotal to have access to information or knowledge which is not just relevant but also, novel. Knowledge workers often have to skim through tens or even hundreds of abstracts or articles to identify those novel documents which can enhance their knowledge. While there are personalized recommenders which can provide relevant documents to these knowledge workers, there aren’t many systems which can identify ‘novel’ documents which are relevant. Critical roadblocks in the discovery of novel documents are big, labelled datasets and complex, expensive infrastructure for training models. This work attempts to overcome these roadblocks by proposing an unsupervised classifier based on word associations to predict the novelty of a document based on its content. Evaluation based on a benchmarking dataset TAP-DLND 1.0 has revealed that, the performance of this classifier is comparable to that of many of the state-of-the-art supervised learning techniques including some of the deep learning models. The results can have potential implications in the way NLP researchers approach novelty detection in documents by not having to rely on large, labelled training datasets which are scarce!},
  archive      = {J_KIS},
  author       = {Nair, Binesh},
  doi          = {10.1007/s10115-023-01989-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1709-1728},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Predicting document novelty: An unsupervised learning approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing multimedia document modeling through extended
orbit-based rhetorical structure: An approach to media weighting for
importance determination. <em>KIS</em>, <em>66</em>(3), 1683–1707. (<a
href="https://doi.org/10.1007/s10115-023-01984-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a graph-based approach to determine the importance of each media in a multimedia document by expanding the traditional four-dimensional model with a dimension that captures the rhetorical relations between different media types. The proposed approach utilizes an algorithm to weight the media types based on their significance. The use of rhetorical structure theory enables the determination of the significance of each media type, making it useful for document adaptation, automatic composition, and automatic generation of summaries. The approach utilizes an extended orbits-based rhetorical structure that is a novel method for determining the importance of media types in multimedia documents. The proposed approach is effective in capturing the importance of each media type and can be utilized in a wide range of applications, making it a potential solution to the limitations of the traditional model. This research has implications for a range of applications, including document adaptation, automatic composition, and automatic generation of summaries.},
  archive      = {J_KIS},
  author       = {Maredj, Azze-Eddine and Sadallah, Madjid and Tonkin, Nourreddine},
  doi          = {10.1007/s10115-023-01984-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1683-1707},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing multimedia document modeling through extended orbit-based rhetorical structure: An approach to media weighting for importance determination},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive graph contrastive learning with joint optimization
of data augmentation and graph encoder. <em>KIS</em>, <em>66</em>(3),
1657–1681. (<a
href="https://doi.org/10.1007/s10115-023-01979-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) has been successfully used to solve the problem of the huge cost of graph data annotation, such as labor cost, time cost, and professional knowledge cost. Recent works have focused on improving the generalization performance of GCL with automated data augmentation. However, GCL methods with automated data augmentation encode the graph representation using fixed graph encoders, which will result in performance loss. To overcome this limitation, we propose Adaptive graph contrastive learning with joint optimization of data Augmentation and graph Encoder (AdaAE). AdaAE is the first method to learn to adapt the graph encoder for each dataset for GCL with automated data augmentation. Specifically, we design a unified GCL search space, which means that we treat both data augmentation and graph encoder as architecture components. AdaAE employs the adaptive architecture optimizer based on a differentiable search method to learn the sample probability distribution of each architecture component. The adaptive architecture optimizer generates an operation of each architecture component based on the sample probability distribution to construct the GCL model. Then, the validation results of the GCL model are used as the feedback signal of the adaptive architecture optimizer to optimize the sample probability distribution of each architecture component. Extensive experiments demonstrate that AdaAE performs more superior than the state-of-the-art baselines. Furthermore, the visualization results confirm that AdaAE can clearly distinguish different classes in the projection space.},
  archive      = {J_KIS},
  author       = {Wu, Zhenpeng and Chen, Jiamin and Al-Sabri, Raeed and Oloulade, Babatounde Moctard and Gao, Jianliang},
  doi          = {10.1007/s10115-023-01979-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1657-1681},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive graph contrastive learning with joint optimization of data augmentation and graph encoder},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic time-aware collaborative sequential recommendation
with attention-based network. <em>KIS</em>, <em>66</em>(3), 1639–1655.
(<a href="https://doi.org/10.1007/s10115-023-01996-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A natural way of user modeling in sequential recommendation is to capture long-term and short-term preferences, respectively, given user historical behaviors and then fuse them together. Most existing approaches building on attention-based network focus only on exploring item–item relations within each user sequence and ignore collaborative relations among different user sequences, which restricts the improvement of recommendation quality, especially on sparse datasets. Moreover, construction and utilization of collaborative signals including the integration with the original information greatly impact the recommendation effects. In this paper, we propose a novel method named dynamic time-aware collaborative sequential recommendation with attention-based network(DTCoSR) to further address the issues. Specifically, we first design a time-aware collaborative item module to gain collaborative item representations for both long- and short-term interests, consisting of neighborhood selection and neighborhood information aggregation. Then, we utilize two independent self-attention networks to extract the two different levels of short-term interests dependent on the item representation and collaborative item representation, respectively, and then adaptively merge them as the final short-term interests. We achieve long-term interest via the correlation between the user embedding and its collaborative item embedding. Finally, DTCoSR fuses long- and short-term interests in an adaptive method. Extensive experiments on three real-world datasets show that DTCoSR outperforms state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Liu, Li},
  doi          = {10.1007/s10115-023-01996-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1639-1655},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic time-aware collaborative sequential recommendation with attention-based network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection techniques for machine learning: A survey
of more than two decades of research. <em>KIS</em>, <em>66</em>(3),
1575–1637. (<a
href="https://doi.org/10.1007/s10115-023-02010-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning algorithms can be less effective on datasets with an extensive feature space due to the presence of irrelevant and redundant features. Feature selection is a technique that effectively reduces the dimensionality of the feature space by eliminating irrelevant and redundant features without significantly affecting the quality of decision-making of the trained model. In the last few decades, numerous algorithms have been developed to identify the most significant features for specific learning tasks. Each algorithm has its advantages and disadvantages, and it is the responsibility of a data scientist to determine the suitability of a specific algorithm for a particular task. However, with the availability of a vast number of feature selection algorithms, selecting the appropriate one can be a daunting task for an expert. These challenges in feature selection have motivated us to analyze the properties of algorithms and dataset characteristics together. This paper presents significant efforts to review existing feature selection algorithms, providing an exhaustive analysis of their properties and relative performance. It also addresses the evolution, formulation, and usefulness of these algorithms. The manuscript further categorizes the algorithms analyzed in this review based on the properties required for a specific dataset and objective under study. Additionally, it discusses popular area-specific feature selection techniques. Finally, it identifies and discusses some open research challenges in feature selection that are yet to be overcome.},
  archive      = {J_KIS},
  author       = {Theng, Dipti and Bhoyar, Kishor K.},
  doi          = {10.1007/s10115-023-02010-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1575-1637},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature selection techniques for machine learning: A survey of more than two decades of research},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review on novelty detection in the non-stationary
environment. <em>KIS</em>, <em>66</em>(3), 1549–1574. (<a
href="https://doi.org/10.1007/s10115-023-02018-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty detection and concept drift detection are essential for the plethora of machine learning applications. The statistical properties of application generated data change over time in the streaming environment, known as concept drift. These changes develop a profound influence on the learning model’s performance. Along with concept drift, the new class emergence (i.e., novel class/novelty detection) is also challenging in the non-stationary distribution of data. Novel class detection finds whether the identifying data points of a data stream are unknown or unusual. The paper presents a survey focusing on the challenges encountered while dealing with real-time data. In addition to this, the chronological discussion on the various existing novelty detectors with their advantages, limitations, critical points, the different research prospect, and future directions are also incorporated in the paper.},
  archive      = {J_KIS},
  author       = {Agrahari, Supriya and Srivastava, Sakshi and Singh, Anil Kumar},
  doi          = {10.1007/s10115-023-02018-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1549-1574},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Review on novelty detection in the non-stationary environment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent developments in geographic information systems across
different application domains: A review. <em>KIS</em>, <em>66</em>(3),
1523–1547. (<a
href="https://doi.org/10.1007/s10115-023-01969-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in geospatial technologies together with the availability of geographic information system (GIS) software and tools with enhanced capabilities in the recent decades have shifted the use of this technology from an early adoption phase to a more mature phase. The application of GIS technology has found new avenues in solving complex socio-economic engineering problems from disaster management and mitigation planning to environmental modelling for sustainable development of cities. In order to reveal the recent developments in use of this technology in different scientific disciplines, a new methodology is developed by considering the semantics to formalize the engineering knowledge in GIS applications. This formalized knowledge is then used to reveal emerging macrotrends, challenges and directions of future research in two different semantic aspects, the “the application” aspect and “the domain” aspect. The review of “the application” aspect shows that a significant change is seen in the applications of GIS, a shift towards complexity in analyses for engineering and building systems for management and decision-making. Further, a systematic keyword analysis on “the domain” aspect is used to develop sub-domains ontologies and a review of literature on each of these sub-domains details the core developments in GIS and some emerging domains of application. Semantic inferences are drawn based on domain-specific requirements, problems and challenges, and outcomes to improve the domain knowledge. The GIS domain knowledge can further be used in developing computational methods using ontologies, formalization of logic and engineering the future GIS systems. The review interestingly finds a growing trend in using GIS as a Decision Support System (DSS), to model domain-specific contexts, semantically integrate data and develop expert intelligence.},
  archive      = {J_KIS},
  author       = {Sowmiya Narayanan, Kuduva Janarthanan and Manimaran, Asaithambi},
  doi          = {10.1007/s10115-023-01969-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1523-1547},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recent developments in geographic information systems across different application domains: A review},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: Estimating time series averages from latent
space of multi-tasking neural networks. <em>KIS</em>, <em>66</em>(2),
1521. (<a href="https://doi.org/10.1007/s10115-023-01981-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Debella, Tsegamlak Terefe and Devanne, Maxime and Weber, Jonathan and Woldegebreal, Dereje Hailemariam and Forestier, Germain},
  doi          = {10.1007/s10115-023-01981-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1521},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: Estimating time series averages from latent space of multi-tasking neural networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-based paper-level classification procedure for
non-traditional sciences using a machine learning approach.
<em>KIS</em>, <em>66</em>(2), 1503–1520. (<a
href="https://doi.org/10.1007/s10115-023-02023-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Science as a whole is organized into broad fields, and as a consequence, research, resources, students, etc., are also classified, assigned, or invited following a similar structure. Some fields have been established for centuries, and some others are just flourishing. Funding, staff, etc., to support fields are offered if there is some activity on it, commonly measured in terms of the number of published scientific papers. How to find them? There exist well-respected listings where scientific journals are ascribed to one or more knowledge fields. Such lists are human-made, but the complexity begins when a field covers more than one area of knowledge. How to discern if a particular paper is devoted to a field not considered in such lists? In this work, we propose a methodology able to classify the universe of papers into two classes; those belonging to the field of interest, and those that do not. This proposed procedure learns from the title and abstract of papers published in monothematic or “pure” journals. Provided that such journals exist, the procedure could be applied to any field of knowledge. We tested the process with Geographic Information Science. The field has contacts with Computer Science, Mathematics, Cartography, and others, a fact which makes the task very difficult. We also tested our procedure and analyzed its results with three different criteria, illustrating its power and capabilities. Interesting findings were found, where our proposed solution reached similar results as human taggers also similar results compared with state-of-the-art related work.},
  archive      = {J_KIS},
  author       = {Moctezuma, Daniela and López-Vázquez, Carlos and Lopes, Lucas and Trevisan, Norton and Pérez, José},
  doi          = {10.1007/s10115-023-02023-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1503-1520},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Text-based paper-level classification procedure for non-traditional sciences using a machine learning approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantics-enabled approach for personalised data lake
exploration. <em>KIS</em>, <em>66</em>(2), 1469–1502. (<a
href="https://doi.org/10.1007/s10115-023-02014-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of Big Data is changing the way data exploration for Business Intelligence is performed, due to the volume, velocity and uncontrolled variety of data on which exploration relies. In particular, data exploration is required in Data Lakes that have been proposed to host heterogeneous data sources, given their flexibility to cope with cumbersome properties of Big Data. However, as data grows, new methods and techniques are required for extracting value and knowledge from data stored within Data Lakes, aggregating data into indicators according to multiple analysis dimensions, to enable a large number of users with different roles and competencies to capitalise on available information. In this paper, we propose PERSEUS (PERSonalised Exploration by User Support), a computer-aided approach for data exploration on top of a Data Lake, structured over three phases: (1) the construction of a semantic metadata catalog on top of the Data Lake, leveraging tools and metrics to ease the annotation of the Data Lake metadata; (2) modelling of indicators and analysis dimensions, guided by an openly available Multi-Dimensional Ontology to enable conformance checking of indicators and let users explore Data Lake contents; (3) enrichment of the definition of indicators with personalisation aspects, based on users’ profiles and preferences, to make easier and more usable the exploration of data for a large number of users. Results of an experimental evaluation in the Smart City domain are presented with the aim of demonstrating the feasibility of the approach.},
  archive      = {J_KIS},
  author       = {Bianchini, Devis and De Antonellis, Valeria and Garda, Massimiliano},
  doi          = {10.1007/s10115-023-02014-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1469-1502},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A semantics-enabled approach for personalised data lake exploration},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Top-k approximate selection for typicality query results
over spatio-textual data. <em>KIS</em>, <em>66</em>(2), 1425–1468. (<a
href="https://doi.org/10.1007/s10115-023-02013-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial keyword query is a classical query processing mode for spatio-textual data, which aims to provide users the spatio-textual objects with the highest spatial proximity and textual similarity to the given query. However, the top-k result objects obtained by using the spatial keyword query mode are often similar to each other, while users hope that the system can pick top-k typicality results from the candidate query results in order to make users understand the representative features of the candidate result set. To deal with the problem of typicality analysis and typical object selection of spatio-textual data query results, a typicality evaluation and top-k approximate selection approach is proposed. First, the approach calculates the synthetic distances on dimensions of geographic location, textual semantics, and numeric attributes between all spatio-textual objects. And then, a hybrid index structure that can simultaneously support the location, text, and numeric multi-dimension matching is presented in order to expeditiously obtain the candidate query results. According to the synthetic distances between spatio-textual objects, a Gaussian kernel probability density estimation-based method for measuring the typicality of query results is proposed. To facilitate the query result analysis and top-k typical object selection, the Tournament strategy-based and local neighborhood-based top-k typical object approximate selection algorithms are presented, respectively. The experimental results demonstrated that the text semantic relevancy measuring method for spatio-textual objects is accurate and reasonable, and the local neighborhood-based top-k typicality result approximate selection algorithm achieved both the low error rate and high execution efficiency. The source code and datasets used in this paper are available to be accessed from https://github.com/JiaShengS/Typicality_analysis/ .},
  archive      = {J_KIS},
  author       = {Meng, Xiangfu and Zhang, Xiaoyan and Huo, Hongjin and Leng, Qiangkui},
  doi          = {10.1007/s10115-023-02013-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1425-1468},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Top-k approximate selection for typicality query results over spatio-textual data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microaneurysms detection in fundus images using local
fourier transform and neighbourhood analysis. <em>KIS</em>,
<em>66</em>(2), 1403–1423. (<a
href="https://doi.org/10.1007/s10115-023-01991-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microaneurysms, tiny, circular red dots that occur in retinal fundus images, are one of the earliest symptoms of diabetic retinopathy. Because microaneurysms are small and delicate, detecting them can be difficult. Their small size and cunning character make automatic detection of them difficult. The automatic detection of microaneurysms in retinal fundus images is proposed in this research using a local Fourier transform and neighbourhood analysis-based multi-scale approach technique. The suggested method is broken down into three stages: image preprocessing, the detection of retinal vessels and microaneurysm candidates, and labelling of the candidates. A multi-scale framework is used to develop every stage of the algorithm, with the exception of the initial image preprocessing, giving the mechanism for efficient microaneurysms detection. In contrast to the short-time Fourier transform, which extracts the neighbourhood of each pixel and calculates each local Fourier transform separately, the local Fourier transform is employed in this study to extract the MA. After that, neighbourhood analysis is performed to name the microaneurysm because the item is actually a collection of independent little images rather than the entire image. Three separate data sets and different types of performance indicators are used to examine the robustness of the proposed model. Through the prominent performance, the proposed model is able to outperform other existing models. The classification accuracy of the proposed method for MESSIDOR and ORIGA data set is 99.28% and 98.95%, respectively.},
  archive      = {J_KIS},
  author       = {Perumal, T. Sudarson Rama and Jayachandran, A. and Kumar, S. Ratheesh},
  doi          = {10.1007/s10115-023-01991-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1403-1423},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Microaneurysms detection in fundus images using local fourier transform and neighbourhood analysis},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology-based soft computing and machine learning model for
efficient retrieval. <em>KIS</em>, <em>66</em>(2), 1371–1402. (<a
href="https://doi.org/10.1007/s10115-023-01990-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unstructured and unorganized data always degrade the performance of search techniques and produce irrelevant results in response to the query as well as decrease the speed of retrieval results. Ontology in semantic web (SW) provides an adequate solution to represent the knowledge, because of its backbone knowledge of an application or domain. But, domain ontology has three basic problems while retrieving useful knowledge from a domain ontology: (a) structuring/arrangement, (b) unnecessary knowledge reduction, selection and extraction, and (c) speeding up the retrieval process. To resolve these problems, we proposed multi-level k-mean clustering approach with rough set and Bayesian network model for ontology (MLK-rBO). The proposed model works in four different phases—clustering, knowledge discovery, building a probabilistic network, and model evaluation. The model ensembles three different techniques, namely clustering, rough set (RS), and Bayesian network (BN). Finally, the proposed model is tested with statistical parameters and compared with other models, namely decision tree (DT), random forest (RF), and support vector machine (SVM) to evaluate performance. By analyzing experimental results, we observed that the MLK-rBO gives better accuracy: 98.36% for survey data (fever) and 86% for Wine quality data than available models.},
  archive      = {J_KIS},
  author       = {Anand, Sanjay Kumar and Kumar, Suresh},
  doi          = {10.1007/s10115-023-01990-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1371-1402},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ontology-based soft computing and machine learning model for efficient retrieval},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Next-generation antivirus for JavaScript malware detection
based on dynamic features. <em>KIS</em>, <em>66</em>(2), 1337–1370. (<a
href="https://doi.org/10.1007/s10115-023-01978-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many kinds of Exploit Kits, each one being built with several vulnerabilities, but almost all of them are written in JavaScript. So, we created an antivirus, endowed with machine learning, expert in detecting JavaScript malware based on Runtime Behaviors. In our methodology, JavaScript is executed, in a controlled environment. The goal was to investigate suspicious file behavior. Our antivirus, as a whole, dynamically monitors and ponders 7690 suspicious behaviors that the JavaScript file can do in Windows 7. As experiments, the authorial antivirus is compared to antiviruses based on deep as based on shallow networks. Our antivirus achieves an average accuracy of 99.75% in the distinction between benign and malware, accompanied by a training time of 8.92 s. Establishing the relationship between accuracy and training time is essential in information security. Eight (8) new malware are released every second. An antivirus with excessive training time can become obsolete even when released. As our proposed model can overcome the limitations of state-of-the-art, our antivirus combines high accuracy and fast training. In addition, the authorial antivirus is able to detect JavaScript malware, endowed with digital antiforense, such as obfuscates, polymorphic and fileless attacks.},
  archive      = {J_KIS},
  author       = {de Lima, Sidney M. L. and Souza, Danilo M. and Pinheiro, Ricardo P. and Silva, Sthéfano H. M. T. and Lopes, Petrônio G. and de Lima, Rafael D. T. and de Oliveira, Jemerson R. and Monteiro, Thyago de A. and Fernandes, Sérgio M. M. and Albuquerque, Edison de Q. and da Silva, Washington W. A. and dos Santos, Wellington P.},
  doi          = {10.1007/s10115-023-01978-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1337-1370},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Next-generation antivirus for JavaScript malware detection based on dynamic features},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial enhanced attributed network embedding.
<em>KIS</em>, <em>66</em>(2), 1301–1336. (<a
href="https://doi.org/10.1007/s10115-023-01980-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed network embedding aims to extract latent features of complex networks from structural topology and node attributes. Existing embedding models either use two separate learning processes to capture the complementarity of network topology and node attributes, or one process to capture their consistency. However, it is rare to simultaneously consider the complementarity and consistency of the network topology and node attributes in a single model, and network noise is often ignored. To address these issues, an unsupervised adversarial enhanced attributed network embedding model is proposed in this paper, called AEANE. The model firstly designs a novel feature extractor to simultaneously learn the consistency and complementarity features of node attributes and network topology, aiming to capture more accurate and comprehensive features. Secondly, a noisy version of the feature extractor is constructed as a fake feature generator to quickly generate more unseen fake features, aiming to simulate network noise and construct two game players. Finally, a novel adversarial auto-encoder network is designed, which integrates the auto-encoder network and the generative adversarial network into a single network, aiming to quickly learn robust node embedding and tolerate network noise. Experiments on four publicly available datasets and three analysis tasks demonstrate the advantages of our AEANE model.},
  archive      = {J_KIS},
  author       = {Chen, Lei and Li, Yuan and Deng, Xingye and Liu, Canwei and He, Tingqin and Xiao, Ruifeng},
  doi          = {10.1007/s10115-023-01980-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1301-1336},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adversarial enhanced attributed network embedding},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic action graphs for goal recognition problems with
inaccurately initialised fluents. <em>KIS</em>, <em>66</em>(2),
1257–1300. (<a
href="https://doi.org/10.1007/s10115-023-01976-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal recognisers attempt to infer an agent’s intentions from a sequence of observed actions. This is an important component of intelligent systems that aim to assist or thwart actors; however, there are many challenges to overcome. For example, the initial state of the environment could be partially unknown, and agents can act suboptimally and observations could be missing. Approaches that adapt classical planning techniques to goal recognition have previously been proposed, but, generally, they assume the initial world state is accurately defined. In this paper, a state is inaccurate if any fluent’s value is unknown or incorrect. Our aim is to develop a goal recognition approach that is as accurate as the current state-of-the-art algorithms and whose accuracy does not deteriorate when the initial state is inaccurately defined. To cope with this complication, we propose solving goal recognition problems by means of an Action Graph. An Action Graph models the dependencies, i.e. order constraints, between all actions rather than just actions within a plan. Leaf nodes correspond to actions and are connected to their dependencies via operator nodes. After generating an Action Graph, the graph’s nodes are labelled with their distance from each hypothesis goal. This distance is based on the number and type of nodes traversed to reach the node in question from an action node that results in the goal state being reached. For each observation, the goal probabilities are then updated based on either the distance the observed action’s node is from each goal or the change in distance. Our experimental results, for 15 different domains, demonstrate that our approach is robust to inaccuracies within the defined initial state.},
  archive      = {J_KIS},
  author       = {Harman, Helen and Simoens, Pieter},
  doi          = {10.1007/s10115-023-01976-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1257-1300},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cyclic action graphs for goal recognition problems with inaccurately initialised fluents},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online log parsing using evolving research tree.
<em>KIS</em>, <em>66</em>(2), 1231–1255. (<a
href="https://doi.org/10.1007/s10115-023-01953-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logs are a reliable source of information for development and maintenance purposes. They record information at runtime regarding the state of a system and are commonly used to analyze its behavior. Parsing operations on logs structure the information embedded within the log message and are a crucial step for many log mining applications. In such use cases, parsing effectiveness can impact performance. For systems that require real-time performance, parsing efficiency is also an important factor. In this paper, we present USTEP, an online log parser that uses an evolving tree structure to encode and discover new parsing rules on the fly. Our evaluation of 14 datasets from different logging environments highlights the superiority of our method in terms of robustness and effectiveness compared to the state of the art. Our analysis of space and time complexity shows that USTEP is the only considered method capable of processing logs in constant time regardless of their length. We also propose here USTEP-UP, a way of running multiple USTEP instances in parallel.},
  archive      = {J_KIS},
  author       = {Vervaet, Arthur and Callau-Zori, Mar and Chabchoub, Yousra and Chiky, Raja},
  doi          = {10.1007/s10115-023-01953-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1231-1255},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online log parsing using evolving research tree},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized few-shot node classification: Toward an
uncertainty-based solution. <em>KIS</em>, <em>66</em>(2), 1205–1229. (<a
href="https://doi.org/10.1007/s10115-023-01975-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For real-world graph data, the node class distribution is inherently imbalanced and long-tailed, which naturally leads to a few-shot learning scenario with limited nodes labeled for newly emerging classes. There are many carefully designed solutions for such a few-shot learning problem via methods such as data augmentation, learning transferable initialization, learning prototypes, and many more. However, most, if not all, of them are based on a strong assumption that all the test nodes exclusively come from novel classes, which is impractical in real-world applications. In this paper, we study a broader and more realistic problem named generalized few-shot node classification, where the test samples can be from both novel classes and base classes. Compared with the standard few-shot node classification, this new problem imposes several unique challenges, including asymmetric classification and inconsistent preference. To counter those challenges, we propose a shot-aware neural node classifier (Stager) equipped with an uncertainty-based weight assigner module for adaptive propagation. As the existing meta-learning solutions cannot handle this new problem, we propose a novel training paradigm named imbalanced episodic training to ensure the label distribution is consistent between the meta-training and meta-test scenarios. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of our proposed model and training paradigm.},
  archive      = {J_KIS},
  author       = {Xu, Zhe and Ding, Kaize and Wang, Yu-Xiong and Liu, Huan and Tong, Hanghang},
  doi          = {10.1007/s10115-023-01975-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1205-1229},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Generalized few-shot node classification: Toward an uncertainty-based solution},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot partial multi-label learning with synthetic
features network. <em>KIS</em>, <em>66</em>(2), 1167–1203. (<a
href="https://doi.org/10.1007/s10115-023-01988-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In partial multi-label learning (PML) problems, each training sample is partially annotated with a candidate label set, among which only a subset of labels are valid. The major hardship for PML is that its training procedure is prone to be misled by false positive labels concealed in the candidate label set. To train a noise-robust multi-label predictor for PML problem, most existing methods hold the assumption that sufficient training samples are available. However, in actual fact, especially when dealing with new tasks, we more often only have a few PML samples for the target task. In this paper, we propose a unified model called FsPML-SF (Few-shot Partial Multi-Label Learning with Synthetic Features Network). FsPML-SF includes three modules: label disambiguation, data augmentation and classifier induction. Specifically, FsPML-SF attempts to update the label credibility of each PML sample by leveraging the feature and semantic similarities, the label credibility of other samples and label co-occurrence in a unified objective function. Next, FsPML-SF introduces a synthetic feature network to generate more training samples from pairs of given samples with corresponding label credibility values. FsPML-SF then utilizes the original and synthesized samples to induce a noise-tolerant multi-label classifier. We conducted extensive experiments on benchmark datasets, FsPML-SF outperforms recent competitive PML baselines and few-shot solutions. Both the label denoising and data augmentation improve the performance of PML on few-shot data.},
  archive      = {J_KIS},
  author       = {Sun, Yifan and Zhao, Yunfeng and Yu, Guoxian and Yan, Zhongmin and Domeniconi, Carlotta},
  doi          = {10.1007/s10115-023-01988-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1167-1203},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Few-shot partial multi-label learning with synthetic features network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Enumerating all multi-constrained s-t paths on temporal
graph. <em>KIS</em>, <em>66</em>(2), 1135–1165. (<a
href="https://doi.org/10.1007/s10115-023-01958-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the problem of multi-constrained s-t simple paths enumeration on temporal graph, which aims to list all temporal paths with the minimum timestamp in the given time interval. To solve this problem, a two-stage algorithm based on temporal subgraph is proposed. In the first stage, two algorithms for creating temporal subgraph are proposed to reduce the search scope. The first creates subgraph based on temporal graph, which uses the results of the vertex pruning algorithm to prune the temporal graph. The second creates temporal subgraph based on edge stream, which reorders the edge stream, traverses the edge stream bidirectionally, and uses the flags to prune edges. In the second stage, the depth-first search algorithm is used to find the paths on the temporal subgraph, and the pruning strategies are used to reduce the search scope. We conduct comprehensive experiments on 8 real datasets. Our experiment results demonstrate the efficiency of our proposed algorithm.},
  archive      = {J_KIS},
  author       = {Jin, Yue and Chen, Zijun and Liu, Wenyuan},
  doi          = {10.1007/s10115-023-01958-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1135-1165},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enumerating all multi-constrained s-t paths on temporal graph},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SR-HetGNN: Session-based recommendation with heterogeneous
graph neural network. <em>KIS</em>, <em>66</em>(2), 1111–1134. (<a
href="https://doi.org/10.1007/s10115-023-01986-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The session-based recommendation system aims to predict the user’s next click based on their previous session sequence. The current studies generally learn user preferences according to the transitions of items in the user’s session sequence. However, other effective information in the session sequence, such as user profiles, is largely ignored which may lead to the model unable to learn the user’s specific preferences. In this paper, we propose SR-HetGNN, a novel session recommendation method that uses a heterogeneous graph neural network (HetGNN) to learn session embeddings and capture the specific preferences of anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs containing various types of nodes according to the session sequence, which can capture the dependencies among items, users, and sessions. Second, HetGNN captures the complex transitions between items and learns the item embeddings containing user information. Finally, local and global session embeddings are combined with the attentional network to obtain the final session embedding, considering the influence of users’ long and short-term preferences. SR-HetGNN is shown to be superior to the existing state-of-the-art session-based recommendation methods through extensive experiments over two real large datasets Diginetica and Tmall. The code and datasets can be found in our GitHub repository https://github.com/LeeHY1996/SR-HetGNN .},
  archive      = {J_KIS},
  author       = {Chen, Jinpeng and Li, Haiyang and Zhang, Xudong and Zhang, Fan and Wang, Senzhang and Wei, Kaimin and Ji, Jiaqi},
  doi          = {10.1007/s10115-023-01986-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1111-1134},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SR-HetGNN: Session-based recommendation with heterogeneous graph neural network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online influence maximization under continuous independent
cascade model with node-edge-level feedback. <em>KIS</em>,
<em>66</em>(2), 1091–1110. (<a
href="https://doi.org/10.1007/s10115-023-01982-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online influence maximization problem in social networks. We concentrate on solving two challenges in this paper. First, we work with continuous independent cascade model instead of independent cascade model. In the independent cascade model, the influence diffusion is limited and imprecise because an activated node can only attempt to impact a neighbor through a directed edge for once. Therefore, we propose a continuous influence set and combine this set with the independent cascade model to build a continuous independent cascade model, which realizes multiple activation and can be more customized to attract more targeted users. Second, we devise node-edge-level feedback instead of node-level feedback. In the node-level feedback, though the combined influence is relatively easy to observe, the exact edge which cause the influence seldom reveals. Therefore, we use node-edge-level feedback to generate the source nodes which activate the active node. We improve the IMFB algorithm and propose the CIC_IMFB-NE algorithm. The CIC_IMFB-NE algorithm is more efficient than the existing online influence maximization algorithm. Our experiments demonstrate the excellence of CIC_IMFB-NE in terms of regret bound in real life.},
  archive      = {J_KIS},
  author       = {Liu, Chao and Xu, Haichao and Liu, Xiaoyang},
  doi          = {10.1007/s10115-023-01982-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1091-1110},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online influence maximization under continuous independent cascade model with node-edge-level feedback},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tuning parameters of apache spark with gauss–pareto-based
multi-objective optimization. <em>KIS</em>, <em>66</em>(2), 1065–1090.
(<a href="https://doi.org/10.1007/s10115-023-02032-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When there is a need to make an ultimate decision about the unique features of big data platforms, one should note that they have configurable parameters. Apache Spark is an open-source big data processing platform that can process real-time data, and it requires an advanced central processing unit and high memory capacity. Therefore, it gives us a great number of configurable parameters such as the number of cores and driver memory that are tuned during the execution. Different from the preceding works, in this study, a Kriging-based multi-objective optimization method is developed. Kriging-based means executing a surrogate model to create a response surface by providing a set of optimal solutions. The most important advantage of the proposed method over the alternatives is that it consists of three fitness functions. The method is evaluated on the MLlib library and the benchmarks of Hibench. MLlib provides various machine learning algorithms that are suitable to execute on resilient distributed data sets. The experimental results show that the proposed method outperformed the alternatives in hypervolume improvement and reducing uncertainty. Further, the results support the hypothesis that focusing on the parameters associated with data compression and memory usage improves the effectiveness of multi-objective optimization methods developed for Spark. Multi-objective optimization leads to an inevitable complexity in Spark due to the dimensionality of objective functions. Despite the fact that simplifying the setup and steps of optimization has proven to be the most effective way to reduce that complexity, it is not very effective to avoid ambiguity of the Pareto front. While the proposed method achieved 1.93x speedup in benchmark experiments, there is a remarkable difference (0.63 of speedup) between the speedup of our method and that of the closest competitor. Increasing the number of cores in multi-objective optimization does not contribute to speedup; rather, it leads to waste of CPU sources. Instead, the optimal number of cores should be determined by checking the changes of speedup with varying Spark configurations.},
  archive      = {J_KIS},
  author       = {Öztürk, M. Maruf},
  doi          = {10.1007/s10115-023-02032-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1065-1090},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tuning parameters of apache spark with Gauss–Pareto-based multi-objective optimization},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SS-WDRN: Sparrow search optimization-based weighted dual
recurrent network for software fault prediction. <em>KIS</em>,
<em>66</em>(2), 1037–1064. (<a
href="https://doi.org/10.1007/s10115-023-01932-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting software faults at the primary stage is a challenging role for software engineers and tech industries. During the development of software projects, it is necessary to predict the number of probable faults to have occurred on software rather than detecting whether the software modules are faulty or not. Discovering the number of expected faults helps software professionals to develop more reliable and high-quality software systems. However, the prediction model’s performance gets affected while dealing with complicated software projects with increased cost factors such as time, effort, and resources. Therefore, to address the issue associated with handling complex software projects, a novel weighted dual cross-recurrent network-based levy sparrow search (WDCRN-LSS) model is proposed in this paper. The WDCRN-LSS approach by learning the data features with optimal hyperparameters accurately predicts the expected software faults in an earlier phase. Here, 17 PROMISE datasets containing 20 features each are utilized as input data for the proposed WDCRN-LSS model. The data inconsistencies are eliminated and then transformed to a suitable format for training through normalization, data transformation, and label encoding procedures. The preprocessed data are then trained using the proposed WDCRN-LSS model for the prediction of the expected number of software faults in the projects. With the excellent learning capability of feature representations, the proposed WDCRN-LSS model predicts software faults on upcoming/under-development software projects precisely. Thus, the proposed WDCRN-LSS model enhances software quality and minimizes cost factors such as time, resources, and effort that are depleted in developing software. The proposed WDCRN-LSS model’s efficiency is investigated by utilizing evaluation measures namely error rate, precision, recall, F1-measure, the area under the curve, accuracy, and specificity. The experimental result manifests the efficiency of the proposed WDCRN-LSS model with a software fault detection accuracy of about 98.1%.},
  archive      = {J_KIS},
  author       = {Brundha Elci, J. and Nandagopalan, S.},
  doi          = {10.1007/s10115-023-01932-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1037-1064},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SS-WDRN: Sparrow search optimization-based weighted dual recurrent network for software fault prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying influential nodes based on new layer metrics and
layer weighting in multiplex networks. <em>KIS</em>, <em>66</em>(2),
1011–1035. (<a
href="https://doi.org/10.1007/s10115-023-01983-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying influential nodes in multiplex complex networks have a critical importance to implement in viral marketing and other real-world information diffusion applications. However, selecting suitable influential spreaders in multiplex networks are more complex due to existing multiple layers. Each layer of multiplex networks has its particular importance. Based on this research, an important layer with strong spreaders is a layer positioned in a well-connected neighborhood with more active edges, active critical nodes, the ratio of active nodes and their connections to all possible connections, and the intersection of intralayer communication compared to other layers. In this paper, we have formulated a layer weighting method based on mentioned layer’s parameters and proposed an algorithm for mapping and computing the rank of nodes based on their spreading capability in multiplex networks. Thus, the result of layer weighting is used in mapping and compressing centrality vector values to a scalar value for calculating the centrality of nodes in multiplex networks by a coupled set of equations. In addition, based on this new method, the important layer parameters are combined for the first time to utilize in computing the influence of nodes from different layers. Experimental results on both synthetic and real-world networks show that the proposed layer weighting and mapping method significantly is effective in detecting high influential spreaders against compared methods. These results validate the specific attention to suitable layer weighting measure for identifying potential spreaders in multiplex network.},
  archive      = {J_KIS},
  author       = {Bouyer, Asgarali and Mohammadi, Moslem and Arasteh, Bahman},
  doi          = {10.1007/s10115-023-01983-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1011-1035},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Identifying influential nodes based on new layer metrics and layer weighting in multiplex networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Directed dynamic attribute graph anomaly detection based on
evolved graph attention for blockchain. <em>KIS</em>, <em>66</em>(2),
989–1010. (<a href="https://doi.org/10.1007/s10115-023-02033-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is gradually becoming an important data storage platform for Internet digital copyright confirmation, electronic deposit, and data sharing. Anomaly detection on the blockchain has received extensive attention as the foundation for securing blockchain-based digital applications. However, the current blockchain anomaly detection for obtaining network nodes’ depth and dynamic change features still needs improvement. In this paper, we propose a public blockchain anomaly detection method based on evolved graph attention. Different from general blockchain network modeling methods, we first adopt a dynamic attribute graph network construction method to model each transaction using edges to provide more learnable transaction attribute information for graph representation learning in blockchain networks. Then, we propose an evoluted graph attention network structure to fully extract the deep features of blockchain nodes by learning the temporal evolution characteristics of blockchain networks and dynamically updating the node learning weights of subgraphs in different timestamps. In order to solve the dataset imbalance problem, we also apply the GraphSMOTE method for graph-structured data on public blockchain networks for the first time. Finally, we identify node labels in blockchain networks using a binary classification method and verify our proposed scheme through multiple rounds of experiments.},
  archive      = {J_KIS},
  author       = {Liu, Chenlei and Xu, Yuhua and Sun, Zhixin},
  doi          = {10.1007/s10115-023-02033-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {989-1010},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Directed dynamic attribute graph anomaly detection based on evolved graph attention for blockchain},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new ontology-based similarity approach for measuring
caching coverages provided by mediation systems. <em>KIS</em>,
<em>66</em>(2), 959–987. (<a
href="https://doi.org/10.1007/s10115-023-01974-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most mediation systems use a caching policy in order to overcome their performance challenges. One of the most widely adopted strategies is known as semantic caching. Semantic caches are called so because they store the descriptions of all submitted queries. Although they may seem to be based on semantics because of their name, this is not really the case. In fact, they actually compare the syntax of the cached queries to the syntax of the new query to retrieve responses from the cache. This can lead to significant delays, especially if multiple requests are stored in the cache. In this work, we propose a new semantic approach based on ontologies to compute the semantic similarity between two given queries, and we provide also a new algorithm to filter all regions of the cache that do not semantically cover a user query. In this way, the use of the cache would be optimal and fast at the same time, despite the large number of regions in the cache. In fact, only the most beneficial regions will be processed to retrieve data from the cache.},
  archive      = {J_KIS},
  author       = {Ajarroud, Ouafa and Zellou, Ahmed and Idri, Ali},
  doi          = {10.1007/s10115-023-01974-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {959-987},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new ontology-based similarity approach for measuring caching coverages provided by mediation systems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binarized spiking neural networks optimized with nomadic
people optimization-based sentiment analysis for social product
recommendation. <em>KIS</em>, <em>66</em>(2), 933–958. (<a
href="https://doi.org/10.1007/s10115-023-01956-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analytics is essential for many industries that use computing applications, like real-time purchasing and e-commerce. Big data is used to promote products and improve the communication among retailers and shoppers. At present, individuals frequently utilize online promotions to identify the best shops for purchasing higher-quality goods. This shopping experience shared on social media platforms can be used to observe the opinions regarding the shoppers shop. New customers search the shop for knowing information about manufacturing date (MRD), manufacturing price (MRP), offers, quality, and suggestions. All these information are provided only through previous customer experience. On the product cover or label, the MRP and MRD are already available. Numerous methods have been employed to predict the details of product, but none of them provides accurate details. To overcome these issues, binarized spiking neural networks optimized with Nomadic People Optimization-based sentiment analysis is proposed for social product recommendations (BSNN-NPO). The product–product (P–P) similarity and collaborative filtering (CF) techniques are used for modeling the new recommendation system. The P–P similarity approach predicts the best products, while CF method predicts the best shops. The product data along customer reviews is gathered through Amazon product recommendation. From the results and comparison, it is found that the proposed BSNN-NPO method outperforms than other approaches. The performance of proposed technique offers higher mean absolute percentage error 38.56%, 23.67%, and 30.22% and lower mean squared error 34.67%, 45.7%, and 15.21% compared to the existing models, respectively.},
  archive      = {J_KIS},
  author       = {Agarwal, Gaurav and Dinkar, Shail Kumar and Agarwal, Ajay},
  doi          = {10.1007/s10115-023-01956-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {933-958},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Binarized spiking neural networks optimized with nomadic people optimization-based sentiment analysis for social product recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big data analytics enabled deep convolutional neural network
for the diagnosis of cancer. <em>KIS</em>, <em>66</em>(2), 905–931. (<a
href="https://doi.org/10.1007/s10115-023-01971-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has been shown to be a formidable instrument in managing Big Healthcare Data, and it has seen considerable success in bioinformatics. The advancement of big data in biological sciences has given rise to big data analytics (BDA) and artificial intelligence (AI). Because the AI methodologies used in bioinformatics are parallel and iterative, scalable big data management employing distributed and parallel technology is possible. The growth of bioinformatics has resulted in significant storage and administration issues; to share information, such large amounts of data must be handled efficiently. Computational developments in information technology have enabled analytical systems to cope with such data. Therefore, this study emphasizes the impact of big data and BDA in bioinformatics. A practical use of BDAs and AI in cancer classification was given, combining a unique Analysis of Variance (ANOVA) approach with Ant Colony Optimization (ACO) as a hybrid feature selection to pick significant genes while minimizing gene redundancy. Deep Convolutional Neural Networks (DCNN) were employed to classify the datasets. It is because microarray data are produced from gene expression data, and it frequently has a limited number of samples but a huge feature collection size. Using the same datasets, the suggested system outperformed earlier state-of-the-art approaches. The results of the proposed model on all the Leukemia, DLBCL, Colon, and SRDCT datasets revealed an average classification accuracy of 97.7%, 99.9%, 99.9% and 100%, respectively.},
  archive      = {J_KIS},
  author       = {Awotunde, Joseph Bamidele and Panigrahi, Ranjit and Shukla, Shubham and Panda, Baidyanath and Bhoi, Akash Kumar},
  doi          = {10.1007/s10115-023-01971-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {905-931},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Big data analytics enabled deep convolutional neural network for the diagnosis of cancer},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed membership distribution-free model. <em>KIS</em>,
<em>66</em>(2), 879–904. (<a
href="https://doi.org/10.1007/s10115-023-02021-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. To model such complex networks, we propose a general framework—the mixed membership distribution-free (MMDF) model. MMDF has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. Especially, overlapping signed networks with latent community structures can also be generated from our model. We use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. We also propose the fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. We then provide a method to determine the number of communities for weighted networks by taking advantage of our fuzzy weighted modularity. Numerical simulations and real data applications are carried out to demonstrate the usefulness of our mixed membership distribution-free model and our fuzzy weighted modularity.},
  archive      = {J_KIS},
  author       = {Qing, Huan and Wang, Jingli},
  doi          = {10.1007/s10115-023-02021-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {879-904},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mixed membership distribution-free model},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilabel classification using crowdsourcing under budget
constraints. <em>KIS</em>, <em>66</em>(2), 841–877. (<a
href="https://doi.org/10.1007/s10115-023-01973-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel classification has excelled in several distinct fields during the past few decades but still has significant limitations. One of the critical concerns is the lack or insufficient availability of label instances, and data labelling also needs time and budget, which is a challenge. Crowdsourcing overcomes the problem of label availability, yet, it has drawbacks such as label quality and budget limitations. The paper introduced the multilabel reverse auction framework to address the lack of crowd worker&#39;s issue. Each crowd worker must provide cost and confidence for each task for a specific domain. Furthermore, two methods for systematic budget selection are presented to address the insufficient domain coverage within the budget limitation: Greedy bid selection and Multi cover bid selection. Both approaches choose the most inexpensive crowd workers while considering worker expertise and domain coverage. Crowd version binary relevance and multilabel k-nearest neighbours are also introduced to support label aggregation and reduce low-quality workers&#39; impact while considering the domain. An experimental study shows the effectiveness of our approach on seven multilabel datasets using diverse crowds. It delivers more than 16% improvement compared to the random selection with a majority voting baseline technique. The proposed method is compared against five benchmark algorithms and provides promising results when minimal availability of data and workers.},
  archive      = {J_KIS},
  author       = {Suyal, Himanshu and Singh, Avtar},
  doi          = {10.1007/s10115-023-01973-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {841-877},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multilabel classification using crowdsourcing under budget constraints},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualizations for universal deep-feature representations:
Survey and taxonomy. <em>KIS</em>, <em>66</em>(2), 811–840. (<a
href="https://doi.org/10.1007/s10115-023-01933-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data science and content-based retrieval, we find many domain-specific techniques that employ a data processing pipeline with two fundamental steps. First, data entities are represented by some visualizations, while in the second step, the visualizations are used with a machine learning model to extract deep features. Deep convolutional neural networks (DCNN) became the standard and reliable choice. The purpose of using DCNN is either a specific classification task or just a deep feature representation of visual data for additional processing (e.g., similarity search). Whereas the deep feature extraction is a domain-agnostic step in the pipeline (inference of an arbitrary visual input), the visualization design itself is domain-dependent and ad hoc for every use case. In this paper, we survey and analyze many instances of data visualizations used with deep learning models (mostly DCNN) for domain-specific tasks. Based on the analysis, we synthesize a taxonomy that provides a systematic overview of visualization techniques suitable for usage with the models. The aim of the taxonomy is to enable the future generalization of the visualization design process to become completely domain-agnostic, leading to the automation of the entire feature extraction pipeline. As the ultimate goal, such an automated pipeline could lead to universal deep feature data representations for content-based retrieval.},
  archive      = {J_KIS},
  author       = {Skopal, Tomáš and Peška, Ladislav and Hoksza, David and Sixtová, Ivana and Bernhauer, David},
  doi          = {10.1007/s10115-023-01933-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {811-840},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Visualizations for universal deep-feature representations: Survey and taxonomy},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review and evaluation of elastic distance functions for
time series clustering. <em>KIS</em>, <em>66</em>(2), 765–809. (<a
href="https://doi.org/10.1007/s10115-023-01952-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series clustering is the act of grouping time series data without recourse to a label. Algorithms that cluster time series can be classified into two groups: those that employ a time series specific distance measure and those that derive features from time series. Both approaches usually rely on traditional clustering algorithms such as k-means. Our focus is on partitional clustering algorithms that employ elastic distance measures, i.e. distances that perform some kind of realignment whilst measuring distance. We describe nine commonly used elastic distance measures and compare their performance with k-means and k-medoids clusterer. Our findings, based on experiments using the UCR time series archive, are surprising. We find that, generally, clustering with DTW distance is not better than using Euclidean distance and that distance measures that employ editing in conjunction with warping are significantly better than other approaches. We further observe that using k-medoids clusterer rather than k-means improves the clusterings for all nine elastic distance measures. One function, the move–split–merge (MSM) distance, is the best performing algorithm of this study, with time warp edit (TWE) distance a close second. Our conclusion is that MSM or TWE with k-medoids clusterer should be considered as a good alternative to DTW for clustering time series with elastic distance measures. We provide implementations, extensive results and guidance on reproducing results on the associated GitHub repository.},
  archive      = {J_KIS},
  author       = {Holder, Christopher and Middlehurst, Matthew and Bagnall, Anthony},
  doi          = {10.1007/s10115-023-01952-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {765-809},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review and evaluation of elastic distance functions for time series clustering},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the impact of drift detection mechanisms on stock
market forecasting. <em>KIS</em>, <em>66</em>(1), 723–763. (<a
href="https://doi.org/10.1007/s10115-023-02025-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stock market is an important segment of the economy that circulates a large volume of assets. Several factors may affect the stock market transactions, leading to fluctuations in the stock values that may pose a problem for those who seek to forecast future stock values and maximize their profits. This issue is more serious when the stock values present the concept drift phenomenon, which means that the stock value’s patterns change over time. In this work, we aimed to evaluate whether machine learning-based predictors that incorporate mechanisms to deal with concept drift are suitable for stock market forecasting. To do so, a historic database of stock prices of 10 companies, negotiated in the Brazilian stock exchange and collected over 20 years, was used. We compared the performance of predictors based on different paradigms, with and without mechanisms to deal with concept drift, and the results showed that, although the strategies that handle concept drift demand longer computational times, they also tend to present smaller prediction errors. The highlight was the EOS-D approach, which had the best performance in 6 of the 10 stocks analyzed considering one-to-one comparisons.},
  archive      = {J_KIS},
  author       = {Fernando Panicachi Cocovilo Filho, Luis and Palermo Coelho, Guilherme},
  doi          = {10.1007/s10115-023-02025-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {723-763},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluating the impact of drift detection mechanisms on stock market forecasting},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query-based denormalization using hypergraph (QBDNH): A
schema transformation model for migrating relational to NoSQL databases.
<em>KIS</em>, <em>66</em>(1), 681–722. (<a
href="https://doi.org/10.1007/s10115-023-02017-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of NoSQL databases, many large applications have migrated from relational databases (RDB) due to their superior flexibility and performance. Database migration from RDB to NoSQL databases involves schema transformation and data migration, which is not straightforward. The challenge lies in that RDB stores data in normalized form, whereas NoSQL supports denormalization. To address the challenge of schema transformation, this paper proposes a model called query-based denormalization using hypergraph (QBDNH) from RDB to the NoSQL database. The model takes the inputs from existing relational tables and queries and transforms them into the denormalized NoSQL model using hypergraphs. The approach overcomes limitations like complex relationship representation and data access pattern coverage of existing graph-based denormalization techniques. The proposed model reduces the overall time, cost, and effort needed to transform the schema manually. To validate the effectiveness of QBDNH, the experiments are conducted on the TPC-H dataset, and the performance of QBDNH is compared to existing graph-based denormalization models such as TLD, CLDA, and Kuszera. The evaluation is carried out in two parts: the first part analyzed the query speedup factor, while the second part measured efficiency improvement based on query pipeline execution. The results revealed that QBDNH achieved a notable query performance improvement with speedup factors of 1.29, 1.35, and 1.40 compared to existing TLD, CLDA, and Kuszera models. Furthermore, QBDNH significantly enhanced pipeline utilization compared to TLD and Kuszera.},
  archive      = {J_KIS},
  author       = {Bansal, Neha and Sachdeva, Shelly and Awasthi, Lalit K.},
  doi          = {10.1007/s10115-023-02017-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {681-722},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Query-based denormalization using hypergraph (QBDNH): A schema transformation model for migrating relational to NoSQL databases},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEHP: Stacking-based ensemble learning on novel features for
review helpfulness prediction. <em>KIS</em>, <em>66</em>(1), 653–679.
(<a href="https://doi.org/10.1007/s10115-023-02020-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The review’s helpfulness and its impact on purchase decisions are well established. This study presents a robust helpfulness prediction model for customer reviews. To this end, significant review textual features and newly defined reviewer characteristics are explored with a stacking-based ensemble model. More specifically, stylistic, time complexity, summary language, psychological, and linguistics features are introduced. According to our knowledge, these features are not explored earlier with the stacking-based ensemble model for review helpfulness prediction. The proposed predictive model is evaluated on three benchmark Amazon review datasets, consisting of 200,979 reviews in total. Two algorithms are proposed to help readers for understanding the methodology and researchers to regenerate the results. We compared several machine-learning, stacking-based ensemble, and 1-dimenional convolutional neural network (1D CNN) models. The stacking-based ensemble model shows benchmark performance by obtaining 0.009 mean square error with a hybrid combination of the proposed (reviewer and textual) features. Moreover, the proposed model outperformed five baselines including the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model by reducing mean square error by 40%. The results show that review textual features are better predictors than reviewer features as a standalone model. The findings of this article have significant implications for the researchers and the business owners.},
  archive      = {J_KIS},
  author       = {Malik, Muhammad Shahid Iqbal and Nawaz, Aftab},
  doi          = {10.1007/s10115-023-02020-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {653-679},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEHP: Stacking-based ensemble learning on novel features for review helpfulness prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced autoencoder-based fraud detection: A novel approach
with noise factor encoding and SMOTE. <em>KIS</em>, <em>66</em>(1),
635–652. (<a href="https://doi.org/10.1007/s10115-023-02016-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection is a critical task across various domains, requiring accurate identification of fraudulent activities within vast arrays of transactional data. The significant challenges in effectively detecting fraud stem from the inherent class imbalance between normal and fraudulent instances. To address this issue, we propose a novel approach that combines autoencoder-based noise factor encoding (NFE) with the synthetic minority oversampling technique (SMOTE). Our study evaluates the efficacy of this approach using three datasets with severe class imbalance. We compare three autoencoder variants—autoencoder (AE), variational autoencoder (VAE), and contractive autoencoder (CAE)—enhanced by the NFE technique. This technique involves training autoencoder models on real fraud data with an added noise factor during the encoding process, followed by combining this altered data with genuine fraud data. Subsequently, SMOTE is employed for oversampling. Through extensive experimentation, we assess various evaluation metrics. Our results demonstrate the superiority of the autoencoder-based NFE approach over the use of traditional oversampling methods like SMOTE alone. Specifically, the AE–NFE method outperforms other techniques in most cases, although the VAE–NFE and CAE–NFE methods also exhibit promising results in specific scenarios. This study highlights the effectiveness of leveraging autoencoder-based NFE and SMOTE for fraud detection. By addressing class imbalance and enhancing the performance of fraud detection models, our approach enables more accurate identification and prevention of fraudulent activities in real-world applications.},
  archive      = {J_KIS},
  author       = {Çakır, Mert Yılmaz and Şirin, Yahya},
  doi          = {10.1007/s10115-023-02016-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {635-652},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhanced autoencoder-based fraud detection: A novel approach with noise factor encoding and SMOTE},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term prediction of daily solar irradiance using
bayesian deep learning and climate simulation data. <em>KIS</em>,
<em>66</em>(1), 613–633. (<a
href="https://doi.org/10.1007/s10115-023-01955-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar Irradiance depicts the light energy produced by the Sun that hits the Earth. This energy is important for renewable energy generation and is intrinsically fluctuating. Forecasting solar irradiance is crucial for efficient solar energy generation and management. Work in the literature focused on the short-term prediction of solar irradiance, using meteorological data to forecast the irradiance for the next hours, days, or weeks. Facing climate change and the continuous increase in greenhouse gas emissions, particularly from the use of fossil fuels, the reliance on renewable energy sources, such as solar energy, is expanding. Consequently, governments and practitioners are calling for efficient long-term energy generation plans, which could enable 100% renewable-based electricity systems to match energy demand. In this paper, we aim to perform the long-term prediction of daily solar irradiance, by leveraging the downscaled climate simulations of Global Circulation Models (GCMs). We propose a novel Bayesian deep learning framework, named DeepSI (denoting Deep Solar Irradiance), that employs bidirectional long short-term memory autoencoders, prefixed to a transformer, with an uncertainty quantification component based on the Monte Carlo dropout sampling technique. We use DeepSI to predict daily solar irradiance for three different locations within the United States. These locations include the Solar Star power station in California, Medford in New Jersey, and Farmers Branch in Texas. Experimental results showcase the suitability of DeepSI for predicting daily solar irradiance from the simulated climate data, its superiority over related machine learning methods, and its ability to reproduce the daily variability. We further use DeepSI with future climate simulations to produce long-term projections of daily solar irradiance, up to year 2099.},
  archive      = {J_KIS},
  author       = {Gerges, Firas and Boufadel, Michel C. and Bou-Zeid, Elie and Nassif, Hani and Wang, Jason T. L.},
  doi          = {10.1007/s10115-023-01955-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {613-633},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Long-term prediction of daily solar irradiance using bayesian deep learning and climate simulation data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel image denoising algorithm combining attention
mechanism and residual UNet network. <em>KIS</em>, <em>66</em>(1),
581–611. (<a href="https://doi.org/10.1007/s10115-023-01965-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images are easily polluted by noise in the process of acquisition and transmission, which will affect people&#39;s understanding and utilization of knowledge and information in images. Therefore, image denoising, as a classic problem, has received extensive attention from researchers. At present, many image denoising methods based on deep learning have been proposed and achieved good performance. However, most existing methods are insufficient in acquiring and utilizing crucial information in the image when removing noise under complex image denoising tasks such as blind denoising and real-world denoising, resulting in the loss of fine details in the reconstructed image. To overcome this shortcoming, in this paper, we propose a novel image denoising algorithm combining attention mechanism and residual UNet network, named Att-ResUNet. Specifically, we propose a novel UNet-based image denoising framework, which employs residual enhancement blocks and skip connections to form global–local residuals, which can fuse multi-scale global context and local features to more thoroughly capture and remove hidden noise in the image. A channel attention mechanism is introduced, which can better focus on the crucial information in the image and improve the denoising performance. In addition, we use adaptive average pooling for down-sampling, which can preserve more image structure information, reduce the loss of edge details, and adopt a residual learning strategy to enhance the learning and expressive capabilities of the denoising model. Extensive experiments on several publicly available standard datasets demonstrate the superiority of our method over 15 state-of-the-art methods and achieve excellent denoising performance. Compared with mainstream methods, our method outperforms current state-of-the-art methods by up to 0.76 dB and 1.10 dB on PSNR evaluation metrics on BSD68 and Set12 datasets, respectively. Notably, our method achieves an average PSNR value of 37.88 dB on the CC dataset in real-world denoising experiments, a significant improvement of 2.14 dB over the most advanced methods.},
  archive      = {J_KIS},
  author       = {Ding, Shifei and Wang, Qidong and Guo, Lili and Zhang, Jian and Ding, Ling},
  doi          = {10.1007/s10115-023-01965-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {581-611},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel image denoising algorithm combining attention mechanism and residual UNet network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IFM-RCNN: A hybrid text classifier with enhanced performance
of binary drug classification from tweets using improved faster
mask-recurrent convolutional neural network. <em>KIS</em>,
<em>66</em>(1), 557–579. (<a
href="https://doi.org/10.1007/s10115-023-01957-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant growth of social media and online websites helps to garner a significant amount of healthcare data and contributes toward the development of the healthcare industry. Authorized health assets, which include specialized healthcare authorities, medical professionals, substandard monitoring and diagnosing equipment, and pharmaceuticals, became insufficient during the Coronavirus Disease (COVID-19) pandemic in 2019. Social networks have played a significant role in selling drugs during those times. However, the risk of drug abuse is increased, and also there occurs several challenges in the management and detection of drug activities. This research will focus on identifying relevant features in the drug database by developing a new target label model using deep learning approaches. This helps in avoiding unbalanced data, which in turn results in enhanced overall performance. The objective of this work is to address uneven classification performance by using social media comments as input in order to improve the accuracy of the training model. A new powerful hybrid machine learning method to extract clinical terms from customer feedback and learn the system by categorizing medical conditions and drug names is developed in this work. The Improved Faster Mask-Recurrent Convolutional Neural Network (IFM-RCNN), along with natural language processing, is trained with 4589 hand-labeled examples with 18,465 synthetically generated tweets. The suggested IFM-RCNN model attained an (area under the ROC curve) AUC of 0.98 and 0.94, a precision of 86.95% and an accuracy of 92.65%, respectively. The results show an enhanced capacity to anticipate drug activities with minimal differentiation. The proposed methodology has improved performance. It can be utilized for clinical applications, and the findings revealed that the suggested model can be applied in real-time applications.},
  archive      = {J_KIS},
  author       = {Lavanya, P. M. and Sasikala, E.},
  doi          = {10.1007/s10115-023-01957-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {557-579},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IFM-RCNN: A hybrid text classifier with enhanced performance of binary drug classification from tweets using improved faster mask-recurrent convolutional neural network},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trilingual conversational intent decoding for response
retrieval. <em>KIS</em>, <em>66</em>(1), 535–556. (<a
href="https://doi.org/10.1007/s10115-023-01972-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rich diversity of human language allows speakers to seamlessly transition between multiple languages during conversations. While humans have the remarkable ability to become proficient in multiple languages in a short period, developing machines that can converse in multiple natural languages with an understanding of diverse dialects requires sophisticated Natural Language Processing (NLP) techniques such as dialect recognition and intent extraction. This facilitates mutual understanding between parties who use phrases, sentences, words, or expressions from multiple languages within a single context. The work in this paper, propose a trilingual approach to multi-dialect conversation modeling within the same conversational session and context for a mix of English, Hindi–English text, Hindi–Devanagari text and Yoruba text. The model identifies the language used and determines the intent behind a query to respond in the same dialect. Our model is capable of detecting the end of a conversation, and it also detects the predominant dialect and responds accordingly in scenarios where a user’s input query contains a mix of languages. This approach is particularly useful in situations where there is limited data available for multilingual or trilingual conversation tasks based on Intent Detection (ID). We evaluate our proposed pipeline and model on three benchmark ID datasets and a trilingual dialogue dataset for response retrieval by intent decoding. Our model outperforms existing approaches in terms of performance metrics and has faster training time. Moreover, our trilingual approach to multi-dialect conversation modeling provides a versatile tool for efficient and effective inter-dialect conversational automation, even when dealing with large datasets, with minimal parameters and low resource overhead. The lightweight architectural pipeline and efficient algorithms used in our model contribute to its high performance and versatility.},
  archive      = {J_KIS},
  author       = {Godslove, Julius Femi and Nayak, Ajit Kumar},
  doi          = {10.1007/s10115-023-01972-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {535-556},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trilingual conversational intent decoding for response retrieval},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature importance measure of a multilayer perceptron based
on the presingle-connection layer. <em>KIS</em>, <em>66</em>(1),
511–533. (<a href="https://doi.org/10.1007/s10115-023-01959-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields, the interpretability of machine learning models holds equal importance to their prediction accuracy. Highly accurate predictions are possible with a multilayer perceptron (MLP) neural network, but its application in high-risk fields is constrained by its lack of interpretability. To solve this issue, this paper introduces an MLP with a presingle-connection layer (SMLP). The SMLP incorporates a single-to-single connection layer with the ReLU function before the original MLP. By examining the weights of the single-connection layer after training the model, the significance of the input features can be determined. The experimental results demonstrate that this method can accurately measure the feature importance with the MLP. It offers advantages such as a straightforward theory, practical implementation, strong stability, and high reliability when compared with other widely used feature importance algorithms. Moreover, this measure effectively reveals the black box of the MLP, indicates the influence of input features on the prediction, and provides a quantitative standard for feature selection in MLP.},
  archive      = {J_KIS},
  author       = {Zhang, Wenyi and Shen, Xiaohua and Zhang, Haoran and Yin, Zhaohui and Sun, Jiayu and Zhang, Xisheng and Zou, Lejun},
  doi          = {10.1007/s10115-023-01959-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {511-533},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature importance measure of a multilayer perceptron based on the presingle-connection layer},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Merkle tree-blockchain-assisted privacy preservation of
electronic medical records on offering medical data protection through
hybrid heuristic algorithm. <em>KIS</em>, <em>66</em>(1), 481–509. (<a
href="https://doi.org/10.1007/s10115-023-01937-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing number of medical records are changed by the electronic folders that can be shared and transmitted in real-time in recent years as a result of the speedy improvement of data mechanisms and network methodologies. Yet, security breaches and privacy issues could affect medical data sent over open communication channels. Due to its distinctive qualities including immutability, blockchain technology, anonymity, decentralization, and, verifiability has drawn more notice in various fields. With a variety of multimedia techniques, healthcare organizations all over the globe are evolving into more user-centered, harmonized, and effective models. The management of enormous amounts of information, like records and photographs of all the individual improve the person&#39;s labor requirements and the security hazards. Recent advances in the healthcare industry have led to the creation of enormous numbers of electronic health records (EHRs). The owner of the data can manage the EHR data and share thanks with certain individuals in the EHR system. It is challenging for information to guarantee protection and detection procedures due to the enormous amount of information in the medical care models. Hence, in order to secure the EHR, the development of artificial intelligence techniques like blockchain and a new privacy preservation model is recommended for this work. Particularly, the Merkle tree is a key component of blockchain technology. In this proposed model, the Merkle tree is applied that helps for effective and safe authentication of huge data frameworks with the purpose of verifiability of the posted patient data. This model consists of two main phases, such as data sanitization and data restoration. Here, the sanitization operation is based on the creation of the optimal key using a new iteration-based firefly reptile search algorithm (IFRSA). The creation of the optimal key is established by functioning a multi-objective function which includes the factors like Euclidean distance, hiding ratio, and, data preservation ratio among encrypted data using the original key and the key with a variation. Finally, the electronic health care model becomes more private, dependable, and helpful through our suggested system. The comparison shows the supremacy of the developed approach.},
  archive      = {J_KIS},
  author       = {Lakshmanan, M. and Anandha Mala, G. S.},
  doi          = {10.1007/s10115-023-01937-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {481-509},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Merkle tree-blockchain-assisted privacy preservation of electronic medical records on offering medical data protection through hybrid heuristic algorithm},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A template-based approach for question answering over
knowledge bases. <em>KIS</em>, <em>66</em>(1), 453–479. (<a
href="https://doi.org/10.1007/s10115-023-01966-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of answering complex questions formulated by users in natural language. Since traditional information retrieval systems are not suitable for complex questions, these questions are usually run over knowledge bases, such as Wikidata or DBpedia. We propose a semi-automatic approach for transforming a natural language question into a SPARQL query that can be easily processed over a knowledge base. The approach applies classification techniques to associate a natural language question with a proper query template from a set of predefined templates. The nature of our approach is semi-automatic as the query templates are manually written by human assessors, who are the experts of the knowledge bases, whereas the classification and query processing steps are completely automatic. Our experiments on the large-scale CSQA dataset for question-answering corroborate the effectiveness of our approach.},
  archive      = {J_KIS},
  author       = {Formica, Anna and Mele, Ida and Taglino, Francesco},
  doi          = {10.1007/s10115-023-01966-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {453-479},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A template-based approach for question answering over knowledge bases},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding stance classification of BERT models: An
attention-based framework. <em>KIS</em>, <em>66</em>(1), 419–451. (<a
href="https://doi.org/10.1007/s10115-023-01962-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BERT produces state-of-the-art solutions for many natural language processing tasks at the cost of interpretability. As works discuss the value of BERT’s attention weights to this purpose, we contribute to the field by examining this issue in the context of stance classification. We propose an interpretability framework to identify the most influential words for correctly predicting stances using BERT models. Unlike related work, we develop a broader level of interpretability focused on the overall model behaviour, aggregating tokens’ attentions into words’ attention weights that can be semantically related to the domain and proposing metrics to measure words relevance in correct predictions. We developed a broad experimental setting to analyse the premises underlying our framework regarding word attention scores and the capability concerning interpretability, adopting three case studies of stances expressed on Twitter on issues about the pandemic, and four pre-trained BERT models. We concluded that our method is not affected by the characteristics of BERT-models vocabularies, that words with high absolute attention have a higher probability of positive influence on correct classification, and that the influential words represent the domains. We observed many common words compared to a baseline method, but the words yielded by our method were considered more relevant according to a qualitative assessment.},
  archive      = {J_KIS},
  author       = {Sáenz, Carlos Abel Córdova and Becker, Karin},
  doi          = {10.1007/s10115-023-01962-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {419-451},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Understanding stance classification of BERT models: An attention-based framework},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressed and queryable self-indexes for RDF archives.
<em>KIS</em>, <em>66</em>(1), 381–417. (<a
href="https://doi.org/10.1007/s10115-023-01967-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF compression and querying are consolidated topics in the Web of Data, with a plethora of solutions to efficiently store and query static datasets. However, as RDF data changes along time, it becomes necessary to keep different versions of RDF datasets, in what is called an RDF archive. For large RDF datasets, naive techniques to store these versions lead to significant scalability problems. In this paper, we present v-RDF-SI, one of the first RDF archiving solutions that aim at joining both compression and fast querying. In v-RDF-SI, we extend existing RDF representations based on compact data structures to provide efficient support of version-based queries in compressed space. We present two implementations of v-RDF-SI, named v-RDFCSA and v-HDT, based, respectively, on RDFCSA (an RDF self-index) and HDT (a W3C-supported compressed RDF representation). We experimentally evaluate v-RDF-SI over a public benchmark named BEAR, showing that v-RDF-SI drastically reduces space requirements, being up to 40 times smaller than the baselines provided by BEAR, and 4 times smaller than alternatives based on compact data structures, while yielding significantly faster query times in most cases. On average, the fastest variants of v-RDF-SI outperform the alternatives by almost an order of magnitude.},
  archive      = {J_KIS},
  author       = {Cerdeira-Pena, Ana and de Bernardo, Guillermo and Fariña, Antonio and Fernández, Javier D. and Martínez-Prieto, Miguel A.},
  doi          = {10.1007/s10115-023-01967-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {381-417},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Compressed and queryable self-indexes for RDF archives},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalizing flows for conditional independence testing.
<em>KIS</em>, <em>66</em>(1), 357–380. (<a
href="https://doi.org/10.1007/s10115-023-01964-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting conditional independencies plays a key role in several statistical and machine learning tasks, especially in causal discovery algorithms, yet it remains a highly challenging problem due to dimensionality and complex relationships presented in data. In this study, we introduce LCIT (Latent representation-based Conditional Independence Test)—a novel method for conditional independence testing based on representation learning. Our main contribution involves a hypothesis testing framework in which to test for the independence between X and Y given Z, we first learn to infer the latent representations of target variables X and Y that contain no information about the conditioning variable Z. The latent variables are then investigated for any significant remaining dependencies, which can be performed using a conventional correlation test. Moreover, LCIT can also handle discrete and mixed-type data in general by converting discrete variables into the continuous domain via variational dequantization. The empirical evaluations show that LCIT outperforms several state-of-the-art baselines consistently under different evaluation metrics, and is able to adapt really well to both nonlinear, high-dimensional, and mixed data settings on a diverse collection of synthetic and real data sets.},
  archive      = {J_KIS},
  author       = {Duong, Bao and Nguyen, Thin},
  doi          = {10.1007/s10115-023-01964-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {357-380},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Normalizing flows for conditional independence testing},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing classification accuracy through feature
extraction: A comparative study of discretization and clustering
approaches on sensor-based datasets. <em>KIS</em>, <em>66</em>(1),
339–356. (<a href="https://doi.org/10.1007/s10115-023-01960-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy in a classification problem is directly related to the ability of features to adequately represent the differences between classes. In sensor-based datasets, measurements taken from the sensor form feature vectors. Measuring a given physical signal with different sensors enables it to be expressed with various feature vectors. For this reason, using sensor fusion is preferred in data acquisition. However, each new sensor added to the system brings problems such as complex sensory and supply circuit structures, extra energy consumption, signal sampling complexity, and time-consumption. On the other hand, in cases where sensor fusion cannot be applied, the ability of data from one sensor to represent classes may be insufficient. To avoid these problems, discretization and clustering approaches are suitable to derive more features from fewer sensors. The aim is to improve the accuracy of classifiers by deriving new feature vectors that can represent sensor data. This research reveals the contributions of clustering and discretization approaches as feature extraction methods to improve classification accuracy. In this study, three widely used machine learning techniques are investigated on Perfume, Wine, Seeds, and Gas datasets from the UCI repository. This comprehensive empirical study indicates that the accuracy of classifiers improves by up to 20% on datasets obtained from some sensors by using both discretization and clustering as feature-extracting methods.},
  archive      = {J_KIS},
  author       = {Esme, Engin},
  doi          = {10.1007/s10115-023-01960-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {339-356},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing classification accuracy through feature extraction: A comparative study of discretization and clustering approaches on sensor-based datasets},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A linear primal–dual multi-instance SVM for big data
classifications. <em>KIS</em>, <em>66</em>(1), 307–338. (<a
href="https://doi.org/10.1007/s10115-023-01961-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance learning (MIL) handles data that is organized into sets of instances known as bags. Traditionally, MIL is used in the supervised-learning setting for classifying bags which contain any number of instances. However, many traditional MIL algorithms do not scale efficiently to large datasets. In this paper, we present a novel primal–dual multi-instance support vector machine that can operate efficiently on large-scale data. Our method relies on an algorithm derived using a multi-block variation of the alternating direction method of multipliers. The approach presented in this work is able to scale to large-scale data since it avoids iteratively solving quadratic programming problems which are broadly used to optimize MIL algorithms based on SVMs. In addition, we improve our derivation to include an additional optimization designed to avoid solving a least-squares problem in our algorithm, which increases the utility of our approach to handle a large number of features as well as bags. Finally, we derive a kernel extension of our approach to learn nonlinear decision boundaries for enhanced classification capabilities. We apply our approach to both synthetic and real-world multi-instance datasets to illustrate the scalability, promising predictive performance, and interpretability of our proposed method.},
  archive      = {J_KIS},
  author       = {Brand, Lodewijk and Seo, Hoon and Baker, Lauren Zoe and Ellefsen, Carla and Sargent, Jackson and Wang, Hua},
  doi          = {10.1007/s10115-023-01961-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {307-338},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A linear primal–dual multi-instance SVM for big data classifications},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced ambient air quality prediction through weighted
feature selection and improved reptile search ensemble learning.
<em>KIS</em>, <em>66</em>(1), 267–305. (<a
href="https://doi.org/10.1007/s10115-023-01947-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution causes a pivotal impact throughout the world that affects natural resources. It also makes hazardous damage to the environment and defects in human health. The World Health Organization states that the report of air pollution is the major reason of human ailments such as lung cancer, early death, asthma, premature birth, and stroke. Due to the influence of changes in weather and climate caused by air pollution, global warming, acid rain, rainfall declines and depletion of the ozone layer occur. To mitigate these issues, preventive measures for air quality are prerequisites. Therefore, air quality monitoring is considered the main aspect of acquiring decision-making support that yields accurate predictions. In addition, there is a need of evaluating the quality of ambient (outdoor) air depending on the observations of pollutants. To achieve this, an automated air quality prediction model is proposed by using modified probability ratio-based RSA (MPR-RSA) and ensemble-based air quality prediction (EAQP). In the first step, the input data are undergone the preprocessing step. The preprocessing is done through various methods such as data imputation, data cleansing, and data transformation. Then, the preprocessed data are given to extract the significant features. The extracted features are obtained by statistical features, spatial features, and temporal features. To enhance the predictive accuracy, the weighted feature selection is employed, where the weight parameter is optimized by the proposed MPR-RSA algorithm. Then, the classification process is accomplished by EAQP, where the hyper-parameters are optimized by the same MPR-RSA algorithm. Here, the ensemble model is constructed by a single Prediction approach as support vector regression, recurrent neural network, extreme learning, bi-directional long short-term memory, and multi-layer perceptron neural network. Finally, the performance is analyzed with various parameters to prove that the proposed model becomes an advanced air quality prediction. Throughout the analysis, the RMSE of the proposed model achieves 9.96%, which can be a lesser value than the other existing heuristic algorithms. Hence, the proposed prediction model attains the low value of RMSE and MAE, which offers early forecasts of ambient air pollution to evade the damage and impacts to the environment.},
  archive      = {J_KIS},
  author       = {Lakshmipathy, M. and Prasad, M. J. Shanthi and Kodandaramaiah, G. N.},
  doi          = {10.1007/s10115-023-01947-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {267-305},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Advanced ambient air quality prediction through weighted feature selection and improved reptile search ensemble learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Property graph representation learning for node
classification. <em>KIS</em>, <em>66</em>(1), 237–265. (<a
href="https://doi.org/10.1007/s10115-023-01963-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning (graph embedding) has led to breakthrough results in various machine learning graph-based applications such as node classification, link prediction and recommendation. Many real-world graphs can be characterized as the property graphs, because besides the structure information, there exists rich property information related to each node in the graphs. Many existing graph representation learning methods—e.g. random walk-based methods like DeepWalk and Node2vec, focus only on the structure of graph for learning the node embedding. Although graph representation learning based on neural networks (e.g. typical GNN methods such as GraphSAGE) uses the property of nodes as the initial features of nodes and then aggregates feature information of the neighbours, their limitation is that the neighbourhood of a node is considered to be uniform—i.e. there is no way to differentiate among neighbours of a node when learning a node embedding. Additionally, their definition of neighbourhood is local, i.e. only nodes connected to the current node are considered as neighbours. Hence, those methods fail to capture implicit/latent relationships among nodes, which are implicit in the given structure. In this study, our aim is to improve the performance of graph representation learning methods on property graphs. We present a new framework called Enhanced Property Graph Embedding (EPGE)—a graph representation learning framework to address above-mentioned limitations. Our proposed framework relies on the notion of latent neighbourhood, as well as systematic sampling of neighbouring nodes to obtain better representation of the nodes. The experimental results on five publicly available graph datasets demonstrate that EPGE outperforms state-of-the-art baselines for the task of node classification. We further evaluate the superiority of our proposed formulation by defining a novel quantitative metric to measure the usefulness of the sampled neighbourhood in the graph.},
  archive      = {J_KIS},
  author       = {Li, Shu and Zaidi, Nayyar A. and Du, Meijie and Zhou, Zhou and Zhang, Hongfei and Li, Gang},
  doi          = {10.1007/s10115-023-01963-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {237-265},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Property graph representation learning for node classification},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A residual utility-based concept for high-utility itemset
mining. <em>KIS</em>, <em>66</em>(1), 211–235. (<a
href="https://doi.org/10.1007/s10115-023-01948-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge discovery in databases aims at finding useful information for decision-making. The problem of high-utility itemset mining (HUIM) has specifically garnered huge research attention, as it aims to find relevant information on patterns in a database, which conform to a user-defined utility function. The mined patterns are used for making data-backed decisions in the fields of healthcare, e-commerce, web analytics, etc. Various algorithms exist in the literature related to mining the high-utility items from the databases; however, most of them require multiple database scans, or deploy complex data structures. The utility-list is an efficient list-based data structure that is being widely adopted in the design of HUIM algorithms. The existing utility-list-based algorithms, however, suffer from some drawbacks like extensive use of inefficient join operations, multiple definitions of join operations, etc. Though the HUIM is an important research area, yet very little research has been directed towards improving the design of data structures used for the mining process. In this paper, we introduce the concept of residual utility to design two new data structures, called residue-map and master-map. Using these two data structures, a new algorithm, called R-Miner, is introduced for mining the high-utility items. In order to further optimise the mining process, the cumulative utility value is used as an upper bound and additional pruning conditions are also discussed. Several experiments are carried out on both real and synthetic datasets to compare the performance of R-Miner with the existing list-based algorithms. The experimental results show that the R-Miner improves the performance by up to the order of 2 as compared to the list-based algorithms: EFIM, H-Miner, HUI-Miner, FHM, and ULB-Miner.},
  archive      = {J_KIS},
  author       = {Sra, Pushp and Chand, Satish},
  doi          = {10.1007/s10115-023-01948-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {211-235},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A residual utility-based concept for high-utility itemset mining},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining parallel sentences from internet with multi-view
knowledge distillation for low-resource language pairs. <em>KIS</em>,
<em>66</em>(1), 187–209. (<a
href="https://doi.org/10.1007/s10115-023-01925-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural machine translation (NMT), which relies on a large training data (bilingual parallel sentences, for NMT) to obtain the state-of-the-art performance, is similar with deep learning. In order to construct NMT systems, the number of parallel sentences is very important. However, these bilingual resources are scarce for many low-resource language pairs. Although several works attempt to obtain bilingual parallel data from Internet, the quality and quantity of mined bilingual corpus are limited for low-resource language pairs. To address this problem, we propose the multi-view knowledge distillation model (MvKD) that use the knowledge of high-resource language pairs transfer into low-resource languages by leveraging internal language invariant in different languages. In particular, we treat the mining bilingual parallel sentence pair task as classifying task and use the multi-view classifier to detect bilingual parallel sentence pair. For multi-view classifier, we use two views to recognize the semantic difference of two sentences: (i) word-level representations and (ii) sentence-level representations. We encode sentence-level representations to capture semantically similar of two sentences. Moreover, we encode word-level representations to capture word translations in a pair of parallel sentences to avoid the problem that semantically similar but non-parallel sentences. Experimental results demonstrate that our proposed method can significantly mine amount of bilingual corpus and improve the quality of parallel sentences. In particular, we carry out the experiments on several real-world low-resource situations and achieve excellent results.},
  archive      = {J_KIS},
  author       = {Zhu, Shaolin and Gu, Shiwei and Li, Shangjie and Xu, Lin and Xiong, Deyi},
  doi          = {10.1007/s10115-023-01925-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {187-209},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining parallel sentences from internet with multi-view knowledge distillation for low-resource language pairs},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collab-RS: Semantic recommendation of external collaborators
for projects in software ecosystems. <em>KIS</em>, <em>66</em>(1),
147–186. (<a href="https://doi.org/10.1007/s10115-023-01954-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software development industry has evolved in recent years, presenting new challenges. In this scenario, software ecosystems have emerged as a new development paradigm through which external contributors support software production by providing solutions that complement a common ecosystem platform. Due to the many technologies, frameworks, and domains that an ecosystem can host, many collaborators acquainted with various domain topics and skills have also come into play. Recruiting collaborators becomes complex due to the varying degrees of knowledge and skills each collaborator has and their multiple competencies. There is a need to support the decision-making in the collaborator’s recruitment, using the knowledge related to their skills. This work presents a solution supported by an ontology capable of recommending external collaborators for specific projects. The solution encompasses an architecture based on semantic models and expertise retrieval techniques. The architecture scores the collaborators’ level of knowledge about topics and provides contextual information for the recommendation. Two studies were conducted involving two real software ecosystem platforms (Node.js and E-SECO). Results reveal that our approach can (i) use semantic models and inference mechanisms, (ii) offer context information essential for recruiter decision-making, and (iii) support recruiter’ decision on contributor selection.},
  archive      = {J_KIS},
  author       = {Oliveira, Márcio and Braga, Regina and Ghiotto, Gleiph and David, José Maria N. and Campos, Fernanda and Ströele, Victor},
  doi          = {10.1007/s10115-023-01954-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {147-186},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Collab-RS: Semantic recommendation of external collaborators for projects in software ecosystems},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hi-GNN: Hierarchical interactive graph neural networks for
auxiliary information-enhanced recommendation. <em>KIS</em>,
<em>66</em>(1), 115–145. (<a
href="https://doi.org/10.1007/s10115-023-01949-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networked auxiliary information (e.g., user social network, item transition network, etc.) plays a significant role to alleviate the sparse behavioral information (e.g., click, purchase, rating, etc.) in recent recommender systems, which promotes auxiliary information-enhanced recommendation (AIER) to be flourishing. However, existing studies on AIER-treated auxiliary information and behavioral information independently and ignored complex relationships between two types of information, which leads to suboptimal recommendation performance. Toward to this end, we propose hierarchical interactive graph neural networks, short for Hi-GNN, for AIER. Specifically, we firstly learn the behavioral information and the auxiliary information from user and item sides by recursively performing graph neural networks. And then, we design the hierarchical interaction layer to model the relative importance and the mutual association between the behavioral information and the auxiliary information, which furthermore improves performance of AIER by more rationally integrating networked auxiliary information. Experimental results on three real-world datasets demonstrate that Hi-GNN outperforms state-of-the-art methods on recommendation performance and has better resistance to sparse data.},
  archive      = {J_KIS},
  author       = {Zhang, Xiongtao and Gan, Mingxin},
  doi          = {10.1007/s10115-023-01949-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {115-145},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hi-GNN: Hierarchical interactive graph neural networks for auxiliary information-enhanced recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoAssign+: Automatic shared embedding assignment in
streaming recommendation. <em>KIS</em>, <em>66</em>(1), 89–113. (<a
href="https://doi.org/10.1007/s10115-023-01951-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of streaming recommender systems, conventional methods for addressing new user IDs or item IDs typically involve assigning initial ID embeddings randomly. However, this practice results in two practical challenges: (i) Items or users with limited interactive data may yield suboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs necessitates consistently expanding the embedding table, leading to unnecessary memory consumption. In light of these concerns, we introduce a reinforcement learning-driven framework, namely AutoAssign+, that facilitates Automatic Shared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an Identity Agent as an actor network, which plays a dual role: (i) representing low-frequency IDs field-wise with a small set of shared embeddings to enhance the embedding initialization and (ii) dynamically determining which ID features should be retained or eliminated in the embedding table. The policy of the agent is optimized with the guidance of a critic network. To evaluate the effectiveness of our approach, we perform extensive experiments on three commonly used benchmark datasets. Our experiment results demonstrate that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem. Furthermore, our framework yields a reduction in memory usage of approximately 20–30%, verifying its practical effectiveness and efficiency for streaming recommender systems.},
  archive      = {J_KIS},
  author       = {Liu, Ziru and Chen, Kecheng and Song, Fengyi and Chen, Bo and Zhao, Xiangyu and Guo, Huifeng and Tang, Ruiming},
  doi          = {10.1007/s10115-023-01951-1},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {89-113},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {AutoAssign+: Automatic shared embedding assignment in streaming recommendation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novelets: A new primitive that allows online detection of
emerging behaviors in time series. <em>KIS</em>, <em>66</em>(1), 59–87.
(<a href="https://doi.org/10.1007/s10115-023-01936-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the world’s data are time series. While offline exploration of time series can be useful, time series is almost unique in allowing the possibility of direct and immediate intervention. For example, if we are monitoring an industrial process and have an algorithm that predicts imminent failure, we could direct a controller to open a pressure release valve or initiate an evacuation plan. There is a plethora of tools to monitor time series for known behaviors (pattern matching), previously unknown highly conserved behaviors (motifs), evolving behaviors (chains) and unexpected behaviors (anomalies). In this work, we claim that there is another useful primitive, emerging behaviors that are worth monitoring for. We call such behaviors Novelets. We explain that Novelets are not anomalies, chains, or motifs but can be informally thought of as initially apparent anomalies that are later discovered to be motifs. We will show that Novelets have a natural interpretation in many disciplines, including science, medicine, and industry. As we will further demonstrate, Novelet discovery can have many downstream uses, including prognostics and abnormal behavior detection. We will demonstrate the utility of our proposed primitive on a diverse set of challenging domains.},
  archive      = {J_KIS},
  author       = {Mercer, Ryan and Keogh, Eamonn},
  doi          = {10.1007/s10115-023-01936-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {59-87},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Novelets: A new primitive that allows online detection of emerging behaviors in time series},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural architecture prediction. <em>KIS</em>,
<em>66</em>(1), 29–58. (<a
href="https://doi.org/10.1007/s10115-023-01968-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown their superiority in the modeling of graph data. Recently, increasing attention has been paid to automatic graph neural architecture search, aiming to overcome the shortcomings of manually constructing GNN architectures that requires a lot of expert experience. However, existing graph neural architecture search (GraphNAS) methods can only select architecture from the partial evaluated GNN architectures. To solve the challenges, we propose a Graph Neural Architecture Prediction (GraphNAP) framework, which can select the optimal GNN architecture from the search space efficiently. To achieve this goal, a neural predictor is designed in GraphNAP. Firstly, the neural predictor is trained by a small number of sampled GNN architectures. Then, the trained neural predictor is used to predict all GNN architectures in the search space. In this way, GraphNAP can efficiently explore the performance of all GNN architectures in the search space and then select the optimal GNN architecture. The experimental results show that GraphNAP outperforms state-of-the-art both handcrafted and GraphNAS-based methods for both graph and node classification tasks. The python implementation of GraphNAP can be found at https://github.com/BeObm/GraphNAP .},
  archive      = {J_KIS},
  author       = {Gao, Jianliang and Oloulade, Babatounde Moctard and Al-Sabri, Raeed and Chen, Jiamin and Lyu, Tengfei and Wu, zhenpeng},
  doi          = {10.1007/s10115-023-01968-6},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {29-58},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph neural architecture prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on preprocessing algorithm selection with
meta-learning. <em>KIS</em>, <em>66</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s10115-023-01970-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several AutoML tools aim to facilitate the usability of machine learning algorithms, automatically recommending algorithms using techniques such as meta-learning, grid search, and genetic programming. However, the preprocessing step is usually not well handled by those tools. Thus, in this work, we present a systematic review of preprocessing algorithms selection with meta-learning, aiming to find the state of the art in this field. To perform this task, we acquired 450 references, of which we selected 37 to be evaluated and analyzed according to a set of questions earlier defined. Thus, we managed to identify information such as what was published on the subject; the topics more often presented in those works; the most frequently recommended preprocessing algorithms; the most used features selected to extract information for the meta-learning; the machine learning algorithms employed as meta-learners and base-learners in those works; and the performance metrics that are chosen as the target of the applications.},
  archive      = {J_KIS},
  author       = {Pio, Pedro B. and Rivolli, Adriano and Carvalho, André C. P. L. F. de and Garcia, Luís P. F.},
  doi          = {10.1007/s10115-023-01970-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review on preprocessing algorithm selection with meta-learning},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
