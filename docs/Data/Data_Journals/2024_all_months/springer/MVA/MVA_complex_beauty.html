<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---138">MVA - 138</h2>
<ul>
<li><details>
<summary>
(2024). A novel key point based ROI segmentation and image
captioning using guidance information. <em>MVA</em>, <em>35</em>(6),
1–17. (<a href="https://doi.org/10.1007/s00138-024-01597-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, image captioning has become an intriguing task that has attracted many researchers. This paper proposes a novel keypoint-based segmentation algorithm for extracting regions of interest (ROI) and an image captioning model guided by this information to generate more accurate image captions. The Difference of Gaussian (DoG) is used to identify keypoints. A novel ROI segmentation algorithm then utilizes these keypoints to extract the ROI. Features of the ROI are extracted, and the text features of related images are merged into a common semantic space using canonical correlation analysis (CCA) to produce the guiding information. The text features are constructed using a Bag of Words (BoW) model. Based on the guiding information and the entire image features, an LSTM generates a caption for the image. The guiding information helps the LSTM focus on important semantic regions in the image to generate the most significant keywords in the image caption. Experiments on the Flickr8k dataset show that the proposed ROI segmentation algorithm accurately identifies the ROI, and the image captioning model with the guidance information outperforms state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Selvakani, Jothi Lakshmi and Ranganathan, Bhuvaneshwari and Palanisamy, Geetha},
  doi          = {10.1007/s00138-024-01597-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel key point based ROI segmentation and image captioning using guidance information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specular surface detection with deep static specular flow
and highlight. <em>MVA</em>, <em>35</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-024-01603-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To apply robot teaching to a factory with many mirror-polished parts, it is necessary to detect the specular surface accurately. Deep models for mirror detection have been studied by designing mirror-specific features, e.g., contextual contrast and similarity. However, mirror-polished parts such as plastic molds, tend to have complex shapes and ambiguous boundaries, and thus, existing mirror-specific deep features could not work well. To overcome the problem, we propose introducing attention maps based on the concept of static specular flow (SSF), condensed reflections of the surrounding scene, and specular highlight (SH), bright light spots, frequently appearing even in complex-shaped specular surfaces and applying them to deep model-based multi-level features. Then, we adaptively integrate approximated mirror maps generated by multi-level SSF, SH, and existing mirror detectors to detect complex specular surfaces. Through experiments with our original data sets with spherical mirrors and real-world plastic molds, we show the effectiveness of the proposed method.},
  archive      = {J_MVA},
  author       = {Hachiya, Hirotaka and Yoshimura, Yuto},
  doi          = {10.1007/s00138-024-01603-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Specular surface detection with deep static specular flow and highlight},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater image object detection based on multi-scale
feature fusion. <em>MVA</em>, <em>35</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-024-01606-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection and classification technology is one of the most important ways for humans to explore the oceans. However, existing methods are still insufficient in terms of accuracy and speed, and have poor detection performance for small objects such as fish. In this paper, we propose a multi-scale aggregation enhanced (MAE-FPN) object detection method based on the feature pyramid network, including the multi-scale convolutional calibration module (MCCM) and the feature calibration distribution module (FCDM). First, we design the MCCM module, which can adaptively extract feature information from objects at different scales. Then, we built the FCDM structure to make the multi-scale information fusion more appropriate and to alleviate the problem of missing features from small objects. Finally, we construct the Fish Segmentation and Detection (FSD) dataset by fusing multiple data augmentation methods, which enriches the data resources for underwater object detection and solves the problem of limited training resources for deep learning. We conduct experiments on FSD and public datasets, and the results show that the proposed MAE-FPN network significantly improves the detection performance of underwater objects, especially small objects.},
  archive      = {J_MVA},
  author       = {Yang, Chao and Zhang, Ce and Jiang, Longyu and Zhang, Xinwen},
  doi          = {10.1007/s00138-024-01606-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Underwater image object detection based on multi-scale feature fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Removing cloud shadows from ground-based solar imagery.
<em>MVA</em>, <em>35</em>(6), 1–8. (<a
href="https://doi.org/10.1007/s00138-024-01607-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study and prediction of space weather entails the analysis of solar images showing structures of the Sun’s atmosphere. When imaged from the Earth’s ground, images may be polluted by terrestrial clouds which hinder the detection of solar structures. We propose a new method to remove cloud shadows, based on a U-Net architecture, and compare classical supervision with conditional GAN. We evaluate our method on two different imaging modalities, using both real images and a new dataset of synthetic clouds. Quantitative assessments are obtained through image quality indices (RMSE, PSNR, SSIM, and FID). We demonstrate improved results with regards to the traditional cloud removal technique and a sparse coding baseline, on different cloud types and textures.},
  archive      = {J_MVA},
  author       = {Chaoui, Amal and Morgan, Jay Paul and Paiement, Adeline and Aboudarham, Jean},
  doi          = {10.1007/s00138-024-01607-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Removing cloud shadows from ground-based solar imagery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intercity rail platform abnormal action recognition based on
a skeleton tracking and recognition framework. <em>MVA</em>,
<em>35</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01608-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The skeleton data of limbs are unreliable due to occlusion and camera viewpoints on intercity railway platforms. It hinders the acquisition of skeleton sequences and disturbs the skeleton-based abnormal action recognition. To overcome these issues, this work proposes a framework consisting of a pose tracking module and an abnormal action recognition module. The proposed pose tracking module maintains the identities of multiple human poses across frames and provides skeleton sequences as input for recognition. Instead of utilizing the whole skeleton, the pose tracking method tracks the trunk for more stable results of identity association as the estimations of the limbs are unreliable. In addition, a position embedding graph convolutional network (PEGCN) is proposed to recognize abnormal actions. PEGCN utilizes a simple cosine encoding as position embeddings for enhancing the differentiation of skeleton vertices and an SElayer for extracting temporal dynamics. The pose tracking method achieves 66.42% tracking accuracy scores and higher frame rates than previous methods on the PoseTrack dataset. Additionally, PEGCN achieves competitive results on the Intercity Railway Action Dataset (IRAD) and the public NTU-RGB+D dataset.},
  archive      = {J_MVA},
  author       = {Dong, Jiaxun and Liu, Weiming and Zheng, Zhongxing and Xie, Wei and Wang, Liang and Mao, Liang and Qiu, Qisheng and Ling, Guangzheng},
  doi          = {10.1007/s00138-024-01608-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Intercity rail platform abnormal action recognition based on a skeleton tracking and recognition framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transgaze: Exploring plain vision transformers for gaze
estimation. <em>MVA</em>, <em>35</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01609-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, plain vision transformers (ViTs) have shown impressive performance in various computer vision tasks due to their powerful modeling capabilities and large-scale pre-training. However, they have yet to show excellent results in gaze estimation tasks. In this paper, we take the advanced Vision Transformers further into the task of Gaze Estimation (TransGaze). Our framework adeptly integrates the distinctive local features of the eyes while maintaining a simple and flexible structure. It can seamlessly adapt to various large-scale pre-trained models, enhancing its versatility and applicability in different contexts. It first demonstrates the pre-trained ViTs could also show strong capabilities on gaze estimation tasks. Our approach employs the following strategies: (i) Enhancing the self-attention module among facial feature maps through straightforward token manipulation, effectively achieving complex feature fusion, a feat previously requiring more intricate methods; (ii) Leveraging the plain of TransGaze and the inherent adaptability of Plain ViT, we introduce a pre-trained model for gaze estimation. This model reduces training time by over 50 % and exhibits strong generalization performance. We evaluate our TransGaze on GazeCapture and MPIIFaceGaze datasets and achieve state-of-the-art performance with less training costs. Our models and codes will be available.},
  archive      = {J_MVA},
  author       = {Ye, Lang and Wang, Xinggang and Yao, Jingfeng and Liu, Wenyu},
  doi          = {10.1007/s00138-024-01609-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Transgaze: Exploring plain vision transformers for gaze estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-efficiency automated triaxial robot grasping system for
motor rotors using 3D structured light sensor. <em>MVA</em>,
<em>35</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01610-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence and computer vision, numerous technologies have been introduced to automate manufacturing in the industry. Typical metal workpieces in the industry often have highly reflective surfaces, come in various sizes, and are positioned irregularly. The motor rotor presented in this paper is one such representative workpiece. Traditional grasping methods for workpiece loading and unloading are pre-programmed and often struggle to cope with complex and disordered situations. In this paper, we introduce a structured light (SL) sensor as the visual guide for the triaxial robot. Furthermore, we propose a high-precision hand-eye calibration method for the non-orthogonal coordinate system of the triaxial robot. Additionally, a motor rotor center localization method based on U-Net image segmentation is proposed. By combining the high-precision hand-eye calibration and localization, we can accurately and automatically locate and grasp the rotor. We have conducted sufficient experiments to verify the effectiveness and accuracy of our system.},
  archive      = {J_MVA},
  author       = {Liang, Jixin and Ye, Yuping and Wu, Di and Chen, Siyuan and Song, Zhan},
  doi          = {10.1007/s00138-024-01610-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {High-efficiency automated triaxial robot grasping system for motor rotors using 3D structured light sensor},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight real-time detection method of small objects
for home service robots. <em>MVA</em>, <em>35</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01611-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home service robots face a challenge in accurately and promptly detecting small objects in unstructured scenarios when the target is far from the robot vision system. Therefore, a lightweight small object detection algorithm is proposed based on the Single Shot Multi Box Detector and the lightweight feature extraction unit in MobileNetV3. In this study, a lightweight feature fusion unit is proposed based on Cross Stage Partial and Channel Shuffle. Inspired by the Path Aggregation Network, a lightweight feature fusion module is designed to improve the small object detection ability. Besides, using the proposed lightweight feature fusion unit, only one feature fusion is performed on each feature map in the lightweight feature fusion module, which effectively reduces the Params. Then, based on the object detection task, a one-shot Neural Architecture Search method and a new search space setting method are designed to further simplify the backbone and improve detection precision. The experimental results show that the proposed lightweight method achieves a high-precision real-time detection of small objects on the robot, with only 0.207M Params and 0.211G FLOPs.},
  archive      = {J_MVA},
  author       = {Zhang, Tie and Zhao, Fangyi and Zou, Yanbiao and Zheng, Jingfu},
  doi          = {10.1007/s00138-024-01611-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A lightweight real-time detection method of small objects for home service robots},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modality person re-identification via modality-synergy
alignment learning. <em>MVA</em>, <em>35</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01612-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification aims to match the identity of the same person from different modalities. The main challenge is the modality difference between visible and infrared images. Most existing methods mainly use generative adversarial networks to generate compensatory images of the corresponding modality to reduce the modality difference, or design diverse two-stream networks to learn global feature representations and extract globally shared features. However, due to the substantial difference between visible and infrared modalities, the created pseudo-modalities often struggle to effectively bridge the gap between modalities and tend to introduce noise. The extracted modality-shared features typically exhibit weak discriminative capability, inevitably leading to the loss of critical discriminative features related to person identity and a lack of robustness to noisy images. To tackle these challenges, we introduce a modality synergy alignment learning network. This network incorporates a novel data augmentation technique known as SliceMix, which mixes random sections of cross-modality images to synthesize a new sample that exhibits both discriminative to identity and robust to noise, thereby facilitating the learning of modality-invariant feature representations. By adjusting the mixing ratio, mixed modalities can be generated flexibly to minimize the impact of modality imbalance. Additionally, a modality alignment module is introduced to ensure similarity within the modality class and accentuate the differences between modalities. Moreover, we propose a data augmentation method called random channel grayscale, which enhances the network’s robustness to color changes and expands data diversity. Comprehensive experiments on mainstream datasets, including SYSU-MM01 and RegDB, demonstrated that our method significantly improves the performance of cross-modality retrieval.},
  archive      = {J_MVA},
  author       = {Lin, Yuju and Wang, Banghai},
  doi          = {10.1007/s00138-024-01612-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cross-modality person re-identification via modality-synergy alignment learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain disentanglement and contrastive learning with
source-guided sampling for unsupervised domain adaptation person
re-identification. <em>MVA</em>, <em>35</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s00138-024-01613-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fully supervised Person re-id methods have already been well developed. Still, they cannot be easily applied to real-life applications because of the domain gap between real-world databases and training datasets. And annotating ground truth label for the entire surveillance system with multiple cameras and videos are labor-intensive and impracticable in the real application. Besides, as the awareness of the right to privacy is rising, it becomes more challenging to collect sufficient training data from the public. Thence, the difficulty of constructing a new dataset for deployment not only arises from the labor cost of labeling but also because the raw data from the public are hard to come by. To be better adapted to real-life system deployment, we proposed an unsupervised domain adaptation based method, which involves Domain Disentanglement Network and Source-Guided Contrastive learning (SGCL). DD-Net first narrows down the domain gap between two datasets, and then SGCL utilizes the labeled source dataset as the clue to guide the training on the target domain. With these two modules, the knowledge transfer can be completed successfully from the training dataset to real-world scenarios. The conducted experiment shows that the proposed method is competitive with the state-of-the-art methods on two public datasets and even outperforms them under the setting of the small-scale target dataset. Therefore, not only the Person Re-ID, but also the object tracking in video or surveillance system can benefit from our new approach when we went to deploy to different environments.},
  archive      = {J_MVA},
  author       = {Wu, Cheng-Hsuan and Liu, An-Sheng and Chen, Chiung-Tao and Fu, Li-Chen},
  doi          = {10.1007/s00138-024-01613-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Domain disentanglement and contrastive learning with source-guided sampling for unsupervised domain adaptation person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A collaborative SLAM method for dual payload-carrying UAVs
in denied environments. <em>MVA</em>, <em>35</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01614-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of collaborative localization in search tasks in denied environments, particularly when traditional visual-inertial localization techniques reach their limits. A novel fusion localization method is proposed. It couples dual Payload-Carrying unmanned aerial vehicles (UAVs) using collaborative simultaneous localization and mapping (SLAM) techniques. This method aims to improve the system&#39;s search range and payload capacity. The paper utilizes SLAM technology to achieve self-motion estimation, reducing dependence on external devices. It incorporates a collaborative SLAM backend that provides the necessary information for system navigation, path planning, and motion control, ensuring consistent localization coordinates among the UAVs. Then, a joint localization optimization method based on Kalman filtering is introduced. By fusing the localization information from the visual sensors located beneath the UAVs and using the baseline variation between the 2 UAVs as a reference, the method employs a recursive prediction approach to jointly optimize the self-estimated states and the collaborative SLAM state estimates. Experimental validation demonstrates a 31.6% improvement in localization accuracy in complex tasks compared to non-fusion localization method. Furthermore, to address the cooperative trajectory tracking problem of UAVs after system path planning, a baseline-predicting fuzzy Proportional-Integral-Derivative flight controller is designed. Compared to conventional methods, this controller takes into account delays and system oscillations, achieving better tracking performance and dynamic adjustments.},
  archive      = {J_MVA},
  author       = {Rao, Jinjun and Liu, Nengwei and Chen, Jinbo and Liu, Mei and Lei, Jingtao and Giernacki, Wojciech},
  doi          = {10.1007/s00138-024-01614-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A collaborative SLAM method for dual payload-carrying UAVs in denied environments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ensemble approach for accelerated and noise-resilient
parallel MRI reconstruction utilizing CycleGANs. <em>MVA</em>,
<em>35</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01617-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is often constrained by long acquisition times. Accelerated acquisition techniques can reduce scan time but may introduce artifacts and decrease image resolution. Additionally, MRI data invariably contains noise (often Gaussian or Rician), further impacting image quality and diagnostic value. Conventional approaches often address noise and undersampling artifacts separately, leading to suboptimal image reconstruction. This work proposes a novel ensemble of CycleGAN models integrated within the Joint Sensitivity Encoding (JSENSE) framework to address these challenges in a comprehensive manner. Each CycleGAN within the ensemble is specifically trained to target distinct aspects of image degradation. This approach leverages the complementary strengths of the models to achieve a superior image reconstruction. Experiments demonstrate the effectiveness of the proposed ensemble model, outperforming conventional methods and our prior work in terms of visual quality and quantitative metrics. Significant gains were observed at higher acceleration factors and greater noise levels. The integration of CycleGANs with JSENSE yielded superior structural similarity and image clarity compared to traditional parallel MRI (pMRI) and single-model methods.},
  archive      = {J_MVA},
  author       = {Saju, Gulfam Ahmed and Okinaka, Alan and Akhi, Marjan and Chang, Yuchou},
  doi          = {10.1007/s00138-024-01617-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An ensemble approach for accelerated and noise-resilient parallel MRI reconstruction utilizing CycleGANs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Axes-aligned non-linear optimized PnP algorithm.
<em>MVA</em>, <em>35</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01618-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose estimation is an important component of many real-world computer vision systems. Most existing pose estimation algorithms need a large number of point correspondences to accurately determine the pose of an object. Since the number of point correspondences depends on the object’s appearance, lighting and other external conditions, detecting many points may not be feasible. In many real-world applications, the movement of objects is limited, e.g. due to gravity. Hence, detecting objects with only three degrees of freedom is usually sufficient. This allows us to improve the accuracy of pose estimation by changing the underlying equations of the perspective-n-point problem to three variables instead of six. By using the simplified equations, our algorithm is more robust against detection errors with limited point correspondences. In this article, we study three scenarios where such constraints apply. The first one is about parking a vehicle on a specific spot. Here, a stationary camera is detecting the vehicle to assist the driver. The second scenario describes the perspective of a moving camera detecting objects in its environment. This scenario is common for driver assistance systems, autonomous cars or mobile robots. Third, we describe a camera observing objects from a birds-eye view, which occurs in industrial applications. In all three scenarios, observed objects can only move in the ground plane and rotate around the vertical axis. Hence, three degrees of freedom are sufficient to estimate the pose. Experiments with synthetic data and real-world photographs have shown that our algorithm outperforms state-of-the-art pose estimation algorithms. Depending on the scenario, our algorithm is able to achieve 50% better accuracy, while being equally fast.},
  archive      = {J_MVA},
  author       = {Roch, Peter and Shahbaz Nejad, Bijan and Handte, Marcus and Marrón, Pedro José},
  doi          = {10.1007/s00138-024-01618-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Axes-aligned non-linear optimized PnP algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimize multiscale feature hybrid-net deep learning
approach used for automatic pancreas image segmentation. <em>MVA</em>,
<em>35</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01619-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and clean pancreatic segmentation problem is a significant and difficult challenge in the research of medical imaging. The proposed approach is helpful in Bottom-Up techniques for quickly and accurately identifying pancreatic cancer-affected areas in the kidney and pancreas. The proposed approach is employed for multiple organ detection with fine outliers from Computed Tomography scan images with sharp precision. In the proposed approach, 19 layers are used with 4 convolution layers. The optimized multiscale feature hybrid block is utilized. The convolution layer incorporates the small feature block with a similar or different size of kernel block. The dense convolution is divided into sub-convolution layers with their own &quot;Batch normalization&quot; layer. The convolution operation is performed on sub-convolution layers with the &quot;Swish activation&quot; function. The aggregates function only uses features that are necessary, causing unused data to be dropped at the convolution layer. The Validation Accuracy(98.82% ), precision value (84.14±19.03) Training global accuracy (95.486%) values are improvised as compared to State-of-Art. The proposed approach achieved a dice similarity index score upto 90.29±84.34%. During testing, the segmentation of medical images by the proposed approach, takes 1 to 2 s. In deep learning approaches, many convolution layers are added to the model to achieve high accuracy but training as well as unnecessary computational time also increases. The proposed approach overcomes this drawback with an optimized convolution layer number. The proposed approach is considering the only hybrid sub-convolution layer with required multiscale feature blocks only.},
  archive      = {J_MVA},
  author       = {Paithane, Pradip},
  doi          = {10.1007/s00138-024-01619-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Optimize multiscale feature hybrid-net deep learning approach used for automatic pancreas image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slope-embedded ViT-based model for lane line detection under
occlusions. <em>MVA</em>, <em>35</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01621-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based lane line detection has garnered substantial success in common scenarios. However, detecting lane lines under conditions of severe occlusion, where visual cues are largely absent, remains a considerable challenge. To address this issue, we propose a cutting-edge strategy that utilizes an enhanced Vision Transformer (ViT) for the de-occlusion of lane lines. Our approach significantly improves the accuracy of lane line detection by integrating a fused feature map with prior knowledge. Specifically, we refine the ViT model by employing overlapping patches technology to reconstruct occluded lane lines from the input image. Subsequently, we extract the feature maps from the model and integrate them with slope and category information pertaining to the lane lines, facilitating more robust and accurate lane line detection. Additionally, we introduce an innovative sensitivity loss function that evaluates not only pixel value errors but also spatial discrepancies between pixels. We assessed our strategy on three benchmark datasets: TuSimple, CULane, and CurveLanes. Our results demonstrate that our approach outperforms existing methods in terms of accuracy and F1-score on all these datasets.},
  archive      = {J_MVA},
  author       = {Su, Yang and Shi, Xianrang and Wang, Rong and Zhang, Hengyu and Li, Zezhi and Ti, Yan and Song, Tinglun and Puig, Vicenc},
  doi          = {10.1007/s00138-024-01621-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Slope-embedded ViT-based model for lane line detection under occlusions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised metric learning incorporating weighted
triplet constraint and riemannian manifold optimization for
classification. <em>MVA</em>, <em>35</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01581-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning focuses on finding similarities between data and aims to enlarge the distance between the samples with different labels. This work proposes a semi-supervised metric learning method based on the point-to-class structure of the labeled data, which is computationally less expensive, especially than using point-to-point structure. Specifically, the point-to-class structure is formulated into a new triplet constraint, which could narrow the distance of inner-class data and enlarge the distance of inter-class data simultaneously. Moreover, for measuring dissimilarity between different classes, weights are introduced into the triplet constraint and forms the weighted triplet constraint. Then, two kinds of regularizers such as spatial regularizer are rationally incorporated respectively in this model to mitigate the overfitting phenomenon and preserve the topological structure of the data. Furthermore, Riemannian gradient descent algorithm is adopted to solve the proposed model, since it can fully exploit the geometric structure of Riemannian manifolds and the proposed model can be regarded as a generalization of the unconstrained optimization problem in Euclidean space on Riemannian manifold. By introducing such solution strategy, the variables are constrained to a specific Riemannian manifold in each step of the iterative solution process, thereby enabling efficient and accurate model resolution. Finally, we conduct classification experiments on various data sets and compare the classification performance to state-of-the-art methods. The experimental results demonstrate that our proposed method has better performance in classification, especially for hyperspectral image data.},
  archive      = {J_MVA},
  author       = {Xia, Yizhe and Zhang, Hongjuan},
  doi          = {10.1007/s00138-024-01581-9},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semi-supervised metric learning incorporating weighted triplet constraint and riemannian manifold optimization for classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An insect vision-inspired neuromorphic vision systems in
low-light obstacle avoidance for intelligent vehicles. <em>MVA</em>,
<em>35</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01582-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lobular Giant Motion Detector (LGMD) is a neuron in the insect visual system that has been extensively studied, especially in locusts. This neuron is highly sensitive to rapidly approaching objects, allowing insects to react quickly to avoid potential threats such as approaching predators or obstacles. In the realm of intelligent vehicles, due to the lack of performance of conventional RGB cameras in extreme light conditions or at high-speed movements. Inspired by biological mechanisms, we have developed a novel neuromorphic dynamic vision sensor (DVS) driven LGMD spiking neural network (SNN) model. SNNs, distinguished by their bio-inspired spiking dynamics, offer a unique advantage in processing time-varying visual data, particularly in scenarios where rapid response and energy efficiency are paramount. Our model incorporates two distinct types of Leaky Integrate-and-Fire (LIF) neuron models and synapse models, which have been instrumental in reducing network latency and enhancing the system’s reaction speed. And addressing the challenge of noise in event streams, we have implemented denoising techniques to ensure the integrity of the input data. Integrating the proposed methods, ultimately, the model was integrated into an intelligent vehicle to conduct real-time obstacle avoidance testing in response to looming objects in simulated real scenarios. The experimental results show that the model’s ability to compensate for the limitations of traditional RGB cameras in detecting looming targets in the dark, and can detect looming targets and implement effective obstacle avoidance in complex and diverse dark environments.},
  archive      = {J_MVA},
  author       = {Wang, Haiyang and Wang, Songwei and Qian, Longlong},
  doi          = {10.1007/s00138-024-01582-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An insect vision-inspired neuromorphic vision systems in low-light obstacle avoidance for intelligent vehicles},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fourier feature network for 3D vessel reconstruction from
biplane angiograms. <em>MVA</em>, <em>35</em>(5), 1–8. (<a
href="https://doi.org/10.1007/s00138-024-01585-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction of biplane cerebral angiograms remains a challenging, unsolved research problem due to the loss of depth information and the unknown pixelwise correlation between input images. The occlusions arising from only two views complicate the reconstruction of fine vessel details and the simultaneous addressing of inherent missing information. In this paper, we take an incremental step toward solving this problem by reconstructing the corresponding 2D slice of the cerebral angiogram using biplane 1D image data. We developed a coordinate-based neural network that encodes the 1D image data along with a deterministic Fourier feature mapping from a given input point, resulting in a slice reconstruction that is more spatially accurate. Using only one 1D row of biplane image data, our Fourier feature network reconstructed the corresponding volume slices with a peak signal-to-noise ratio (PSNR) of 26.32 ± 0.36, a structural similarity index measure (SSIM) of 61.38 ± 1.79, a mean squared error (MSE) of 0.0023 ± 0.0002, and a mean absolute error (MAE) of 0.0364 ± 0.0029. Our research has implications for future work aimed at improving backprojection-based reconstruction by first examining individual slices from 1D information as a prerequisite.},
  archive      = {J_MVA},
  author       = {Wu, Sean and Kaneko, Naoki and Liebeskind, David S. and Scalzo, Fabien},
  doi          = {10.1007/s00138-024-01585-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fourier feature network for 3D vessel reconstruction from biplane angiograms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dyna-MSDepth: Multi-scale self-supervised monocular depth
estimation network for visual SLAM in dynamic scenes. <em>MVA</em>,
<em>35</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01586-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular Simultaneous Localization And Mapping (SLAM) suffers from scale drift, leading to tracking failure due to scale ambiguity. Deep learning has significantly advanced self-supervised monocular depth estimation, enabling scale drift reduction. Nonetheless, current self-supervised learning approaches fail to provide scale-consistent depth maps, estimate depth in dynamic environments, or perceive multi-scale information. In response to these limitations, this paper proposes Dyna-MSDepth, a novel method for estimating multi-scale, stable, and reliable depth maps in dynamic environments. Dyna-MSDepth incorporates multi-scale high-order spatial semantic interaction into self-supervised training. This integration enhances the model’s capacity to discern intricate texture nuances and distant depth cues. Dyna-MSDepth is evaluated on challenging dynamic datasets, including KITTI, TUM, BONN, and DDAD, employing rigorous qualitative evaluations and quantitative experiments. Furthermore, the accuracy of the depth maps estimated by Dyna-MSDepth is assessed in monocular SLAM. Extensive experiments confirm the superior multi-scale depth estimation capabilities of Dyna-MSDepth, highlighting its significant value in dynamic environments. Code is available at https://github.com/Pepper-FlavoredChewingGum/Dyna-MSDepth .},
  archive      = {J_MVA},
  author       = {Yao, Jianjun and Li, Yingzhao and Li, Jiajia},
  doi          = {10.1007/s00138-024-01586-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dyna-MSDepth: Multi-scale self-supervised monocular depth estimation network for visual SLAM in dynamic scenes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised collaborative localization learning method
for sewer pipe defect detection. <em>MVA</em>, <em>35</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01587-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term corrosion and external disturbances can lead to defects in sewer pipes, which threaten important parts of urban infrastructure. The automatic defect detection algorithm based on closed-circuit televisions (CCTV) has gradually matured using supervised deep learning. However, there are different types and sizes of sewer pipe defects, and relying on human inspection to detect defects is time-consuming and subjective. Therefore, a few-shot, accurate and automatic method for sewer pipe defect with localization and fine-grained classification is needed. Thus, this study constructs a few-shot image-level dataset of 15 categories using the sewer dataset ML-Sewer and then presents a collaborative localization network based on weakly supervised learning to automatically classify and detect defects. Specifically, an attention refinement module (ARM) is designed to obtain classification results and high-level semantic features. Furthermore, considering the correlation between target regions and the extraction of target edge information, we designed a collaborative localization module (CLM) consisting of two branches. Then, to ensure that the network focuses on the complete target area, this study applies an image iteration module (IIM). Finally, the results of the two branches in the CLM are fused to acquire target localization. The experimental results show that the proposed model exhibits favorable performance in detecting sewer pipe defects. The proposed method exhibits prediction classification accuracy that reaches 69.76 $$\%$$ and a positioning accuracy rate that reaches 65.32 $$\%$$ , which is higher than the performances of other weakly supervised detection models in sewer pipe defect detection.},
  archive      = {J_MVA},
  author       = {Yang, Yang and Yang, Shangqin and Zhao, Qi and Cao, Honghui and Peng, Xinjie},
  doi          = {10.1007/s00138-024-01587-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Weakly supervised collaborative localization learning method for sewer pipe defect detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boundary enhancement and refinement network for camouflaged
object detection. <em>MVA</em>, <em>35</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01588-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection aims to locate and segment objects accurately that conceal themselves well in the environment. Despite the advancements in deep learning methods, prevalent issues persist, including coarse boundary identification in complex scenes and the ineffective integration of multi-source features. To this end, we propose a novel boundary enhancement and refinement network named BERNet, which mainly consists of three modules for enhancing and refining boundary information: an asymmetric edge module (AEM) with multi-groups dilated convolution block (GDCB), a residual mixed pooling enhanced module (RPEM), and a multivariate information interaction refiner module (M2IRM). AEM with GDCB is designed to obtain rich boundary clues, where different dilation rates are used to expand the receptive field. RPEM is capable of enhancing boundary features under the guidance of boundary cues to improve the detection accuracy of small and multiple camouflaged objects. M2IRM is introduced to refine the side-out prediction maps progressively under the supervision of the ground truth by the fusion of multi-source information. Comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our BERNet with competitive state-of-the-art methods under the most evaluation metrics.},
  archive      = {J_MVA},
  author       = {Xia, Chenxing and Cao, Huizhen and Gao, Xiuju and Ge, Bin and Li, Kuan-Ching and Fang, Xianjin and Zhang, Yan and Liang, Xingzhu},
  doi          = {10.1007/s00138-024-01588-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boundary enhancement and refinement network for camouflaged object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFFAE-net: Semantic segmentation of point clouds using
multi-scale feature fusion and attention enhancement networks.
<em>MVA</em>, <em>35</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01589-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud data can reflect more information about the real 3D space, which has gained increasing attention in computer vision field. But the unstructured and unordered nature of point clouds poses many challenges in their study. How to learn the global features of the point cloud in the original point cloud is a problem that has been accompanied by the research. In the research based on the structure of the encoder and decoder, many researchers focus on designing the encoder to better extract features, and do not further explore more globally representative features according to the features of the encoder and decoder. To solve this problem, we propose the MFFAE-Net method, which aims to obtain more globally representative point cloud features by using the feature learning of encoder decoder stage.Our method first enhances the feature information of the input point cloud by merging the information of its neighboring points, which is helpful for the following point cloud feature extraction work. Secondly, the channel attention module is used to further process the extracted features, so as to highlight the role of important channels in the features. Finally, we fuse features of different scales from encoding features and decoding features as well as features of the same scale, so as to obtain more global point cloud features, which will help improve the segmentation results of point clouds. Experimental results show that the method performs well on some objects in S3DIS dataset and Toronto3d dataset.},
  archive      = {J_MVA},
  author       = {Liu, Wei and Lu, Yisheng and Zhang, Tao},
  doi          = {10.1007/s00138-024-01589-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MFFAE-net: Semantic segmentation of point clouds using multi-scale feature fusion and attention enhancement networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust visual-based method and new datasets for ego-lane
index estimation in urban environment. <em>MVA</em>, <em>35</em>(5),
1–12. (<a href="https://doi.org/10.1007/s00138-024-01590-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correct and robust ego-lane index estimation is crucial for autonomous driving in the absence of high-definition maps, especially in urban environments. Previous ego-lane index estimation approaches rely on feature extraction, which limits the robustness. To overcome these shortages, this study proposes a robust ego-lane index estimation framework upon only the original visual image. After optimization of the processing route, the raw image was randomly cropped in the height direction and then input into a double supervised LaneLoc network to obtain the index estimations and confidences. A post-process was also proposed to achieve the global ego-lane index from the estimated left and right indexes with the total lane number. To evaluate our proposed method, we manually annotated the ego-lane index of public datasets which can work as an ego-lane index estimation baseline for the first time. The proposed algorithm achieved 96.48/95.40% (precision/recall) on the CULane dataset and 99.45/99.49% (precision/recall) on the TuSimple dataset, demonstrating the effectiveness and efficiency of lane localization in diverse driving environments. The code and dataset annotation results will be exposed publicly on https://github.com/haomo-ai/LaneLoc .},
  archive      = {J_MVA},
  author       = {Wang, Dianzheng and Liang, Dongyi and Li, Shaomiao},
  doi          = {10.1007/s00138-024-01590-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Robust visual-based method and new datasets for ego-lane index estimation in urban environment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active perception based on deep reinforcement learning for
autonomous robotic damage inspection. <em>MVA</em>, <em>35</em>(5),
1–20. (<a href="https://doi.org/10.1007/s00138-024-01591-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an artificial intelligence framework is developed to facilitate the use of robotics for autonomous damage inspection. While considerable progress has been achieved by utilizing state-of-the-art computer vision approaches for damage detection, these approaches are still far away from being used for autonomous robotic inspection systems due to the uncertainties in data collection and data interpretation. To address this gap, this study proposes a framework that will enable robots to select the best course of action for active damage perception and reduction of uncertainties. By doing so, the required information is collected efficiently for a better understanding of damage severity which leads to reliable decision-making. More specifically, the active damage perception task is formulated as a Partially Observable Markov Decision Process, and a deep reinforcement learning-based active perception agent is proposed to learn the near-optimal policy for this task. The proposed framework is evaluated for the autonomous assessment of cracks on metallic surfaces of an underwater nuclear reactor. Active perception exhibits a notable enhancement in the crack Intersection over Union (IoU) performance, yielding an increase of up to 69% when compared to its raster scanning counterpart given a similar inspection time. Additionally, the proposed method can perform a rapid inspection that reduces the overall inspection time by more than two times while achieving a 15% higher crack IoU than that of the dense raster scanning approach.},
  archive      = {J_MVA},
  author       = {Tang, Wen and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-024-01591-7},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Active perception based on deep reinforcement learning for autonomous robotic damage inspection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial imitation learning-based network for
category-level 6D object pose estimation. <em>MVA</em>, <em>35</em>(5),
1–11. (<a href="https://doi.org/10.1007/s00138-024-01592-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-level 6D object pose estimation is a very fundamental and key research in computer vision. In order to get rid of the dependence on the object 3D models, analysis-by-synthesis object pose estimation methods have recently been widely studied. While these methods have certain improvements in generalization, the accuracy of category-level object pose estimation still needs to be improved. In this paper, we propose a category-level 6D object pose estimation network based on adversarial imitation learning, named AIL-Net. AIL-Net adopts the state-action distribution matching criterion and is able to perform expert actions that have not appeared in the dataset. This prevents the object pose estimation from falling into a bad state. We further design a framework for estimating object pose through generative adversarial imitation learning. This method is able to distinguish between expert policy and imitation policy in AIL-Net. Experimental results show that our approach achieves competitive category-level object pose estimation performance on REAL275 dataset and Cars dataset.},
  archive      = {J_MVA},
  author       = {Sun, Shantong and Bao, Xu and Kaushik, Aryan},
  doi          = {10.1007/s00138-024-01592-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial imitation learning-based network for category-level 6D object pose estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient ground segmentation approach for LiDAR point
cloud utilizing adjacent grids. <em>MVA</em>, <em>35</em>(5), 1–10. (<a
href="https://doi.org/10.1007/s00138-024-01593-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ground segmentation is crucial for guiding mobile robots and identifying nearby objects. However, it should be noted that the ground often presents complex topographical features, such as slopes and rugged terrains, which significantly increase the challenges associated with accurate ground segmentation tasks. To address this issue, we propose a novel approach to achieve rapid ground segmentation. The proposed method uses a multi-partition approach to extract ground points for each partition, followed by assessing the correction plane based on geometric characteristics of the ground surface and similarity among adjacent planes. An adaptive threshold is also introduced to enhance efficiency in extracting complex urban pavement. Our method was benchmarked against several contemporary techniques on the SemanticKITTI dataset. The precision was elevated by 1.72 $$\%$$ , and the precision deviation was diminished by 1.02 $$\%$$ , culminating in the most accurate and robust outcomes among the evaluated methods.},
  archive      = {J_MVA},
  author       = {Dong, Longyu and Liu, Dejun and Dong, Youqiang and Park, Bongrae and Wan, Zhibo},
  doi          = {10.1007/s00138-024-01593-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An efficient ground segmentation approach for LiDAR point cloud utilizing adjacent grids},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera-based mapping in search-and-rescue via flying and
ground robot teams. <em>MVA</em>, <em>35</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01594-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search and rescue (SaR) is challenging, due to the unknown environmental situation after disasters occur. Robotics has become indispensable for precise mapping of the environment and for locating the victims. Combining flying and ground robots more effectively serves this purpose, due to their complementary features in terms of viewpoint and maneuvering. To this end, a novel, cost-effective framework for mapping unknown environments is introduced that leverages You Only Look Once and video streams transmitted by a ground and a flying robot. The integrated mapping approach is for performing three crucial SaR tasks: localizing the victims, i.e., determining their position in the environment and their body pose, tracking the moving victims, and providing a map of the ground elevation that assists both the ground robot and the SaR crew in navigating the SaR environment. In real-life experiments at the CyberZoo of the Delft University of Technology, the framework proved very effective and precise for all these tasks, particularly in occluded and complex environments.},
  archive      = {J_MVA},
  author       = {Esteves Henriques, Bernardo and Baglioni, Mirko and Jamshidnejad, Anahita},
  doi          = {10.1007/s00138-024-01594-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Camera-based mapping in search-and-rescue via flying and ground robot teams},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal superimposed crossover module for effective
continuous sign language. <em>MVA</em>, <em>35</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s00138-024-01595-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ultimate goal of continuous sign language recognition is to facilitate communication between special populations and normal people, which places high demands on the real-time and deployable nature of the model. However, researchers have paid little attention to these two properties in previous studies on CSLR. In this paper, we propose a novel CSLR model ResNetT based on temporal superposition crossover module and ResNet, which replaces the parameterized computation with shifts in the temporal dimension and efficiently extracts temporal features without increasing the number of parameters and computation. The ResNetT is able to improve the real-time performance and deployability of the model while ensuring its accuracy. The core is our proposed zero-parameter and zero-computation module TSCM, and we combine TSCM with 2D convolution to form &quot;TSCM+2D&quot; hybrid convolution, which provides powerful spatial-temporal modeling capability, zero-parameter increase, and lower deployment cost compared with other spatial-temporal convolutions. Further, we apply &quot;TSCM+2D&quot; to ResBlock to form the new ResBlockT, which is the basis of the novel CSLR model ResNetT. We introduce stochastic gradient stops and multilevel connected temporal classification (CTC) loss to train this model, which reduces training memory usage while decreasing the final recognized word error rate (WER) and extends the ResNet network from image classification tasks to video recognition tasks. In addition, this study is the first in the field of CSLR to use only 2D convolution to extract spatial-temporal features of sign language videos for end-to-end recognition learning. Experiments on two large-scale continuous sign language datasets demonstrate the efficiency of the method.},
  archive      = {J_MVA},
  author       = {Zhu, Qidan and Li, Jing and Yuan, Fei and Gan, Quan},
  doi          = {10.1007/s00138-024-01595-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Temporal superimposed crossover module for effective continuous sign language},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergetic proto-pull and reciprocal points for open set
recognition. <em>MVA</em>, <em>35</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01596-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition (OSR) aims to accept and classify known classes while rejecting unknown classes, which is the key technology for pattern recognition algorithms to be widely applied in practice. The challenges to OSR is to reduce the empirical classification risk of known classes and the open space risk of potential unknown classes. However, the existing OSR methods less consider to optimize the open space risk, and much dark information in unknown space is not taken into account, which results in that many unknown classes are misidentified as known classes. Therefore, we present a self-supervised learningbased OSR method with synergetic proto-pull and reciprocal points, which can remarkably reduce the risks of empirical classification and open space. Especially, we propose a new concept of proto-pull point, which can be synergistically combined with reciprocal points to shrink the feature spaces of known and unknown classes, and increase the feature distance between different classes, so as to form a good feature distribution. In addition, a self-supervised learning task of identifying the directions of rotated images is introduced in OSR model training, which is benefit for the OSR mdoel to capture more distinguishing features, and decreases both empirical classification and open space risks. The final experimental results on benchmark datasets show that our propsoed approach outperforms most existing OSR methods.},
  archive      = {J_MVA},
  author       = {Deng, Xin and Yang, Luyao and Zhang, Ao and Wang, Jingwen and Wang, Hexu and Xing, Tianzhang and Xu, Pengfei},
  doi          = {10.1007/s00138-024-01596-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Synergetic proto-pull and reciprocal points for open set recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cmf-transformer: Cross-modal fusion transformer for human
action recognition. <em>MVA</em>, <em>35</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01598-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human action recognition, both spatio-temporal videos and skeleton features alone can achieve good recognition performance, however, how to combine these two modalities to achieve better performance is still a worthy research direction. In order to better combine the two modalities, we propose a novel Cross-Modal Transformer for human action recognition—CMF-Transformer, which effectively fuses two different modalities. In spatio-temporal modality, video frames are used as inputs and directional attention is used in the transformer to obtain the order of recognition between different spatio-temporal blocks. In skeleton joint modality, skeleton joints are used as inputs to explore more complete correlations in different skeleton joints by spatio-temporal cross-attention in the transformer. Subsequently, a multimodal collaborative recognition strategy is used to identify the respective features and connectivity features of two modalities separately, and then weight the identification results separately to synergistically identify target action by fusing the features under the two modalities. A series of experiments on three benchmark datasets demonstrate that the performance of CMF-Transformer in this paper outperforms most current state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Wang, Jun and Xia, Limin and Wen, Xin},
  doi          = {10.1007/s00138-024-01598-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cmf-transformer: Cross-modal fusion transformer for human action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer with multi-level grid features and depth pooling
for image captioning. <em>MVA</em>, <em>35</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01599-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is an exciting yet challenging problem in both computer vision and natural language processing research. In recent years, this problem has been addressed by Transformer-based models optimized with Cross-Entropy loss and boosted performance via Self-Critical Sequence Training. Two types of representations are embedded into captioning models: grid features and region features, and there have been attempts to include 2D geometry information in the self-attention computation. However, the 3D order of object appearances is not considered, leading to confusion for the model in cases of complex scenes with overlapped objects. In addition, recent studies using only feature maps from the last layer or block of a pretrained CNN-based model may lack spatial information. In this paper, we present the Transformer-based captioning model dubbed TMDNet. Our model includes one module to aggregate multi-level grid features (MGFA) to enrich the representation ability using prior knowledge, and another module to effectively embed the image’s depth-grid aggregation (DGA) into the model space for better performance. The proposed model demonstrates its effectiveness via evaluation on the MS-COCO “Karpathy” test split across five standard metrics.},
  archive      = {J_MVA},
  author       = {Bui, Doanh C. and Nguyen, Tam V. and Nguyen, Khang},
  doi          = {10.1007/s00138-024-01599-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Transformer with multi-level grid features and depth pooling for image captioning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient driving behavior prediction approach using
physiological auxiliary and adaptive LSTM. <em>MVA</em>, <em>35</em>(5),
1–12. (<a href="https://doi.org/10.1007/s00138-024-01600-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving behavior prediction is crucial in designing a modern Advanced driver assistance system (ADAS). Such predictions can improve driving safety by alerting the driver to the danger of unsafe or risky traffic situations. In this research, an efficient approach, Driver behavior network (DBNet) is proposed for driving behavior prediction using multiple modality data, i.e. front view video frames and driver physiological signals. Firstly, a Relation-guided spatial attention (RGSA) module is adopted to generate driving scene-centric features by modeling both local and global information from video frames. Secondly, a new Global shrinkage (GS) block is designed to incorporate soft thresholding as nonlinear transformation layer to generate physiological features and eliminate noise-related information from physiological signals. Finally, a customized Adaptive focal loss based Long short term memory (AFL-LSTM) network is introduced to learn the multi-modal features and capture the dependencies within driving behaviors simultaneously. We applied our approach on real data collected during drives in both urban and freeway environment in an instrumented vehicle. The experimental findings demonstrate that the DBNet can predict the upcoming driving behavior efficiently and significantly outperform other state-of-the-art models.},
  archive      = {J_MVA},
  author       = {Gao, Jun and Yi, Jiangang and Murphey, Yi Lu},
  doi          = {10.1007/s00138-024-01600-9},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An efficient driving behavior prediction approach using physiological auxiliary and adaptive LSTM},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast no-reference deep image dehazing. <em>MVA</em>,
<em>35</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01601-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a deep learning method for image dehazing and clarification. The main advantages of the method are high computational speed and using unpaired image data for training. The method adapts the Zero-DCE approach (Li et al. in IEEE Trans Pattern Anal Mach Intell 44(8):4225–4238, 2021) for the image dehazing problem and uses high-order curves to adjust the dynamic range of images and achieve dehazing. Training the proposed dehazing neural network does not require paired hazy and clear datasets but instead utilizes a set of loss functions, assessing the quality of dehazed images to drive the training process. Experiments on a large number of real-world hazy images demonstrate that our proposed network effectively removes haze while preserving details and enhancing brightness. Furthermore, on an affordable GPU-equipped laptop, the processing speed can reach 1000 FPS for images with 2K resolution, making it highly suitable for real-time dehazing applications.},
  archive      = {J_MVA},
  author       = {Qin, Hongyi and Belyaev, Alexander G.},
  doi          = {10.1007/s00138-024-01601-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fast no-reference deep image dehazing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced keypoint information and pose-weighted re-ID
features for multi-person pose estimation and tracking. <em>MVA</em>,
<em>35</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01602-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose estimation and tracking are crucial research directions in the field of artificial intelligence, with widespread applications in virtual reality, action recognition, and human-computer interaction. While existing pose tracking algorithms predominantly follow the top-down paradigm, they face challenges, such as pose occlusion and motion blur in complex scenes, leading to tracking inaccuracies. To address these challenges, we leverage enhanced keypoint information and pose-weighted re-identification (re-ID) features to improve the performance of multi-person pose estimation and tracking. Specifically, our proposed Decouple Heatmap Network decouples heatmaps into keypoint confidence and position. The refined keypoint information are utilized to reconstruct occluded poses. For the pose tracking task, we introduce a more efficient pipeline founded on pose-weighted re-ID features. This pipeline integrates a Pose Embedding Network to allocate weights to re-ID features and achieves the final pose tracking through a novel tracking matching algorithm. Extensive experiments indicate that our approach performs well in both multi-person pose estimation and tracking and achieves state-of-the-art results on the PoseTrack 2017 and 2018 datasets. Our source code is available at: https://github.com/TaoTaoPei/posetracking .},
  archive      = {J_MVA},
  author       = {Wang, Xiangyang and Pei, Tao and Wang, Rui},
  doi          = {10.1007/s00138-024-01602-7},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced keypoint information and pose-weighted re-ID features for multi-person pose estimation and tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object recognition consistency in regression for active
detection. <em>MVA</em>, <em>35</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01604-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning has achieved great success in image classification because of selecting the most informative samples for data labeling and model training. However, the potential of active learning has been far from being realised in object detection due to its unique challenge in utilizing localization information. A popular compromise is to simply take active classification learning over detected object candidates. To consider the localization information of object detection, current effort usually falls into the model-dependent fashion, which either works on specific detection frameworks or relies on additionally designed modules. In this paper, we propose model-agnostic Object Recognition Consistency in Regression (ORCR), which can holistically measure the uncertainty information of classification and localization of each detected candidate from object detection. The philosophy behind ORCR is to obtain the detection uncertainty by calculating the classification consistency through localization regression at two successive detection scales. In the light of the proposed ORCR, we devise an active learning framework that enables an effortless deployment to any object detection architecture. Experimental results on the PASCAL VOC and MS-COCO benchmarks show that our method achieves better performance while simplifying the active detection process.},
  archive      = {J_MVA},
  author       = {Jing, Ming and Ou, Zhilong and Wang, Hongxing and Li, Jiaxin and Zhao, Ziyi},
  doi          = {10.1007/s00138-024-01604-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Object recognition consistency in regression for active detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: BoostTrack: Boosting the similarity measure and
detection confidence for improved multiple object tracking.
<em>MVA</em>, <em>35</em>(5), 1–2. (<a
href="https://doi.org/10.1007/s00138-024-01605-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Stanojevic, Vukasin D. and Todorovic, Branimir T.},
  doi          = {10.1007/s00138-024-01605-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-2},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction: BoostTrack: boosting the similarity measure and detection confidence for improved multiple object tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight segmentation algorithm of feasible area and
targets of unmanned surface cleaning vessels. <em>MVA</em>,
<em>35</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01537-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve real-time segmentation with accurate delineation for feasible areas and target recognition in Unmanned Surface Cleaning Vessel (USCV) image processing, a segmentation approach leveraging visual sensors on USCVs was developed. Initial data collection was executed with remote-controlled cleaning vessels, followed by data cleansing, image deduplication, and manual selection. This led to the creation of WaterSeg dataset, tailored for segmentation tasks in USCV contexts. Upon comparing various deep learning-driven semantic segmentation techniques, a novel, efficient Muti-Cascade Semantic Segmentation Network (MCSSNet) emerged. Comprehensive tests demonstrated that, relative to the state of the art, MCSSNet achieved an average accuracy of 90.64%, a segmentation speed of 44.55fps, and a 45% reduction in model parameters.},
  archive      = {J_MVA},
  author       = {Shen, Jingfu and Zhang, Yuanliang and Liu, Feiyue and Liu, Chun},
  doi          = {10.1007/s00138-024-01537-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Lightweight segmentation algorithm of feasible area and targets of unmanned surface cleaning vessels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty estimates for semantic segmentation: Providing
enhanced reliability for automated motor claims handling. <em>MVA</em>,
<em>35</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s00138-024-01541-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network models for image segmentation can be a powerful tool for the automation of motor claims handling processes in the insurance industry. A crucial aspect is the reliability of the model outputs when facing adverse conditions, such as low quality photos taken by claimants to document damages. We explore the use of a meta-classification model to empirically assess the precision of segments predicted by a model trained for the semantic segmentation of car body parts. Different sets of features correlated with the quality of a segment are compared, and an AUROC score of 0.915 is achieved for distinguishing between high- and low-quality segments. By removing low-quality segments, the average $$m{\textit{IoU}} $$ of the segmentation output is improved by 16 percentage points and the number of wrongly predicted segments is reduced by 77%.},
  archive      = {J_MVA},
  author       = {Küchler, Jan and Kröll, Daniel and Schoenen, Sebastian and Witte, Andreas},
  doi          = {10.1007/s00138-024-01541-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Uncertainty estimates for semantic segmentation: Providing enhanced reliability for automated motor claims handling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medtransnet: Advanced gating transformer network for medical
image classification. <em>MVA</em>, <em>35</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01542-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate medical image classification poses a significant challenge in designing expert computer-aided diagnosis systems. While deep learning approaches have shown remarkable advancements over traditional techniques, addressing inter-class similarity and intra-class dissimilarity across medical imaging modalities remains challenging. This work introduces the advanced gating transformer network (MedTransNet), a deep learning model tailored for precise medical image classification. MedTransNet utilizes channel and multi-gate attention mechanisms, coupled with residual interconnections, to learn category-specific attention representations from diverse medical imaging modalities. Additionally, the use of gradient centralization during training helps in preventing overfitting and improving generalization, which is especially important in medical imaging applications where the availability of labeled data is often limited. Evaluation on benchmark datasets, including APTOS-2019, Figshare, and SARS-CoV-2, demonstrates effectiveness of the proposed MedTransNet across tasks such as diabetic retinopathy severity grading, multi-class brain tumor classification, and COVID-19 detection. Experimental results showcase MedTransNet achieving 85.68% accuracy for retinopathy grading, 98.37% ( $$\pm \,0.44$$ ) for tumor classification, and 99.60% for COVID-19 detection, surpassing recent deep learning models. MedTransNet holds promise for significantly improving medical image classification accuracy.},
  archive      = {J_MVA},
  author       = {Shaik, Nagur Shareef and Cherukuri, Teja Krishna and Veeranjaneulu, N and Bodapati, Jyostna Devi},
  doi          = {10.1007/s00138-024-01542-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Medtransnet: Advanced gating transformer network for medical image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive overview of deep learning techniques for 3D
point cloud classification and semantic segmentation. <em>MVA</em>,
<em>35</em>(4), 1–54. (<a
href="https://doi.org/10.1007/s00138-024-01543-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points. To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field. It serves as a comprehensive review on two major tasks in 3D point cloud processing—namely, 3D shape classification and semantic segmentation.},
  archive      = {J_MVA},
  author       = {Sarker, Sushmita and Sarker, Prithul and Stone, Gunner and Gorman, Ryan and Tavakkoli, Alireza and Bebis, George and Sattarvand, Javad},
  doi          = {10.1007/s00138-024-01543-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-54},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FDT − Dr2T: A unified dense radiology report generation
transformer framework for x-ray images. <em>MVA</em>, <em>35</em>(4),
1–13. (<a href="https://doi.org/10.1007/s00138-024-01544-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical Image Captioning (MIC), is a developing area of artificial intelligence that combines two main research areas, computer vision and natural language processing. In order to support clinical workflows and decision-making, MIC is used in a variety of applications pertaining to diagnosis, therapy, report production, and computer-aided diagnosis. The generation of long and coherent reports highlighting correct abnormalities is a challenging task. Therefore, in this direction, this paper presents an efficient $$FDT-D{r}^{2}T$$ framework for the generation of coherent radiology reports with efficient exploitation of medical content. The proposed framework leverages the fusion of texture features and deep features in the first stage by incorporating ISCM-LBP + PCA-HOG feature extraction algorithm and Convolutional Triple Attention-based Efficient XceptionNet ( $$C-TaXNet$$ ). Further, fused features from the FDT module are utilized by the Dense Radiology Report Generation Transformer ( $$D{r}^{2}T$$ ) model with modified multi-head attention generating dense radiology reports by highlighting specific crucial abnormalities. To evaluate the performance of the proposed $$FDT-D{r}^{2}T$$ extensive experiments are conducted on publicly available IU Chest X-ray dataset and the best performance of the work is observed as 0.531 BLEU@1, 0.398 BLEU@2, 0.322 BLEU@3, 0.251 BLEU@4, 0.384 CIDEr, 0.506 ROUGE-L, 0.277 METEOR. An ablation study is carried out to support the experiments. Overall, the results obtained demonstrate the efficiency and efficacy of the proposed framework.},
  archive      = {J_MVA},
  author       = {Sharma, Dhruv and Dhiman, Chhavi and Kumar, Dinesh},
  doi          = {10.1007/s00138-024-01544-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FDT − Dr2T: A unified dense radiology report generation transformer framework for X-ray images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual feature learning with hierarchical calibration for
gaze estimation. <em>MVA</em>, <em>35</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-024-01545-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze estimation aims to predict accurate gaze direction from natural eye images, which is an extreme challenging task due to both random variations in head pose and person-specific biases. Existing works often independently learn features from binocular images and directly concatenate them for gaze estimation. In this paper, we propose a simple yet effective two-stage framework for gaze estimation, in which both residual feature learning (RFL) and hierarchical gaze calibration (HGC) networks are designed to consistently improve the performance of gaze estimation. Specifically, the RFL network extracts informative features by jointly exploring the symmetric and asymmetric factors between left and right eyes, which can produce accurate initial predictions as much as possible. Besides, the HGC network cascades a personal-specific transform module to further transform the distribution of gaze point from coarse to fine, which can effectively compensate the subjective bias in initial predictions. Extensive experiments on both EVE and MPIIGaze datasets show that our method outperforms the state-of-the-art approaches.},
  archive      = {J_MVA},
  author       = {Yin, Zhengdan and Zhou, Sanping and Wang, Le and Dai, Tao and Hua, Gang and Zheng, Nanning},
  doi          = {10.1007/s00138-024-01545-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Residual feature learning with hierarchical calibration for gaze estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tree-based approach for visible and thermal sensor fusion
in winter autonomous driving. <em>MVA</em>, <em>35</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01546-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on autonomous vehicles has been at a peak recently. One of the most researched aspects is the performance degradation of sensors in harsh weather conditions such as rain, snow, fog, and hail. This work addresses this performance degradation by fusing multiple sensor modalities inside the neural network used for detection. The proposed fusion method removes the pre-process fusion stage. It directly produces detection boxes from numerous images. It reduces the computation cost by providing detection and fusion simultaneously. By separating the network during the initial layers, the network can easily be modified for new sensors. Intra-network fusion improves robustness to missing inputs and applies to all compatible types of inputs while reducing the peak computing cost by using a valley-fill algorithm. Our experiments demonstrate that adopting a parallel multimodal network to fuse thermal images in the network improves object detection during difficult weather conditions such as harsh winters by up to 5% mAP while reducing dataset bias during complicated weather conditions. It also happens with around 50% fewer parameters than late-fusion approaches, which duplicate the whole network instead of the first section of the feature extractor.},
  archive      = {J_MVA},
  author       = {Boisclair, Jonathan and Amamou, Ali and Kelouwani, Sousso and Alam, M. Zeshan and Oueslati, Hedi and Zeghmi, Lotfi and Agbossou, Kodjo},
  doi          = {10.1007/s00138-024-01546-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A tree-based approach for visible and thermal sensor fusion in winter autonomous driving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi sentence description of complex manipulation action
videos. <em>MVA</em>, <em>35</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s00138-024-01547-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic video description necessitates generating natural language statements that encapsulate the actions, events, and objects within a video. An essential human capability in describing videos is to vary the level of detail, a feature that existing automatic video description methods, which typically generate single, fixed-level detail sentences, often overlook. This work delves into video descriptions of manipulation actions, where varying levels of detail are crucial to conveying information about the hierarchical structure of actions, also pertinent to contemporary robot learning techniques. We initially propose two frameworks: a hybrid statistical model and an end-to-end approach. The hybrid method, requiring significantly less data, statistically models uncertainties within video clips. Conversely, the end-to-end method, more data-intensive, establishes a direct link between the visual encoder and the language decoder, bypassing any statistical processing. Furthermore, we introduce an Integrated Method, aiming to amalgamate the benefits of both the hybrid statistical and end-to-end approaches, enhancing the adaptability and depth of video descriptions across different data availability scenarios. All three frameworks utilize LSTM stacks to facilitate description granularity, allowing videos to be depicted through either succinct single sentences or elaborate multi-sentence narratives. Quantitative results demonstrate that these methods produce more realistic descriptions than other competing approaches.},
  archive      = {J_MVA},
  author       = {Ziaeetabar, Fatemeh and Safabakhsh, Reza and Momtazi, Saeedeh and Tamosiunaite, Minija and Wörgötter, Florentin},
  doi          = {10.1007/s00138-024-01547-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi sentence description of complex manipulation action videos},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAMTrack: A combined appearance-motion method for
multiple-object tracking. <em>MVA</em>, <em>35</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01548-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking has emerged as an essential process for various applications in the field of computer vision, such as autonomous driving. Recently, object tracking technology has experienced rapid growth, particularly its applications in self-driving vehicles. Tracking systems typically follow the detection-based tracking paradigm, which is affected by the detection results. Although deep learning has led to significant improvements in object detection, data association remains dependent on factors such as spatial location, motion, and appearance, to associate new observations with existing tracks. In this study, we introduce a novel approach called Combined Appearance-Motion Tracking (CAMTrack) to enhance data association by integrating object appearances and their corresponding movements. The proposed tracking method utilizes an appearance-motion model using an appearance-affinity network and an Interactive Multiple Model (IMM). We deploy the appearance model to address the visual affinity between objects across frames and employed the motion model to incorporate motion constraints to obtain robust position predictions under maneuvering movements. Moreover, we also propose a Two-phase association algorithm which is an effective way to recover lost tracks back from previous frames. CAMTrack was evaluated on the widely recognized object tracking benchmarks-KITTI and MOT17. The results showed the superior performance of the proposed method, highlighting its potential to contribute to advances in object tracking.},
  archive      = {J_MVA},
  author       = {Bui, Duy Cuong and Nguyen, Ngan Linh and Hoang, Anh Hiep and Yoo, Myungsik},
  doi          = {10.1007/s00138-024-01548-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CAMTrack: A combined appearance-motion method for multiple-object tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal fine-grained grocery product recognition using
image and OCR text. <em>MVA</em>, <em>35</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-024-01549-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic recognition of grocery products can be used to improve customer flow at checkouts and reduce labor costs and store losses. Product recognition is, however, a challenging task for machine learning-based solutions due to the large number of products and their variations in appearance. In this work, we tackle the challenge of fine-grained product recognition by first extracting a large dataset from a grocery store containing products that are only differentiable by subtle details. Then, we propose a multimodal product recognition approach that uses product images with extracted OCR text from packages to improve fine-grained recognition of grocery products. We evaluate several image and text models separately and then combine them using different multimodal models of varying complexities. The results show that image and textual information complement each other in multimodal models and enable a classifier with greater recognition performance than unimodal models, especially when the number of training samples is limited. Therefore, this approach is suitable for many different scenarios in which product recognition is used to further improve recognition performance. The dataset can be found at https://github.com/Tubbias/finegrainocr .},
  archive      = {J_MVA},
  author       = {Pettersson, Tobias and Riveiro, Maria and Löfström, Tuwe},
  doi          = {10.1007/s00138-024-01549-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multimodal fine-grained grocery product recognition using image and OCR text},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermal infrared action recognition with two-stream shift
graph convolutional network. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01550-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive deployment of camera-based IoT devices in our society is heightening the vulnerability of citizens’ sensitive information and individual data privacy. In this context, thermal imaging techniques become essential for data desensitization, entailing the elimination of sensitive data to safeguard individual privacy. Meanwhile, thermal imaging techniques can also play a important role in industry by considering the industrial environment with low resolution, high noise and unclear objects’ features. Moreover, existing works often process the entire video as a single entity, which results in suboptimal robustness by overlooking individual actions occurring at different times. In this paper, we propose a lightweight algorithm for action recognition in thermal infrared videos using human skeletons to address this. Our approach includes YOLOv7-tiny for target detection, Alphapose for pose estimation, dynamic skeleton modeling, and Graph Convolutional Networks (GCN) for spatial-temporal feature extraction in action prediction. To overcome detection and pose challenges, we created OQ35-human and OQ35-keypoint datasets for training. Besides, the proposed model enhances robustness by using visible spectrum data for GCN training. Furthermore, we introduce the two-stream shift Graph Convolutional Network to improve the action recognition accuracy. Our experimental results on the custom thermal infrared action dataset (InfAR-skeleton) demonstrate Top-1 accuracy of 88.06% and Top-5 accuracy of 98.28%. On the filtered kinetics-skeleton dataset, the algorithm achieves Top-1 accuracy of 55.26% and Top-5 accuracy of 83.98%. Thermal Infrared Action Recognition ensures the protection of individual privacy while meeting the requirements of action recognition.},
  archive      = {J_MVA},
  author       = {Liu, Jishi and Wang, Huanyu and Wang, Junnian and He, Dalin and Xu, Ruihan and Tang, Xiongfeng},
  doi          = {10.1007/s00138-024-01550-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Thermal infrared action recognition with two-stream shift graph convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning more discriminative local descriptors with
parameter-free weighted attention for few-shot learning. <em>MVA</em>,
<em>35</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-024-01551-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning for image classification comes up as a hot topic in computer vision, which aims at fast learning from a limited number of labeled images and generalize over the new tasks. In this paper, motivated by the idea of Fisher Score, we propose a Discriminative Local Descriptors Attention model that uses the ratio of intra-class and inter-class similarity to adaptively highlight the representative local descriptors without introducing any additional parameters, while most of the existing local descriptors based methods utilize the neural networks that inevitably involve the tedious parameter tuning. Experiments on four benchmark datasets show that our method achieves higher accuracy compared with the state-of-art approaches for few-shot learning. Specifically, our method is optimal on the CUB-200 dataset, and outperforms the second best competitive algorithm by 4.12 $$\%$$ and 0.49 $$\%$$ under the 5-way 1-shot and 5-way 5-shot settings, respectively.},
  archive      = {J_MVA},
  author       = {Song, Qijun and Zhou, Siyun and Chen, Die},
  doi          = {10.1007/s00138-024-01551-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Learning more discriminative local descriptors with parameter-free weighted attention for few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised contrastive learning with multi-scale interaction
and integrity learning for salient object detection. <em>MVA</em>,
<em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01552-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is designed to mimic human visual mechanisms to identify and segment the most salient part of an image. Although related works have achieved great progress in SOD, they are limited when it comes to interferences of non-salient objects, finely shaped objects and co-salient objects. To improve the effectiveness and capability of SOD, we propose a supervised contrastive learning network with multi-scale interaction and integrity learning named SCLNet. It adopts contrastive learning (CL), multi-reception field confusion (MRFC) and context enhancement (CE) mechanisms. Using this method, the input image is first divided into two branches after two different data augmentations. Unlike existing models, which focus more on boundary guidance, we add a random position mask on one branch to break the continuous of objects. Through the CL module, we obtain more semantic information than appearance information by learning the invariance of different data augmentations. The MRFC module is then designed to learn the internal connections and common influences of various reception field features layer by layer. Next, the obtained features are learned through the CE module for the integrity and continuity of salient objects. Finally, comprehensive evaluations on five challenging benchmark datasets show that SCLNet achieves superior results. Code is available at https://github.com/YuPangpangpang/SCLNet .},
  archive      = {J_MVA},
  author       = {Bi, Yu and Chen, Zhenxue and Liu, Chengyun and Liang, Tian and Zheng, Fei},
  doi          = {10.1007/s00138-024-01552-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Supervised contrastive learning with multi-scale interaction and integrity learning for salient object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual foreign object detection system for wireless
charging of electric vehicles. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01553-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless charging of electric vehicles can be achieved by installing a transmitter coil into the ground and a receiver coil at the underbody of a vehicle. In order to charge efficiently, accurate alignment of the charging components must be accomplished, which can be achieved with a camera-based positioning system. Due to an air gap between both charging components, foreign objects can interfere with the charging process and pose potential hazards to the environment. Various foreign object detection systems have been developed with the motivation to increase the safety of wireless charging. In this paper, we propose a foreign object detection technique that utilizes the integrated camera of an embedded positioning system. Due to operation in an outdoor environment, we cannot determine the types of objects that may occur in advance. Accordingly, our approach achieves object-type independence by learning the features of the charging surface, to then classify anomalous regions as foreign objects. To examine the capability of detecting foreign objects, we evaluate our approach by conducting experiments with images depicting known and unknown object types. For the experiments, we use an image dataset recorded by a positioning camera of an operating wireless charging station in an outdoor environment, which we published alongside our research. As a benchmark system, we employ YOLOv8 (Jocher et al. in Ultralytics YOLO, 2023), a state-of-the-art neural network that has been used in various contexts for foreign object detection. While we acknowledge the performance of YOLOv8 for known object types, our approach achieves up to 18% higher precision and 46% higher detection success for unknown objects.},
  archive      = {J_MVA},
  author       = {Shahbaz Nejad, Bijan and Roch, Peter and Handte, Marcus and Marrón, Pedro José},
  doi          = {10.1007/s00138-024-01553-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A visual foreign object detection system for wireless charging of electric vehicles},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEA-net: Edge-enhanced assistance network for infrared small
target detection. <em>MVA</em>, <em>35</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01554-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, the performance of infrared small target detection (IRSTD) has been significantly improved. A precise shape of the target edge is crucial for segmenting small infrared targets. However, existing CNN-based methods do not effectively integrate the edge and shape information of the target, leading to the distortion of the detected target shape. To alleviate this problem, this paper proposes an edge-enhanced assistance network (EEA-Net) for IRSTD. Specifically, we design an edge-gate module (E-G) to extract the edge information and convey it to deep layers through a cascading structure. Based on E-G, we further propose an information-preserving attention module (IPA) to reduce the loss of edge information in the network. In addition, we design a global perception module (GP) to facilitate the flow of edge information and enhance the network’s ability to fuse information at multiple scales. Extensive experiments on the public datasets demonstrate the superiority of the proposed method over state-of-the-art IRSTD methods.},
  archive      = {J_MVA},
  author       = {Wang, Chen and Hu, Xiaopeng and Gao, Xiang and Wei, Haoyu and Tao, Jiawei and Wang, Fan},
  doi          = {10.1007/s00138-024-01554-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {EEA-net: Edge-enhanced assistance network for infrared small target detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scale-adaptive gesture computing: Detection, tracking and
recognition in controlled complex environments. <em>MVA</em>,
<em>35</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s00138-024-01555-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complexity intensifies when gesticulations span various scales. Traditional scale-invariant object recognition methods often falter when confronted with case-sensitive characters in the English alphabet. The literature underscores a notable gap, the absence of an open-source multi-scale un-instructional gesture database featuring a comprehensive dictionary. In response, we have created the NITS (gesture scale) database, which encompasses isolated mid-air gesticulations of ninety-five alphanumeric characters. In this research, we present a scale-centric framework that addresses three critical aspects: (1) detection of smaller gesture objects: our framework excels at detecting smaller gesture objects, such as a red color marker. (2) Removal of redundant self co-articulated strokes: we propose an effective approach to eliminate redundant self co-articulated strokes often present in gesture trajectories. (3) Scale-variant approach for recognition: to tackle the scale vs. size ambiguity in recognition, we introduce a novel scale-variant methodology. Our experimental results reveal a substantial improvement of approximately 16% compared to existing state-of-the-art recognition models for mid-air gesture recognition. These outcomes demonstrate that our proposed approach successfully emulates the perceptibility found in the human visual system, even when utilizing data from monophthalmic vision. Furthermore, our findings underscore the imperative need for comprehensive studies encompassing scale variations in gesture recognition.},
  archive      = {J_MVA},
  author       = {Kirupakaran, Anish Monsley and Laskar, Rabul Hussain},
  doi          = {10.1007/s00138-024-01555-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Scale-adaptive gesture computing: Detection, tracking and recognition in controlled complex environments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trusted 3D self-supervised representation learning with
cross-modal settings. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01556-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal setting employing 2D images and 3D point clouds in self-supervised representation learning is proven to be an effective way to enhance visual perception capabilities. However, different modalities have different data formats and representations. Directly using features extracted from cross-modal datasets may lead to information conflicting and collapsing. We refer to this problem as uncertainty in network learning. Therefore, reducing uncertainty to obtain trusted descriptions has become the key to improving network performance. Motivated by this, we propose our trusted cross-modal network in self-supervised learning (TCMSS). It can obtain trusted descriptions by a trusted combination module as well as improve network performance with a well-designed loss function. In the trusted combination module, we utilize the Dirichlet distribution and the subjective logic to parameterize the features and acquire probabilistic uncertainty at the same. Then, the Dempster-Shafer Theory (DST) is used to obtain trusted descriptions by weighting uncertainty to the parameterized results. We have also designed our trusted domain loss function, including domain loss and trusted loss. It can effectively improve the prediction accuracy of the network by applying contrastive learning between different feature descriptions. The experimental results show that our model outperforms previous results on linear classification in ScanObjectNN as well as few-shot classification in both ModelNet40 and ScanObjectNN. In addition, part segmentation also reports a superior result to previous methods in ShapeNet. Further, the ablation studies validate the potency of our method for a better point cloud understanding.},
  archive      = {J_MVA},
  author       = {Han, Xu and Cheng, Haozhe and Shi, Pengcheng and Zhu, Jihua},
  doi          = {10.1007/s00138-024-01556-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Trusted 3D self-supervised representation learning with cross-modal settings},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multimodal-based finger spelling recognition for thai
sign language: A new benchmark and model composition. <em>MVA</em>,
<em>35</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s00138-024-01557-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based sign language recognition is vital for improving communication for the deaf and hard of hearing. Creating and maintaining quality of Thai sign language video datasets is challenging due to a lack of resources. Tackling this issue, we rigorously investigate a design and development of deep learning-based system for Thai Finger Spelling recognition, assessing various models with a new dataset of 90 standard letters performed by 43 diverse signers. We investigate seven deep learning models with three distinct modalities for our analysis: video-only methods (including RGB-sequencing-based CNN-LSTM and VGG-LSTM), human body joint coordinate sequences (processed by LSTM, BiLSTM, GRU, and Transformer models), and skeleton analysis (using TGCN with graph-structured skeleton representation). A thorough assessment of these models is conducted across seven circumstances, encompassing single-hand postures, single-hand motions with one, two, and three strokes, as well as two-hand postures with both static and dynamic point-on-hand interactions. The research highlights that the TGCN model is the optimal lightweight model in all scenarios. In single-hand pose cases, a combination of the Transformer and TGCN models of two modalities delivers outstanding performance, excelling in four particular conditions: single-hand poses, single-hand poses requiring one, two, and three strokes. In contrast, two-hand poses with static or dynamic point-on-hand interactions present substantial challenges, as the data from joint coordinates is inadequate due to hand obstructions, stemming from insufficient coordinate sequence data and the lack of a detailed skeletal graph structure. The study recommends integrating RGB-sequencing with visual modality to enhance the accuracy of two-handed sign language gestures.},
  archive      = {J_MVA},
  author       = {Vijitkunsawat, Wuttichai and Racharak, Teeradaj and Le Nguyen, Minh},
  doi          = {10.1007/s00138-024-01557-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep multimodal-based finger spelling recognition for thai sign language: A new benchmark and model composition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human–object interaction detection based on disentangled
axial attention transformer. <em>MVA</em>, <em>35</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01558-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection aims to localize and infer interactions between human and objects in an image. Recent work proposed transformer encoder–decoder architectures for HOI detection with exceptional performance, but possess certain drawbacks: they do not employ a complete disentanglement strategy to learn more discriminative features for different sub-tasks; they cannot achieve sufficient contextual exchange within each branch, which is crucial for accurate relational reasoning; their transformer models suffer from high computational costs and large memory usage due to complex attention calculations. In this work, we propose a disentangled transformer network that disentangles both the encoder and decoder into three branches for human detection, object detection, and interaction classification. Then we propose a novel feature unify decoder to associate the predictions of each disentangled decoder, and introduce a multiplex relation embedding module and an attentive fusion module to perform sufficient contextual information exchange among branches. Additionally, to reduce the model’s computational cost, a position-sensitive axial attention is incorporated into the encoder, allowing our model to achieve a better accuracy-complexity trade-off. Extensive experiments are conducted on two public HOI benchmarks to demonstrate the effectiveness of our approach. The results indicate that our model outperforms other methods, achieving state-of-the-art performance.},
  archive      = {J_MVA},
  author       = {Xia, Limin and Xiao, Qiyue},
  doi          = {10.1007/s00138-024-01558-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Human–object interaction detection based on disentangled axial attention transformer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual home staging and relighting from a single panorama
under natural illumination. <em>MVA</em>, <em>35</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01559-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual staging technique can digitally showcase a variety of real-world scenes. However, relighting indoor scenes from a single image is challenging due to unknown scene geometry, material properties, and outdoor spatially-varying lighting. In this study, we use the High Dynamic Range (HDR) technique to capture an indoor panorama and its paired outdoor hemispherical photograph, and we develop a novel inverse rendering approach for scene relighting and editing. Our method consists of four key components: (1) panoramic furniture detection and removal, (2) automatic floor layout design, (3) global rendering with scene geometry, new furniture objects, and the real-time outdoor photograph, and (4) virtual staging with new camera position, outdoor illumination, scene texture, and electrical light. The results demonstrate that a single indoor panorama can be used to generate high-quality virtual scenes under new environmental conditions. Additionally, we contribute a new calibrated HDR (Cali-HDR) dataset that consists of 137 paired indoor and outdoor photographs. The animation for virtual rendered scenes is available here .},
  archive      = {J_MVA},
  author       = {Ji, Guanzhou and Sawyer, Azadeh O. and Narasimhan, Srinivasa G.},
  doi          = {10.1007/s00138-024-01559-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Virtual home staging and relighting from a single panorama under natural illumination},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chfnet: A coarse-to-fine hierarchical refinement model for
monocular depth estimation. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01560-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many researchers have exploited multiple depth estimation architectures to produce high-quality depth maps from a single image. For monocular depth estimation, abundant multiscale features can significantly improve the prediction accuracy. Furthermore, multilevel refinement of the depth map through the model can effectively enhance the overall quality of the depth map. Therefore, we propose an efficient and effective module called light densely connected atrous spatial pyramid (LightDASP), which is employed to extract multiscale information at denser and larger scales from different levels of encoded features without significantly increasing the model size. Next, we propose a hierarchical reconstruction strategy that generates more accurate depth maps by refining the depth maps generated in the previous stage after each decoding stage. Additionally, to provide spatial location information to the decoder, the edge map is incorporated into the generation of a more rational refinement map. The experimental results, conducted on benchmark datasets in both indoor and outdoor scenes, demonstrate that our approach achieves efficient and competitive performance compared to existing methods for monocular depth estimation. We strike a balance between performance and efficiency, resulting in a model with greater potential for practical application. The code is available at https://github.com/ChenAndJiayi/CHFNet upon article acceptance.},
  archive      = {J_MVA},
  author       = {Chen, Han and Wang, Yongxiong},
  doi          = {10.1007/s00138-024-01560-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Chfnet: A coarse-to-fine hierarchical refinement model for monocular depth estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). React: Recognize every action everywhere all at once.
<em>MVA</em>, <em>35</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01561-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of computer vision, Group Activity Recognition (GAR) plays a vital role, finding applications in sports video analysis, surveillance, and social scene understanding. This paper introduces Recognize Every Action Everywhere All At Once (REACT), a novel architecture designed to model complex contextual relationships within videos. REACT leverages advanced transformer-based models for encoding intricate contextual relationships, enhancing understanding of group dynamics. Integrated Vision-Language Encoding facilitates efficient capture of spatiotemporal interactions and multi-modal information, enabling comprehensive scene understanding. The model’s precise action localization refines joint understanding of text and video data, enabling precise bounding box retrieval and enhancing semantic links between textual descriptions and visual reality. Actor-Specific Fusion strikes a balance between actor-specific details and contextual information, improving model specificity and robustness in recognizing group activities. Experimental results demonstrate REACT’s superiority over state-of-the-art GAR approaches, achieving higher accuracy in recognizing and understanding group activities across diverse datasets. This work significantly advances group activity recognition, offering a robust framework for nuanced scene comprehension.},
  archive      = {J_MVA},
  author       = {Chappa, Naga V. S. Raviteja and Nguyen, Pha and Dobbs, Page Daniel and Luu, Khoa},
  doi          = {10.1007/s00138-024-01561-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {React: Recognize every action everywhere all at once},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation of realistic synthetic cable images to train deep
learning segmentation models. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01562-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is one of the most important and studied problems in machine vision, which has been solved with high accuracy by many deep learning models. However, all these models present a significant drawback, they require large and diverse datasets to be trained. Gathering and annotating all these images manually would be extremely time-consuming, hence, numerous researchers have proposed approaches to facilitate or automate the process. Nevertheless, when the objects to be segmented are deformable, such as cables, the automation of this process becomes more challenging, as the dataset needs to represent their high diversity of shapes while keeping a high level of realism, and none of the existing solutions have been able to address it effectively. Therefore, this paper proposes a novel methodology to automatically generate highly realistic synthetic datasets of cables for training deep learning models in image segmentation tasks. This methodology utilizes Blender to create photo-realistic cable scenes and a Python pipeline to introduce random variations and natural deformations. To prove its performance, a dataset composed of 25000 synthetic cable images and their corresponding masks was generated and used to train six popular deep learning segmentation models. These models were then utilized to segment real cable images achieving outstanding results (over 70% IoU and 80% Dice coefficient for all the models). Both the methodology and the generated dataset are publicly available in the project’s repository.},
  archive      = {J_MVA},
  author       = {MalvidoFresnillo, Pablo and Mohammed, Wael M. and Vasudevan, Saigopal and PerezGarcia, Jose A. and MartinezLastra, Jose L.},
  doi          = {10.1007/s00138-024-01562-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Generation of realistic synthetic cable images to train deep learning segmentation models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale information fusion generative adversarial
network for real-world noisy image denoising. <em>MVA</em>,
<em>35</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01563-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is crucial for enhancing image quality, improving visual effects, and boosting the accuracy of image analysis and recognition. Most of the current image denoising methods perform superior on synthetic noise images, but their performance is limited on real-world noisy images since the types and distributions of real noise are often uncertain. To address this challenge, a multi-scale information fusion generative adversarial network method is proposed in this paper. Specifically, In this method, the generator is an end-to-end denoising network that consists of a novel encoder–decoder network branch and an improved residual network branch. The encoder–decoder branch extracts rich detailed and contextual information from images at different scales and utilizes a feature fusion method to aggregate multi-scale information, enhancing the feature representation performance of the network. The residual network further compensates for the compressed and lost information in the encoder stage. Additionally, to effectively aid the generator in accomplishing the denoising task, convolution kernels of various sizes are added to the discriminator to improve its image evaluation ability. Furthermore, the dual denoising loss function is presented to enhance the model’s capability in performing noise removal and image restoration. Experimental results show that the proposed method exhibits superior objective performance and visual quality than some state-of-the-art methods on three real-world datasets.},
  archive      = {J_MVA},
  author       = {Hu, Xuegang and Zhao, Wei},
  doi          = {10.1007/s00138-024-01563-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-scale information fusion generative adversarial network for real-world noisy image denoising},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFMANet: A multispectral pedestrian detection network using
multi-resolution RGB feature reuse with multi-scale FIR attentions.
<em>MVA</em>, <em>35</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-024-01564-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of multispectral pedestrian detection, especially under challenging low-illumination, the existing methods, characterized by cross-modality feature interaction, lack generalization and are hard to achieve the optimal balance in multimodal interaction for different data distributions. To address these issues, we propose a more efficient network using multi-resolution feature reuse with multi-scale attention (MFMANet), tailored for multispectral pedestrian detection. An enhanced UNet is explored to reuse multi-resolution features, compensating the pixel-level information to decoder for object detection improvement. The contour and temperature attention mechanisms are strategically designed to focus on shape and crucial areas of pedestrians, effectively overcoming the loss of detail information commonly associated with RGB modality. Extensive experiments conducted on the KAIST and CVC-14 datasets validate the superior performance of MFMANet, with results indicating mean Average Precision (mAP) of 96.7% and 95.8%, and Miss Rate (MR) of 6.65% and 18.7%, respectively. These findings underscore the enhanced precision and computational efficiency of MFMANet, positioning it as a significant improvement over traditional methods in multispectral pedestrian detection.},
  archive      = {J_MVA},
  author       = {Guo, Jiaren and Zhang, Yuzhen and Zheng, Jianyin and Huang, Zihao and Tao, Yanyun},
  doi          = {10.1007/s00138-024-01564-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MFMANet: A multispectral pedestrian detection network using multi-resolution RGB feature reuse with multi-scale FIR attentions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-modal framework for continuous and isolated hand
gesture recognition utilizing movement epenthesis detection.
<em>MVA</em>, <em>35</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01565-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition, having multitudinous applications in the real world, is one of the core areas of research in the field of human-computer interaction. In this paper, we propose a novel method for isolated and continuous hand gesture recognition utilizing the movement epenthesis detection and removal. For this purpose, the present work detects and removes the movement epenthesis frames from the isolated and continuous hand gesture videos. In this paper, we have also proposed a novel modality based on the temporal difference that extracts hand regions, removes gesture irrelevant factors and provides temporal information contained in the hand gesture videos. Using the proposed modality and other modalities such as the RGB modality, depth modality and segmented hand modality, features are extracted using Googlenet Caffe Model. Next, we derive a set of discriminative features by fusing the acquired features that form a feature vector representing the sign gesture in question. We have designed and used a Bidirectional Long Short-Term Memory Network (Bi-LSTM) for classification purpose. To test the efficacy of our proposed work, we applied our method on various publicly available continuous and isolated hand gesture datasets like ChaLearn LAP IsoGD, ChaLearn LAP ConGD, IPN Hand, and NVGesture. We observe in our experiments that our proposed method performs exceptionally well with several individual modalities as well as combination of modalities of these datasets. The combined effect of the proposed modality and movement epenthesis frames removal led to significant improvement in gesture recognition accuracy and considerable reduction in computational burden. Thus the obtained results advocate our proposed approach to be at par with the existing state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Nayan, Navneet and Ghosh, Debashis and Pradhan, Pyari Mohan},
  doi          = {10.1007/s00138-024-01565-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A multi-modal framework for continuous and isolated hand gesture recognition utilizing movement epenthesis detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual contrast discriminator with sharing attention for video
anomaly detection. <em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01566-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of video anomalies is a well-known issue in the realm of visual research. The volume of normal and abnormal sample data in this field is unbalanced, hence unsupervised training is generally used in research. Since the development of deep learning, the field of video anomaly has developed from reconstruction-based detection methods to prediction-based detection methods, and then to hybrid detection methods. To identify the presence of anomalies, these methods take advantage of the differences between ground-truth frames and reconstruction or prediction frames. Thus, the evaluation of the results is directly impacted by the quality of the generated frames. Built around the Dual Contrast Discriminator for Video Sequences (DCDVS) and the corresponding loss function, we present a novel hybrid detection method for further explanation. With less false positives and more accuracy, this method improves the discriminator’s guidance on the reconstruction-prediction network’s generation performance. we integrate optical flow processing and attention processes into the Auto-encoder (AE) reconstruction network. The network’s sensitivity to motion information and its ability to concentrate on important areas are improved by this integration. Additionally, DCDVS’s capacity to successfully recognize significant features gets improved by introducing the attention module implemented through parameter sharing. Aiming to reduce the risk of network overfitting, we also invented reverse augmentation, a data augmentation technique designed specifically for temporal data. Our approach achieved outstanding performance with AUC scores of 99.4, 92.9, and 77.3 $$\%$$ on the UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets, respectively, demonstrates competitiveness with advanced methods and validates its effectiveness.},
  archive      = {J_MVA},
  author       = {Zeng, Yiwenhao and Chen, Yihua and Yu, Songsen and Yang, Mingzhang and Chen, Rongrong and Xu, Fang},
  doi          = {10.1007/s00138-024-01566-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dual contrast discriminator with sharing attention for video anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Poly-cam: High resolution class activation map for
convolutional neural networks. <em>MVA</em>, <em>35</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01567-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for explainable AI continues to rise alongside advancements in deep learning technology. Existing methods such as convolutional neural networks often struggle to accurately pinpoint the image features justifying a network’s prediction due to low-resolution saliency maps (e.g., CAM), smooth visualizations from perturbation-based techniques, or numerous isolated peaky spots in gradient-based approaches. In response, our work seeks to merge information from earlier and later layers within the network to create high-resolution class activation maps that not only maintain a level of competitiveness with previous art in terms of insertion-deletion faithfulness metrics but also significantly surpass it regarding the precision in localizing class-specific features.},
  archive      = {J_MVA},
  author       = {Englebert, Alexandre and Cornu, Olivier and Vleeschouwer, Christophe De},
  doi          = {10.1007/s00138-024-01567-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Poly-cam: High resolution class activation map for convolutional neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MDUNet: Deep-prior unrolling network with multi-parameter
data integration for low-dose computed tomography reconstruction.
<em>MVA</em>, <em>35</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-024-01568-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this study is to reconstruct a high-quality computed tomography (CT) image from low-dose acquisition using an unrolling deep learning-based reconstruction network with less computational complexity and a more generalized model. We propose a MDUNet: Multi-parameters deep-prior unrolling network that employs the cascaded convolutional and deconvolutional blocks to unroll the model-based iterative reconstruction within a finite number of iterations by data-driven training. Furthermore, the embedded data consistency constraint in MDUNet ensures that the input low-dose images and the low-dose sinograms are consistent as well as incorporate the physics imaging geometry. Additionally, multi-parameter training was employed to enhance the model&#39;s generalization during the training process. Experimental results based on AAPM Low-dose CT datasets show that the proposed MDUNet significantly outperforms other state-of-the-art (SOTA) methods quantitatively and qualitatively. Also, the cascaded blocks reduce the computational complexity with reduced training parameters and generalize well on different datasets. In addition, the proposed MDUNet is validated on 8 different organs of interest, with more detailed structures recovered and high-quality images generated. The experimental results demonstrate that the proposed MDUNet generates favorable improvement over other competing methods in terms of visual quality, quantitative performance, and computational efficiency. The MDUNet has improved image quality with reduced computational cost and good generalization which effectively lowers radiation dose and reduces scanning time, making it favorable for future clinical deployment.},
  archive      = {J_MVA},
  author       = {Komolafe, Temitope Emmanuel and Wang, Nizhuan and Tian, Yuchi and Adeniji, Adegbola Oyedotun and Zhou, Liang},
  doi          = {10.1007/s00138-024-01568-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MDUNet: Deep-prior unrolling network with multi-parameter data integration for low-dose computed tomography reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance analysis of various deep learning models based
on max-min CNN for lung nodule classification on CT images.
<em>MVA</em>, <em>35</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01569-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains one of the leading causes of cancer-related deaths worldwide, underlining the urgent need for accurate and early detection and classification methods. In this paper, we present a comprehensive study that evaluates and compares different deep learning techniques for accurately distinguishing between nodule and non-nodule in 2D CT images. Our work introduced an innovative deep learning strategy called “Max-Min CNN” to improve lung nodule classification. Three models have been developed based on the Max-Min strategy: (1) a Max-Min CNN model built and trained from scratch, (2) a Bilinear Max-Min CNN composed of two Max-Min CNN streams whose outputs were bilinearly pooled by a Kronecker product, and (3) a hybrid Max-Min ViT combining a ViT model built from scratch and the proposed Max-Min CNN architecture as a backbone. To ensure an objective analysis of our findings, we evaluated each proposed model on 3186 images from the public LUNA16 database. Experimental results demonstrated the outperformance of the proposed hybrid Max-Min ViT over the Bilinear Max-Min CNN and the Max-Min CNN, with an accuracy rate of 98.03% versus 96.89% and 95.82%, respectively. This study clearly demonstrated the contribution of the Max-Min strategy in improving the effectiveness of deep learning models for pulmonary nodule classification on CT images.},
  archive      = {J_MVA},
  author       = {Mastouri, Rekka and Khlifa, Nawres and Neji, Henda and Hantous-Zannad, Saoussen},
  doi          = {10.1007/s00138-024-01569-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Performance analysis of various deep learning models based on max-min CNN for lung nodule classification on CT images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motioninsights: Real-time object tracking in streaming
video. <em>MVA</em>, <em>35</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01570-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MotionInsights facilitates object detection and tracking from multiple video streams in real-time. Leveraging the distributed stream processing capabilities of Apache Flink and Apache Kafka (as an intermediate message broker), the system models video processing as a data flow stream processing pipeline. Each video frame is split into smaller blocks, which are dispatched to be processed in parallel by a number of Flink operators. In the first stage, each block undergoes background subtraction and component labeling. The connected components from each frame are grouped, and the eligible components are merged into objects. In the last stage of the pipeline, all objects from each frame are concentrated to produce the trajectory of each object. The Flink application is deployed as a Kubernetes cluster in the Google Cloud Platform. Experimenting in a Flink cluster with 7 machines, revealed that MotionInsights achieves up to 6 times speedup compared to a monolithic (nonparallel) implementation while providing accurate trajectory patterns. The highest (i.e., more than 6 times) speed-up was observed with video streams of the highest resolution. Compared to existing systems that use custom or proprietary architectures, MotionInsights is independent of the underlying hardware platform and can be deployed on common CPU architectures and the cloud.},
  archive      = {J_MVA},
  author       = {Banelas, Dimitrios and Petrakis, Euripides G. M.},
  doi          = {10.1007/s00138-024-01570-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Motioninsights: Real-time object tracking in streaming video},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Adversarial defence by learning differentiated feature
representation in deep ensemble. <em>MVA</em>, <em>35</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-024-01571-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have been shown to be vulnerable to critical attacks under adversarial conditions. Attackers are able to generate powerful adversarial examples by searching for adversarial perturbations, without interfering with model training or directly modifying the model. This phenomenon indicates an endogenous problem in existing deep learning frameworks. Therefore, optimizing individual models for defense is often limited and can always be defeated by new attack methods. Ensemble defense has been shown to be effective in defending against adversarial attacks by combining diverse models. However, the problem of insufficient differentiation among existing models persists. Active defense in cyberspace security has successfully defended against unknown vulnerabilities by integrating subsystems with multiple different implementations to achieve a unified mission objective. Inspired by this, we propose exploring the feasibility of achieving model differentiation by changing the data features used in training individual models, as they are the core factor of functional implementation. We utilize several feature extraction methods to preprocess the data and train differentiated models based on these features. By generating adversarial perturbations to attack different models, we demonstrate that the feature representation of the data is highly resistant to adversarial perturbations. The entire ensemble is able to operate normally in an error-bearing environment.},
  archive      = {J_MVA},
  author       = {Chen, Xi and Huang, Wei and Guo, Wei and Zhang, Fan and Du, Jiayu and Zhou, Zhizhong},
  doi          = {10.1007/s00138-024-01571-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial defence by learning differentiated feature representation in deep ensemble},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning approaches to hand–eye calibration in
robots. <em>MVA</em>, <em>35</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s00138-024-01572-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of hand–eye calibration in robotic systems by developing Continual Learning (CL)-based approaches. Traditionally, robots require explicit models to transfer knowledge from camera observations to their hands or base. However, this poses limitations, as the hand–eye calibration parameters are typically valid only for the current camera configuration. We, therefore, propose a flexible and autonomous hand–eye calibration system that can adapt to changes in camera pose over time. Three CL-based approaches are introduced: the naive CL approach, the reservoir rehearsal approach, and the hybrid approach combining reservoir sampling with new data evaluation. The naive CL approach suffers from catastrophic forgetting, while the reservoir rehearsal approach mitigates this issue by sampling uniformly from past data. The hybrid approach further enhances performance by incorporating reservoir sampling and assessing new data for novelty. Experiments conducted in simulated and real-world environments demonstrate that the CL-based approaches, except for the naive approach, achieve competitive performance compared to traditional batch learning-based methods. This suggests that treating hand–eye calibration as a time sequence problem enables the extension of the learned space without complete retraining. The adaptability of the CL-based approaches facilitates accommodating changes in camera pose, leading to an improved hand–eye calibration system.},
  archive      = {J_MVA},
  author       = {Bahadir, Ozan and Siebert, Jan Paul and Aragon-Camarasa, Gerardo},
  doi          = {10.1007/s00138-024-01572-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Continual learning approaches to hand–eye calibration in robots},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards scanning electron microscopy image denoising: A
state-of-the-art overview, benchmark, taxonomies, and future direction.
<em>MVA</em>, <em>35</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-024-01573-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning electron microscope (SEM) enables imaging of micro-nano scale objects. It is an analytical tool widely used in the material, earth and life sciences. However, SEM images often suffer from high noise levels, influenced by factors such as dwell time, the time during which the electron beam remains per pixel during acquisition. Slower dwell times reduce noise but risk damaging the sample, while faster ones introduce uncertainty. To this end, the latest state-of-the-art denoising techniques must be explored. Experimentation is crucial to identify the most effective methods that balance noise reduction and sample preservation, ensuring high-quality SEM images with enhanced clarity and accuracy. A thorough analysis tracing the evolution of image denoising techniques was conducted, ranging from classical methods to deep learning approaches. A comprehensive taxonomy of this reverse problem solutions was established, detailing the developmental flow of these methods. Subsequently, the latest state-of-the-art techniques were identified and reviewed based on their reproducibility and the public availability of their source code. The selected techniques were then tested and investigated using scanning electron microscope images. After in-depth analysis and benchmarking, it is clear that the existing deep learning-based denoising techniques fall short in maintaining a balance between noise reduction and preserving crucial information for SEM images. Issues like information removal and over-smoothing have been identified. To address these constraints, there is a critical need for the development of SEM image denoising techniques that prioritize both noise reduction and information preservation. Additionally, one can see that the combination of several networks, such as the generative adversarial network and the convolutional neural network (CNN), known as BoostNet, or the vision transformer and the CNN, known as SCUNet, improves denoising performance. It is recommended to use blind techniques to denoise real noise while taking into account detail preservation and tackling excessive smoothing, particularly in the context of SEM. In the future the use of explainable AI will facilitate the debugging and the identification of these problems.},
  archive      = {J_MVA},
  author       = {Rahman, Sheikh Shah Mohammad Motiur and Salomon, Michel and Dembélé, Sounkalo},
  doi          = {10.1007/s00138-024-01573-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Towards scanning electron microscopy image denoising: A state-of-the-art overview, benchmark, taxonomies, and future direction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of data augmentation techniques on subjective
tasks. <em>MVA</em>, <em>35</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01574-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is widely applied in various computer vision problems for artificially increasing the size of a dataset by transforming the original data. These techniques are employed in small datasets to prevent overfitting, and also in problems where labelling is difficult. Nevertheless, data augmentation assumes that transformations preserve groundtruth labels, something not true for subjective problems such as aesthetic quality assessment, in which image transformations can alter their aesthetic quality groundtruth. In this work, we study how data augmentation affects subjective problems. We train a series of models, changing the probability of augmenting images and the intensity of such augmentations. We train models on AVA for quality prediction, on Photozilla for photo style prediction, and on subjective and objective labels of CelebA. Results show that subjective tasks get worse results than objective tasks with traditional augmentation techniques, and this worsening depends on the specific type of subjectivity.},
  archive      = {J_MVA},
  author       = {Gonzalez-Naharro, Luis and Flores, M. Julia and Martínez-Gómez, Jesus and Puerta, Jose M.},
  doi          = {10.1007/s00138-024-01574-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Evaluation of data augmentation techniques on subjective tasks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-managed network ensembles for video prediction.
<em>MVA</em>, <em>35</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01575-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative approach that leverages a tree structure to effectively manage a large ensemble of neural networks for tackling complex video prediction tasks. Our proposed method introduces a novel technique for partitioning the function domain into simpler subsets, enabling piecewise learning by the ensemble. Seamlessly accessed by an accompanying tree structure with a time complexity of O(log(N)), this ensemble-tree framework progressively expands while training examples become more complex. The tree construction process incorporates a specialized algorithm that utilizes localized comparison functions, learned at each decision node. To evaluate the effectiveness of our method, we conducted experiments in two challenging scenarios: action-conditional video prediction in a 3D video game environment and error detection in real-world 3D printing scenarios. Our approach consistently outperformed existing methods by a significant margin across various experiments. Additionally, we introduce a new evaluation methodology for long-term video prediction tasks, which demonstrates improved alignment with qualitative observations. The results highlight the efficacy and superiority of our ensemble-tree approach in addressing complex video prediction challenges.},
  archive      = {J_MVA},
  author       = {Fall, Everett and Chang, Kai-Wei and Chen, Liang-Gee},
  doi          = {10.1007/s00138-024-01575-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Tree-managed network ensembles for video prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online camera auto-calibration appliable to road
surveillance. <em>MVA</em>, <em>35</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01576-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera calibration is an essential prerequisite for road surveillance applications, which determines the accuracy of obtaining three-dimensional spatial information from surveillance video. The common practice for calibration is collecting the correspondences between the object points and their projections on surveillance, which usually needs to operate the calibrator manually. However, complex traffic and calibrator requirement limit the applicability of existing methods to road scenes. This paper proposes an online camera auto-calibration method for road surveillance to overcome the above problem. It constructs a large-scale virtual checkerboard adopting the road information from surveillance video, in which the structural size of the checkerboard can be easily obtained in advance because of the standardization for road design. The position coordinates of checkerboard corners are used for calibrating camera parameters, which is designed as a “coarse-to-fine” two-step procedure to recover the camera intrinsic and extrinsic parameters efficiently. Experimental results based on real datasets demonstrate that the proposed approach can accurately estimate camera parameters without manual involvement or additional information input. It achieves competitive effects on road surveillance auto-calibration while having lower requirements and computational costs than the automatic state-of-the-art.},
  archive      = {J_MVA},
  author       = {Guo, Shusen and Yu, Xianwen and Sha, Yuejin and Ju, Yifan and Zhu, Mingchen and Wang, Jiafu},
  doi          = {10.1007/s00138-024-01576-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Online camera auto-calibration appliable to road surveillance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IAFPN: Interlayer enhancement and multilayer fusion network
for object detection. <em>MVA</em>, <em>35</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s00138-024-01577-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature pyramid network (FPN) improves object detection performance by means of top-down multilevel feature fusion. However, the current FPN-based methods have not effectively utilized the interlayer features to suppress the aliasing effects in the feature downward fusion process. We propose an interlayer attention feature pyramid network that attempts to integrate attention gates into FPN through interlayer enhancement to establish the correlation between context and model, thereby highlighting the salient region of each layer and suppressing the aliasing effects. Moreover, in order to avoid feature dilution in the feature downward fusion process and inability of multilayer features to utilize each other, simplified non-local algorithm is used in the multilayer fusion module to fuse and enhance the multiscale features. A comprehensive analysis of MS COCO and PASCAL VOC benchmarks demonstrate that our network achieves precise object localization and also outperforms current FPN-based object detection algorithms.},
  archive      = {J_MVA},
  author       = {Li, Zhicheng and Yang, Chao and Jiang, Longyu},
  doi          = {10.1007/s00138-024-01577-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {IAFPN: Interlayer enhancement and multilayer fusion network for object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework of specialized knowledge distillation for
siamese tracker on challenging attributes. <em>MVA</em>, <em>35</em>(4),
1–18. (<a href="https://doi.org/10.1007/s00138-024-01578-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Siamese network-based trackers have achieved significant improvements in real-time tracking. Despite their success, performance bottlenecks caused by unavoidably complex scenarios in target-tracking tasks are becoming increasingly non-negligible. For example, occlusion and fast motion are factors that can easily cause tracking failures and are labeled in many high-quality tracking databases as challenging attributes. In addition, Siamese trackers tend to suffer from high memory costs, which restricts their applicability to mobile devices with tight memory budgets. To address these issues, we propose a Specialized teachers Distilled Siamese Tracker (SDST) framework to learn a student tracker, which is small, fast, and has enhanced performance in challenging attributes. SDST introduces two types of teachers for multi-teacher distillation: general teacher and specialized teachers. The former imparts basic knowledge to the students. The latter is used to transfer specialized knowledge to students, which helps improve their performance in challenging attributes. For students to efficiently capture critical knowledge from the two types of teachers, SDST is equipped with a carefully designed multi-teacher knowledge distillation model. Our model contains two processes: general teacher-student knowledge transfer and specialized teachers-student knowledge transfer. Extensive empirical evaluations of several popular Siamese trackers demonstrated the generality and effectiveness of our framework. Moreover, the results on Large-scale Single Object Tracking (LaSOT) show that the proposed method achieves a significant improvement of more than 2–4% in most challenging attributes. SDST also maintained high overall performance while achieving compression rates of up to 8x and framerates of 252 FPS and obtaining outstanding accuracy on all challenging attributes.},
  archive      = {J_MVA},
  author       = {Li, Yiding and Shimada, Atsushi and Minematsu, Tsubasa and Tang, Cheng},
  doi          = {10.1007/s00138-024-01578-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A framework of specialized knowledge distillation for siamese tracker on challenging attributes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adversarial sample detection method based on
heterogeneous denoising. <em>MVA</em>, <em>35</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01579-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been used in many computer-vision-based applications. However, deep neural networks are vulnerable to adversarial examples that have been crafted specifically to fool a system while being imperceptible to humans. In this paper, we propose a detection defense method based on heterogeneous denoising on foreground and background (HDFB). Since an image region that dominates to the output classification is usually sensitive to adversarial perturbations, HDFB focuses defense on the foreground region rather than the whole image. First, HDFB uses class activation map to segment examples into foreground and background regions. Second, the foreground and background are encoded to square patches. Third, the encoded foreground is zoomed in and out and is denoised in two scales. Subsequently, the encoded background is denoised once using bilateral filtering. After that, the denoised foreground and background patches are decoded. Finally, the decoded foreground and background are stitched together as a denoised sample for classification. If the classifications of the denoised and input images are different, the input image is detected as an adversarial example. The comparison experiments are implemented on CIFAR-10 and MiniImageNet. The average detection rate (DR) against white-box attacks on the test sets of the two datasets is 86.4%. The average DR against black-box attacks on MiniImageNet is 88.4%. The experimental results suggest that HDFB shows high performance on adversarial examples and is robust against white-box and black-box adversarial attacks. However, HDFB is insecure if its defense parameters are exposed to attackers.},
  archive      = {J_MVA},
  author       = {Zhu, Lifang and Liu, Chao and Zhang, Zhiqiang and Cheng, Yifan and Jie, Biao and Ding, Xintao},
  doi          = {10.1007/s00138-024-01579-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An adversarial sample detection method based on heterogeneous denoising},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GOA-net: Generic occlusion aware networks for visual
tracking. <em>MVA</em>, <em>35</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01580-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is a frequent phenomenon that hinders the task of visual object tracking. Since occlusion can be from any object and in any shape, data augmentation techniques will not greatly help identify or mitigate the tracker loss. Some of the existing works deal with occlusion only in an unsupervised manner. This paper proposes a generic deep learning framework for identifying occlusion in a given frame by formulating it as a supervised classification task for the first time. The proposed architecture introduces an “occlusion classification” branch into supervised trackers. This branch helps in the effective learning of features and also provides occlusion status for each frame. A metric is proposed to measure the performance of trackers under occlusion at frame level. The efficacy of the proposed framework is demonstrated on two supervised tracking paradigms: One is from the most commonly used Siamese region proposal class of trackers, and another from the emerging transformer-based trackers. This framework is tested on six diverse datasets (GOT-10k, LaSOT, OTB2015, TrackingNet, UAV123, and VOT2018), and it achieved significant improvements in performance over the corresponding baselines while performing on par with the state-of-the-art trackers. The contributions in this work are more generic, as any supervised tracker can easily adopt them.},
  archive      = {J_MVA},
  author       = {Dasari, Mohana Murali and Gorthi, Rama Krishna},
  doi          = {10.1007/s00138-024-01580-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {GOA-net: Generic occlusion aware networks for visual tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: Adversarial defence by learning differentiated
feature representation in deep ensemble. <em>MVA</em>, <em>35</em>(4),
1. (<a href="https://doi.org/10.1007/s00138-024-01583-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Chen, Xi and Huang, Wei and Guo, Wei and Zhang, Fan and Du, Jiayu and Zhou, Zhizhong},
  doi          = {10.1007/s00138-024-01583-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction: Adversarial defence by learning differentiated feature representation in deep ensemble},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rocnet: 3D robust registration of points clouds using deep
learning. <em>MVA</em>, <em>35</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-024-01584-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new method for 3D points cloud registration based on deep learning. The architecture is composed of three distinct blocs: (i) an encoder with a convolutional graph-based descriptor that encodes the immediate neighborhood of each point and an attention mechanism that encodes the variations of the surface normals. Such descriptors are refined by highlighting attention between the points of the same set (source and target) and then between the points of the two sets. (ii) a matching process that estimates a matrix of correspondences using the Sinkhorn algorithm. (iii) Finally, the rigid transformation between the two points clouds is calculated by RANSAC using the best scores of the correspondence matrix. We conduct experiments on the ModelNet40 and real-world Bunny datasets, and our proposed architecture shows promising results, outperforming state-of-the-art methods in most simulated configurations.},
  archive      = {J_MVA},
  author       = {Slimani, Karim and Tamadazte, Brahim and Achard, Catherine},
  doi          = {10.1007/s00138-024-01584-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Rocnet: 3D robust registration of points clouds using deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). AFMCT: Adaptive fusion module based on cross-modal
transformer block for 3D object detection. <em>MVA</em>, <em>35</em>(3),
1–13. (<a href="https://doi.org/10.1007/s00138-024-01509-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lidar and camera are essential sensors for environment perception in autonomous driving. However, fully fusing heterogeneous data from multiple sources remains a non-trivial challenge. As a result, 3D object detection based on multi-modal sensor fusion are often inferior to single-modal methods only based on Lidar, which indicates that multi-sensor machine vision still needs development. In this paper, we propose an adaptive fusion module based on cross-modal transformer block(AFMCT) for 3D object detection by utilizing a bidirectional enhancing strategy. Specifically, we first enhance image feature by extracting an attention-based point feature based on a cross-modal transformer block and linking them in a concatenation fashion, followed by another cross-modal transformer block acting on the enhanced image feature to strengthen the point feature with image semantic information. Extensive experiments operated on the 3D detection benchmark of the KITTI dataset reveal that our proposed structure can significantly improve the detection accuracy of Lidar-only methods and outperform the existing advanced multi-sensor fusion modules by at least 0.45%, which indicates that our method might be a feasible solution to improving 3D object detection based on multi-sensor fusion.},
  archive      = {J_MVA},
  author       = {Zhang, Bingli and Wang, Yixin and Zhang, Chengbiao and Jiang, Junzhao and Pan, Zehao and Cheng, Jin and Zhang, Yangyang and Wang, Xinyu and Yang, Chenglei and Wang, Yanhui},
  doi          = {10.1007/s00138-024-01509-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {AFMCT: Adaptive fusion module based on cross-modal transformer block for 3D object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing the generalization of 3D registration methods
with a featureless baseline and an unbiased benchmark. <em>MVA</em>,
<em>35</em>(3), 1–30. (<a
href="https://doi.org/10.1007/s00138-024-01510-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent 3D registration methods are mostly learning-based that either find correspondences in feature space and match them, or directly estimate the registration transformation from the given point cloud features. Therefore, these feature-based methods have difficulties with generalizing onto point clouds that differ substantially from their training data. This issue is not so apparent because of the problematic benchmark definitions that cannot provide any in-depth analysis and contain a bias toward similar data. Therefore, we propose a methodology to create a 3D registration benchmark, given a point cloud dataset, that provides a more informative evaluation of a method w.r.t. other benchmarks. Using this methodology, we create a novel FAUST-partial (FP) benchmark, based on the FAUST dataset, with several difficulty levels. The FP benchmark addresses the limitations of the current benchmarks: lack of data and parameter range variability, and allows to evaluate the strengths and weaknesses of a 3D registration method w.r.t. a single registration parameter. Using the new FP benchmark, we provide a thorough analysis of the current state-of-the-art methods and observe that the current method still struggle to generalize onto severely different out-of-sample data. Therefore, we propose a simple featureless traditional 3D registration baseline method based on the weighted cross-correlation between two given point clouds. Our method achieves strong results on current benchmarking datasets, outperforming most deep learning methods. Our source code is available on github.com/DavidBoja/exhaustive-grid-search.},
  archive      = {J_MVA},
  author       = {Bojanić, David and Bartol, Kristijan and Forest, Josep and Petković, Tomislav and Pribanić, Tomislav},
  doi          = {10.1007/s00138-024-01510-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Addressing the generalization of 3D registration methods with a featureless baseline and an unbiased benchmark},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ssman: Self-supervised masked adaptive network for 3D human
pose estimation. <em>MVA</em>, <em>35</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01514-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modern deep learning-based models for 3D human pose estimation from monocular images always lack the adaption ability between occlusion and non-occlusion scenarios, which might restrict the performance of current methods when faced with various scales of occluded conditions. In an attempt to tackle this problem, we propose a novel network called self-supervised masked adaptive network (SSMAN). Firstly, we leverage different levels of masks to cover the richness of occlusion in fully in-the-wild environment. Then, we design a multi-line adaptive network, which could be trained with various scales of masked images in parallel. Based on this masked adaptive network, we train it with self-supervised learning to enforce the consistency across the outputs under different mask ratios. Furthermore, a global refinement module is proposed to leverage global features of the human body to refine the human pose estimated solely by local features. We perform extensive experiments both on the occlusion datasets like 3DPW-OCC and OCHuman and general datasets such as Human3.6M and 3DPW. The results show that SSMAN achieves new state-of-the-art performance on both lightly and heavily occluded benchmarks and is highly competitive with significant improvement on standard benchmarks.},
  archive      = {J_MVA},
  author       = {Shi, Yu and Yue, Tianyi and Zhao, Hu and He, Guoping and Ren, Keyan},
  doi          = {10.1007/s00138-024-01514-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ssman: Self-supervised masked adaptive network for 3D human pose estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud registration with quantile assignment.
<em>MVA</em>, <em>35</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01517-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a fundamental problem in computer vision. The problem encompasses critical tasks such as feature estimation, correspondence matching, and transformation estimation. The point cloud registration problem can be cast as a quantile matching problem. We refined the quantile assignment algorithm by integrating prevalent feature descriptors and transformation estimation methods to enhance the correspondence between the source and target point clouds. We evaluated the performances of these descriptors and methods with our approach through controlled experiments on a dataset we constructed using well-known 3D models. This systematic investigation led us to identify the most suitable methods for complementing our approach. Subsequently, we devised a new end-to-end, coarse-to-fine pairwise point cloud registration framework. Finally, we tested our framework on indoor and outdoor benchmark datasets and compared our results with state-of-the-art point cloud registration methods.},
  archive      = {J_MVA},
  author       = {Oğuz, Ecenur and Doğan, Yalım and Güdükbay, Uğur and Karaşan, Oya and Pınar, Mustafa},
  doi          = {10.1007/s00138-024-01517-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Point cloud registration with quantile assignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial robustness improvement for deep neural networks.
<em>MVA</em>, <em>35</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s00138-024-01519-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are key components for the implementation of autonomy in systems that operate in highly complex and unpredictable environments (self-driving cars, smart traffic systems, smart manufacturing, etc.). It is well known that DNNs are vulnerable to adversarial examples, i.e. minimal and usually imperceptible perturbations, applied to their inputs, leading to false predictions. This threat poses critical challenges, especially when DNNs are deployed in safety or security-critical systems, and renders as urgent the need for defences that can improve the trustworthiness of DNN functions. Adversarial training has proven effective in improving the robustness of DNNs against a wide range of adversarial perturbations. However, a general framework for adversarial defences is needed that will extend beyond a single-dimensional assessment of robustness improvement; it is essential to consider simultaneously several distance metrics and adversarial attack strategies. Using such an approach we report the results from extensive experimentation on adversarial defence methods that could improve DNNs resilience to adversarial threats. We wrap up by introducing a general adversarial training methodology, which, according to our experimental results, opens prospects for an holistic defence against a range of diverse types of adversarial perturbations.},
  archive      = {J_MVA},
  author       = {Eleftheriadis, Charis and Symeonidis, Andreas and Katsaros, Panagiotis},
  doi          = {10.1007/s00138-024-01519-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial robustness improvement for deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PTDS CenterTrack: Pedestrian tracking in dense scenes with
re-identification and feature enhancement. <em>MVA</em>, <em>35</em>(3),
1–18. (<a href="https://doi.org/10.1007/s00138-024-01520-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking in dense scenes has always been a major difficulty in this field. Although some existing algorithms achieve excellent results in multi-object tracking, they fail to achieve good generalization when the application background is transferred to more challenging dense scenarios. In this work, we propose PTDS(Pedestrian Tracking in Dense Scene) CenterTrack based on the CenterTrack for object center point detection and tracking. It utilizes dense inter-frame similarity to perform object appearance feature comparisons to predict the inter-frame position changes of objects, extending CenterTrack by using only motion features. We propose a feature enhancement method based on a hybrid attention mechanism, which adds information on the temporal dimension between frames to the features required for object detection, and connects the two tasks of detection and tracking. Under the MOT20 benchmark, PTDS CenterTrack has achieved 55.6%MOTA, 55.1%IDF1, 45.1%HOTA, which is an increase of 10.1 percentage points, 4.0 percentage points, and 4.8 percentage points respectively compared to CenterTrack.},
  archive      = {J_MVA},
  author       = {Wen, Jiazheng and Liu, Huanyu and Li, Junbao},
  doi          = {10.1007/s00138-024-01520-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {PTDS CenterTrack: Pedestrian tracking in dense scenes with re-identification and feature enhancement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal teacher with masked transformers for
semi-supervised action proposal generation. <em>MVA</em>,
<em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01521-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By conditioning on unit-level predictions, anchor-free models for action proposal generation have displayed impressive capabilities, such as having a lightweight architecture. However, task performance depends significantly on the quality of data used in training, and most effective models have relied on human-annotated data. Semi-supervised learning, i.e., jointly training deep neural networks with a labeled dataset as well as an unlabeled dataset, has made significant progress recently. Existing works have either primarily focused on classification tasks, which may require less annotation effort, or considered anchor-based detection models. Inspired by recent advances in semi-supervised methods on anchor-free object detectors, we propose a teacher-student framework for a two-stage action detection pipeline, named Temporal Teacher with Masked Transformers (TTMT), to generate high-quality action proposals based on an anchor-free transformer model. Leveraging consistency learning as one self-training technique, the model jointly trains an anchor-free student model and a gradually progressing teacher counterpart in a mutually beneficial manner. As the core model, we design a Transformer-based anchor-free model to improve effectiveness for temporal evaluation. We integrate bi-directional masks and devise encoder-only Masked Transformers for sequences. Jointly training on boundary locations and various local snippet-based features, our model predicts via the proposed scoring function for generating proposal candidates. Experiments on the THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of our model for temporal proposal generation task.},
  archive      = {J_MVA},
  author       = {Pehlivan, Selen and Laaksonen, Jorma},
  doi          = {10.1007/s00138-024-01521-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Temporal teacher with masked transformers for semi-supervised action proposal generation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An image quality assessment method based on edge extraction
and singular value for blurriness. <em>MVA</em>, <em>35</em>(3), 1–13.
(<a href="https://doi.org/10.1007/s00138-024-01522-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic assessment of perceived image quality is crucial in the field of image processing. To achieve this idea, we propose an image quality assessment (IQA) method for blurriness. The features of gradient and singular value were extracted in this method instead of the single feature in the traditional IQA algorithms. According to the insufficient size of existing public image quality assessment datasets to support deep learning, machine learning was introduced to fuse the features of multiple domains, and a new no-reference (NR) IQA method for blurriness denoted Feature fusion IQA(Ffu-IQA) was proposed. The Ffu-IQA uses a probabilistic model to estimate the probability of each edge detection blur in the image, and then uses machine learning to aggregate the probability information to obtain the edge quality score. After that uses the singular value obtained by singular value decomposition of the image matrix to calculate the singular value score. Finally, machine learning pooling is used to obtain the true quality score. Ffu-IQA achieves PLCC scores of 0.9570 and 0.9616 on CSIQ and TID2013, respectively, and SROCC scores of 0.9380 and 0.9531, which are better than most traditional image quality assessment methods for blurriness.},
  archive      = {J_MVA},
  author       = {Zhou, Lei and Liu, Chuanlin and Yadav, Amit and Azam, Sami and Karim, Asif},
  doi          = {10.1007/s00138-024-01522-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An image quality assessment method based on edge extraction and singular value for blurriness},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral image dynamic range reconstruction using deep
neural network-based denoising methods. <em>MVA</em>, <em>35</em>(3),
1–14. (<a href="https://doi.org/10.1007/s00138-024-01523-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) measurement is among the most useful tools in agriculture for early disease detection. However, the cost of HS cameras that can perform the desired detection tasks is prohibitive-typically fifty thousand to hundreds of thousands of dollars. In a previous study at the Agricultural Research Organization’s Volcani Institute (Israel), a low-cost, high-performing HS system was developed which included a point spectrometer and optical components. Its main disadvantage was long shooting time for each image. Shooting time strongly depends on the predetermined integration time of the point spectrometer. While essential for performing monitoring tasks in a reasonable time, shortening integration time from a typical value in the range of 200 ms to the 10 ms range results in deterioration of the dynamic range of the captured scene. In this work, we suggest correcting this by learning the transformation from data measured with short integration time to that measured with long integration time. Reduction of the dynamic range and consequent low SNR were successfully overcome using three developed deep neural networks models based on a denoising auto-encoder, DnCNN and LambdaNetworks architectures as a backbone. The best model was based on DnCNN using a combined loss function of $$\ell _{2}$$ and Kullback–Leibler divergence on images with 20 consecutive channels. The full spectrum of the model achieved a mean PSNR of 30.61 and mean SSIM of 0.9, showing total improvement relatively to the 10 ms measurements’ mean PSNR and mean SSIM values by 60.43% and 94.51%, respectively.},
  archive      = {J_MVA},
  author       = {Cheplanov, Loran and Avidan, Shai and Bonfil, David J. and Klapp, Iftach},
  doi          = {10.1007/s00138-024-01523-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hyperspectral image dynamic range reconstruction using deep neural network-based denoising methods},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel based local matching network for video object
segmentation. <em>MVA</em>, <em>35</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-024-01524-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the methods based on space-time memory network have achieved advanced performance in semi-supervised video object segmentation, which has attracted wide attention. However, this kind of methods still have a fatal limitation. It has the interference problem of similar objects caused by the way of non-local matching, which seriously limits the performance of video object segmentation. To solve this problem, we propose a Kernel-guided Attention Matching Network (KAMNet) by the use of local matching instead of non-local matching. At first, KAMNet uses spatio-temporal attention mechanism to enhance the model’s discrimination between foreground objects and background areas. Then KAMNet utilizes gaussian kernel to guide the matching between the current frame and the reference set. Because the gaussian kernel decays away from the center, it can limit the matching to the central region, thus achieving local matching. Our KAMNet gets speed-accuracy trade-off on benchmark datasets DAVIS 2016 ( $$ \mathcal {J \&amp; F}$$ of 87.6%) and DAVIS 2017 ( $$ \mathcal {J \&amp; F}$$ of 76.0%) with 0.12 second per frame.},
  archive      = {J_MVA},
  author       = {Wang, Guoqiang and Li, Lan and Zhu, Min and Zhao, Rui and Zhang, Xiang},
  doi          = {10.1007/s00138-024-01524-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Kernel based local matching network for video object segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOMH: You only look once for multi-task driving perception
with high efficiency. <em>MVA</em>, <em>35</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s00138-024-01525-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the requirements of high accuracy, lightweight and real-time performance of the panoptic driving perception system, this paper proposes an efficient multi-task network (YOLOMH). The network uses a shared encoder and three independent decoding heads to simultaneously complete the three major panoptic driving perception tasks of traffic object detection, road drivable area segmentation and road lane segmentation. Thanks to our innovative design of the YOLOMH network structure: first, we design an appropriate information input structure based on the different information requirements between different tasks, and secondly, we propose a Hybrid Deep Atrous Spatial Pyramid Pooling module to efficiently complete the feature fusion work of the neck network, and finally effective approaches such as Anchor-free detection head and Depthwise Separable Convolution are introduced into the network, making the network more efficient while being lightweight. Experimental results show that our model achieves competitive results in both accuracy and speed on the challenging BDD100K dataset, especially in terms of inference speed, The model’s inference speed on NVIDIA TESLA V100 is as high as 107 Frames Per Second (FPS), far exceeding the 49 FPS of the YOLOP network under the same experimental settings. This well meets the requirements of autonomous vehicles for high system accuracy and low latency.},
  archive      = {J_MVA},
  author       = {Fang, Liu and Bowen, Sun and Jianxi, Miao and Weixing, Su},
  doi          = {10.1007/s00138-024-01525-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {YOLOMH: You only look once for multi-task driving perception with high efficiency},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keyframe-based RGB-d dense visual SLAM fused semantic cues
in dynamic scenes. <em>MVA</em>, <em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01526-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of dense visual SLAM is still a challenging problem in dynamic environments. In this paper, we propose a novel keyframe-based dense visual SLAM to handle a highly dynamic environment by using an RGB-D camera. The proposed method uses cluster-based residual models and semantic cues to detect dynamic objects, resulting in motion segmentation that outperforms traditional methods. The method also employs motion-segmentation based keyframe selection strategies and frame-to-keyframe matching scheme that reduce the influence of dynamic objects, thus minimizing trajectory errors. We further filter out dynamic object influence based on motion segmentation and then employ true matches from keyframes, which are near the current keyframe, to facilitate loop closure. Finally, a pose graph is established and optimized using the g2o framework. Our experimental results demonstrate the success of our approach in handling highly dynamic sequences, as evidenced by the more robust motion segmentation results and significantly lower trajectory drift compared to several state-of-the-art dense visual odometry or SLAM methods on challenging public benchmark datasets.},
  archive      = {J_MVA},
  author       = {Zhou, Wugen and Peng, Xiaodong and Li, Yun and Fan, Mingrui and Liu, Bo},
  doi          = {10.1007/s00138-024-01526-2},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Keyframe-based RGB-D dense visual SLAM fused semantic cues in dynamic scenes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The improvement of ground truth annotation in public
datasets for human detection. <em>MVA</em>, <em>35</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01527-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of annotations in the datasets is crucial for supervised machine learning as it significantly affects the performance of models. While many public datasets are widely used, they often suffer from annotations errors, including missing annotations, incorrect bounding box sizes, and positions. It results in low accuracy of machine learning models. However, most researchers have traditionally focused on improving model performance by enhancing algorithms, while overlooking concerns regarding data quality. This so-called model-centric AI approach has been predominant. In contrast, a data-centric AI approach, advocated by Andrew Ng at the DATA and AI Summit 2022, emphasizes enhancing data quality while keeping the model fixed, which proves to be more efficient in improving performance. Building upon this data-centric approach, we propose a method to enhance the quality of public datasets such as MS-COCO and Open Image Dataset. Our approach involves automatically retrieving missing annotations and correcting the size and position of existing bounding boxes in these datasets. Specifically, our study deals with human object detection, which is one of the prominent applications of artificial intelligence. Experimental results demonstrate improved performance with models such as Faster-RCNN, EfficientDet, and RetinaNet. We can achieve up to 32% compared to original datasets in the term of mAP after applying both proposed methods to dataset which is transformed the grouped of instances to individual instance. In summary, our methods significantly enhance the model’s performance by improving the quality of annotations at a lower cost with less time than manual improvement employed in other studies.},
  archive      = {J_MVA},
  author       = {Nou, Sotheany and Lee, Joong-Sun and Ohyama, Nagaaki and Obi, Takashi},
  doi          = {10.1007/s00138-024-01527-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {The improvement of ground truth annotation in public datasets for human detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). USIR-net: Sand-dust image restoration based on unsupervised
learning. <em>MVA</em>, <em>35</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01528-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sand-dust weather, the influence of sand-dust particles on imaging equipment often results in images with color deviation, blurring, and low contrast, among other issues. These problems making many traditional image restoration methods unable to accurately estimate the semantic information of the images and consequently resulting in poor restoration of clear images. Most current image restoration methods in the field of deep learning are based on supervised learning, which requires pairing and labeling a large amount of data, and the possibility of manual annotation errors. In light of this, we propose an unsupervised sand-dust image restoration network. The overall model adopts an improved CycleGAN to fit unpaired sand-dust images. Firstly, multiscale skip connections in the multiscale cascaded attention module are used to enhance the feature fusion effect after downsampling. Secondly, multi-head convolutional attention with multiple input concatenations is employed, with each head using different kernel sizes to improve the ability to restore detail information. Finally, the adaptive decoder-encoder module is used to achieve adaptive fitting of the model and output the restored image. According to the experiments conducted on the dataset, the qualitative and quantitative indicators of USIR-Net are superior to the selected comparison algorithms, furthermore, in additional experiments conducted on haze removal and underwater image enhancement, we have demonstrated the wide applicability of our model.},
  archive      = {J_MVA},
  author       = {Ding, Yuan and Wu, Kaijun},
  doi          = {10.1007/s00138-024-01528-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {USIR-net: Sand-dust image restoration based on unsupervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DisRot: Boosting the generalization capability of few-shot
learning via knowledge distillation and self-supervised learning.
<em>MVA</em>, <em>35</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01529-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to adapt quickly to new categories with limited samples. Despite significant progress in utilizing meta-learning for solving FSL tasks, challenges such as overfitting and poor generalization still exist. Building upon the demonstrated significance of powerful feature representation, this work proposes disRot, a novel two-strategy training mechanism, which combines knowledge distillation and rotation prediction task for the pre-training phase of transfer learning. Knowledge distillation enables shallow networks to learn relational knowledge contained in deep networks, while the self-supervised rotation prediction task provides class-irrelevant and transferable knowledge for the supervised task. Simultaneous optimization for these two tasks allows the model learn generalizable and transferable feature embedding. Extensive experiments on the miniImageNet and FC100 datasets demonstrate that disRot can effectively improve the generalization ability of the model and is comparable to the leading FSL methods.},
  archive      = {J_MVA},
  author       = {Ma, Chenyu and Jia, Jinfang and Huang, Jianqiang and Wu, Li and Wang, Xiaoying},
  doi          = {10.1007/s00138-024-01529-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DisRot: Boosting the generalization capability of few-shot learning via knowledge distillation and self-supervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-person 3D pose estimation from unlabelled data.
<em>MVA</em>, <em>35</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01530-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Its numerous applications make multi-human 3D pose estimation a remarkably impactful area of research. Nevertheless, it presents several challenges, especially when approached using multiple views and regular RGB cameras as the only input. First, each person must be uniquely identified in the different views. Secondly, it must be robust to noise, partial occlusions, and views where a person may not be detected. Thirdly, many pose estimation approaches rely on environment-specific annotated datasets that are frequently prohibitively expensive and/or require specialised hardware. Specifically, this is the first multi-camera, multi-person data-driven approach that does not require an annotated dataset. In this work, we address these three challenges with the help of self-supervised learning. In particular, we present a three-staged pipeline and a rigorous evaluation providing evidence that our approach performs faster than other state-of-the-art algorithms, with comparable accuracy, and most importantly, does not require annotated datasets. The pipeline is composed of a 2D skeleton detection step, followed by a Graph Neural Network to estimate cross-view correspondences of the people in the scenario, and a Multi-Layer Perceptron that transforms the 2D information into 3D pose estimations. Our proposal comprises the last two steps, and it is compatible with any 2D skeleton detector as input. These two models are trained in a self-supervised manner, thus avoiding the need for datasets annotated with 3D ground-truth poses.},
  archive      = {J_MVA},
  author       = {Rodriguez-Criado, Daniel and Bachiller-Burgos, Pilar and Vogiatzis, George and Manso, Luis J.},
  doi          = {10.1007/s00138-024-01530-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-person 3D pose estimation from unlabelled data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). BoostTrack: Boosting the similarity measure and detection
confidence for improved multiple object tracking. <em>MVA</em>,
<em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01531-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling unreliable detections and avoiding identity switches are crucial for the success of multiple object tracking (MOT). Ideally, MOT algorithm should use true positive detections only, work in real-time and produce no identity switches. To approach the described ideal solution, we present the BoostTrack, a simple yet effective tracing-by-detection MOT method that utilizes several lightweight plug and play additions to improve MOT performance. We design a detection-tracklet confidence score and use it to scale the similarity measure and implicitly favour high detection confidence and high tracklet confidence pairs in one-stage association. To reduce the ambiguity arising from using intersection over union (IoU), we propose a novel Mahalanobis distance and shape similarity additions to boost the overall similarity measure. To utilize low-detection score bounding boxes in one-stage association, we propose to boost the confidence scores of two groups of detections: the detections we assume to correspond to the existing tracked object, and the detections we assume to correspond to a previously undetected object. The proposed additions are orthogonal to the existing approaches, and we combine them with interpolation and camera motion compensation to achieve results comparable to the standard benchmark solutions while retaining real-time execution speed. When combined with appearance similarity, our method outperforms all standard benchmark solutions on MOT17 and MOT20 datasets. It ranks first among online methods in HOTA metric in the MOT Challenge on MOT17 and MOT20 test sets. We make our code available at https://github.com/vukasin-stanojevic/BoostTrack .},
  archive      = {J_MVA},
  author       = {Stanojevic, Vukasin D. and Todorovic, Branimir T.},
  doi          = {10.1007/s00138-024-01531-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {BoostTrack: Boosting the similarity measure and detection confidence for improved multiple object tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor-guided learning for image denoising using anisotropic
PDEs. <em>MVA</em>, <em>35</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s00138-024-01532-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce an advanced approach for enhanced image denoising using an improved space-variant anisotropic Partial Differential Equation (PDE) framework. Leveraging Weickert-type operators, this method relies on two critical parameters: $$\lambda $$ and $$\theta $$ , defining local image geometry and smoothing strength. We propose an automatic parameter estimation technique rooted in PDE-constrained optimization, incorporating supplementary information from the original clean image. By combining these components, our approach achieves superior image denoising, pushing the boundaries of image enhancement methods. We employed a modified Alternating Direction Method of Multipliers (ADMM) procedure for numerical optimization, demonstrating its efficacy through thorough assessments and affirming its superior performance compared to alternative denoising methods.},
  archive      = {J_MVA},
  author       = {Limami, Fakhr-eddine and Hadri, Aissam and Afraites, Lekbir and Laghrib, Amine},
  doi          = {10.1007/s00138-024-01532-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Tensor-guided learning for image denoising using anisotropic PDEs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lunar ground segmentation using a modified u-net neural
network. <em>MVA</em>, <em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01533-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation plays a significant role in unstructured and planetary scene understanding, offering to a robotic system or a planetary rover valuable knowledge about its surroundings. Several studies investigate rover-based scene recognition planetary-like environments but there is a lack of a semantic segmentation architecture, focused on computing systems with low resources and tested on the lunar surface. In this study, a lightweight encoder-decoder neural network (NN) architecture is proposed for rover-based ground segmentation on the lunar surface. The proposed architecture is composed by a modified MobilenetV2 as encoder and a lightweight U-net decoder while the training and evaluation process were conducted using a publicly available synthetic dataset with lunar landscape images. The proposed model provides robust segmentation results, allowing the lunar scene understanding focused on rocks and boulders. It achieves similar accuracy, compared with original U-net and U-net-based architectures which are 110–140 times larger than the proposed architecture. This study, aims to contribute in lunar landscape segmentation utilizing deep learning techniques, while it proves a great potential in autonomous lunar navigation ensuring a safer and smoother navigation on the moon. To the best of our knowledge, this is the first study which propose a lightweight semantic segmentation architecture for the lunar surface, aiming to reinforce the autonomous rover navigation.},
  archive      = {J_MVA},
  author       = {Petrakis, Georgios and Partsinevelos, Panagiotis},
  doi          = {10.1007/s00138-024-01533-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Lunar ground segmentation using a modified U-net neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniGlasses: An optical aid for stereo vision CNNs to enable
omnidirectional image processing. <em>MVA</em>, <em>35</em>(3), 1–15.
(<a href="https://doi.org/10.1007/s00138-024-01534-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo vision is a key technology for 3D scene reconstruction from image pairs. Most approaches process perspective images from commodity cameras. These images, however, have a very limited field of view and only picture a small portion of the scene. In contrast, omnidirectional images, also known as fisheye images, exhibit a much larger field of view and allow a full 3D scene reconstruction with a small amount of cameras if placed carefully. However, omnidirectional images are strongly distorted which make the 3D reconstruction much more sophisticated. Nowadays, a lot of research is conducted on CNNs for omnidirectional stereo vision. Nevertheless, a significant gap between estimation accuracy and throughput can be observed in the literature. This work aims to bridge this gap by introducing a novel set of transformations, namely OmniGlasses. These are incorporated into the architecture of a fast network, i.e., AnyNet, originally designed for scene reconstruction on perspective images. Our network, Omni-AnyNet, produces accurate omnidirectional distance maps with a mean absolute error of around 13 cm at 48.4 fps and is therefore real-time capable.},
  archive      = {J_MVA},
  author       = {Seuffert, Julian B. and Perez Grassi, Ana C. and Ahmed, Hamza and Seidel, Roman and Hirtz, Gangolf},
  doi          = {10.1007/s00138-024-01534-2},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {OmniGlasses: An optical aid for stereo vision CNNs to enable omnidirectional image processing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AP-TransNet: A polarized transformer based aerial human
action recognition framework. <em>MVA</em>, <em>35</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01535-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones are widespread and actively employed in a variety of applications due to their low cost and quick mobility and enabling new forms of action surveillance. However, owing to various challenges- limited no. of aerial view samples, aerial footage suffers with camera motion, illumination changes, small actor size, occlusion, complex backgrounds, and varying view angles, human action recognition in aerial videos even more challenging. Maneuvering the same, we propose Aerial Polarized-Transformer Network (AP-TransNet) to recognize human actions in aerial view using both spatial and temporal details of the video feed. In this paper, we present the Polarized Encoding Block that performs ( $${\text{i}})$$ Selection with Rejection to select the significant features and reject least informative features similar to Light photometry phenomena and ( $${\text{ii}})$$ boosting operation increases the dynamic range of encodings using non-linear softmax normalization at the bottleneck tensors in both channel and spatial sequential branches. The performance of the proposed AP-TransNet is evaluated by conducting extensive experiments on three publicly available benchmark datasets: drone action dataset, UCF-ARG Dataset and Multi-View Outdoor Dataset (MOD20) supporting with ablation study. The proposed work outperformed the state-of-the-arts.},
  archive      = {J_MVA},
  author       = {Dhiman, Chhavi and Varshney, Anunay and Vyapak, Ved},
  doi          = {10.1007/s00138-024-01535-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {AP-TransNet: A polarized transformer based aerial human action recognition framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Study on defect detection of metal castings based on
supervised enhancement and attention distillation. <em>MVA</em>,
<em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01536-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metal castings are essential parts and their quality greatly impacts the product’s performance. Current techniques for identifying surface defects in metal castings usually suffer from issues like inadequate accuracy in defect recognition. In comparison to other models, the performance of the Deformable DETR model in detecting defects in metal castings is commendable, yet its attentional mechanism inherently demands a larger sample size and exhibits a slower convergence rate. However, the dataset collected in this paper for metal castings possesses a limited number of samples and imbalanced data, which consequently limits the overall effectiveness of the Deformable DETR model on this task. Responding to the above, a supervised enhancement algorithm for one-to-many assignment based on the Deformable DETR was proposed in this paper. This algorithm accelerates convergence speed and reduces data requirements while eliminating Non-Maximum Suppression post-processing. Applying the supervised enhancement algorithm to DETR on this dataset also improves defect recall. In addition, attention distillation was applied to the deformable attention mechanism to reduce time and space complexity to O(Llog(L)). It creates favourable conditions for the generation of attention weights, which are changed from a linear transformation of query to query-key interaction, and emphasises casting defects while ensuring global attention. Furthermore, data processing methods such as image slicing and data augmentation are also used to enhance casting defect detection ability in this paper. Finally, the recall rate and precision rate of metal casting defects have been improved, increasing to 97.7% and 85.0%, respectively.},
  archive      = {J_MVA},
  author       = {Pan, Haixia and Zhao, Han and Wei, Xingyun and Zhang, Dongdong and Dong, Biao and Lan, Jiahua},
  doi          = {10.1007/s00138-024-01536-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Study on defect detection of metal castings based on supervised enhancement and attention distillation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ET-PointPillars: Improved PointPillars for 3D object
detection based on optimized voxel downsampling. <em>MVA</em>,
<em>35</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-024-01538-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preprocessing of point cloud data has always been an important problem in 3D object detection. Due to the large volume of point cloud data, voxelization methods are often used to represent the point cloud while reducing data density. However, common voxelization randomly selects sampling points from voxels, which often fails to represent local spatial features well due to noise. To preserve local features, this paper proposes an optimized voxel downsampling(OVD) method based on evidence theory. This method uses fuzzy sets to model basic probability assignments (BPAs) for each candidate point, incorporating point location information. It then employs evidence theory to fuse the BPAs and determine the selected sampling points. In the PointPillars 3D object detection algorithm, the point cloud is partitioned into pillars and encoded using each pillar’s points. Convolutional neural networks are used for feature extraction and detection. Another contribution is the proposed improved PointPillars based on evidence theory (ET-PointPillars) by introducing an OVD-based feature point sampling module in the PointPillars’ pillar feature network, which can select feature points in pillars using the optimized method, computes offsets to these points, and adds them as features to facilitate learning more object characteristics, improving traditional PointPillars. Experiments on the KITTI datasets validate the method’s ability to preserve local spatial features. Results showed improved detection precision, with a $$2.73\%$$ average increase for pedestrians and cyclists on KITTI.},
  archive      = {J_MVA},
  author       = {Liu, Yiyi and Yang, Zhengyi and Tong, JianLin and Yang, Jiajia and Peng, Jiongcheng and Zhang, Lihang and Cheng, Wangxin},
  doi          = {10.1007/s00138-024-01538-y},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ET-PointPillars: Improved PointPillars for 3D object detection based on optimized voxel downsampling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSRUNet: A recurrent neural network for spatiotemporal
sequence forecasting based on parallel simple recurrent unit.
<em>MVA</em>, <em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01539-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised video prediction is widely applied in intelligent decision-making scenarios due to its capability to model unknown scenes. Traditional video prediction models based on Long Short-Term Memory (LSTM) and Gate Recurrent Unit (GRU) consume large amounts of computational resources while constantly losing the original picture information. This paper addresses the challenges discussed and introduces PSRUNet, a novel model featuring the lightweight ParallelSRU unit. By prioritizing global spatiotemporal features and minimizing redundancy, PSRUNet effectively enhances the model’s early perception of complex spatiotemporal changes. The addition of an encoder-decoder architecture captures high-dimensional image information, and information recall is introduced to mitigate gradient vanishing during deep network training. We evaluated the performance of PSRUNet and analyzed the capabilities of ParallelSRU in real-world applications, including short-term precipitation forecasting, traffic flow prediction, and human behavior prediction. Experimental results across multiple video prediction benchmarks demonstrate that PSRUNet achieves remarkably efficient and cost-effective predictions, making it a promising solution for meeting the real-time and accuracy requirements of practical business scenarios.},
  archive      = {J_MVA},
  author       = {Tian, Wei and Luo, Fan and Shen, Kailing},
  doi          = {10.1007/s00138-024-01539-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {PSRUNet: A recurrent neural network for spatiotemporal sequence forecasting based on parallel simple recurrent unit},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust semantic segmentation method of urban scenes in snowy
environment. <em>MVA</em>, <em>35</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01540-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation plays a crucial role in various computer vision tasks, such as autonomous driving in urban scenes. The related researches have made significant progress. However, since most of the researches focus on how to enhance the performance of semantic segmentation models, there is a noticeable lack of attention given to the performance deterioration of these models in severe weather. To address this issue, we study the robustness of the multimodal semantic segmentation model in snowy environment, which represents a subset of severe weather conditions. The proposed method generates realistically simulated snowy environment images by combining unpaired image translation with adversarial snowflake generation, effectively misleading the segmentation model’s predictions. These generated adversarial images are then utilized for model robustness learning, enabling the model to adapt to the harshest snowy environment and enhancing its robustness to artificially adversarial perturbance to some extent. The experimental visualization results show that the proposed method can generate approximately realistic snowy environment images, and yield satisfactory visual effects for both daytime and nighttime scenes. Moreover, the experimental quantitation results generated by MFNet Dataset indicate that compared with the model without enhancement, the proposed method achieves average improvements of 4.82% and 3.95% on mAcc and mIoU, respectively. These improvements enhance the adaptability of the multimodal semantic segmentation model to snowy environments and contribute to road safety. Furthermore, the proposed method demonstrates excellent applicability, as it can be seamlessly integrated into various multimodal semantic segmentation models.},
  archive      = {J_MVA},
  author       = {Yin, Hanqi and Yin, Guisheng and Sun, Yiming and Zhang, Liguo and Tian, Ye},
  doi          = {10.1007/s00138-024-01540-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Robust semantic segmentation method of urban scenes in snowy environment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view spectral clustering based on constrained
laplacian rank. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01497-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph-based approach is a representative clustering method among multi-view clustering algorithms. However, it remains a challenge to quickly acquire complementary information in multi-view data and to execute effective clustering when the quality of the initially constructed data graph is inadequate. Therefore, we propose multi-view spectral clustering based on constrained Laplacian rank method, a new graph-based method (CLRSC). The following are our contributions: (1) Self-representation learning and CLR are extended to multi-view and they are connected into a unified framework to learn a common affinity matrix. (2) To achieve the overall optimization we construct a graph learning method based on constrained Laplacian rank and combine it with spectral clustering. (3) An iterative optimization-based procedure we designed and showed that our algorithm is convergent. Finally, sufficient experiments are carried out on 5 benchmark datasets. The experimental results on MSRC-v1 and BBCSport datasets show that the accuracy (ACC) of the method is 10.95% and 4.61% higher than the optimal comparison algorithm, respectively.},
  archive      = {J_MVA},
  author       = {Song, Jinmei and Liu, Baokai and Yu, Yao and Zhang, Kaiwu and Du, Shiqiang},
  doi          = {10.1007/s00138-023-01497-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-view spectral clustering based on constrained laplacian rank},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-accuracy 3D locators tracking in real time using
monocular vision. <em>MVA</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01498-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of medical applications, precise localization of medical instruments and bone structures is crucial to ensure computer-assisted surgical interventions. In orthopedic surgery, existing devices typically rely on stereoscopic vision. Their purpose is to aid the surgeon in screw fixation of prostheses or bone removal. This article addresses the challenge of localizing a rigid object consisting of randomly arranged planar markers using a single camera. This approach is especially vital in medical situations where accurate object alignment relative to a camera is necessary at distances ranging from 80 cm to 120 cm. In addition, the size limitation of a few tens of centimeters ensures that the resulting locator does not obstruct the work area. This rigid locator consists of a solid at the surface of which a set of plane markers (ArUco) are glued. These plane markers are randomly distributed over the surface in order to systematically have a minimum of two visible markers whatever the orientation of the locator. The calibration of the locator involves finding the relative positions between the individual planar elements and is based on a bundle adjustment approach. One of the main and known difficulties associated with planar markers is the problem of pose ambiguity. To solve this problem, our method lies in the formulation of an efficient initial solution for the optimization step. After the calibration step, the reached positioning uncertainties of the locator are better than two-tenth of a cubic millimeter and one-tenth of a degree, regardless of the orientation of the locator in space. To assess the proposed method, the locator is rigidly attached to a stylus of about twenty centimeters length. Thanks to this approach, the tip of this stylus seen by a 16.1 megapixel camera at a distance of about 1 m is localized in real time in a cube lower than 1 mm side. A surface registration application is proposed by using the stylus on an artificial scapula.},
  archive      = {J_MVA},
  author       = {Elmo Kulanesan, C. and Vacher, P. and Charleux, L. and Roux, E.},
  doi          = {10.1007/s00138-023-01498-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {High-accuracy 3D locators tracking in real time using monocular vision},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Obs-tackle: An obstacle detection system to assist
navigation of visually impaired using smartphones. <em>MVA</em>,
<em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s00138-023-01499-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the prevalence of vision impairment continues to rise worldwide, there is an increasing need for affordable and accessible solutions that improve the daily experiences of individuals with vision impairment. The Visually Impaired (VI) are often prone to falls and injuries due to their inability to recognize dangers on the path while navigating. It is therefore crucial that they are aware of potential hazards in both known and unknown environments. Obstacle detection plays a key role in navigation assistance solutions for VI users. There has been a surge in experimentation on obstacle detection since the introduction of autonomous navigation in automobiles, robots, and drones. Previously, auditory, laser, and depth sensors dominated obstacle detection; however, advances in computer vision and deep learning have enabled it using simpler tools like smartphone cameras. While previous approaches to obstacle detection using estimated depth data have been effective, they suffer from limitations such as compromised accuracy when adapted for edge devices and the incapability to identify objects in the scene. To address these limitations, we propose an indoor and outdoor obstacle detection and identification technique that combines semantic segmentation with depth estimation data. We hypothesize that this combination of techniques will enhance obstacle detection and identification compared to using depth data alone. To evaluate the effectiveness of our proposed Obstacle detection method, we validated it against ground truth Obstacle data derived from the DIODE and NYU Depth v2 dataset. Our experimental results demonstrate that the proposed method achieves near 85% accuracy in detecting nearby obstacles with lower false positive and false negative rates. The demonstration of the proposed system deployed as an Android app-‘Obs-tackle’ is available at https://youtu.be/PSn-FEc5EQg?si=qPGB13tkYkD1kSOf .},
  archive      = {J_MVA},
  author       = {Vijetha, U. and Geetha, V.},
  doi          = {10.1007/s00138-023-01499-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Obs-tackle: An obstacle detection system to assist navigation of visually impaired using smartphones},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interaction semantic segmentation network via progressive
supervised learning. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01500-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation requires both low-level details and high-level semantics, without losing too much detail and ensuring the speed of inference. Most existing segmentation approaches leverage low- and high-level features from pre-trained models. We propose an interaction semantic segmentation network via Progressive Supervised Learning (ISSNet). Unlike a simple fusion of two sets of features, we introduce an information interaction module to embed semantics into image details, they jointly guide the response of features in an interactive way. We develop a simple yet effective boundary refinement module to provide refined boundary features for matching corresponding semantic. We introduce a progressive supervised learning strategy throughout the training level to significantly promote network performance, not architecture level. Our proposed ISSNet shows optimal inference time. We perform extensive experiments on four datasets, including Cityscapes, HazeCityscapes, RainCityscapes and CamVid. In addition to performing better in fine weather, proposed ISSNet also performs well on rainy and foggy days. We also conduct ablation study to demonstrate the role of our proposed component. Code is available at: https://github.com/Ruini94/ISSNet},
  archive      = {J_MVA},
  author       = {Zhao, Ruini and Xie, Meilin and Feng, Xubin and Guo, Min and Su, Xiuqin and Zhang, Ping},
  doi          = {10.1007/s00138-023-01500-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interaction semantic segmentation network via progressive supervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of adaptable conventional image processing
pipelines and deep learning on limited datasets. <em>MVA</em>,
<em>35</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-023-01501-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to study the impact of limited datasets on deep learning techniques and conventional methods in semantic image segmentation and to conduct a comparative analysis in order to determine the optimal scenario for utilizing both approaches. We introduce a synthetic data generator, which enables us to evaluate the impact of the number of training samples as well as the difficulty and diversity of the dataset. We show that deep learning methods excel when large datasets are available and conventional image processing approaches perform well when the datasets are small and diverse. Since transfer learning is a common approach to work around small datasets, we are specifically assessing its impact and found only marginal impact. Furthermore, we implement the conventional image processing pipeline to enable fast and easy application to new problems, making it easy to apply and test conventional methods alongside deep learning with minimal overhead.},
  archive      = {J_MVA},
  author       = {Münke, Friedrich Rieken and Schützke, Jan and Berens, Felix and Reischl, Markus},
  doi          = {10.1007/s00138-023-01501-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A review of adaptable conventional image processing pipelines and deep learning on limited datasets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization model based on attention mechanism for few-shot
image classification. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01502-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has emerged as the leading approach for pattern recognition, but its reliance on large labeled datasets poses challenges in real-world applications where obtaining annotated samples is difficult. Few-shot learning, inspired by human learning, enables fast adaptation to new concepts with limited examples. Optimization-based meta-learning has gained popularity as a few-shot learning method. However, it struggles with capturing long-range dependencies of gradients and has slow convergence rates, making it challenging to extract features from limited samples. To overcome these issues, we propose MLAL, an optimization model based on attention for few-shot learning. The model comprises two parts: the attention-LSTM meta-learner, which optimizes gradients hierarchically using the self-attention mechanism, and the cross-attention base-learner, which uses the cross-attention mechanism to cross-learn the common category features of support and query sets in a meta-task. Extensive experiments on two benchmark datasets show that MLAL achieves exceptional 1-shot and 5-shot classification accuracy on MiniImagenet and TiredImagenet. The codes for our proposed method are available at https://github.com/wflrz123/MLAL .},
  archive      = {J_MVA},
  author       = {Liao, Ruizhi and Zhai, Junhai and Zhang, Feng},
  doi          = {10.1007/s00138-023-01502-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Optimization model based on attention mechanism for few-shot image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional filtering distillation for object detection.
<em>MVA</em>, <em>35</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01503-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a common and effective method in model compression, which trains a compact student model to mimic the capability of a large teacher model to get superior generalization. Previous works on knowledge distillation are underperforming for challenging tasks such as object detection, compared to the general application of unsophisticated classification tasks. In this paper, we propose that the failure of knowledge distillation on object detection is mainly caused by the imbalance between features of informative and invalid background. Not all background noise is redundant, and the valuable background noise after screening contains relations between foreground and background. Therefore, we propose a novel regional filtering distillation (RFD) algorithm to solve this problem through two modules: region selection and attention-guided distillation. Region selection first filters massive invalid backgrounds and retains knowledge-dense regions on near object anchor locations. Attention-guided distillation further improves distillation performance on object detection tasks by extracting the relations between foreground and background to migrate key features. Extensive experiments on both one-stage and two-stage detectors have been conducted to prove the effectiveness of RFD. For example, RFD improves 2.8% and 2.6% mAP for ResNet50-RetinaNet and ResNet50-FPN student networks on the MS COCO dataset, respectively. We also evaluate our method with the Faster R-CNN model on Pascal VOC and KITTI benchmark, which obtain 1.52% and 4.36% mAP promotions for the ResNet18-FPN student network, respectively. Furthermore, our method increases 5.70% of mAP for MobileNetv2-SSD compared to the original model. The proposed RFD technique performs highly on detection tasks through regional filtering distillation. In the future, we plan to extend it to more challenging task scenarios, such as segmentation.},
  archive      = {J_MVA},
  author       = {Wu, Pingfan and Zhang, Jiayu and Sun, Han and Liu, Ningzhong},
  doi          = {10.1007/s00138-023-01503-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Regional filtering distillation for object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STARNet: Spatio-temporal aware recurrent network for
efficient video object detection on embedded devices. <em>MVA</em>,
<em>35</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01504-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of converting various object detection methods from image to video remains unsolved. When applied to video, image methods frequently fail to generalize effectively due to issues, such as blurriness, different and unclear positions, low quality, and other relevant issues. Additionally, the lack of a good long-term memory in video object detection presents an additional challenge. In the majority of instances, the outputs of successive frames are known to be quite similar; therefore, this fact is relied upon. Furthermore, the information contained in a series of successive or non-successive frames is greater than that contained in a single frame. In this study, we present a novel recurrent cell for feature propagation and identify the optimal location of layers to increase the memory interval. As a result, we achieved higher accuracy compared to other proposed methods in other studies. Hardware limitations can exacerbate this challenge. The paper aims to implement and increase the efficiency of the methods on embedded devices. We achieved 68.7% mAP accuracy on the ImageNet VID dataset for embedded devices in real-time and at a speed of 52 fps.},
  archive      = {J_MVA},
  author       = {Hajizadeh, Mohammad and Sabokrou, Mohammad and Rahmani, Adel},
  doi          = {10.1007/s00138-023-01504-0},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {STARNet: Spatio-temporal aware recurrent network for efficient video object detection on embedded devices},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling confusion among actions for action segmentation
with adaptive margin and energy-driven refinement. <em>MVA</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s00138-023-01505-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video action segmentation is a crucial task in evaluating the ability to understand human activities. Previous works on this task mainly focus on capturing complex temporal structures and fail to consider the feature ambiguity among similar actions and the biased training sets, thus they are easy to confuse some actions. In this paper, we propose a novel action segmentation framework, called DeConfuNet, to solve the above issue. First, we design a discriminative enhancement module (DEM) trained by an adaptive margin-guided discriminative feature learning which adjusts the margin adaptively to increase the feature distinguishability among similar actions, and whose multi-stage reasoning and adaptive feature fusion structures provide structural advantages for distinguishing similar actions. Second, we propose an equalizing influence module (EIM) that can overcome the impact of biased training sets by balancing the influence of training samples under a coefficient-adaptive loss function. Third, an energy and context-driven refinement module (ECRM) further alleviates the impact of the unbalanced influence of training samples by fusing and refining the inference of DEM and EIM, which utilizes the phased prediction including context and energy clues to assimilate untrustworthy segments, alleviating over-segmentation hugely. Extensive experiments show the effectiveness of each proposed technique, they verify that the DEM and EIM are complementary in reasoning and cooperate to overcome the confusion issue, and our approach achieves significant improvement and state-of-the-art performance of accuracy, edit score, and F1 score on the challenging 50Salads, GTEA, and Breakfast benchmarks.},
  archive      = {J_MVA},
  author       = {Ma, Zhichao and Li, Kan},
  doi          = {10.1007/s00138-023-01505-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Tackling confusion among actions for action segmentation with adaptive margin and energy-driven refinement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGBGAN: Minority class image generation for class-imbalanced
datasets. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01506-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance frequently arises in the context of image classification. Conventional generative adversarial networks (GANs) have a tendency to produce samples from the majority class when trained on class-imbalanced datasets. To address this issue, the Balancing GAN with gradient penalty (BAGAN-GP) has been proposed, but the outcomes may still exhibit a bias toward the majority categories when the similarity between images from different categories is substantial. In this study, we introduce a novel approach called the Pre-trained Gated Variational Autoencoder with Self-attention for Balancing Generative Adversarial Network (SGBGAN) as an image augmentation technique for generating high-quality images. The proposed method utilizes a Gated Variational Autoencoder with Self-attention (SA-GVAE) to initialize the GAN and transfers pre-trained SA-GVAE weights to the GAN. Our experimental results on Fashion-MNIST, CIFAR-10, and a highly unbalanced medical image dataset demonstrate that the SGBGAN outperforms other state-of-the-art methods. Results on Fréchet inception distance (FID) and structural similarity measures (SSIM) show that our model overcomes the instability problems that exist in other GANs. Especially on the Cells dataset, the FID of a minority class increases up to 23.09% compared to the latest BAGAN-GP, and the SSIM of a minority class increases up to 10.81%. It is proved that SGBGAN overcomes the class imbalance restriction and generates high-quality minority class images. The diagram provides an overview of the technical approach employed in this research paper. To address the issue of class imbalance within the dataset, a novel technique called the Gated Variational Autoencoder with Self-attention (SA-GVAE) is proposed. This SA-GVAE is utilized to initialize the Generative Adversarial Network (GAN), with the pre-trained weights from SA-GVAE being transferred to the GAN. Consequently, a Pre-trained Gated Variational Autoencoder with Self-attention for Balancing GAN (SGBGAN) is formed, serving as an image augmentation tool to generate high-quality images. Ultimately, the generation of minority samples is employed to restore class balance within the dataset.},
  archive      = {J_MVA},
  author       = {Wan, Qian and Guo, Wenhui and Wang, Yanjiang},
  doi          = {10.1007/s00138-023-01506-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SGBGAN: Minority class image generation for class-imbalanced datasets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end optimized image compression with the
frequency-oriented transform. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01507-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression constitutes a significant challenge amid the era of information explosion. Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs. However, an inherent challenge associated with these methods lies in their lack of interpretability. Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform. The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion. The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept. Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components. Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric. Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method that could preserve semantic fidelity besides signal-level precision.},
  archive      = {J_MVA},
  author       = {Zhang, Yuefeng and Lin, Kai},
  doi          = {10.1007/s00138-023-01507-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {End-to-end optimized image compression with the frequency-oriented transform},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Target–distractor memory joint tracking algorithm via
credit allocation network. <em>MVA</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s00138-024-01508-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tracking framework based on the memory network has gained significant attention due to its enhanced adaptability to variations in target appearance. However, the performance of the framework is limited by the negative effects of distractors in the background. Hence, this paper proposes a method for tracking using Credit Allocation Network to join target and distractor memory. Specifically, we design a Credit Allocation Network (CAN) that is updated online via Guided Focus Loss. The CAN produces credit scores for tracking results by learning features of the target object, ensuring the update of reliable samples for storage in the memory pool. Furthermore, we construct a multi-domain memory model that simultaneously captures target and background information from multiple historical intervals, which can build a more compatible object appearance model while increasing the diversity of the memory sample. Moreover, a novel target–distractor joint localization strategy is presented, which read target and distractor information from memory frames based on cross-attention, so as to cancel out wrong responses in the target response map by using the distractor response map. The experimental results on OTB-2015, GOT-10k, UAV123, LaSOT, and VOT-2018 datasets show the competitiveness and effectiveness of the proposed method compared to other trackers.},
  archive      = {J_MVA},
  author       = {Zhang, Huanlong and Wang, Panyun and Chen, Zhiwu and Zhang, Jie and Li, Linwei},
  doi          = {10.1007/s00138-024-01508-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Target–distractor memory joint tracking algorithm via credit allocation network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). That’s BAD: Blind anomaly detection by implicit local
feature clustering. <em>MVA</em>, <em>35</em>(2), 1–10. (<a
href="https://doi.org/10.1007/s00138-024-01511-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on visual anomaly detection (AD) of industrial objects/textures have achieved quite good performance. They consider an unsupervised setting, specifically the one-class setting, in which we assume the availability of a set of normal (i.e., anomaly-free) images for training. In this paper, we consider a more challenging scenario of unsupervised AD, in which we detect anomalies in a given set of images that might contain both normal and anomalous samples. The setting does not assume the availability of known normal data and thus is completely free from human annotation, which differs from the standard AD considered in recent studies. For clarity, we call the setting blind anomaly detection (BAD). We show that BAD can be converted into a local outlier detection problem and propose a novel method named PatchCluster that can accurately detect image- and pixel-level anomalies. Experimental results show that PatchCluster shows a promising performance without the knowledge of normal data, even comparable to the SOTA methods applied in the one-class setting needing it.},
  archive      = {J_MVA},
  author       = {Zhang, Jie and Suganuma, Masanori and Okatani, Takayuki},
  doi          = {10.1007/s00138-024-01511-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {That’s BAD: Blind anomaly detection by implicit local feature clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gradient fusion-based image data augmentation method for
reflective workpieces detection under small size datasets. <em>MVA</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01512-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various of Convolutional Neural Network-based object detection models have been widely used in the industrial field. However, the high accuracy of the object detection of these models is difficult to obtain in the industrial sorting line. This is due to the use of small dataset considering of production cost and the changing features of the reflective workpiece. In order to increase the detecting accuracy, a gradient fusion-based image data augmentation method was presented in this paper. It consisted of a high-dynamic range (HDR) exposing algorithm and an image reconstructing algorithm. It augmented the image data for the training and predicting by increasing the feature richness within the regions of reflection and shadow of the image. Tests were conducted on the comparison with other exposing and image fusion methods. The universality of the proposed method was analyzed by testing on various kinds of workpieces and different models including YOLOv8 and SSD. Finally, the Gradient-weighted Class Activation Mapping (Grad-CAM) method and Mean Average Precision (mAP) were used to analyze the model performance improvement. The results showed that the proposed data augmentation method improved the feature richness of the image and the accuracy of the object detection for the reflective workpieces under small size datasets.},
  archive      = {J_MVA},
  author       = {Zhang, Baori and Cai, Haolang and Wen, Lingxiang},
  doi          = {10.1007/s00138-024-01512-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A gradient fusion-based image data augmentation method for reflective workpieces detection under small size datasets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pixel and channel enhanced up-sampling module for
biomedical image segmentation. <em>MVA</em>, <em>35</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s00138-024-01513-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Up-sampling operations are frequently utilized to recover the spatial resolution of feature maps in neural networks for segmentation task. However, current up-sampling methods, such as bilinear interpolation or deconvolution, do not fully consider the relationship of feature maps, which have negative impact on learning discriminative features for semantic segmentation. In this paper, we propose a pixel and channel enhanced up-sampling (PCE) module for low-resolution feature maps, aiming to use the relationship of adjacent pixels and channels for learning discriminative high-resolution feature maps. Specifically, the proposed up-sampling module includes two main operations: (1) increasing spatial resolution of feature maps with pixel shuffle and (2) recalibrating channel-wise high-resolution feature response. Our proposed up-sampling module could be integrated into CNN and Transformer segmentation architectures. Extensive experiments on three different modality datasets of biomedical images, including computed tomography (CT), magnetic resonance imaging (MRI) and micro-optical sectioning tomography images (MOST) demonstrate the proposed method could effectively improve the performance of representative segmentation models.},
  archive      = {J_MVA},
  author       = {Zhang, Xuan and Xu, Guoping and Wu, Xinglong and Liao, Wentao and Leng, Xuesong and Wang, Xiaxia and He, Xinwei and Li, Chang},
  doi          = {10.1007/s00138-024-01513-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A pixel and channel enhanced up-sampling module for biomedical image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised siamese keypoint inference network for human
pose estimation and tracking. <em>MVA</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01515-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation and tracking are important tasks to help understand human behavior. Currently, human pose estimation and tracking face the challenges of missed detection due to sparse annotation of video datasets and difficulty in associating partially occluded and unoccluded cases of the same person. To address these challenges, we propose a self-supervised learning-based method, which infers the correspondence between keypoints to associate persons in the videos. Specifically, we propose a bounding box recovery module to recover missed detections and a Siamese keypoint inference network to solve the issue of error matching caused by occlusions. The local–global attention module, which is designed in the Siamese keypoint inference network, learns the varying dependence information of human keypoints between frames. To simulate the occlusions, we mask random pixels in the image before pre-training using knowledge distillation to associate the differing occlusions of the same person. Our method achieves better results than state-of-the-art methods for human pose estimation and tracking on the PoseTrack 2018 and PoseTrack 2021 datasets. Code is available at: https://github.com/yhtian2023/SKITrack .},
  archive      = {J_MVA},
  author       = {Wang, Xiangyang and Tian, Yuhui and Wang, Rui},
  doi          = {10.1007/s00138-024-01515-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Self-supervised siamese keypoint inference network for human pose estimation and tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FESAR: SAR ship detection model based on local spatial
relationship capture and fused convolutional enhancement. <em>MVA</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s00138-024-01516-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) is instrumental in ship monitoring owing to its all-weather capabilities and high resolution. In SAR images, ship targets frequently display blurred or mixed boundaries with the background, and instances of occlusion or partial occlusion may occur. Additionally, multi-scale transformations and small-target ships pose challenges for ship detection. To tackle these challenges, we propose a novel SAR ship detection model, FESAR. Firstly, in addressing multi-scale transformations in ship detection, we propose the Fused Convolution Enhancement Module (FCEM). This network incorporates distinct convolutional branches designed to capture local and global features, which are subsequently fused and enhanced. Secondly, a Spatial Relationship Analysis Module (SRAM) with a spatial-mixing layer is designed to analyze the local spatial relationship between the ship target and the background, effectively combining local information to discern feature distinctions between the ship target and the background. Finally, a new backbone network, SPD-YOLO, is designed to perform deep downsampling for the comprehensive extraction of semantic information related to ships. To validate the model’s performance, an extensive series of experiments was conducted on the public datasets HRSID, LS-SSDD-v1.0, and SSDD. The results demonstrate the outstanding performance of the proposed FESAR model compared to numerous state-of-the-art (SOTA) models. Relative to the baseline model, FESAR exhibits an improvement in mAP by 2.6% on the HRSID dataset, 5.5% on LS-SSDD-v1.0, and 0.2% on the SSDD dataset. In comparison with numerous SAR ship detection models, FESAR demonstrates superior comprehensive performance.},
  archive      = {J_MVA},
  author       = {Liu, Chongchong and Yan, Chunman},
  doi          = {10.1007/s00138-024-01516-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FESAR: SAR ship detection model based on local spatial relationship capture and fused convolutional enhancement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive interpolation and 3D reconstruction algorithm
for underwater images. <em>MVA</em>, <em>35</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-024-01518-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction technology is gradually applied to underwater scenes, which has become a crucial research direction for human ocean exploration and exploitation. However, due to the complexity of the underwater environment, the number of high-quality underwater images acquired by underwater robots is limited and cannot meet the requirements of 3D reconstruction. Therefore, this paper proposes an adaptive 3D reconstruction algorithm for underwater targets. We apply the frame interpolation technique to underwater 3D reconstruction, an unprecedented technical attempt. In this paper, we design a single-stage large-angle span underwater image interpolation model, which has an excellent enhancement effect on degraded underwater 2D images compared with other methods. Current methods make it challenging to balance the relationship between feature information acquisition and underwater image quality improvement. In this paper, an optimized cascaded feature pyramid scheme and an adaptive bidirectional optical flow estimation algorithm based on underwater NRIQA metrics are proposed and applied to the proposed model to solve the above problems. The intermediate image output from the model improves the image quality and retains the detailed information. Experiments show that the method proposed in this paper outperforms other methods when dealing with several typical degradation types of underwater images. In underwater 3D reconstruction, the intermediate image generated by the model is used as input instead of the degraded image to obtain a denser 3D point cloud and better visualization. Our method is instructive to the problem of acquiring underwater high-quality target images and underwater 3D reconstruction.},
  archive      = {J_MVA},
  author       = {Tang, Zhijie and Xu, Congqi and Yan, Siyu},
  doi          = {10.1007/s00138-024-01518-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An adaptive interpolation and 3D reconstruction algorithm for underwater images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual progressive strategy for long-tailed visual
recognition. <em>MVA</em>, <em>35</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01480-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike the roughly balanced dataset used in the experiments, the long-tail phenomenon in the dataset is more common when applied in practice. Most previous work has typically used re-sampling, re-weighting, and ensemble learning to mitigate the long-tail problem. The first two are the most commonly used (as are we) due to their better generality. Differently, assigning weights to classes directly using the inverse of the sample size to solve such problems may not be a good strategy, which often sacrifices the performance of the head classes. We propose a new approach to cost allocation, which consists of two parts: the first part is trained in an unweighted manner to ensure that the network is adequately fitted to the head data. The second part then dynamically assigns weights based on the relative difficulty of the class levels.In addition, we propose a novel, practical Grabcut-based data augmentation approach to increase the diversity and differentiation of the mid-tail class data. Extensive experiments on public and self-constructed long-tailed datasets demonstrate the effectiveness of our approach and achieve excellent performance.},
  archive      = {J_MVA},
  author       = {Liang, Hong and Cao, Guoqing and Shao, Mingwen and Zhang, Qian},
  doi          = {10.1007/s00138-023-01480-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A dual progressive strategy for long-tailed visual recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic label assignment object detection mehtod on only
one feature map. <em>MVA</em>, <em>35</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01481-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep learning-based object detection methods are proposed based on multi-level feature environments. Although some researchers have tried to detect on one-level features, where multiple feature maps are utilized. In this paper, we aim to propose a novel anchor-free object detection approach with an automatic label assignment strategy on only one feature map. The proposed method follows the main idea of AutoAssign to achieve the label assignment strategy. However, to make the strategy work appropriately in one feature map environment, several modifications have been made. A prior knowledge fusion method called ‘Center Weighting Fusion’ is proposed for label assignment strategy, where the Gaussian mixture model function is applied to calculate the weights of each object on one feature map. By doing so, some objects that are close to each other, whose weights will be merged and generate some points (Recheck Points) that are shared with multiple objects. For those ‘Recheck Points,’ the detector will judge how many objects share this point and generate a corresponding number of different-sized proposals. In detecting different-sized objects, we propose a ‘Uniform Detection’ method to limit each point’s regression distance according to the target’s category. A large number of experimental data show that the proposed method presents competitive detection accuracy with normal anchor-free detectors (43.8% mAP), while it is smaller (30% smaller) and faster (50% better).},
  archive      = {J_MVA},
  author       = {Ma, Tingsong and Huang, Zengxi and Yang, Nijing and Zhu, Changyu and Deng, Ping},
  doi          = {10.1007/s00138-023-01481-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic label assignment object detection mehtod on only one feature map},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IoU-aware feature fusion r-CNN for dense object detection.
<em>MVA</em>, <em>35</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s00138-023-01483-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Densely packed scenes contain a multitude of similar or even identical objects positioned closely, which brings about difficulties in object-detecting. In such scenes, people usually embed the feature pyramid network module into Faster R-CNN for multi-scale detection. However, we find it is not necessary. It is observed that there exists a waste of ground-truth boxes during training, causing the imbalance of training samples and the poor performance of Faster R-CNN. Therefore, we propose an online multiple-step sampling method to increase the utilization of ground truth. Besides, we propose a novel IoU-aware feature fusion R-CNN to take place of the feature pyramid network. It could effectively improve the detection accuracy of Faster R-CNN while simplifying the structure of the feature pyramid network. Our approach improves the base detector and the detection results on SKU-110 k benchmarks indicate that our approach offers a good trade-off between accuracy and speed.},
  archive      = {J_MVA},
  author       = {Hong, Jixuan and He, Xueqin and Deng, Zhaoli and Yang, Chenhui},
  doi          = {10.1007/s00138-023-01483-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {IoU-aware feature fusion R-CNN for dense object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual complexity of urban streetscapes: Human vs computer
vision. <em>MVA</em>, <em>35</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01484-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding visual complexity of urban environments may improve urban design strategies and limit visual pollution due to advertising, road signage, telecommunication systems and machinery. This paper aims at quantifying visual complexity specifically in urban streetscapes, by submitting a collection of geo-referenced photographs to a group of more than 450 internet users. The average complexity ranking issued from this survey was compared with a set of computer vision predictions, attempting to find the optimal match. Overall, a computer vision indicator matching comprehensively the survey outcome did not clearly emerge from the analysis, but a set of perceptual hypotheses demonstrated that some categories of stimuli are more relevant. The results show how images with contrasting colour regions and sharp edges are more prone to drive the feeling of high complexity.},
  archive      = {J_MVA},
  author       = {Florio, Pietro and Leduc, Thomas and Sutter, Yannick and Brémond, Roland},
  doi          = {10.1007/s00138-023-01484-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Visual complexity of urban streetscapes: Human vs computer vision},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Csf: Global–local shading orders for intrinsic image
decomposition. <em>MVA</em>, <em>35</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01485-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition faces the long-standing challenge from the coupling of the components of the image-the surface albedo, direct illumination, and ambient illumination in the observed image. Without knowing the absolute values of the image components, we propose inferring shading by ordering pixels by relative brightness. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of the local shading field. The brightness order is a nonlocal metric that can be used to compare any two pixels, including those with different reflectances and shadings. Low-order fittings are used for pixel pairs within local regions of smooth shading. They can capture the global order structure and local variations in the shading when used together. To integrate the pairwise orders into a globally consistent order, we propose a Consistency-aware Selective Fusion method. The iterative selection process solves the inconsistencies between pairwise orders obtained using different estimation methods. To avoid polluting the global order, inconsistent or unreliable pairwise orders will be automatically excluded from the fusion. Experimental results show that the proposed model effectively recovers the shading, including deep shadows, on the MIT Intrinsic Image dataset. Moreover, our model works well on natural images from the IIW, UIUC Shadow, and NYU-depth datasets, where the colors of direct lights and ambient lights are quite different.},
  archive      = {J_MVA},
  author       = {Zhang, Handan and Liu, Tie and Liu, Yuanliu and Yuan, Zejian},
  doi          = {10.1007/s00138-023-01485-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Csf: Global–local shading orders for intrinsic image decomposition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot object detection via data augmentation and
distribution calibration. <em>MVA</em>, <em>35</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01486-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General object detection has been widely developed and studied over the past few years, while few-shot object detection is still in the exploratory stage. Learning effective knowledge from a limited number of samples is challenging, as the trained model is prone to over-fitting due to biased feature distributions in a few training samples. There exist two significant challenges in traditional few-shot object detection methods: (1) The scarcity of extreme samples aggravates the proposal distribution bias, hindering the evolution of regions of interest (ROI) heads toward new categories; (2) Due to the scarce of the samples in novel categories, the region proposal network (RPN) is identified as a key source of classification errors, resulting in a significant decrease in detection performance on novel categories. To overcome these challenges, an effective knowledge transfer method based on distributed calibration and data augmentation is proposed. Firstly, the biased novel category distributions are calibrated with the basic category distributions; secondly, a drift compensation strategy is utilized to reduce the negative impact on new categories classifications during the fine-tuning process; thirdly, synthetic features are obtained from calibrated distributions of novel categories and added to the subsequent training process. Furthermore, the domain-aware data augmentation is utilized to alleviate the issue of data scarcity by exploiting the cross-image foreground—background mixture to increase the diversity and rationality of augmented data. Experimental results demonstrate the effectiveness and applicability of the proposed method.},
  archive      = {J_MVA},
  author       = {Zhu, Songhao and Zhang, Kai},
  doi          = {10.1007/s00138-023-01486-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Few-shot object detection via data augmentation and distribution calibration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast anchor-based graph-regularized low-rank
representation approach for large-scale subspace clustering.
<em>MVA</em>, <em>35</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01487-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-regularized low-rank representation (GLRR) is an important subspace clustering (SC) algorithm, which has been widely used in pattern recognition and other related fields. It can not only represent the global structure of data, but also capture the nonlinear geometric information. However, GLRR has encountered bottlenecks in dealing with large-scale SC problems since it contains singular value decomposition and similarity matrix construction. To solve this problem, we propose a novel method, i.e., fast anchor-based graph-regularized low-rank representation (FA-GLRR) approach for large-scale subspace clustering. Specifically, anchor graph is first used to accelerate the construction of similarity matrix, and then, some equivalent transformations are given to transform large-scale problems into small-scale problems. These two strategies reduce the computational complexity of GLRR dramatically. Experiments on several common datasets demonstrate the superiority of FA-GLRR in terms of time performance and clustering performance.},
  archive      = {J_MVA},
  author       = {Fan, Lili and Lu, Guifu and Tang, Ganyi and Wang, Yong},
  doi          = {10.1007/s00138-023-01487-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A fast anchor-based graph-regularized low-rank representation approach for large-scale subspace clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stereo vision SLAM with moving vehicles tracking in
outdoor environment. <em>MVA</em>, <em>35</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01488-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assumption of a static environment is a prerequisite for most of the traditional visual simultaneous localization and mapping (v-SLAM) algorithms, which limits their widespread application in a dynamic environment. Furthermore, in many applications such as autonomous driving, robot collaboration and AR/VR, it is necessary to track the moving objects in the environment. In this work, we propose a v-SLAM method that can effectively track multiple objects in dynamic environments by integrating a 3D object detection thread into the ORB-SLAM2 framework. The dynamic objects were detected and tracked in three steps. Firstly, 3D object detection was performed on the current frame, and the 3D bounding box was projected into a bird&#39;s-eye view. Secondly, an association for the object is made based on the motion state of the object and the bounding box in the bird’s-eye view. Thirdly, we track the object and remove feature points corresponding to the dynamic region. In addition, we set up a multi-view constraint adjustment for static objects to jointly optimize the pose of the camera, object, and map point. Experiments have been conducted on the KITTI-odom and KITTI-raw datasets. The performance of our method was verified in challenging scenarios. We demonstrate that dynamic object tracking not only provides useful information for scene understanding, but also help to improve camera tracking.},
  archive      = {J_MVA},
  author       = {Hong, Chuyuan and Zhong, Meiling and Jia, Zhaoqian and You, Changjiang and Wang, Zhiguo},
  doi          = {10.1007/s00138-023-01488-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A stereo vision SLAM with moving vehicles tracking in outdoor environment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal graph-guided global attention network for
video-based person re-identification. <em>MVA</em>, <em>35</em>(1),
1–18. (<a href="https://doi.org/10.1007/s00138-023-01489-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global attention learning has been extensively applied in video-based person re-identification due to its superiority in capturing contextual correlations. However, existing global attention learning methods usually adopt the conventional neural network to model non-Euclidean contextual correlations, resulting in a limited representation ability. Inspired by the graph-structure property of the contextual correlations, we propose a spatial-temporal graph-guided global attention network (STG $$^3$$ A) for video-based person re-identification. STG $$^3$$ A comprises two graph-guided attention modules to capture the spatial contexts within a frame and temporal contexts across all frames in a sequence for global attention learning. Furthermore, the graphs from both modules are encoded as graph representations, which combine with weighted representations to grasp the spatial-temporal contextual information adequately for video feature learning. To reduce the effect of noisy graph nodes and learn robust graph representations, a graph node attention is developed to trade-off the importance of each graph node, leading to noise-tolerant graph models. Finally, we design a graph-guided fusion scheme to integrate the representations output by these two attentive modules for a more compact video feature. Extensive experiments on MARS and DukeMTMCVideoReID datasets demonstrate the superior performance of the STG $$^3$$ A.},
  archive      = {J_MVA},
  author       = {Li, Xiaobao and Wang, Wen and Li, Qingyong and Zhang, Jiang},
  doi          = {10.1007/s00138-023-01489-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Spatial-temporal graph-guided global attention network for video-based person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalized margin loss for action unit detection.
<em>MVA</em>, <em>35</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-023-01490-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of recognition of naturally-appearing human facial movements (action units), as an intermediate step toward their aggregation for the recognition and understanding of facial expressions. With respect to the proposed method, we introduce a domain adaptation solution that is applied to deep convolutional networks, taking advantage of the networks capability of providing simultaneous predictions and discriminative embeddings. In this way, we adapt information gathered from training on mutual expression recognition to facial action unit detection. The described strategy is evaluated in the context of action units in the wild within the EmotioNet dataset and action units acquired in laboratory conditions within the DISFA and CK+ datasets. Our method achieves results comparable to state-of-the-art and demonstrates superior recognition in the case of rarely occurring action units. Additionally, the embedding space structuring is significantly enhanced with respect to the results obtained by classical losses.},
  archive      = {J_MVA},
  author       = {Racoviteanu, Andrei and Florea, Corneliu and Florea, Laura and Vertan, Constantin},
  doi          = {10.1007/s00138-023-01490-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Normalized margin loss for action unit detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DetailPoint: Detailed feature learning on point clouds with
attention mechanism. <em>MVA</em>, <em>35</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01491-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud analysis is an important part of 3D geometric processing. It has been widely used in many fields, such as automatic driving and robots. Although great progress has been made in recent years, there are still some unresolved problems. For example, current methods devote employing MLP to extract local features after search k neighbor points, they cannot effectively model the dependency relationship between the anchor point and k neighboring points. In addition, the prevailing models may not exploit the inherent structural similarities present in the global scope. To solve these issues, we propose a feature extraction model named DetailPoint to get detailed local information and long-range global dependency of point clouds. DetailPoint possess three units: the shallow local learning unit, the deep local learning unit and the deep global learning unit. We first use the SLL to extract shallow local features, and then use the DLL to learn deep local features. In these two units, we design a dual-path extraction method to acquire detail local features with dependencies. Finally, the DGL unit is employed to improve the generalization ability of local features and establish global interaction. These three units are connected in series to form our DetailPoint. We evaluated the performance of our model on four datasets, ScanObjectNN and ModelNet40 for shape classification, the ShapeNet dataset for part segmentation, and the S3DIS dataset for sementatic segmentations. The experimental results demonstrate that DetailPoint is capable of expressing point clouds more effectively, resulting in superior performance compared to existing methods.},
  archive      = {J_MVA},
  author       = {Li, Ying and Bai, Jincheng and Sheng, Huankun},
  doi          = {10.1007/s00138-023-01491-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DetailPoint: Detailed feature learning on point clouds with attention mechanism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to explore by reinforcement over high-level
options. <em>MVA</em>, <em>35</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01492-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous 3D environment exploration is a fundamental task for various applications such as navigation and object searching. The goal of exploration is to investigate a new environment and build a map efficiently. In this paper, we propose a new method which grants an agent two intertwined options of behaviors: “look-around” and “frontier navigation.” This is implemented by an option-critic architecture and trained by reinforcement learning algorithms. In each time step, an agent produces an option and a corresponding action according to the policy. We also take advantage of macro-actions by incorporating classic path-planning techniques to increase training efficiency. We demonstrate the effectiveness of the proposed method on two publicly available 3D environment datasets, and the results show our method achieves higher coverage than competing techniques with better efficiency. We also show that our method can be transferred and applied on a rover robot in real-world environments.},
  archive      = {J_MVA},
  author       = {Liu, Juncheng and McCane, Brendan and Mills, Steven},
  doi          = {10.1007/s00138-023-01492-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Learning to explore by reinforcement over high-level options},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pixel representations, sampling, and label correction for
semantic part detection. <em>MVA</em>, <em>35</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01493-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic part detection within an object is of importance in the field of computer vision. This study proposes a novel approach to semantic part detection that starts by employing a convolutional neural network to concatenate a selection of feature maps from the network into a long vector for pixel representation. Using this dedicated pixel representation, we implement a range of techniques, such as Poisson disk sampling for pixel sampling and Poisson matting for pixel label correction. These techniques efficiently facilitate the training of a practical pixel classifier for part detection. Our experimental exploration investigated various factors that affect the model’s performance, including training data labeling (with or without the aid of Poisson matting), hypercolumn representation dimensionality, neural network architecture, post-processing techniques, and pixel classifier selection. In addition, we conducted a comparative analysis of our approach with established object detection methods.},
  archive      = {J_MVA},
  author       = {Huang, Jiao-Chuan and Lin, You-Lin and Fang, Wen-Chieh},
  doi          = {10.1007/s00138-023-01493-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pixel representations, sampling, and label correction for semantic part detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biomimetic oculomotor control with spiking neural networks.
<em>MVA</em>, <em>35</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01494-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are comprised of artificial neurons that, like their biological counterparts, communicate via electrical spikes. SNNs have been hailed as the next wave of deep learning as they promise low latency and low-power consumption when run on neuromorphic hardware. Current deep neural network models for computer vision often require power-hungry GPUs to train and run, making them great candidates to replace with SNNs. We develop and train a biomimetic, SNN-driven, neuromuscular oculomotor controller for a realistic biomechanical model of the human eye. Inspired by the ON and OFF bipolar cells of the retina, we use event-based data flow in the SNN to direct the necessary extraocular muscle-driven eye movements. We train our SNN models from scratch, using modified deep learning techniques. Classification tasks are straightforward to implement with SNNs and have received the most research attention, but visual tracking is a regression task. We use surrogate gradients and introduce a linear layer to convert membrane voltages from the final spiking layer into the desired outputs. Our SNN foveation network enhances the biomimetic properties of the virtual eye model and enables it to perform reliable visual tracking. Overall, with event-based data processed by an SNN, our oculomotor controller successfully tracks a visual target while activating 87.3% fewer neurons than a conventional neural network.},
  archive      = {J_MVA},
  author       = {Saquib, Taasin and Terzopoulos, Demetri},
  doi          = {10.1007/s00138-023-01494-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Biomimetic oculomotor control with spiking neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local region-learning modules for point cloud
classification. <em>MVA</em>, <em>35</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00138-023-01495-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data organization via forming local regions is an integral part of deep learning networks that process 3D point clouds in a hierarchical manner. At each level, the point cloud is sampled to extract representative points and these points are used to be centers of local regions. The organization of local regions is of considerable importance since it determines the location and size of the receptive field at a particular layer of feature aggregation. In this paper, we present two local region-learning modules: Center Shift Module to infer the appropriate shift for each center point, and Radius Update Module to alter the radius of each local region. The parameters of the modules are learned through optimizing the loss associated with the particular task within an end-to-end network. We present alternatives for these modules through various ways of modeling the interactions of the features and locations of 3D points in the point cloud. We integrated both modules independently and together to the PointNet++ and PointCNN object classification architectures, and demonstrated that the modules contributed to a significant increase in classification accuracy for the ScanObjectNN data set consisting of scans of real-world objects. Our further experiments on ShapeNet data set showed that the modules are also effective on 3D CAD models.},
  archive      = {J_MVA},
  author       = {Turgut, Kaya and Dutagaci, Helin},
  doi          = {10.1007/s00138-023-01495-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Local region-learning modules for point cloud classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cancelable face recognition using phase retrieval and
complex principal component analysis network. <em>MVA</em>,
<em>35</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01496-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the necessity of sensitive information protection in face image, a cancelable template generation model for multimodal face images is proposed in this paper. Firstly, the visual meaningful face images are transformed into phase-only functions through phase retrieval in gyrator domain. Then random projection and chaotic-based mask are constituted into modulation for achieving revocability and distinguishability. The interim results are mapped to a higher-dimensional space using random Fourier features. Followed by two-stage complex-valued principal component analysis, the convolutional filters are learned efficiently. Together with binary hashing and decimal coding, local histogram features are obtained and forwarded to SVM for training and recognition. Experiments performed on three publicly multimodal datasets demonstrate that the proposed algorithm can obtain higher accuracy, precision, recall and F-score in comparison with some existing algorithms while the templates are non-invertible and easy to revoke.},
  archive      = {J_MVA},
  author       = {Shao, Zhuhong and Li, Leding and Zhang, Zuowei and Li, Bicao and Liu, Xilin and Shang, Yuanyuan and Chen, Bin},
  doi          = {10.1007/s00138-023-01496-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cancelable face recognition using phase retrieval and complex principal component analysis network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
