<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv---300">IJCV - 300</h2>
<ul>
<li><details>
<summary>
(2024). Guest editorial: Special issue on ACCV 2022. <em>IJCV</em>,
<em>132</em>(12), 6207. (<a
href="https://doi.org/10.1007/s11263-024-02123-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Wang, Lei and Gall, Juergen and Chin, Tat-Jun and Sato, Imari and Chellappa, Rama},
  doi          = {10.1007/s11263-024-02123-6},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6207},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on ACCV 2022},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic refiner: Object-aware temporal representation
learning for multi-view 3D detection and tracking. <em>IJCV</em>,
<em>132</em>(12), 6184–6206. (<a
href="https://doi.org/10.1007/s11263-024-02176-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified object-aware temporal learning framework for multi-view 3D detection and tracking tasks. Having observed that the efficacy of the temporal fusion strategy in recent multi-view perception methods may be weakened by distractors and background clutters in historical frames, we propose a cyclic learning mechanism to improve the robustness of multi-view representation learning. The essence is constructing a backward bridge to propagate information from model predictions (e.g., object locations and sizes) to image and BEV features, which forms a circle with regular inference. After backward refinement, the responses of target-irrelevant regions in historical frames would be suppressed, decreasing the risk of polluting future frames and improving the object awareness ability of temporal fusion. We further tailor an object-aware association strategy for tracking based on the cyclic learning model. The cyclic learning model not only provides refined features, but also delivers finer clues (e.g., scale level) for tracklet association. The proposed cycle learning method and association module together contribute a novel and unified multi-task framework. Experiments on nuScenes show that the proposed model achieves consistent performance gains over baselines of different designs (i.e., dense query-based BEVFormer, sparse query-based SparseBEV and LSS-based BEVDet4D) on both detection and tracking evaluation. Codes and models will be released.},
  archive      = {J_IJCV},
  author       = {Guo, Mingzhe and Zhang, Zhipeng and Jing, Liping and He, Yuan and Wang, Ke and Fan, Heng},
  doi          = {10.1007/s11263-024-02176-7},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6184-6206},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cyclic refiner: Object-aware temporal representation learning for multi-view 3D detection and tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unsupervised domain adaptation via
domain-transformer. <em>IJCV</em>, <em>132</em>(12), 6163–6183. (<a
href="https://doi.org/10.1007/s11263-024-02174-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a vital problem in pattern analysis and machine intelligence, Unsupervised Domain Adaptation (UDA) attempts to transfer an effective feature learner from a labeled source domain to an unlabeled target domain. Inspired by the success of the Transformer, several advances in UDA are achieved by adopting pure transformers as network architectures, but such a simple application can only capture patch-level information and lacks interpretability. To address these issues, we propose the Domain-Transformer (DoT) with domain-level attention mechanism to capture the long-range correspondence between the cross-domain samples. On the theoretical side, we provide a mathematical understanding of DoT: (1) We connect the domain-level attention with optimal transport theory, which provides interpretability from Wasserstein geometry; (2) From the perspective of learning theory, Wasserstein distance-based generalization bounds are derived, which explains the effectiveness of DoT for knowledge transfer. On the methodological side, DoT integrates the domain-level attention and manifold structure regularization, which characterize the sample-level information and locality consistency for cross-domain cluster structures. Besides, the domain-level attention mechanism can be used as a plug-and-play module, so DoT can be implemented under different neural network architectures. Instead of explicitly modeling the distribution discrepancy at domain-level or class-level, DoT learns transferable features under the guidance of long-range correspondence, so it is free of pseudo-labels and explicit domain discrepancy optimization. Extensive experiment results on several benchmark datasets validate the effectiveness of DoT.},
  archive      = {J_IJCV},
  author       = {Ren, Chuan-Xian and Zhai, Yiming and Luo, You-Wei and Yan, Hong},
  doi          = {10.1007/s11263-024-02174-9},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6163-6183},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards unsupervised domain adaptation via domain-transformer},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision transformers: From semantic segmentation to dense
prediction. <em>IJCV</em>, <em>132</em>(12), 6142–6162. (<a
href="https://doi.org/10.1007/s11263-024-02173-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, our model, termed as SEgmentation TRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the test leaderboard on the day of submission) and performs competitively on Cityscapes. However, the basic ViT architecture falls short in broader dense prediction applications, such as object detection and instance segmentation, due to its lack of a pyramidal structure, high computational demand, and insufficient local context. For tackling general dense visual prediction tasks in a cost-effective manner, we further formulate a family of Hierarchical Local-Global (HLG) Transformers, characterized by local attention within windows and global-attention across windows in a pyramidal architecture. Extensive experiments show that our methods achieve appealing performance on a variety of dense prediction tasks (e.g., object detection and instance segmentation and semantic segmentation) as well as image classification. Our code and models are available at https://github.com/fudan-zvg/SETR .},
  archive      = {J_IJCV},
  author       = {Zhang, Li and Lu, Jiachen and Zheng, Sixiao and Zhao, Xinxuan and Zhu, Xiatian and Fu, Yanwei and Xiang, Tao and Feng, Jianfeng and Torr, Philip H. S.},
  doi          = {10.1007/s11263-024-02173-w},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6142-6162},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vision transformers: From semantic segmentation to dense prediction},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust image restoration with an adaptive huber function
based fidelity. <em>IJCV</em>, <em>132</em>(12), 6127–6141. (<a
href="https://doi.org/10.1007/s11263-024-02163-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous image restoration algorithms have been proposed in the last several decades. These algorithms usually optimize an objective function consisting of an $${\ell _2}$$ norm based fidelity and a regularization term, whose optimality could be justified from the view of maximum a posteriori estimation with an assumption that the noise is Gaussian. However, it is known that the $${\ell _2}$$ norm based fidelity is very sensitive to gross errors that may appear in the observation. Since real-world image restoration tasks are usually hindered by abnormal pixels, impulsive noise, and other heavy-tailed noise, the utility of these traditional algorithms is limited. Although some robust algorithms have been proposed by replacing the $${\ell _2}$$ norm based fidelity with a robust one, they are designed for specific restoration tasks (e.g., multi-frame super-resolution) with a fixed image prior (e.g., the total-variation) and have not provided a principled way to justify the choice of a robust fidelity term. Currently designing a robust algorithm for general image restoration tasks is still an open problem. This paper studies the problem of robust image restoration in both theoretical and algorithmic manners. In the theoretical part, we point out that Huber function based fidelity could be justified from the pespective of minimax estimation, which facilities the choice of the robust fidelity term. In the algorithmic part, we first propose an adaptive approach to set the threshold of the Huber function, and then we derive an efficient and flexible method to solve the proposed robust formulation of the image restoration problem, which enables the proposed algorithm to incorporate various image priors. Experiments have demonstrated the robustness of the proposed algorithm and its utility in real-world image restoration tasks.},
  archive      = {J_IJCV},
  author       = {Song, Lingfei and Huang, Hua},
  doi          = {10.1007/s11263-024-02163-y},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6127-6141},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust image restoration with an adaptive huber function based fidelity},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting deep ensemble for out-of-distribution detection:
A loss landscape perspective. <em>IJCV</em>, <em>132</em>(12),
6107–6126. (<a
href="https://doi.org/10.1007/s11263-024-02156-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Out-of-Distribution (OoD) detection methods address to detect OoD samples from In-Distribution (InD) data mainly by exploring differences in features, logits and gradients in Deep Neural Networks (DNNs). We in this work propose a new perspective upon loss landscape and mode ensemble to investigate OoD detection. In the optimization of DNNs, there exist many local optima in the parameter space, or namely modes. Interestingly, we observe that these independent modes, which all reach low-loss regions with InD data (training and test data), yet yield significantly different loss landscapes with OoD data. Such an observation provides a novel view to investigate the OoD detection from the loss landscape, and further suggests significantly fluctuating OoD detection performance across these modes. For instance, FPR values of the RankFeat (Song et al. in Advances in Neural Information Processing Systems 35:17885–17898, 2022) method can range from 46.58% to 84.70% among 5 modes, showing uncertain detection performance evaluations across independent modes. Motivated by such diversities on OoD loss landscape across modes, we revisit the deep ensemble method for OoD detection through mode ensemble, leading to improved performance and benefiting the OoD detector with reduced variances. Extensive experiments covering varied OoD detectors and network structures illustrate high variances across modes and validate the superiority of mode ensemble in boosting OoD detection. We hope this work could attract attention in the view of independent modes in the loss landscape of OoD data and more reliable evaluations on OoD detectors.},
  archive      = {J_IJCV},
  author       = {Fang, Kun and Tao, Qinghua and Huang, Xiaolin and Yang, Jie},
  doi          = {10.1007/s11263-024-02156-x},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6107-6126},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting deep ensemble for out-of-distribution detection: A loss landscape perspective},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous semantic transfer for multi-label recognition
with partial labels. <em>IJCV</em>, <em>132</em>(12), 6091–6106. (<a
href="https://doi.org/10.1007/s11263-024-02127-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition with partial labels (MLR-PL), in which some labels are known while others are unknown for each image, may greatly reduce the cost of annotation and thus facilitate large-scale MLR. We find that strong semantic correlations exist within each image and across different images, and these correlations can help transfer the knowledge possessed by the known labels to retrieve the unknown labels and thus improve the performance of the MLR-PL task. In this work, we propose a novel heterogeneous semantic transfer (HST) framework that consists of two complementary transfer modules that explore both within-image and cross-image semantic correlations to transfer the knowledge possessed by known labels to generate pseudo labels for the unknown labels. Specifically, an intra-image semantic transfer (IST) module learns an image-specific label co-occurrence matrix for each image and maps the known labels to complement the unknown labels based on these matrices. Additionally, a cross-image transfer (CST) module learns category-specific feature-prototype similarities and then helps complement the unknown labels that have high degrees of similarity with the corresponding prototypes. It is worthy-noting that the HST framework requires searching appropriate thresholds to determine the co-occurrence and similarity scores to generate pseudo labels for the IST and CST modules, respectively. To avoid highly time-consuming and resource-intensive manual tuning, we introduce a differential threshold learning algorithm that adjusts the nondifferential indication function to a differential formulation to automatically learn the appropriate thresholds. Finally, both the known and generated pseudo labels are used to train MLR models. Extensive experiments conducted on the Microsoft COCO, Visual Genome, and Pascal VOC 2007 datasets show that the proposed HST framework achieves superior performance to that of current state-of-the-art algorithms. Specifically, it obtains mean average precision (mAP) improvements of 1.4, 3.3, and 0.4% on the three datasets over the results of the best-performing previously developed algorithm.},
  archive      = {J_IJCV},
  author       = {Chen, Tianshui and Pu, Tao and Liu, Lingbo and Shi, Yukai and Yang, Zhijing and Lin, Liang},
  doi          = {10.1007/s11263-024-02127-2},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6091-6106},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Heterogeneous semantic transfer for multi-label recognition with partial labels},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Procedure-aware action quality assessment: Datasets and
performance evaluation. <em>IJCV</em>, <em>132</em>(12), 6069–6090. (<a
href="https://doi.org/10.1007/s11263-024-02146-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of procedure-aware action quality assessment, which analyzes the action quality by delving into the semantic and spatial-temporal relationships among various composed steps of the action. Most existing action quality assessment methods regress on deep features of entire videos to learn diverse scores, which ignore the relationships among different fine-grained steps in actions and result in limitations in visual interpretability and generalization ability. To address these issues, we construct a fine-grained competitive sports video dataset called FineDiving with detailed semantic and temporal annotations, which helps understand the internal structures of each action. We also propose a new approach (i.e., spatial-temporal segmentation attention, STSA) that introduces procedure segmentation to parse an action into consecutive steps, learns powerful representations from these steps by constructing spatial motion attention and procedure-aware cross-attention, and designs a fine-grained contrastive regression to achieve an interpretable scoring mechanism. In addition, we build a benchmark on the FineDiving dataset to evaluate the performance of representative action quality assessment methods. Then, we expand FineDiving to FineDiving+ and construct three new benchmarks to investigate the transferable abilities between different diving competitions, between synchronized and individual dives, and between springboard and platform dives to demonstrate the generalization abilities of our STSA in unknown scenarios, scoring rules, action types, and difficulty degrees. Extensive experiments demonstrate that our approach, designed for procedure-aware action quality assessment, achieves substantial improvements. Our dataset and code are available at https://github.com/xujinglin/FineDiving .},
  archive      = {J_IJCV},
  author       = {Xu, Jinglin and Rao, Yongming and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-024-02146-z},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6069-6090},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Procedure-aware action quality assessment: Datasets and performance evaluation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast global image smoothing via quasi weighted least
squares. <em>IJCV</em>, <em>132</em>(12), 6039–6068. (<a
href="https://doi.org/10.1007/s11263-024-02105-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image smoothing is a long-studied research area with tremendous approaches proposed. However, how to perform high-quality image smoothing with less computational cost still remains a challenging problem. In this paper, we try to solve this problem with a newly proposed global optimization based method named quasi weighted least squares. In our method, the 2D image is first re-ordered into a 1D vector via a newly proposed 2D-to-1D transformation. We then properly remove some original 2D neighborhood connections. The remaining neighboring pixels can simply form 1D neighborhood connections in the transformed 1D vector while they still contain the 2D neighborhood information in the original 2D image space. These together result in a quite compact linear system that can be easily and efficiently solved, which makes our method a fast global image smoothing approach. Our method is on par with the fastest approaches in terms of processing speed, however, it is able to yield comparable performance with the state-of-the-art ones in terms of smoothing quality. Our method can also work as a solver to approximate the weighted least squares problem in complex systems, and it can achieve similar results but runs much faster. The efficiency and effectiveness of our method are validated through comprehensive experiments in several tasks. Our code is publicly available at: https://github.com/wliusjtu/Q-WLS.},
  archive      = {J_IJCV},
  author       = {Liu, Wei and Zhang, Pingping and Qin, Hongxing and Huang, Xiaolin and Yang, Jie and Ng, Michael},
  doi          = {10.1007/s11263-024-02105-8},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6039-6068},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast global image smoothing via quasi weighted least squares},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning feature restoration transformer for robust dehazing
visual object tracking. <em>IJCV</em>, <em>132</em>(12), 6021–6038. (<a
href="https://doi.org/10.1007/s11263-024-02182-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep-learning-based visual object tracking has obtained promising results. However, a drastic performance drop is observed when transferring a pre-trained model to changing weather conditions, such as hazy imaging scenarios, where the data distribution differs from that of a natural training set. This problem challenges the open-world practical applications of accurate target tracking. In principle, visual tracking performance relies on the discriminative degree of features between the target and its surroundings, rather than the image-level visual quality. To this end, we design a feature restoration transformer that adaptively enhances the representation capability of the extracted visual features for robust tracking in both natural and hazy scenarios. Specifically, a feature restoration transformer is constructed with dedicated self-attention hierarchies for the refinement of potentially contaminated deep feature maps. We endow the feature extraction process with a refinement mechanism typically for hazy imaging scenarios, establishing a tracking system that is robust against foggy videos. In essence, the feature restoration transformer is jointly trained with a Siamese tracking transformer. Intuitively, the supervision for learning discriminative and salient features is facilitated by the entire restoration tracking system. The experimental results obtained on hazy imaging scenarios demonstrate the merits and superiority of the proposed restoration tracking system, with complementary restoration power to image-level dehazing. In addition, consistent advantages of our design can be observed when generalised to different video attributes, demonstrating its capacity to deal with open-world scenarios.},
  archive      = {J_IJCV},
  author       = {Xu, Tianyang and Pan, Yifan and Feng, Zhenhua and Zhu, Xuefeng and Cheng, Chunyang and Wu, Xiao-Jun and Kittler, Josef},
  doi          = {10.1007/s11263-024-02182-9},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6021-6038},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning feature restoration transformer for robust dehazing visual object tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal prototypes for open-world semantic segmentation.
<em>IJCV</em>, <em>132</em>(12), 6004–6020. (<a
href="https://doi.org/10.1007/s11263-024-02165-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semantic segmentation, generalizing a visual system to both seen categories and novel categories at inference time has always been practically valuable yet challenging. To enable such functionality, existing methods mainly rely on either providing several support demonstrations from the visual aspect or characterizing the informative clues from the textual aspect (e.g., the class names). Nevertheless, both two lines neglect the complementary intrinsic of low-level visual and high-level language information, while the explorations that consider visual and textual modalities as a whole to promote predictions are still limited. To close this gap, we propose to encompass textual and visual clues as multi-modal prototypes to allow more comprehensive support for open-world semantic segmentation, and build a novel prototype-based segmentation framework to realize this promise. To be specific, unlike the straightforward combination of bi-modal clues, we decompose the high-level language information as multi-aspect prototypes and aggregate the low-level visual information as more semantic prototypes, on basis of which, a fine-grained complementary fusion makes the multi-modal prototypes more powerful and accurate to promote the prediction. Based on an elastic mask prediction module that permits any number and form of prototype inputs, we are able to solve the zero-shot, few-shot and generalized counterpart tasks in one architecture. Extensive experiments on both PASCAL- $$5^i$$ and COCO- $$20^i$$ datasets show the consistent superiority of the proposed method compared with the previous state-of-the-art approaches, and a range of ablation studies thoroughly dissects each component in our framework both quantitatively and qualitatively that verify their effectiveness.},
  archive      = {J_IJCV},
  author       = {Yang, Yuhuan and Ma, Chaofan and Ju, Chen and Zhang, Fei and Yao, Jiangchao and Zhang, Ya and Wang, Yanfeng},
  doi          = {10.1007/s11263-024-02165-w},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {6004-6020},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal prototypes for open-world semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CD-iNet: Deep invertible network for perceptual image color
difference measurement. <em>IJCV</em>, <em>132</em>(12), 5983–6003. (<a
href="https://doi.org/10.1007/s11263-024-02087-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image color difference (CD) measurement, a crucial concept in color science and imaging technology, aims to quantify the perceived difference between two colors. Most widely recognized CD formulae are recommended by the Commission Internationale de l’Èclairage (CIE), which are tailored to homogeneous color patches and may not generalize effectively to images encompassing diverse content. Developing effective CD metrics for natural images remains an active and ongoing area of research. Drawing inspiration from the design principles found in CIE-recommended formulae, which place a premium on achieving a perceptually uniform color space, we posit that an ideal color space should adhere to the following criteria: (1) Characterizing any color pixel with three degrees of freedom, which is necessary and sufficient; (2) The visual distance between two pixels is proportional to the Euclidean distance, i.e., perceptual uniformity; (3) The transformation between color spaces is inherently reversible and has low computational complexity. To satisfy these criteria, we investigate to leverage deep invertible neural network (DINNs) to learn an invertible coordinate transform, in which the Euclidean distance is employed to compute the CD on a pixel-by-pixel basis within the transformed color space and subsequently average the resulting CD map to obtain the global CD for a pair of images. By using DINNs, the acquired coordinate transform can maintain three-dimensional properties and mathematical invertibility. The resulting metric, referred to as CD-iNet, is end-to-end optimized on color patch datasets and image datasets simultaneously. Extensive quantitative and qualitative experiments on smartphone photograph datasets demonstrate the superiority of CD-iNet over existing metrics. Besides, CD-iNet can produce competitive local CD maps without requiring dense supervision and be robust against geometric distortions. More importantly, the transformed color space exhibits reasonable characteristics of perceptual uniformity, e.g., low cross-contamination between color attributes. Codes are available at: https://github.com/hellooks/CD-iNet .},
  archive      = {J_IJCV},
  author       = {Wang, Zhihua and Xu, Keshuo and Ding, Keyan and Jiang, Qiuping and Zuo, Yifan and Ni, Zhangkai and Fang, Yuming},
  doi          = {10.1007/s11263-024-02087-7},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5983-6003},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CD-iNet: Deep invertible network for perceptual image color difference measurement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source-free domain adaptive object detection.
<em>IJCV</em>, <em>132</em>(12), 5950–5982. (<a
href="https://doi.org/10.1007/s11263-024-02170-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the transferability of object detection models in real-world scenarios where data is sampled from disparate distributions, considerable attention has been devoted to domain adaptive object detection (DAOD). Researchers have also investigated multi-source DAOD to confront the challenges posed by training samples originating from different source domains. However, existing methods encounter difficulties when source data is unavailable due to privacy preservation policies or transmission cost constraints. To address these issues, we introduce and address the problem of Multi-source-free Domain Adaptive Object Detection (MSFDAOD), which seeks to perform domain adaptation for object detection using multi-source-pretrained models without any source data or target labels. Specifically, we propose a novel Divide-and-Aggregate Contrastive Adaptation (DACA) framework. First, multiple mean-teacher detection models perform effective knowledge distillation and class-wise contrastive learning within each source domain feature space, denoted as “Divide”. Meanwhile, DACA integrates proposals, obtains unified pseudo-labels, and assigns dynamic weights to student prediction aggregation, denoted as “Aggregate”. The two-step process of “Divide” and “Aggregate” enables our method to efficiently leverage the advantages of multiple source-free models and aggregate their contributions to adaptation in a self-supervised manner. Extensive experiments are conducted on multiple popular benchmark datasets, and the results demonstrate that the proposed DACA framework significantly outperforms state-of-the-art approaches for MSFDAOD tasks.},
  archive      = {J_IJCV},
  author       = {Zhao, Sicheng and Yao, Huizai and Lin, Chuang and Gao, Yue and Ding, Guiguang},
  doi          = {10.1007/s11263-024-02170-z},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5950-5982},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-source-free domain adaptive object detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting diffusion prior for real-world image
super-resolution. <em>IJCV</em>, <em>132</em>(12), 5929–5949. (<a
href="https://doi.org/10.1007/s11263-024-02168-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution. Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we employ a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches. Code and models are available at https://github.com/IceClear/StableSR .},
  archive      = {J_IJCV},
  author       = {Wang, Jianyi and Yue, Zongsheng and Zhou, Shangchen and Chan, Kelvin C. K. and Loy, Chen Change},
  doi          = {10.1007/s11263-024-02168-7},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5929-5949},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploiting diffusion prior for real-world image super-resolution},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Infproto-powered adaptive classifier and agnostic feature
learning for single domain generalization in medical images.
<em>IJCV</em>, <em>132</em>(12), 5905–5928. (<a
href="https://doi.org/10.1007/s11263-024-02158-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a single domain generalization (DG) framework that generalizes from one source domain to arbitrary unseen domains is practical yet challenging in medical image segmentation, mainly due to the domain shift and limited source domain information. To tackle these issues, we reason that domain-adaptive classifier learning and domain-agnostic feature extraction are key components in single DG, and further propose an adaptive infinite prototypes (InfProto) scheme to facilitate the learning of the two components. InfProto harnesses high-order statistics and infinitely samples class-conditional instance-specific prototypes to form the classifier for discriminability enhancement. We then introduce probabilistic modeling and provide a theoretic upper bound to implicitly perform the infinite prototype sampling in the optimization of InfProto. Incorporating InfProto, we design a hierarchical domain-adaptive classifier to elasticize the model for varying domains. This classifier infinitely samples prototypes from the instance and mini-batch data distributions, forming the instance-level and mini-batch-level domain-adaptive classifiers, thereby generalizing to unseen domains. To extract domain-agnostic features, we assume each instance in the source domain is a micro source domain and then devise three complementary strategies, i.e., instance-level infinite prototype exchange, instance-batch infinite prototype interaction, and consistency regularization, to constrain outputs of the hierarchical domain-adaptive classifier. These three complementary strategies minimize distribution shifts among micro source domains, enabling the model to get rid of domain-specific characterizations and, in turn, concentrating on semantically discriminative features. Extensive comparison experiments demonstrate the superiority of our approach compared with state-of-the-art counterparts, and comprehensive ablation studies verify the effect of each proposed component. Notably, our method exhibits average improvements of 15.568% and 17.429% in dice on polyp and surgical instrument segmentation benchmarks.},
  archive      = {J_IJCV},
  author       = {Guo, Xiaoqing and Liu, Jie and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02158-9},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5905-5928},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Infproto-powered adaptive classifier and agnostic feature learning for single domain generalization in medical images},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploration and exploitation of unlabeled data for open-set
semi-supervised learning. <em>IJCV</em>, <em>132</em>(12), 5888–5904.
(<a href="https://doi.org/10.1007/s11263-024-02155-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address a complex but practical scenario in semi-supervised learning (SSL) named open-set SSL, where unlabeled data contain both in-distribution (ID) and out-of-distribution (OOD) samples. Unlike previous methods that only consider ID samples to be useful and aim to filter out OOD ones completely during training, we argue that the exploration and exploitation of both ID and OOD samples can benefit SSL. To support our claim, (i) we propose a prototype-based clustering and identification algorithm that explores the inherent similarity and difference among samples at feature level and effectively cluster them around several predefined ID and OOD prototypes, thereby enhancing feature learning and facilitating ID/OOD identification; (ii) we propose an importance-based sampling method that exploits the difference in importance of each ID and OOD sample to SSL, thereby reducing the sampling bias and improving the training. Our proposed method achieves state-of-the-art in several challenging benchmarks, and improves upon existing SSL methods even when ID samples are totally absent in unlabeled data.},
  archive      = {J_IJCV},
  author       = {Zhao, Ganlong and Li, Guanbin and Qin, Yipeng and Zhang, Jinjin and Chai, Zhenhua and Wei, Xiaolin and Lin, Liang and Yu, Yizhou},
  doi          = {10.1007/s11263-024-02155-y},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5888-5904},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploration and exploitation of unlabeled data for open-set semi-supervised learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unified defense for face forgery and spoofing
attacks via dual space reconstruction learning. <em>IJCV</em>,
<em>132</em>(12), 5862–5887. (<a
href="https://doi.org/10.1007/s11263-024-02151-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world face recognition systems are vulnerable to diverse face attacks, ranging from digitally manipulated artifacts to physically crafted spoofing attacks. Existing works primarily focus on using an image classification network to address one type of attack but disregarding another. However, face recognition systems in real-world scenarios always encounter diverse simultaneous attacks, rendering the aforementioned single-attack detecting solution ineffective. Besides, excessive reliance on a classifier might easily fail when encountering face attacks with unknown patterns, as the category-level difference learned by classification backbones cannot generalize well to new attacks. Considering that real data are captured from actual individuals, while attack samples are generated by various distinct techniques, our focus is on extracting compact representations of real faces. This approach allows us to identify the fundamental differences between genuine and attack images, enabling us to address both manipulated artifacts and spoofing attacks simultaneously. Concretely, we propose a dual space reconstruction learning framework that models the commonalities of genuine faces in both spatial and frequency domains. With the learned characteristics of real faces, the model is more likely to segregate diverse attack samples as outliers from genuine images. Besides, we introduce a dynamic filtering module that filters out the redundant information retained by the reconstruction and enhances the critical divergence between the real and the attack to achieve better classification features. Since the training samples only cover limited style variations, which hampers the generalization to unseen domains, we further design a consistency regularized training strategy that mimics distribution shifts during training and imposes specific constraints to encourage style-irrelevant features. Moreover, in view of the lack of accessible benchmarks for unified evaluation of the detection competence against both face forgery and spoofing attacks, we set up a new challenging benchmark, named UniAttack, to foster the exploration of effective solutions to face attack detection. Both qualitative and quantitative results from existing and proposed benchmarks unequivocally demonstrate the superiority of our methods over state-of-the-art approaches.},
  archive      = {J_IJCV},
  author       = {Cao, Junyi and Zhang, Ke-Yue and Yao, Taiping and Ding, Shouhong and Yang, Xiaokang and Ma, Chao},
  doi          = {10.1007/s11263-024-02151-2},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5862-5887},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards unified defense for face forgery and spoofing attacks via dual space reconstruction learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-driven heterogeneous network for video deraining.
<em>IJCV</em>, <em>132</em>(12), 5841–5861. (<a
href="https://doi.org/10.1007/s11263-024-02148-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring clear frames from rainy videos poses a significant challenge due to the swift motion of rain streaks. Traditional frame-based visual sensors, which record dense scene content synchronously, struggle to capture the fast-moving rain information. Conversely, the novel bio-inspired event camera, known for its high temporal resolution and low latency, effectively records the motion trajectories of rapidly falling rain through asynchronously generated sparse event sequences. In light of these attributes, we introduce a novel event-driven convolutional spiking network for video deraining. For video restoration, we employ a Convolutional Neural Network as the backbone, extracting dense features from video sequences to map rain-soaked frames to clear ones. To remove rain, we meticulously design a bio-inspired Spiking Neural Network that adapts to the sparse event sequences, capturing features of falling rain. We then establish a bimodal feature fusion module that combines dense convolutional features with sparse spiking features. This fusion aids the backbone in accurately pinpointing rain streaks across spatiotemporal dimensions. Thus, our diverse network extracts and collaborates information from both events and videos, enhancing deraining performance. Experiments conducted on synthetic and real-world datasets prove that our network markedly surpasses existing video deraining techniques.},
  archive      = {J_IJCV},
  author       = {Fu, Xueyang and Cao, Chengzhi and Xu, Senyan and Zhang, Fanrui and Wang, Kunyu and Zha, Zheng-Jun},
  doi          = {10.1007/s11263-024-02148-x},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5841-5861},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Event-driven heterogeneous network for video deraining},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PosMLP-video: Spatial and temporal relative position
encoding for efficient video recognition. <em>IJCV</em>,
<em>132</em>(12), 5820–5840. (<a
href="https://doi.org/10.1007/s11263-024-02154-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, vision Transformers and MLPs have demonstrated remarkable performance in image understanding tasks. However, their inherently dense computational operators, such as self-attention and token-mixing layers, pose significant challenges when applied to spatio-temporal video data. To address this gap, we propose PosMLP-Video, a lightweight yet powerful MLP-like backbone for video recognition. Instead of dense operators, we use efficient relative positional encoding to build pairwise token relations, leveraging small-sized parameterized relative position biases to obtain each relation score. Specifically, to enable spatio-temporal modeling, we extend the image PosMLP’s positional gating unit to temporal, spatial, and spatio-temporal variants, namely PoTGU, PoSGU, and PoSTGU, respectively. These gating units can be feasibly combined into three types of spatio-temporal factorized positional MLP blocks, which not only decrease model complexity but also maintain good performance. Additionally, we enrich relative positional relationships by using channel grouping. Experimental results on three video-related tasks demonstrate that PosMLP-Video achieves competitive speed-accuracy trade-offs compared to the previous state-of-the-art models. In particular, PosMLP-Video pre-trained on ImageNet1K achieves 59.0%/70.3% top-1 accuracy on Something-Something V1/V2 and 82.1% top-1 accuracy on Kinetics-400 while requiring much fewer parameters and FLOPs than other models. The code is released at https://github.com/zhouds1918/PosMLP_Video .},
  archive      = {J_IJCV},
  author       = {Hao, Yanbin and Zhou, Diansong and Wang, Zhicai and Ngo, Chong-Wah and Wang, Meng},
  doi          = {10.1007/s11263-024-02154-z},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5820-5840},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PosMLP-video: Spatial and temporal relative position encoding for efficient video recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FD-GAN: Generalizable and robust forgery detection via
generative adversarial networks. <em>IJCV</em>, <em>132</em>(12),
5801–5819. (<a
href="https://doi.org/10.1007/s11263-024-02136-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalization across various forgeries and robustness against corruption are pressing challenges of forgery detection. Although previous works boost generalization with the help of data augmentations, they rarely consider the robustness against corruption. To tackle these two issues of generalization and robustness simultaneously, in this paper, we propose a novel forgery detection generative adversarial network (FD-GAN), which consists of two generators (a blend-based generator and a transfer-based generator) and a discriminator. Concretely, the blend-based generator and the transfer-based generator can adaptively create challenging synthetic images with more flexible strategies to improve generalization. Besides, the discriminator is designed to judge whether the input is synthetic and predicts the manipulated regions with a collaboration of spatial and frequency branches. And the frequency branch utilizes Low-rank Estimation algorithms to filter out adversarial corruption in the input for robustness. Furthermore, to present a deeper understanding of FD-GAN, we apply theoretical analysis on forgery detection, which provides some guidelines on data augmentations for improving generalization and mathematical support for robustness. Extensive experiments demonstrate that FD-GAN exhibits better generalization and robustness. For example, FD-GAN outperforms 14 existing methods on 3 benchmarks in generalization evaluation, and it separately improves the performance against 6 kinds of adversarial attacks and 7 types of distortions by 16.2% and 2.3% on average in robustness evaluation.},
  archive      = {J_IJCV},
  author       = {Xu, Nanqing and Feng, Weiwei and Zhang, Tianzhu and Zhang, Yongdong},
  doi          = {10.1007/s11263-024-02136-1},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5801-5819},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FD-GAN: Generalizable and robust forgery detection via generative adversarial networks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional prompting for anti-forgetting in domain
incremental learning. <em>IJCV</em>, <em>132</em>(12), 5783–5800. (<a
href="https://doi.org/10.1007/s11263-024-02134-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Incremental Learning (DIL) focuses on handling complex domain shifts of a continuous data stream for visual tasks such as image classification and image segmentation. In real life, severe domain gaps in DIL are generated from various sources such as data style shifts, data quality degradation, environment changes, and so on. The well-known catastrophic forgetting issue in DIL becomes even more critical when simultaneously considering multiple sources of domain shifts. In this paper, we propose a unified and effective paradigm named Compositional Prompting (C-Prompt) to mitigate the critical forgetting challenge in DIL for image classification tasks. Unlike a popular type of conventional DIL approaches that need to retain abundant exemplars from the old domains, our exemplar-free C-Prompt leverages a prompt-guided Batch-wise Exponential Moving Average (BEMA) strategy to adaptively consolidate learned knowledge without retaining any exemplars. A set of prompts shared across different domains is designed to estimate the knowledge shifts for automatically balancing knowledge acquisition and forgetting. To enhance the learning ability, our proposed C-Prompt explores a domain-specific pool of learnable prompts for each domain, and all the prompt pools are further exploited in a cross-domain compositional manner to facilitate inference. Since the latest prompting-based DIL methods aim to learn one individual prompt for each domain, they always suffer from critical performance degradation caused by the incorrect prediction of domain index during inference and the limited learning capacity by using a single prompt per domain. Instead, our C-Prompt can not only readily acquire domain-specific knowledge but also exploit domain-shared knowledge. Extensive experiments on various large-scale multi-domain benchmarks have demonstrated the superiority of our proposed C-Prompt compared with state-of-the-art methods. Code is available at https://github.com/zhoujiahuan1991/IJCV2024-C-Prompt .},
  archive      = {J_IJCV},
  author       = {Liu, Zichen and Peng, Yuxin and Zhou, Jiahuan},
  doi          = {10.1007/s11263-024-02134-3},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5783-5800},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Compositional prompting for anti-forgetting in domain incremental learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization via ensemble stacking for face
presentation attack detection. <em>IJCV</em>, <em>132</em>(12),
5759–5782. (<a
href="https://doi.org/10.1007/s11263-024-02152-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face presentation attack detection (PAD) plays a pivotal role in securing face recognition systems against spoofing attacks. Although great progress has been made in designing face PAD methods, developing a model that can generalize well to unseen test domains remains a significant challenge. Moreover, due to the different types of spoofing attacks, creating a dataset with a sufficient number of samples for training deep neural networks is a laborious task. This work proposes a comprehensive solution that combines synthetic data generation and deep ensemble learning to enhance the generalization capabilities of face PAD. Specifically, synthetic data is generated by blending a static image with spatiotemporal-encoded images using alpha composition and video distillation. In this way, we simulate motion blur with varying alpha values, thereby generating diverse subsets of synthetic data that contribute to a more enriched training set. Furthermore, multiple base models are trained on each subset of synthetic data using stacked ensemble learning. This allows the models to learn complementary features and representations from different synthetic subsets. The meta-features generated by the base models are used as input for a new model called the meta-model. The latter combines the predictions from the base models, leveraging their complementary information to better handle unseen target domains and enhance overall performance. Experimental results from seven datasets—WMCA, CASIA-SURF, OULU-NPU, CASIA-MFSD, Replay-Attack, MSU-MFSD, and SiW-Mv2—highlight the potential to enhance presentation attack detection by using large-scale synthetic data and a stacking-based ensemble approach.},
  archive      = {J_IJCV},
  author       = {Muhammad, Usman and Laaksonen, Jorma and Romaissa Beddiar, Djamila and Oussalah, Mourad},
  doi          = {10.1007/s11263-024-02152-1},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5759-5782},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Domain generalization via ensemble stacking for face presentation attack detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-vocabulary animal keypoint detection with
semantic-feature matching. <em>IJCV</em>, <em>132</em>(12), 5741–5758.
(<a href="https://doi.org/10.1007/s11263-024-02126-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current image-based keypoint detection methods for animal (including human) bodies and faces are generally divided into fully supervised and few-shot class-agnostic approaches. The former typically relies on laborious and time-consuming manual annotations, posing considerable challenges in expanding keypoint detection to a broader range of keypoint categories and animal species. The latter, though less dependent on extensive manual input, still requires necessary support images with annotation for reference during testing. To realize zero-shot keypoint detection without any prior annotation, we introduce the Open-Vocabulary Keypoint Detection (OVKD) task, which is innovatively designed to use text prompts for identifying arbitrary keypoints across any species. In pursuit of this goal, we have developed a novel framework named Open-Vocabulary Keypoint Detection with Semantic-feature Matching (KDSM). This framework synergistically combines vision and language models, creating an interplay between language features and local keypoint visual features. KDSM enhances its capabilities by integrating Domain Distribution Matrix Matching (DDMM) and other special modules, such as the Vision-Keypoint Relational Awareness (VKRA) module, improving the framework’s generalizability and overall performance. Our comprehensive experiments demonstrate that KDSM significantly outperforms the baseline in terms of performance and achieves remarkable success in the OVKD task. Impressively, our method, operating in a zero-shot fashion, still yields results comparable to state-of-the-art few-shot species class-agnostic keypoint detection methods. Codes and data are available at https://github.com/zhanghao5201/KDSM .},
  archive      = {J_IJCV},
  author       = {Zhang, Hao and Xu, Lumin and Lai, Shenqi and Shao, Wenqi and Zheng, Nanning and Luo, Ping and Qiao, Yu and Zhang, Kaipeng},
  doi          = {10.1007/s11263-024-02126-3},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5741-5758},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Open-vocabulary animal keypoint detection with semantic-feature matching},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial immune system of secure face recognition against
adversarial attacks. <em>IJCV</em>, <em>132</em>(12), 5718–5740. (<a
href="https://doi.org/10.1007/s11263-024-02153-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based face recognition models are vulnerable to adversarial attacks. In contrast to general noises, the presence of imperceptible adversarial noises can lead to catastrophic errors in deep face recognition models. The primary difference between adversarial noise and general noise lies in its specificity. Adversarial attack methods give rise to noises tailored to the characteristics of the individual image and recognition model at hand. Diverse samples and recognition models can engender specific adversarial noise patterns, which pose significant challenges for adversarial defense. Addressing this challenge in the realm of face recognition presents a more formidable endeavor due to the inherent nature of face recognition as an open set task. In order to tackle this challenge, it is imperative to employ customized processing for each individual input sample. Drawing inspiration from the biological immune system, which can identify and respond to various threats, this paper aims to create an artificial immune system to provide adversarial defense for face recognition. The proposed defense model incorporates the principles of antibody cloning, mutation, selection, and memory mechanisms to generate a distinct “antibody” for each input sample, wherein the term “antibody” refers to a specialized noise removal manner. Furthermore, we introduce a self-supervised adversarial training mechanism that serves as a simulated rehearsal of immune system invasions. Extensive experimental results demonstrate the efficacy of the proposed method, surpassing state-of-the-art adversarial defense methods. The source code is available here , or you can visit this website: https://github.com/RenMin1991/SIDE},
  archive      = {J_IJCV},
  author       = {Ren, Min and Wang, Yunlong and Zhu, Yuhao and Huang, Yongzhen and Sun, Zhenan and Li, Qi and Tan, Tieniu},
  doi          = {10.1007/s11263-024-02153-0},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5718-5740},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Artificial immune system of secure face recognition against adversarial attacks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RNAS-CL: Robust neural architecture search by cross-layer
knowledge distillation. <em>IJCV</em>, <em>132</em>(12), 5698–5717. (<a
href="https://doi.org/10.1007/s11263-024-02133-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks are often vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the tools for developing novel deep neural architectures, demonstrates superior performance in prediction accuracy in various machine learning applications. However, the performance of a neural architecture discovered by NAS against adversarial attacks has not been sufficiently studied, especially under the regime of knowledge distillation. Given the presence of a robust teacher, we investigate if NAS would produce a robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer knowledge distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student-teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental results demonstrate the effectiveness of RNAS-CL and show that RNAS-CL produces compact and adversarially robust neural architectures. Our results point to new approaches for finding compact and robust neural architecture for many applications. The code of RNAS-CL is available at https://github.com/Statistical-Deep-Learning/RNAS-CL .},
  archive      = {J_IJCV},
  author       = {Nath, Utkarsh and Wang, Yancheng and Turaga, Pavan and Yang, Yingzhen},
  doi          = {10.1007/s11263-024-02133-4},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5698-5717},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RNAS-CL: Robust neural architecture search by cross-layer knowledge distillation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HybridPrompt: Domain-aware prompting for cross-domain
few-shot learning. <em>IJCV</em>, <em>132</em>(12), 5681–5697. (<a
href="https://doi.org/10.1007/s11263-024-02086-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Learning (CD-FSL) aims at recognizing unseen classes from target domains that vastly differ from training classes from source domains, utilizing only a few labeled samples. However, the substantial domain disparities between target and source domains pose huge challenges to few-shot generalization. To resolve domain disparities, we propose HybridPrompt, a novel architecture for Domain-Aware Prompting that integrates a variety of cross-domain learned prompts as knowledge experts for CD-FSL. The proposed method enjoys several merits. First, to encode knowledge from diverse source domains, several Domain Prompts are introduced to capture domain-specific knowledge. Subsequently, to facilitate the cross-domain transfer of valuable knowledge, a Transferred Prompt is specifically tailored for each target task by retrieving highly relevant Domain Prompts based on domain properties. Finally, to complement insufficient transferred information, an Adaptive Prompt is learned to incorporate additional target characteristics for model adaptation. Consequently, the collaboration of these three types of prompts contributes to a hybridly prompted model that achieves domain-aware encoding, transfer, and adaptation, thereby enhancing adaptability on unseen domains. Extensive experimental results on the Meta-Dataset benchmark demonstrate that our method achieves superior performance against state-of-the-art methods. The source code is available at https://github.com/Jamine-W/HybridPrompt .},
  archive      = {J_IJCV},
  author       = {Wu, Jiamin and Zhang, Tianzhu and Zhang, Yongdong},
  doi          = {10.1007/s11263-024-02086-8},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5681-5697},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HybridPrompt: Domain-aware prompting for cross-domain few-shot learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning spatiotemporal inconsistency via thumbnail layout
for face deepfake detection. <em>IJCV</em>, <em>132</em>(12), 5663–5680.
(<a href="https://doi.org/10.1007/s11263-024-02054-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection. Current video-level methods are mostly based on 3D CNNs resulting in high computational demands, although have achieved good performance. This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. This transformation process involves sequentially masking frames at the same positions within each frame. These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails. TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications. Furthermore, we introduce a graph reasoning block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues. The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability. Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems. The code is available at https://github.com/rainy-xu/TALL4Deepfake .},
  archive      = {J_IJCV},
  author       = {Xu, Yuting and Liang, Jian and Sheng, Lijun and Zhang, Xiao-Yu},
  doi          = {10.1007/s11263-024-02054-2},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5663-5680},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning spatiotemporal inconsistency via thumbnail layout for face deepfake detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized out-of-distribution detection: A survey.
<em>IJCV</em>, <em>132</em>(12), 5635–5662. (<a
href="https://doi.org/10.1007/s11263-024-02117-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e.,AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. Despite comprehensive surveys of related fields, the summarization of OOD detection methods remains incomplete and requires further advancement. This paper specifically addresses the gap in recent technical developments in the field of OOD detection. It also provides a comprehensive discussion of representative methods from other sub-tasks and how they relate to and inspire the development of OOD detection methods. The survey concludes by identifying open challenges and potential research directions.},
  archive      = {J_IJCV},
  author       = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02117-4},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5635-5662},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized out-of-distribution detection: A survey},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pattern-expandable image copy detection. <em>IJCV</em>,
<em>132</em>(12), 5618–5634. (<a
href="https://doi.org/10.1007/s11263-024-02140-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-world visual recognition aims to empower models to identify objects in real-world settings, particularly when they encounter domains or categories that are not included in the training dataset. This paper proposes a specific open-world visual recognition task, i.e. Pattern-Expandable Image Copy Detection (PE-ICD). In realistic scenarios, the continuous emergence of novel tampering patterns necessitates fast upgrades to the ICD system to prevent confusion in already-trained models. Therefore, our PE-ICD focuses on two aspects, i.e., rehearsal-free upgrade and backward-compatible deployment: (1) The rehearsal-free upgrade utilizes only the new patterns to save time, as re-training on the old patterns can be very time-consuming. (2) The backward-compatible deployment allows for comparing the updated query features against the outdated gallery features, thereby avoiding the need to re-extract features for the extensively large gallery. To lay the foundation for PE-ICD research, we construct the first regulated pattern set, CrossPattern, and propose Pattern Stripping (P-Strip). CrossPattern regulates both base and novel patterns during the initial training and subsequent upgrades. Given a query, our P-Strip separates the tamper patterns by decomposing it into an image feature and multiple pattern features. The advantage of P-Strip is that we can easily introduce new pattern features with minimal impact on the image feature and previously seen pattern features. Experimental results show that P-Strip supports both rehearsal-free upgrading and backward compatibility. Our code is publicly available at https://github.com/WangWenhao0716/PEICD .},
  archive      = {J_IJCV},
  author       = {Wang, Wenhao and Sun, Yifan and Yang, Yi},
  doi          = {10.1007/s11263-024-02140-5},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5618-5634},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pattern-expandable image copy detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning temporal variations for 4D point cloud
segmentation. <em>IJCV</em>, <em>132</em>(12), 5603–5617. (<a
href="https://doi.org/10.1007/s11263-024-02149-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR-based 3D scene perception is a fundamental and important task for autonomous driving. Most state-of-the-art methods on LiDAR-based 3D recognition tasks focus on single-frame 3D point cloud data, ignoring temporal information. We argue that the temporal information across the frames provides crucial knowledge for 3D scene perceptions, especially in the driving scenario. In this paper, we focus on spatial and temporal variations to better explore temporal information across 3D frames. We design a temporal variation-aware interpolation module and a temporal voxel-point refinement module to capture the temporal variation in the 4D point cloud. The temporal variation-aware interpolation generates local features from the previous and current frames by capturing spatial coherence and temporal variation information. The temporal voxel-point refinement module builds a temporal graph on the 3D point cloud sequences and captures the temporal variation with a graph convolution module, transforming coarse voxel-level predictions into fine point-level predictions. With our proposed modules, we achieve superior performances on SemanticKITTI, SemantiPOSS and NuScenes.},
  archive      = {J_IJCV},
  author       = {Shi, Hanyu and Wei, Jiacheng and Wang, Hao and Liu, Fayao and Lin, Guosheng},
  doi          = {10.1007/s11263-024-02149-w},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5603-5617},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning temporal variations for 4D point cloud segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-temporal supervised learning for universal remote
sensing change detection. <em>IJCV</em>, <em>132</em>(12), 5582–5602.
(<a href="https://doi.org/10.1007/s11263-024-02141-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitemporal supervised learning paradigm always dominates remote sensing change detection using numerous labeled bitemporal image pairs, especially for high spatial resolution (HSR) remote sensing imagery. However, it is very expensive and labor-intensive to label change regions in large-scale bitemporal HSR remote sensing image pairs. In this paper, we propose single-temporal supervised learning (STAR) for universal remote sensing change detection from a new perspective of exploiting changes between unpaired images as supervisory signals. STAR enables us to train a high-accuracy change detector only using unpaired labeled images and can generalize to real-world bitemporal image pairs. To demonstrate the flexibility and scalability of STAR, we design a simple yet unified change detector, termed ChangeStar2, capable of addressing binary change detection, object change detection, and semantic change detection in one architecture. ChangeStar2 achieves state-of-the-art performances on eight public remote sensing change detection datasets, covering above two supervised settings, multiple change types, multiple scenarios.},
  archive      = {J_IJCV},
  author       = {Zheng, Zhuo and Zhong, Yanfei and Ma, Ailong and Zhang, Liangpei},
  doi          = {10.1007/s11263-024-02141-4},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5582-5602},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Single-temporal supervised learning for universal remote sensing change detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inter-feature relationship certifies robust generalization
of adversarial training. <em>IJCV</em>, <em>132</em>(12), 5565–5581. (<a
href="https://doi.org/10.1007/s11263-024-02111-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whilst adversarial training has been shown as a promising wisdom to promote model robustness in computer vision and machine learning, adversarially trained models often suffer from poor robust generalization on unseen adversarial examples. Namely, there still remains a big gap between the performance on training and test adversarial examples. In this paper, we propose to tackle this issue from a new perspective of the inter-feature relationship. Specifically, we aim to generate adversarial examples which maximize the loss function while maintaining the inter-feature relationship of natural data as well as penalizing the correlation distance between natural features and adversarial counterparts. As a key contribution, we prove that training with such examples while penalizing the distance between correlations can help promote both the generalization on natural and adversarial examples theoretically. We empirically validate our method through extensive experiments over different vision datasets (CIFAR-10, CIFAR-100, and SVHN), against several competitive methods. Our method substantially outperforms the baseline adversarial training by a large margin, especially for PGD20 on CIFAR-10, CIFAR-100, and SVHN with around 20%, 15% and 29% improvements.},
  archive      = {J_IJCV},
  author       = {Zhang, Shufei and Qian, Zhuang and Huang, Kaizhu and Wang, Qiu-Feng and Gu, Bin and Xiong, Huan and Yi, Xinping},
  doi          = {10.1007/s11263-024-02111-w},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5565-5581},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Inter-feature relationship certifies robust generalization of adversarial training},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards task sampler learning for meta-learning.
<em>IJCV</em>, <em>132</em>(12), 5534–5564. (<a
href="https://doi.org/10.1007/s11263-024-02145-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning aims to learn general knowledge with diverse training tasks conducted from limited data, and then transfer it to new tasks. It is commonly believed that increasing task diversity will enhance the generalization ability of meta-learning models. However, this paper challenges this view through empirical and theoretical analysis. We obtain three conclusions: (i) there is no universal task sampling strategy that can guarantee the optimal performance of meta-learning models; (ii) over-constraining task diversity may incur the risk of under-fitting or over-fitting during training; and (iii) the generalization performance of meta-learning models are affected by task diversity, task entropy, and task difficulty. Based on this insight, we design a novel task sampler, called Adaptive Sampler (ASr). ASr is a plug-and-play module that can be integrated into any meta-learning framework. It dynamically adjusts task weights according to task diversity, task entropy, and task difficulty, thereby obtaining the optimal probability distribution for meta-training tasks. Finally, we conduct experiments on a series of benchmark datasets across various scenarios, and the results demonstrate that ASr has clear advantages. The code is publicly available at https://github.com/WangJingyao07/Adaptive-Sampler .},
  archive      = {J_IJCV},
  author       = {Wang, Jingyao and Qiang, Wenwen and Su, Xingzhe and Zheng, Changwen and Sun, Fuchun and Xiong, Hui},
  doi          = {10.1007/s11263-024-02145-0},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5534-5564},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards task sampler learning for meta-learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond learned metadata-based raw image reconstruction.
<em>IJCV</em>, <em>132</em>(12), 5514–5533. (<a
href="https://doi.org/10.1007/s11263-024-02143-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While raw images possess distinct advantages over sRGB images, e.g., linearity and fine-grained quantization levels, they are not widely adopted by general users due to their substantial storage requirements. Very recent studies propose to compress raw images by designing sampling masks within the pixel space of the raw image. However, these approaches often leave space for pursuing more effective image representations and compact metadata. In this work, we propose a novel framework that learns a compact representation in the latent space, serving as metadata, in an end-to-end manner. Compared with lossy image compression, we analyze the intrinsic difference of the raw image reconstruction task caused by rich information from the sRGB image. Based on the analysis, a novel design of the backbone with asymmetric and hybrid spatial feature resolutions is proposed, which significantly improves the rate-distortion performance. Besides, we propose a novel design of the sRGB-guided context model, which can better predict the order masks of encoding/decoding based on both the sRGB image and the the masks of already processed features. Benefited from the better modeling of the correlation between order masks, the already processed information can be better utilized. Moreover, a novel sRGB-guided adaptive quantization precision strategy, which dynamically assigns varying levels of quantization precision to different regions, further enhances the representation ability of the model. Finally, based on the iterative properties of the proposed context model, we propose a novel strategy to achieve variable bit rates using a single model. This strategy allows for the continuous convergence of a wide range of bit rates. We demonstrate how our raw image compression scheme effectively allocates more bits to image regions that hold greater global importance. Extensive experimental results validate the superior performance of the proposed method, achieving high-quality raw image reconstruction with a smaller metadata size, compared with existing SOTA methods.},
  archive      = {J_IJCV},
  author       = {Wang, Yufei and Yu, Yi and Yang, Wenhan and Guo, Lanqing and Chau, Lap-Pui and Kot, Alex C. and Wen, Bihan},
  doi          = {10.1007/s11263-024-02143-2},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5514-5533},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Beyond learned metadata-based raw image reconstruction},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Of mice and mates: Automated classification and modelling of
mouse behaviour in groups using a single model across cages.
<em>IJCV</em>, <em>132</em>(12), 5491–5513. (<a
href="https://doi.org/10.1007/s11263-024-02118-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioural experiments often happen in specialised arenas, but this may confound the analysis. To address this issue, we provide tools to study mice in the home-cage environment, equipping biologists with the possibility to capture the temporal aspect of the individual’s behaviour and model the interaction and interdependence between cage-mates with minimal human intervention. Our main contribution is the novel Global Behaviour Model (GBM) which summarises the joint behaviour of groups of mice across cages, using a permutation matrix to match the mouse identities in each cage to the model. In support of the above, we also (a) developed the Activity Labelling Module (ALM) to automatically classify mouse behaviour from video, and (b) released two datasets, ABODe for training behaviour classifiers and IMADGE for modelling behaviour.},
  archive      = {J_IJCV},
  author       = {Camilleri, Michael P. J. and Bains, Rasneer S. and Williams, Christopher K. I.},
  doi          = {10.1007/s11263-024-02118-3},
  journal      = {International Journal of Computer Vision},
  month        = {12},
  number       = {12},
  pages        = {5491-5513},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Of mice and mates: Automated classification and modelling of mouse behaviour in groups using a single model across cages},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: HCLR-net: Hybrid contrastive learning
regularization with locally randomized perturbation for underwater image
enhancement. <em>IJCV</em>, <em>132</em>(11), 5490. (<a
href="https://doi.org/10.1007/s11263-024-02131-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhou, Jingchun and Sun, Jiaming and Li, Chongyi and Jiang, Qiuping and Zhou, Man and Lam, Kin-Man and Zhang, Weishi and Fu, Xianping},
  doi          = {10.1007/s11263-024-02131-6},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5490},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: HCLR-net: hybrid contrastive learning regularization with locally randomized perturbation for underwater image enhancement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Editor’s note: Special issue on computer vision approach
for animal tracking and modeling. <em>IJCV</em>, <em>132</em>(11), 5489.
(<a href="https://doi.org/10.1007/s11263-024-02121-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02121-8},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5489},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision approach for animal tracking and modeling},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). DisCO: Portrait distortion correction with
perspective-aware 3D GANs. <em>IJCV</em>, <em>132</em>(11), 5471–5488.
(<a href="https://doi.org/10.1007/s11263-024-02085-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Close-up facial images captured at short distances often suffer from perspective distortion, resulting in exaggerated facial features and unnatural/unattractive appearances. We propose a simple yet effective method for correcting perspective distortions in a single close-up face image. We first perform 3D GAN inversion using a perspective-distorted input facial image by jointly optimizing the intrinsic and extrinsic camera parameters and the face latent code. To address the ambiguity inherent in this joint optimization, we develop starting from a short distance, optimization scheduling, reparametrizations, and geometric regularization. Re-rendering the portrait at a proper focal length and camera distance effectively corrects perspective distortions and produces more natural-looking results. We also incorporate a workflow to handle full images rather than limiting our method to cropped faces. Our experiments show that our method compares favorably against previous approaches qualitatively and quantitatively. We showcase numerous examples validating the applicability of our method on in-the-wild portrait photos. Our code is available at https://github.com/lightChaserX/DisCO .},
  archive      = {J_IJCV},
  author       = {Wang, Zhixiang and Liu, Yu-Lun and Huang, Jia-Bin and Satoh, Shin’ichi and Ma, Sizhuo and Krishnan, Gurunandan and Wang, Jian},
  doi          = {10.1007/s11263-024-02085-9},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5471-5488},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DisCO: Portrait distortion correction with perspective-aware 3D GANs},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual out-of-distribution detection in open-set noisy
environments. <em>IJCV</em>, <em>132</em>(11), 5453–5470. (<a
href="https://doi.org/10.1007/s11263-024-02139-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of noisy examples in the training set inevitably hampers the performance of out-of-distribution (OOD) detection. In this paper, we investigate a previously overlooked problem called OOD detection under asymmetric open-set noise, which is frequently encountered and significantly reduces the identifiability of OOD examples. We analyze the generating process of asymmetric open-set noise and observe the influential role of the confounding variable, entangling many open-set noisy examples with partial in-distribution (ID) examples referred to as hard-ID examples due to spurious-related characteristics. To address the issue of the confounding variable, we propose a novel method called Adversarial Confounder REmoving (ACRE) that utilizes progressive optimization with adversarial learning to curate three collections of potential examples (easy-ID, hard-ID, and open-set noisy) while simultaneously developing invariant representations and reducing spurious-related representations. Specifically, by obtaining easy-ID examples with minimal confounding effect, we learn invariant representations from ID examples that aid in identifying hard-ID and open-set noisy examples based on their similarity to the easy-ID set. By triplet adversarial learning, we achieve the joint minimization and maximization of distribution discrepancies across the three collections, enabling the dual elimination of the confounding variable. We also leverage potential open-set noisy examples to optimize a K+1-class classifier, further removing the confounding variable and inducing a tailored K+1-Guided scoring function. Theoretical analysis establishes the feasibility of ACRE, and extensive experiments demonstrate its effectiveness and generalization. Code is available at https://github.com/Anonymous-re-ssl/ACRE0 .},
  archive      = {J_IJCV},
  author       = {He, Rundong and Han, Zhongyi and Nie, Xiushan and Yin, Yilong and Chang, Xiaojun},
  doi          = {10.1007/s11263-024-02139-y},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5453-5470},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visual out-of-distribution detection in open-set noisy environments},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CA-MoEiT: Generalizable face anti-spoofing via dual
cross-attention and semi-fixed mixture-of-expert. <em>IJCV</em>,
<em>132</em>(11), 5439–5452. (<a
href="https://doi.org/10.1007/s11263-024-02135-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the generalization of face anti-spo-ofing (FAS) is increasingly concerned, it is still in the initial stage to solve it based on Vision Transformer (ViT). In this paper, we present a cross-domain FAS framework, dubbed the Transformer with dual Cross-Attention and semi-fixed Mixture-of-Expert (CA-MoEiT), for stimulating the generalization of Face Anti-Spoofing (FAS) from three aspects: (1) Feature augmentation. We insert a MixStyle after PatchEmbed layer to synthesize diverse patch embeddings from novel domains and enhance the generalizability of the trained model. (2) Feature alignment. We design a dual cross-attention mechanism which extends the self-attention to align the common representation from multiple domains. (3) Feature complement. We design a semi-fixed MoE (SFMoE) to selectively replace MLP by introducing a fixed super expert. Benefiting from the gate mechanism in SFMoE, professional experts are adaptively activated with independent learning domain-specific information, which is used as a supplement to domain-invariant features learned by the super expert to further improve the generalization. It is important that the above three technologies can be compatible with any variant of ViT as plug-and-play modules. Extensive experiments show that the proposed CA-MoEiT is effective and outperforms the state-of-the-art methods on several public datasets.},
  archive      = {J_IJCV},
  author       = {Liu, Ajian},
  doi          = {10.1007/s11263-024-02135-2},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5439-5452},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CA-MoEiT: Generalizable face anti-spoofing via dual cross-attention and semi-fixed mixture-of-expert},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards generalized UAV object detection: A novel
perspective from frequency domain disentanglement. <em>IJCV</em>,
<em>132</em>(11), 5410–5438. (<a
href="https://doi.org/10.1007/s11263-024-02108-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deploying unmanned aerial vehicle (UAV) object detection networks to complex, real-world scenes, generalization ability is often reduced due to domain shift. While most existing domain-generalized object detection methods disentangle domain-invariant features spatially, our exploratory experiments revealed a key insight for UAV object detection (UAV-OD): frequency domain contributions exhibit more pronounced disparities in generalization compared to generic object detection involving larger objects, since UAV-OD detects smaller objects. Therefore, frequency domain disentanglement stands out as a more direct, effective approach for UAV-OD. This paper proposes a novel frequency domain disentanglement method to improve UAV-OD generalization. Specifically, our framework leverages two learnable filters extracting domain-invariant and domain-specific spectra. Additionally, we design two contrastive losses: an image-level loss and an instance-level loss guiding training. These losses enable the filters to focus on extracting domain-invariant and domain-specific spectra, achieving better disentangling. Extensive experiments across multiple datasets, including UAVDT and Visdrone2019-DET, utilizing Faster R-CNN and YOLOv5, show our approach consistently and significantly outperforms baseline and state-of-the-art domain generalization methods. Our code is available at https://github.com/wangkunyu241/UAV-Frequency .},
  archive      = {J_IJCV},
  author       = {Wang, Kunyu and Fu, Xueyang and Ge, Chengjie and Cao, Chengzhi and Zha, Zheng-Jun},
  doi          = {10.1007/s11263-024-02108-5},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5410-5438},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards generalized UAV object detection: A novel perspective from frequency domain disentanglement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OV-DAR: Open-vocabulary object detection and attributes
recognition. <em>IJCV</em>, <em>132</em>(11), 5387–5409. (<a
href="https://doi.org/10.1007/s11263-024-02144-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we endeavor to localize all potential objects in an image and infer their visual categories, attributes, and shapes, even in instances where certain objects have not been encompassed in the model’s supervised training. This is similar to the challenge posed by open-vocabulary object detection and recognition. The proposed OV-DAR framework, in contrast to previous object detection and recognition frameworks, offers superior advantages and performance in terms of generalization, universality, and granularity expression. Specifically, OV-DAR disentangles the open-vocabulary object detection and recognition problem into two components: class-agnostic object proposal and open-vocabulary classification. It employs co-training to maintain a balance between the performance of these two components. For the former, we construct class-agnostic object proposal networks based on the anchor/query with the SAM foundation model, which demonstrates robust generalization in object proposing and masking. For the latter, we merge available object-centered category classification and attribute prediction data, take co-learning for efficient fine-tuning of CLIP, and subsequently augment the open-vocabulary capability on object-centered category/attribute prediction tasks using freely accessible online image–text pairs. To ensure the efficiency and accuracy of open-vocabulary classification, we devise a structure akin to Faster R-CNN and fully exploit the knowledge of object-centered CLIP for end-to-end multi-object open-vocabulary category and attribute prediction by knowledge distillation. We conduct comprehensive experiments on VAW, MS-COCO, LSA, and OVAD datasets. The results not only illustrate the complementarity of semantic category and attribute recognition for visual scene understanding but also underscore the generalization capability of OV-DAR in localizing, categorizing, attributing, and masking tasks and open-world scene perception.},
  archive      = {J_IJCV},
  author       = {Chen, Keyan and Jiang, Xiaolong and Wang, Haochen and Yan, Cilin and Gao, Yan and Tang, Xu and Hu, Yao and Xie, Weidi},
  doi          = {10.1007/s11263-024-02144-1},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5387-5409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OV-DAR: Open-vocabulary object detection and attributes recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diff-font: Diffusion model for robust one-shot font
generation. <em>IJCV</em>, <em>132</em>(11), 5372–5386. (<a
href="https://doi.org/10.1007/s11263-024-02137-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Font generation presents a significant challenge due to the intricate details needed, especially for languages with complex ideograms and numerous characters, such as Chinese and Korean. Although various few-shot (or even one-shot) font generation methods have been introduced, most of them rely on GAN-based image-to-image translation frameworks that still face (i) unstable training issues, (ii) limited fidelity in replicating font styles, and (iii) imprecise generation of complex characters. To tackle these problems, we propose a unified one-shot font generation framework called Diff-Font, based on the diffusion model. In particular, we approach font generation as a conditional generation task, where the content of characters is managed through predefined embedding tokens and the desired font style is extracted from a one-shot reference image. For glyph-rich characters such as Chinese and Korean, we incorporate additional inputs for strokes or components as fine-grained conditions. Owing to the proposed diffusion training process, these three types of information can be effectively modeled, resulting in stable training. Simultaneously, the integrity of character structures can be learned and preserved. To the best of our knowledge, Diff-Font is the first work to utilize a diffusion model for font generation tasks. Comprehensive experiments demonstrate that Diff-Font outperforms prior font generation methods in both high-fidelity font style replication and the generation of intricate characters. Our method achieves state-of-the-art results in both qualitative and quantitative aspects.},
  archive      = {J_IJCV},
  author       = {He, Haibin and Chen, Xinyuan and Wang, Chaoyue and Liu, Juhua and Du, Bo and Tao, Dacheng and Yu, Qiao},
  doi          = {10.1007/s11263-024-02137-0},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5372-5386},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Diff-font: Diffusion model for robust one-shot font generation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence intervals for error rates in 1:1 matching tasks:
Critical statistical analysis and recommendations. <em>IJCV</em>,
<em>132</em>(11), 5346–5371. (<a
href="https://doi.org/10.1007/s11263-024-02078-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching algorithms predict relationships between items in a collection. For example, in 1:1 face verification, a matching algorithm predicts whether two face images depict the same person. Accurately assessing the uncertainty of the error rates of such algorithms can be challenging when test data are dependent and error rates are low, two aspects that have been often overlooked in the literature.In this work, we review methods for constructing confidence intervals for error rates in 1:1 matching tasks. We derive and examine the statistical properties of these methods, demonstrating how coverage and interval width vary with sample size, error rates, and degree of data dependence with experiments on synthetic and real-world datasets. Based on our findings, we provide recommendations for best practices for constructing confidence intervals for error rates in 1:1 matching tasks.l},
  archive      = {J_IJCV},
  author       = {Fogliato, Riccardo and Patil, Pratik and Perona, Pietro},
  doi          = {10.1007/s11263-024-02078-8},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5346-5371},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Confidence intervals for error rates in 1:1 matching tasks: critical statistical analysis and recommendations},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CG-FAS: Cross-label generative augmentation for face
anti-spoofing. <em>IJCV</em>, <em>132</em>(11), 5330–5345. (<a
href="https://doi.org/10.1007/s11263-024-02132-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Anti-Spoofing (FAS) is essential to secure face recognition systems from various physical attacks. A sufficient and diverse training set helps to build robust FAS models. To exploit the potential of FAS datasets, we propose to generate high-quality data including live and diverse presentation attacks (PAs) faces, for data augmentation during the model training stage. Our method is called Cross-label Generative augmentation for Face Anti-Spoofing (CG-FAS), which could convert a live face into a 3D high-fidelity mask, replay, print, or other extra physical PAs. Correspondingly, CG-FAS can also restore a specific physical presentation attack into a live face. This function is realized by innovatively building an Interchange Bridge matrix, which stores disentangled spoof clues between PAs and live faces. To verify the effects of these generated data, we utilize them as augmentation data and conduct experiments on several typical FAS benchmarks. Extensive experimental results demonstrate the superior performance gain with CG-FAS for off-the-shelf data-driven FAS models. We hope the CG-FAS can shine a light on the deep FAS community to alleviate the data-hungry issue. The code will be released soon at: https://github.com/liuxingwt/CG-FAS .},
  archive      = {J_IJCV},
  author       = {Liu, Xing and Su, Anyang and Wu, Minghui and Yu, Zitong and Wu, Kangle and An, Da and Hao, Jie and Xu, Mengzhen and Zhao, Chenxu and Lei, Zhen},
  doi          = {10.1007/s11263-024-02132-5},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5330-5345},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CG-FAS: Cross-label generative augmentation for face anti-spoofing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing weakly-supervised audio-visual video parsing via
segment-wise pseudo labeling. <em>IJCV</em>, <em>132</em>(11),
5308–5329. (<a
href="https://doi.org/10.1007/s11263-024-02142-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Audio-Visual Video Parsing task aims to identify and temporally localize the events that occur in either or both the audio and visual streams of audible videos. It often performs in a weakly-supervised manner, where only video event labels are provided, i.e., the modalities and the timestamps of the labels are unknown. Due to the lack of densely annotated labels, recent work attempts to leverage pseudo labels to enrich the supervision. A commonly used strategy is to generate pseudo labels by categorizing the known video event labels for each modality. However, the labels are still confined to the video level, and the temporal boundaries of events remain unlabeled. In this paper, we propose a new pseudo label generation strategy that can explicitly assign labels to each video segment by utilizing prior knowledge learned from the open world. Specifically, we exploit the large-scale pretrained models, namely CLIP and CLAP, to estimate the events in each video segment and generate segment-level visual and audio pseudo labels, respectively. We then propose a new loss function to exploit these pseudo labels by taking into account their category-richness and segment-richness. A label denoising strategy is also adopted to further improve the visual pseudo labels by flipping them whenever abnormally large forward losses occur. We perform extensive experiments on the LLP dataset and demonstrate the effectiveness of each proposed design and we achieve state-of-the-art video parsing performance on all types of event parsing, i.e., audio event, visual event, and audio-visual event. Furthermore, our experiments verify that the high-quality segment-level pseudo labels provided by our method can be flexibly combined with other audio-visual video parsing backbones and consistently improve their performances. We also examine the proposed pseudo label generation strategy on a relevant weakly-supervised audio-visual event localization task and the experimental results again verify the benefits and generalization of our method.},
  archive      = {J_IJCV},
  author       = {Zhou, Jinxing and Guo, Dan and Zhong, Yiran and Wang, Meng},
  doi          = {10.1007/s11263-024-02142-3},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5308-5329},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Advancing weakly-supervised audio-visual video parsing via segment-wise pseudo labeling},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-teacher universal distillation based on information
hiding for defense against facial manipulation. <em>IJCV</em>,
<em>132</em>(11), 5293–5307. (<a
href="https://doi.org/10.1007/s11263-024-02050-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of AI-based facial manipulation techniques has made manipulated facial images highly deceptive. These techniques can be misused maliciously, which poses a severe threat to information security. Many effective detection methods have been developed to distinguish whether an image has been manipulated. However, malicious facial manipulation images or videos have been widely spread and had a harmful impact before detection. Thus protecting images from manipulation through proactive defense techniques has become the focus of current research. Currently, existing proactive defense methods disrupt the manipulation process through an adversarial attack on the facial manipulation network, which distorts or blurs parts of the manipulated facial image. Nevertheless, these methods are only slightly disruptive in defending against some facial manipulation methods, and the outputs are not only a stigmatized portrait but also that people still can not distinguish the real and fake. To overcome this issue, we propose a Multi-Teacher Universal Distillation based on information hiding for defense against facial manipulation. First, we propose a facial manipulation adversarial attacks network based on information hiding called IHA-Net. IHA-Net can hide the warning image in the protected image without affecting its visual quality and make the facial information disappear after manipulation to present the warning message. In this way, it prevents privacy leakage and stigmatization. Then to address the problem that the protected image cannot defend against multiple facial manipulations simultaneously, we propose the Multi-Teacher Universal Distillation framework. We use multiple trained teacher networks to co-direct the learning of the student network, allowing the student network to defend against multiple manipulation networks simultaneously. Specifically, we designed Multi-scale Discriminators for knowledge distillation at the feature map level to enable the student network to learn more rich knowledge from the teacher network. Furthermore, to balance the influence of multiple teacher networks on the student network during the training process, we designed a Dynamic Balancing Loss module that dynamically adjusts during the training process. Finally, extensive experiments on advanced facial manipulation systems demonstrate that the proposed method outperforms the state-of-the-art approaches.},
  archive      = {J_IJCV},
  author       = {Li, Xin and Ni, Rongrong and Zhao, Yao and Ni, Yu and Li, Haoliang},
  doi          = {10.1007/s11263-024-02050-6},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5293-5307},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-teacher universal distillation based on information hiding for defense against facial manipulation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust semantic segmentation against patch-based
attack via attention refinement. <em>IJCV</em>, <em>132</em>(11),
5270–5292. (<a
href="https://doi.org/10.1007/s11263-024-02120-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism has been proven effective on various visual tasks in recent years. In the semantic segmentation task, the attention mechanism is applied in various methods, including the case of both convolution neural networks and vision transformer as backbones. However, we observe that the attention mechanism is vulnerable to patch-based adversarial attacks. Through the analysis of the effective receptive field, we attribute it to the fact that the wide receptive field brought by global attention may lead to the spread of the adversarial patch. To address this issue, in this paper, we propose a robust attention mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks. Compared to the vallina attention mechanism, RAM introduces two novel modules called max attention suppression and random attention dropout, both of which aim to refine the attention matrix and limit the influence of a single adversarial patch on the semantic segmentation results of other positions. Extensive experiments demonstrate the effectiveness of our RAM to improve the robustness of semantic segmentation models against various patch-based attack methods under different attack settings.},
  archive      = {J_IJCV},
  author       = {Yuan, Zheng and Zhang, Jie and Wang, Yude and Shan, Shiguang and Chen, Xilin},
  doi          = {10.1007/s11263-024-02120-9},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5270-5292},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards robust semantic segmentation against patch-based attack via attention refinement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained multimodal DeepFake classification via
heterogeneous graphs. <em>IJCV</em>, <em>132</em>(11), 5255–5269. (<a
href="https://doi.org/10.1007/s11263-024-02128-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the abuse of deepfakes is a well-known issue since deepfakes can lead to severe security and privacy problems. And this situation is getting worse, as attackers are no longer limited to unimodal deepfakes, but use multimodal deepfakes, i.e., both audio forgery and video forgery, to better achieve malicious purposes. The existing unimodal or ensemble deepfake detectors are demanded with fine-grained classification capabilities for the growing technique on multimodal deepfakes. To address this gap, we propose a graph attention network based on heterogeneous graph for fine-grained multimodal deepfake classification, i.e., not only distinguishing the authenticity of samples, but also identifying the forged types, e.g., video or audio or both. To this end, we propose a positional coding-based heterogeneous graph construction method that converts an audio-visual sample into a multimodal heterogeneous graph according to relevant hyperparameters. Moreover, a cross-modal graph interaction module is devised to utilize audio-visual synchronization patterns for capturing inter-modal complementary information. The de-homogenization graph pooling operation is elaborately designed to keep differences in graph node features for enhancing the representation of graph-level features. Through the heterogeneous graph attention network, we can efficiently model intra- and inter-modal relationships of multimodal data both at spatial and temporal scales. Extensive experimental results on two audio-visual datasets FakeAVCeleb and LAV-DF demonstrate that our proposed model obtains significant performance gains as compared to other state-of-the-art competitors. The code is available at https://github.com/yinql1995/Fine-grained-Multimodal-DeepFake-Classification/.},
  archive      = {J_IJCV},
  author       = {Yin, Qilin and Lu, Wei and Cao, Xiaochun and Luo, Xiangyang and Zhou, Yicong and Huang, Jiwu},
  doi          = {10.1007/s11263-024-02128-1},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5255-5269},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fine-grained multimodal DeepFake classification via heterogeneous graphs},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Quality-invariant domain generalization for face
anti-spoofing. <em>IJCV</em>, <em>132</em>(11), 5239–5254. (<a
href="https://doi.org/10.1007/s11263-024-02092-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Anti-Spoofing (FAS) plays a critical role in safeguarding face recognition systems, while previous FAS methods suffer from poor generalization when applied to unseen domains. Although recent methods have made progress via domain generalization technology, they are still sensitive to variations in face quality caused by task-irrelevant factors like camera and illumination. In this paper, we propose a novel Quality-Invariant Domain Generalization method (QIDG) with a teacher-student architecture, which aligns liveness features into a quality-invariant space to alleviate interference from task-irrelated factors. Specifically, QIDG utilizes the teacher model to produce face quality representations, which serve as the guidance for the student model to explore the quality-invariant space. To seek this space, the student model devises two novel modules, i.e., a dual adversarial learning module (DAL) and a quality feature assembly module (QFA). The former produces domain-invariant liveness features and task-irrelated quality features. While the latter assembles these two features from the same faces into complete quality representations, as well as assembles these two features from living faces in different domains. In this way, QIDG not only achieves the alignment of the domain-invariant liveness features to the quality-invariant space, but also promotes compactness of living faces from different domains in the feature space. Extensive cross-domain experiments demonstrate the superiority of our method on five public databases.},
  archive      = {J_IJCV},
  author       = {Liu, Yongluo and Li, Zun and Xu, Yaowen and Guo, Zhizhi and Zou, Zhaofan and Wu, Lifang},
  doi          = {10.1007/s11263-024-02092-w},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5239-5254},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Quality-invariant domain generalization for face anti-spoofing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking vision transformer and masked autoencoder in
multimodal face anti-spoofing. <em>IJCV</em>, <em>132</em>(11),
5217–5238. (<a
href="https://doi.org/10.1007/s11263-024-02055-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M $$^{2}$$ A $$^{2}$$ E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M $$^{2}$$ A $$^{2}$$ E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.},
  archive      = {J_IJCV},
  author       = {Yu, Zitong and Cai, Rizhao and Cui, Yawen and Liu, Xin and Hu, Yongjian and Kot, Alex C.},
  doi          = {10.1007/s11263-024-02055-1},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5217-5238},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking vision transformer and masked autoencoder in multimodal face anti-spoofing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intriguing property and counterfactual explanation of GAN
for remote sensing image generation. <em>IJCV</em>, <em>132</em>(11),
5192–5216. (<a
href="https://doi.org/10.1007/s11263-024-02125-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have achieved remarkable progress in the natural image field. However, when applying GANs in the remote sensing (RS) image generation task, an extraordinary phenomenon is observed: the GAN model is more sensitive to the amount of training data for RS image generation than for natural image generation (Fig. 1). In other words, the generation quality of RS images will change significantly with the number of training categories or samples per category. In this paper, we first analyze this phenomenon from two kinds of toy experiments and conclude that the amount of feature information contained in the GAN model decreases with reduced training data (Fig. 2). Then we establish a structural causal model (SCM) of the data generation process and interpret the generated data as the counterfactuals. Based on this SCM, we theoretically prove that the quality of generated images is positively correlated with the amount of feature information. This provides insights for enriching the feature information learned by the GAN model during training. Consequently, we propose two innovative adjustment schemes, namely uniformity regularization and entropy regularization, to increase the information learned by the GAN model at the distributional and sample levels, respectively. Extensive experiments on eight RS datasets and three natural datasets show the effectiveness and versatility of our methods. The source code is available at https://github.com/rootSue/Causal-RSGAN .},
  archive      = {J_IJCV},
  author       = {Su, Xingzhe and Qiang, Wenwen and Hu, Jie and Zheng, Changwen and Wu, Fengge and Sun, Fuchun},
  doi          = {10.1007/s11263-024-02125-4},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5192-5216},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Intriguing property and counterfactual explanation of GAN for remote sensing image generation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViDSOD-100: A new dataset and a baseline model for RGB-d
video salient object detection. <em>IJCV</em>, <em>132</em>(11),
5173–5191. (<a
href="https://doi.org/10.1007/s11263-024-02051-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of depth sensor, more and more RGB-D videos could be obtained. Identifying the foreground in RGB-D videos is a fundamental and important task. However, the existing salient object detection (SOD) works only focus on either static RGB-D images or RGB videos, ignoring the collaborating of RGB-D and video information. In this paper, we first collect a new annotated RGB-D video SOD (ViDSOD-100) dataset, which contains 100 videos within a total of 9362 frames, acquired from diverse natural scenes. All the frames in each video are manually annotated to a high-quality saliency annotation. Moreover, we propose a new baseline model, named attentive triple-fusion network (ATF-Net), for RGB-D video salient object detection. Our method aggregates the appearance information from an input RGB image, spatio-temporal information from an estimated motion map, and the geometry information from the depth map by devising three modality-specific branches and a multi-modality integration branch. The modality-specific branches extract the representation of different inputs, while the multi-modality integration branch combines the multi-level modality-specific features by introducing the encoder feature aggregation (MEA) modules and decoder feature aggregation (MDA) modules. The experimental findings conducted on both our newly introduced ViDSOD-100 dataset and the well-established DAVSOD dataset highlight the superior performance of the proposed ATF-Net.This performance enhancement is demonstrated both quantitatively and qualitatively, surpassing the capabilities of current state-of-the-art techniques across various domains, including RGB-D saliency detection, video saliency detection, and video object segmentation. We shall release our data, our results, and our code upon the publication of this work.},
  archive      = {J_IJCV},
  author       = {Lin, Junhao and Zhu, Lei and Shen, Jiaxing and Fu, Huazhu and Zhang, Qing and Wang, Liansheng},
  doi          = {10.1007/s11263-024-02051-5},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5173-5191},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ViDSOD-100: A new dataset and a baseline model for RGB-D video salient object detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-set single-domain generalization for robust face
anti-spoofing. <em>IJCV</em>, <em>132</em>(11), 5151–5172. (<a
href="https://doi.org/10.1007/s11263-024-02129-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing is a critical component of face recognition technology. However, it suffers from poor generalizability for cross-scenario target domains due to the simultaneous presence of unseen domains and unknown attack types. In this paper, we first propose a challenging but practical problem for face anti-spoofing, open-set single-domain generalization-based face anti-spoofing, aiming to learn face anti-spoofing models that generalize well to unseen target domains with known and unknown attack types based on a single source domain. To address this problem, we propose a novel unknown-aware causal generalized representation learning framework. Specifically, the proposed network consists of two modules: (1) causality-inspired intervention domain augmentation, which generates out-of-distribution images to eliminate spurious correlations between spoof-irrelevant variant factors and category labels for generalized causal feature learning; and (2) unknown-aware probability calibration, which performs known and unknown attack detection based on the original and generated images to further improve the generalizability for unknown attack types. The results of extensive qualitative and quantitative experiments demonstrate that the proposed method learns well-generalized features for both domain shift and unknown attack types based on a single source domain. Our method achieves state-of-the-art cross-scenario generalizability for both live faces and known attack types and unknown attack types.},
  archive      = {J_IJCV},
  author       = {Jiang, Fangling and Li, Qi and Wang, Weining and Ren, Min and Shen, Wei and Liu, Bing and Sun, Zhenan},
  doi          = {10.1007/s11263-024-02129-0},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5151-5172},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Open-set single-domain generalization for robust face anti-spoofing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exert diversity and mitigate bias: Domain generalizable
person re-identification with a comprehensive benchmark. <em>IJCV</em>,
<em>132</em>(11), 5124–5150. (<a
href="https://doi.org/10.1007/s11263-024-02124-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID), aiming at retrieving persons of the same identity across non-overlapping cameras, holds immense practical significance for security and surveillance applications. In pursuit of a more general and practical solution, recent research attention has gradually shifted from the traditional single-domain ReID to the domain generalizable person re-identification (DG-ReID). However, the DG-ReID landscape lacks a meticulously designed and all-encompassing benchmark to provide a common ground for competing approaches. To this end, in this paper, we first delve into the intricate challenges of DG-ReID and introduce a comprehensive and large-scale benchmark with enhanced distributional variety and shifts to facilitate the research progress. Furthermore, in response to the highlighted challenges, a novel DG-ReID framework based on diverse feature space learning with domain factorization is proposed to effectively learn rich domain-adaptive discriminative features through the two designed blocks with fairly limited additional cost in both memory and computation. Firstly, the feature diversification block promotes a diverse feature space capable of learning domain-specific characteristics under the rich distributional variety. Secondly, the domain-adaptive shielding block applies channel-wise shielding operations based on subspace-based domain factorization in order to prevent the model from prediction bias caused by distributional shifts. Our extensive experiments demonstrate the effectiveness of the proposed framework, surpassing the performance of current state-of-the-art methods under various evaluation protocols.},
  archive      = {J_IJCV},
  author       = {Hu, Bingyu and Liu, Jiawei and Zheng, Yufei and Zheng, Kecheng and Zha, Zheng-Jun},
  doi          = {10.1007/s11263-024-02124-5},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5124-5150},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exert diversity and mitigate bias: Domain generalizable person re-identification with a comprehensive benchmark},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning generalizable mixed-precision quantization via
attribution imitation. <em>IJCV</em>, <em>132</em>(11), 5101–5123. (<a
href="https://doi.org/10.1007/s11263-024-02130-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging large-scale datasets in realistic applications. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to large-scale datasets with only a small amount of data, so that the search cost is significantly reduced without performance degradation. Specifically, we observe that locating network attribution correctly is general ability for accurate visual analysis across different data distribution. Therefore, despite of pursuing higher accuracy and lower model complexity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts via capacity-aware attribution imitation for generalizable mixed-precision quantization strategy search, where the capacity of quantized networks is considered to fully utilize the network capacity without insufficiency. Since slight noise in attribution is amplified by discrete ranking operations with significant rank errors, mimicking the attribution ranks of the full-precision models obstructs the quantized networks to correctly locate the attribution. To address this, we further present a robust generalizable mixed-precision quantization method to smooth the attribution for rank error alleviation by hierarchical attribution partitioning, which efficiently partitions the attribution pixels in high spatial resolution and assigns the same attribution value for pixels within a group. Moreover, we propose dynamic capacity-aware attribution imitation to adjust the concentration degree of the attribution according to sample hardness, so that sufficient model capacity is achieved with full utilization for each image. Extensive experiments on image classification and object detection show that our GMPQ and R-GMPQ obtain competitive accuracy-complexity trade-offs with significantly reduced search cost compared to the state-of-the-art mixed-precision networks.},
  archive      = {J_IJCV},
  author       = {Wang, Ziwei and Xiao, Han and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-024-02130-7},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5101-5123},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning generalizable mixed-precision quantization via attribution imitation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Generate transferable adversarial physical camouflages via
triplet attention suppression. <em>IJCV</em>, <em>132</em>(11),
5084–5100. (<a
href="https://doi.org/10.1007/s11263-024-02098-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are vulnerable to adversarial examples. As one of the most threatening types for practical deep learning systems, physical adversarial examples have received extensive attention in recent years. However, due to the insufficient focus on intrinsic characteristics such as model-agnostic features, existing studies generate adversarial perturbations with unsatisfactory transferability on attacking different models. Motivated by the viewpoint that attention reflects the intrinsic characteristics of the recognition process, we propose the Transferable Attention Attack (TA $$_2$$ ) method to generate adversarial camouflages with strong transferable attacking ability by taking advantage of visual attention mechanism, i.e., triplet attention suppression. As for attacking, we generate transferable adversarial camouflages by distracting the model-shared similar attention patterns from the target to non-target regions, therefore promoting the transferable attacking ability. Furthermore, we enhance the attacking ability by converging the model attention of the non-ground-truth class, which exploits the lateral inhibition of visual models and activates the model perception for wrong classes. Besides, considering the visually suspicious appearance, we also introduce human attention to help improve their visual naturalness. We conduct extensive experiments in both the digital and physical worlds for classification tasks and comprehensively investigate the effectiveness of the discovered model attention mechanism, demonstrating that our method outperforms state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Wang, Jiakai and Liu, Xianglong and Yin, Zixin and Wang, Yuxuan and Guo, Jun and Qin, Haotong and Wu, Qingtao and Liu, Aishan},
  doi          = {10.1007/s11263-024-02098-4},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5084-5100},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generate transferable adversarial physical camouflages via triplet attention suppression},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards non co-occurrence incremental object detection with
unlabeled in-the-wild data. <em>IJCV</em>, <em>132</em>(11), 5066–5083.
(<a href="https://doi.org/10.1007/s11263-024-02048-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks have shown remarkable results in the task of object detection. However, their performance suffers critical drops when they are subsequently trained on novel classes without any sample from the base classes originally used to train the model. This phenomenon is known as catastrophic forgetting. Recently, several incremental learning methods are proposed to mitigate catastrophic forgetting for object detection. Despite the effectiveness, these methods require co-occurrence of the unlabeled base classes in the training data of the novel classes. This requirement is impractical in many real-world settings since the base classes do not necessarily co-occur with the novel classes. In view of this limitation, we consider a more practical setting of complete absence of co-occurrence of the base and novel classes for the object detection task. We propose the use of unlabeled in-the-wild data to bridge the non co-occurrence caused by the missing base classes during the training of additional novel classes. To this end, we introduce a blind sampling strategy based on the responses of the base-class model and pre-trained novel-class model to select a smaller relevant dataset from the large in-the-wild dataset for incremental learning. We then design a dual-teacher distillation framework to transfer the knowledge distilled from the base- and novel-class teacher models to the student model using the sampled in-the-wild data. Additionally, the novel class data is in the training to facilitate the learning of discriminative representations between base and novel classes. Furthermore, on the consideration that the training samples are all false positives when there is no class overlap in the in-the-wild data, we propose a single-teacher distillation framework to relieve the mutual suppression of the dual-teacher distillation framework and balance a trade-off between the performances of base and novel classes. Experimental results on the PASCAL VOC and MS-COCO datasets show that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods when there is no co-occurrence between the base and novel classes during training.},
  archive      = {J_IJCV},
  author       = {Dong, Na and Zhang, Yongqiang and Ding, Mingli and Lee, Gim Hee},
  doi          = {10.1007/s11263-024-02048-0},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5066-5083},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards non co-occurrence incremental object detection with unlabeled in-the-wild data},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OV-VIS: Open-vocabulary video instance segmentation.
<em>IJCV</em>, <em>132</em>(11), 5048–5065. (<a
href="https://doi.org/10.1007/s11263-024-02076-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventionally, the goal of Video Instance Segmentation (VIS) is to segment and categorize objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation (OV-VIS), which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark OV-VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset (LV-VIS), that contains well-annotated objects from 1196 diverse categories, significantly surpassing the category size of existing datasets by more than an order of magnitude. Third, we propose a transformer-based OV-VIS model, OV2Seg+, which associates per-frame segmentation masks with a memory-induced transformer and clarifies objects in videos with a voting module given language guidance. In addition, to monitor the progress, we set up the evaluation protocols for OV-VIS and propose a set of strong baseline models to facilitate future endeavors. Extensive experiments on LV-VIS and four existing VIS datasets demonstrate the strong zero-shot generalization ability of OV2Seg+. The dataset and code are released here https://github.com/haochenheheda/LVVIS . The competition website is provided here https://www.codabench.org/competitions/1748 .},
  archive      = {J_IJCV},
  author       = {Wang, Haochen and Yan, Cilin and Chen, Keyan and Jiang, Xiaolong and Tang, Xu and Hu, Yao and Kang, Guoliang and Xie, Weidi and Gavves, Efstratios},
  doi          = {10.1007/s11263-024-02076-w},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5048-5065},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OV-VIS: Open-vocabulary video instance segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Ultra-high resolution image segmentation via locality-aware
context fusion and alternating local enhancement. <em>IJCV</em>,
<em>132</em>(11), 5030–5047. (<a
href="https://doi.org/10.1007/s11263-024-02045-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware context fusion based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present the alternating local enhancement module that restricts the negative impact of redundant information introduced from the contexts, and thus is endowed with the ability of fixing the locality-aware features to produce refined results. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks and verify the effectiveness of the proposed modules. Our released codes will be available at: https://github.com/liqiokkk/FCtL .},
  archive      = {J_IJCV},
  author       = {Liu, Wenxi and Li, Qi and Lin, Xindai and Yang, Weixiang and He, Shengfeng and Yu, Yuanlong},
  doi          = {10.1007/s11263-024-02045-3},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5030-5047},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Ultra-high resolution image segmentation via locality-aware context fusion and alternating local enhancement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-based implicit feature transform for few-shot
classification. <em>IJCV</em>, <em>132</em>(11), 5014–5029. (<a
href="https://doi.org/10.1007/s11263-024-02113-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize instances from previously unseen classes based on a very limited number of examples. However, models often face the challenge of overfitting due to the biased distribution computed from extremely scarce training data. This work proposes a Semantic-based Implicit Feature Transform (SIFT) method to implicitly generate high-quality features for few-shot learning tasks. In this method, we employ an encode-transform-decode pipeline to facilitate the direct transfer of feature instances from base classes to novel classes via semantic transformation, ensuring the generation of semantically meaningful features. Additionally, a compactness constraint is imposed on the generated features, with novel class prototypes serving as anchors, to ensure that the features are distributed around the class centers. Furthermore, we integrate the cluster centers of the query set with the initial prototypes computed from the support set to produce less biased class prototypes, which can serve as better anchors for feature reconstruction. The experimental results reveal that our method outperforms the baselines by substantial margins and achieves state-of-the-art few-shot classification performance on the miniImageNet and CIFAR-FS datasets in both inductive and transductive settings, demonstrating the superiority of our method. Furthermore, the comprehensive ablation studies provide additional validation of its effectiveness. The code is available at: https://github.com/pmhDL/SIFT.git .},
  archive      = {J_IJCV},
  author       = {Pan, Mei-Hong and Xin, Hong-Yi and Shen, Hong-Bin},
  doi          = {10.1007/s11263-024-02113-8},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {5014-5029},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-based implicit feature transform for few-shot classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inheriting bayer’s legacy: Joint remosaicing and denoising
for quad bayer image sensor. <em>IJCV</em>, <em>132</em>(11), 4992–5013.
(<a href="https://doi.org/10.1007/s11263-024-02114-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel binning-based Quad sensors (mega-pixel resolution camera sensor) offer a promising solution to address the hardware limitations of compact cameras for low-light imaging. However, the binning process leads to reduced spatial resolution and introduces non-Bayer CFA artifacts. In this paper, we propose a Quad CFA-driven remosaicing model that effectively converts noisy Quad Bayer and standard Bayer patterns compatible to existing Image Signal Processor (ISP) without any loss in resolution. To enhance the practicality of the remosaicing model for real-world images affected by mixed noise, we introduce a novel dual-head joint remosaicing and denoising network (DJRD), which addresses the order of denoising and remosaicing by performing them in parallel. In DJRD, we customize two denoising branches for Quad Bayer and Bayer inputs. These branches model non-local and local dependencies, CFA location, and frequency information using residual convolutional layers, Swin Transformer, and wavelet transform-based CNN. Furthermore, to improve the model’s performance on challenging cases, we fine-tune DJRD to handle difficult scenarios by identifying problematic patches through Moire and zipper detection metrics. This post-training phase allows the model to focus on resolving complex image regions. Extensive experiments conducted on simulated and real images in both Bayer and sRGB domains demonstrate that DJRD outperforms competing models by approximately 3 dB, while maintaining the simplicity of implementation without adding any hardware.},
  archive      = {J_IJCV},
  author       = {Zeng, Haijin and Feng, Kai and Cao, Jiezhang and Huang, Shaoguang and Zhao, Yongqiang and Luong, Hiep and Aelterman, Jan and Philips, Wilfried},
  doi          = {10.1007/s11263-024-02114-7},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4992-5013},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Inheriting bayer’s legacy: Joint remosaicing and denoising for quad bayer image sensor},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB guided ToF imaging system: A survey of deep
learning-based methods. <em>IJCV</em>, <em>132</em>(11), 4954–4991. (<a
href="https://doi.org/10.1007/s11263-024-02089-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating an RGB camera into a ToF imaging system has become a significant technique for perceiving the real world. The RGB guided ToF imaging system is crucial to several applications, including face anti-spoofing, saliency detection, and trajectory prediction. Depending on the distance of the working range, the implementation schemes of the RGB guided ToF imaging systems are different. Specifically, ToF sensors with a uniform field of illumination, which can output dense depth but have low resolution, are typically used for close-range measurements. In contrast, LiDARs, which emit laser pulses and can only capture sparse depth, are usually employed for long-range detection. In the two cases, depth quality improvement for RGB guided ToF imaging corresponds to two sub-tasks: guided depth super-resolution and guided depth completion. In light of the recent significant boost to the field provided by deep learning, this paper comprehensively reviews the works related to RGB guided ToF imaging, including network structures, learning strategies, evaluation metrics, benchmark datasets, and objective functions. Besides, we present quantitative comparisons of state-of-the-art methods on widely used benchmark datasets. Finally, we discuss future trends and the challenges in real applications for further research.},
  archive      = {J_IJCV},
  author       = {Qiao, Xin and Poggi, Matteo and Deng, Pengchao and Wei, Hao and Ge, Chenyang and Mattoccia, Stefano},
  doi          = {10.1007/s11263-024-02089-5},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4954-4991},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RGB guided ToF imaging system: A survey of deep learning-based methods},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-constraint transferable generative adversarial
networks for cross-modal brain image synthesis. <em>IJCV</em>,
<em>132</em>(11), 4937–4953. (<a
href="https://doi.org/10.1007/s11263-024-02109-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in generative models has led to the drastic growth of research in image generation. Existing approaches show visually compelling results by learning multi-modal distributions, but they still lack realism, especially in certain scenarios like medical image synthesis. In this paper, we propose a novel Brain Generative Adversarial Network (BrainGAN) that explores GANs with multi-constraint and transferable property for cross-modal brain image synthesis. We formulate BrainGAN by introducing a unified framework with new constraints that can enhance modal matching, texture details and anatomical structure, simultaneously. We show how BrainGAN can learn meaningful tissue representations with rich variability of brain images. In addition to generating 3D volumes that are visually indistinguishable from real ones, we model adversarial discriminators and segmentors jointly, along with the proposed cost functions, which forces our networks to synthesize brain MRIs with realistic textures conditioned on anatomical structures. BrainGAN is evaluated on three public datasets, where it consistently outperforms the other state-of-the-art approaches by a large margin, advancing cross-modal synthesis of brain images both visually and practically.},
  archive      = {J_IJCV},
  author       = {Huang, Yawen and Zheng, Hao and Li, Yuexiang and Zheng, Feng and Zhen, Xiantong and Qi, GuoJun and Shao, Ling and Zheng, Yefeng},
  doi          = {10.1007/s11263-024-02109-4},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4937-4953},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-constraint transferable generative adversarial networks for cross-modal brain image synthesis},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An outlook into the future of egocentric vision.
<em>IJCV</em>, <em>132</em>(11), 4880–4936. (<a
href="https://doi.org/10.1007/s11263-024-02095-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What will the future be? We wonder! In this survey, we explore the gap between current research in egocentric vision and the ever-anticipated future, where wearable computing, with outward facing cameras and digital overlays, is expected to be integrated in our every day lives. To understand this gap, the article starts by envisaging the future through character-based stories, showcasing through examples the limitations of current technology. We then provide a mapping between this future and previously defined research tasks. For each task, we survey its seminal works, current state-of-the-art methodologies and available datasets, then reflect on shortcomings that limit its applicability to future research. Note that this survey focuses on software models for egocentric vision, independent of any specific hardware. The paper concludes with recommendations for areas of immediate explorations so as to unlock our path to the future always-on, personalised and life-enhancing egocentric vision.},
  archive      = {J_IJCV},
  author       = {Plizzari, Chiara and Goletto, Gabriele and Furnari, Antonino and Bansal, Siddhant and Ragusa, Francesco and Farinella, Giovanni Maria and Damen, Dima and Tommasi, Tatiana},
  doi          = {10.1007/s11263-024-02095-7},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4880-4936},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An outlook into the future of egocentric vision},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CSDG-FAS: Closed-space domain generalization for face
anti-spoofing. <em>IJCV</em>, <em>132</em>(11), 4866–4879. (<a
href="https://doi.org/10.1007/s11263-024-02052-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization based Face Anti-spoofing (FAS) aims to enhance its ability to work in unseen domains. Existing methods endeavor to extract a discriminative common space through the alignment of distribution in each domain. However, he inherent diversity within spoof faces significantly challenges the establishment of such a unified space. In this work, we reframe domain generalization-based FAS as an anomaly detection problem, positing that real faces tend to aggregate within a compact, closed space, whereas spoof faces exhibit a preference for dispersion within an open space. Specifically, we introduce a novel Closed Space Domain Generalization (CSDG) framework, consisting of a novel designed Dynamic Feature Queue and a Domain Alignment Module. The former is dedicated to maintaining a distinct class center for real faces, achieved by continuously widening its separation from the dynamically evolving spoof face queue; The latter aims to further align the distribution of real faces across diverse domains. Moreover, we propose a Progressive Training Strategy to effectively mine challenging samples across multiple domains during the training phase. Furthermore, we highlight the success of our proposed methods by achieving the first prize in the Surveillance Face Anti-Spoofing track at Challenge@CVPR 2023. Subsequently, we demonstrate the efficacy of the CSDG framework on two intra-domain datasets, as well as in two challenging cross-domain FAS experiments.},
  archive      = {J_IJCV},
  author       = {Wang, Keyao and Zhang, Guosheng and Yue, Haixiao and Liang, Yanyan and Huang, Mouxiao and Zhang, Gang and Han, Junyu and Ding, Errui and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02052-4},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4866-4879},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CSDG-FAS: Closed-space domain generalization for face anti-spoofing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exemplar-free lifelong person re-identification via
prompt-guided adaptive knowledge consolidation. <em>IJCV</em>,
<em>132</em>(11), 4850–4865. (<a
href="https://doi.org/10.1007/s11263-024-02110-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong person re-identification (LReID) refers to matching people across different cameras given continuous data streams. The challenge of catastrophic forgetting of old knowledge and the effective acquisition of new knowledge form a significant dilemma for LReID. Most current LReID methods propose to retain abundant exemplars from historical data, which are further rehearsed to fully fine-tune the whole model. However, such a learning paradigm will inevitably hinder data privacy and result in substantial computation costs. In this paper, we propose a paradigm for exemplar-free LReID through model re-parameterization. Without retaining any exemplars, our designed method adopts a novel Prompt-guided Adaptive Exponential Moving Average (PAEMA) strategy to achieve dynamic knowledge consolidation. Our key idea is to leverage visual prompting as the guidance for model re-parameterization to benefit knowledge preservation. Conventional Exponential Moving Average (EMA) methods rely on fixed or time-varied constants as weighting parameters, the unpredictable correlation between new and old data streams may lead to varying levels of model parameter drifting during LReID learning. Hence, we argue that a proper weighting parameter should be conditioned on the variation of new and old models to provide an adaptive knowledge consolidation for LReID. To do so, an adaptive mechanism is proposed to utilize the visual prompt as a surrogate for model variation estimation. Consequently, without using any exemplars, the forgetting issue in LReID is greatly alleviated. Experiments on various LReID benchmarks have verified the superiority of our method against the state-of-the-art lifelong learning and LReID approaches. Code is available at https://github.com/zhoujiahuan1991/IJCV2024-PAEMA/ .},
  archive      = {J_IJCV},
  author       = {Li, Qiwei and Xu, Kunlun and Peng, Yuxin and Zhou, Jiahuan},
  doi          = {10.1007/s11263-024-02110-x},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4850-4865},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exemplar-free lifelong person re-identification via prompt-guided adaptive knowledge consolidation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning hierarchical visual transformation for domain
generalizable visual matching and recognition. <em>IJCV</em>,
<em>132</em>(11), 4823–4849. (<a
href="https://doi.org/10.1007/s11263-024-02106-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks are prone to learn domain-dependent shortcuts and thus usually suffer from severe performance degradation when tested in unseen target domains due to their poor ability of out-of-distribution generalization, which significantly limits the real-world applications. The main reason is the domain shift lying in the large distribution gap between source and unseen target data. To this end, this paper takes a step towards training robust models for domain generalizable visual tasks, which mainly focuses on learning domain-invariant visual representation to alleviate the domain shift. Specifically, we first propose an effective Hierarchical Visual Transformation (HVT) network to (1) first transform the training sample hierarchically into new domains with diverse distributions from three levels: Global, Local, and Pixel, (2) then maximize the visual discrepancy between the source domain and new domains, and minimize the cross-domain feature inconsistency to capture domain-invariant features. Besides, we further enhance the HVT network by introducing the environment-invariant learning. To be specific, we enforce the invariance of the visual representation across automatically inferred environments by minimizing invariant learning loss that considers the weighted average of environmental losses. In this way, we can prevent the model from relying on the spurious features for prediction, thus helping the model to effectively learn domain-invariant representation and narrow the domain gap in various visual matching and recognition tasks, such as stereo matching, pedestrian retrieval, and image classification. We term our extended HVT as EHVT to show distinction. We integrate our EHVT network into different models and evaluate its effectiveness and compatibility on several public benchmark datasets. Extensive experiments clearly show that our EHVT can substantially enhance the generalization performance in various tasks. Our codes are available at https://github.com/cty8998/EHVT-VisualDG .},
  archive      = {J_IJCV},
  author       = {Yang, Xun and Chang, Tianyu and Zhang, Tianzhu and Wang, Shanshan and Hong, Richang and Wang, Meng},
  doi          = {10.1007/s11263-024-02106-7},
  journal      = {International Journal of Computer Vision},
  month        = {11},
  number       = {11},
  pages        = {4823-4849},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning hierarchical visual transformation for domain generalizable visual matching and recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Deep unpaired blind image super-resolution
using self-supervised learning and exemplar distillation. <em>IJCV</em>,
<em>132</em>(10), 4819–4821. (<a
href="https://doi.org/10.1007/s11263-023-01980-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Dong, Jiangxin and Bai, Haoran and Tang, Jinhui and Pan, Jinshan},
  doi          = {10.1007/s11263-023-01980-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4819-4821},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: Deep unpaired blind image super-resolution using self-supervised learning and exemplar distillation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Editor’s note: Special issue on robust vision.
<em>IJCV</em>, <em>132</em>(10), 4818. (<a
href="https://doi.org/10.1007/s11263-024-02122-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02122-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4818},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on robust vision},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLG: General video recognition with web textual knowledge.
<em>IJCV</em>, <em>132</em>(10), 4792–4817. (<a
href="https://doi.org/10.1007/s11263-024-02081-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video recognition (action recognition) in an open world is quite challenging, as we need to handle different settings such as closed-set, long-tail, few-shot, and open-set. The majority of existing works often address each individual setting separately using various frameworks. However, these separate investigations would ignore the possibility of knowledge sharing across different settings, and stymie progress in video recognition as well as its application in the real world. By leveraging semantic knowledge from noisy text descriptions crawled from the Internet, we focus on the general video recognition (GVR) task of solving recognition problems of different settings within a unified framework. The core contribution of this paper is twofold. First, we build a comprehensive video recognition benchmark to facilitate the research of GVR, called Kinetics-Text. This dataset covers the mentioned four common settings, and provides multi-source text descriptions for all action classes for utilizing external textual knowledge from the Internet. Second, inspired by the flexibility of language representation, we analyse the correspondence between the video and text descriptions of its category and present a unified visual-linguistic framework (VLG) to solve the problem of GVR with an effective two-stage training paradigm. Our VLG is first pre-trained on video and language datasets to learn a shared feature space, and then devises a flexible bi-modal attention head to collaborate high-level semantic concepts under different settings. Extensive results show that our VLG obtains the state-of-the-art performance under four settings, and the superior performance demonstrates the effectiveness and generalization ability of our proposed framework. We hope our work makes a step towards the general video recognition and could serve as a baseline for future research. Code and datasets have been released in https://github.com/MCG-NJU/VLG .},
  archive      = {J_IJCV},
  author       = {Lin, Jintao and Liu, Zhaoyang and Wang, Wenhai and Wu, Wayne and Wang, Limin},
  doi          = {10.1007/s11263-024-02081-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4792-4817},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {VLG: General video recognition with web textual knowledge},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial reweighting with <span
class="math display"><em>α</em></span> -power maximization for domain
adaptation. <em>IJCV</em>, <em>132</em>(10), 4768–4791. (<a
href="https://doi.org/10.1007/s11263-024-02107-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practical Domain Adaptation (DA) tasks, e.g., Partial DA (PDA), open-set DA, universal DA, and test-time adaptation, have gained increasing attention in the machine learning community. In this paper, we propose a novel approach, dubbed Adversarial Reweighting with $$\alpha $$ -Power Maximization (ARPM), for PDA where the source domain contains private classes absent in target domain. In ARPM, we propose a novel adversarial reweighting model that adversarially learns to reweight source domain data to identify source-private class samples by assigning smaller weights to them, for mitigating potential negative transfer. Based on the adversarial reweighting, we train the transferable recognition model on the reweighted source distribution to be able to classify common class data. To reduce the prediction uncertainty of the recognition model on the target domain for PDA, we present an $$\alpha $$ -power maximization mechanism in ARPM, which enriches the family of losses for reducing the prediction uncertainty for PDA. Extensive experimental results on five PDA benchmarks, e.g., Office-31, Office-Home, VisDA-2017, ImageNet-Caltech, and DomainNet, show that our method is superior to recent PDA methods. Ablation studies also confirm the effectiveness of components in our approach. To theoretically analyze our method, we deduce an upper bound of target domain expected error for PDA, which is approximately minimized in our approach. We further extend ARPM to open-set DA, universal DA, and test time adaptation, and verify the usefulness through experiments.},
  archive      = {J_IJCV},
  author       = {Gu, Xiang and Yu, Xi and Yang, Yan and Sun, Jian and Xu, Zongben},
  doi          = {10.1007/s11263-024-02107-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4768-4791},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adversarial reweighting with $$\alpha $$ -power maximization for domain adaptation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WATCHER: Wavelet-guided texture-content hierarchical
relation learning for deepfake detection. <em>IJCV</em>,
<em>132</em>(10), 4746–4767. (<a
href="https://doi.org/10.1007/s11263-024-02116-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breathtaking advances in face forgery techniques produce visually untraceable deepfake videos, thus potential malicious abuse of these techniques has sparked great concerns. Existing deepfake detectors primarily focus on specific forgery patterns with global features extracted by CNN backbones for forgery detection. Due to inadequate exploration of content and texture features, they often suffer from overfitting method-specific forged regions, thus exhibiting limited generalization to increasingly realistic forgeries. In this paper, we propose a Wavelet-guided Texture-Content HiErarchical Relation (WATCHER) Learning framework to delve deeper into the relation-aware texture-content features. Specifically, we propose a Wavelet-guided AutoEncoder scheme to retrieve the general visual representation, which is aware of high-frequency details for understanding forgeries. To further excavate fine-grained counterfeit clues, a Texture-Content Attention Maps Learning module is presented to enrich the contextual information of content and texture features via multi-level attention maps in a hierarchical learning protocol. Finally, we propose a Progressive Multi-domain Feature Interaction module in pursuit to perform semantic reasoning on relationship-enhanced texture-content forgery features. Extensive experiments on popular benchmark datasets substantiate the superiority of our WATCHER model, consistently trumping state-of-the-art methods by a significant margin.},
  archive      = {J_IJCV},
  author       = {Wang, Yuan and Chen, Chen and Zhang, Ning and Hu, Xiyuan},
  doi          = {10.1007/s11263-024-02116-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4746-4767},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WATCHER: Wavelet-guided texture-content hierarchical relation learning for deepfake detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RMS-FlowNet++: Efficient and robust multi-scale scene flow
estimation for large-scale point clouds. <em>IJCV</em>,
<em>132</em>(10), 4724–4745. (<a
href="https://doi.org/10.1007/s11263-024-02093-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed RMS-FlowNet++ is a novel end-to-end learning-based architecture for accurate and efficient scene flow estimation that can operate on high-density point clouds. For hierarchical scene flow estimation, existing methods rely on expensive Farthest-Point-Sampling (FPS) to sample the scenes, must find large correspondence sets across the consecutive frames and/or must search for correspondences at a full input resolution. While this can improve the accuracy, it reduces the overall efficiency of these methods and limits their ability to handle large numbers of points due to memory requirements. In contrast to these methods, our architecture is based on an efficient design for hierarchical prediction of multi-scale scene flow. To this end, we develop a special flow embedding block that has two advantages over the current methods: First, a smaller correspondence set is used, and second, the use of Random-Sampling (RS) is possible. In addition, our architecture does not need to search for correspondences at a full input resolution. Exhibiting high accuracy, our RMS-FlowNet++ provides a faster prediction than state-of-the-art methods, avoids high memory requirements and enables efficient scene flow on dense point clouds of more than 250K points at once. Our comprehensive experiments verify the accuracy of RMS-FlowNet++ on the established FlyingThings3D data set with different point cloud densities and validate our design choices. Furthermore, we demonstrate that our model has a competitive ability to generalize to the real-world scenes of the KITTI data set without fine-tuning.},
  archive      = {J_IJCV},
  author       = {Battrawy, Ramy and Schuster, René and Stricker, Didier},
  doi          = {10.1007/s11263-024-02093-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4724-4745},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RMS-FlowNet++: Efficient and robust multi-scale scene flow estimation for large-scale point clouds},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporally consistent enhancement of low-light videos via
spatial-temporal compatible learning. <em>IJCV</em>, <em>132</em>(10),
4703–4723. (<a
href="https://doi.org/10.1007/s11263-024-02084-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal inconsistency is the annoying artifact that has been commonly introduced in low-light video enhancement, but current methods tend to overlook the significance of utilizing both data-centric clues and model-centric design to tackle this problem. In this context, our work makes a comprehensive exploration from the following three aspects. First, to enrich the scene diversity and motion flexibility, we construct a synthetic diverse low/normal-light paired video dataset with a carefully designed low-light simulation strategy, which can effectively complement existing real captured datasets. Second, for better temporal dependency utilization, we develop a Temporally Consistent Enhancer Network (TCE-Net) that consists of stacked 3D convolutions and 2D convolutions to exploit spatial-temporal clues in videos. Last, the temporal dynamic feature dependencies are exploited to obtain consistency constraints for different frame indexes. All these efforts are powered by a Spatial-Temporal Compatible Learning (STCL) optimization technique, which dynamically constructs specific training loss functions adaptively on different datasets. As such, multiple-frame information can be effectively utilized and different levels of information from the network can be feasibly integrated, thus expanding the synergies on different kinds of data and offering visually better results in terms of illumination distribution, color consistency, texture details, and temporal coherence. Extensive experimental results on various real-world low-light video datasets clearly demonstrate the proposed method achieves superior performance to state-of-the-art methods. Our code and synthesized low-light video database will be publicly available at https://github.com/lingyzhu0101/low-light-video-enhancement.git .},
  archive      = {J_IJCV},
  author       = {Zhu, Lingyu and Yang, Wenhan and Chen, Baoliang and Zhu, Hanwei and Meng, Xiandong and Wang, Shiqi},
  doi          = {10.1007/s11263-024-02084-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4703-4723},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Temporally consistent enhancement of low-light videos via spatial-temporal compatible learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A geometric model for polarization imaging on projective
cameras. <em>IJCV</em>, <em>132</em>(10), 4688–4702. (<a
href="https://doi.org/10.1007/s11263-024-02119-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of Shape-from-Polarization (SfP) methods work under the oversimplified assumption of using orthographic cameras. Indeed, it is still unclear how Stokes vector projection behaves when the incoming rays are not orthogonal to the image plane. In this paper, we try to answer this question with a new geometric model describing how a general projective camera captures the light polarization state. Based on the optical properties of a tilted polarizer, our model is implemented as a pre-processing operation acting on raw images, and a scene-independent rotation of the reconstructed normal field. Moreover, our model is consistent with state-of-the-art forward and inverse renderers (as Mitsuba3 and ART), intrinsically enforces physical constraints among the captured channels, and handles the demosaicing of DoFP sensors. Experiments on existing and new datasets demonstrate the accuracy of the model when applied to commercially available polarimetric cameras.},
  archive      = {J_IJCV},
  author       = {Pistellato, Mara and Bergamasco, Filippo},
  doi          = {10.1007/s11263-024-02119-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4688-4702},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A geometric model for polarization imaging on projective cameras},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-ID-leak: Membership inference attacks against person
re-identification. <em>IJCV</em>, <em>132</em>(10), 4673–4687. (<a
href="https://doi.org/10.1007/s11263-024-02115-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) has rapidly advanced due to its widespread real-world applications. It poses a significant risk of exposing private data from its training dataset. This paper aims to quantify this risk by conducting a membership inference (MI) attack. Most existing MI attack methods focus on classification models, while Re-ID follows a distinct paradigm for training and inference. Re-ID is a fine-grained recognition task that involves complex feature embedding, and the model outputs commonly used by existing MI algorithms, such as logits and losses, are inaccessible during inference. Since Re-ID models the relative relationship between image pairs rather than individual semantics, we conduct a formal and empirical analysis that demonstrates that the distribution shift of the inter-sample similarity between the training and test sets is a crucial factor for membership inference and exists in most Re-ID datasets and models. Thus, we propose a novel MI attack method based on the distribution of inter-sample similarity, which involves sampling a set of anchor images to represent the similarity distribution that is conditioned on a target image. Next, we consider two attack scenarios based on information that the attacker has. In the “one-to-one” scenario, where the attacker has access to the target Re-ID model and dataset, we propose an anchor selector module to select anchors accurately representing the similarity distribution. Conversely, in the “one-to-any” scenario, which resembles real-world applications where the attacker has no access to the target Re-ID model and dataset, leading to the domain-shift problem, we propose two alignment strategies. Moreover, we introduce the patch-attention module as a replacement for the anchor selector. Experimental evaluations demonstrate the effectiveness of our proposed approaches in Re-ID tasks in both attack scenarios.},
  archive      = {J_IJCV},
  author       = {Gao, Junyao and Jiang, Xinyang and Dou, Shuguang and Li, Dongsheng and Miao, Duoqian and Zhao, Cairong},
  doi          = {10.1007/s11263-024-02115-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4673-4687},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Re-ID-leak: Membership inference attacks against person re-identification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M-RRFS: A memory-based robust region feature synthesizer for
zero-shot object detection. <em>IJCV</em>, <em>132</em>(10), 4651–4672.
(<a href="https://doi.org/10.1007/s11263-024-02112-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the goal to detect both the object categories appearing in the training phase and those never have been observed before testing, zero-shot object detection (ZSD) becomes a challenging yet anticipated task in the community. Current approaches tackle this problem by drawing on the feature synthesis techniques used in the zero-shot image classification (ZSC) task without delving into the inherent problems of ZSD. In this paper, we analyze the out-standing challenges that ZSD presents compared with ZSC—severe intra-class variation, complex category co-occurrence, open test scenario, and reveal their interference to the region feature synthesis process. In view of this, we propose a novel memory-based robust region feature synthesizer (M-RRFS) for ZSD, which is equipped with the Intra-class Semantic Diverging (IntraSD), the Inter-class Structure Preserving (InterSP), and the Cross-Domain Contrast Enhancing (CrossCE) mechanisms to overcome the inadequate intra-class diversity, insufficient inter-class separability, and weak inter-domain contrast problems. Moreover, when designing the whole learning framework, we develop an asynchronous memory container (AMC) to explore the cross-domain relationship between the seen class domain and unseen class domain to reduce the overlap between the distributions of them. Based on AMC, a memory-assisted ZSD inference process is also proposed to further boost the prediction accuracy. To evaluate the proposed approach, comprehensive experiments on MS-COCO, PASCAL VOC, ILSVRC and DIOR datasets are conducted, and superior performances have been achieved. Notably, we achieve new state-of-the-art performances on MS-COCO dataset, i.e., 64.0 $$\%$$ , 60.9 $$\%$$ and 55.5 $$\%$$ Recall@100 with IoU $$= 0.4, 0.5, 0.6 $$ respectively, and 15.1 $$\%$$ mAp with IoU $$=0.5$$ , under the 48/17 category split setting. Meanwhile, experiments on the DIOR dataset actually build the earliest benchmark for evaluating zero-shot object detection performance on remote sensing images. https://github.com/HPL123/M-RRFS .},
  archive      = {J_IJCV},
  author       = {Huang, Peiliang and Zhang, Dingwen and Cheng, De and Han, Longfei and Zhu, Pengfei and Han, Junwei},
  doi          = {10.1007/s11263-024-02112-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4651-4672},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {M-RRFS: A memory-based robust region feature synthesizer for zero-shot object detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking out-of-distribution detection from a
human-centric perspective. <em>IJCV</em>, <em>132</em>(10), 4633–4650.
(<a href="https://doi.org/10.1007/s11263-024-02099-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-Of-Distribution (OOD) detection has received broad attention over the years, aiming to ensure the reliability and safety of deep neural networks (DNNs) in real-world scenarios by rejecting incorrect predictions. However, we notice a discrepancy between the conventional evaluation vs. the essential purpose of OOD detection. On the one hand, the conventional evaluation exclusively considers risks caused by label-space distribution shifts while ignoring the risks from input-space distribution shifts. On the other hand, the conventional evaluation reward detection methods for not rejecting the misclassified image in the validation dataset. However, the misclassified image can also cause risks and should be rejected. We appeal to rethink OOD detection from a human-centric perspective, that a proper detection method should reject the case that the deep model’s prediction mismatches the human expectations and adopt the case that the deep model’s prediction meets the human expectations. We propose a human-centric evaluation and conduct extensive experiments on 45 classifiers and 8 test datasets. We find that the simple baseline OOD detection method can achieve comparable and even better performance than the recently proposed methods, which means that the development in OOD detection in the past years may be overestimated. Additionally, our experiments demonstrate that model selection is non-trivial for OOD detection and should be considered as an integral of the proposed method, which differs from the claim in existing works that proposed methods are universal across different models.},
  archive      = {J_IJCV},
  author       = {Zhu, Yao and Chen, Yuefeng and Li, Xiaodan and Zhang, Rong and Xue, Hui and Tian, Xiang and Jiang, Rongxin and Zheng, Bolun and Chen, Yaowu},
  doi          = {10.1007/s11263-024-02099-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4633-4650},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking out-of-distribution detection from a human-centric perspective},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ManiCLIP: Multi-attribute face manipulation from text.
<em>IJCV</em>, <em>132</em>(10), 4616–4632. (<a
href="https://doi.org/10.1007/s11263-024-02088-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a novel multi-attribute face manipulation method based on textual descriptions. Previous text-based image editing methods either require test-time optimization for each individual image or are restricted to single attribute editing. Extending these methods to multi-attribute face image editing scenarios will introduce undesired excessive attribute change, e.g., text-relevant attributes are overly manipulated and text-irrelevant attributes are also changed. In order to address these challenges and achieve natural editing over multiple face attributes, we propose a new decoupling training scheme where we use group sampling to get text segments from same attribute categories, instead of whole complex sentences. Further, to preserve other existing face attributes, we encourage the model to edit the latent code of each attribute separately via an entropy constraint. During the inference phase, our model is able to edit new face images without any test-time optimization, even from complex textual prompts. We show extensive experiments and analysis to demonstrate the efficacy of our method, which generates natural manipulated faces with minimal text-irrelevant attribute editing. Code and pre-trained model are available at https://github.com/hwang1996/ManiCLIP .},
  archive      = {J_IJCV},
  author       = {Wang, Hao and Lin, Guosheng and del Molino, Ana García and Wang, Anran and Feng, Jiashi and Shen, Zhiqi},
  doi          = {10.1007/s11263-024-02088-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4616-4632},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ManiCLIP: Multi-attribute face manipulation from text},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SUBTLE: An unsupervised platform with temporal link
embedding that maps animal behavior. <em>IJCV</em>, <em>132</em>(10),
4589–4615. (<a
href="https://doi.org/10.1007/s11263-024-02072-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While huge strides have recently been made in language-based machine learning, the ability of artificial systems to comprehend the sequences that comprise animal behavior has been lagging behind. In contrast, humans instinctively recognize behaviors by finding similarities in behavioral sequences. Here, we develop an unsupervised behavior-mapping framework, SUBTLE (spectrogram-UMAP-based temporal-link embedding), to capture comparable behavioral repertoires from 3D action skeletons. To find the best embedding method, we devise a temporal proximity index (TPI) as a new metric to gauge temporal representation in the behavioral embedding space. The method achieves the best TPI score compared to current embedding strategies. Its spectrogram-based UMAP clustering not only identifies subtle inter-group differences but also matches human-annotated labels. SUBTLE framework automates the tasks of both identifying behavioral repertoires like walking, grooming, standing, and rearing, and profiling individual behavior signatures like subtle inter-group differences by age. SUBTLE highlights the importance of temporal representation in the behavioral embedding space for human-like behavioral categorization.},
  archive      = {J_IJCV},
  author       = {Kwon, Jea and Kim, Sunpil and Kim, Dong-Kyum and Joo, Jinhyeong and Kim, SoHyung and Cha, Meeyoung and Lee, C. Justin},
  doi          = {10.1007/s11263-024-02072-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4589-4615},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SUBTLE: An unsupervised platform with temporal link embedding that maps animal behavior},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient high-quality vectorized modeling of large-scale
scenes. <em>IJCV</em>, <em>132</em>(10), 4564–4588. (<a
href="https://doi.org/10.1007/s11263-024-02059-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel high-quality vectorized modeling pipeline for large-scale scenes. With a reconstructed dense point cloud and its corresponding multi-view source images and camera parameters as input, our system can efficiently reconstruct a geometrically complete and detail-preserved vectorized model. Unlike most existing planar shape assembling methods which cannot handle large-scale vectorized modeling well due to the limitation of memory and computation, we can achieve complete high-quality vectorized modeling for complicated large-scale scenes in a time and memory efficient way. Our pipeline first carries out a 3D semantic segmentation on the dense point cloud, by performing 2D semantic labeling on the source images with a semantic segmentation network and fusing the 2D semantic labels into the point cloud. According to the fused dense 3D semantic labels, we then divide the scene into main structure including the grounds, walls and ceilings, and isolated objects that do not belong to the main structure. After the scene division completes, vectorized modeling is performed successively on the main structure and isolated objects to extract their polygonal models respectively instead of vectorizing the whole scene, to improve both time and memory efficiencies. Additionally, the previously vectorized main polygonal structures are used as priors to refine the segmentation and guide the vectorization of the objects to ensure the geometrical completeness and topological consistency of the entire vectorized model. Especially, during the vectorization procedure, a well designed binary space partition tree is designed to better slice the space so that high-quality polygonal mesh with more geometric details can be reconstructed with both time and memory efficiencies. Experiments with quantitative and qualitative evaluations on large-scale scenes demonstrate the accuracy and efficiency of the proposed vectorization pipeline. We also compare our method with state-of-the-art planar shape reconstruction methods to show its effectiveness in reconstructing large-scale vectorized models.},
  archive      = {J_IJCV},
  author       = {Xiang, Xiaojun and Jiang, Hanqing and Yu, Yihao and Shen, Donghui and Zhen, Jianan and Bao, Hujun and Zhou, Xiaowei and Zhang, Guofeng},
  doi          = {10.1007/s11263-024-02059-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4564-4588},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Efficient high-quality vectorized modeling of large-scale scenes},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GridFormer: Residual dense transformer with grid structure
for image restoration in adverse weather conditions. <em>IJCV</em>,
<em>132</em>(10), 4541–4563. (<a
href="https://doi.org/10.1007/s11263-024-02056-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration in adverse weather conditions is a difficult task in computer vision. In this paper, we propose a novel transformer-based framework called GridFormer which serves as a backbone for image restoration under adverse weather conditions. GridFormer is designed in a grid structure using a residual dense transformer block, and it introduces two core designs. First, it uses an enhanced attention mechanism in the transformer layer. The mechanism includes stages of the sampler and compact self-attention to improve efficiency, and a local enhancement stage to strengthen local information. Second, we introduce a residual dense transformer block (RDTB) as the final GridFormer layer. This design further improves the network’s ability to learn effective features from both preceding and current local features. The GridFormer framework achieves state-of-the-art results on five diverse image restoration tasks in adverse weather conditions, including image deraining, dehazing, deraining &amp; dehazing, desnowing, and multi-weather restoration. The source code and pre-trained models will be released.},
  archive      = {J_IJCV},
  author       = {Wang, Tao and Zhang, Kaihao and Shao, Ziqian and Luo, Wenhan and Stenger, Bjorn and Lu, Tong and Kim, Tae-Kyun and Liu, Wei and Li, Hongdong},
  doi          = {10.1007/s11263-024-02056-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4541-4563},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {GridFormer: Residual dense transformer with grid structure for image restoration in adverse weather conditions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid CNN-transformer architecture for efficient
large-scale video snapshot compressive imaging. <em>IJCV</em>,
<em>132</em>(10), 4521–4540. (<a
href="https://doi.org/10.1007/s11263-024-02101-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video snapshot compressive imaging (SCI) uses a low-speed 2D detector to capture high-speed scene, where the dynamic scene is modulated by different masks and then compressed into a snapshot measurement. Following this, a reconstruction algorithm is needed to reconstruct the high-speed video frames. Although state-of-the-art (SOTA) deep learning-based reconstruction algorithms have achieved impressive results, they still face the following challenges due to excessive model complexity and GPU memory limitations: (1) These models need high computational cost, and (2) They are usually unable to reconstruct large-scale video frames at high compression ratios. To address these issues, we develop an efficient network for video SCI by using hierarchical residual-like connections and hybrid CNN-Transformer structure within a single residual block, dubbed EfficientSCI++. The EfficientSCI++ network can well explore spatial-temporal correlation using convolution in the spatial domain and Transformer in the temporal domain, respectively. We are the first time to demonstrate that a UHD color video ( $$1644\times {3840}\times {3}$$ ) with high compression ratio (40) can be reconstructed from a snapshot 2D measurement using a single end-to-end deep learning model with PSNR above 34 dB. Moreover, a mixed-precision model is trained to further accelerate the video SCI reconstruction process and save memory footprint. Extensive results on both simulation and real data demonstrate that, compared with precious SOTA methods, our proposed EfficientSCI++ and EfficientSCI can achieve comparable reconstruction quality with much cheaper computational cost and better real-time performance. Code is available at https://github.com/mcao92/EfficientSCI-plus-plus .},
  archive      = {J_IJCV},
  author       = {Cao, Miao and Wang, Lishun and Zhu, Mingyu and Yuan, Xin},
  doi          = {10.1007/s11263-024-02101-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4521-4540},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hybrid CNN-transformer architecture for efficient large-scale video snapshot compressive imaging},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional adversarial training for better robust
generalization. <em>IJCV</em>, <em>132</em>(10), 4510–4520. (<a
href="https://doi.org/10.1007/s11263-024-02103-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) has been demonstrated as one of the most promising defense methods against various adversarial attacks. To our knowledge, existing AT-based methods usually train with the locally most adversarial perturbed points and treat all the perturbed points equally, which may lead to considerably weaker adversarial robust generalization on test data. In this work, we introduce a new adversarial training framework that considers the diversity as well as characteristics of the perturbed points in the vicinity of benign samples. To realize the framework, we propose a Regional Adversarial Training (RAT) defense method that first utilizes the attack path generated by the typical iterative attack method of projected gradient descent (PGD), and constructs an adversarial region based on the attack path. Then, RAT samples diverse perturbed training points efficiently inside this region, and utilizes a distance-aware label smoothing mechanism to capture our intuition that perturbed points at different locations should have different impact on the model performance. Extensive experiments on several benchmark datasets show that RAT consistently makes significant improvement on standard adversarial training (SAT), and exhibits better robust generalization.},
  archive      = {J_IJCV},
  author       = {Song, Chuanbiao and Fan, Yanbo and Zhou, Aoyang and Wu, Baoyuan and Li, Yiming and Li, Zhifeng and He, Kun},
  doi          = {10.1007/s11263-024-02103-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4510-4520},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Regional adversarial training for better robust generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthetic data for video surveillance applications of
computer vision: A review. <em>IJCV</em>, <em>132</em>(10), 4473–4509.
(<a href="https://doi.org/10.1007/s11263-024-02102-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in synthetic data for several computer vision applications, such as automotive, detection and tracking, surveillance, medical image analysis and robotics. Early use of synthetic data was aimed at performing controlled experiments under the analysis by synthesis approach. Currently, synthetic data are mainly used for training computer vision models, especially deep learning ones, to address well-known issues of real data, such as manual annotation effort, data imbalance and bias, and privacy-related restrictions. In this work, we survey the use of synthetic training data focusing on applications related to video surveillance, whose relevance has rapidly increased in the past few years due to their connection to security: crowd counting, object and pedestrian detection and tracking, behaviour analysis, person re-identification and face recognition. Synthetic training data are even more interesting in this kind of application, to address further, specific issues arising, e.g., from typically unconstrained image or video acquisition conditions and cross-scene application scenarios. We categorise and discuss the existing methods for creating synthetic data, analyse the synthetic data sets proposed in the literature for each of the considered applications, and provide an overview of their effectiveness as training data. We finally discuss whether and to what extent the existing synthetic data sets mitigate the issues of real data, highlight existing open issues, and suggest future research directions in this field.},
  archive      = {J_IJCV},
  author       = {Delussu, Rita and Putzu, Lorenzo and Fumera, Giorgio},
  doi          = {10.1007/s11263-024-02102-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4473-4509},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Synthetic data for video surveillance applications of computer vision: A review},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instant3D: Instant text-to-3D generation. <em>IJCV</em>,
<em>132</em>(10), 4456–4472. (<a
href="https://doi.org/10.1007/s11263-024-02097-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-3D generation has attracted much attention from the computer vision community. Existing methods mainly optimize a neural field from scratch for each text prompt, relying on heavy and repetitive training cost which impedes their practical deployment. In this paper, we propose a novel framework for fast text-to-3D generation, dubbed Instant3D. Once trained, Instant3D is able to create a 3D object for an unseen text prompt in less than one second with a single run of a feedforward network. We achieve this remarkable speed by devising a new network that directly constructs a 3D triplane from a text prompt. The core innovation of our Instant3D lies in our exploration of strategies to effectively inject text conditions into the network. In particular, we propose to combine three key mechanisms: cross-attention, style injection, and token-to-plane transformation, which collectively ensure precise alignment of the output with the input text. Furthermore, we propose a simple yet effective activation function, the scaled-sigmoid, to replace the original sigmoid function, which speeds up the training convergence by more than ten times. Finally, to address the Janus (multi-head) problem in 3D generation, we propose an adaptive Perp-Neg algorithm that can dynamically adjust its concept negation scales according to the severity of the Janus problem during training, effectively reducing the multi-head effect. Extensive experiments on a wide variety of benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods both qualitatively and quantitatively, while achieving significantly better efficiency. The code, data, and models are available at https://ming1993li.github.io/Instant3DProj/ .},
  archive      = {J_IJCV},
  author       = {Li, Ming and Zhou, Pan and Liu, Jia-Wei and Keppo, Jussi and Lin, Min and Yan, Shuicheng and Xu, Xiangyu},
  doi          = {10.1007/s11263-024-02097-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4456-4472},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Instant3D: Instant text-to-3D generation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive correlation filtering method for text-based
person search. <em>IJCV</em>, <em>132</em>(10), 4440–4455. (<a
href="https://doi.org/10.1007/s11263-024-02094-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to align person images with natural language descriptions, which can be widely used in video surveillance field, such as missing person searching and suspect tracking. In this task, extracting distinct representations and aligning them among identities based on descriptions is a crucial yet challenging problem. Most previous methods rely on additional language parsers or vision techniques to identify and select the relevant regions and words from inputs. However, these methods suffer from heavy computation costs and error accumulation. Meanwhile, simply using horizontal segmentation images to obtain local-level features would harm the reliability of models. To address these problems, we first present a novel Simple and Robust Correlation Filtering (SRCF) method which is capable of effectively extracting key clues and aligning discriminative features. Different from previous works, we design two different types of filtering modules (including denoising filters and dictionary filters) to extract essential features and establish multi-modal mappings. Furthermore, despite the SRCF being pretty well, it is still struggling with semantic ambiguity and uni-modal updating. Therefore, we further propose Multi-modal Adaptive Correlation Filtering (MACF) method that adaptively learns the vital regions and keywords with a shared update strategy. Meanwhile, we introduce a new mutually conditional gate to dynamically control the updating process of filters. Extensive experiments demonstrate that both proposed methods improve the robustness and reliability of the model and achieve better performance on the two text-based person search datasets.},
  archive      = {J_IJCV},
  author       = {Sun, Mengyang and Suo, Wei and Wang, Peng and Niu, Kai and Liu, Le and Lin, Guosheng and Zhang, Yanning and Wu, Qi},
  doi          = {10.1007/s11263-024-02094-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4440-4455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An adaptive correlation filtering method for text-based person search},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SA <span class="math display"><sup>3</sup></span> WT:
Adaptive wavelet-based transformer with self-paced auto augmentation for
face forgery detection. <em>IJCV</em>, <em>132</em>(10), 4417–4439. (<a
href="https://doi.org/10.1007/s11263-024-02091-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery detection (FFD) on digital images has become increasingly challenging with the proliferation of sophisticated manipulation techniques. In this study, we propose a novel approach, named Adaptive Wavelet-based Transformer with Self-paced Auto Augmentation (SA $$^3$$ WT), which naturally combines the global representation capabilities of visual transformers with adaptive enhancement of fine-grained artifacts in the frequency domain to effectively capture forgery patterns. In particular, to adequately handle various clues, the network incorporates Wavelet-based Mixed Attention (WMA) Transformer block to better leverage the information residing in all frequency sub-bands and a Residual Reserve Fine-grained Sampler (RRFS) to enhance detailed forgery artifacts while learning hierarchical global representations. By deeply mixing the modeling processes of global representations and fine-grained features throughout the network, the model captures rich forgery clues while simultaneously bypassing the fusion issue arising from their separate extraction. Furthermore, Self-paced Auto Augmentation Strategy (SAAS) facilitates model learning by unifying data augmentation and active learning in a coupled manner. Extensive experiments conducted on several benchmarks demonstrate the superiority of SA $$^3$$ WT compared to state-of-the-art methods. The ablation studies and cross-dataset evaluations confirm the significance of the specifically designed modules, in terms of both effectiveness and generalization. Our findings suggest that the pure visual transformers also provide a promising direction for advanced forgery detection in real-world scenarios.},
  archive      = {J_IJCV},
  author       = {Li, Yihui and Zhang, Yifan and Yang, Hongyu and Chen, Binghui and Huang, Di},
  doi          = {10.1007/s11263-024-02091-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4417-4439},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SA $$^3$$ WT: Adaptive wavelet-based transformer with self-paced auto augmentation for face forgery detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking object detection robustness against real-world
corruptions. <em>IJCV</em>, <em>132</em>(10), 4398–4416. (<a
href="https://doi.org/10.1007/s11263-024-02096-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid recent development, deep learning based object detection techniques have been applied to various real-world software systems, especially in safety-critical applications like autonomous driving. However, few studies are conducted to systematically investigate the robustness of state-of-the-art object detection techniques against real-world image corruptions and yet few benchmarks of object detection methods in terms of robustness are publicly available. To bridge this gap, we initiate to create a public benchmark of COCO-C and BDD100K-C, composed of sixteen real-world corruptions according to the real damages in camera sensors and image pipeline. Based on that, we further perform a systematic empirical study and evaluation of twelve representative object detectors covering three different categories of architectures (i.e., two-stage, one-stage, transformer architectures) to identify the current challenges and explore future opportunities. Our key findings include (1) the proposed real-world corruptions pose a threat to object detectors, especially for the corruptions involving colour changes, (2) a detector with a high mAP may still be vulnerable to real-world corruptions, (3) if there are potential cross-scenarios applications, the one-stage detectors are recommended, (4) when object detection architectures suffer from real-world corruptions, the effectiveness of existing robustness enhancement methods is limited, and (5) two-stage and one-stage object detection architectures are more likely to miss detect objects compared with transformer-based methods against the proposed corruptions. Our results highlight the need for designing robust object detection methods against real-world corruption and the need for more effective robustness enhancement methods for existing object detectors.},
  archive      = {J_IJCV},
  author       = {Liu, Jiawei and Wang, Zhijie and Ma, Lei and Fang, Chunrong and Bai, Tongtong and Zhang, Xufan and Liu, Jia and Chen, Zhenyu},
  doi          = {10.1007/s11263-024-02096-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4398-4416},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking object detection robustness against real-world corruptions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-vocabulary text-driven human image generation.
<em>IJCV</em>, <em>132</em>(10), 4379–4397. (<a
href="https://doi.org/10.1007/s11263-024-02079-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating human images from open-vocabulary text descriptions is an exciting but challenging task. Previous methods (i.e., Text2Human) face two challenging problems: (1) they cannot well handle the open-vocabulary setting by arbitrary text inputs (i.e., unseen clothing appearances) and heavily rely on limited preset words (i.e., pattern styles of clothing appearances); (2) the generated human image is inaccuracy in open-vocabulary settings. To alleviate these drawbacks, we propose a flexible diffusion-based framework, namely HumanDiffusion, for open-vocabulary text-driven human image generation (HIG). The proposed framework mainly consists of two novel modules: the Stylized Memory Retrieval (SMR) module and the Multi-scale Feature Mapping (MFM) module. Encoded by the vision-language pretrained CLIP model, we obtain coarse features of the local human appearance. Then, the SMR module utilizes an external database that contains clothing texture details to refine the initial coarse features. Through SMR refreshing, we can achieve the HIG task with arbitrary text inputs, and the range of expression styles is greatly expanded. Later, the MFM module embedding in the diffusion backbone can learn fine-grained appearance features, which effectively achieves precise semantic-coherence alignment of different body parts with appearance features and realizes the accurate expression of desired human appearance. The seamless combination of the proposed novel modules in HumanDiffusion realizes the freestyle and high accuracy of text-guided HIG and editing tasks. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SOTA) performance, especially in the open-vocabulary setting.},
  archive      = {J_IJCV},
  author       = {Zhang, Kaiduo and Sun, Muyi and Sun, Jianxin and Zhang, Kunbo and Sun, Zhenan and Tan, Tieniu},
  doi          = {10.1007/s11263-024-02079-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4379-4397},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Open-vocabulary text-driven human image generation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalance-aware discriminative clustering for unsupervised
semantic segmentation. <em>IJCV</em>, <em>132</em>(10), 4362–4378. (<a
href="https://doi.org/10.1007/s11263-024-02083-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised semantic segmentation (USS) aims at partitioning an image into semantically meaningful segments by learning from a collection of unlabeled images. The effectiveness of current approaches is plagued by difficulties in coordinating representation learning and pixel clustering, modeling the varying feature distributions of different classes, handling outliers and noise, and addressing the pixel class imbalance problem. This paper introduces a novel approach, termed Imbalance-Aware Dense Discriminative Clustering (IDDC), for USS, which addresses all these difficulties in a unified framework. Different from existing approaches, which learn USS in two stages (i.e., generating and updating pseudo masks, or refining and clustering embeddings), IDDC learns pixel-wise feature representation and dense discriminative clustering in an end-to-end and self-supervised manner, through a novel objective function that transfers the manifold structure of pixels in the embedding space of a vision Transformer (ViT) to the label space while tolerating the noise in pixel affinities. During inference, the trained model directly outputs the classification probability of each pixel conditioned on the image. In addition, this paper proposes a new regularizer, based on the Weibull function, to handle pixel class imbalance and cluster degeneration in a single shot. Experimental results demonstrate that IDDC significantly outperforms all previous USS methods on three real-world datasets, COCO-Stuff-27, COCO-Stuff-171, and Cityscapes. Extensive ablation studies validate the effectiveness of each design. Our code is available at https://github.com/MY-LIU100101/IDDC .},
  archive      = {J_IJCV},
  author       = {Liu, Mingyuan and Zhang, Jicong and Tang, Wei},
  doi          = {10.1007/s11263-024-02083-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4362-4378},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Imbalance-aware discriminative clustering for unsupervised semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PRCL: Probabilistic representation contrastive learning for
semi-supervised semantic segmentation. <em>IJCV</em>, <em>132</em>(10),
4343–4361. (<a
href="https://doi.org/10.1007/s11263-024-02016-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.},
  archive      = {J_IJCV},
  author       = {Xie, Haoyu and Wang, Changqi and Zhao, Jian and Liu, Yang and Dan, Jun and Fu, Chong and Sun, Baigui},
  doi          = {10.1007/s11263-024-02016-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4343-4361},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PRCL: Probabilistic representation contrastive learning for semi-supervised semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive discriminative regularization for visual
classification. <em>IJCV</em>, <em>132</em>(10), 4327–4342. (<a
href="https://doi.org/10.1007/s11263-024-02080-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to improve discriminative feature learning is central in classification. Existing works address this problem by explicitly increasing inter-class separability and intra-class compactness by constructing positive and negative pairs for contrastive learning or posing tighter class separating margins. These methods do not exploit the similarity between different classes as they adhere to independent identical distributions assumption in data. In this paper, we embrace the real-world data distribution setting in that some classes share semantic overlaps due to their similar appearances or concepts. Regarding this hypothesis, we propose a novel regularization to improve discriminative learning. We first calibrate the estimated highest likelihood of one sample based on its semantically neighboring classes, then encourage the overall likelihood predictions to be deterministic by imposing an adaptive exponential penalty. As the gradient of the proposed method is roughly proportional to the uncertainty of the predicted likelihoods, we name it adaptive discriminative regularization (ADR), trained along with a standard cross entropy loss in classification. Extensive experiments demonstrate that it can yield consistent and non-trivial performance improvements in a variety of visual classification tasks (over 10 benchmarks). Furthermore, we find it is robust to long-tailed and noisy label data distribution. Its flexible design enables its compatibility with mainstream classification architectures and losses.},
  archive      = {J_IJCV},
  author       = {Zhao, Qingsong and Wang, Yi and Dou, Shuguang and Gong, Chen and Wang, Yin and Zhao, Cairong},
  doi          = {10.1007/s11263-024-02080-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4327-4342},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive discriminative regularization for visual classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the usage of pre-trained features for stereo
matching. <em>IJCV</em>, <em>132</em>(10), 4305–4326. (<a
href="https://doi.org/10.1007/s11263-024-02090-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many vision tasks, utilizing pre-trained features results in improved performance and consistently benefits from the rapid advancement of pre-training technologies. However, in the field of stereo matching, the use of pre-trained features has not been extensively researched. In this paper, we present the first systematical exploration into the utilization of pre-trained features for stereo matching. To provide flexible employment for any combination of pre-trained backbones and stereo matching networks, we develop the deformable neck (DN) that decouples the network architectures of these two components. The core idea of DN is to utilize the deformable attention mechanism to iteratively fuse pre-trained features from shallow to deep layers. Empirically, our exploration reveals the crucial factors that influence using pre-trained features for stereo matching. We further investigate the role of instance-level information of pre-trained features, demonstrating it benefits stereo matching while can be suppressed during convolution-based feature fusion. Built on the attention mechanism, the proposed DN module effectively utilizes the instance-level information in pre-trained features. Besides, we provide an understanding of the efficiency-accuracy tradeoff, concluding that using pre-trained features can also be a good alternative with efficiency consideration.},
  archive      = {J_IJCV},
  author       = {Zhang, Jiawei and Huang, Lei and Bai, Xiao and Zheng, Jin and Gu, Lin and Hancock, Edwin},
  doi          = {10.1007/s11263-024-02090-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4305-4326},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring the usage of pre-trained features for stereo matching},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical study on multi-domain robust semantic
segmentation. <em>IJCV</em>, <em>132</em>(10), 4289–4304. (<a
href="https://doi.org/10.1007/s11263-024-02100-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively leverage the plentiful existing datasets to train a robust and high-performance model is of great significance for many practical applications. However, a model trained on a naive merge of different datasets tends to obtain poor performance due to annotation conflicts and domain divergence. In this paper, we attempt to train a unified model that is expected to perform well across domains on several popularity segmentation datasets. We conduct a comprehensive analysis to assess the impact of various training schemes and model selection on multi-domain learning with extensive experiments. Based on the analysis, we propose a robust solution that consistently enhances the model performance across different domains. Our solution ranks 2nd on RVC 2022 semantic segmentation task, with a dataset only 1/3 size of the 1st model used.},
  archive      = {J_IJCV},
  author       = {Liu, Yajie and Ge, Pu and Liu, Qingjie and Fan, Shichao and Wang, Yunhong},
  doi          = {10.1007/s11263-024-02100-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4289-4304},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An empirical study on multi-domain robust semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and analysis of efficient attention in transformers
for social group activity recognition. <em>IJCV</em>, <em>132</em>(10),
4269–4288. (<a
href="https://doi.org/10.1007/s11263-024-02082-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social group activity recognition is a challenging task extended from group activity recognition, where social groups must be recognized with their activities and group members. Existing methods tackle this task by leveraging region features of individuals following existing group activity recognition methods. However, the effectiveness of region features is susceptible to person localization and variable semantics of individual actions. To overcome these issues, we propose leveraging attention modules in transformers to generate social group features. In this method, multiple embeddings are used to aggregate features for a social group, each of which is assigned to a group member without duplication. Due to this non-duplicated assignment, the number of embeddings must be significant to avoid missing group members and thus renders attention in transformers ineffective. To find optimal attention designs with a large number of embeddings, we explore several design choices of queries for feature aggregation and self-attention modules in transformer decoders. Extensive experimental results show that the proposed method achieves state-of-the-art performance and verify that the proposed attention designs are highly effective on social group activity recognition.},
  archive      = {J_IJCV},
  author       = {Tamura, Masato},
  doi          = {10.1007/s11263-024-02082-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4269-4288},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Design and analysis of efficient attention in transformers for social group activity recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-driven spectrum-consistent federated learning for
palmprint verification. <em>IJCV</em>, <em>132</em>(10), 4253–4268. (<a
href="https://doi.org/10.1007/s11263-024-02077-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint as biometrics has gained increasing attention recently due to its discriminative ability and robustness. However, existing methods mainly improve palmprint verification within one spectrum, which is challenging to verify across different spectrums. Additionally, in distributed server-client-based deployment, palmprint verification systems predominantly necessitate clients to transmit private data for model training on the centralized server, thereby engendering privacy apprehensions. To alleviate the above issues, in this paper, we propose a physics-driven spectrum-consistent federated learning method for palmprint verification, dubbed as PSFed-Palm. PSFed-Palm draws upon the inherent physical properties of distinct wavelength spectrums, wherein images acquired under similar wavelengths display heightened resemblances. Our approach first partitions clients into short- and long-spectrum groups according to the wavelength range of their local spectrum images. Subsequently, we introduce anchor models for short- and long-spectrum, which constrain the optimization directions of local models associated with long- and short-spectrum images. Specifically, a spectrum-consistent loss that enforces the model parameters and feature representation to align with their corresponding anchor models is designed. Finally, we impose constraints on the local models to ensure their consistency with the global model, effectively preventing model drift. This measure guarantees spectrum consistency while protecting data privacy, as there is no need to share local data. Extensive experiments are conducted to validate the efficacy of our proposed PSFed-Palm approach. The proposed PSFed-Palm demonstrates compelling performance despite only a limited number of training data. The codes have been released at https://github.com/Zi-YuanYang/PSFed-Palm .},
  archive      = {J_IJCV},
  author       = {Yang, Ziyuan and Teoh, Andrew Beng Jin and Zhang, Bob and Leng, Lu and Zhang, Yi},
  doi          = {10.1007/s11263-024-02077-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4253-4268},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Physics-driven spectrum-consistent federated learning for palmprint verification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-MuPPET: 3D multi-pigeon pose estimation and tracking.
<em>IJCV</em>, <em>132</em>(10), 4235–4252. (<a
href="https://doi.org/10.1007/s11263-024-02074-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markerless methods for animal posture tracking have been rapidly developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple camera views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For identity matching of individuals in all views, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain IDs across views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator in terms of median error and Percentage of Correct Keypoints. Additionally, we benchmark the inference speed of 3D-MuPPET, with up to 9.45 fps in 2D and 1.89 fps in 3D, and perform quantitative tracking evaluation, which yields encouraging results. Finally, we showcase two novel applications for 3D-MuPPET. First, we train a model with data of single pigeons and achieve comparable results in 2D and 3D posture estimation for up to 5 pigeons. Second, we show that 3D-MuPPET also works in outdoors without additional annotations from natural environments. Both use cases simplify the domain shift to new species and environments, largely reducing annotation effort needed for 3D posture tracking. To the best of our knowledge we are the first to present a framework for 2D/3D animal posture and trajectory tracking that works in both indoor and outdoor environments for up to 10 individuals. We hope that the framework can open up new opportunities in studying animal collective behaviour and encourages further developments in 3D multi-animal posture tracking.},
  archive      = {J_IJCV},
  author       = {Waldmann, Urs and Chan, Alex Hoi Hang and Naik, Hemal and Nagy, Máté and Couzin, Iain D. and Deussen, Oliver and Goldluecke, Bastian and Kano, Fumihiro},
  doi          = {10.1007/s11263-024-02074-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4235-4252},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {3D-MuPPET: 3D multi-pigeon pose estimation and tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards diverse binary segmentation via a simple yet general
gated network. <em>IJCV</em>, <em>132</em>(10), 4157–4234. (<a
href="https://doi.org/10.1007/s11263-024-02058-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many binary segmentation tasks, most CNNs-based methods use a U-shape encoder-decoder network as their basic structure. They ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control mechanism between them, the other is without considering the disparity of the contributions from different encoder levels. In this work, we propose a simple yet general gated network (GateNet) to tackle them all at once. With the help of multi-level gate units, the valuable context information from the encoder can be selectively transmitted to the decoder. In addition, we design a gated dual branch structure to build the cooperation among the features of different levels and improve the discrimination ability of the network. Furthermore, we introduce a “Fold” operation to improve the atrous convolution and form a novel folded atrous convolution, which can be flexibly embedded in ASPP or DenseASPP to accurately localize foreground objects of various scales. GateNet can be easily generalized to many binary segmentation tasks, including general and specific object segmentation and multi-modal segmentation. Without bells and whistles, our network consistently performs favorably against the state-of-the-art methods under 10 metrics on 33 datasets of 10 binary segmentation tasks.},
  archive      = {J_IJCV},
  author       = {Zhao, Xiaoqi and Pang, Youwei and Zhang, Lihe and Lu, Huchuan and Zhang, Lei},
  doi          = {10.1007/s11263-024-02058-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4157-4234},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards diverse binary segmentation via a simple yet general gated network},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). HCLR-net: Hybrid contrastive learning regularization with
locally randomized perturbation for underwater image enhancement.
<em>IJCV</em>, <em>132</em>(10), 4132–4156. (<a
href="https://doi.org/10.1007/s11263-024-01987-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement presents a significant challenge due to the complex and diverse underwater environments that result in severe degradation phenomena such as light absorption, scattering, and color distortion. More importantly, obtaining paired training data for these scenarios is a challenging task, which further hinders the generalization performance of enhancement models. To address these issues, we propose a novel approach, the Hybrid Contrastive Learning Regularization (HCLR-Net). Our method is built upon a distinctive hybrid contrastive learning regularization strategy that incorporates a unique methodology for constructing negative samples. This approach enables the network to develop a more robust sample distribution. Notably, we utilize non-paired data for both positive and negative samples, with negative samples are innovatively reconstructed using local patch perturbations. This strategy overcomes the constraints of relying solely on paired data, boosting the model’s potential for generalization. The HCLR-Net also incorporates an Adaptive Hybrid Attention module and a Detail Repair Branch for effective feature extraction and texture detail restoration, respectively. Comprehensive experiments demonstrate the superiority of our method, which shows substantial improvements over several state-of-the-art methods in terms of quantitative metrics, significantly enhances the visual quality of underwater images, establishing its innovative and practical applicability. Our code is available at: https://github.com/zhoujingchun03/HCLR-Net .},
  archive      = {J_IJCV},
  author       = {Zhou, Jingchun and Sun, Jiaming and Li, Chongyi and Jiang, Qiuping and Zhou, Man and Lam, Kin-Man and Zhang, Weishi and Fu, Xianping},
  doi          = {10.1007/s11263-024-01987-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4132-4156},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HCLR-net: Hybrid contrastive learning regularization with locally randomized perturbation for underwater image enhancement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In memoriam: Xiaoou tang. <em>IJCV</em>, <em>132</em>(10),
4131. (<a href="https://doi.org/10.1007/s11263-024-02037-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Matsushita , Yasuyuki and Lazebnik, Svetlana and Matas, Jiri},
  doi          = {10.1007/s11263-024-02037-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {4131},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In memoriam: Xiaoou tang},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on traditional computer
vision in the age of deep learning. <em>IJCV</em>, <em>132</em>(9),
4128–4130. (<a
href="https://doi.org/10.1007/s11263-024-02062-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Poggi, Matteo and Arrigoni, Federica and Fusiello, Andrea and Mattoccia, Stefano and Bartoli, Adrien and Sattler, Torsten and Pajdla, Tomas},
  doi          = {10.1007/s11263-024-02062-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4128-4130},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on traditional computer vision in the age of deep learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on the british machine vision
conference 2022. <em>IJCV</em>, <em>132</em>(9), 4123–4127. (<a
href="https://doi.org/10.1007/s11263-024-02038-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Guang and Aviles-Rivero, Angelica and Fang, Yingying and Feng, Zhenhua and Ciocca, Gianluigi and Hicks, Yulia and Reyes-Aldasoro, Constantino Carlos},
  doi          = {10.1007/s11263-024-02038-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4123-4127},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on the british machine vision conference 2022},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meet JEANIE: A similarity measure for 3D skeleton sequences
via temporal-viewpoint alignment. <em>IJCV</em>, <em>132</em>(9),
4091–4122. (<a
href="https://doi.org/10.1007/s11263-024-02070-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects’ poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects’ poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be matched to the query temporal block with the same or adjacent (next) temporal index, and adjacent camera views to achieve joint local temporal-viewpoint warping. JEANIE selects the smallest distance among matching paths with different temporal-viewpoint warping patterns, an advantage over DTW which only performs temporal alignment. We also propose an unsupervised FSAR akin to clustering of sequences with JEANIE as a distance measure. JEANIE achieves state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II on supervised and unsupervised FSAR, and their meta-learning inspired fusion.},
  archive      = {J_IJCV},
  author       = {Wang, Lei and Liu, Jun and Zheng, Liang and Gedeon, Tom and Koniusz, Piotr},
  doi          = {10.1007/s11263-024-02070-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4091-4122},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Meet JEANIE: A similarity measure for 3D skeleton sequences via temporal-viewpoint alignment},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L3AM: Linear adaptive additive angular margin loss for
video-based hand gesture authentication. <em>IJCV</em>, <em>132</em>(9),
4073–4090. (<a
href="https://doi.org/10.1007/s11263-024-02068-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extractors significantly impact the performance of biometric systems. In the field of hand gesture authentication, existing studies focus on improving the model architectures and behavioral characteristic representation methods to enhance their feature extractors. However, loss functions, which can guide extractors to produce more discriminative identity features, are neglected. In this paper, we improve the margin-based Softmax loss functions, which are mainly designed for face authentication, in two aspects to form a new loss function for hand gesture authentication. First, we propose to replace the commonly used cosine function in the margin-based Softmax losses with a linear function to measure the similarity between identity features and proxies (the weight matrix of Softmax, which can be viewed as class centers). With the linear function, the main gradient magnitude decreases monotonically as the quality of the model improves during training, thus allowing the model to be quickly optimized in the early stage and precisely fine-tuned in the late stage. Second, we design an adaptive margin scheme to assign margin penalties to different samples according to their separability and the model quality in each iteration. Our adaptive margin scheme constrains the gradient magnitude. It can reduce radical (excessively large) gradient magnitudes and provide moderate (not too small) gradient magnitudes for model optimization, contributing to more stable training. The linear function and the adaptive margin scheme are complementary. Combining them, we obtain the proposed linear adaptive additive angular margin (L3AM) loss. To demonstrate the effectiveness of L3AM loss, we conduct extensive experiments on seven hand-related authentication datasets, compare it with 25 state-of-the-art (SOTA) loss functions, and apply it to eight SOTA hand gesture authentication models. The experimental results show that L3AM loss further improves the performance of the eight authentication models and outperforms the 25 losses. The code is available at https://github.com/SCUT-BIP-Lab/L3AM .},
  archive      = {J_IJCV},
  author       = {Song, Wenwei and Kang, Wenxiong and Kong, Adams Wai-Kin and Zhang, Yufeng and Qiao, Yitao},
  doi          = {10.1007/s11263-024-02068-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4073-4090},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {L3AM: Linear adaptive additive angular margin loss for video-based hand gesture authentication},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A causal inspired early-branching structure for domain
generalization. <em>IJCV</em>, <em>132</em>(9), 4052–4072. (<a
href="https://doi.org/10.1007/s11263-024-02061-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning domain-invariant semantic representations is crucial for achieving domain generalization (DG), where a model is required to perform well on unseen target domains. One critical challenge is that standard training often results in entangled semantic and domain-specific features. Previous works suggest formulating the problem from a causal perspective and solving the entanglement problem by enforcing marginal independence between the causal (i.e.semantic) and non-causal (i.e.domain-specific) features. Despite its simplicity, the basic marginal independent-based idea alone may be insufficient to identify the causal feature. By d-separation, we observe that the causal feature can be further characterized by being independent of the domain conditioned on the object, and we propose the following two strategies as complements for the basic framework. First, the observation implicitly implies that for the same object, the causal feature should not be associated with the non-causal feature, revealing that the common practice of obtaining the two features with a shared base feature extractor and two lightweight prediction heads might be inappropriate. To meet the constraint, we propose a simple early-branching structure, where the causal and non-causal feature obtaining branches share the first few blocks while diverging thereafter, for better structure design; Second, the observation implies that the causal feature remains invariant across different domains for the same object. To this end, we suggest that augmentation should be incorporated into the framework to better characterize the causal feature, and we further suggest an effective random domain sampling scheme to fulfill the task. Theoretical and experimental results show that the two strategies are beneficial for the basic marginal independent-based framework.},
  archive      = {J_IJCV},
  author       = {Chen, Liang and Zhang, Yong and Song, Yibing and Zhang, Zhen and Liu, Lingqiao},
  doi          = {10.1007/s11263-024-02061-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4052-4072},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A causal inspired early-branching structure for domain generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling up multi-domain semantic segmentation with sentence
embeddings. <em>IJCV</em>, <em>132</em>(9), 4036–4051. (<a
href="https://doi.org/10.1007/s11263-024-02060-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art semantic segmentation methods have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort. We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in Int J Comput Visi 111(1):98–136, 2015) at $$60\%$$ and $$65\%$$ mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at https://github.com/YvanYin/SSIW .},
  archive      = {J_IJCV},
  author       = {Yin, Wei and Liu, Yifan and Shen, Chunhua and Sun, Baichuan and van den Hengel, Anton},
  doi          = {10.1007/s11263-024-02060-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4036-4051},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Scaling up multi-domain semantic segmentation with sentence embeddings},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end video text spotting with transformer.
<em>IJCV</em>, <em>132</em>(9), 4019–4035. (<a
href="https://doi.org/10.1007/s11263-024-02063-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent video text spotting methods usually require the three-staged pipeline, i.e., detecting text in individual images, recognizing localized text, tracking text streams with post-processing to generate final results. The previous methods typically follow the tracking-by-match paradigm and develop sophisticated pipelines, which is an not effective solution. In this paper, rooted in Transformer sequence modeling, we propose a simple, yet effective end-to-end trainable video text DEtection, Tracking, and Recognition framework (TransDeTR), which views the VTS task as a direct long-range temporal modeling problem. TransDeTR mainly includes two advantages: (1) Different from the explicit match paradigm in the adjacent frame, the proposed TransDeTR tracks and recognizes each text implicitly by the different query termed ‘text query’ over long-range temporal sequence (more than 7 frames). (2) TransDeTR is the first end-to-end trainable video text spotting framework, which simultaneously addresses the three sub-tasks (e.g., text detection, tracking, recognition). Extensive experiments on four video text datasets (e.g., ICDAR2013 Video, ICDAR2015 Video) are conducted to demonstrate that TransDeTR achieves state-of-the-art performance with up to $$11.0\%$$ improvements on detection, tracking, and spotting tasks. Code can be found at: https://github.com/weijiawu/TransDETR .},
  archive      = {J_IJCV},
  author       = {Wu, Weijia and Cai, Yuanqiang and Shen, Chunhua and Zhang, Debing and Fu, Ying and Zhou, Hong and Luo, Ping},
  doi          = {10.1007/s11263-024-02063-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4019-4035},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {End-to-end video text spotting with transformer},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Species-agnostic patterned animal re-identification by
aggregating deep local features. <em>IJCV</em>, <em>132</em>(9),
4003–4018. (<a
href="https://doi.org/10.1007/s11263-024-02071-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access to large image volumes through camera traps and crowdsourcing provides novel possibilities for animal monitoring and conservation. It calls for automatic methods for analysis, in particular, when re-identifying individual animals from the images. Most existing re-identification methods rely on either hand-crafted local features or end-to-end learning of fur pattern similarity. The former does not need labeled training data, while the latter, although very data-hungry typically outperforms the former when enough training data is available. We propose a novel re-identification pipeline that combines the strengths of both approaches by utilizing modern learnable local features and feature aggregation. This creates representative pattern feature embeddings that provide high re-identification accuracy while allowing us to apply the method to small datasets by using pre-trained feature descriptors. We report a comprehensive comparison of different modern local features and demonstrate the advantages of the proposed pipeline on two very different species.},
  archive      = {J_IJCV},
  author       = {Nepovinnykh, Ekaterina and Chelak, Ilia and Eerola, Tuomas and Immonen, Veikka and Kälviäinen, Heikki and Kholiavchenko, Maksim and Stewart, Charles V.},
  doi          = {10.1007/s11263-024-02071-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {4003-4018},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Species-agnostic patterned animal re-identification by aggregating deep local features},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matching compound prototypes for few-shot action
recognition. <em>IJCV</em>, <em>132</em>(9), 3977–4002. (<a
href="https://doi.org/10.1007/s11263-024-02017-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of few-shot action recognition aims to recognize novel action classes using only a small number of labeled training samples. How to better describe the action in each video and how to compare the similarity between videos are two of the most critical factors in this task. Directly describing the video globally or by its individual frames cannot well represent the spatiotemporal dependencies within an action. On the other hand, naively matching the global representations of two videos is also not optimal since action can happen at different locations in a video with different speeds. In this work, we propose a novel approach that describes each video using multiple types of prototypes and then computes the video similarity with a particular matching strategy for each type of prototypes. To better model the spatiotemporal dependency, we describe the video by generating prototypes that model the multi-level spatiotemporal relations via transformers. There are a total of three types of prototypes. The first type of prototypes are trained to describe specific aspects of the action in the video e.g., the start of the action, regardless of its timestamp. These prototypes are directly matched one-to-one between two videos to compare their similarity. The second type of prototypes are the timestamp-centered prototypes that are trained to focus on specific timestamps of the video. To deal with the temporal variation of actions in a video, we apply bipartite matching to allow the matching of prototypes of different timestamps. The third type of prototypes are generated from the timestamp-centered prototypes, which regularize their temporal consistency while serving as an auxiliary summarization of the whole video. Experiments demonstrate that our proposed method achieves state-of-the-art results on multiple benchmarks.},
  archive      = {J_IJCV},
  author       = {Huang, Yifei and Yang, Lijin and Chen, Guo and Zhang, Hongjie and Lu, Feng and Sato, Yoichi},
  doi          = {10.1007/s11263-024-02017-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3977-4002},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Matching compound prototypes for few-shot action recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-agnostic priors for semantic segmentation under
unsupervised domain adaptation and domain generalization. <em>IJCV</em>,
<em>132</em>(9), 3954–3976. (<a
href="https://doi.org/10.1007/s11263-024-02041-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, an important challenge to deep neural networks comes from adjusting the varying properties of different image domains. To study this problem, researchers have been investigating a practical setting in which the deep neural networks are trained on a labeled source domain and then transferred to an unlabeled or even unseen target domain. The major difficulty lies in the potential domain gap, which essentially arises from the overfitting in the source domain. Hence, it is important to introduce generalized priors to alleviate the issue. From this perspective, this paper presents a novel framework that forces visual features to align with domain-agnostic priors (DAP). Specifically, we study two kinds of priors, (i) language-guided embedding and (ii) class-level relationship, and we believe that more such priors can be constructed. Our framework, referred to as DAP, is evaluated on both unsupervised domain adaptation (UDA) and domain generalization (DG) where the target domain is unlabeled and even unseen, respectively. We use the standard benchmark that performs transfer semantic segmentation on synthesized datasets (i.e., GTAv and SYNTHIA) and a real dataset (i.e., Cityscapes). Experiments validate the effectiveness of DAP with competitive accuracy in all tasks. In particular, language-guided priors work sufficiently well for UDA, while class-level priors serve as useful complements for DG. The proposed frameworks shed light that domain transfer benefits from better proxies, possibly from other modalities.},
  archive      = {J_IJCV},
  author       = {Huo, Xinyue and Xie, Lingxi and Hu, Hengtong and Zhou, Wengang and Li, Houqiang and Tian, Qi},
  doi          = {10.1007/s11263-024-02041-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3954-3976},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Domain-agnostic priors for semantic segmentation under unsupervised domain adaptation and domain generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light flickering guided reflection removal. <em>IJCV</em>,
<em>132</em>(9), 3933–3953. (<a
href="https://doi.org/10.1007/s11263-024-02073-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When photographing through a piece of glass, reflections usually degrade the quality of captured images or videos. In this paper, by exploiting periodically varying light flickering, we investigate the problem of removing strong reflections from contaminated image sequences or videos with a unified capturing setup. We propose a learning-based method that utilizes short-term and long-term observations of mixture videos to exploit one-side contextual clues in fluctuant components and brightness-consistent clues in consistent components for achieving layer separation and flickering removal, respectively. A dataset containing synthetic and real mixture videos with light flickering is built for network training and testing. The effectiveness of the proposed method is demonstrated by the comprehensive evaluation on synthetic and real data, the application for video flickering removal, and the exploratory experiment on high-speed scenes.},
  archive      = {J_IJCV},
  author       = {Hong, Yuchen and Chang, Yakun and Liang, Jinxiu and Ma, Lei and Huang, Tiejun and Shi, Boxin},
  doi          = {10.1007/s11263-024-02073-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3933-3953},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Light flickering guided reflection removal},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PIE: Physics-inspired low-light enhancement. <em>IJCV</em>,
<em>132</em>(9), 3911–3932. (<a
href="https://doi.org/10.1007/s11263-024-01995-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a physics-inspired contrastive learning paradigm for low-light enhancement, called PIE. PIE primarily addresses three issues: (i) To resolve the problem of existing learning-based methods often training a LLE model with strict pixel-correspondence image pairs, we eliminate the need for pixel-correspondence paired training data and instead train with unpaired images. (ii) To address the disregard for negative samples and the inadequacy of their generation in existing methods, we incorporate physics-inspired contrastive learning for LLE and design the Bag of Curves (BoC) method to generate more reasonable negative samples that closely adhere to the underlying physical imaging principle. (iii) To overcome the reliance on semantic ground truths in existing methods, we propose an unsupervised regional segmentation module, ensuring regional brightness consistency while eliminating the dependency on semantic ground truths. Overall, the proposed PIE can effectively learn from unpaired positive/negative samples and smoothly realize non-semantic regional enhancement, which is clearly different from existing LLE efforts. Besides the novel architecture of PIE, we explore the gain of PIE on downstream tasks such as semantic segmentation and face detection. Training on readily available open data and extensive experiments demonstrate that our method surpasses the state-of-the-art LLE models over six independent cross-scenes datasets. PIE runs fast with reasonable GFLOPs in test time, making it easy to use on mobile devices.},
  archive      = {J_IJCV},
  author       = {Liang, Dong and Xu, Zhengyan and Li, Ling and Wei, Mingqiang and Chen, Songcan},
  doi          = {10.1007/s11263-024-01995-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3911-3932},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PIE: Physics-inspired low-light enhancement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position, padding and predictions: A deeper look at position
information in CNNs. <em>IJCV</em>, <em>132</em>(9), 3889–3910. (<a
href="https://doi.org/10.1007/s11263-024-02069-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. Theoretically, an implication of this fact is that a filter may know what it is looking at, but not where it is positioned in the image. In this paper, we first test this hypothesis and reveal that a surprising degree of absolute position information is encoded in commonly used CNNs. We show that zero padding drives CNNs to encode position information in their internal representations, while a lack of padding precludes position encoding. This observation gives rise to deeper questions about the role of position information in CNNs: (i) What boundary heuristics enable optimal position encoding for downstream tasks? (ii) Does position encoding affect the learning of semantic representations? (iii) Does position encoding always improve performance? To provide answers, we perform the largest case study to date on the role that padding and border heuristics play in CNNs. We design novel tasks that allow us to quantify boundary effects as a function of the distance to the border. Numerous semantic objectives reveal the effect of the border on semantic representations. Finally, we demonstrate the implications of these findings on multiple real-world tasks to show that position information can both help or hurt performance.},
  archive      = {J_IJCV},
  author       = {Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos G. and Bruce, Neil D. B.},
  doi          = {10.1007/s11263-024-02069-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3889-3910},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Position, padding and predictions: A deeper look at position information in CNNs},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). MutualFormer: Multi-modal representation learning via
cross-diffusion attention. <em>IJCV</em>, <em>132</em>(9), 3867–3888.
(<a href="https://doi.org/10.1007/s11263-024-02067-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregating multi-modal data to obtain reliable data representation attracts more and more attention. Recent studies demonstrate that Transformer models usually work well for multi-modal tasks. Existing Transformers generally either adopt the cross-attention (CA) mechanism or simple concatenation to achieve the information interaction among different modalities which generally ignore the issue of modality gap. In this work, we re-think Transformer and extend it to MutualFormer for multi-modal data representation. Rather than CA in Transformer, MutualFormer employs our new design of cross-diffusion attention (CDA) to conduct the information communication among different modalities. Comparing with CA, the main advantages of the proposed CDA are three aspects. First, the cross-affinities in CDA are defined based on the individual modal affinities (token metrics) which thus can naturally alleviate the issue of modality/domain gap existed in traditional token feature based CA definition. Second, CDA provides a general scheme which can either be used for multi-modal representation or serve as the post-optimization for existing CA models. Third, CDA is implemented efficiently. We successfully apply the MutualFormer on several multi-modal learning tasks. Extensive experiments demonstrate the effectiveness of the proposed MutualFormer.},
  archive      = {J_IJCV},
  author       = {Wang, Xixi and Wang, Xiao and Jiang, Bo and Tang, Jin and Luo, Bin},
  doi          = {10.1007/s11263-024-02067-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3867-3888},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MutualFormer: Multi-modal representation learning via cross-diffusion attention},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated heterogeneous graph attention network for
incomplete multi-modal clustering. <em>IJCV</em>, <em>132</em>(9),
3847–3866. (<a
href="https://doi.org/10.1007/s11263-024-02066-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-modal clustering (IMmC) is challenging due to the unexpected missing of some modalities in data. A key to this problem is to explore complementarity information among different samples with incomplete information of unpaired data. Despite preliminary progress, existing methods suffer from (1) relying heavily on paired data, and (2) difficulty in mining complementarity on data with high missing rates. To address the problems, we propose a novel method, Integrated Heterogeneous Graph ATtention (IHGAT) network, for IMmC. To fully exploit the complementarity among different samples and modalities, we first construct a set of integrated heterogeneous graphs based on the similarity graph learned from unified latent representations and the modality-specific availability graphs formed by the existing relations of different samples. Thereafter, the attention mechanism is applied to the constructed integrated heterogeneous graph to aggregate the embedded content of heterogeneous neighbors for each node. In this way, the representations of missing modalities can be learned based on the complementarity information of other samples and their other modalities. Finally, the consistency of probability distribution is embedded into the network for clustering. Consequently, the proposed method can form a complete latent space where incomplete information can be supplemented by other related samples via the learned intrinsic structure. Extensive experiments on eight public datasets show that the proposed IHGAT outperforms existing methods under various settings and is typically more robust in cases of high missing rates.},
  archive      = {J_IJCV},
  author       = {Wang, Yu and Yao, Xinjie and Zhu, Pengfei and Li, Weihao and Cao, Meng and Hu, Qinghua},
  doi          = {10.1007/s11263-024-02066-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3847-3866},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Integrated heterogeneous graph attention network for incomplete multi-modal clustering},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open-world, diverse, cross-spatial-temporal benchmark for
dynamic wild person re-identification. <em>IJCV</em>, <em>132</em>(9),
3823–3846. (<a
href="https://doi.org/10.1007/s11263-024-02057-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) has made great strides thanks to the data-driven deep learning techniques. However, the existing benchmark datasets lack diversity, and models trained on these data cannot generalize well to dynamic wild scenarios. To meet the goal of improving the explicit generalization of ReID models, we develop a new Open-World, Diverse, Cross-Spatial-Temporal dataset named OWD with several distinct features. (1) Diverse collection scenes: multiple independent open-world and highly dynamic collecting scenes, including streets, intersections, shopping malls, etc. (2) Diverse lighting variations: long time spans from daytime to nighttime with abundant illumination changes. (3) Diverse person status: multiple camera networks in all seasons with normal/adverse weather conditions and diverse pedestrian appearances (e.g., clothes, personal belongings, poses, etc.). (4) Protected privacy: invisible faces for privacy critical applications. To improve the implicit generalization of ReID, we further propose a Latent Domain Expansion (LDE) method to develop the potential of source data, which decouples discriminative identity-relevant and trustworthy domain-relevant features and implicitly enforces domain-randomized identity feature space expansion with richer domain diversity to facilitate domain-invariant representations. Our comprehensive evaluations with most benchmark datasets in the community are crucial for progress, although this work is far from the grand goal toward open-world and dynamic wild applications. The project page is https://github.com/fxw13/OWD .},
  archive      = {J_IJCV},
  author       = {Zhang, Lei and Fu, Xiaowei and Huang, Fuxiang and Yang, Yi and Gao, Xinbo},
  doi          = {10.1007/s11263-024-02057-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3823-3846},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An open-world, diverse, cross-spatial-temporal benchmark for dynamic wild person re-identification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I2DFormer+: Learning image to document summary attention for
zero-shot image classification. <em>IJCV</em>, <em>132</em>(9),
3806–3822. (<a
href="https://doi.org/10.1007/s11263-024-02053-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the tremendous progress in zero-shot learning (ZSL), the majority of existing methods still rely on human-annotated attributes, which are difficult to annotate and scale. An unsupervised alternative is to represent each class using the word embedding associated with its semantic class name. However, word embeddings extracted from pre-trained language models do not necessarily capture visual similarities, resulting in poor zero-shot performance. In this work, we argue that online textual documents, e.g., Wikipedia, contain rich visual descriptions about object classes, therefore can be used as powerful unsupervised side information for ZSL. To this end, we propose I2DFormer+, a novel transformer-based ZSL framework that jointly learn to encode images and documents by aligning both modalities in a shared embedding space. I2DFormer+ utilizes our novel Document Summary Transformer (DSTransformer), a text transformer, that learns to encode a sequence of text into a fixed set of summary tokens. These summary tokens are utilized by a cross-model attention module that learns finegrained interactions between image patches and the summary of the document. Consequently, our I2DFormer+ not only learns highly discriminative document embeddings that capture visual similarities but also gains the ability to explain what regions of the image are important for the decision. Quantitatively, we demonstrate that I2DFormer+ significantly outperforms previous unsupervised semantic embeddings under both zero-shot and generalized zero-shot learning settings on three public datasets. Qualitatively, we show that our methods lead to highly interpretable results. Furthermore, we scale our model to the large scale zero-shot learning setting and show state-of-the-art performance on two challenging ImageNet benchmarks.},
  archive      = {J_IJCV},
  author       = {Naeem, Muhammad Ferjad and Xian, Yongqin and Gool, Luc Van and Tombari, Federico},
  doi          = {10.1007/s11263-024-02053-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3806-3822},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {I2DFormer+: Learning image to document summary attention for zero-shot image classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Descriptor distillation: A teacher-student-regularized
framework for learning local descriptors. <em>IJCV</em>,
<em>132</em>(9), 3787–3805. (<a
href="https://doi.org/10.1007/s11263-024-02039-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a fast and discriminative patch descriptor is a challenging topic in computer vision. Recently, many existing works focus on training various descriptor learning networks by minimizing a triplet loss (or its variants), which is expected to decrease the distance between each positive pair and increase the distance between each negative pair. However, such an expectation has to be lowered due to the non-perfect convergence of network optimizer to a local solution. Addressing this problem and the open computational speed problem, we propose a Descriptor Distillation framework for local descriptor learning, called DesDis, where a student model gains knowledge from a pre-trained teacher model, and it is further enhanced via a designed teacher-student regularizer. This teacher-student regularizer is to constrain the difference between the positive (also negative) pair similarity from the teacher model and that from the student model, and we theoretically prove that a more effective student model could be trained by minimizing a weighted combination of the triplet loss and this regularizer, than its teacher which is trained by minimizing the triplet loss singly. Under the proposed DesDis, many existing descriptor networks could be embedded as the teacher model, and accordingly, both equal-weight and light-weight student models could be derived, which outperform their teacher in either accuracy or speed. Experimental results on 3 public datasets demonstrate that the equal-weight student models, derived from the proposed DesDis framework by utilizing three typical descriptor learning networks as teacher models, could achieve significantly better performances than their teachers and several other comparative methods. In addition, the derived light-weight models could achieve 8 times or even faster speeds than the comparative methods under similar patch verification performances.},
  archive      = {J_IJCV},
  author       = {Liu, Yuzhen and Dong, Qiulei},
  doi          = {10.1007/s11263-024-02039-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3787-3805},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Descriptor distillation: A teacher-student-regularized framework for learning local descriptors},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WildCLIP: Scene and animal attribute retrieval from camera
trap data with domain-adapted vision-language models. <em>IJCV</em>,
<em>132</em>(9), 3770–3786. (<a
href="https://doi.org/10.1007/s11263-024-02026-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildlife observation with camera traps has great potential for ethology and ecology, as it gathers data non-invasively in an automated way. However, camera traps produce large amounts of uncurated data, which is time-consuming to annotate. Existing methods to label these data automatically commonly use a fixed pre-defined set of distinctive classes and require many labeled examples per class to be trained. Moreover, the attributes of interest are sometimes rare and difficult to find in large data collections. Large pretrained vision-language models, such as contrastive language image pretraining (CLIP), offer great promises to facilitate the annotation process of camera-trap data. Images can be described with greater detail, the set of classes is not fixed and can be extensible on demand and pretrained models can help to retrieve rare samples. In this work, we explore the potential of CLIP to retrieve images according to environmental and ecological attributes. We create WildCLIP by fine-tuning CLIP on wildlife camera-trap images and to further increase its flexibility, we add an adapter module to better expand to novel attributes in a few-shot manner. We quantify WildCLIP’s performance and show that it can retrieve novel attributes in the Snapshot Serengeti dataset. Our findings outline new opportunities to facilitate annotation processes with complex and multi-attribute captions. The code is available at https://github.com/amathislab/wildclip .},
  archive      = {J_IJCV},
  author       = {Gabeff, Valentin and Rußwurm, Marc and Tuia, Devis and Mathis, Alexander},
  doi          = {10.1007/s11263-024-02026-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3770-3786},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WildCLIP: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal machine learning in image-based and clinical
biomedicine: Survey and prospects. <em>IJCV</em>, <em>132</em>(9),
3753–3769. (<a
href="https://doi.org/10.1007/s11263-024-02032-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also highlights the need for principled assessments and practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers and personnel. Despite advancements, challenges such as data biases and the scarcity of “big data” in many biomedical domains persist. We conclude with a discussion on principled innovation and collaborative efforts to further the mission of seamless integration of multimodal ML models into biomedical practice.},
  archive      = {J_IJCV},
  author       = {Warner, Elisa and Lee, Joonsang and Hsu, William and Syeda-Mahmood, Tanveer and Kahn Jr., Charles E. and Gevaert, Olivier and Rao, Arvind},
  doi          = {10.1007/s11263-024-02032-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3753-3769},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multimodal machine learning in image-based and clinical biomedicine: Survey and prospects},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting the softmax with additional confidence scores for
improved selective classification with out-of-distribution data.
<em>IJCV</em>, <em>132</em>(9), 3714–3752. (<a
href="https://doi.org/10.1007/s11263-024-02029-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Combination (SIRC), that augments a softmax-based confidence score with a secondary class-agnostic feature-based score. Thus, the ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD, whilst existing OOD detection methods fail to do so. Interestingly, we find that the secondary scores investigated for SIRC do not consistently improve performance on all tested OOD datasets. To address this issue, we further extend SIRC to incorporate multiple secondary scores (SIRC+). This further improves SCOD performance, both generally, and in terms of consistency over diverse distribution shifts. Code is available at https://github.com/Guoxoug/SIRC .},
  archive      = {J_IJCV},
  author       = {Xia, Guoxuan and Bouganis, Christos-Savvas},
  doi          = {10.1007/s11263-024-02029-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3714-3752},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Augmenting the softmax with additional confidence scores for improved selective classification with out-of-distribution data},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VNAS: Variational neural architecture search. <em>IJCV</em>,
<em>132</em>(9), 3689–3713. (<a
href="https://doi.org/10.1007/s11263-024-02014-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable neural architecture search delivers point estimation to the optimal architecture, which yields arbitrarily high confidence to the learned architecture. This approach thus suffers in calibration and robustness, in contrast with the maximum a posteriori estimation scheme. In this paper, we propose a novel Variational Neural Architecture Search (VNAS) method that estimates and exploits the weight variability in the following three steps. VNAS first learns the weight distribution through variational inference which minimizes the expected lower bound on the marginal likelihood of architecture using unbiased Monte Carlo gradient estimation. A group of optimal architecture candidates is then drawn according to the learned weight distribution with the complexity constraint. The optimal architecture is further inferred under a novel training-free architecture-performance estimator, designed to score the network architectures at initialization without training, which significantly reduces the computational cost of the optimal architecture estimator. Extensive experiments show that VNAS significantly outperforms the state-of-the-art methods in classification performance and adversarial robustness.},
  archive      = {J_IJCV},
  author       = {Ma, Benteng and Zhang, Jing and Xia, Yong and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02014-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3689-3713},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {VNAS: Variational neural architecture search},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On finite difference jacobian computation in deformable
image registration. <em>IJCV</em>, <em>132</em>(9), 3678–3688. (<a
href="https://doi.org/10.1007/s11263-024-02047-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Producing spatial transformations that are diffeomorphic is a key goal in deformable image registration. As a diffeomorphic transformation should have positive Jacobian determinant $$\vert J\vert $$ everywhere, the number of pixels (2D) or voxels (3D) with $$\vert J\vert &lt;0$$ has been used to test for diffeomorphism and also to measure the irregularity of the transformation. For digital transformations, $$\vert J\vert $$ is commonly approximated using a central difference, but this strategy can yield positive $$\vert J\vert $$ ’s for transformations that are clearly not diffeomorphic—even at the pixel or voxel resolution level. To show this, we first investigate the geometric meaning of different finite difference approximations of $$\vert J\vert $$ . We show that to determine if a deformation is diffeomorphic for digital images, the use of any individual finite difference approximation of $$\vert J\vert $$ is insufficient. We further demonstrate that for a 2D transformation, four unique finite difference approximations of $$\vert J\vert $$ ’s must be positive to ensure that the entire domain is invertible and free of folding at the pixel level. For a 3D transformation, ten unique finite differences approximations of $$\vert J\vert $$ ’s are required to be positive. Our proposed digital diffeomorphism criteria solves several errors inherent in the central difference approximation of $$\vert J\vert $$ and accurately detects non-diffeomorphic digital transformations. The source code of this work is available at https://github.com/yihao6/digital_diffeomorphism .},
  archive      = {J_IJCV},
  author       = {Liu, Yihao and Chen, Junyu and Wei, Shuwen and Carass, Aaron and Prince, Jerry},
  doi          = {10.1007/s11263-024-02047-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3678-3688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On finite difference jacobian computation in deformable image registration},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with noisy correspondence. <em>IJCV</em>,
<em>132</em>(9), 3656–3677. (<a
href="https://doi.org/10.1007/s11263-024-02064-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a new learning paradigm for noisy labels, i.e., noisy correspondence (NC). Unlike the well-studied noisy labels that consider the errors in the category annotation of a sample, the NC refers to the errors in the alignment relationship of two data points. Although such false positive pairs are common especially in the data harvested from the Internet, which however are neglected by most existing works. By taking cross-modal retrieval as a showcase, we propose a method called learning with noisy correspondence (LNC). In brief, the LNC first roughly obtains the clean and noisy subsets from the original data and then rectifies the false positive pairs by using a novel adaptive prediction function. Finally, the LNC adopts a novel triplet loss with soft margins to endow cross-modal retrieval the robustness to the NC. To verify the effectiveness of the proposed LNC, we conduct experiments on six benchmark datasets in image-text and video-text retrieval tasks. Besides the effectiveness of the LNC, the experimental results show the necessity of the explicit solution to the NC faced by not only the standard model training paradigm but also the pre-training and fine-tuning paradigms.},
  archive      = {J_IJCV},
  author       = {Huang, Zhenyu and Hu, Peng and Niu, Guocheng and Xiao, Xinyan and Lv, Jiancheng and Peng, Xi},
  doi          = {10.1007/s11263-024-02064-0},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3656-3677},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning with noisy correspondence},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble quadratic assignment network for graph matching.
<em>IJCV</em>, <em>132</em>(9), 3633–3655. (<a
href="https://doi.org/10.1007/s11263-024-02040-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching is a commonly used technique in computer vision and pattern recognition. Recent data-driven approaches have improved the graph matching accuracy remarkably, whereas some traditional algorithm-based methods are more robust to feature noises, outlier nodes, and global transformation (e.g. rotation). In this paper, we propose a graph neural network (GNN) based approach to combine the advantage of data-driven and traditional methods. In the GNN framework, we transform traditional graph matching solvers as single-channel GNNs on the association graph and extend the single-channel architecture to the multi-channel network. The proposed model can be seen as an ensemble method that fuses multiple algorithms at every iteration. Instead of averaging the estimates at the end of the ensemble, in our approach, the independent iterations of the ensembled algorithms exchange their information after each iteration via a $$1\,\times \,1$$ channel-wise convolution layer. Experiments show that our model improves the performance of traditional algorithms significantly. In addition, we propose a random sampling strategy to reduce the computational complexity and GPU memory usage, so that the model is applicable to matching graphs with thousands of nodes. We evaluate the performance of our method on three tasks: geometric graph matching, semantic feature matching, and few-shot 3D shape classification. The proposed model performs comparably or outperforms the best existing GNN-based methods.},
  archive      = {J_IJCV},
  author       = {Tan, Haoru and Wang, Chuang and Wu, Sitong and Zhang, Xu-Yao and Yin, Fei and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-024-02040-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3633-3655},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Ensemble quadratic assignment network for graph matching},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRetinex: A progressive color-shift aware retinex model for
low-light image enhancement. <em>IJCV</em>, <em>132</em>(9), 3610–3632.
(<a href="https://doi.org/10.1007/s11263-024-02065-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light environments introduce various complex degradations into captured images. Retinex-based methods have demonstrated effective enhancement performance by decomposing an image into illumination and reflectance, allowing for selective adjustment and removal of degradations. However, different types of pollutions in reflectance are often treated together. The absence of explicit distinction and definition of various pollution types results in residual pollutions in the results. Typically, the color shift, which is generally spatially invariant, differs from other spatially variant pollution and proves challenging to eliminate with denoising methods. The remaining color shift compromises color constancy both theoretically and in practice. In this paper, we consider different manifestations of degradations and further decompose them. We propose a color-shift aware Retinex model, termed as CRetinex, which decomposes an image into reflectance, color shift, and illumination. Specific networks are designed to remove spatially variant pollution, correct color shift, and adjust illumination separately. Comparative experiments with the state-of-the-art demonstrate the qualitative and quantitative superiority of our approach. Furthermore, extensive experiments on multiple datasets, including real and synthetic images, along with extended validation, confirm the effectiveness of color-shift aware decomposition and the generalization of CRetinex over a wide range of low-light levels.},
  archive      = {J_IJCV},
  author       = {Xu, Han and Zhang, Hao and Yi, Xunpeng and Ma, Jiayi},
  doi          = {10.1007/s11263-024-02065-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3610-3632},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CRetinex: A progressive color-shift aware retinex model for low-light image enhancement},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-aware conversion from ANN to SNN via post-training
parameter calibration. <em>IJCV</em>, <em>132</em>(9), 3586–3609. (<a
href="https://doi.org/10.1007/s11263-024-02046-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Network (SNN), originating from the neural behavior in biology, has been recognized as one of the next-generation neural networks. Conventionally, SNNs can be obtained by converting from pre-trained Artificial Neural Networks (ANNs) by replacing the non-linear activation with spiking neurons without changing the parameters. In this work, we argue that simply copying and pasting the weights of ANN to SNN inevitably results in activation mismatch, especially for ANNs that are trained with batch normalization (BN) layers. To tackle the activation mismatch issue, we first provide a theoretical analysis by decomposing local layer-wise conversion error, and then quantitatively measure how this error propagates throughout the layers using the second-order analysis. Motivated by the theoretical results, we propose a set of layer-wise parameter calibration algorithms, which adjusts the parameters to minimize the activation mismatch. To further remove the dependency on data, we propose a privacy-preserving conversion regime by distilling synthetic data from source ANN and using it to calibrate the SNN. Extensive experiments for the proposed algorithms are performed on modern architectures and large-scale tasks including ImageNet classification and MS COCO detection. We demonstrate that our method can handle the SNN conversion and effectively preserve high accuracy even in 32 time steps. For example, our calibration algorithms can increase up to 63% accuracy when converting MobileNet against baselines.},
  archive      = {J_IJCV},
  author       = {Li, Yuhang and Deng, Shikuang and Dong, Xin and Gu, Shi},
  doi          = {10.1007/s11263-024-02046-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3586-3609},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Error-aware conversion from ANN to SNN via post-training parameter calibration},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSODv2: A deep calibrated few-shot object detection network.
<em>IJCV</em>, <em>132</em>(9), 3566–3585. (<a
href="https://doi.org/10.1007/s11263-024-02049-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for object detection typically necessitate a substantial amount of training data, and creating high-quality training data is time-consuming. We propose a novel Few-Shot Object Detection network (FSODv2) in this paper that aims to detect objects from previously unseen categories using only a few annotated examples. Attention RPN, Multi-Relation Detector, and Contrastive Training strategy are central to our method (Fan et al., in: CVPR, 2020), which exploit similarity between few shot support set and query set to detect novel objects while suppressing false detection in the background. We also contribute a new dataset, FSOD-1k, which contains 1000 categories of various objects with high-quality annotations to train our network. To the best of our knowledge, this is one of the first datasets designed for few-shot object detection. This paper improves our FSOD model through well-designed model calibration in three areas: (1) we propose an improved FPN with multi-scale support inputs to calibrate the multi-scale support-query feature matching by exploiting multi-scale features from the same support image with different input scales; (2) we introduce a support classification supervision branch to calibrate the support feature supervision, aligning to the query feature training supervision; (3) we propose backbone calibration to preserve prior knowledge while alleviating backbone bias toward base classes by employing classification dataset to help our model calibration procedure, where such dataset has previously only been used for pre-training in other related works. Besides, we propose a Fast Attention RPN to improve evaluation speed and save computational memory during inference. Once trained, our few-shot network can detect objects from previously unseen categories without further training or fine-tuning, resulting in new state-of-the-art performance on different datasets in the few-shot setting. Our method is general in scope and has numerous potential applications. The dataset link is https://github.com/fanq15/Few-Shot-Object-Detection-Dataset .},
  archive      = {J_IJCV},
  author       = {Fan, Qi and Zhuo, Wei and Tang, Chi-Keung and Tai, Yu-Wing},
  doi          = {10.1007/s11263-024-02049-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3566-3585},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FSODv2: A deep calibrated few-shot object detection network},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMoT: Mixture-of-modality-tokens transformer for composed
multimodal conditional image synthesis. <em>IJCV</em>, <em>132</em>(9),
3537–3565. (<a
href="https://doi.org/10.1007/s11263-024-02044-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal conditional image synthesis (MCIS) methods generate images conditioned on any combinations of various modalities that require all of them must be exactly conformed, hindering the synthesis controllability and leaving the potential of cross-modality under-exploited. To this end, we propose to generate images conditioned on the compositions of multimodal control signals, where modalities are imperfectly complementary, i.e., composed multimodal conditional image synthesis (CMCIS). Specifically, we observe two challenging issues of the proposed CMCIS task, i.e., the modality coordination problem and the modality imbalance problem. To tackle these issues, we introduce a Mixture-of-Modality-Tokens Transformer (MMoT) that adaptively fuses fine-grained multimodal control signals, a multimodal balanced training loss to stabilize the optimization of each modality, and a multimodal sampling guidance to balance the strength of each modality control signal. Comprehensive experimental results demonstrate that MMoT achieves superior performance on both unimodal conditional image synthesis and MCIS tasks with high-quality and faithful image synthesis on complex multimodal conditions. The project website is available at https://jabir-zheng.github.io/MMoT .},
  archive      = {J_IJCV},
  author       = {Zheng, Jianbin and Liu, Daqing and Wang, Chaoyue and Hu, Minghui and Yang, Zuopeng and Ding, Changxing and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02044-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3537-3565},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MMoT: Mixture-of-modality-tokens transformer for composed multimodal conditional image synthesis},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EATFormer: Improving vision transformer inspired by
evolutionary algorithm. <em>IJCV</em>, <em>132</em>(9), 3509–3536. (<a
href="https://doi.org/10.1007/s11263-024-02034-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by biological evolution, this paper explains the rationality of Vision Transformer by analogy with the proven practical evolutionary algorithm (EA) and derives that both have consistent mathematical formulation. Then inspired by effective EA variants, we propose a novel pyramid EATFormer backbone that only contains the proposed EA-based transformer (EAT) block, which consists of three residual parts, i.e., Multi-scale region aggregation, global and local interaction, and feed-forward network modules, to model multi-scale, interactive, and individual information separately. Moreover, we design a task-related head docked with transformer backbone to complete final information fusion more flexibly and improve a modulated deformable MSA to dynamically model irregular locations. Massive quantitative and quantitative experiments on image classification, downstream tasks, and explanatory experiments demonstrate the effectiveness and superiority of our approach over state-of-the-art methods. E.g., our Mobile (1.8 M), Tiny (6.1 M), Small (24.3 M), and Base (49.0 M) models achieve 69.4, 78.4, 83.1, and 83.9 Top-1 only trained on ImageNet-1K with naive training recipe; EATFormer-Tiny/Small/Base armed Mask-R-CNN obtain 45.4/47.4/49.0 box AP and 41.4/42.9/44.2 mask AP on COCO detection, surpassing contemporary MPViT-T, Swin-T, and Swin-S by 0.6/1.4/0.5 box AP and 0.4/1.3/0.9 mask AP separately with less FLOPs; Our EATFormer-Small/Base achieve 47.3/49.3 mIoU on ADE20K by Upernet that exceeds Swin-T/S by 2.8/1.7. Code is available at https://github.com/zhangzjn/EATFormer .},
  archive      = {J_IJCV},
  author       = {Zhang, Jiangning and Li, Xiangtai and Wang, Yabiao and Wang, Chengjie and Yang, Yibo and Liu, Yong and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02034-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3509-3536},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EATFormer: Improving vision transformer inspired by evolutionary algorithm},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperbolic deep learning in computer vision: A survey.
<em>IJCV</em>, <em>132</em>(9), 3484–3508. (<a
href="https://doi.org/10.1007/s11263-024-02043-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep representation learning is a ubiquitous part of modern computer vision. While Euclidean space has been the de facto standard manifold for learning visual representations, hyperbolic space has recently gained rapid traction for learning in computer vision. Specifically, hyperbolic learning has shown a strong potential to embed hierarchical structures, learn from limited samples, quantify uncertainty, add robustness, limit error severity, and more. In this paper, we provide a categorization and in-depth overview of current literature on hyperbolic learning for computer vision. We research both supervised and unsupervised literature and identify three main research themes in each direction. We outline how hyperbolic learning is performed in all themes and discuss the main research problems that benefit from current advances in hyperbolic learning for computer vision. Moreover, we provide a high-level intuition behind hyperbolic geometry and outline open research questions to further advance research in this direction.},
  archive      = {J_IJCV},
  author       = {Mettes, Pascal and Ghadimi Atigh, Mina and Keller-Ressel, Martin and Gu, Jeffrey and Yeung, Serena},
  doi          = {10.1007/s11263-024-02043-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3484-3508},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hyperbolic deep learning in computer vision: A survey},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InterGen: Diffusion-based multi-human motion generation
under complex interactions. <em>IJCV</em>, <em>132</em>(9), 3463–3483.
(<a href="https://doi.org/10.1007/s11263-024-02042-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have recently seen tremendous progress in diffusion advances for generating realistic human motions. Yet, they largely disregard the multi-human interactions. In this paper, we present InterGen, an effective diffusion-based approach that enables layman users to customize high-quality two-person interaction motions, with only text guidance. We first contribute a multimodal dataset, named InterHuman. It consists of about 107 M frames for diverse two-person interactions, with accurate skeletal motions and 23,337 natural language descriptions. For the algorithm side, we carefully tailor the motion diffusion model to our two-person interaction setting. To handle the symmetry of human identities during interactions, we propose two cooperative transformer-based denoisers that explicitly share weights, with a mutual attention mechanism to further connect the two denoising processes. Then, we propose a novel representation for motion input in our interaction diffusion model, which explicitly formulates the global relations between the two performers in the world frame. We further introduce two novel regularization terms to encode spatial relations, equipped with a corresponding damping scheme during the training of our interaction diffusion model. Extensive experiments validate the effectiveness of InterGen ( https://tr3e.github.io/intergen-page/ ). Notably, it can generate more diverse and compelling two-person motions than previous methods and enables various downstream applications for human interactions.},
  archive      = {J_IJCV},
  author       = {Liang, Han and Zhang, Wenqian and Li, Wenxuan and Yu, Jingyi and Xu, Lan},
  doi          = {10.1007/s11263-024-02042-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3463-3483},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {InterGen: Diffusion-based multi-human motion generation under complex interactions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pictorial and apictorial polygonal jigsaw puzzles from
arbitrary number of crossing cuts. <em>IJCV</em>, <em>132</em>(9),
3428–3462. (<a
href="https://doi.org/10.1007/s11263-024-02033-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jigsaw puzzle solving, the problem of constructing a coherent whole from a set of non-overlapping unordered visual fragments, is fundamental to numerous applications, and yet most of the literature of the last two decades has focused thus far on less realistic puzzles whose pieces are identical squares. Here we formalize a new type of jigsaw puzzle where the pieces are general convex polygons generated by cutting through a global polygonal shape/image with an arbitrary number of straight cuts, a generation model inspired by the celebrated Lazy caterer’s sequence. We analyze the theoretical properties of such puzzles, including the inherent challenges in solving them once pieces are contaminated with geometrical noise. To cope with such difficulties and obtain tractable solutions, we abstract the problem as a multi-body spring-mass dynamical system endowed with hierarchical loop constraints and a layered reconstruction process. We define evaluation metrics and present experimental results on both apictorial and pictorial puzzles to show that they are solvable completely automatically.},
  archive      = {J_IJCV},
  author       = {Harel, Peleg and Shahar, Ofir Itzhak and Ben-Shahar, Ohad},
  doi          = {10.1007/s11263-024-02033-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3428-3462},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pictorial and apictorial polygonal jigsaw puzzles from arbitrary number of crossing cuts},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UrbanEvolver: Function-aware urban layout regeneration.
<em>IJCV</em>, <em>132</em>(9), 3408–3427. (<a
href="https://doi.org/10.1007/s11263-024-02030-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban regeneration is an important strategy for land redevelopment, to address the urban decay in cities. Among many tasks, urban layout is the foundation for urban regeneration. In this paper, we target a new task called function-aware urban layout regeneration, and propose UrbanEvolver, a function-aware deep generative model for the task. Given a target region to be regenerated, our model outputs a regenerated urban layout (i.e., roads and buildings) for the target region by considering the function (i.e., land use type) of the target region and its surrounding context (i.e., the functions and urban layouts of the surrounding regions). UrbanEvolver first extracts implicit regeneration rules from the target function and the surrounding context by encoding them separately in different scales through the function-layout adaptive (FA) blocks, and then constrains the regenerated urban layout based on the learned regeneration rules. To enforce the regenerated layout to be valid and to follow the road structure, we design a set of losses covering both pixel-level and geometry-level constraints. To train our model, we collect a large-scale urban layout dataset covering more than 147 K regions under 1300 km $$^2$$ with rich annotations, including functions, region shapes, urban road layouts, and urban building layouts. We conduct extensive experiments to show that our model outperforms the baseline methods in generating practical and function-aware urban layouts based on the given target function and surrounding context.},
  archive      = {J_IJCV},
  author       = {Qin, Yiming and Zhao, Nanxuan and Yang, Jiale and Pan, Siyuan and Sheng, Bin and Lau, Rynson W. H.},
  doi          = {10.1007/s11263-024-02030-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3408-3427},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UrbanEvolver: Function-aware urban layout regeneration},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-language alignment learning under affinity and
divergence principles for few-shot out-of-distribution generalization.
<em>IJCV</em>, <em>132</em>(9), 3375–3407. (<a
href="https://doi.org/10.1007/s11263-024-02036-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in fine-tuning large-scale vision-language pre-trained models (VL-PTMs) have shown promising results in quick adaption to downstream tasks. However, prior research often lacks comprehensive investigation into out-of-distribution (OOD) generalization. Fine-tuning has a potential risk of overfitting, especially on few-shot OOD datasets when significant distribution shifts occur between the few-shot training examples and test sets. Previous research on fine-tuning’s robustness to distribution shifts does not consider different characteristics of distribution shifts and may not effectively handle noisy data with spurious correlations. To address these challenges, we propose the Vision-Language Alignment Learning under Affinity and Divergence Principles (VLAD) to adapt VL-PTMs to robust few-shot OOD generalization with theoretical guarantees. Built upon the large-scale pre-trained vision-language foundation model CLIP, we leverage frozen language embeddings as invariant anchors to protect against distribution shifts, while using adapter layers to fine-tune pre-trained visual features for improved vision-language alignment. Besides, we introduce affinity and divergence principles to further mitigate overfitting during the vision-language aligning process by increasing class discrimination and suppressing non-causal features. More importantly, we offer theoretical evidence highlighting the superiority of general language knowledge in achieving more robust OOD generalization performance. The tighter upper bound of the OOD generalization errors by the proposed regularization loss is also shown in theoretical analysis. Our approach is substantiated by extensive experiments and ablation studies on diverse datasets, validating our theoretical findings. The code is available at https://github.com/LinLLLL/VLAD .},
  archive      = {J_IJCV},
  author       = {Zhu, Lin and Yin, Weihan and Yang, Yiyao and Wu, Fan and Zeng, Zhaoyu and Gu, Qinying and Wang, Xinbing and Zhou, Chenghu and Ye, Nanyang},
  doi          = {10.1007/s11263-024-02036-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {3375-3407},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vision-language alignment learning under affinity and divergence principles for few-shot out-of-distribution generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Softmax-free linear transformers. <em>IJCV</em>,
<em>132</em>(8), 3355–3374. (<a
href="https://doi.org/10.1007/s11263-024-02035-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of SOftmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under low-rank matrix decomposition. For computational robustness, we estimate the Moore–Penrose inverse using an iterative Newton–Raphson method in the forward process only, while calculating its theoretical gradients only once in the backward process. To further expand applicability (e.g., dense prediction tasks), an efficient symmetric normalization technique is introduced. Extensive experiments on ImageNet, COCO and ADE20K show that our SOFT significantly improves the computational efficiency of existing ViT variants. With linear complexity, much longer token sequences are permitted by SOFT, resulting in superior trade-off between accuracy and complexity. Code and models are available at https://github.com/fudan-zvg/SOFT .},
  archive      = {J_IJCV},
  author       = {Lu, Jiachen and Zhang, Junge and Zhu, Xiatian and Feng, Jianfeng and Xiang, Tao and Zhang, Li},
  doi          = {10.1007/s11263-024-02035-5},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3355-3374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Softmax-free linear transformers},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-shot neural face reenactment via finding directions in
GAN’s latent space. <em>IJCV</em>, <em>132</em>(8), 3324–3354. (<a
href="https://doi.org/10.1007/s11263-024-02018-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present our framework for neural face/head reenactment whose goal is to transfer the 3D head orientation and expression of a target face to a source face. Previous methods focus on learning embedding networks for identity and head pose/expression disentanglement which proves to be a rather hard task, degrading the quality of the generated images. We take a different approach, bypassing the training of such networks, by using (fine-tuned) pre-trained GANs which have been shown capable of producing high-quality facial images. Because GANs are characterized by weak controllability, the core of our approach is a method to discover which directions in latent GAN space are responsible for controlling head pose and expression variations. We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, inherently captures disentangled directions for head pose, identity, and expression. Moreover, we show that by embedding real images in the GAN latent space, our method can be successfully used for the reenactment of real-world faces. Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment. Extensive qualitative and quantitative results show that our approach typically produces reenacted faces of notably higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1 &amp; 2.},
  archive      = {J_IJCV},
  author       = {Bounareli, Stella and Tzelepis, Christos and Argyriou, Vasileios and Patras, Ioannis and Tzimiropoulos, Georgios},
  doi          = {10.1007/s11263-024-02018-6},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3324-3354},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {One-shot neural face reenactment via finding directions in GAN’s latent space},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PL <span class="math display"><sub>1</sub></span> p:
Point-line minimal problems under partial visibility in three views.
<em>IJCV</em>, <em>132</em>(8), 3302–3323. (<a
href="https://doi.org/10.1007/s11263-024-01992-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a complete classification of minimal problems for generic arrangements of points and lines in space observed partially by three calibrated perspective cameras when each line is incident to at most one point. This is a large class of interesting minimal problems that allows missing observations in images due to occlusions and missed detections. There is an infinite number of such minimal problems; however, we show that they can be reduced to 140,616 equivalence classes by removing superfluous features and relabeling the cameras. We also introduce camera-minimal problems, which are practical for designing minimal solvers, and show how to pick a simplest camera-minimal problem for each minimal problem. This simplification results in 74,575 equivalence classes. Only 76 of these were known; the rest are new. To identify problems having potential for practical solving of image matching and 3D reconstruction, we present several natural subfamilies of camera-minimal problems as well as compute solution counts for all camera-minimal problems which have fewer than 300 solutions for generic data.},
  archive      = {J_IJCV},
  author       = {Duff, Timothy and Kohn, Kathlén and Leykin, Anton and Pajdla, Tomas},
  doi          = {10.1007/s11263-024-01992-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3302-3323},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PL $${}_{1}$$ p: Point-line minimal problems under partial visibility in three views},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning technique for human parsing: A survey and
outlook. <em>IJCV</em>, <em>132</em>(8), 3270–3301. (<a
href="https://doi.org/10.1007/s11263-024-02031-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human parsing aims to partition humans in image or video into multiple pixel-level semantic parts. In the last decade, it has gained significantly increased interest in the computer vision community and has been utilized in a broad range of practical applications, from security monitoring, to social media, to visual special effects, just to name a few. Although deep learning-based human parsing solutions have made remarkable achievements, many important concepts, existing challenges, and potential research directions are still confusing. In this survey, we comprehensively review three core sub-tasks: single human parsing, multiple human parsing, and video human parsing, by introducing their respective task settings, background concepts, relevant problems and applications, representative literature, and datasets. We also present quantitative performance comparisons of the reviewed methods on benchmark datasets. Additionally, to promote sustainable development of the community, we put forward a transformer-based human parsing framework, providing a high-performance baseline for follow-up research through universal, concise, and extensible solutions. Finally, we point out a set of under-investigated open issues in this field and suggest new directions for future study. We also provide a regularly updated project page, to continuously track recent developments in this fast-advancing field: https://github.com/soeaver/awesome-human-parsing .},
  archive      = {J_IJCV},
  author       = {Yang, Lu and Jia, Wenhe and Li, Shan and Song, Qing},
  doi          = {10.1007/s11263-024-02031-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3270-3301},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep learning technique for human parsing: A survey and outlook},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised point cloud representation learning by
clustering and neural rendering. <em>IJCV</em>, <em>132</em>(8),
3251–3269. (<a
href="https://doi.org/10.1007/s11263-024-02027-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has contributed to the rapid advancement of unsupervised learning on 3D point clouds. However, we argue that data augmentation is not ideal, as it requires a careful application-dependent selection of the types of augmentations to be performed, thus potentially biasing the information learned by the network during self-training. Moreover, several unsupervised methods only focus on uni-modal information, thus potentially introducing challenges in the case of sparse and textureless point clouds. To address these issues, we propose an augmentation-free unsupervised approach for point clouds, named CluRender, to learn transferable point-level features by leveraging uni-modal information for soft clustering and cross-modal information for neural rendering. Soft clustering enables self-training through a pseudo-label prediction task, where the affiliation of points to their clusters is used as a proxy under the constraint that these pseudo-labels divide the point cloud into approximate equal partitions. This allows us to formulate a clustering loss to minimize the standard cross-entropy between pseudo and predicted labels. Neural rendering generates photorealistic renderings from various viewpoints to transfer photometric cues from 2D images to the features. The consistency between rendered and real images is then measured to form a fitting loss, combined with the cross-entropy loss to self-train networks. Experiments on downstream applications, including 3D object detection, semantic segmentation, classification, part segmentation, and few-shot learning, demonstrate the effectiveness of our framework in outperforming state-of-the-art techniques.},
  archive      = {J_IJCV},
  author       = {Mei, Guofeng and Saltori, Cristiano and Ricci, Elisa and Sebe, Nicu and Wu, Qiang and Zhang, Jian and Poiesi, Fabio},
  doi          = {10.1007/s11263-024-02027-5},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3251-3269},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised point cloud representation learning by clustering and neural rendering},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multi-source predictor for zero-shot video object
segmentation. <em>IJCV</em>, <em>132</em>(8), 3232–3250. (<a
href="https://doi.org/10.1007/s11263-024-02024-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static and moving objects often occur in real-life videos. Most video object segmentation methods only focus on extracting and exploiting motion cues to perceive moving objects. Once faced with the frames of static objects, the moving object predictors may predict failed results caused by uncertain motion information, such as low-quality optical flow maps. Besides, different sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, existing approaches only consider either the RGB or RGB and optical flow. In this paper, we propose a novel adaptive multi-source predictor for zero-shot video object segmentation (ZVOS). In the static object predictor, the RGB source is converted to depth and static saliency sources, simultaneously. In the moving object predictor, we propose the multi-source fusion structure. First, the spatial importance of each source is highlighted with the help of the interoceptive spatial attention module (ISAM). Second, the motion-enhanced module (MEM) is designed to generate pure foreground motion attention for improving the representation of static and moving features in the decoder. Furthermore, we design a feature purification module (FPM) to filter the inter-source incompatible features. By using the ISAM, MEM and FPM, the multi-source features are effectively fused. In addition, we put forward an adaptive predictor fusion network (APF) to evaluate the quality of the optical flow map and fuse the predictions from the static object predictor and the moving object predictor in order to prevent over-reliance on the failed results caused by low-quality optical flow maps. Experiments show that the proposed model outperforms the state-of-the-art methods on three challenging ZVOS benchmarks. And, the static object predictor precisely predicts a high-quality depth map and static saliency map at the same time.},
  archive      = {J_IJCV},
  author       = {Zhao, Xiaoqi and Chang, Shijie and Pang, Youwei and Yang, Jiaxing and Zhang, Lihe and Lu, Huchuan},
  doi          = {10.1007/s11263-024-02024-8},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3232-3250},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive multi-source predictor for zero-shot video object segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open set recognition in real world. <em>IJCV</em>,
<em>132</em>(8), 3208–3231. (<a
href="https://doi.org/10.1007/s11263-024-02015-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition (OSR) constitutes a critical endeavor within the domain of computer vision, frequently deployed in applications, such as autonomous driving and medical imaging recognition. Existing OSR methodologies predominantly center on the acquisition of a profound association between image data and corresponding labels, facilitating the extraction of discriminative features instrumental for distinguishing novel categories. Nevertheless, real-world scenarios often introduce not only novel classes (referred to semantic shift) but also intricate environmental modifications that engender alterations in the distribution of established classes (termed as covariate shift). The latter phenomenon has the potential to undermine the robust correlation between images and labels established by conventional statistical correlation modeling approaches, consequently resulting in significant degradation of OSR performance. Causal correlation stands as the fundamental linkage between entities, routinely harnessed by humans to enhance their cognitive capacities for a more profound comprehension of the intricate world. With inspiration drawn from this perspective, our work herein introduces the causal inference-inspired open set recognition (CISOR) approach tailored for real-world OSR (RWOSR). CISOR represents the pioneering initiative to leverage the stability inherent in causal correlation to construct two pivotal modules: the covariate causal independence (CCI) module and the semantic causal uniqueness (SCU) module, both instrumental in addressing the RWOSR problem. The CCI module adeptly confronts the challenge of covariate shift by imposing constraints on the correlations between inter-class causal features. This strategy effectively mitigates the impact of spurious correlations between distinct categories on the generalization capacity of discriminative features. Furthermore, in order to counteract the issue of semantic shift, the SCU module harnesses correlations between causal features within the same class as constraints, thereby facilitating the extraction of resilient causal features endowed with superior discriminative capabilities. Empirical findings substantiate the superior efficacy of the proposed CIOSR method when compared to state-of-the-art approaches across diverse RWOSR benchmark datasets. The source code of this article will be available at https://github.com/yangzhen1252/RWOSR1 .},
  archive      = {J_IJCV},
  author       = {Yang, Zhen and Yue, Jun and Ghamisi, Pedram and Zhang, Shiliang and Ma, Jiayi and Fang, Leyuan},
  doi          = {10.1007/s11263-024-02015-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3208-3231},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Open set recognition in real world},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does confusion really hurt novel class discovery?
<em>IJCV</em>, <em>132</em>(8), 3191–3207. (<a
href="https://doi.org/10.1007/s11263-024-02012-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When sampling data of specific classes (i.e., known classes) for a scientific task, collectors may encounter unknown classes (i.e., novel classes). Since these novel classes might be valuable for future research, collectors will also sample them and assign them to several clusters with the help of known-class data. This assigning process is known as novel class discovery (NCD). However, category confusion is common in the sampling process and may make the NCD unreliable. To tackle this problem, this paper introduces a new and more realistic setting, where collectors may misidentify known classes and even confuse known classes with novel classes—we name it NCD under unreliable sampling (NUSA). We find that NUSA will empirically degrade existing NCD methods if taking no care of sampling errors. To handle NUSA, we propose an effective solution, named hidden-prototype-based discovery network (HPDN): (1) we try to obtain relatively clean data representations even with the confusedly sampled data; (2) we propose a mini-batch K-means variant for robust clustering, alleviating the negative impact of residual errors embedded in the representations by detaching the noisy supervision timely. Experiments demonstrate that, under NUSA, HPDN significantly outperforms competitive baselines (e.g., $$6\%$$ more than the best baseline on CIFAR-10) and remains robust when encountering serious sampling errors.},
  archive      = {J_IJCV},
  author       = {Chi, Haoang and Yang, Wenjing and Liu, Feng and Lan, Long and Qin, Tao and Han, Bo},
  doi          = {10.1007/s11263-024-02012-y},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3191-3207},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Does confusion really hurt novel class discovery?},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization with small data. <em>IJCV</em>,
<em>132</em>(8), 3172–3190. (<a
href="https://doi.org/10.1007/s11263-024-02028-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose to tackle the problem of domain generalization in the context of insufficient samples. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the distribution over distributions (i.e., the global perspective alignment) and the distribution-based contrastive semantic alignment (i.e., the local perspective alignment). Extensive experimental results on three challenging medical datasets show the effectiveness of our proposed method in the context of insufficient data compared with state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Chen, Kecheng and Gal, Elena and Yan, Hong and Li, Haoliang},
  doi          = {10.1007/s11263-024-02028-4},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3172-3190},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Domain generalization with small data},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on global LiDAR localization: Challenges, advances
and open problems. <em>IJCV</em>, <em>132</em>(8), 3139–3171. (<a
href="https://doi.org/10.1007/s11263-024-02019-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge about the own pose is key for all mobile robot applications. Thus pose estimation is part of the core functionalities of mobile robots. Over the last two decades, LiDAR scanners have become the standard sensor for robot localization and mapping. This article aims to provide an overview of recent progress and advancements in LiDAR-based global localization. We begin by formulating the problem and exploring the application scope. We then present a review of the methodology, including recent advancements in several topics, such as maps, descriptor extraction, and cross-robot localization. The contents of the article are organized under three themes. The first theme concerns the combination of global place retrieval and local pose estimation. The second theme is upgrading single-shot measurements to sequential ones for sequential global localization. Finally, the third theme focuses on extending single-robot global localization to cross-robot localization in multi-robot systems. We conclude the survey with a discussion of open challenges and promising directions in global LiDAR localization. To our best knowledge, this is the first comprehensive survey on global LiDAR localization for mobile robots.},
  archive      = {J_IJCV},
  author       = {Yin, Huan and Xu, Xuecheng and Lu, Sha and Chen, Xieyuanli and Xiong, Rong and Shen, Shaojie and Stachniss, Cyrill and Wang, Yue},
  doi          = {10.1007/s11263-024-02019-5},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3139-3171},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A survey on global LiDAR localization: Challenges, advances and open problems},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBNet: A plug-and-play network for segmentation-based scene
text detection. <em>IJCV</em>, <em>132</em>(8), 3119–3138. (<a
href="https://doi.org/10.1007/s11263-024-02022-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, segmentation-based methods are quite popular in scene text detection, which mainly contain two steps: text kernel segmentation and expansion. However, the segmentation process only considers each pixel independently, and the expansion process is difficult to achieve a favorable accuracy-speed trade-off. In this paper, we propose a context-aware and boundary-guided network (CBN) to tackle these problems. In CBN, a basic text detector is first used to predict initial segmentation results. Then, we propose a context-aware module to enhance text kernel feature representations, which considers both global and local contexts. Finally, we introduce a boundary-guided module to expand enhanced text kernels adaptively with only the pixels on the contours, which not only obtains accurate text boundaries but also keeps high speed, especially on high-resolution output maps. In particular, with a lightweight backbone, the basic detector equipped with our proposed CBN achieves state-of-the-art results on several popular benchmarks, and our proposed CBN can be plugged into several segmentation-based methods. Code will be available on https://github.com/XiiZhao/cbn.pytorch .},
  archive      = {J_IJCV},
  author       = {Zhao, Xi and Feng, Wei and Zhang, Zheng and Lv, Jingjing and Zhu, Xin and Lin, Zhangang and Hu, Jinghe and Shao, Jingping},
  doi          = {10.1007/s11263-024-02022-w},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3119-3138},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CBNet: A plug-and-play network for segmentation-based scene text detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated detection of cat facial landmarks. <em>IJCV</em>,
<em>132</em>(8), 3103–3118. (<a
href="https://doi.org/10.1007/s11263-024-02006-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of animal affective computing is rapidly emerging, and analysis of facial expressions is a crucial aspect. One of the most significant challenges that researchers in the field currently face is the scarcity of high-quality, comprehensive datasets that allow the development of models for facial expressions analysis. One of the possible approaches is the utilisation of facial landmarks, which has been shown for humans and animals. In this paper we present a novel dataset of cat facial images annotated with bounding boxes and 48 facial landmarks grounded in cat facial anatomy. We also introduce a landmark detection convolution neural network-based model which uses a magnifying ensemble method. Our model shows excellent performance on cat faces and is generalizable to human and other animals facial landmark detection.},
  archive      = {J_IJCV},
  author       = {Martvel, George and Shimshoni, Ilan and Zamansky, Anna},
  doi          = {10.1007/s11263-024-02006-w},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3103-3118},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Automated detection of cat facial landmarks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PanAf20K: A large video dataset for wild ape detection and
behaviour recognition. <em>IJCV</em>, <em>132</em>(8), 3086–3102. (<a
href="https://doi.org/10.1007/s11263-024-02003-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across $$\sim $$ 20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts. The dataset and code are available from the project website: PanAf20K},
  archive      = {J_IJCV},
  author       = {Brookes, Otto and Mirmehdi, Majid and Stephens, Colleen and Angedakin, Samuel and Corogenes, Katherine and Dowd, Dervla and Dieguez, Paula and Hicks, Thurston C. and Jones, Sorrel and Lee, Kevin and Leinert, Vera and Lapuente, Juan and McCarthy, Maureen S. and Meier, Amelia and Murai, Mizuki and Normand, Emmanuelle and Vergnes, Virginie and Wessling, Erin G. and Wittig, Roman M. and Langergraber, Kevin and Maldonado, Nuria and Yang, Xinyu and Zuberbühler, Klaus and Boesch, Christophe and Arandjelovic, Mimi and Kühl, Hjalmar and Burghardt, Tilo},
  doi          = {10.1007/s11263-024-02003-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3086-3102},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PanAf20K: A large video dataset for wild ape detection and behaviour recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal fusion and progressive decoding network for
RGB-d salient object detection. <em>IJCV</em>, <em>132</em>(8),
3067–3085. (<a
href="https://doi.org/10.1007/s11263-024-02020-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detection (SOD) methods tend to achieve higher performance by integrating additional modules, such as feature enhancement and edge generation. There is no doubt that these modules will inevitably produce feature redundancy and performance degradation. To this end, we exquisitely design a cross-modal fusion and progressive decoding network (termed CPNet) to achieve RGB-D SOD tasks. The designed network structure only includes three indispensable parts: feature encoding, feature fusion and feature decoding. Specifically, in the feature encoding part, we adopt a two-stream Swin Transformer encoder to extract multi-level and multi-scale features from RGB images and depth images respectively to model global information. In the feature fusion part, we design a cross-modal attention fusion module, which can leverage the attention mechanism to fuse multi-modality and multi-level features. In the feature decoding part, we design a progressive decoder to gradually fuse low-level features and filter noise information to accurately predict salient objects. Extensive experimental results on 6 benchmarks demonstrated that our network surpasses 12 state-of-the-art methods in terms of four metrics. In addition, it is also verified that for the RGB-D SOD task, the addition of the feature enhancement module and the edge generation module is not conducive to improving the detection performance under this framework, which provides new insights into the salient object detection task. Our codes are available at https://github.com/hu-xh/CPNet .},
  archive      = {J_IJCV},
  author       = {Hu, Xihang and Sun, Fuming and Sun, Jing and Wang, Fasheng and Li, Haojie},
  doi          = {10.1007/s11263-024-02020-y},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3067-3085},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cross-modal fusion and progressive decoding network for RGB-D salient object detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty modeling for group re-identification.
<em>IJCV</em>, <em>132</em>(8), 3046–3066. (<a
href="https://doi.org/10.1007/s11263-024-02013-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group re-identification (GReID) aims to correctly associate images containing the same group members captured with non-overlapping camera networks, which has important applications in video surveillance. Unlike the person re-identification, the unique challenge of GReID lies in variations of group structure, including the number and layout of members. Current methods use certainty modeling, in which the specific group structure presented in each image is considered. However, certainty modeling can only describe finite group structures and shows poor generalization for unseen group structures, i.e., group variations that do not exist in the training set. In this paper, we propose a methodology called uncertainty modeling, which excavates near-infinite group structures from finite samples by simulating variations in both number and layout. Specifically, member uncertainty treats the number of intra-group members as a truncated Gaussian distribution instead of a fixed value and then simulates member variations by dynamic sampling. Layout uncertainty constructs random affine transformations about the positions of members to enlarge the fixed schemes in the training set. To implement the proposed methodology, we technically propose an Uncertainty-Modeling Second-Order Transformer (UMSOT) that extracts a first-order token for each member and further uses these tokens to learn a second-order token as a group feature. The UMSOT exploits the structural advantages of the transformer to explicitly extract layout features and efficiently integrate appearance and layout features, which are hardly achievable by current CNN- and GNN-based methods. Comprehensive experiments on four datasets (CSG, SYSUGroup, RoadGroup, and iLIDS-MCTS), fully demonstrate the superiority of the proposed method, which surprisingly outperforms the state-of-the-art method by 30.4% in Rank1 on the CSG dataset. https://github.com/LinlyAC/UMSOT .},
  archive      = {J_IJCV},
  author       = {Zhang, Quan and Lai, Jianhuang and Feng, Zhanxiang and Xie, Xiaohua},
  doi          = {10.1007/s11263-024-02013-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3046-3066},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Uncertainty modeling for group re-identification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SplatFlow: Learning multi-frame optical flow via splatting.
<em>IJCV</em>, <em>132</em>(8), 3023–3045. (<a
href="https://doi.org/10.1007/s11263-024-01993-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occlusion problem remains a crucial challenge in optical flow estimation (OFE). Despite the recent significant progress brought about by deep learning, most existing deep learning OFE methods still struggle to handle occlusions; in particular, those based on two frames cannot correctly handle occlusions because occluded regions have no visual correspondences. However, there is still hope in multi-frame settings, which can potentially mitigate the occlusion issue in OFE. Unfortunately, multi-frame OFE (MOFE) remains underexplored, and the limited studies on it are mainly specially designed for pyramid backbones or else obtain the aligned previous frame’s features, such as correlation volume and optical flow, through time-consuming backward flow calculation or non-differentiable forward warping transformation. This study proposes an efficient MOFE framework named SplatFlow to address these shortcomings. SplatFlow introduces the differentiable splatting transformation to align the previous frame’s motion feature and designs a Final-to-All embedding method to input the aligned motion feature into the current frame’s estimation, thus remodeling the existing two-frame backbones. The proposed SplatFlow is efficient yet more accurate, as it can handle occlusions properly. Extensive experimental evaluations show that SplatFlow substantially outperforms all published methods on the KITTI2015 and Sintel benchmarks. Especially on the Sintel benchmark, SplatFlow achieves errors of 1.12 (clean pass) and 2.07 (final pass), with surprisingly significant 19.4% and 16.2% error reductions, respectively, from the previous best results submitted. The code for SplatFlow is available at https://github.com/wwsource/SplatFlow .},
  archive      = {J_IJCV},
  author       = {Wang, Bo and Zhang, Yifan and Li, Jian and Yu, Yang and Sun, Zhenping and Liu, Li and Hu, Dewen},
  doi          = {10.1007/s11263-024-01993-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {3023-3045},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SplatFlow: Learning multi-frame optical flow via splatting},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on adaptive cameras. <em>IJCV</em>,
<em>132</em>(8), 2989–3022. (<a
href="https://doi.org/10.1007/s11263-024-02025-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper surveys adaptive cameras, i.e. any camera device able to change its geometric settings. We consider their classification in four categories: lensless, dioptric, catadioptric and polydioptric cameras. In each category, we report and describe all the existing adaptive cameras. Then, the known applications of these devices are summarized. Finally, we discuss open research lines for new adaptations of cameras, and their promising uses.},
  archive      = {J_IJCV},
  author       = {Ducrocq, Julien and Caron, Guillaume},
  doi          = {10.1007/s11263-024-02025-7},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2989-3022},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A survey on adaptive cameras},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of near-instance-level attribute bottleneck for
zero-shot learning. <em>IJCV</em>, <em>132</em>(8), 2962–2988. (<a
href="https://doi.org/10.1007/s11263-024-02021-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) involves transferring knowledge from seen classes to unseen classes by establishing connections between visual and semantic spaces. Traditional ZSL methods identify novel classes by class-level attribute vectors, which implies an information bottleneck. These approaches often use class-level attribute vectors as the fitting target during training, disregarding the individual variations within a class. Moreover, the attributes used for training lack location information and are prone to mismatch with local regions of visual features. To this end, we introduce a Near-Instance-Level Attribute Bottleneck (IAB) to alter class-level attribute vectors as well as visual features throughout the training phase to better reflect their naturalistic correspondences. Specifically, our Near-Instance-Wise Attribute Adaptation (NAA) modifies class attribute vectors to obtain multiple attribute basis vectors, generating a subspace that is more relevant to instance-level samples. Additionally, our Vision Attribute Relation Strengthening (VARS) module searches for attribute-related regions within the features, offering additional location information during the training phase. The proposed method is evaluated on four ZSL benchmarks, revealing that it is superior or competitive to the state-of-the-art methods on ZSL and the more challenging Generalized Zero-Shot Learning (GZSL) settings. Extensive experiments corroborate the sustainability of this study as one of the most potential directions for ZSL, i.e., the effectiveness of enhancing the visual-semantic relationships formed during training using a simple model structure. Code is available at: https://github.com/LanchJL/IAB-GZSL .},
  archive      = {J_IJCV},
  author       = {Jiang, Chenyi and Shen, Yuming and Chen, Dubing and Zhang, Haofeng and Shao, Ling and Torr, Philip H. S.},
  doi          = {10.1007/s11263-024-02021-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2962-2988},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Estimation of near-instance-level attribute bottleneck for zero-shot learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based non-rigid reconstruction of low-rank
parametrized deformations from contours. <em>IJCV</em>, <em>132</em>(8),
2943–2961. (<a
href="https://doi.org/10.1007/s11263-024-02011-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reconstruction of fast non-rigid object deformations over time is a challenge for conventional frame-based cameras. In recent years, event cameras have gained significant attention due to their bio-inspired properties, such as high temporal resolution and high dynamic range. In this paper, we propose a novel approach for reconstructing such deformations using event measurements. Under the assumption of a static background, where all events are generated by the motion, our approach estimates the deformation of objects from events generated at the object contour in a probabilistic optimization framework. It associates events to mesh faces on the contour and maximizes the alignment of the line of sight through the event pixel with the associated face. In experiments on synthetic and real data of human body motion, we demonstrate the advantages of our method over state-of-the-art optimization and learning-based approaches for reconstructing the motion of human arms and hands. In addition, we propose an efficient event stream simulator to synthesize realistic event data for human motion.},
  archive      = {J_IJCV},
  author       = {Xue, Yuxuan and Li, Haolong and Leutenegger, Stefan and Stückler, Jörg},
  doi          = {10.1007/s11263-024-02011-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2943-2961},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Event-based non-rigid reconstruction of low-rank parametrized deformations from contours},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training object detectors from scratch: An empirical study
in the era of vision transformer. <em>IJCV</em>, <em>132</em>(8),
2929–2942. (<a
href="https://doi.org/10.1007/s11263-024-01988-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performance of self-attention mechanism in the language field, transformers tailored for visual data have drawn significant attention and triumphed over CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the “pre-train and fine-tune” paradigm of vision transformer and train transformer based object detector from scratch. Some earlier works in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to a vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights to help other researchers and practitioners, and inspire more interesting research in other fields, such as remote sensing, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on the MS COCO dataset demonstrate that vision transformer based detectors trained from scratch can also achieve similar performance to their counterparts with ImageNet pre-training.},
  archive      = {J_IJCV},
  author       = {Hong, Weixiang and Ren, Wang and Lao, Jiangwei and Xie, Lele and Zhong, Liheng and Wang, Jian and Chen, Jingdong and Liu, Honghai and Chu, Wei},
  doi          = {10.1007/s11263-024-01988-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2929-2942},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Training object detectors from scratch: An empirical study in the era of vision transformer},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust heterogeneous model fitting for multi-source image
correspondences. <em>IJCV</em>, <em>132</em>(8), 2907–2928. (<a
href="https://doi.org/10.1007/s11263-024-02023-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional feature detection and description methods, such as scale-invariant feature transform, are susceptible to nonlinear radiation distortions (NRDs) and geometric distortions (GDs), which in turn generate a large number of outliers or incorrect correspondences. To address this issue, this paper proposes a simple yet effective heterogeneous model fitting (MIMF) for multi-source image correspondences. First, a multi-orientation phase consistency model is constructed, which fuses phase consistency, image amplitude and orientation to detect the correct correspondences of feature points. This model effectively reduces the influence of NRDs. Second, sub-region grids and orientation histograms are exploited to construct the log-polar descriptors with variable-size bins, which are robust to GDs. Finally, a heterogeneous model fitting method is proposed, which can effectively estimate the parameters of the transformation model for alleviating the influence of outliers. Experiments are performed on six public datasets and one constructed dataset containing ten types of multi-source images, and the experimental results show that the proposed MIMF method outperforms several state-of-the-art competing methods in terms of matching performance.},
  archive      = {J_IJCV},
  author       = {Lin, Shuyuan and Huang, Feiran and Lai, Taotao and Lai, Jianhuang and Wang, Hanzi and Weng, Jian},
  doi          = {10.1007/s11263-024-02023-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2907-2928},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust heterogeneous model fitting for multi-source image correspondences},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FunnyNet-w: Multimodal learning of funny moments in videos
in the wild. <em>IJCV</em>, <em>132</em>(8), 2885–2906. (<a
href="https://doi.org/10.1007/s11263-024-02000-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W’s ability to predict funny moments in the wild. FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information.},
  archive      = {J_IJCV},
  author       = {Liu, Zhi-Song and Courant, Robin and Kalogeiton, Vicky},
  doi          = {10.1007/s11263-024-02000-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2885-2906},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FunnyNet-W: Multimodal learning of funny moments in videos in the wild},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to generalize over subpartitions for
heterogeneity-aware domain adaptive nuclei segmentation. <em>IJCV</em>,
<em>132</em>(8), 2861–2884. (<a
href="https://doi.org/10.1007/s11263-024-02004-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation scarcity and cross-modality/stain data distribution shifts are two major obstacles hindering the application of deep learning models for nuclei analysis, which holds a broad spectrum of potential applications in digital pathology. Recently, unsupervised domain adaptation (UDA) methods have been proposed to mitigate the distributional gap between different imaging modalities for unsupervised nuclei segmentation in histopathology images. However, existing UDA methods are built upon the assumption that data distributions within each domain should be uniform. Based on the over-simplified supposition, they propose to align the histopathology target domain with the source domain integrally, neglecting severe intra-domain discrepancy over subpartitions incurred by mixed cancer types and sampling organs. In this paper, for the first time, we propose to explicitly consider the heterogeneity within the histopathology domain and introduce open compound domain adaptation (OCDA) to resolve the crux. In specific, a two-stage disentanglement framework is proposed to acquire domain-invariant feature representations at both image and instance levels. The holistic design addresses the limitations of existing OCDA approaches which struggle to capture instance-wise variations. Two regularization strategies are specifically devised herein to leverage the rich subpartition-specific characteristics in histopathology images and facilitate subdomain decomposition. Moreover, we propose a dual-branch nucleus shape and structure preserving module to prevent nucleus over-generation and deformation in the synthesized images. Experimental results on both cross-modality and cross-stain scenarios over a broad range of diverse datasets demonstrate the superiority of our method compared with state-of-the-art UDA and OCDA methods.},
  archive      = {J_IJCV},
  author       = {Fan, Jianan and Liu, Dongnan and Chang, Hang and Cai, Weidong},
  doi          = {10.1007/s11263-024-02004-y},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2861-2884},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to generalize over subpartitions for heterogeneity-aware domain adaptive nuclei segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniMod1K: Towards a more universal large-scale dataset and
benchmark for multi-modal learning. <em>IJCV</em>, <em>132</em>(8),
2845–2860. (<a
href="https://doi.org/10.1007/s11263-024-01999-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of large-scale high-quality datasets has stimulated the rapid development of deep learning in recent years. However, most computer vision tasks focus on the visual modality only, resulting in a huge imbalance in the number of annotated data for other modalities. While several multi-modal datasets have been made available, the majority of them are confined to only two modalities, serving a single specific computer vision task. To redress the data deficiency for multi-modal learning and applications, a new dataset named UniMod1K is presented in this work. UniMod1K involves three data modalities: vision, depth, and language. For the vision and depth modalities, the UniMod1K dataset contains 1050 RGB-D sequences, comprising a total of some 2.5 million frames. Regarding the language modality, the proposed dataset includes 1050 sentences describing the target object in each video. To demonstrate the advantages of training on a larger multi-modal dataset, such as UniMod1K, and to stimulate research enabled by the dataset, we address several multi-modal tasks, namely multi-modal object tracking and monocular depth estimation. To establish a performance baseline, we propose novel baseline methods for RGB-D object tracking, vision-language tracking and vision-depth-language tracking. Additionally, we conduct comprehensive experiments for each of these tasks. The results highlight the potential of the UniMod1K dataset to improve the performance of multi-modal approaches. The dataset and codes can be accessed at https://github.com/xuefeng-zhu5/UniMod1K .},
  archive      = {J_IJCV},
  author       = {Zhu, Xue-Feng and Xu, Tianyang and Liu, Zongtao and Tang, Zhangyong and Wu, Xiao-Jun and Kittler, Josef},
  doi          = {10.1007/s11263-024-01999-8},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2845-2860},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UniMod1K: Towards a more universal large-scale dataset and benchmark for multi-modal learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-aligned matching for enhanced DETR convergence and
multi-scale feature fusion. <em>IJCV</em>, <em>132</em>(8), 2825–2844.
(<a href="https://doi.org/10.1007/s11263-024-02005-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed DEtection TRansformer (DETR) has established a fully end-to-end paradigm for object detection. However, DETR suffers from slow training convergence, which hinders its applicability to various detection tasks. We observe that DETR’s slow convergence is largely attributed to the difficulty in matching object queries to relevant regions due to the unaligned semantics between object queries and encoded image features. With this observation, we design Semantic-Aligned-Matching DETR++ (SAM-DETR++) to accelerate DETR’s convergence and improve detection performance. The core of SAM-DETR++ is a plug-and-play module that projects object queries and encoded image features into the same feature embedding space, where each object query can be easily matched to relevant regions with similar semantics. Besides, SAM-DETR++ searches for multiple representative keypoints and exploits their features for semantic-aligned matching with enhanced representation capacity. Furthermore, SAM-DETR++ can effectively fuse multi-scale features in a coarse-to-fine manner on the basis of the designed semantic-aligned matching. Extensive experiments show that the proposed SAM-DETR++ achieves superior convergence speed and competitive detection accuracy. Additionally, as a plug-and-play method, SAM-DETR++ can complement existing DETR convergence solutions with even better performance, achieving 44.8% AP with merely 12 training epochs and 49.1% AP with 50 training epochs on COCO val 2017 with ResNet-50. Codes are available at https://github.com/ZhangGongjie/SAM-DETR .},
  archive      = {J_IJCV},
  author       = {Zhang, Gongjie and Luo, Zhipeng and Huang, Jiaxing and Lu, Shijian and Xing, Eric P.},
  doi          = {10.1007/s11263-024-02005-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2825-2844},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-aligned matching for enhanced DETR convergence and multi-scale feature fusion},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-architecture knowledge distillation. <em>IJCV</em>,
<em>132</em>(8), 2798–2824. (<a
href="https://doi.org/10.1007/s11263-024-02002-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer network architecture has gained attention due to its ability to learn global relations and its superior performance. To boost performance, it is natural to distill complementary knowledge from a Transformer network to a convolutional neural network (CNN). However, most existing knowledge distillation methods only consider homologous-architecture distillation, which may not be suitable for cross-architecture scenarios, such as from Transformer to CNN. To address this problem, we analyze the globality and transferability of models, which reflect the ability to capture global knowledge and transfer knowledge from teacher to student, respectively. Inspired by our observations, a novel cross-architecture knowledge distillation method is proposed, which supports bi-directional distillation including from Transformer to CNN and from CNN to Transformer. Specifically, rather than directly mimicking the output and intermediate features of the teacher, a partial cross-attention projector (PCA/iPCA) and a group-wise linear projector (GL/iGL) are introduced to align the student features with the teacher’s in two projected feature spaces. To better match the teacher’s knowledge with the student’s knowledge, an adaptive distillation router (ADR) is presented to decide the knowledge from which layer the teacher should be distilled to guide which layer of the student. A multi-view robust training scheme is further presented, to improve the robustness of the framework for distillation. Extensive experiments show that the proposed method outperforms 17 state-of-the-art methods on both small-scale and large-scale datasets.},
  archive      = {J_IJCV},
  author       = {Liu, Yufan and Cao, Jiajiong and Li, Bing and Hu, Weiming and Ding, Jingting and Li, Liang and Maybank, Stephen},
  doi          = {10.1007/s11263-024-02002-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2798-2824},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cross-architecture knowledge distillation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hugs bring double benefits: Unsupervised cross-modal hashing
with multi-granularity aligned transformers. <em>IJCV</em>,
<em>132</em>(8), 2765–2797. (<a
href="https://doi.org/10.1007/s11263-024-02009-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-modal hashing (UCMH) has been commonly explored to support large-scale cross-modal retrieval of unlabeled data. Despite promising progress, most existing approaches are developed on convolutional neural network and multilayer perceptron architectures, sacrificing the quality of hash codes due to limited capacity for excavating multi-modal semantics. To pursue better content understanding, we break this convention for UCMH and delve into a transformer-based paradigm. Unlike naïve adaptations via backbone substitution that overlook the heterogeneous semantics from transformers, we propose a multi-granularity learning framework called hugging to bridge the modality gap. Specifically, we first construct a fine-grained semantic space composed of a series of aggregated local embeddings that capture implicit attribute-level semantics. In the hash learning stage, we innovatively incorporate fine-grained alignment with these local embeddings to enhance global hash code alignment. Notably, this fine-grained alignment only facilitates robust cross-modal learning without complicating global hash code generation at test time, thus fully maintaining the high efficiency of hash-based retrieval. To make the most of fine-grained information, we further propose a differentiable optimized quantization algorithm and extend our framework to hugging $$^+$$ . This variant neatly integrates quantization learning into the fine-grained alignment during training, producing quantization codes of local embeddings as a gift at test time, which can augment the retrieval performance through an efficient reranking stage. We instantiate simple baselines with contrastive learning objectives for hugging and hugging $$^+$$ , namely HuggingHash and HuggingHash $$^+$$ . Extensive experiments on 4 text-image retrieval and 2 text-video retrieval benchmark datasets show the competitive performance of HuggingHash and HuggingHash $$^+$$ against state-of-the-art baselines. More encouragingly, we also validate that hugging and hugging $$^+$$ are flexible and effective across various baselines, suggesting their universal applicability in the realm of UCMH.},
  archive      = {J_IJCV},
  author       = {Wang, Jinpeng and Zeng, Ziyun and Chen, Bin and Wang, Yuting and Liao, Dongliang and Li, Gongfu and Wang, Yiru and Xia, Shu-Tao},
  doi          = {10.1007/s11263-024-02009-7},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2765-2797},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hugs bring double benefits: Unsupervised cross-modal hashing with multi-granularity aligned transformers},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Annotation-free human sketch quality assessment.
<em>IJCV</em>, <em>132</em>(8), 2743–2764. (<a
href="https://doi.org/10.1007/s11263-024-02001-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As lovely as bunnies are, your sketched version would probably not do them justice (Fig. 1). This paper recognises this very problem and studies sketch quality assessment for the first time—letting you find these badly drawn ones. Our key discovery lies in exploiting the magnitude ( $$L_2$$ norm) of a sketch feature as a quantitative quality metric. We propose Geometry-Aware Classification Layer (GACL), a generic method that makes feature-magnitude-as-quality-metric possible and importantly does it without the need for specific quality annotations from humans. GACL sees feature magnitude and recognisability learning as a dual task, which can be simultaneously optimised under a neat cross-entropy classification loss with theoretic guarantee. This gives GACL a nice geometric interpretation (the better the quality, the easier the recognition), and makes it agnostic to both network architecture changes and the underlying sketch representation. Through a large scale human study of 160,000 trials, we confirm the agreement between our GACL-induced metric and human quality perception. We further demonstrate how such a quality assessment capability can for the first time enable three practical sketch applications. Interestingly, we show GACL not only works on abstract visual representations such as sketch but also extends well to natural images on the problem of image quality assessment (IQA). Last but not least, we spell out the general properties of GACL as general-purpose data re-weighting strategy and demonstrate its applications in vertical problems such as noisy label cleansing. Code will be made publicly available at https://github.com/yanglan0225/SketchX-Quantifying-Sketch-Quality .},
  archive      = {J_IJCV},
  author       = {Yang, Lan and Pang, Kaiyue and Zhang, Honggang and Song, Yi-Zhe},
  doi          = {10.1007/s11263-024-02001-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2743-2764},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Annotation-free human sketch quality assessment},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new dataset and a distractor-aware architecture for
transparent object tracking. <em>IJCV</em>, <em>132</em>(8), 2729–2742.
(<a href="https://doi.org/10.1007/s11263-024-02010-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.},
  archive      = {J_IJCV},
  author       = {Lukežič, Alan and Trojer, Žiga and Matas, Jiří and Kristan, Matej},
  doi          = {10.1007/s11263-024-02010-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2729-2742},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A new dataset and a distractor-aware architecture for transparent object tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReliTalk: Relightable talking portrait generation from a
single video. <em>IJCV</em>, <em>132</em>(8), 2713–2728. (<a
href="https://doi.org/10.1007/s11263-024-02007-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait’s reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. Our code is released in ( https://github.com/arthur-qiu/ReliTalk ).},
  archive      = {J_IJCV},
  author       = {Qiu, Haonan and Chen, Zhaoxi and Jiang, Yuming and Zhou, Hang and Fan, Xiangyu and Yang, Lei and Wu, Wayne and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02007-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {2713-2728},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ReliTalk: Relightable talking portrait generation from a single video},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning adaptive spatio-temporal inference transformer for
coarse-to-fine animal visual tracking: Algorithm and benchmark.
<em>IJCV</em>, <em>132</em>(7), 2698–2712. (<a
href="https://doi.org/10.1007/s11263-024-02008-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced general visual object tracking models have been drastically developed with the access of large annotated datasets and progressive network architectures. However, a general tracker always suffers domain shift when directly adopting to specific testing scenarios. In this paper, we dedicate to addressing the animal tracking problem by proposing a spatio-temporal inference module and a coarse-to-fine tracking strategy. In terms of tracking animals, non-rigid deformation is a typical challenge. Therefore, we particularly design a novel transformer-based inference structure where the changing animal state is transmitted across continuous frames. By explicitly transmitting the appearance variations, this spatio-temporal module enables adaptive target learning, boosting the animal tracking performance compared to the fixed template matching approaches. Besides, considering the altered contours of animals in different frames, we propose to perform coarse-to-fine tracking to obtain a fine-grained animal bounding box with a dedicated distribution-aware regression module. The coarse tracking phase focuses on distinguishing the target against potential distractors in the background. While the fine-grained tracking phase aims at accurately regressing the final animal bounding box. To facilitate animal tracking evaluation, we captured and annotated 145 video sequences with 20 categories from the zoo, forming a new test set for animal tracking, coined ZOO145. We also collected a dataset, AnimalSOT, with 162 video sequences from existing tracking test benchmarks. The experimental performance on animal tracking datasets, MoCA, ZOO145, and AnimalSOT, demonstrate the merit of the proposed approach against advanced general tracking approaches, providing a baseline for future animal tracking studies.},
  archive      = {J_IJCV},
  author       = {Xu, Tianyang and Kang, Ze and Zhu, Xuefeng and Wu, Xiao-Jun},
  doi          = {10.1007/s11263-024-02008-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2698-2712},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning adaptive spatio-temporal inference transformer for coarse-to-fine animal visual tracking: Algorithm and benchmark},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking the robustness of LiDAR semantic segmentation
models. <em>IJCV</em>, <em>132</em>(7), 2674–2697. (<a
href="https://doi.org/10.1007/s11263-024-01991-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using LiDAR semantic segmentation models for safety-critical applications such as autonomous driving, it is essential to understand and improve their robustness with respect to a large range of LiDAR corruptions. In this paper, we aim to comprehensively analyze the robustness of LiDAR semantic segmentation models under various corruptions. To rigorously evaluate the robustness and generalizability of current approaches, we propose a new benchmark, including two corruption datasets SemanticKITTI-C and SemanticPOSS-C, which feature 16 out-of-domain LiDAR corruptions in three groups, namely adverse weather, measurement noise and cross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic segmentation models, especially spanning different input representations (e.g., point clouds, voxels, projected images, and etc.), network architectures and training schemes. Through this study, we obtain two insights: (1) We find out that the input representation plays a crucial role in robustness. Specifically, under specific corruptions, different representations perform variously. (2) Although state-of-the-art methods on LiDAR semantic segmentation achieve promising results on clean data, they are less robust when dealing with noisy data. Finally, based on the above observations, we design a robust LiDAR segmentation model (RLSeg) which greatly boosts the robustness with simple but effective modifications. It is promising that our benchmark, comprehensive analysis, and observations can boost future research in robust LiDAR semantic segmentation for safety-critical applications.},
  archive      = {J_IJCV},
  author       = {Yan, Xu and Zheng, Chaoda and Xue, Ying and Li, Zhen and Cui, Shuguang and Dai, Dengxin},
  doi          = {10.1007/s11263-024-01991-2},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2674-2697},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking the robustness of LiDAR semantic segmentation models},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are multi-view edges incomplete for depth estimation?
<em>IJCV</em>, <em>132</em>(7), 2639–2673. (<a
href="https://doi.org/10.1007/s11263-023-01890-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation tries to obtain 3D scene geometry from low-dimensional data like 2D images. This is a vital operation in computer vision and any general solution must preserve all depth information of potential relevance to support higher-level tasks. For scenes with well-defined depth, this work shows that multi-view edges can encode all relevant information—that multi-view edges are complete. For this, we follow Elder’s complementary work on the completeness of 2D edges for image reconstruction. We deploy an image-space geometric representation: an encoding of multi-view scene edges as constraints and a diffusion reconstruction method for inverting this code into depth maps. Due to inaccurate constraints, diffusion-based methods have previously underperformed against deep learning methods; however, we will reassess the value of diffusion-based methods and show their competitiveness without requiring training data. To begin, we work with structured light fields and epipolar plane images (EPIs). EPIs present high-gradient edges in the angular domain: with correct processing, EPIs provide depth constraints with accurate occlusion boundaries and view consistency. Then, we present a differentiable representation form that allows the constraints and the diffusion reconstruction to be optimized in an unsupervised way via a multi-view reconstruction loss. This is based around point splatting via radiative transport, and extends to unstructured multi-view images. We evaluate our reconstructions for accuracy, occlusion handling, view consistency, and sparsity to show that they retain the geometric information required for higher-level tasks.},
  archive      = {J_IJCV},
  author       = {Khan, Numair and Kim, Min H. and Tompkin, James},
  doi          = {10.1007/s11263-023-01890-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2639-2673},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Are multi-view edges incomplete for depth estimation?},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relative norm alignment for tackling domain shift in deep
multi-modal classification. <em>IJCV</em>, <em>132</em>(7), 2618–2638.
(<a href="https://doi.org/10.1007/s11263-024-01998-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal learning has gained significant attention due to its ability to enhance machine learning algorithms. However, it brings challenges related to modality heterogeneity and domain shift. In this work, we address these challenges by proposing a new approach called Relative Norm Alignment (RNA) loss. RNA loss exploits the observation that variations in marginal distributions between modalities manifest as discrepancies in their mean feature norms, and rebalances feature norms across domains, modalities, and classes. This rebalancing improves the accuracy of models on test data from unseen (“target”) distributions. In the context of Unsupervised Domain Adaptation (UDA), we use unlabeled target data to enhance feature transferability. We achieve this by combining RNA loss with an adversarial domain loss and an Information Maximization term that regularizes predictions on target data. We present a comprehensive analysis and ablation of our method for both Domain Generalization and UDA settings, testing our approach on different modalities for tasks such as first and third person action recognition, object recognition, and fatigue detection. Experimental results show that our approach achieves competitive or state-of-the-art performance on the proposed benchmarks, showing the versatility and effectiveness of our method in a wide range of applications.},
  archive      = {J_IJCV},
  author       = {Planamente, Mirco and Plizzari, Chiara and Peirone, Simone Alberto and Caputo, Barbara and Bottino, Andrea},
  doi          = {10.1007/s11263-024-01998-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2618-2638},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Relative norm alignment for tackling domain shift in deep multi-modal classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focus for free in density-based counting. <em>IJCV</em>,
<em>132</em>(7), 2600–2617. (<a
href="https://doi.org/10.1007/s11263-024-01990-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Shi, Zenglin and Mettes, Pascal and Snoek, Cees G. M.},
  doi          = {10.1007/s11263-024-01990-3},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2600-2617},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Focus for free in density-based counting},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HVDistill: Transferring knowledge from images to point
clouds via unsupervised hybrid-view distillation. <em>IJCV</em>,
<em>132</em>(7), 2585–2599. (<a
href="https://doi.org/10.1007/s11263-023-01981-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhang, Sha and Deng, Jiajun and Bai, Lei and Li, Houqiang and Ouyang, Wanli and Zhang, Yanyong},
  doi          = {10.1007/s11263-023-01981-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2585-2599},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HVDistill: Transferring knowledge from images to point clouds via unsupervised hybrid-view distillation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning based prediction of pulmonary hypertension in
newborns using echocardiograms. <em>IJCV</em>, <em>132</em>(7),
2567–2584. (<a
href="https://doi.org/10.1007/s11263-024-01996-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary hypertension (PH) in newborns and infants is a complex condition associated with several pulmonary, cardiac, and systemic diseases contributing to morbidity and mortality. Thus, accurate and early detection of PH and the classification of its severity is crucial for appropriate and successful management. Using echocardiography, the primary diagnostic tool in pediatrics, human assessment is both time-consuming and expertise-demanding, raising the need for an automated approach. Little effort has been directed towards automatic assessment of PH using echocardiography, and the few proposed methods only focus on binary PH classification on the adult population. In this work, we present an explainable multi-view video-based deep learning approach to predict and classify the severity of PH for a cohort of 270 newborns using echocardiograms. We use spatio-temporal convolutional architectures for the prediction of PH from each view, and aggregate the predictions of the different views using majority voting. Our results show a mean F1-score of 0.84 for severity prediction and 0.92 for binary detection using 10-fold cross-validation and 0.63 for severity prediction and 0.78 for binary detection on the held-out test set. We complement our predictions with saliency maps and show that the learned model focuses on clinically relevant cardiac structures, motivating its usage in clinical practice. To the best of our knowledge, this is the first work for an automated assessment of PH in newborns using echocardiograms.},
  archive      = {J_IJCV},
  author       = {Ragnarsdottir, Hanna and Ozkan, Ece and Michel, Holger and Chin-Cheong, Kieran and Manduchi, Laura and Wellmann, Sven and Vogt, Julia E.},
  doi          = {10.1007/s11263-024-01996-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2567-2584},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep learning based prediction of pulmonary hypertension in newborns using echocardiograms},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InterCap: Joint markerless 3D tracking of humans and objects
in interaction from multi-view RGB-d images. <em>IJCV</em>,
<em>132</em>(7), 2551–2566. (<a
href="https://doi.org/10.1007/s11263-024-01984-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios},
  doi          = {10.1007/s11263-024-01984-1},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2551-2566},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {InterCap: Joint markerless 3D tracking of humans and objects in interaction from multi-view RGB-D images},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HSCNet++: Hierarchical scene coordinate classification and
regression for visual localization with transformer. <em>IJCV</em>,
<em>132</em>(7), 2530–2550. (<a
href="https://doi.org/10.1007/s11263-023-01982-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization, state-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently, deep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and thus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous environment, learning such a regression task directly can be difficult for a single network. In this work, we present a new hierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-fine manner from a single RGB image. The proposed method, which is an extension of HSCNet, allows us to train compact models which scale robustly to large environments. It sets a new state-of-the-art for single-image localization on the 7-Scenes, 12-Scenes, Cambridge Landmarks datasets, and the combined indoor scenes.},
  archive      = {J_IJCV},
  author       = {Wang, Shuzhe and Laskar, Zakaria and Melekhov, Iaroslav and Li, Xiaotian and Zhao, Yi and Tolias, Giorgos and Kannala, Juho},
  doi          = {10.1007/s11263-023-01982-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2530-2550},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HSCNet++: Hierarchical scene coordinate classification and regression for visual localization with transformer},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust object re-identification with coupled noisy labels.
<em>IJCV</em>, <em>132</em>(7), 2511–2529. (<a
href="https://doi.org/10.1007/s11263-024-01997-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Mouxing and Huang, Zhenyu and Peng, Xi},
  doi          = {10.1007/s11263-024-01997-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2511-2529},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust object re-identification with coupled noisy labels},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric prior guided feature representation learning for
long-tailed classification. <em>IJCV</em>, <em>132</em>(7), 2493–2510.
(<a href="https://doi.org/10.1007/s11263-024-01983-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Ma, Yanbiao and Jiao, Licheng and Liu, Fang and Yang, Shuyuan and Liu, Xu and Chen, Puhua},
  doi          = {10.1007/s11263-024-01983-2},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2493-2510},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Geometric prior guided feature representation learning for long-tailed classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatially-varying illumination-aware indoor harmonization.
<em>IJCV</em>, <em>132</em>(7), 2473–2492. (<a
href="https://doi.org/10.1007/s11263-024-01994-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of spatially-varying illumination-aware indoor harmonization. Existing image harmonization works either focus on extracting no more than 2D information (e.g., low-level statistics or image filters) from the background image or rely on the non-linear representations of deep neural networks to adjust the foreground appearance. However, from a physical point of view, realistic image harmonization requires the perception of illumination at the foreground position in the scene (i.e., Spatially-Varying (SV) illumination), especially for indoor scenes. To solve indoor harmonization, we present a novel learning-based framework, which attempts to mimic the physical model of image formation. The proposed framework consists of a new neural harmonization architecture with four compact neural modules, which jointly learn SV illumination, shading, albedo, and rendering. In particular, a multilayer perceptron-based neural illumination field is designed to recover the illumination with finer details. Besides, we construct the first large-scale synthetic indoor harmonization benchmark dataset in which the foreground focuses on humans and is rendered and perturbed by SV illuminations. An object placement formula is also derived to ensure that the foreground object is placed in the background at a reasonable size. Extensive experiments on synthetic and real data demonstrate that our proposed approach achieves better results than prior works.},
  archive      = {J_IJCV},
  author       = {Hu, Zhongyun and Li, Jiahao and Wang, Xue and Wang, Qing},
  doi          = {10.1007/s11263-024-01994-z},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2473-2492},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatially-varying illumination-aware indoor harmonization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised training of universal visual concepts for
multi-domain semantic segmentation. <em>IJCV</em>, <em>132</em>(7),
2450–2472. (<a
href="https://doi.org/10.1007/s11263-024-01986-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on multiple datasets becomes a method of choice towards strong generalization in usual scenes and graceful performance degradation in edge cases. Unfortunately, popular datasets often have discrepant granularities. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. Furthermore, many datasets have overlapping labels. For instance, pickups are labeled as trucks in VIPER, cars in Vistas, and vans in ADE20k. We address this challenge by considering labels as unions of universal visual concepts. This allows seamless and principled learning on multi-domain dataset collections without requiring any relabeling effort. Our method improves within-dataset and cross-dataset generalization, and provides opportunity to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the WildDash 2 benchmark.},
  archive      = {J_IJCV},
  author       = {Bevandić, Petra and Oršić, Marin and Šarić, Josip and Grubišić, Ivan and Šegvić, Siniša},
  doi          = {10.1007/s11263-024-01986-z},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2450-2472},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weakly supervised training of universal visual concepts for multi-domain semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-dataset detection with transformers. <em>IJCV</em>,
<em>132</em>(7), 2443–2449. (<a
href="https://doi.org/10.1007/s11263-024-01985-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a unified model from multiple datasets is very challenging. In this paper, we propose a multi-dataset detector using the transformer (MDT). To enhance the effectiveness of the fusion of multiple datasets, we propose alternative learning to suppress the noisy data. To speed up the training of big data, we use scale shifting to save computational effort. Experiments on OpenImages, COCO, and Mapillary datasets show that our approach can significantly accelerate training while improving performance on multiple datasets. In the Robust Vision Challenge 2022, our solution won 1st place on the object detection track.},
  archive      = {J_IJCV},
  author       = {Ke, Bo and Qiao, Ruizhi and Sun, Xing},
  doi          = {10.1007/s11263-024-01985-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2443-2449},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-dataset detection with transformers},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oriented r-CNN and beyond. <em>IJCV</em>, <em>132</em>(7),
2420–2442. (<a
href="https://doi.org/10.1007/s11263-024-01989-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Xie, Xingxing and Cheng, Gong and Wang, Jiabao and Li, Ke and Yao, Xiwen and Han, Junwei},
  doi          = {10.1007/s11263-024-01989-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2420-2442},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Oriented R-CNN and beyond},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust monocular depth estimation: A new baseline
and benchmark. <em>IJCV</em>, <em>132</em>(7), 2401–2419. (<a
href="https://doi.org/10.1007/s11263-023-01979-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Xian, Ke and Cao, Zhiguo and Shen, Chunhua and Lin, Guosheng},
  doi          = {10.1007/s11263-023-01979-4},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2401-2419},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards robust monocular depth estimation: A new baseline and benchmark},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based image and video inpainting: A survey.
<em>IJCV</em>, <em>132</em>(7), 2367–2400. (<a
href="https://doi.org/10.1007/s11263-023-01977-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image and video inpainting is a classic problem in computer vision and computer graphics, aiming to fill in the plausible and realistic content in the missing areas of images and videos. With the advance of deep learning, this problem has achieved significant progress recently. The goal of this paper is to comprehensively review the deep learning-based methods for image and video inpainting. Specifically, we sort existing methods into different categories from the perspective of their high-level inpainting pipeline, present different deep learning architectures, including CNN, VAE, GAN, diffusion models, etc., and summarize techniques for module design. We review the training objectives and the common benchmark datasets. We present evaluation metrics for low-level pixel and high-level perceptional similarity, conduct a performance evaluation, and discuss the strengths and weaknesses of representative inpainting methods. We also discuss related real-world applications. Finally, we discuss open challenges and suggest potential future research directions.},
  archive      = {J_IJCV},
  author       = {Quan, Weize and Chen, Jiaxi and Liu, Yanli and Yan, Dong-Ming and Wonka, Peter},
  doi          = {10.1007/s11263-023-01977-6},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2367-2400},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep learning-based image and video inpainting: A survey},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). View-invariant skeleton action representation learning via
motion retargeting. <em>IJCV</em>, <em>132</em>(7), 2351–2366. (<a
href="https://doi.org/10.1007/s11263-023-01967-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Di and Wang, Yaohui and Dantcheva, Antitza and Garattoni, Lorenzo and Francesca, Gianpiero and Brémond, François},
  doi          = {10.1007/s11263-023-01967-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {2351-2366},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {View-invariant skeleton action representation learning via motion retargeting},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GyroFlow+: Gyroscope-guided unsupervised deep homography and
optical flow learning. <em>IJCV</em>, <em>132</em>(6), 2331–2349. (<a
href="https://doi.org/10.1007/s11263-023-01978-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing homography and optical flow methods are erroneous in challenging scenes, such as fog, rain, night, and snow because the basic assumptions such as brightness and gradient constancy are broken. To address this issue, we present an unsupervised learning approach that fuses gyroscope into homography and optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module (SGF) to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. Meanwhile, we propose a homography decoder module (HD) to combine gyro field and intermediate results of SGF to produce the homography. To the best of our knowledge, this is the first deep learning framework that fuses gyroscope data and image content for both deep homography and optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-the-art methods in both regular and challenging scenes. The code and dataset are available at https://github.com/lhaippp/GyroFlowPlus .},
  archive      = {J_IJCV},
  author       = {Li, Haipeng and Luo, Kunming and Zeng, Bing and Liu, Shuaicheng},
  doi          = {10.1007/s11263-023-01978-5},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2331-2349},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {GyroFlow+: Gyroscope-guided unsupervised deep homography and optical flow learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delving into identify-emphasize paradigm for combating
unknown bias. <em>IJCV</em>, <em>132</em>(6), 2310–2330. (<a
href="https://doi.org/10.1007/s11263-023-01969-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset biases are notoriously detrimental to model robustness and generalization. The identify-emphasize paradigm appears to be effective in dealing with unknown biases. However, we discover that it is still plagued by two challenges: A, the quality of the identified bias-conflicting samples is far from satisfactory; B, the emphasizing strategies only produce suboptimal performance. In this paper, for challenge A, we propose an effective bias-conflicting scoring method (ECS) to boost the identification accuracy, along with two practical strategies — peer-picking and epoch-ensemble. For challenge B, we point out that the gradient contribution statistics can be a reliable indicator to inspect whether the optimization is dominated by bias-aligned samples. Then, we propose gradient alignment (GA), which employs gradient statistics to balance the contributions of the mined bias-aligned and bias-conflicting samples dynamically throughout the learning process, forcing models to leverage intrinsic features to make fair decisions. Furthermore, we incorporate self-supervised (SS) pretext tasks into training, which enable models to exploit richer features rather than the simple shortcuts, resulting in more robust models. Experiments are conducted on multiple datasets in various settings, demonstrating that the proposed solution can mitigate the impact of unknown biases and achieve state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Zhao, Bowen and Chen, Chen and Wang, Qian-Wei and He, Anfeng and Xia, Shu-Tao},
  doi          = {10.1007/s11263-023-01969-6},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2310-2330},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Delving into identify-emphasize paradigm for combating unknown bias},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning by asking questions for knowledge-based novel
object recognition. <em>IJCV</em>, <em>132</em>(6), 2290–2309. (<a
href="https://doi.org/10.1007/s11263-023-01976-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world object recognition, there are numerous object classes to be recognized. Traditional image recognition methods based on supervised learning can only recognize object classes present in the training data, and have limited applicability in the real world. In contrast, humans can recognize novel objects by questioning and acquiring knowledge about them. Inspired by this, we propose a framework for acquiring external knowledge by generating questions that enable the model to instantly recognize novel objects. Our framework comprises three components: the object classifier (OC), which performs knowledge-based object recognition, the question generator (QG), which generates knowledge-aware questions to acquire novel knowledge, and the policy decision (PD) Model, which determines the “policy” of questions to be asked. The PD model utilizes two strategies, namely “confirmation” and “exploration”—the former confirms candidate knowledge while the latter explores completely new knowledge. Our experiments demonstrate that the proposed pipeline effectively acquires knowledge about novel objects compared to several baselines, and realizes novel object recognition utilizing the obtained knowledge. We also performed a real-world evaluation in which humans responded to the generated questions, and the model used the acquired knowledge to retrain the OC, which is a fundamental step toward a real-world human-in-the-loop learning-by-asking framework. We plan to release the dataset immediately upon acceptance of our work.},
  archive      = {J_IJCV},
  author       = {Uehara, Kohei and Harada, Tatsuya},
  doi          = {10.1007/s11263-023-01976-7},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2290-2309},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning by asking questions for knowledge-based novel object recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability-adaptive consistency regularization for
weakly-supervised point cloud segmentation. <em>IJCV</em>,
<em>132</em>(6), 2276–2289. (<a
href="https://doi.org/10.1007/s11263-023-01975-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised point cloud segmentation with extremely limited labels is highly desirable to alleviate the expensive costs of collecting densely annotated 3D points. This paper explores applying the consistency regularization that is commonly used in weakly-supervised learning, for its point cloud counterpart with multiple data-specific augmentations, which has not been well studied. We observe that the straightforward way of applying consistency constraints to weakly-supervised point cloud segmentation has two major limitations: noisy pseudo labels due to the conventional confidence-based selection and insufficient consistency constraints due to discarding unreliable pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency Network (RAC-Net) to use both prediction confidence and model uncertainty to measure the reliability of pseudo labels and apply consistency training on all unlabeled points while with different consistency constraints for different points based on the reliability of corresponding pseudo labels. Experimental results on the S3DIS and ScanNet-v2 benchmark datasets show that our model achieves superior performance in weakly-supervised point cloud segmentation. The code will be released publicly at https://github.com/wu-zhonghua/RAC-Net .},
  archive      = {J_IJCV},
  author       = {Wu, Zhonghua and Wu, Yicheng and Lin, Guosheng and Cai, Jianfei},
  doi          = {10.1007/s11263-023-01975-8},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2276-2289},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Reliability-adaptive consistency regularization for weakly-supervised point cloud segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic-based feature embedding of 4-d light fields
for compressive imaging and denoising. <em>IJCV</em>, <em>132</em>(6),
2255–2275. (<a
href="https://doi.org/10.1007/s11263-023-01974-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-dimensional nature of the 4-D light field (LF) poses great challenges in achieving efficient and effective feature embedding, that severely impacts the performance of downstream tasks. To tackle this crucial issue, in contrast to existing methods with empirically-designed architectures, we propose a probabilistic-based feature embedding (PFE), which learns a feature embedding architecture by assembling various low-dimensional convolution patterns in a probability space for fully capturing spatial-angular information. Building upon the proposed PFE, we then leverage the intrinsic linear imaging model of the coded aperture camera to construct a cycle-consistent 4-D LF reconstruction network from coded measurements. Moreover, we incorporate PFE into an iterative optimization framework for 4-D LF denoising. Our extensive experiments demonstrate the significant superiority of our methods on both real-world and synthetic 4-D LF images, both quantitatively and qualitatively, when compared with state-of-the-art methods. The source code will be publicly available at https://github.com/lyuxianqiang/LFCA-CR-NET .},
  archive      = {J_IJCV},
  author       = {Lyu, Xianqiang and Hou, Junhui},
  doi          = {10.1007/s11263-023-01974-9},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2255-2275},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Probabilistic-based feature embedding of 4-D light fields for compressive imaging and denoising},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial network applications in industry 4.0:
A review. <em>IJCV</em>, <em>132</em>(6), 2195–2254. (<a
href="https://doi.org/10.1007/s11263-023-01966-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breakthrough brought by generative adversarial networks (GANs) in computer vision (CV) applications has gained a lot of attention in different fields due to their ability to capture the distribution of a dataset and generate high-quality similar images. From one side, this technology has been rapidly adopted as an alternative to traditional applications and introduced novel perspectives in data augmentation, domain transfer, image expansion, image restoration, image segmentation, and super-resolution. From another side, we found that due to the lack of industrial datasets and the limitation for acquiring and accurately annotating new images, GANs form an exciting solution to generate new industrial image datasets or to restore and augment existing ones. Therefore, we introduce a review of the latest trend in GANs applications and project them in industrial use cases. We conducted our experiments with synthetic images and analyzed most of GAN’s failures and image artifacts to provide training’s best practices.},
  archive      = {J_IJCV},
  author       = {Abou Akar, Chafic and Abdel Massih, Rachelle and Yaghi, Anthony and Khalil, Joe and Kamradt, Marc and Makhoul, Abdallah},
  doi          = {10.1007/s11263-023-01966-9},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2195-2254},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generative adversarial network applications in industry 4.0: A review},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S <span class="math display"><sup>2</sup></span> p <span
class="math display"><sup>3</sup></span>: Self-supervised polarimetric
pose prediction. <em>IJCV</em>, <em>132</em>(6), 2177–2194. (<a
href="https://doi.org/10.1007/s11263-023-01965-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the first self-supervised 6D object pose prediction from multimodal RGB + polarimetric images. The novel training paradigm comprises (1) a physical model to extract geometric information of polarized light, (2) a teacher–student knowledge distillation scheme and (3) a self-supervised loss formulation through differentiable rendering and an invertible physical constraint. Both networks leverage the physical properties of polarized light to learn robust geometric representations by encoding shape priors and polarization characteristics derived from our physical model. Geometric pseudo-labels from the teacher support the student network without the need for annotated real data. Dense appearance and geometric information of objects are obtained through a differentiable renderer with the predicted pose for self-supervised direct coupling. The student network additionally features our proposed invertible formulation of the physical shape priors that enables end-to-end self-supervised training through physical constraints of derived polarization characteristics compared against polarimetric input images. We specifically focus on photometrically challenging objects with texture-less or reflective surfaces and transparent materials for which the most prominent performance gain is reported.},
  archive      = {J_IJCV},
  author       = {Ruhkamp, Patrick and Gao, Daoyi and Navab, Nassir and Busam, Benjamin},
  doi          = {10.1007/s11263-023-01965-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2177-2194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {S $$^{2}$$ p $$^{3}$$: Self-supervised polarimetric pose prediction},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ToTem NRSfM: Object-wise non-rigid structure-from-motion
with a topological template. <em>IJCV</em>, <em>132</em>(6), 2135–2176.
(<a href="https://doi.org/10.1007/s11263-023-01923-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Non-Rigid Structure-from-Motion (NRSfM) method to reconstruct an object whose topology is known. We represent the topology by a 3D shape that weakly resembles the object, which we call a Topological Template (ToTem). The ToTem has two main differences with the template used in Shape-from-Template (SfT). First, the shape in the ToTem is not necessarily feasible for the object, whereas it must be in the SfT template. Second, the ToTem only models shape, excluding the classical texture-map representing colour in the SfT template. These two differences greatly alleviate the practical difficulty of constructing a template. However, they make the reconstruction problem challenging, as they preclude the use of strong deformation constraints between the template shape and the reconstruction and the possibility of directly establishing correspondences between the template and the images. Our method uses an isometric deformation prior and proceeds in four steps. First, it reconstructs point-clouds from the images. Second, it aligns the ToTem to the point-clouds. Third, it creates a coherent surface parameterisation. Fourth, it performs a global refinement, posed as Bundle Adjustment (BA). We show experimentally that our method outperforms the existing methods for its isolated steps and NRSfM methods overall, in terms of 3D accuracy, ability to reconstruct the object’s visible surface and ability to approximate the object’s invisible surface.},
  archive      = {J_IJCV},
  author       = {Sengupta, Agniva and Bartoli, Adrien},
  doi          = {10.1007/s11263-023-01923-6},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2135-2176},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ToTem NRSfM: Object-wise non-rigid structure-from-motion with a topological template},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logit normalization for long-tail object detection.
<em>IJCV</em>, <em>132</em>(6), 2114–2134. (<a
href="https://doi.org/10.1007/s11263-023-01971-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data with skewed distributions poses a serious challenge to existing object detectors. The unbalanced label distribution leads to a bias towards dominate labels, resulting in the worse detection performance on the rare classes than the dominant classes. More unfortunately, the label samplers in these detectors shift the training label distributions to a new skewed distribution, thereby severely limiting the effectiveness of previous prior-based methods such as Logit Adjustment (Menon et al., in ICLR. OpenReview.net, 2021). Additionally, the tremendous ratio of the background samples to the samples per foreground category further hinders the learning of classification on foreground categories. To mitigate these issues, in this paper, we propose Logit Normalization (LogN), a simple technique to self-calibrate the classification logits of detectors in a similar way to Batch Normalization (BN). LogN first leverages the consistency between logit statistics and the training label distribution to eliminate the long-tail bias of detectors in a normalized manner. Second, based on the independence between fore-background imbalance and long-tail distribution, we also introduce a background calibration for LogN, which effectively improves the overall performance by restoring the background discriminability. In general, our LogN is training- and tuning-free (i.e. require no extra training and tuning process), model- and label distribution-agnostic (i.e. generalization to different kinds of detectors and datasets), and also plug-and-play (i.e. direct application without any bells and whistles). Extensive experiments on the LVIS dataset demonstrate the superior performance of LogN to the state-of-the-art methods with various detectors (e.g. two-stage detectors, one-stage detectors, query-based detectors) and backbones (e.g. VITs, Swin Transformers). We also provide in-depth studies on different aspects of our LogN. We also conduct experiments on multiple datasets such as Open Images and ImageNet-LT. The results show that LogN can improve performance on other object detection datasets and the image classification task. Our LogN can serve as a strong baseline for long-tail object detection and is expected to inspire future research in this field.},
  archive      = {J_IJCV},
  author       = {Zhao, Liang and Teng, Yao and Wang, Limin},
  doi          = {10.1007/s11263-023-01971-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2114-2134},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Logit normalization for long-tail object detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking and analysis of unsupervised object
segmentation from real-world single images. <em>IJCV</em>,
<em>132</em>(6), 2077–2113. (<a
href="https://doi.org/10.1007/s11263-023-01973-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of unsupervised object segmentation from single images. We do not introduce a new algorithm, but systematically investigate the effectiveness of existing unsupervised models on challenging real-world images. We first introduce seven complexity factors to quantitatively measure the distributions of background and foreground object biases in appearance and geometry for datasets with human annotations. With the aid of these factors, we empirically find that, not surprisingly, existing unsupervised models fail to segment generic objects in real-world images, although they can easily achieve excellent performance on numerous simple synthetic datasets, due to the vast gap in objectness biases between synthetic and real images. By conducting extensive experiments on multiple groups of ablated real-world datasets, we ultimately find that the key factors underlying the failure of existing unsupervised models on real-world images are the challenging distributions of background and foreground object biases in appearance and geometry. Because of this, the inductive biases introduced in existing unsupervised models can hardly capture the diverse object distributions. Our research results suggest that future work should exploit more explicit objectness biases in the network design.},
  archive      = {J_IJCV},
  author       = {Yang, Yafei and Yang, Bo},
  doi          = {10.1007/s11263-023-01973-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2077-2113},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking and analysis of unsupervised object segmentation from real-world single images},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable task-inspired adaptive filter pruning for
neural networks under multiple constraints. <em>IJCV</em>,
<em>132</em>(6), 2060–2076. (<a
href="https://doi.org/10.1007/s11263-023-01972-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for filter pruning mostly rely on specific data-driven paradigms but lack the interpretability. Besides, these approaches usually assign layer-wise compression ratios automatically only under given FLOPs by neural architecture search algorithms or just manually, which are short of efficiency. In this paper, we propose a novel interpretable task-inspired adaptive filter pruning method for neural networks to solve the above problems. First, we treat filters as semantic detectors and develop the task-inspired importance criteria by evaluating correlations between input tasks and feature maps, and observing the information flow through filters between adjacent layers. Second, we refer to the human neurobiological mechanism for the better interpretability, where the retained first layer filters act as individual information receivers. Third, inspired by the phenomenon that each filter has a deterministic impact on FLOPs and network parameters, we provide an efficient adaptive compression ratio allocation strategy based on differentiable pruning approximation under multiple budget constraints, as well as considering the performance objective. The proposed method is validated with extensive experiments on the state-of-the-art neural networks, which significantly outperforms all the existing filter pruning methods and achieves the best trade-off between neural network compression and task performance. With ResNet-50 on ImageNet, our approach reduces 75.49% parameters and 70.90% FLOPs, only suffering from 2.31% performance degradation.},
  archive      = {J_IJCV},
  author       = {Guo, Yang and Gao, Wei and Li, Ge},
  doi          = {10.1007/s11263-023-01972-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2060-2076},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Interpretable task-inspired adaptive filter pruning for neural networks under multiple constraints},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data augmentation for low-level vision: CutBlur and
mixture-of-augmentation. <em>IJCV</em>, <em>132</em>(6), 2041–2059. (<a
href="https://doi.org/10.1007/s11263-023-01970-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation (DA) is an effective way to improve the performance of deep networks. Unfortunately, current methods are mostly developed for high-level vision tasks (eg, image classification) and few are studied for low-level (eg, image restoration). In this paper, we provide a comprehensive analysis of the existing DAs in the frequency domain. We find that the methods that largely manipulate the spatial information can hinder the image restoration process and hurt the performance. Based on our analyses, we propose CutBlur and mixture-of-augmentation (MoA). CutBlur cuts a low-quality patch and pastes it to the corresponding high-quality image region, or vice versa. The key intuition is to provide enough DA effect while keeping the pixel distribution intact. This characteristic of CutBlur enables a model to learn not only “how” but also “where” to reconstruct an image. Eventually, the model understands “how much” to restore given pixels, which allows it to generalize better to unseen data distributions. We further improve the restoration performance by MoA that incorporates the curated list of DAs. We demonstrate the effectiveness of our methods by conducting extensive experiments on several low-level vision tasks on both single or a mixture of distortion tasks. Our results show that CutBlur and MoA consistently and significantly improve the performance especially when the model size is big and the data is collected under real-world environments. Our code is available at https://github.com/clovaai/cutblur .},
  archive      = {J_IJCV},
  author       = {Ahn, Namhyuk and Yoo, Jaejun and Sohn, Kyung-Ah},
  doi          = {10.1007/s11263-023-01970-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2041-2059},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Data augmentation for low-level vision: CutBlur and mixture-of-augmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task bias in contrastive vision-language models.
<em>IJCV</em>, <em>132</em>(6), 2026–2040. (<a
href="https://doi.org/10.1007/s11263-023-01945-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidental supervision from language has become a popular approach for learning generic visual representations that can be prompted to perform many recognition tasks in computer vision. We conduct an in-depth exploration of the CLIP model and show that its visual representation is often strongly biased towards solving some tasks more than others. Moreover, which task the representation will be biased towards is unpredictable, with little consistency across images. To resolve this task bias, we show how to learn a ‘task guidance token’ that can be appended to the input to prompt the representation towards features relevant to their task of interest. Our results show that this task guidance can be independent of the input image and still effectively provide a conditioning mechanism to steer visual representations towards the desired task.},
  archive      = {J_IJCV},
  author       = {Menon, Sachit and Chandratreya, Ishaan Preetam and Vondrick, Carl},
  doi          = {10.1007/s11263-023-01945-0},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2026-2040},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Task bias in contrastive vision-language models},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint learning of audio–visual saliency prediction and sound
source localization on multi-face videos. <em>IJCV</em>,
<em>132</em>(6), 2003–2025. (<a
href="https://doi.org/10.1007/s11263-023-01950-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual and audio events simultaneously occur and both attract attention. However, most existing saliency prediction works ignore the influence of audio and only consider vision modality. In this paper, we propose a multi-task learning method for audio–visual saliency prediction and sound source localization on multi-face video by leveraging visual, audio and face information. Specifically, we first introduce a large-scale database of multi-face video in visual-audio condition, containing eye-tracking data and sound source annotations. Using this database, we find that sound influences human attention, and conversely attention offers a cue to determine sound source on multi-face video. Guided by these findings, an audio–visual multi-task network (AVM-Net) is introduced to predict saliency and locate sound source. AVM-Net consists of three branches corresponding to visual, audio and face modalities. The visual branch has a two-stream architecture to capture spatial and temporal information. Face and audio branches encode audio signals and faces, respectively. Finally, a spatio-temporal multi-modal graph is constructed to model the interaction among multiple faces. With joint optimization of these branches, the intrinsic correlation of the tasks of saliency prediction and sound source localization is utilized and their performance is boosted by each other. Experiments show that the proposed method outperforms 12 state-of-the-art saliency prediction methods, and achieves competitive results in sound source localization.},
  archive      = {J_IJCV},
  author       = {Qiao, Minglang and Liu, Yufan and Xu, Mai and Deng, Xin and Li, Bing and Hu, Weiming and Borji, Ali},
  doi          = {10.1007/s11263-023-01950-3},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {2003-2025},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Joint learning of Audio–Visual saliency prediction and sound source localization on multi-face videos},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SyDog-video: A synthetic dog video dataset for temporal pose
estimation. <em>IJCV</em>, <em>132</em>(6), 1986–2002. (<a
href="https://doi.org/10.1007/s11263-023-01946-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to estimate the pose of dogs from videos using a temporal deep learning model as this can result in more accurate pose predictions when temporary occlusions or substantial movements occur. Generally, deep learning models require a lot of data to perform well. To our knowledge, public pose datasets containing videos of dogs are non existent. To solve this problem, and avoid manually labelling videos as it can take a lot of time, we generated a synthetic dataset containing 500 videos of dogs performing different actions using Unity3D. Diversity is achieved by randomising parameters such as lighting, backgrounds, camera parameters and the dog’s appearance and pose. We evaluate the quality of our synthetic dataset by assessing the model’s capacity to generalise to real data. Usually, networks trained on synthetic data perform poorly when evaluated on real data, this is due to the domain gap. As there was still a domain gap after improving the quality of the synthetic dataset and inserting diversity, we bridged the domain gap by applying 2 different methods: fine-tuning and using a mixed dataset to train the network. Additionally, we compare the model pre-trained on synthetic data with models pre-trained on a real-world animal pose datasets. We demonstrate that using the synthetic dataset is beneficial for training models with (small) real-world datasets. Furthermore, we show that pre-training the model with the synthetic dataset is the go to choice rather than pre-training on real-world datasets for solving the pose estimation task from videos of dogs.},
  archive      = {J_IJCV},
  author       = {Shooter, Moira and Malleson, Charles and Hilton, Adrian},
  doi          = {10.1007/s11263-023-01946-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1986-2002},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SyDog-video: A synthetic dog video dataset for temporal pose estimation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep depth from focal stack with defocus model for
camera-setting invariance. <em>IJCV</em>, <em>132</em>(6), 1970–1985.
(<a href="https://doi.org/10.1007/s11263-023-01964-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose deep depth from focal stack (DDFS), which takes a focal stack as input of a neural network for estimating scene depth. Defocus blur is a useful cue for depth estimation. However, the size of the blur depends on not only scene depth but also camera settings such as focus distance, focal length, and f-number. Current learning-based methods without any defocus models cannot estimate a correct depth map if camera settings are different at training and test times. Our method takes a plane sweep volume as input for the constraint between scene depth, defocus images, and camera settings, and this intermediate representation enables depth estimation with different camera settings at training and test times. This camera-setting invariance can enhance the applicability of DDFS. The experimental results also indicate that our method is robust against a synthetic-to-real domain gap.},
  archive      = {J_IJCV},
  author       = {Fujimura, Yuki and Iiyama, Masaaki and Funatomi, Takuya and Mukaigawa, Yasuhiro},
  doi          = {10.1007/s11263-023-01964-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1970-1985},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep depth from focal stack with defocus model for camera-setting invariance},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grounded affordance from exocentric view. <em>IJCV</em>,
<em>132</em>(6), 1945–1969. (<a
href="https://doi.org/10.1007/s11263-023-01962-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affordance grounding aims to locate objects’ “action possibilities” regions, an essential step toward embodied intelligence. Due to the diversity of interactive affordance, i.e., the uniqueness of different individual habits leads to diverse interactions, which makes it difficult to establish an explicit link between object parts and affordance labels. Human has the ability that transforms various exocentric interactions into invariant egocentric affordance to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from the exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. However, there is some “interaction bias” between personas, mainly regarding different regions and views. To this end, we devise a cross-view affordance knowledge transfer framework that extracts affordance-specific features from exocentric interactions and transfers them to the egocentric view to solve the above problems. Furthermore, the perception of affordance regions is enhanced by preserving affordance co-relations. In addition, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality. The code is available via: github.com/lhc1224/Cross-View-AG .},
  archive      = {J_IJCV},
  author       = {Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01962-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1945-1969},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Grounded affordance from exocentric view},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Infrared adversarial patches with learnable shapes and
locations in the physical world. <em>IJCV</em>, <em>132</em>(6),
1928–1944. (<a
href="https://doi.org/10.1007/s11263-023-01963-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the extensive application of infrared object detectors in the safety-critical tasks, it is necessary to evaluate their robustness against adversarial examples in the real world. However, current few physical infrared attacks are complicated to implement in practical application because of their complex transformation from the digital world to physical world. To address this issue, in this paper, we propose a physically feasible infrared attack method called “infrared adversarial patches”. Considering the imaging mechanism of infrared cameras by capturing objects’ thermal radiation, infrared adversarial patches conduct attacks by attaching a patch of thermal insulation materials on the target object to manipulate its thermal distribution. To enhance adversarial attacks, we present a novel aggregation regularization to guide the simultaneous learning for the patch’s shape and location on the target object. Thus, a simple gradient-based optimization can be adapted to solve for them. We verify infrared adversarial patches in different object detection tasks with various object detectors. Experimental results show that our method achieves more than 90% Attack Success Rate (ASR) versus the pedestrian detector and vehicle detector in the physical environment, where the objects are captured in different angles, distances, postures, and scenes. More importantly, infrared adversarial patch is easy to implement, and it only needs 0.5 h to be manufactured in the physical world, which verifies its effectiveness and efficiency. Another advantage of our infrared adversarial patches is the ability to extend to attack the visible object detector in the physical world. As a consequence, we can simultaneously perform the infrared and visible physical attacks by a unified adversarial patch, which shows the good generalization.},
  archive      = {J_IJCV},
  author       = {Wei, Xingxing and Yu, Jie and Huang, Yao},
  doi          = {10.1007/s11263-023-01963-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1928-1944},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Infrared adversarial patches with learnable shapes and locations in the physical world},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyperSTAR: Task-aware hyperparameter recommendation for
training and compression. <em>IJCV</em>, <em>132</em>(6), 1913–1927. (<a
href="https://doi.org/10.1007/s11263-023-01961-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter optimization (HPO) methods alleviate the significant effort required to obtain hyperparameters that perform optimally on visual learning problems. Existing methods are computationally inefficient because they are task agnostic (i.e., they do not adapt to a given task). We present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a task-aware HPO algorithm that improves HPO efficiency for a target dataset by using prior knowledge from previous hyperparameter searches to recommend effective hyperparameters conditioned on the target dataset. HyperSTAR ranks and recommends hyperparameters by predicting their performance on the target dataset. To do so, it learns a joint dataset-hyperparameter space in an end-to-end manner that enables its performance predictor to use previously found effective hyperparameters for other similar tasks. The hyperparameter recommendations of HyperSTAR combined with existing HPO techniques lead to a task-aware HPO system that reduces the time to find the optimal hyperparameters for the target learning problem. Our experiments on image classification, object detection, and model pruning validate that HyperSTAR reduces the evaluation of different hyperparameter configurations by about $$50\%$$ compared to existing methods and, when combined with Hyperband, uses only $$25\%$$ of the budget required by the vanilla Hyperband and Bayesian Optimized Hyperband to achieve the best performance.},
  archive      = {J_IJCV},
  author       = {Liu, Chang and Mittal, Gaurav and Karianakis, Nikolaos and Fragoso, Victor and Yu, Ye and Fu, Yun and Chen, Mei},
  doi          = {10.1007/s11263-023-01961-0},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1913-1927},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HyperSTAR: Task-aware hyperparameter recommendation for training and compression},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). CLIP-guided prototype modulating for few-shot action
recognition. <em>IJCV</em>, <em>132</em>(6), 1899–1912. (<a
href="https://doi.org/10.1007/s11263-023-01917-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from large-scale contrastive language-image pre-training like CLIP has shown remarkable success in a wide range of downstream tasks recently, but it is still under-explored on the challenging few-shot action recognition (FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge of CLIP to alleviate the inaccurate prototype estimation issue due to data scarcity, which is a critical problem in low-shot regimes. To this end, we present a CLIP-guided prototype modulating framework called CLIP-FSAR, which consists of two key components: a video-text contrastive objective and a prototype modulation. Specifically, the former bridges the task discrepancy between CLIP and the few-shot video task by contrasting videos and corresponding class text descriptions. The latter leverages the transferable textual concepts from CLIP to adaptively refine visual prototypes with a temporal Transformer. By this means, CLIP-FSAR can take full advantage of the rich semantic priors in CLIP to obtain reliable prototypes and achieve accurate few-shot classification. Extensive experiments on five commonly used benchmarks demonstrate the effectiveness of our proposed method, and CLIP-FSAR significantly outperforms existing state-of-the-art methods under various settings. The source code and models are publicly available at https://github.com/alibaba-mmai-research/CLIP-FSAR .},
  archive      = {J_IJCV},
  author       = {Wang, Xiang and Zhang, Shiwei and Cen, Jun and Gao, Changxin and Zhang, Yingya and Zhao, Deli and Sang, Nong},
  doi          = {10.1007/s11263-023-01917-4},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1899-1912},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CLIP-guided prototype modulating for few-shot action recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards defending multiple <span
class="math display"><em>ℓ</em><sub><em>p</em></sub></span> -norm
bounded adversarial perturbations via gated batch normalization.
<em>IJCV</em>, <em>132</em>(6), 1881–1898. (<a
href="https://doi.org/10.1007/s11263-023-01884-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been extensive evidence demonstrating that deep neural networks are vulnerable to adversarial examples, which motivates the development of defenses against adversarial attacks. Existing adversarial defenses typically improve model robustness against individual specific perturbation types (e.g., $$\ell _{\infty }$$ -norm bounded adversarial examples). However, adversaries are likely to generate multiple types of perturbations in practice (e.g., $$\ell _1$$ , $$\ell _2$$ , and $$\ell _{\infty }$$ perturbations). Some recent methods improve model robustness against adversarial attacks in multiple $$\ell _p$$ balls, but their performance against each perturbation type is still far from satisfactory. In this paper, we observe that different $$\ell _p$$ bounded adversarial perturbations induce different statistical properties that can be separated and characterized by the statistics of Batch Normalization (BN). We thus propose Gated Batch Normalization (GBN) to adversarially train a perturbation-invariant predictor for defending multiple $$\ell _p$$ bounded adversarial perturbations. GBN consists of a multi-branch BN layer and a gated sub-network. Each BN branch in GBN is in charge of one perturbation type to ensure that the normalized output is aligned towards learning perturbation-invariant representation. Meanwhile, the gated sub-network is designed to separate inputs added with different perturbation types. We perform an extensive evaluation of our approach on commonly-used dataset including MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN outperforms previous defense proposals against multiple perturbation types (i.e., $$\ell _1$$ , $$\ell _2$$ , and $$\ell _{\infty }$$ perturbations) by large margins.},
  archive      = {J_IJCV},
  author       = {Liu, Aishan and Tang, Shiyu and Chen, Xinyun and Huang, Lei and Qin, Haotong and Liu, Xianglong and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01884-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {1881-1898},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards defending multiple $$\ell _p$$ -norm bounded adversarial perturbations via gated batch normalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: SOTVerse: A user-defined task space of single
object tracking. <em>IJCV</em>, <em>132</em>(5), 1880. (<a
href="https://doi.org/10.1007/s11263-023-01968-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Hu, Shiyu and Zhao, Xin and Huang, Kaiqi},
  doi          = {10.1007/s11263-023-01968-7},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1880},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: SOTVerse: a user-defined task space of single object tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A universal event-based plug-in module for visual object
tracking in degraded conditions. <em>IJCV</em>, <em>132</em>(5),
1857–1879. (<a
href="https://doi.org/10.1007/s11263-023-01959-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing trackers based on RGB/grayscale frames may collapse due to the unreliability of conventional sensors in some challenging scenarios (e.g., motion blur and high dynamic range). Event-based cameras as bioinspired sensors encode brightness changes with high temporal resolution and high dynamic range, thereby providing considerable potential for tracking under degraded conditions. Nevertheless, events lack the fine-grained texture cues provided by RGB/grayscale frames. This complementarity encourages us to fuse visual cues from the frame and event domains for robust object tracking under various challenging conditions. In this paper, we propose a novel event feature extractor to capture spatiotemporal features with motion cues from event-based data by boosting interactions and distinguishing alterations between states at different moments. Furthermore, we develop an effective feature integrator to adaptively fuse the strengths of both domains by balancing their contributions. Our proposed module as the plug-in can be easily applied to off-the-shelf frame-based trackers. We extensively validate the effectiveness of eight trackers extended by our approach on three datasets: EED, VisEvent, and our collected frame-event-based dataset FE141. Experimental results also show that event-based data is a powerful cue for tracking.},
  archive      = {J_IJCV},
  author       = {Zhang, Jiqing and Dong, Bo and Fu, Yingkai and Wang, Yuanchen and Wei, Xiaopeng and Yin, Baocai and Yang, Xin},
  doi          = {10.1007/s11263-023-01959-8},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1857-1879},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A universal event-based plug-in module for visual object tracking in degraded conditions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MS-RAFT+: High resolution multi-scale RAFT. <em>IJCV</em>,
<em>132</em>(5), 1835–1856. (<a
href="https://doi.org/10.1007/s11263-023-01930-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical concepts have proven useful in many classical and learning-based optical flow methods regarding both accuracy and robustness. In this paper we show that such concepts are still useful in the context of recent neural networks that follow RAFT’s paradigm refraining from hierarchical strategies by relying on recurrent updates based on a single-scale all-pairs transform. To this end, we introduce MS-RAFT+: a novel recurrent multi-scale architecture based on RAFT that unifies several successful hierarchical concepts. It employs a coarse-to-fine estimation to enable the use of finer resolutions by useful initializations from coarser scales. Moreover, it relies on RAFT’s correlation pyramid that allows to consider non-local cost information during the matching process. Furthermore, it makes use of advanced multi-scale features that incorporate high-level information from coarser scales. And finally, our method is trained subject to a sample-wise robust multi-scale multi-iteration loss that closely supervises each iteration on each scale, while allowing to discard particularly difficult samples. In combination with an appropriate mixed-dataset training strategy, our method performs favorably. It not only yields highly accurate results on the four major benchmarks (KITTI 2015, MPI Sintel, Middlebury and VIPER), it also allows to achieve these results with a single model and a single parameter setting. Our trained model and code are available at https://github.com/cv-stuttgart/MS_RAFT_plus .},
  archive      = {J_IJCV},
  author       = {Jahedi, Azin and Luz, Maximilian and Rivinius, Marc and Mehl, Lukas and Bruhn, Andrés},
  doi          = {10.1007/s11263-023-01930-7},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1835-1856},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MS-RAFT+: High resolution multi-scale RAFT},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast ultra high-definition video deblurring via multi-scale
separable network. <em>IJCV</em>, <em>132</em>(5), 1817–1834. (<a
href="https://doi.org/10.1007/s11263-023-01958-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant progress has been made in image and video deblurring, much less attention has been paid to process ultra high-definition (UHD) videos (e.g., 4K resolution). In this work, we propose a novel deep model for fast and accurate UHD video deblurring (UHDVD). The proposed UHDVD is achieved by a depth-wise separable-patch architecture, which operates with a multi-scale integration scheme to achieve a large receptive field without adding the number of generic convolutional layers and kernels. Additionally, we adopt the temporal feature attention module to effectively exploit the temporal correlation between video frames to obtain clearer recovered images. We design an asymmetrical encoder–decoder architecture with residual channel-spatial attention blocks to improve accuracy and reduce the depth of the network appropriately. Consequently, the proposed UHDVD achieves real-time performance on 4K videos at 30 fps. To train the proposed model, we build a new dataset comprised of 4K blurry videos and corresponding sharp frames using three different smartphones. Extensive experimental results show that our network performs favorably against the state-of-the-art methods on the proposed 4K dataset and existing 720p and 2K benchmarks in terms of accuracy, speed, and model size.},
  archive      = {J_IJCV},
  author       = {Ren, Wenqi and Deng, Senyou and Zhang, Kaihao and Song, Fenglong and Cao, Xiaochun and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-023-01958-9},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1817-1834},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast ultra high-definition video deblurring via multi-scale separable network},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Exploiting inter-sample affinity for knowability-aware
universal domain adaptation. <em>IJCV</em>, <em>132</em>(5), 1800–1816.
(<a href="https://doi.org/10.1007/s11263-023-01955-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal domain adaptation aims to transfer the knowledge of common classes from the source domain to the target domain without any prior knowledge on the label set, which requires distinguishing in the target domain the unknown samples from the known ones. Recent methods usually focused on categorizing a target sample into one of the source classes rather than distinguishing known and unknown samples, which ignores the inter-sample affinity between known and unknown samples, and may lead to suboptimal performance. Aiming at this issue, we propose a novel UniDA framework where such inter-sample affinity is exploited. Specifically, we introduce a knowability-based labeling scheme which can be divided into two steps: (1) Knowability-guided detection of known and unknown samples based on the intrinsic structure of the neighborhoods of samples, where we leverage the first singular vectors of the affinity matrix to obtain the knowability of every target sample. (2) Label refinement based on neighborhood consistency to relabel the target samples, where we refine the labels of each target sample based on its neighborhood consistency of predictions. Then, auxiliary losses based on the two steps are used to reduce the inter-sample affinity between the unknown and the known target samples. Finally, experiments on four public datasets demonstrate that our method significantly outperforms existing state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Wang, Yifan and Zhang, Lin and Song, Ran and Li, Hongliang and Rosin, Paul L. and Zhang, Wei},
  doi          = {10.1007/s11263-023-01955-y},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1800-1816},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploiting inter-sample affinity for knowability-aware universal domain adaptation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dynamic prototypes for visual pattern debiasing.
<em>IJCV</em>, <em>132</em>(5), 1777–1799. (<a
href="https://doi.org/10.1007/s11263-023-01956-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved great success in academic benchmarks but fails to work effectively in the real world due to the potential dataset bias. The current learning methods are prone to inheriting or even amplifying the bias present in a training dataset and under-represent specific demographic groups. More recently, some dataset debiasing methods have been developed to address the above challenges based on the awareness of protected or sensitive attribute labels. However, the number of protected or sensitive attributes may be considerably large, making it laborious and costly to acquire sufficient manual annotation. To this end, we propose a prototype-based network to dynamically balance the learning of different subgroups for a given dataset. First, an object pattern embedding mechanism is presented to make the network focus on the foreground region. Then we design a prototype learning method to discover and extract the visual patterns from the training data in an unsupervised way. The number of prototypes is dynamic depending on the pattern structure of the feature space. We evaluate the proposed prototype-based network on three widely used polyp segmentation datasets with abundant qualitative and quantitative experiments. Experimental results show that our proposed method outperforms the CNN-based and transformer-based state-of-the-art methods in terms of both effectiveness and fairness metrics. Moreover, extensive ablation studies are conducted to show the effectiveness of each proposed component and various parameter values. Lastly, we analyze how the number of prototypes grows during the training process and visualize the associated subgroups for each learned prototype. The code and data will be released at https://github.com/zijinY/dynamic-prototype-debiasing .},
  archive      = {J_IJCV},
  author       = {Liang, Kongming and Yin, Zijin and Min, Min and Liu, Yan and Ma, Zhanyu and Guo, Jun},
  doi          = {10.1007/s11263-023-01956-x},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1777-1799},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning dynamic prototypes for visual pattern debiasing},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Editor’s note: Special issue on BMVC 2021. <em>IJCV</em>,
<em>132</em>(5), 1776. (<a
href="https://doi.org/10.1007/s11263-023-01896-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-023-01896-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1776},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on BMVC 2021},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoCoNet: Coupled contrastive learning network with
multi-level feature ensemble for multi-modality image fusion.
<em>IJCV</em>, <em>132</em>(5), 1748–1775. (<a
href="https://doi.org/10.1007/s11263-023-01952-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion targets to provide an informative image by combining complementary information from different sensors. Existing learning-based fusion approaches attempt to construct various loss functions to preserve complementary features, while neglecting to discover the inter-relationship between the two modalities, leading to redundant or even invalid information on the fusion results. Moreover, most methods focus on strengthening the network with an increase in depth while neglecting the importance of feature transmission, causing vital information degeneration. To alleviate these issues, we propose a coupled contrastive learning network, dubbed CoCoNet, to realize infrared and visible image fusion in an end-to-end manner. Concretely, to simultaneously retain typical features from both modalities and to avoid artifacts emerging on the fused result, we develop a coupled contrastive constraint in our loss function. In a fused image, its foreground target/background detail part is pulled close to the infrared/visible source and pushed far away from the visible/infrared source in the representation space. We further exploit image characteristics to provide data-sensitive weights, allowing our loss function to build a more reliable relationship with source images. A multi-level attention module is established to learn rich hierarchical feature representation and to comprehensively transfer features in the fusion process. We also apply the proposed CoCoNet on medical image fusion of different types, e.g., magnetic resonance image, positron emission tomography image, and single photon emission computed tomography image. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) performance under both subjective and objective evaluation, especially in preserving prominent targets and recovering vital textural details.},
  archive      = {J_IJCV},
  author       = {Liu, Jinyuan and Lin, Runjia and Wu, Guanyao and Liu, Risheng and Luo, Zhongxuan and Fan, Xin},
  doi          = {10.1007/s11263-023-01952-1},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1748-1775},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CoCoNet: Coupled contrastive learning network with multi-level feature ensemble for multi-modality image fusion},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convex–concave tensor robust principal component analysis.
<em>IJCV</em>, <em>132</em>(5), 1721–1747. (<a
href="https://doi.org/10.1007/s11263-023-01960-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor robust principal component analysis (TRPCA) aims at recovering the underlying low-rank clean tensor and residual sparse component from the observed tensor. The recovery quality heavily depends on the definition of tensor rank which has diverse construction schemes. Recently, tensor average rank has been proposed and the tensor nuclear norm has been proven to be its best convex surrogate. Many improved works based on the tensor nuclear norm have emerged rapidly. Nevertheless, there exist three common drawbacks: (1) the neglect of consideration on relativity between the distribution of large singular values and low-rank constraint; (2) the prior assumption of equal treatment for frontal slices hidden in tensor nuclear norm; (3) the missing convergence of whole iteration sequences in optimization. To address these problems together, in this paper, we propose a convex–concave TRPCA method in which the notion of convex–convex singular value separation (CCSVS) plays a dominant role in the objective. It can adjust the distribution of the first several largest singular values with low-rank controlling in a relative way and emphasize the importance of frontal slices collaboratively. Remarkably, we provide the rigorous convergence analysis of whole iteration sequences in optimization. Besides, a low-rank tensor recovery guarantee is established for the proposed CCSVS model. Extensive experiments demonstrate that the proposed CCSVS significantly outperforms state-of-the-art methods over toy data and real-world datasets, and running time per image is also the fastest.},
  archive      = {J_IJCV},
  author       = {Liu, Youfa and Du, Bo and Chen, Yongyong and Zhang, Lefei and Gong, Mingming and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01960-1},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1721-1747},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Convex–Concave tensor robust principal component analysis},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating more pertinent captions by leveraging semantics
and style on multi-source datasets. <em>IJCV</em>, <em>132</em>(5),
1701–1720. (<a
href="https://doi.org/10.1007/s11263-023-01949-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the task of generating fluent descriptions by training on a non-uniform combination of data sources, containing both human-annotated and web-collected captions. Large-scale datasets with noisy image-text pairs, indeed, provide a sub-optimal source of supervision because of their low-quality descriptive style, while human-annotated datasets are cleaner but smaller in scale. To get the best of both worlds, we propose to leverage and separate semantics and descriptive style through the incorporation of a style token and keywords extracted through a retrieval component. The proposed model avoids the need of object detectors, is trained with a single objective of prompt language modeling, and can replicate the style of human-collected captions while training on sources with different input styles. Experimentally, the model shows a strong capability of recognizing real-world concepts and producing high-quality captions. Extensive experiments are performed on different image captioning datasets, including CC3M, nocaps, and the competitive COCO dataset, where our model consistently outperforms baselines and state-of-the-art approaches.},
  archive      = {J_IJCV},
  author       = {Cornia, Marcella and Baraldi, Lorenzo and Fiameni, Giuseppe and Cucchiara, Rita},
  doi          = {10.1007/s11263-023-01949-w},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1701-1720},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generating more pertinent captions by leveraging semantics and style on multi-source datasets},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware robust fine-tuning. <em>IJCV</em>,
<em>132</em>(5), 1685–1700. (<a
href="https://doi.org/10.1007/s11263-023-01951-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive language-image pre-trained (CLIP) models have zero-shot ability of classifying an image belonging to “ $$\mathtt {[CLASS]}$$ ” by using similarity between the image and the prompt sentence “a $$\mathtt {[CONTEXT]}$$ of $$\mathtt {[CLASS]}$$ ”. Based on exhaustive text cues in “ $$\mathtt {[CONTEXT]}$$ ”, CLIP model is aware of different contexts, e.g. background, style, viewpoint, and exhibits unprecedented robustness against a wide range of distribution shifts. However, recent works find further fine-tuning of CLIP models improves accuracy but sacrifices the robustness on downstream tasks. We conduct an empirical investigation to show fine-tuning will corrupt the context-aware ability of pre-trained CLIP features. To solve this problem, we propose Context-Aware Robust Fine-tuning (CAR-FT). CAR-FT regularizes the model during fine-tuning to capture the context information. Specifically, we use zero-shot prompt weights to get the context distribution contained in the image. By minimizing the Kullback–Leibler divergence (KLD) between context distributions induced by original/fine-tuned CLIP models, CAR-FT makes the context-aware ability of CLIP inherited into downstream tasks, and achieves both higher in-distribution (ID) and out-of-distribution (OOD) accuracy. The experimental results show CAR-FT achieves superior robustness on five OOD test datasets of ImageNet, and meanwhile brings accuracy gains on nine downstream tasks. Additionally, CAR-FT surpasses previous domain generalization (DG) methods and gets 78.5% averaged accuracy on DomainBed benchmark, building the new state-of-the-art.},
  archive      = {J_IJCV},
  author       = {Mao, Xiaofeng and Chen, Yufeng and Jia, Xiaojun and Zhang, Rong and Xue, Hui and Li, Zhao},
  doi          = {10.1007/s11263-023-01951-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1685-1700},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context-aware robust fine-tuning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BioDrone: A bionic drone-based single object tracking
benchmark for robust vision. <em>IJCV</em>, <em>132</em>(5), 1659–1684.
(<a href="https://doi.org/10.1007/s11263-023-01937-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation. The robustness of SOT faces two main challenges: tiny target and fast motion. These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera. To evaluate the robustness of SOT methods, we propose BioDrone—the first bionic drone-based visual benchmark for SOT. Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics. BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT. To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses. Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack Mayer et al. in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 13444–13454, 2021) for robust SOT. Our evaluation leads to new baselines and insights for robust SOT. Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision. The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com .},
  archive      = {J_IJCV},
  author       = {Zhao, Xin and Hu, Shiyu and Wang, Yipei and Zhang, Jing and Hu, Yimin and Liu, Rongshuai and Ling, Haibin and Li, Yin and Li, Renshu and Liu, Kun and Li, Jiadong},
  doi          = {10.1007/s11263-023-01937-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1659-1684},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BioDrone: A bionic drone-based single object tracking benchmark for robust vision},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatio-temporal robust tracker with spatial-channel
transformer and jitter suppression. <em>IJCV</em>, <em>132</em>(5),
1645–1658. (<a
href="https://doi.org/10.1007/s11263-023-01902-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of visual object tracking is reflected not only in the accuracy of the target localisation in every single frame, but also in the smoothness of the predicted motion of the tracked object across consecutive frames. From the perspective of appearance modelling, the success of the state-of-the-art Transformer-based trackers derives from their ability to adaptively associate the representations of related spatial regions. However, the absence of attention in the channel dimension hinders the realisation of their potential tracking capacity. To cope with the commonly occurring misalignment of the spatial scale between the template and a search patch, we propose a novel cross channel correlation mechanism. Accordingly, the relevance of multi-channel features in the channel Transformer is modelled using two different sources of information. The result is a novel spatial-channel Transformer, which integrates information conveyed by features along both, the spatial and channel directions. For temporal modelling, to quantify the temporal smoothness, we propose a jitter metric that measures the cross-frame variation of the predicted bounding boxes as a function of the parameters such as centre displacement, area, and aspect ratio. As the changes of an object between consecutive frames are limited, the proposed jitter loss can be used to monitor the temporal behaviour of the tracking results and penalise erroneus predictions during the training stage, thus enhancing the temporal stability of an appearance-based tracker. Extensive experiments on several well-known benchmarking datasets demonstrate the robustness of the proposed tracker.},
  archive      = {J_IJCV},
  author       = {Zhao, Shaochuan and Xu, Tianyang and Wu, Xiao-Jun and Kittler, Josef},
  doi          = {10.1007/s11263-023-01902-x},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1645-1658},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A spatio-temporal robust tracker with spatial-channel transformer and jitter suppression},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning framework for infrared and visible image
fusion without strict registration. <em>IJCV</em>, <em>132</em>(5),
1625–1644. (<a
href="https://doi.org/10.1007/s11263-023-01948-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, although significant progress has been made in infrared and visible image fusion, existing methods typically assume that the source images have been rigorously registered or aligned prior to image fusion. However, the difference in modalities of infrared and visible images poses a great challenge to achieve strict alignment automatically, affecting the quality of the subsequent fusion procedure. To address this problem, this paper proposes a deep learning framework for misaligned infrared and visible image fusion, aiming to free the fusion algorithm from strict registration. Technically, we design a convolutional neural network (CNN)-Transformer Hierarchical Interactive Embedding (CTHIE) module, which can combine the respective advantages of CNN and Transformer, to extract features from the source images. In addition, by characterizing the correlation between the features extracted from misaligned source images, a Dynamic Re-aggregation Feature Representation (DRFR) module is devised to align the features with a self-attention-based feature re-aggregation scheme. Finally, to effectively utilize the features at different levels of the network, a Fully Perceptual Forward Fusion (FPFF) module via interactive transmission of multi-modal features is introduced for feature fusion to reconstruct the fused image. Experimental results on both synthetic and real-world data demonstrate the effectiveness of the proposed method, verifying the feasibility of directly fusing infrared and visible images without strict registration.},
  archive      = {J_IJCV},
  author       = {Li, Huafeng and Liu, Junyu and Zhang, Yafei and Liu, Yu},
  doi          = {10.1007/s11263-023-01948-x},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1625-1644},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A deep learning framework for infrared and visible image fusion without strict registration},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive study of the robustness for LiDAR-based 3D
object detectors against adversarial attacks. <em>IJCV</em>,
<em>132</em>(5), 1592–1624. (<a
href="https://doi.org/10.1007/s11263-023-01934-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed significant advancements in deep learning-based 3D object detection, leading to its widespread adoption in numerous applications. As 3D object detectors become increasingly crucial for security-critical tasks, it is imperative to understand their robustness against adversarial attacks. This paper presents the first comprehensive evaluation and analysis of the robustness of LiDAR-based 3D detectors under adversarial attacks. Specifically, we extend three distinct adversarial attacks to the 3D object detection task, benchmarking the robustness of state-of-the-art LiDAR-based 3D object detectors against attacks on the KITTI and Waymo datasets. We further analyze the relationship between robustness and detector properties. Additionally, we explore the transferability of cross-model, cross-task, and cross-data attacks. Thorough experiments on defensive strategies for 3D detectors are conducted, demonstrating that simple transformations like flipping provide little help in improving robustness when the applied transformation strategy is exposed to attackers. Finally, we propose balanced adversarial focal training, based on conventional adversarial training, to strike a balance between accuracy and robustness. Our findings will facilitate investigations into understanding and defending against adversarial attacks on LiDAR-based 3D object detectors, thus advancing the field. The source code is publicly available at https://github.com/Eaphan/Robust3DOD .},
  archive      = {J_IJCV},
  author       = {Zhang, Yifan and Hou, Junhui and Yuan, Yixuan},
  doi          = {10.1007/s11263-023-01934-3},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1592-1624},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive study of the robustness for LiDAR-based 3D object detectors against adversarial attacks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagram perception networks for textbook question answering
via joint optimization. <em>IJCV</em>, <em>132</em>(5), 1578–1591. (<a
href="https://doi.org/10.1007/s11263-023-01954-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook question answering requires a system to answer questions with or without diagrams accurately, given multimodal contexts that include rich paragraphs and diagrams. Existing methods usually utilize a pipelined way to extract the most relevant paragraph from multimodal contexts and only employ convolutional neural networks to comprehend diagram semantics under the supervision of answer labels. The former will result in error accumulation, while the latter will lead to poor diagram understanding. To provide a remedy for the above issues, we propose an end-to-end DIagraM Perception network for textbook question answering (DIMP), which is jointly optimized by the supervision of relation predicting, diagram classification, and question answering. Specifically, knowledge extracting is regarded as a sequence classification task and optimized through the supervision of answer labels to alleviate error accumulation. To capture diagram semantics effectively, DIMP uses an explicit relation-aware method that first parses a diagram into several graphs under specific relations and then grasps the information propagation within them. Evaluation on two benchmark datasets shows that our method achieves competitive or better results without large data pre-training and constructing auxiliary tasks compared with current state-of-the-art methods. We provide comprehensive ablation studies and thorough analyses to determine what factors contribute to this success. We also make in-depth analyses for relational graph learning and joint optimization.},
  archive      = {J_IJCV},
  author       = {Ma, Jie and Liu, Jun and Chai, Qi and Wang, Pinghui and Tao, Jing},
  doi          = {10.1007/s11263-023-01954-z},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1578-1591},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Diagram perception networks for textbook question answering via joint optimization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust unpaired image dehazing via density and depth
decomposition. <em>IJCV</em>, <em>132</em>(5), 1557–1577. (<a
href="https://doi.org/10.1007/s11263-023-01940-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the overfitting issue of dehazing models trained on synthetic hazy-clean image pairs, recent methods attempt to boost the generalization ability by training on unpaired data. However, most of existing approaches simply resort to formulating dehazing–rehazing cycles with generative adversarial networks, yet ignore the physical property in the real-world hazy environment, i.e., the haze effect varies along with density and depth. This paper proposes a robust self-augmented image dehazing framework for haze generation and removal. Instead of merely estimating transmission maps or clean content, the proposed scheme focuses on exploring the scattering coefficient and depth information of hazy and clean images. Having the scene depth estimated, our method is capable of re-rendering hazy images with different thicknesses, which benefits the training of the dehazing network. Besides, a dual contrastive perceptual loss is introduced to further improve the quality of both dehazed and rehazed images. Comprehensive experiments are conducted to reveal the advance of our method over other state-of-the-art unpaired dehazing methods in terms of visual quality, model size, and computational cost. Moreover, our model can be robustly trained on, not only synthetic indoor datasets, but also real outdoor scenes with remarkable improvement on the real-world image dehazing. Our code and training data are available at: https://github.com/YaN9-Y/D4_plus .},
  archive      = {J_IJCV},
  author       = {Yang, Yang and Wang, Chaoyue and Guo, Xiaojie and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01940-5},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1557-1577},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust unpaired image dehazing via density and depth decomposition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mimic before reconstruct: Enhancing masked autoencoders with
feature mimicking. <em>IJCV</em>, <em>132</em>(5), 1546–1556. (<a
href="https://doi.org/10.1007/s11263-023-01898-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked Autoencoders (MAE) have been popular paradigms for large-scale vision representation pre-training. However, MAE solely reconstructs the low-level RGB signals after the decoder and lacks supervision upon high-level semantics for the encoder, thus suffering from sub-optimal learned representations and long pre-training epochs. To alleviate this, previous methods simply replace the pixel reconstruction targets of 75% masked tokens by encoded features from pre-trained image-image (DINO) or image-language (CLIP) contrastive learning. Different from those efforts, we propose to Mimic before Reconstruct for Masked Autoencoders, named as MR-MAE, which jointly learns high-level and low-level representations without interference during pre-training. For high-level semantics, MR-MAE employs a mimic loss over 25% visible tokens from the encoder to capture the pre-trained patterns encoded in CLIP and DINO. For low-level structures, we inherit the reconstruction loss in MAE to predict RGB pixel values for 75% masked tokens after the decoder. As MR-MAE applies high-level and low-level targets respectively at different partitions, the learning conflicts between them can be naturally overcome and contribute to superior visual representations for various downstream tasks. On ImageNet-1K, the MR-MAE base pre-trained for only 400 epochs achieves 85.8% top-1 accuracy after fine-tuning, surpassing the 1600-epoch MAE base by $$+2.2$$ % and the previous state-of-the-art BEiT V2 base by $$+0.3$$ %. Pretrained checkpoints are released at https://github.com/Alpha-VL/ConvMAE .},
  archive      = {J_IJCV},
  author       = {Gao, Peng and Lin, Ziyi and Zhang, Renrui and Fang, Rongyao and Li, Hongyang and Li, Hongsheng and Qiao, Yu},
  doi          = {10.1007/s11263-023-01898-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1546-1556},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mimic before reconstruct: Enhancing masked autoencoders with feature mimicking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal representations: A unified look at multiple task
and domain learning. <em>IJCV</em>, <em>132</em>(5), 1521–1545. (<a
href="https://doi.org/10.1007/s11263-023-01931-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified look at jointly learning multiple vision tasks and visual domains through universal representations, a single deep neural network. Learning multiple problems simultaneously involves minimizing a weighted sum of multiple loss functions with different magnitudes and characteristics and thus results in unbalanced state of one loss dominating the optimization and poor results compared to learning a separate model for each problem. To this end, we propose distilling knowledge of multiple task/domain-specific networks into a single deep neural network after aligning its representations with the task/domain-specific ones through small capacity adapters. We rigorously show that universal representations achieve state-of-the-art performances in learning of multiple dense prediction problems in NYU-v2 and Cityscapes, multiple image classification problems from diverse domains in Visual Decathlon Dataset and cross-domain few-shot learning in MetaDataset. Finally we also conduct multiple analysis through ablation and qualitative studies.},
  archive      = {J_IJCV},
  author       = {Li, Wei-Hong and Liu, Xialei and Bilen, Hakan},
  doi          = {10.1007/s11263-023-01931-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1521-1545},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Universal representations: A unified look at multiple task and domain learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAE-GReaT: Convolutional-auxiliary efficient graph reasoning
transformer for dense image predictions. <em>IJCV</em>, <em>132</em>(5),
1502–1520. (<a
href="https://doi.org/10.1007/s11263-023-01928-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) are two primary frameworks for current semantic image recognition tasks in the community of computer vision. The general consensus is that both CNNs and ViT have their latent strengths and weaknesses, e.g., CNNs are good at extracting local features but difficult to aggregate long-range feature dependencies, while ViT is good at aggregating long-range feature dependencies but poorly represents in local features. In this paper, we propose an auxiliary and integrated network architecture, named Convolutional-Auxiliary Efficient Graph Reasoning Transformer (CAE-GReaT), which joints strengths of both CNNs and ViT into a uniform framework. CAE-GReaT stands on the shoulders of the advanced graph reasoning transformer and employs an internal auxiliary convolutional branch to enrich the local feature representations. Besides, to reduce the computational costs in graph reasoning, we also propose an efficient information diffusion strategy. Compared to the existing ViT models, CAE-GReaT not only has the advantage of a purposeful interaction pattern (via the graph reasoning branch), but also can capture fine-grained heterogeneous feature representations (via the auxiliary convolutional branch). Extensive experiments are implemented on three challenging dense image prediction tasks, i.e., semantic segmentation, instance segmentation, and panoptic segmentation. Results demonstrate that CAE-GReaT can achieve consistent performance gains on the state-of-the-art baselines with a slightly computational cost.},
  archive      = {J_IJCV},
  author       = {Zhang, Dong and Lin, Yi and Tang, Jinhui and Cheng, Kwang-Ting},
  doi          = {10.1007/s11263-023-01928-1},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1502-1520},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CAE-GReaT: Convolutional-auxiliary efficient graph reasoning transformer for dense image predictions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot stereo matching with high domain adaptability based
on adaptive recursive network. <em>IJCV</em>, <em>132</em>(5),
1484–1501. (<a
href="https://doi.org/10.1007/s11263-023-01953-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based stereo matching algorithms have been extensively researched in areas such as robot vision and autonomous driving due to their promising performance. However, these algorithms require a large amount of labeled data for training and encounter inadequate domain adaptability, which degraded their applicability and flexibility. This work addresses the two deficiencies and proposes a few-shot trained stereo matching model with high domain adaptability. In the model, stereo matching is formulated as the problem of dynamic optimization in the possible solution space, and a multi-scale matching cost computation method is proposed to obtain the possible solution space for the application scenes. Moreover, an adaptive recurrent 3D convolutional neural network is designed to determine the optimal solution from the possible solution space. Experimental results demonstrate that the proposed model outperforms the state-of-the-art stereo matching algorithms in terms of training requirements and domain adaptability.},
  archive      = {J_IJCV},
  author       = {Wu, Rongcheng and Wang, Mingzhe and Li, Zhidong and Zhou, Jianlong and Chen, Fang and Wang, Xuan and Sun, Changming},
  doi          = {10.1007/s11263-023-01953-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1484-1501},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Few-shot stereo matching with high domain adaptability based on adaptive recursive network},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). FastTrack: A highly efficient and generic GPU-based
multi-object tracking method with parallel kalman filter. <em>IJCV</em>,
<em>132</em>(5), 1463–1483. (<a
href="https://doi.org/10.1007/s11263-023-01933-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kalman Filter based on uniform assumption has been a crucial motion estimation module in trackers. However, it has limitations in non-uniform motion modeling and computational efficiency when applied to large-scale object tracking scenarios. To address these issues, we propose a novel Parallel Kalman Filter (PKF), which simplifies conventional state variables to reduces computational load and enable effective non-uniform modeling. Within PKF, we propose a non-uniform formulation which models non-uniform motion as uniform motion by transforming the time interval $$\Delta t$$ from a constant into a variable related to displacement, and incorporate a deceleration strategy into the control-input model of the formulation to tackle the escape problem in Multi-Object Tracking (MOT); an innovative parallel computation method is also proposed, which transposes the computation graph of PKF from the matrix to the quadratic form, significantly reducing the computational load and facilitating parallel computation between distinct tracklets via CUDA, thus making the time consumption of PKF independent of the input tracklet scale, i.e., O(1). Based on PKF, we introduce Fast, the first fully GPU-based tracker paradigm, which significantly enhances tracking efficiency in large-scale object tracking scenarios; and FastTrack, the MOT system composed of Fast and a general detector, offering high efficiency and generality. Within FastTrack, Fast only requires bounding boxes with scores and class ids for a single association during one iteration, and introduces innovative GPU-based tracking modules, such as an efficient GPU 2D-array data structure for tracklet management, a novel cost matrix implemented in CUDA for automatic association priority determination, a new association metric called HIoU, and the first implementation of the Auction Algorithm in CUDA for the asymmetric assignment problem. Experiments show that the average time per iteration of PKF on a GTX 1080Ti is only 0.2 ms; Fast can achieve a real-time efficiency of 250FPS on a GTX 1080Ti and 42FPS even on a Jetson AGX Xavier, outperforming conventional CPU-based trackers. Concurrently, FastTrack demonstrates state-of-the-art performance on four public benchmarks, specifically MOT17, MOT20, KITTI, and DanceTrack, and attains the highest speed in large-scale tracking scenarios of MOT20.},
  archive      = {J_IJCV},
  author       = {Liu, Chongwei and Li, Haojie and Wang, Zhihui},
  doi          = {10.1007/s11263-023-01933-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1463-1483},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FastTrack: A highly efficient and generic GPU-based multi-object tracking method with parallel kalman filter},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards frame rate agnostic multi-object tracking.
<em>IJCV</em>, <em>132</em>(5), 1443–1462. (<a
href="https://doi.org/10.1007/s11263-023-01943-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. They are neither as flexible as humans nor well-matched to industrial scenarios which require the trackers to be frame rate insensitive in complicated conditions. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT. Besides providing simulations and evaluation metrics, we try to solve new challenges in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.},
  archive      = {J_IJCV},
  author       = {Feng, Weitao and Bai, Lei and Yao, Yongqiang and Yu, Fengwei and Ouyang, Wanli},
  doi          = {10.1007/s11263-023-01943-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {1443-1462},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards frame rate agnostic multi-object tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting across domains via target-oriented transferable
semantic augmentation under prototype constraint. <em>IJCV</em>,
<em>132</em>(4), 1417–1441. (<a
href="https://doi.org/10.1007/s11263-023-01944-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for reducing label annotation cost and adapting to new data distributions gives rise to the emergence of domain adaptation (DA). DA aims to learn a model that performs well on the unlabeled or scarcely labeled target domain by transferring the rich knowledge from a related and well-annotated source domain. Existing DA methods mainly resort to learning domain-invariant representations with a source-supervised classifier shared by two domains. However, such a shared classifier may bias towards source domain, limiting its generalization capability on target data. To alleviate this issue, we present a target-oriented transferable semantic augmentation (T $$^2$$ SA) method, which enhances the generalization ability of the classifier by training it with a target-like augmented domain, constructed by semantically augmenting source data towards target at the feature level in an implicit manner. Specifically, to equip the augmented domain with target semantics, we delicately design a class-wise multivariate normal distribution based on the statistics estimated from features to sample the transformation directions for source data. Moreover, we achieve the augmentation implicitly by minimizing the upper bound of the expected Angular-softmax loss over the augmented domain, which is of high efficiency. Additionally, to further ensure that the augmented domain can imitate target domain nicely and discriminatively, the prototype constraint is enforced on augmented features class-wisely, which minimizes the expected distance between augmented features and corresponding target prototype (i.e., average representation) in Euclidean space. As a general technique, T $$^2$$ SA can be easily plugged into various DA methods to further boost their performances. Extensive experiments under single-source DA, multi-source DA and domain generalization scenarios validate the efficacy of T $$^2$$ SA.},
  archive      = {J_IJCV},
  author       = {Xie, Mixue and Li, Shuang and Gong, Kaixiong and Wang, Yulin and Huang, Gao},
  doi          = {10.1007/s11263-023-01944-1},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1417-1441},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adapting across domains via target-oriented transferable semantic augmentation under prototype constraint},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PartCom: Part composition learning for 3D open-set
recognition. <em>IJCV</em>, <em>132</em>(4), 1393–1416. (<a
href="https://doi.org/10.1007/s11263-023-01947-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address 3D open-set recognition (OSR) that can recognize known classes as well as be aware of unknown classes during testing. The key challenge of 3D OSR is that unknown objects are not available during training and 3D closed set recognition methods trained on known classes usually classify an unknown object as a known one with high confidence. This over-confidence is mainly due to the fact that local part information in 3D shapes provides the main evidence for known class recognition, which nevertheless leads to the incorrect recognition of unknown classes that have similar local parts but arranged very differently. To address this problem, we propose PartCom, a 3D OSR method that calls attention to not only part information but also the part composition that is unique to each class. PartCom uses a part codebook to learn the different parts across object classes, and represents part composition as a latent distribution over the codebook. In this way, both known classes and unknown classes are cast into the space of learned parts, but known classes have composites largely distinguished from unknown ones, which enables OSR. To learn the part codebook, we formulate two necessary constraints to ensure the part codebook encodes diverse parts of different classes compactly and efficiently. In addition, we propose an optional augmenting module of Part-aware Unknown feaTure Synthesis, that further reduces open-set misclassification risks by synthesizing novel part compositions to be regarded as unknown classes. This synthesis is simply achieved by mixing part codes of different classes; training with such augmented data makes classifiers’ decision boundaries more closely fit the known classes and therefore improves open-set recognition. To evaluate the proposed method, we construct four 3D OSR tasks based on datasets of CAD shapes, multi-view scanned shapes, and LiDAR scanned shapes. Extensive experiments show that our method achieves significantly superior results than SOTA baselines on all tasks.},
  archive      = {J_IJCV},
  author       = {Weng, Tingyu and Xiao, Jun and Pan, Hao and Jiang, Haiyong},
  doi          = {10.1007/s11263-023-01947-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1393-1416},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PartCom: Part composition learning for 3D open-set recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image and object geo-localization. <em>IJCV</em>,
<em>132</em>(4), 1350–1392. (<a
href="https://doi.org/10.1007/s11263-023-01942-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of geo-localization broadly refers to the process of determining an entity’s geographical location, typically in the form of Global Positioning System (GPS) coordinates. The entity of interest may be an image, a sequence of images, a video, a satellite image, or even objects visible within the image. Recently, massive datasets of GPS-tagged media have become available due to smartphones and the internet, and deep learning has risen to prominence and enhanced the performance capabilities of machine learning models. These developments have enabled the rise of image and object geo-localization, which has impacted a wide range of applications such as augmented reality, robotics, self-driving vehicles, road maintenance, and 3D reconstruction. This paper provides a comprehensive survey of visual geo-localization, which may involve either determining the location at which an image has been captured (image geo-localization) or geolocating objects within an image (object geo-localization). We will provide an in-depth study of visual geo-localization including a summary of popular algorithms, a description of proposed datasets, and an analysis of performance results to illustrate the current state of the field.},
  archive      = {J_IJCV},
  author       = {Wilson, Daniel and Zhang, Xiaohan and Sultani, Waqas and Wshah, Safwan},
  doi          = {10.1007/s11263-023-01942-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1350-1392},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image and object geo-localization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CCR: Facial image editing with continuity, consistency and
reversibility. <em>IJCV</em>, <em>132</em>(4), 1336–1349. (<a
href="https://doi.org/10.1007/s11263-023-01938-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three problems exist in sequential facial image editing: discontinuous editing, inconsistent editing, and irreversible editing. Discontinuous editing is that the current editing can not retain the previously edited attributes. Inconsistent editing is that swapping the attribute editing orders can not yield the same results. Irreversible editing means that operating on a facial image is irreversible, especially in sequential facial image editing. In this work, we put forward three concepts and their corresponding definitions: editing continuity, consistency, and reversibility. Note that continuity refers to the continuity of attributes, that is, attributes can be continuously edited on any face. Consistency is that not only attributes meet continuity, but also facial identity needs to be consistent. To do so, we propose a novel model to achieve the goal of editing continuity, consistency, and reversibility. Furthermore, a sufficient criterion is defined to determine whether a model is continuous, consistent, and reversible. Extensive qualitative and quantitative experimental results validate our proposed model, and show that a continuous, consistent and reversible editing model has a more flexible editing function while preserving facial identity. We believe that our proposed definitions and model will have wide and promising applications in multimedia processing. Code and data are available at https://github.com/mickoluan/CCR.},
  archive      = {J_IJCV},
  author       = {Yang, Nan and Luan, Xin and Jia, Huidi and Han, Zhi and Li, Xiaofeng and Tang, Yandong},
  doi          = {10.1007/s11263-023-01938-z},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1336-1349},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CCR: Facial image editing with continuity, consistency and reversibility},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning robust multi-scale representation for neural
radiance fields from unposed images. <em>IJCV</em>, <em>132</em>(4),
1310–1335. (<a
href="https://doi.org/10.1007/s11263-023-01936-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an improved solution to the neural image-based rendering problem in computer vision. Given a set of images taken from a freely moving camera at train time, the proposed approach could synthesize a realistic image of the scene from a novel viewpoint at test time. The key ideas presented in this paper are (i) Recovering accurate camera parameters via a robust pipeline from unposed day-to-day images is equally crucial in neural novel view synthesis problem; (ii) It is rather more practical to model object’s content at different resolutions since dramatic camera motion is highly likely in day-to-day unposed images. To incorporate the key ideas, we leverage the fundamentals of scene rigidity, multi-scale neural scene representation, and single-image depth prediction. Concretely, the proposed approach makes the camera parameters as learnable in a neural fields-based modeling framework. By assuming per view depth prediction is given up to scale, we constrain the relative pose between successive frames. From the relative poses, absolute camera pose estimation is modeled via a graph-neural network-based multiple motion averaging within the multi-scale neural-fields network, leading to a single loss function. Optimizing the introduced loss function provides camera intrinsic, extrinsic, and image rendering from unposed images. We demonstrate, with examples, that for a unified framework to accurately model multiscale neural scene representation from day-to-day acquired unposed multi-view images, it is equally essential to have precise camera-pose estimates within the scene representation framework. Without considering robustness measures in the camera pose estimation pipeline, modeling for multi-scale aliasing artifacts can be counterproductive. We present extensive experiments on several benchmark datasets to demonstrate the suitability of our approach.},
  archive      = {J_IJCV},
  author       = {Jain, Nishant and Kumar, Suryansh and Van Gool, Luc},
  doi          = {10.1007/s11263-023-01936-1},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1310-1335},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning robust multi-scale representation for neural radiance fields from unposed images},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Going deeper into recognizing actions in dark environments:
A comprehensive benchmark study. <em>IJCV</em>, <em>132</em>(4),
1292–1309. (<a
href="https://doi.org/10.1007/s11263-023-01932-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While action recognition (AR) has gained large improvements with the introduction of large-scale video datasets and the development of deep neural networks, AR models robust to challenging environments in real-world scenarios are still under-explored. We focus on the task of action recognition in dark environments, which can be applied to fields such as surveillance and autonomous driving at night. Intuitively, current deep networks along with visual enhancement techniques should be able to handle AR in dark environments, however, it is observed that this is not always the case in practice. To dive deeper into exploring solutions for AR in dark environments, we launched the $$\hbox {UG}^{2}{+}$$ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and advancing the robustness of AR models in dark environments. The challenge builds and expands on top of a novel ARID dataset, the first dataset for the task of dark video AR, and guides models to tackle such a task in both fully and semi-supervised manners. Baseline results utilizing current AR models and enhancement methods are reported, justifying the challenging nature of this task with substantial room for improvements. Thanks to the active participation from the research community, notable advances have been made in participants’ solutions, while analysis of these solutions helped better identify possible directions to tackle the challenge of AR in dark environments.},
  archive      = {J_IJCV},
  author       = {Xu, Yuecong and Cao, Haozhi and Yin, Jianxiong and Chen, Zhenghua and Li, Xiaoli and Li, Zhengguo and Xu, Qianwen and Yang, Jianfei},
  doi          = {10.1007/s11263-023-01932-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1292-1309},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Going deeper into recognizing actions in dark environments: A comprehensive benchmark study},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harmonizing base and novel classes: A class-contrastive
approach for generalized few-shot segmentation. <em>IJCV</em>,
<em>132</em>(4), 1277–1291. (<a
href="https://doi.org/10.1007/s11263-023-01939-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for few-shot segmentation (FSSeg) have mainly focused on improving the performance of novel classes while neglecting the performance of base classes. To overcome this limitation, the task of generalized few-shot semantic segmentation (GFSSeg) has been introduced, aiming to predict segmentation masks for both base and novel classes. However, the current prototype-based methods do not explicitly consider the relationship between base and novel classes when updating prototypes, leading to a limited performance in identifying true categories. To address this challenge, we propose a class contrastive loss and a class relationship loss to regulate prototype updates and encourage a large distance between prototypes from different classes, thus distinguishing the classes from each other while maintaining the performance of the base classes. Our proposed approach achieves new state-of-the-art performance for the generalized few-shot segmentation task on PASCAL VOC and MS COCO datasets.},
  archive      = {J_IJCV},
  author       = {Liu, Weide and Wu, Zhonghua and Zhao, Yang and Fang, Yuming and Foo, Chuan-Sheng and Cheng, Jun and Lin, Guosheng},
  doi          = {10.1007/s11263-023-01939-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1277-1291},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Harmonizing base and novel classes: A class-contrastive approach for generalized few-shot segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal object detection with large vision model.
<em>IJCV</em>, <em>132</em>(4), 1258–1276. (<a
href="https://doi.org/10.1007/s11263-023-01929-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, there has been growing interest in developing a broad, universal, and general-purpose computer vision system. Such systems have the potential to address a wide range of vision tasks simultaneously, without being limited to specific problems or data domains. This universality is crucial for practical, real-world computer vision applications. In this study, our focus is on a specific challenge: the large-scale, multi-domain universal object detection problem, which contributes to the broader goal of achieving a universal vision system. This problem presents several intricate challenges, including cross-dataset category label duplication, label conflicts, and the necessity to handle hierarchical taxonomies. To address these challenges, we introduce our approach to label handling, hierarchy-aware loss design, and resource-efficient model training utilizing a pre-trained large vision model. Our method has demonstrated remarkable performance, securing a prestigious second-place ranking in the object detection track of the Robust Vision Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection benchmark. We believe that our comprehensive study will serve as a valuable reference and offer an alternative approach for addressing similar challenges within the computer vision community. The source code for our work is openly available at https://github.com/linfeng93/Large-UniDet .},
  archive      = {J_IJCV},
  author       = {Lin, Feng and Hu, Wenze and Wang, Yaowei and Tian, Yonghong and Lu, Guangming and Chen, Fanglin and Xu, Yong and Wang, Xiaoyu},
  doi          = {10.1007/s11263-023-01929-0},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1258-1276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Universal object detection with large vision model},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascaded iterative transformer for jointly predicting facial
landmark, occlusion probability and head pose. <em>IJCV</em>,
<em>132</em>(4), 1242–1257. (<a
href="https://doi.org/10.1007/s11263-023-01935-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landmark detection under large pose with occlusion has been one of the challenging problems in the field of facial analysis. Recently, many works have predicted pose or occlusion together in the multi-task learning (MTL) paradigm, trying to tap into their dependencies and thus alleviate this issue. However, such implicit dependencies are weakly interpretable and inconsistent with the way humans exploit inter-task coupling relations, i.e., accommodating the induced explicit effects. This is one of the essentials that hinders their performance. To this end, in this paper, we propose a Cascaded Iterative Transformer (CIT) to jointly predict facial landmark, occlusion probability, and pose. The proposed CIT, besides implicitly mining task dependencies in a shared encoder, innovatively employs a cost-effective and portability-friendly strategy to pass the decoders’ predictions as prior knowledge to human-like exploit the coupling-induced effects. Moreover, to the best of our knowledge, no dataset contains all these task annotations simultaneously, so we introduce a new dataset termed MERL-RAV-FLOP based on the MERL-RAV dataset. We conduct extensive experiments on several challenging datasets (300W-LP, AFLW2000-3D, BIWI, COFW, and MERL-RAV-FLOP) and achieve remarkable results. The code and dataset can be accessed in https://github.com/Iron-LYK/CIT .},
  archive      = {J_IJCV},
  author       = {Li, Yaokun and Tan, Guang and Gou, Chao},
  doi          = {10.1007/s11263-023-01935-2},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1242-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cascaded iterative transformer for jointly predicting facial landmark, occlusion probability and head pose},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton ground truth extraction: Methodology, annotation
tool and benchmarks. <em>IJCV</em>, <em>132</em>(4), 1219–1241. (<a
href="https://doi.org/10.1007/s11263-023-01926-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.},
  archive      = {J_IJCV},
  author       = {Yang, Cong and Indurkhya, Bipin and See, John and Gao, Bo and Ke, Yan and Boukhers, Zeyd and Yang, Zhenyu and Grzegorzek, Marcin},
  doi          = {10.1007/s11263-023-01926-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1219-1241},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Skeleton ground truth extraction: Methodology, annotation tool and benchmarks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning portrait drawing with unsupervised parts.
<em>IJCV</em>, <em>132</em>(4), 1205–1218. (<a
href="https://doi.org/10.1007/s11263-023-01927-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating face photos into portrait drawings takes hours for a skilled artist which makes automatic generation of them desirable. Portrait drawing is a difficult image translation task with its own unique challenges. It requires emphasizing important key features of faces as well as ignoring many details of them. Therefore, an image translator should have the capacity to detect facial features and output images with the selected content of the photo preserved. In this work, we propose a method for portrait drawing that only learns from unpaired data with no additional labels. Our method via unsupervised feature learning shows good domain generalization behavior. Our first contribution is an image translation architecture that combines the high-level understanding of images with unsupervised parts and the identity preservation behavior of shallow networks. Our second contribution is a novel asymmetric pose-based cycle consistency loss. This loss relaxes the constraint on the cycle consistency loss which requires an input image to be reconstructed after transformations to a portrait and back to the input image. However, going from an RGB image to a portrait, information loss is expected (e.g. colors, background). This is what cycle consistency constraint tries to prevent and when applied to this scenario, results in learning a translation network that embeds the overall information of RGB images into portraits and causes artifacts in portrait images. Our proposed loss solves this issue. Lastly, we run extensive experiments both on in-domain and out-of-domain images and compare our method with state-of-the-art approaches. We show significant improvements both quantitatively and qualitatively on three datasets.},
  archive      = {J_IJCV},
  author       = {Tasdemir, Burak and Gudukbay, Mustafa Goktan and Eldenk, Dogac and Meric, Adil and Dundar, Aysegul},
  doi          = {10.1007/s11263-023-01927-2},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1205-1218},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning portrait drawing with unsupervised parts},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local compressed video stream learning for generic event
boundary detection. <em>IJCV</em>, <em>132</em>(4), 1187–1204. (<a
href="https://doi.org/10.1007/s11263-023-01921-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic event boundary detection aims to localize the generic, taxonomy-free event boundaries that segment videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which contains significant spatio-temporal redundancy and demands considerable computational power and storage space. To remedy these issues, we propose a novel compressed video representation learning method for event boundary detection that is fully end-to-end leveraging rich information in the compressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we use lightweight ConvNets to extract features of the P-frames in the GOPs and spatial-channel attention module (SCAM) is designed to refine the feature representations of the P-frames based on the compressed information with bidirectional information flow. To learn a suitable representation for boundary detection, we construct the local frames bag for each candidate frame and use the long short-term memory (LSTM) module to capture temporal relationships. We then compute frame differences with group similarities in the temporal domain. This module is only applied within a local window, which is critical for event boundary detection. Finally a simple classifier is used to determine the event boundaries of video sequences based on the learned feature representation. To remedy the ambiguities of annotations and speed up the training process, we use the Gaussian kernel to preprocess the ground-truth event boundaries. Extensive experiments conducted on the Kinetics-GEBD and TAPOS datasets demonstrate that the proposed method achieves considerable improvements compared to previous end-to-end approach while running at the same speed. The code is available at https://github.com/GX77/LCVSL .},
  archive      = {J_IJCV},
  author       = {Zhang, Libo and Gu, Xin and Li, Congcong and Luo, Tiejian and Fan, Heng},
  doi          = {10.1007/s11263-023-01921-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1187-1204},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Local compressed video stream learning for generic event boundary detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InstaFormer++: Multi-domain instance-aware image-to-image
translation with transformer. <em>IJCV</em>, <em>132</em>(4), 1167–1186.
(<a href="https://doi.org/10.1007/s11263-023-01866-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel Transformer-based network architecture for instance-aware image-to-image translation, dubbed InstaFormer, to effectively integrate global- and instance-level information. By considering extracted content features from an image as visual tokens, our model discovers global consensus of content features by considering context information through self-attention module of Transformers. By augmenting such tokens with an instance-level feature extracted from the content feature with respect to bounding box information, our framework is capable of learning an interaction between object instances and the global image, thus boosting the instance-awareness. We replace layer normalization (LayerNorm) in standard Transformers with adaptive instance normalization (AdaIN) to enable a multi-modal translation with style codes. In addition, to improve the instance-awareness and translation quality at object regions, we present an instance-level content contrastive loss defined between input and translated image. Although competitive performance can be attained by InstaFormer, it may face some limitations, i.e., limited scalability in handling multiple domains, and reliance on domain annotations. To overcome this, we propose InstaFormer++ as an extension of Instaformer, which enables multi-domain translation in instance-aware image translation for the first time. We propose to obtain pseudo domain label by leveraging a list of candidate domain labels in a text format and pretrained vision-language model. We conduct experiments to demonstrate the effectiveness of our methods over the latest methods and provide extensive ablation studies.},
  archive      = {J_IJCV},
  author       = {Kim, Soohyun and Baek, Jongbeom and Park, Jihye and Ha, Eunjae and Jung, Homin and Lee, Taeyoung and Kim, Seungryong},
  doi          = {10.1007/s11263-023-01866-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1167-1186},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {InstaFormer++: Multi-domain instance-aware image-to-image translation with transformer},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deeper analysis of volumetric relightable faces.
<em>IJCV</em>, <em>132</em>(4), 1148–1166. (<a
href="https://doi.org/10.1007/s11263-023-01899-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portrait viewpoint and illumination editing is an important problem with several applications in VR/AR, movies, and photography. Comprehensive knowledge of geometry and illumination is critical for obtaining photorealistic results. Current methods are unable to explicitly model in 3D while handling both viewpoint and illumination editing from a single image. In this paper, we propose VoRF, a novel approach that can take even a single portrait image as input and relight human heads under novel illuminations that can be viewed from arbitrary viewpoints. VoRF represents a human head as a continuous volumetric field and learns a prior model of human heads using a coordinate-based MLP with individual latent spaces for identity and illumination. The prior model is learned in an auto-decoder manner over a diverse class of head shapes and appearances, allowing VoRF to generalize to novel test identities from a single input image. Additionally, VoRF has a reflectance MLP that uses the intermediate features of the prior model for rendering One-Light-at-A-Time (OLAT) images under novel views. We synthesize novel illuminations by combining these OLAT images with target environment maps. Qualitative and quantitative evaluations demonstrate the effectiveness of VoRF for relighting and novel view synthesis, even when applied to unseen subjects under uncontrolled illumination. This work is an extension of Rao et al. (VoRF: Volumetric Relightable Faces 2022). We provide extensive evaluation and ablative studies of our model and also provide an application, where any face can be relighted using textual input.},
  archive      = {J_IJCV},
  author       = {Rao, Pramod and Mallikarjun, B. R. and Fox, Gereon and Weyrich, Tim and Bickel, Bernd and Pfister, Hanspeter and Matusik, Wojciech and Zhan, Fangneng and Tewari, Ayush and Theobalt, Christian and Elgharib, Mohamed},
  doi          = {10.1007/s11263-023-01899-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1148-1166},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A deeper analysis of volumetric relightable faces},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SegViT v2: Exploring efficient and continual semantic
segmentation with plain vision transformers. <em>IJCV</em>,
<em>132</em>(4), 1126–1147. (<a
href="https://doi.org/10.1007/s11263-023-01894-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the capability of plain Vision Transformers (ViTs) for semantic segmentation using the encoder–decoder framework and introduce SegViTv2. In this study, we introduce a novel Attention-to-Mask (ATM) module to design a lightweight decoder effective for plain ViT. The proposed ATM converts the global attention map into semantic masks for high-quality segmentation results. Our decoder outperforms popular decoder UPerNet using various ViT backbones while consuming only about $$5\%$$ of the computational cost. For the encoder, we address the concern of the relatively high computational cost in the ViT-based encoders and propose a Shrunk++ structure that incorporates edge-aware query-based down-sampling (EQD) and query-based up-sampling (QU) modules. The Shrunk++ structure reduces the computational cost of the encoder by up to $$50\%$$ while maintaining competitive performance. Furthermore, we propose to adapt SegViT for continual semantic segmentation, demonstrating nearly zero forgetting of previously learned knowledge. Experiments show that our proposed SegViTv2 surpasses recent segmentation methods on three popular benchmarks including ADE20k, COCO-Stuff-10k and PASCAL-Context datasets. The code is available through the following link: https://github.com/zbwxp/SegVit .},
  archive      = {J_IJCV},
  author       = {Zhang, Bowen and Liu, Liyang and Phan, Minh Hieu and Tian, Zhi and Shen, Chunhua and Liu, Yifan},
  doi          = {10.1007/s11263-023-01894-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1126-1147},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SegViT v2: Exploring efficient and continual semantic segmentation with plain vision transformers},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-aware soft prompting: Text-to-text optimization for
few- and zero-shot adaptation of v &amp;l models. <em>IJCV</em>,
<em>132</em>(4), 1108–1125. (<a
href="https://doi.org/10.1007/s11263-023-01904-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft prompt learning has emerged as a promising direction for adapting V &amp;L models to a downstream task using a few training examples. However, current methods significantly overfit the training data suffering from large accuracy degradation when tested on unseen classes from the same domain. In addition, all prior methods operate exclusively under the assumption that both vision and language data is present. To this end, we make the following 5 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we also propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) Moreover, we identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) Importantly, we show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Expanding for the first time the setting to language-only adaptation, (5) we present a novel zero-shot variant of LASP where no visual samples at all are available for the downstream task. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Finally, (c) we show that our zero-shot variant improves upon CLIP without requiring any extra data. Code will be made available.},
  archive      = {J_IJCV},
  author       = {Bulat, Adrian and Tzimiropoulos, Georgios},
  doi          = {10.1007/s11263-023-01904-9},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1108-1125},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Language-aware soft prompting: Text-to-text optimization for few- and zero-shot adaptation of v &amp;L models},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetry-aware neural architecture for embodied visual
navigation. <em>IJCV</em>, <em>132</em>(4), 1091–1107. (<a
href="https://doi.org/10.1007/s11263-023-01909-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing methods for addressing visual navigation employ deep reinforcement learning as the standard tool for the task. However, they tend to be vulnerable to statistical shifts between the training and test data, resulting in poor generalization over novel environments that are out-of-distribution from the training data. In this study, we attempt to improve the generalization ability by utilizing the inductive biases available for the task. Employing the active neural SLAM that learns policies with the advantage actor-critic method as the base framework, we first point out that the mappings represented by the actor and the critic should satisfy specific symmetries. We then propose a network design for the actor and the critic to inherently attain these symmetries. Specifically, we use G-convolution instead of the standard convolution and insert the semi-global polar pooling layer, which we newly design in this study, in the last section of the critic network. Our method can be integrated into existing methods that utilize intermediate goals and 2D occupancy maps. Experimental results show that our method improves generalization ability by a good margin over visual exploration and object goal navigation, which are two main embodied visual navigation tasks.},
  archive      = {J_IJCV},
  author       = {Liu, Shuang and Suganuma, Masanori and Okatani, Takayuki},
  doi          = {10.1007/s11263-023-01909-4},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1091-1107},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Symmetry-aware neural architecture for embodied visual navigation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIVOTrack: A novel dataset and baseline method for
cross-view multi-object tracking in DIVerse open scenes. <em>IJCV</em>,
<em>132</em>(4), 1075–1090. (<a
href="https://doi.org/10.1007/s11263-023-01922-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view multi-object tracking aims to link objects between frames and camera views with substantial overlaps. Although cross-view multi-object tracking has received increased attention in recent years, existing datasets still have several issues, including (1) missing real-world scenarios, (2) lacking diverse scenes, (3) containing a limited number of tracks, (4) comprising only static cameras, and (5) lacking standard benchmarks, which hinder the investigation and comparison of cross-view tracking methods. To solve the aforementioned issues, we introduce DIVOTrack: a new cross-view multi-object tracking dataset for DIVerse Open scenes with dense tracking pedestrians in realistic and non-experimental environments. Our DIVOTrack has fifteen distinct scenarios and 953 cross-view tracks, surpassing all cross-view multi-object tracking datasets currently available. Furthermore, we provide a novel baseline cross-view tracking method with a unified joint detection and cross-view tracking framework named CrossMOT, which learns object detection, single-view association, and cross-view matching with an all-in-one embedding model. Finally, we present a summary of current methodologies and a set of standard benchmarks with our DIVOTrack to provide a fair comparison and conduct a comprehensive analysis of current approaches and our proposed CrossMOT. The dataset and code are available at https://github.com/shengyuhao/DIVOTrack .},
  archive      = {J_IJCV},
  author       = {Hao, Shengyu and Liu, Peiyuan and Zhan, Yibing and Jin, Kaixun and Liu, Zuozhu and Song, Mingli and Hwang, Jenq-Neng and Wang, Gaoang},
  doi          = {10.1007/s11263-023-01922-7},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1075-1090},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DIVOTrack: A novel dataset and baseline method for cross-view multi-object tracking in DIVerse open scenes},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlowNAS: Neural architecture search for optical flow
estimation. <em>IJCV</em>, <em>132</em>(4), 1055–1074. (<a
href="https://doi.org/10.1007/s11263-023-01920-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent optical flow estimators usually employ deep models designed for image classification as the encoders for feature extraction and matching. However, those encoders developed for image classification may be sub-optimal for flow estimation. In contrast, the decoder design of optical flow estimators often requires meticulous design for flow estimation. The disconnect between the encoder and decoder could negatively affect optical flow estimation. To address this issue, we propose a neural architecture search method, FlowNAS, to automatically find the more suitable and stronger encoder architecture for existing flow decoders. We first design a suitable search space, including various convolutional operators, and construct a weight-sharing super-network for efficiently evaluating the candidate architectures. To better train the super-network, we present a Feature Alignment Distillation module that utilizes a well-trained flow estimator to guide the training of the super-network. Finally, a resource-constrained evolutionary algorithm is exploited to determine an optimal architecture (i.e., sub-network). Experimental results show that FlowNAS can be easily incorporated into existing flow estimators and achieves state-of-the-art performance with the trade-off between accuracy and efficiency. Furthermore, the encoder architecture discovered by FlowNAS with the weights inherited from the super-network achieves 4.67% F1-all error on KITTI, an 8.4% reduction of RAFT baseline, surpassing state-of-the-art handcrafted GMA and AGFlow models, while reducing the model complexity and latency. The source code and trained models will be released at https://github.com/VDIGPKU/FlowNAS .},
  archive      = {J_IJCV},
  author       = {Lin, Zhiwei and Liang, Tingting and Xiao, Taihong and Wang, Yongtao and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-023-01920-9},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1055-1074},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FlowNAS: Neural architecture search for optical flow estimation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general paradigm with detail-preserving conditional
invertible network for image fusion. <em>IJCV</em>, <em>132</em>(4),
1029–1054. (<a
href="https://doi.org/10.1007/s11263-023-01924-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning techniques for image fusion either learn image mapping (LIM) directly, which renders them ineffective at preserving details due to the equal consideration to each pixel, or learn detail mapping (LDM), which only attains a limited level of performance because only details are used for reasoning. The recent lossless invertible network (INN) has demonstrated its detail-preserving ability. However, the direct applicability of INN to the image fusion task is limited by the volume-preserving constraint. Additionally, there is the lack of a consistent detail-preserving image fusion framework to produce satisfactory outcomes. To this aim, we propose a general paradigm for image fusion based on a novel conditional INN (named DCINN). The DCINN paradigm has three core components: a decomposing module that converts image mapping to detail mapping; an auxiliary network (ANet) that extracts auxiliary features directly from source images; and a conditional INN (CINN) that learns the detail mapping based on auxiliary features. The novel design benefits from the advantages of INN, LIM, and LDM approaches while avoiding their disadvantages. Particularly, using INN to LDM can easily meet the volume-preserving constraint while still preserving details. Moreover, since auxiliary features serve as conditional features, the ANet allows for the use of more than just details for reasoning without compromising detail mapping. Extensive experiments on three benchmark fusion problems, i.e., pansharpening, hyperspectral and multispectral image fusion, and infrared and visible image fusion, demonstrate the superiority of our approach compared with recent state-of-the-art methods. The code is available at https://github.com/wwhappylife/DCINN},
  archive      = {J_IJCV},
  author       = {Wang, Wu and Deng, Liang-Jian and Ran, Ran and Vivone, Gemine},
  doi          = {10.1007/s11263-023-01924-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1029-1054},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A general paradigm with detail-preserving conditional invertible network for image fusion},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a unified network for robust monocular depth
estimation: Network architecture, training strategy and dataset.
<em>IJCV</em>, <em>132</em>(4), 1012–1028. (<a
href="https://doi.org/10.1007/s11263-023-01915-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust monocular depth estimation (MDE) aims at learning a unified model that works across diverse real-world scenes, which is an important and active topic in computer vision. In this paper, we present Megatron_RVC, our winning solution for the monocular depth challenge in the Robust Vision Challenge (RVC) 2022, where we tackle the challenging problem from three perspectives: network architecture, training strategy and dataset. In particular, we made three contributions towards robust MDE: (1) we built a neural network with high capacity to enable flexible and accurate monocular depth predictions, which contains dedicated components to provide content-aware embeddings and to improve the richness of the details; (2) we proposed a novel mixing training strategy to handle real-world images with different aspect ratios, resolutions and apply tailored loss functions based on the properties of their depth maps; (3) to train a unified network model that covers diverse real-world scenes, we used over 1 million images from different datasets. As of 3rd October 2022, our unified model ranked consistently first across three benchmarks (KITTI, MPI Sintel, and VIPER) among all participants.},
  archive      = {J_IJCV},
  author       = {Xiang, Mochu and Dai, Yuchao and Zhang, Feiyu and Shi, Jiawei and Tian, Xinyu and Zhang, Zhensong},
  doi          = {10.1007/s11263-023-01915-6},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1012-1028},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards a unified network for robust monocular depth estimation: Network architecture, training strategy and dataset},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on the promises and dangers
of large vision models. <em>IJCV</em>, <em>132</em>(4), 1009–1011. (<a
href="https://doi.org/10.1007/s11263-023-01941-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhou, Kaiyang and Liu, Ziwei and Zhai, Xiaohua and Li, Chunyuan and Saenko, Kate},
  doi          = {10.1007/s11263-023-01941-4},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1009-1011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on the promises and dangers of large vision models},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor obstacle discovery on reflective ground via monocular
camera. <em>IJCV</em>, <em>132</em>(3), 987–1007. (<a
href="https://doi.org/10.1007/s11263-023-01925-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual obstacle discovery is a key step towards autonomous navigation of indoor mobile robots. Successful solutions have many applications in multiple scenes. One of the exceptions is the reflective ground. In this case, the reflections on the floor resemble the true world, which confuses the obstacle discovery and leaves navigation unsuccessful. We argue that the key to this problem lies in obtaining discriminative features for reflections and obstacles. Note that obstacle and reflection can be separated by the ground plane in 3D space. With this observation, we firstly introduce a pre-calibration based ground detection scheme that uses robot motion to predict the ground plane. Due to the immunity of robot motion to reflection, this scheme avoids failed ground detection caused by reflection. Given the detected ground, we design a ground-pixel parallax to describe the location of a pixel relative to the ground. Based on this, a unified appearance-geometry feature representation is proposed to describe objects inside rectangular boxes. Eventually, based on segmenting by detection framework, an appearance-geometry fusion regressor is designed to utilize the proposed feature to discover the obstacles. It also prevents our model from concentrating too much on parts of obstacles instead of whole obstacles. For evaluation, we introduce a new dataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with various ground reflections, a total of more than 200 image sequences and 3400 RGB images. The pixel-wise annotations of ground and obstacle provide a comparison to our method and other methods. By reducing the misdetection of the reflection, the proposed approach outperforms others. The source code and the dataset will be available at https://github.com/xuefeng-cvr/IndoorObstacleDiscovery-RG},
  archive      = {J_IJCV},
  author       = {Xue, Feng and Chang, Yicong and Wang, Tianxi and Zhou, Yu and Ming, Anlong},
  doi          = {10.1007/s11263-023-01925-4},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {987-1007},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Indoor obstacle discovery on reflective ground via monocular camera},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring attention shifts for salient instance ranking.
<em>IJCV</em>, <em>132</em>(3), 964–986. (<a
href="https://doi.org/10.1007/s11263-023-01906-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system has limited capacity in simultaneously processing multiple visual inputs. Consequently, humans rely on shifting their attention from one location to another. When viewing an image of complex scenes, psychology studies and behavioural observations show that humans prioritise and sequentially shift attention among multiple visual stimuli. In this paper, we propose to predict the saliency rank of multiple objects by inferring human attention shift. We first construct a new large-scale salient object ranking dataset, with the saliency rank of objects defined by the order that an observer attends to these objects via attention shift. We then propose a new deep learning-based model to leverage both bottom-up and top-down attention mechanisms for saliency rank prediction. Our model includes three novel modules: Spatial Mask Module (SMM), Selective Attention Module (SAM) and Salient Instance Edge Module (SIEM). SMM integrates bottom-up and semantic object properties to enhance contextual object features, from which SAM learns the dependencies between object features and image features for saliency reasoning. SIEM is designed to improve segmentation of salient objects, which helps further improve their rank predictions. Experimental results show that our proposed network achieves state-of-the-art performances on the salient object ranking task across multiple datasets. Code and data are available at https://github.com/SirisAvishek/Attention_Shift_Ranks .},
  archive      = {J_IJCV},
  author       = {Siris, Avishek and Jiao, Jianbo and Tam, Gary K. L. and Xie, Xianghua and Lau, Rynson W. H.},
  doi          = {10.1007/s11263-023-01906-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {964-986},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Inferring attention shifts for salient instance ranking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D adversarial augmentations for robust out-of-domain
predictions. <em>IJCV</em>, <em>132</em>(3), 931–963. (<a
href="https://doi.org/10.1007/s11263-023-01914-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive experiments across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D semantic segmentation. Despite training on a standard single dataset, our approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data.},
  archive      = {J_IJCV},
  author       = {Lehner, Alexander and Gasperini, Stefano and Marcos-Ramiro, Alvaro and Schmidt, Michael and Navab, Nassir and Busam, Benjamin and Tombari, Federico},
  doi          = {10.1007/s11263-023-01914-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {931-963},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {3D adversarial augmentations for robust out-of-domain predictions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SOTVerse: A user-defined task space of single object
tracking. <em>IJCV</em>, <em>132</em>(3), 872–930. (<a
href="https://doi.org/10.1007/s11263-023-01908-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking (SOT) research falls into a cycle—trackers perform well on most benchmarks but quickly fail in challenging scenarios, causing researchers to doubt the insufficient data content and take more effort to construct larger datasets with more challenging situations. However, inefficient data utilization and limited evaluation methods more seriously hinder SOT research. The former causes existing datasets can not be exploited comprehensively, while the latter neglects challenging factors in the evaluation process. In this article, we systematize the representative benchmarks and form a single object tracking metaverse (SOTVerse)—a user-defined SOT task space to break through the bottleneck. We first propose a 3E Paradigm to describe tasks by three components (i.e., environment, evaluation, and executor). Then, we summarize task characteristics, clarify the organization standards, and construct SOTVerse with 12.56 million frames. Specifically, SOTVerse automatically labels challenging factors per frame, allowing users to generate user-defined spaces efficiently via construction rules. Besides, SOTVerse provides two mechanisms with new indicators and successfully evaluates trackers under various subtasks. Consequently, SOTVerse first provides a strategy to improve resource utilization in the computer vision area, making research more standardized. The SOTVerse, toolkit, evaluation server, and results are available at http://metaverse.aitestunion.com .},
  archive      = {J_IJCV},
  author       = {Hu, Shiyu and Zhao, Xin and Huang, Kaiqi},
  doi          = {10.1007/s11263-023-01908-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {872-930},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SOTVerse: A user-defined task space of single object tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In the eye of transformer: Global–local correlation for
egocentric gaze estimation and beyond. <em>IJCV</em>, <em>132</em>(3),
854–871. (<a href="https://doi.org/10.1007/s11263-023-01879-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting human’s gaze from egocentric videos serves as a critical role for human intention understanding in daily activities. In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel global–local correlation module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets – EGTEA Gaze + and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds the previous state-of-the-art model by a large margin. We also apply our model to a novel gaze saccade/fixation prediction task and the traditional action recognition problem. The consistent gains suggest the strong generalization capability of our model. We also provide additional visualizations to support our claim that global–local correlation serves a key representation for predicting gaze fixation from egocentric videos. More details can be found in our website ( https://bolinlai.github.io/GLC-EgoGazeEst ).},
  archive      = {J_IJCV},
  author       = {Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M.},
  doi          = {10.1007/s11263-023-01879-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {854-871},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In the eye of transformer: Global–Local correlation for egocentric gaze estimation and beyond},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Style-hallucinated dual consistency learning: A unified
framework for visual domain generalization. <em>IJCV</em>,
<em>132</em>(3), 837–853. (<a
href="https://doi.org/10.1007/s11263-023-01911-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain shift widely exists in the visual world, while modern deep neural networks commonly suffer from severe performance degradation under domain shift due to poor generalization ability, which limits real-world applications. The domain shift mainly lies in the limited source environmental variations and the large distribution gap between source and unseen target data. To this end, we propose a unified framework, Style-HAllucinated Dual consistEncy learning (SHADE), to handle such domain shift in various visual tasks. Specifically, SHADE is constructed based on two consistency constraints, Style Consistency (SC) and Retrospection Consistency (RC). SC enriches the source situations and encourages the model to learn consistent representation across style-diversified samples. RC leverages general visual knowledge to prevent the model from overfitting to source data and thus largely keeps the representation consistent between the source and general visual models. Furthermore, we present a novel style hallucination module (SHM) to generate style-diversified samples that are essential to consistency learning. SHM selects basis styles from the source distribution, enabling the model to dynamically generate diverse and realistic samples during training. Extensive experiments demonstrate that our versatile SHADE can significantly enhance the generalization in various visual recognition tasks, including image classification, semantic segmentation, and object detection, with different models, i.e., ConvNets and Transformer.},
  archive      = {J_IJCV},
  author       = {Zhao, Yuyang and Zhong, Zhun and Zhao, Na and Sebe, Nicu and Lee, Gim Hee},
  doi          = {10.1007/s11263-023-01911-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {837-853},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Style-hallucinated dual consistency learning: A unified framework for visual domain generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MixStyle neural networks for domain generalization and
adaptation. <em>IJCV</em>, <em>132</em>(3), 822–836. (<a
href="https://doi.org/10.1007/s11263-023-01913-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks do not generalize well to unseen data with domain shifts—a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervised domain generalization, and unsupervised domain adaptation. Our experiments show that MixStyle can significantly boost out-of-distribution generalization performance across a wide range of tasks including image recognition, instance retrieval and reinforcement learning. The source code is released at https://github.com/KaiyangZhou/mixstyle-release .},
  archive      = {J_IJCV},
  author       = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  doi          = {10.1007/s11263-023-01913-8},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {822-836},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MixStyle neural networks for domain generalization and adaptation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SignParser: An end-to-end framework for traffic sign
understanding. <em>IJCV</em>, <em>132</em>(3), 805–821. (<a
href="https://doi.org/10.1007/s11263-023-01912-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intelligent transportation systems, parsing traffic signs and transmitting traffic information to humans is an urgent need. However, despite the success achieved in the detection and recognition of low-level circular or triangular traffic signs, parsing the more complex and informative rectangular traffic signs remains unexplored and challenging. Our work is devoted to the topic called “Traffic Sign Understanding (TSU)”, which is aimed to parse various traffic signs and generate semantic descriptions for them. To achieve this goal, we propose an end-to-end framework that integrates component detection, content reasoning, and semantic description generation. The component detection module first detects initial components in the sign image. Then the content reasoning module acquires the detailed content of the sign, including final components, their relations, and layout category, which provide local and global information for the subsequent module. In the end, the semantic description generation module mines relational attributes and text semantic attributes from the preceding results, embeds them with the layout categories, and transforms them into semantic descriptions through a dynamic prediction transformer. The three modules are trained jointly in an end-to-end manner for optimizing the overall performance. This method achieves state-of-the-art performance not only in the final semantic description generation stage but also on multiple subtasks of the CASIA-Tencent CTSU Dataset. Abundant ablation experiments are provided to prove the effectiveness of this method.},
  archive      = {J_IJCV},
  author       = {Guo, Yunfei and Feng, Wei and Yin, Fei and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-023-01912-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {805-821},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SignParser: An end-to-end framework for traffic sign understanding},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepFTSG: Multi-stream asymmetric USE-net trellis encoders
with shared decoder feature fusion architecture for video motion
segmentation. <em>IJCV</em>, <em>132</em>(3), 776–804. (<a
href="https://doi.org/10.1007/s11263-023-01910-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminating salient moving objects against complex, cluttered backgrounds, with occlusions and challenging environmental conditions like weather and illumination, is essential for stateful scene perception in autonomous systems. We propose a novel deep architecture, named DeepFTSG, for robust moving object detection that incorporates single and multi-stream multi-channel USE-Net trellis asymmetric encoders extending U-Net with squeeze and excitation (SE) blocks and a single shared decoder network for fusing multiple motion and appearance cues. DeepFTSG is a deep learning based approach that builds upon our previous hand-engineered flux tensor split Gaussian (FTSG) change detection video analysis algorithm which won the CDNet CVPR Change Detection Workshop challenge competition. DeepFTSG generalizes much better than top-performing motion detection deep networks, such as the scene-dependent ensemble-based FgSegNet_v2, while using an order of magnitude fewer weights. Short-term motion and longer-term change cues are estimated using general-purpose unsupervised methods—flux tensor and multi-modal background subtraction, respectively. DeepFTSG was evaluated using the CDnet-2014 change detection challenge dataset, the largest change detection video sequence benchmark with 12.3 billion labeled pixels, and had an overall F-measure of 97%. We also evaluated the cross-dataset generalization capability of DeepFTSG trained solely on CDnet-2014 short video segments and then evaluated on unseen SBI-2015, LASIESTA and LaSOT benchmark videos. On the unseen SBI-2015 dataset, DeepFTSG had an F-measure accuracy of 87%, more than 30% higher compared to the top-performing deep network FgSegNet_v2 and outperforms the recently proposed KimHa method by 17%. On the unseen LASIESTA, DeepFTSG had an F-measure of 88% and outperformed the best recent deep learning method BSUV-Net2.0 by 3%. On the unseen LaSOT with axis-aligned bounding box ground-truth, network segmentation masks were converted to bounding boxes for evaluation, DeepFTSG had an F-Measure of 55%, outperforming KimHa method by 14% and FgSegNet_v2 by almost 1.5%. When a customized single DeepFTSG model is trained in a scene-dependent manner for comparison with state-of-the-art approaches, then DeepFTSG performs significantly better, reaching an F-Measure of 97% on SBI-2015 (+ 10%) and 99% on LASIESTA (+ 11%). The source code, pre-trained weights, and video demo for DeepFTSG are available at https://github.com/CIVA-Lab/DeepFTSG .},
  archive      = {J_IJCV},
  author       = {Rahmon, Gani and Palaniappan, Kannappan and Toubal, Imad Eddine and Bunyak, Filiz and Rao, Raghuveer and Seetharaman, Guna},
  doi          = {10.1007/s11263-023-01910-x},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {776-804},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DeepFTSG: Multi-stream asymmetric USE-net trellis encoders with shared decoder feature fusion architecture for video motion segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Background activation suppression for weakly supervised
object localization and semantic segmentation. <em>IJCV</em>,
<em>132</em>(3), 750–775. (<a
href="https://doi.org/10.1007/s11263-023-01919-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization and semantic segmentation aim to localize objects using only image-level labels. Recently, a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve pixel-level localization. While existing FPM-based methods use cross-entropy to evaluate the foreground prediction map and to guide the learning of the generator, this paper presents two astonishing experimental observations on the object localization learning process: For a trained network, as the foreground mask expands, (1) the cross-entropy converges to zero when the foreground mask covers only part of the object region. (2) The activation value continuously increases until the foreground mask expands to the object boundary. Therefore, to achieve a more effective localization performance, we argue for the usage of activation value to learn more object regions. In this paper, we propose a background activation suppression (BAS) method. Specifically, an activation map constraint module is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using foreground region guidance and area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. In addition, our method also achieves state-of-the-art weakly supervised semantic segmentation performance on the PASCAL VOC 2012 and MS COCO 2014 datasets. Code and models are available at https://github.com/wpy1999/BAS-Extension .},
  archive      = {J_IJCV},
  author       = {Zhai, Wei and Wu, Pingyu and Zhu, Kai and Cao, Yang and Wu, Feng and Zha, Zheng-Jun},
  doi          = {10.1007/s11263-023-01919-2},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {750-775},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Background activation suppression for weakly supervised object localization and semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCT: A simple baseline for parameter-efficient fine-tuning
via salient channels. <em>IJCV</em>, <em>132</em>(3), 731–749. (<a
href="https://doi.org/10.1007/s11263-023-01918-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision transformers have strong representations benefit to various downstream tasks. Recently many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called “Salient Channel Tuning&quot; (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780 $$\times $$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot learning surpass other PEFT methods with lower parameter costs, demonstrating our proposed tuning technique’s strong capability and effectiveness in the low-data regime. The code will be available at https://github.com/zhaohengyuan1/SCT.git},
  archive      = {J_IJCV},
  author       = {Zhao, Henry Hengyuan and Wang, Pichao and Zhao, Yuyang and Luo, Hao and Wang, Fan and Shou, Mike Zheng},
  doi          = {10.1007/s11263-023-01918-3},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {731-749},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SCT: A simple baseline for parameter-efficient fine-tuning via salient channels},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic image matting: General and specific semantics.
<em>IJCV</em>, <em>132</em>(3), 710–730. (<a
href="https://doi.org/10.1007/s11263-023-01907-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although conventional matting formulation can separate foreground from background in fractional occupancy which can be caused by highly transparent objects, complex foreground (e.g., net or tree), and objects containing very fine details (e.g., hairs), no previous work has attempted to reason the underlying causes of matting due to various foreground semantics in general. We show how to obtain better alpha mattes by incorporating into our framework semantic classification of matting regions. Specifically, we consider and learn 20 classes of general matting patterns, and propose to extend the conventional trimap to semantic trimap. The proposed semantic trimap can be obtained automatically through patch structure analysis within trimap regions. Meanwhile, we learn a multi-class discriminator to regularize the alpha prediction at semantic level, and content-sensitive weights to balance different regularization losses. Experiments on multiple benchmarks show that our method outperforms other methods benefit from such general alpha semantics and has achieved the most competitive state-of-the-art performance. We further explore the effectiveness of our method on specific semantics by specializing our method into human matting and transparent object matting. Experimental results on specific semantics demonstrate alpha matte semantic information can boost performance for not only general matting but also class-specific matting. Finally, we contribute a large-scale Semantic Image Matting Dataset constructed with careful consideration of data balancing across different semantic classes. Code and dataset are available in https://github.com/nowsyn/SIM .},
  archive      = {J_IJCV},
  author       = {Sun, Yanan and Tang, Chi-Keung and Tai, Yu-Wing},
  doi          = {10.1007/s11263-023-01907-6},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {710-730},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic image matting: General and specific semantics},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are vision transformers robust to spurious correlations?
<em>IJCV</em>, <em>132</em>(3), 689–709. (<a
href="https://doi.org/10.1007/s11263-023-01916-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks may be susceptible to learning spurious correlations that hold on average but not in atypical test samples. As with the recent emergence of vision transformer (ViT) models, it remains unexplored how spurious correlations are manifested in such architectures. In this paper, we systematically investigate the robustness of different transformer architectures to spurious correlations on three challenging benchmark datasets. Our study reveals that for transformers, larger models and more pre-training data significantly improve robustness to spurious correlations. Key to their success is the ability to generalize better from the examples where spurious correlations do not hold. Further, we perform extensive ablations and experiments to understand the role of the self-attention mechanism in providing robustness under spuriously correlated environments. We hope that our work will inspire future research on further understanding the robustness of ViT models to spurious correlations.},
  archive      = {J_IJCV},
  author       = {Ghosal, Soumya Suvra and Li, Yixuan},
  doi          = {10.1007/s11263-023-01916-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {689-709},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Are vision transformers robust to spurious correlations?},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal meta-transfer fusion network for few-shot 3D
model classification. <em>IJCV</em>, <em>132</em>(3), 673–688. (<a
href="https://doi.org/10.1007/s11263-023-01905-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, driven by the increasing concern on 3D techniques, resulting in the large-scale 3D data, 3D model classification has attracted enormous attention from both research and industry communities. Most of the current methods highly depend on sufficient labeled 3D models, which substantially restricts their scalability to novel classes with few annotated training data since it can increase the chance of overfitting. Besides, they only leverage single-modal information (either point cloud or multi-view information), and few works integrate these complementary information for 3D model representation. To overcome these problems, we propose a multi-modal meta-transfer fusion network (M $$^{3}$$ TF), the key of which is to perform few-shot multi-modal representation for 3D model classification. Specifically, we first convert the original 3D data into both multi-view and point cloud modalities, and pre-train individual encoding networks on a large-scale dataset to obtain the optimal initial parameters, which is beneficial to few-shot learning tasks. Then, to enable the network to adjust to few-shot learning tasks, we update the parameters in Scaling and Shifting operation (SS), multi-modal representation fusion (MMRF) and the 3D model classifier to obtain optimal initialization parameters. Since the large-scale training parameters in feature extractors will increase the chance of overfitting, we freeze the feature extractor and introduce a SS operation to adjust its weights. Specifically, SS can reduce the number of training parameters up to 20%, which can effectively avoid overfitting. MMRF can adaptively integrate the multi-modal information based on their significance to the 3D model for a more robust 3D representation. Since there is no available dataset for evaluation, we build three 3D CAD datasets, Meta-ModalNet, Meta-ShapeNet and Meta-RGBD, for this new task and implement the representative methods for fair comparisons. Extensive experimental results can demonstrate the superiority of the proposed method.},
  archive      = {J_IJCV},
  author       = {Zhou, He-Yu and Liu, An-An and Zhang, Chen-Yu and Zhu, Ping and Zhang, Qian-Yi and Kankanhalli, Mohan},
  doi          = {10.1007/s11263-023-01905-8},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {673-688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal meta-transfer fusion network for few-shot 3D model classification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source-free domain adaptation via target prediction
distribution searching. <em>IJCV</em>, <em>132</em>(3), 654–672. (<a
href="https://doi.org/10.1007/s11263-023-01892-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Source-Free Domain Adaptation (SFDA) methods typically adopt the feature distribution alignment paradigm via mining auxiliary information (eg., pseudo-labelling, source domain data generation). However, they are largely limited due to that the auxiliary information is usually error-prone whilst lacking effective error-mitigation mechanisms. To overcome this fundamental limitation, in this paper we propose a novel Target Prediction Distribution Searching (TPDS) paradigm. Theoretically, we prove that in case of sufficient small distribution shift, the domain transfer error could be well bounded. To satisfy this condition, we introduce a flow of proxy distributions that facilitates the bridging of typically large distribution shift from the source domain to the target domain. This results in a progressive searching on the geodesic path where adjacent proxy distributions are regularized to have small shift so that the overall errors can be minimized. To account for the sequential correlation between proxy distributions, we develop a new pairwise alignment with category consistency algorithm for minimizing the adaptation errors. Specifically, a manifold geometry guided cross-distribution neighbour search is designed to detect the data pairs supporting the Wasserstein distance based shift measurement. Mutual information maximization is then adopted over these pairs for shift regularization. Extensive experiments on five challenging SFDA benchmarks show that our TPDS achieves new state-of-the-art performance. The code and datasets are available at https://github.com/tntek/TPDS .},
  archive      = {J_IJCV},
  author       = {Tang, Song and Chang, An and Zhang, Fabian and Zhu, Xiatian and Ye, Mao and Zhang, Changshui},
  doi          = {10.1007/s11263-023-01892-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {654-672},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Source-free domain adaptation via target prediction distribution searching},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual graph networks for pose estimation in crowded scenes.
<em>IJCV</em>, <em>132</em>(3), 633–653. (<a
href="https://doi.org/10.1007/s11263-023-01901-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose estimation in crowded scenes is key to understanding human behavior in real-life applications. Most existing CNN-based pose estimation methods often depend on the appearance of visible parts as cues to localize human joints. However, occlusion is typical in crowded scenes, and invisible body parts have no valid features for joint localization. Introducing prior information about the human pose structure to infer the locations of occluded parts is a natural solution to this problem. In this paper, we argue that learning structural information based on human joints alone is not enough to address human body variations and could be prone to overfitting. From a perspective on the human pose as a dual representation of joints and limbs, we propose a pose refinement network, coined as dual graph network (DGN), to jointly learn its structural information of body joints and limbs by incorporating the cooperative constraints between two branches. Specifically, our DGN has two coupled graph convolutional network (GCN) branches to model the structure information of joints and limbs. Each stage in the branch is composed of a feature aggregator and a GCN module for inter-branch information fusion and intra-branch context extraction, respectively. In addition, to enhance the modeling capacity of GCN, we design an adaptive GCN layer (AGL) embedded in the GCN module to handle each pose instance based on its graph structure. We also propose a heatmap-guided sampling to leverage the features of the body parts to provide rich visual features for the inference of occluded parts. We perform extensive experiments on five challenging datasets to demonstrate the effectiveness of our DGN on pose estimation. Our DGN obtains significant performance improvement from 67.9 to 72.4 mAP in the CrowdPose dataset with the same CNN-based pose estimator and training strategy as the OPEC-Net. It shows that, compared to the OPEC-Net only considering joints, our DGN has a clear advantage due to the joint consideration of both joints and limbs. Meanwhile, our DGN is also helpful for pose estimation in general datasets (i.e., COCO and Pose track) with less occlusion and mutual interference, demonstrating the generalization power of DGN on refining human poses.},
  archive      = {J_IJCV},
  author       = {Tu, Jun and Wu, Gangshan and Wang, Limin},
  doi          = {10.1007/s11263-023-01901-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {633-653},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dual graph networks for pose estimation in crowded scenes},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Editor’s note: Special issue on physics-based vision meets
deep learning. <em>IJCV</em>, <em>132</em>(3), 632. (<a
href="https://doi.org/10.1007/s11263-023-01897-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-023-01897-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {632},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on physics-based vision meets deep learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correspondence distillation from NeRF-based GAN.
<em>IJCV</em>, <em>132</em>(3), 611–631. (<a
href="https://doi.org/10.1007/s11263-023-01903-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural radiance field (NeRF) has shown promising results in preserving the fine details of objects and scenes. However, unlike explicit shape representations e.g., mesh, it remains an open problem to build dense correspondences across different NeRFs of the same category, which is essential in many downstream tasks. The main difficulties of this problem lie in the implicit nature of NeRF and the lack of ground-truth correspondence annotations. In this paper, we show it is possible to bypass these challenges by leveraging the rich semantics and structural priors encapsulated in a pre-trained NeRF-based GAN. Specifically, we exploit such priors from three aspects, namely (1) a dual deformation field that takes latent codes as global structural indicators, (2) a learning objective that regards generator features as geometric-aware local descriptors, and (3) a source of infinite object-specific NeRF samples. Our experiments demonstrate that such priors lead to 3D dense correspondence that is accurate, smooth, and robust. We also show that established dense correspondence across NeRFs can effectively enable many NeRF-based downstream applications such as texture transfer.},
  archive      = {J_IJCV},
  author       = {Lan, Yushi and Loy, Chen Change and Dai, Bo},
  doi          = {10.1007/s11263-023-01903-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {611-631},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correspondence distillation from NeRF-based GAN},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How does fine-tuning impact out-of-distribution detection
for vision-language models? <em>IJCV</em>, <em>132</em>(2), 596–609. (<a
href="https://doi.org/10.1007/s11263-023-01895-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consistently. We also show that prompt learning demonstrates the state-of-the-art OOD detection performance over the zero-shot counterpart.},
  archive      = {J_IJCV},
  author       = {Ming, Yifei and Li, Yixuan},
  doi          = {10.1007/s11263-023-01895-7},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {596-609},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {How does fine-tuning impact out-of-distribution detection for vision-language models?},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIP-adapter: Better vision-language models with feature
adapters. <em>IJCV</em>, <em>132</em>(2), 581–595. (<a
href="https://doi.org/10.1007/s11263-023-01891-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale contrastive vision-language pretraining has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in Radford et al. (International conference on machine learning, PMLR, 2021) to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions. To avoid non-trivial prompt engineering, context optimization (Zhou et al. in Int J Comput Vis 130(9):2337–2348, 2022) has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples. In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning. While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pretrained features. As a consequence, CLIP-Adapter is able to outperform context optimization while maintaining a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach.},
  archive      = {J_IJCV},
  author       = {Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  doi          = {10.1007/s11263-023-01891-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {581-595},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CLIP-adapter: Better vision-language models with feature adapters},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building 3D generative models from minimal data.
<em>IJCV</em>, <em>132</em>(2), 555–580. (<a
href="https://doi.org/10.1007/s11263-023-01870-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for constructing generative models of 3D objects from a single 3D mesh and improving them through unsupervised low-shot learning from 2D images. Our method produces a 3D morphable model that represents shape and albedo in terms of Gaussian processes. Whereas previous approaches have typically built 3D morphable models from multiple high-quality 3D scans through principal component analysis, we build 3D morphable models from a single scan or template. As we demonstrate in the face domain, these models can be used to infer 3D reconstructions from 2D data (inverse graphics) or 3D data (registration). Specifically, we show that our approach can be used to perform face recognition using only a single 3D template (one scan total, not one per person). We extend our model to a preliminary unsupervised learning framework that enables the learning of the distribution of 3D faces using one 3D template and a small number of 2D images. Our approach is motivated as a potential model for the origins of face perception in human infants, who appear to start with an innate face template and subsequently develop a flexible system for perceiving the 3D structure of any novel face from experience with only 2D images of a relatively small number of familiar faces.},
  archive      = {J_IJCV},
  author       = {Sutherland, Skylar and Egger, Bernhard and Tenenbaum, Joshua},
  doi          = {10.1007/s11263-023-01870-2},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {555-580},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Building 3D generative models from minimal data},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The curious layperson: Fine-grained image recognition
without expert labels. <em>IJCV</em>, <em>132</em>(2), 537–554. (<a
href="https://doi.org/10.1007/s11263-023-01885-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of us are not experts in specific fields, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained specifically with that knowledge in mind. Thus, in this paper we consider a new problem: fine-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a fine-grained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets (CUB-200 and Oxford-102 Flowers) and compare with several strong baselines and the state of the art in cross-modal retrieval. Code is available at: https://github.com/subhc/clever .},
  archive      = {J_IJCV},
  author       = {Choudhury, Subhabrata and Laina, Iro and Rupprecht, Christian and Vedaldi, Andrea},
  doi          = {10.1007/s11263-023-01885-9},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {537-554},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {The curious layperson: Fine-grained image recognition without expert labels},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-pot multi-frame denoising. <em>IJCV</em>,
<em>132</em>(2), 515–536. (<a
href="https://doi.org/10.1007/s11263-023-01887-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficacy of learning-based denoising techniques is heavily reliant on the quality of clean supervision. Unfortunately, acquiring clean images in many scenarios is a challenging task. Conversely, capturing multiple noisy frames of the same field of view is feasible and natural in real-life scenarios. Thus, it is imperative to explore the potential of noisy data in model training and avoid the limitations imposed by clean labels. In this paper, we propose a novel unsupervised learning strategy called one-pot denoising (OPD), which is the first unsupervised multi-frame denoising method. OPD differs from traditional supervision schemes, such as supervised Noise2Clean, unsupervised Noise2Noise, and self-supervised Noise2Void, as it employs mutual supervision among all the multiple frames. This provides learning with more diverse supervision and allows models to better exploit the correlation among frames. Notably, we reveal that Noise2Noise is a special case of the proposed OPD. We provide two specific implementations, namely OPD-random coupling and OPD-alienation loss, to achieve OPD during model training based on data allocation and loss refine, respectively. Our experiments demonstrate that OPD outperforms other unsupervised denoising methods and is comparable to non-transformer-based supervised N2C methods for several classic noise patterns, including additive white Gaussian noise, signal-dependent Poisson noise, and multiplicative Bernoulli noise. Additionally, OPD shows remarkable performance in more challenging tasks such as mixed-blind denoising, denoising random-valued impulse noise, and text removal. The source code and pre-trained models are available at https://github.com/LujiaJin/One-Pot_Multi-Frame_Denoising .},
  archive      = {J_IJCV},
  author       = {Jin, Lujia and Guo, Qing and Zhao, Shi and Zhu, Lei and Chen, Qian and Ren, Qiushi and Lu, Yanye},
  doi          = {10.1007/s11263-023-01887-7},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {515-536},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {One-pot multi-frame denoising},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MineGAN++: Mining generative models for efficient knowledge
transfer to limited data domains. <em>IJCV</em>, <em>132</em>(2),
490–514. (<a href="https://doi.org/10.1007/s11263-023-01882-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the often enormous effort required to train GANs, both computationally as well as in dataset collection, the re-use of pretrained GANs largely increases the potential impact of generative models. Therefore, we propose a novel knowledge transfer method for generative models based on mining the knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs. This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain. Mining effectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods, such as mode collapse and lack of flexibility. Furthermore, to prevent overfitting on small target domains, we introduce sparse subnetwork selection, that restricts the set of trainable neurons to those that are relevant for the target dataset. We perform comprehensive experiments on several challenging datasets using various GAN architectures (BigGAN, Progressive GAN, and StyleGAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods. In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs. MineGAN .},
  archive      = {J_IJCV},
  author       = {Wang, Yaxing and Gonzalez-Garcia, Abel and Wu, Chenshen and Herranz, Luis and Khan, Fahad Shahbaz and Jui, Shangling and Yang, Jian and van de Weijer, Joost},
  doi          = {10.1007/s11263-023-01882-y},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {490-514},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MineGAN++: Mining generative models for efficient knowledge transfer to limited data domains},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sfnet: Faster and accurate semantic segmentation via
semantic flow. <em>IJCV</em>, <em>132</em>(2), 466–489. (<a
href="https://doi.org/10.1007/s11263-023-01875-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on exploring effective methods for faster and accurate semantic segmentation. A common practice to improve the performance is to attain high-resolution feature maps with strong semantic representation. Two strategies are widely used: atrous convolutions and feature pyramid fusion, while both are either computationally intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels and broadcast high-level features to high-resolution features effectively and efficiently. Furthermore, integrating our FAM to a standard feature pyramid structure exhibits superior performance over other real-time methods, even on lightweight backbone networks, such as ResNet-18 and DFNet. Then to further speed up the inference procedure, we also present a novel Gated Dual Flow Alignment Module to directly align high-resolution feature maps and low-resolution feature maps where we term the improved version network as SFNet-Lite. Extensive experiments are conducted on several challenging datasets, where results show the effectiveness of both SFNet and SFNet-Lite. In particular, when using Cityscapes test set, the SFNet-Lite series achieve 80.1 mIoU while running at 60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using STDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets (i.e., Cityscapes, Mapillary, IDD, and BDD) into one large dataset, which we named Unified Driving Segmentation (UDS) dataset. It contains diverse domain and style information. We benchmark several representative works on UDS. Both SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS, which serves as a strong baseline in such a challenging setting. The code and models are publicly available at https://github.com/lxtGH/SFSegNets .},
  archive      = {J_IJCV},
  author       = {Li, Xiangtai and Zhang, Jiangning and Yang, Yibo and Cheng, Guangliang and Yang, Kuiyuan and Tong, Yunhai and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01875-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {466-489},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Sfnet: Faster and accurate semantic segmentation via semantic flow},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intra- &amp; extra-source exemplar-based style synthesis for
improved domain generalization. <em>IJCV</em>, <em>132</em>(2), 446–465.
(<a href="https://doi.org/10.1007/s11263-023-01878-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an exemplar-based style synthesis pipeline to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image, preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, i.e., intra-source style augmentation ( $$\textrm{ISSA}$$ ) effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $$12.4\%$$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. $$\textrm{ISSA}$$ is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $$3\%$$ mIoU in Cityscapes to Dark Zürich. In addition, we demonstrate the strong plug-n-play ability of the proposed style synthesis pipeline, which is readily usable for extra-source exemplars e.g., web-crawled images, without any retraining or fine-tuning. Moreover, we study a new use case to indicate neural network’s generalization capability by building a stylized proxy validation set. This application has significant practical sense for selecting models to be deployed in the open-world environment. Our code is available at https://github.com/boschresearch/ISSA .},
  archive      = {J_IJCV},
  author       = {Li, Yumeng and Zhang, Dan and Keuper, Margret and Khoreva, Anna},
  doi          = {10.1007/s11263-023-01878-8},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {446-465},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Intra- &amp; extra-source exemplar-based style synthesis for improved domain generalization},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep richardson–lucy deconvolution for low-light image
deblurring. <em>IJCV</em>, <em>132</em>(2), 428–445. (<a
href="https://doi.org/10.1007/s11263-023-01877-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images taken under the low-light condition often contain blur and saturated pixels at the same time. Deblurring images with saturated pixels is quite challenging. Because of the limited dynamic range, the saturated pixels are usually clipped in the imaging process and thus cannot be modeled by the linear blur model. Previous methods use manually designed smooth functions to approximate the clipping procedure. Their deblurring processes often require empirically defined parameters, which may not be the optimal choices for different images. In this paper, we develop a data-driven approach to model the saturated pixels by a learned latent map. Based on the new model, the non-blind deblurring task can be formulated into a maximum a posterior problem, which can be effectively solved by iteratively computing the latent map and the latent image. Specifically, the latent map is computed by learning from a map estimation network, and the latent image estimation process is implemented by a Richardson–Lucy (RL)-based updating scheme. To estimate high-quality deblurred images without amplified artifacts, we develop a prior estimation network to obtain prior information, which is further integrated into the RL scheme. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art algorithms both quantitatively and qualitatively on synthetic and real-world images.},
  archive      = {J_IJCV},
  author       = {Chen, Liang and Zhang, Jiawei and Li, Zhenhua and Wei, Yunxuan and Fang, Faming and Ren, Jimmy and Pan, Jinshan},
  doi          = {10.1007/s11263-023-01877-9},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {428-445},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep Richardson–Lucy deconvolution for low-light image deblurring},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning robust facial representation from the view of
diversity and closeness. <em>IJCV</em>, <em>132</em>(2), 410–427. (<a
href="https://doi.org/10.1007/s11263-023-01893-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed remarkable progress in deep face recognition due to the advancement of both deep convolutional neural networks and loss functions. In this work, we provide an intrinsic analysis to reveal the working mechanism of softmax from the view of closeness and diversity. We find that enhancing the closeness of easy samples and preserving the diversity of hard samples can improve feature representation robustness. However, most of the previous works aim to improve the closeness of intraclass samples and fail to emphasize hard sample diversity. To solve the above issue, we developed a novel robust feature representation model, which leverages the rate-distortion theory to characterize the proportions of closeness and diversity, in conjunction with the designed hard sample mining scheme to further enhance the discriminative ability of the deep model. Specifically, the proposed model compresses the coding rate of easy samples for closeness, and expands the coding rate of hard samples for diversity. A novel hard sample mining scheme is designed to ensure that easy samples and hard samples are balanced in each batch. For each batch, we also guarantee that hard samples are both from the intraclass samples led by various noises and from the interclass samples with similar appearances. Extensive experimental results on popular benchmarks demonstrate the superiority of our proposed approach over state-of-the-art competitors.},
  archive      = {J_IJCV},
  author       = {Zhao, Chaoyu and Qian, Jianjun and Zhu, Shumin and Xie, Jin and Yang, Jian},
  doi          = {10.1007/s11263-023-01893-9},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {410-427},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning robust facial representation from the view of diversity and closeness},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferring vision-language models for visual recognition:
A classifier perspective. <em>IJCV</em>, <em>132</em>(2), 392–409. (<a
href="https://doi.org/10.1007/s11263-023-01876-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring knowledge from pre-trained deep models for downstream tasks, particularly with limited labeled samples, is a fundamental problem in computer vision research. Recent advances in large-scale, task-agnostic vision-language pre-trained models, which are learned with billions of samples, have shed new light on this problem. In this study, we investigate how to efficiently transfer aligned visual and textual knowledge for downstream visual recognition tasks. We first revisit the role of the linear classifier in the vanilla transfer learning framework, and then propose a new paradigm where the parameters of the classifier are initialized with semantic targets from the textual encoder and remain fixed during optimization. To provide a comparison, we also initialize the classifier with knowledge from various resources. In the empirical study, we demonstrate that our paradigm improves the performance and training speed of transfer learning tasks. With only minor modifications, our approach proves effective across 17 visual datasets that span three different data domains: image, video, and 3D point cloud.},
  archive      = {J_IJCV},
  author       = {Wu, Wenhao and Sun, Zhun and Song, Yuxin and Wang, Jingdong and Ouyang, Wanli},
  doi          = {10.1007/s11263-023-01876-w},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {392-409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Transferring vision-language models for visual recognition: A classifier perspective},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A region-based randers geodesic approach for image
segmentation. <em>IJCV</em>, <em>132</em>(2), 349–391. (<a
href="https://doi.org/10.1007/s11263-023-01881-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geodesic model based on the eikonal partial differential equation (PDE) has served as a fundamental tool for the applications of image segmentation and boundary detection in the past two decades. However, the existing approaches commonly only exploit the image edge-based features for computing minimal geodesic paths, potentially limiting their performance in complicated segmentation situations. In this paper, we introduce a new variational image segmentation model based on the minimal geodesic path framework and the eikonal PDE, where the region-based appearance term that defines then regional homogeneity features can be taken into account for estimating the associated minimal geodesic paths. This is done by constructing a Randers geodesic metric interpretation of the region-based active contour energy functional. As a result, the minimization of the active contour energy functional is transformed into finding the solution to the Randers eikonal PDE. We also suggest a practical interactive image segmentation strategy, where the target boundary can be delineated by the concatenation of several piecewise geodesic paths. We invoke the Finsler variant of the fast marching method to estimate the geodesic distance map, yielding an efficient implementation of the proposed region-based Randers geodesic model for image segmentation. Experimental results on both synthetic and real images exhibit that our model indeed achieves encouraging segmentation performance.},
  archive      = {J_IJCV},
  author       = {Chen, Da and Mirebeau, Jean-Marie and Shu, Huazhong and Cohen, Laurent D.},
  doi          = {10.1007/s11263-023-01881-z},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {349-391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A region-based randers geodesic approach for image segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blind image deblurring with unknown kernel size and
substantial noise. <em>IJCV</em>, <em>132</em>(2), 319–348. (<a
href="https://doi.org/10.1007/s11263-023-01883-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring (BID) has been extensively studied in computer vision and adjacent fields. Modern methods for BID can be grouped into two categories: single-instance methods that deal with individual instances using statistical inference and numerical optimization, and data-driven methods that train deep-learning models to deblur future instances directly. Data-driven methods can be free from the difficulty in deriving accurate blur models, but are fundamentally limited by the diversity and quality of the training data—collecting sufficiently expressive and realistic training data is a standing challenge. In this paper, we focus on single-instance methods that remain competitive and indispensable. However, most such methods do not prescribe how to deal with unknown kernel size and substantial noise, precluding practical deployment. Indeed, we show that several state-of-the-art (SOTA) single-instance methods are unstable when the kernel size is overspecified, and/or the noise level is high. On the positive side, we propose a practical BID method that is stable against both, the first of its kind. Our method builds on the recent ideas of solving inverse problems by integrating physical models and structured deep neural networks, without extra training data. We introduce several crucial modifications to achieve the desired stability. Extensive empirical tests on standard synthetic datasets, as well as real-world NTIRE2020 and RealBlur datasets, show the superior effectiveness and practicality of our BID method compared to SOTA single-instance as well as data-driven methods. The code of our method is available at https://github.com/sun-umn/Blind-Image-Deblurring .},
  archive      = {J_IJCV},
  author       = {Zhuang, Zhong and Li, Taihui and Wang, Hengkang and Sun, Ju},
  doi          = {10.1007/s11263-023-01883-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {319-348},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Blind image deblurring with unknown kernel size and substantial noise},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CDistNet: Perceiving multi-domain character distance for
robust text recognition. <em>IJCV</em>, <em>132</em>(2), 300–318. (<a
href="https://doi.org/10.1007/s11263-023-01880-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with a rare shape). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, visual and semantic are still separately modeled and they are merely loosely associated. In this paper, we propose a novel module called multi-domain character distance perception (MDCDP) to establish a visually and semantically related position embedding. MDCDP uses the position embedding to query both visual and semantic features following the cross-attention mechanism. The two kinds of clues are fused into the position branch, generating a content-aware embedding that well perceives character spacing and orientation variants, character semantic affinities, and clues tying the two kinds of information. They are summarized as the multi-domain character distance. We develop CDistNet that stacks multiple MDCDPs to guide a gradually precise distance modeling. Thus, the feature-character alignment is well build even though various recognition difficulties are presented. We verify CDistNet on ten challenging public datasets and two series of augmented datasets created by ourselves. The experiments demonstrate that CDistNet performs highly competitively. It not only ranks top-tier in standard benchmarks, but also outperforms recent popular methods by obvious margins on real and augmented datasets presenting severe text deformation, poor linguistic support, and rare character layouts. In addition, the visualization shows that CDistNet achieves proper information utilization in both visual and semantic domains. Our code is available at https://github.com/simplify23/CDistNet .},
  archive      = {J_IJCV},
  author       = {Zheng, Tianlun and Chen, Zhineng and Fang, Shancheng and Xie, Hongtao and Jiang, Yu-Gang},
  doi          = {10.1007/s11263-023-01880-0},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {300-318},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CDistNet: Perceiving multi-domain character distance for robust text recognition},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single pixel spectral color constancy. <em>IJCV</em>,
<em>132</em>(2), 287–299. (<a
href="https://doi.org/10.1007/s11263-023-01867-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color constancy is still one of the biggest challenges in camera color processing. Convolutional neural networks have been able to improve the situation but there are still problems in many conditions, especially in scenes where a single color is dominating. In this work, we approach the problem from a slightly different setting. What if we could have some other information than the raw RGB image data. What kind of information would help to bring significant improvements while still be feasible in a mobile device. These questions sparked an idea for a novel approach for computational color constancy. Instead of raw RGB images used by the existing algorithms to estimate the scene white points, our approach is based on the scene’s average color spectra-single pixel spectral measurement. We show that as few as 10–14 spectral channels are sufficient. Notably, the sensor output has five orders of magnitude less data than in raw RGB images of a 10MPix camera. The spectral sensor captures the “spectral fingerprints” of different light sources and the illuminant white point can be accurately estimated by a standard regressor. The regressor can be trained with generated measurements using the existing RGB color constancy datasets. For this purpose, we propose a spectral data generation pipeline that can be used if the dataset camera model is known and thus its spectral characterization can be obtained. To verify the results with real data, we collected a real spectral dataset with a commercial spectrometer. On all datasets the proposed Single Pixel Spectral Color Constancy obtains the highest accuracy in the both single and cross-dataset experiments. The method is particularly effective for the difficult scenes for which the average improvements are 40–70% compared to state-of-the-arts. The approach can be extended to multi-illuminant case for which the experimental results also provide promising results.},
  archive      = {J_IJCV},
  author       = {Koskinen, Samu and Acar, Erman and Kämäräinen, Joni-Kristian},
  doi          = {10.1007/s11263-023-01867-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {287-299},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Single pixel spectral color constancy},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: Automatic generation of 3D scene animation based
on dynamic knowledge graphs and contextual encoding. <em>IJCV</em>,
<em>132</em>(1), 285. (<a
href="https://doi.org/10.1007/s11263-023-01888-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Song, Wenfeng and Zhang, Xinyu and Guo, Yuting and Li, Shuai and Hao, Aimin and Qin, Hong},
  doi          = {10.1007/s11263-023-01888-6},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {285},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Automatic generation of 3D scene animation based on dynamic knowledge graphs and contextual encoding},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: PIDray: A large-scale x-ray benchmark for
real-world prohibited item detection. <em>IJCV</em>, <em>132</em>(1),
284. (<a href="https://doi.org/10.1007/s11263-023-01889-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhang, Libo and Jiang, Lutao and Ji, Ruyi and Fan, Heng},
  doi          = {10.1007/s11263-023-01889-5},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {284},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: PIDray: a large-scale X-ray benchmark for real-world prohibited item detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot segmentation via divide-and-conquer proxies.
<em>IJCV</em>, <em>132</em>(1), 261–283. (<a
href="https://doi.org/10.1007/s11263-023-01886-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot segmentation (FSS) is a marginally explored but challenging task that aims to identify unseen classes of objects with only a handful of densely annotated samples. By and large, current FSS approaches perform meta-inference based on the prototype learning paradigm, which fails to fully exploit the underlying information from support image-mask pairs, resulting in multiple segmentation failures, such as incomplete objects, ambiguous boundaries, and distractor activation. For this purpose, a flexible and generic framework is developed in the spirit of divide-and-conquer. We first implement a novel self-reasoning scheme on the labeled support image, and then divide the coarse segmentation mask into several regions with different properties. By employing effective masked average pooling techniques, a series of support-induced proxies are generated on the fly, each performing a specific role in conquering the above challenges. Furthermore, we meticulously devise the parallel decoder structure and semantic consistency regularization to eliminate confusion and enhance discrimination. In stark contrast to conventional prototype-based approaches, our proposed divide-and-conquer proxies (DCP) can provide “episode” level guidelines that go well beyond the object cues themselves. Extensive experiments are conducted on FSS benchmarks to verify the effectiveness, including standard settings as well as cross-domain settings. In particular, we propose a temporal DCP and successfully extend it to video object segmentation via memory repository and progressive propagation, illustrating the high scalability. The source codes are available at https://github.com/chunbolang/DCP .},
  archive      = {J_IJCV},
  author       = {Lang, Chunbo and Cheng, Gong and Tu, Binfei and Han, Junwei},
  doi          = {10.1007/s11263-023-01886-8},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {261-283},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Few-shot segmentation via divide-and-conquer proxies},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical skeleton meta-prototype contrastive learning
with hard skeleton mining for unsupervised person re-identification.
<em>IJCV</em>, <em>132</em>(1), 238–260. (<a
href="https://doi.org/10.1007/s11263-023-01864-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learning model is proposed to cluster and contrast the most typical skeleton features (“prototypes”) from different-level skeletons. By converting original prototypes into meta-prototypes with multiple homogeneous transformations, we induce the model to learn the inherent consistency of prototypes to capture more effective skeleton features for person re-ID. Furthermore, we devise a hard skeleton mining mechanism to adaptively infer the informative importance of each skeleton, so as to focus on harder skeletons to learn more discriminative skeleton representations. Extensive evaluations on five datasets demonstrate that our approach outperforms a wide variety of state-of-the-art skeleton-based methods. We further show the general applicability of our method to cross-view person re-ID and RGB-based scenarios with estimated skeletons.},
  archive      = {J_IJCV},
  author       = {Rao, Haocong and Leung, Cyril and Miao, Chunyan},
  doi          = {10.1007/s11263-023-01864-0},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {238-260},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical skeleton meta-prototype contrastive learning with hard skeleton mining for unsupervised person re-identification},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring vision-language models for imbalanced learning.
<em>IJCV</em>, <em>132</em>(1), 224–237. (<a
href="https://doi.org/10.1007/s11263-023-01868-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid out of memory problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%, and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We further analyze the influence of pre-training data size, backbones, and training cost. Our study highlights the significance of imbalanced learning algorithms in face of VLMs pre-trained by huge data. We release our code at https://github.com/Imbalance-VLM/Imbalance-VLM .},
  archive      = {J_IJCV},
  author       = {Wang, Yidong and Yu, Zhuohao and Wang, Jindong and Heng, Qiang and Chen, Hao and Ye, Wei and Xie, Rui and Xie, Xing and Zhang, Shikun},
  doi          = {10.1007/s11263-023-01868-w},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {224-237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring vision-language models for imbalanced learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context autoencoder for self-supervised representation
learning. <em>IJCV</em>, <em>132</em>(1), 208–223. (<a
href="https://doi.org/10.1007/s11263-023-01852-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised representation pretraining. We pretrain an encoder by making predictions in the encoded representation space. The pretraining tasks include two tasks: masked representation prediction—predict the representations for the masked patches, and masked patch reconstruction—reconstruct the masked patches. The network is an encoder–regressor–decoder architecture: the encoder takes the visible patches as input; the regressor predicts the representations of the masked patches, which are expected to be aligned with the representations computed from the encoder, using the representations of visible patches and the positions of visible and masked patches; the decoder reconstructs the masked patches from the predicted encoded representations. The CAE design encourages the separation of learning the encoder (representation) from completing the pertaining tasks: masked representation prediction and masked patch reconstruction tasks, and making predictions in the encoded representation space empirically shows the benefit to representation learning. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, object detection and instance segmentation, and classification. The code will be available at https://github.com/Atten4Vis/CAE .},
  archive      = {J_IJCV},
  author       = {Chen, Xiaokang and Ding, Mingyu and Wang, Xiaodi and Xin, Ying and Mo, Shentong and Wang, Yunhao and Han, Shumin and Luo, Ping and Zeng, Gang and Wang, Jingdong},
  doi          = {10.1007/s11263-023-01852-4},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {208-223},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context autoencoder for self-supervised representation learning},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correlation information bottleneck: Towards adapting
pretrained multimodal models for robust visual question answering.
<em>IJCV</em>, <em>132</em>(1), 185–207. (<a
href="https://doi.org/10.1007/s11263-023-01858-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from large-scale pretrained vision language models (VLMs), the performance of visual question answering (VQA) has approached human oracles. However, finetuning such models on limited data often suffers from overfitting and poor generalization issues, leading to a lack of model robustness. In this paper, we aim to improve input robustness from an information bottleneck perspective when adapting pretrained VLMs to the downstream VQA task. Input robustness refers to the ability of models to defend against visual and linguistic input variations, as well as shortcut learning involved in inputs. Generally, the representations obtained by pretrained VLMs inevitably contain irrelevant and redundant information for a specific downstream task, resulting in statistically spurious correlations and insensitivity to input variations. To encourage representations to converge to a minimal sufficient statistic in multimodal learning, we propose Correlation Information Bottleneck (CIB), which seeks a tradeoff between compression and redundancy in representations by minimizing the mutual information (MI) between inputs and representations while maximizing the MI between outputs and representations. Moreover, we derive a tight theoretical upper bound for the mutual information between multimodal inputs and representations, incorporating different internal correlations that guide models to learn more robust representations and facilitate modality alignment. Extensive experiments consistently demonstrate the effectiveness and superiority of the proposed CIB in terms of input robustness and accuracy.},
  archive      = {J_IJCV},
  author       = {Jiang, Jingjing and Liu, Ziyi and Zheng, Nanning},
  doi          = {10.1007/s11263-023-01858-y},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {185-207},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correlation information bottleneck: Towards adapting pretrained multimodal models for robust visual question answering},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative noise robust sparse orthogonal label
regression-based domain adaptation. <em>IJCV</em>, <em>132</em>(1),
161–184. (<a href="https://doi.org/10.1007/s11263-023-01865-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) aims to enable a learning model trained from a source domain to generalize well on a target domain, despite the mismatch of data distributions between the two domains. State-of-the-art DA methods have so far focused on the search of a latent shared feature space where source and target domain data can be aligned either statistically and/or geometrically. In this paper, we propose a novel unsupervised DA method, namely Discriminative Noise Robust Sparse Orthogonal Label Regression-based Domain Adaptation (DOLL-DA). The proposed DOLL-DA derives from a novel integrated model which searches a shared feature subspace where data labels are orthogonally regressed using a label embedding trick, and source and target domain data are discriminatively aligned statistically through optimization of some repulse force terms. Furthermore, in minimizing a novel Noise Robust Sparse Orthogonal Label Regression(NRS_OLR) term, the proposed model explicitly accounts for data outliers to avoid negative transfer and introduces the property of sparsity when regressing data labels. We carry out comprehensive experiments in comparison with 35 state of the art DA methods using 8 standard DA benchmarks and 49 cross-domain image classification tasks. The proposed DA method demonstrates its effectiveness and consistently outperforms the state-of-the-art DA methods with a margin which reaches 17 points on the CMU PIE dataset. To gain insight into the proposed DOLL-DA, we also derive three additional DA methods based on three partial models from the full model, namely OLR, CDDA+, and JOLR-DA, highlighting the added value of (1) discriminative statistical data alignment; (2) Noise Robust Sparse Orthogonal Label Regression; and (3) their joint optimization through the full DA model. In addition, we also perform time complexity and an in-depth empiric analysis of the proposed DA method in terms of its sensitivity w.r.t. hyper-parameters, convergence speed, impact of the base classifier and random label initialization as well as performance stability w.r.t. target domain data being used in training.},
  archive      = {J_IJCV},
  author       = {Luo, Lingkun and Hu, Shiqiang and Chen, Liming},
  doi          = {10.1007/s11263-023-01865-z},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {161-184},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Discriminative noise robust sparse orthogonal label regression-based domain adaptation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Accurate fine-grained object recognition with
structure-driven relation graph networks. <em>IJCV</em>,
<em>132</em>(1), 137–160. (<a
href="https://doi.org/10.1007/s11263-023-01873-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained object recognition (FGOR) aims to learn discriminative features that can identify the subtle distinctions between visually similar objects. However, less effort has been devoted to overcoming the impact of object’s personalized differences, e.g., varying posture or perspective. We argue that the personalized differences could decline the network’s perception of discriminative features, thus discarding some discriminative clues and degrading the FGOR performance accordingly. This motivates us to explore the intrinsic structure knowledge: the fixed spatial correlation between object parts, and thus apply this knowledge to associate diverse semantic parts and recover the missing discriminative details caused by the personalized differences accordingly. In this paper, we propose an end-to-end Structure-driven Relation Graph Network (SRGN) for fine-grained object recognition, and target at exploring and exploiting the object structure information without any additional annotations to associate diverse semantic parts, making the network sensitive to discriminative details influenced by personalized differences. Specifically, the core of SRGN is a Structure-aware Axial Graph (SAG) module, which first infers the structure embedding by establishing the correlation between position information and visual features along the axial direction, and then applies this embedding as aggregation weights to emphasize each discriminative representation by weighted reassembling all relevant features to it. Additionally, our SAG can be readily extensible to a multi-graph schema, that leverages the complementary advantages of different structure embeddings between the position information and visual content, further improving SAG. In this way, our SRGN can demonstrate remarkable robustness in scenarios characterized by extreme distribution perturbations, ultimately leading to superior performance. Extensive experiments and explainable visualizations validate the efficacy of the proposed approach on widely-used fine-grained benchmarks.},
  archive      = {J_IJCV},
  author       = {Wang, Shijie and Wang, Zhihui and Li, Haojie and Chang, Jianlong and Ouyang, Wanli and Tian, Qi},
  doi          = {10.1007/s11263-023-01873-z},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {137-160},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Accurate fine-grained object recognition with structure-driven relation graph networks},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking the complementary-view multi-human association
and tracking. <em>IJCV</em>, <em>132</em>(1), 118–136. (<a
href="https://doi.org/10.1007/s11263-023-01857-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using multiple moving cameras with different and time-varying views can significantly expand the capability of multiple human tracking in larger areas and with various perspectives. In particular, the use of moving cameras of complementary top and horizontal views can facilitate multi-human detection and tracking from both global and local perspectives. As a new challenging problem that draws more and more attention in recent years, one main issue is the lack of a comprehensive dataset for credible performance evaluation. In this paper, we present such a new dataset consisting of videos synchronously recorded by drone and wearable cameras, with high-quality annotations of the covered subjects and their cross-frame and cross-view associations. We also propose a pertinent baseline algorithm for multi-view multiple human tracking and evaluate it on this new dataset against the annotated ground truths. Experimental results verify the usefulness of the new dataset and the effectiveness of the proposed baseline algorithm.},
  archive      = {J_IJCV},
  author       = {Han, Ruize and Feng, Wei and Wang, Feifan and Qian, Zekun and Yan, Haomin and Wang, Song},
  doi          = {10.1007/s11263-023-01857-z},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {118-136},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking the complementary-view multi-human association and tracking},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards high-resolution specular highlight detection.
<em>IJCV</em>, <em>132</em>(1), 95–117. (<a
href="https://doi.org/10.1007/s11263-023-01845-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specular highlight detection is an essential task with various applications in computer vision. This paper aims to detect specular highlights in single high-resolution images using deep learning while avoiding excessive GPU memory consumption. To achieve this, we present a high-resolution specular highlight detection dataset with manual annotations of specular highlights. Given our dataset, we propose a patch-level bidirectional refinement network for high-resolution specular highlight detection. The main idea is to utilize both the pathway from small-scale patch to large-scale patch and its reverse pathway to progressively refine the detection results of adjacent-scale specular highlight patches. Moreover, based on our detection network, we propose a modified inpainting framework for specular highlight removal as an application. Lastly, we provide ten potential research directions for specular highlight detection, inspiring researchers for further study.},
  archive      = {J_IJCV},
  author       = {Fu, Gang and Zhang, Qing and Zhu, Lei and Lin, Qifeng and Wang, Yihao and Fan, Siyuan and Xiao, Chunxia},
  doi          = {10.1007/s11263-023-01845-3},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {95-117},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards high-resolution specular highlight detection},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonlinear, regularized, and data-independent modulation
for continuously interactive image processing network. <em>IJCV</em>,
<em>132</em>(1), 74–94. (<a
href="https://doi.org/10.1007/s11263-023-01874-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most studies on convolutional Neural Network (CNN) based image processing have proposed networks that can be optimized for a single level. Here, the term “level&quot; refers to the specific objective defined for each task, such as the degree of noise in denoising tasks. Hence, they underperform on other levels and must be retrained to deliver optimal performance. Using multiple models to cover multiple levels involves very high computational costs. To solve these problems, recent approaches train the networks on two different levels and propose their own modulation methods to enable the arbitrary intermediate levels. However, many of them 1) have difficulty adapting from one level to the other, 2) suffer from unintended artifacts in the intermediate levels, or 3) require large memory and computational cost. In this paper, we propose a novel framework using Filter Transition Network (FTN), which is a non-linear module that easily adapts to new levels, is regularized to prevent undesirable side-effects, and extremely lightweight being a data-independent module. Additionally, for stable learning of FTN, we newly propose a method to initialize nonlinear CNNs with identity mappings. Extensive results for various image processing tasks indicate that the performance of FTN is stable regarding adaptation and modulation and is comparable to that of the other heavy frameworks.},
  archive      = {J_IJCV},
  author       = {Lee, Hyeongmin and Kim, Taeoh and Son, Hanbin and Baek, Sangwook and Cheon, Minsu and Lee, Sangyoun},
  doi          = {10.1007/s11263-023-01874-y},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {74-94},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A nonlinear, regularized, and data-independent modulation for continuously interactive image processing network},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coatrsnet: Fully exploiting convolution and attention for
stereo matching by region separation. <em>IJCV</em>, <em>132</em>(1),
56–73. (<a href="https://doi.org/10.1007/s11263-023-01872-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is a fundamental technique for many vision and robotics applications. State-of-the-art methods either employ convolutional neural networks with spatially-shared kernels or utilize content-dependent interactions (e.g., local or global attention) to augment convolution operations. Despite of great improvements being made, existing methods could either suffer from a high computational cost arising from global attention operations or a suboptimal performance at edge regions due to spatially-shared convolutions. In this paper, we propose a CoAtRS stereo matching method to exert the complementary advantages of convolution and attention to the full via region separation. Our method can adaptively adopt the most suitable feature extraction and aggregation patterns for smooth and edge regions with less computational cost. In addition, we propose D-global attention which performs global filtering on the disparity dimension to better fuse cost volumes of different regions and alleviate the locality defects of convolutions. Our CoAtRS stereo matching method can also be embedded conveniently in various existing 3D CNN stereo networks. The resulting networks can achieve significant improvements in terms of both accuracy and efficiency. Furthermore, we design an accurate network (named CoAtRSNet) which achieves the state-of-the-art results on five public datasets. At the time of writing, CoAtRSNet ranks 1st–3rd on all the metrics published on the ETH3D website, ranks 2nd on Scene Flow, and ranks 1st for the Root-Mean-Square metric, 2nd for the average error metric and 3rd for the bad 0.5 metric on the Middlebury benchmark.},
  archive      = {J_IJCV},
  author       = {Cheng, Junda and Xu, Gangwei and Guo, Peng and Yang, Xin},
  doi          = {10.1007/s11263-023-01872-0},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {56-73},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Coatrsnet: Fully exploiting convolution and attention for stereo matching by region separation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The right spin: Learning object motion from
rotation-compensated flow fields. <em>IJCV</em>, <em>132</em>(1), 40–55.
(<a href="https://doi.org/10.1007/s11263-023-01859-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A good understanding of geometrical concepts as well as a broad familiarity with objects lead to excellent human perception of moving objects. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer and even camouflage. How we perceive moving objects so reliably is a longstanding research question in computer vision and borrows findings from related areas such as psychology, cognitive science and physics. One approach to the problem is to teach a deep network to model all of these effects. This is in contrast with the strategy used by human vision, where cognitive processes and body design are tightly coupled and each is responsible for certain aspects of correctly identifying moving objects. Similarly, from the computer vision perspective there is evidence that classical, geometry-based techniques are better suited to the “motion-based” parts of the problem, while deep networks are more suitable for modeling appearance. In this work, we argue that the coupling of camera rotation and camera translation can create complex motion fields that are difficult for a deep network to untangle directly. We present a novel probabilistic model to estimate the camera’s rotation given the motion field. We then rectify the flow field to obtain a rotation-compensated motion field for subsequent segmentation. This strategy of first estimating camera motion, and then allowing a network to learn the remaining parts of the problem, yields improved results on the widely used DAVIS benchmark as well as the more recent motion segmentation data set MoCA (Moving Camouflaged Animals).},
  archive      = {J_IJCV},
  author       = {Bideau, Pia and Learned-Miller, Erik and Schmid, Cordelia and Alahari, Karteek},
  doi          = {10.1007/s11263-023-01859-x},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {40-55},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {The right spin: Learning object motion from rotation-compensated flow fields},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crots: Cross-domain teacher–student learning for source-free
domain adaptive semantic segmentation. <em>IJCV</em>, <em>132</em>(1),
20–39. (<a href="https://doi.org/10.1007/s11263-023-01863-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to transfer source knowledge to the target domain from pre-trained source models without accessing private source data. Existing SFDA methods typically adopt the self-training strategy employing the pre-trained source model to generate pseudo-labels for unlabeled target data. However, these methods are subject to strict limitations: (1) The discrepancy between source and target domains results in intense noise and unreliable pseudo-labels. Overfitting noisy pseudo-labeled target data will lead to drastic performance degradation. (2) Considering the class-imbalanced pseudo-labels, the target model is prone to forget the minority classes. Aiming at these two limitations, this study proposes a CROss domain Teacher–Student learning framework (namely CROTS) to achieve source-free domain adaptive semantic segmentation. Specifically, with pseudo-labels provided by the intra-domain teacher model, CROTS incorporates Spatial-Aware Data Mixing to generate diverse samples by randomly mixing different patches respecting to their spatial semantic layouts. Meanwhile, during inter-domain teacher–student learning, CROTS fosters Rare-Class Patches Mining strategy to mitigate the class imbalance phenomenon. To this end, the inter-domain teacher model helps exploit long-tailed rare classes and promote their contributions to student learning. Extensive experimental results have demonstrated that: (1) CROTS mitigates the overfitting issue and contributes to stable performance improvement, i.e., + 16.0% mIoU and + 16.5% mIoU for SFDA in GTA5 $$\rightarrow $$ Cityscapes and SYNTHIA $$\rightarrow $$ Cityscapes, respectively; (2) CROTS improves task performance for long-tailed rare classes, alleviating the issue of class imbalance; (3) CROTS achieves superior performance comparing to other SFDA competitors; (4) CROTS can be applied under the black-box SFDA setting, even outperforming many white-box SFDA methods. Our codes will be publicly available at https://github.com/luoxin13/CROTS .},
  archive      = {J_IJCV},
  author       = {Luo, Xin and Chen, Wei and Liang, Zhengfa and Yang, Longqi and Wang, Siwei and Li, Chen},
  doi          = {10.1007/s11263-023-01863-1},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {20-39},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Crots: Cross-domain Teacher–Student learning for source-free domain adaptive semantic segmentation},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards language-guided visual recognition via dynamic
convolutions. <em>IJCV</em>, <em>132</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11263-023-01871-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we are committed to establishing a unified and end-to-end multi-modal network via exploring language-guided visual recognition. To approach this target, we first propose a novel multi-modal convolution module called Language-guided Dynamic Convolution (LaConv). Its convolution kernels are dynamically generated based on natural language information, which can help extract differentiated visual features for different multi-modal examples. Based on the LaConv module, we further build a fully language-driven convolution network, termed as LaConvNet, which can unify the visual recognition and multi-modal reasoning in one forward structure. To validate LaConv and LaConvNet, we conduct extensive experiments on seven benchmark datasets of three vision-and-language tasks, i.e., visual question answering, referring expression comprehension and segmentation. The experimental results not only show the competitive or better performance of LaConvNet against existing multi-modal networks, but also witness the merits of LaConvNet as an unified structure, including compact network, low computational cost and high generalization ability. Our source code is released in SimREC project: https://github.com/luogen1996/LaConvNet .},
  archive      = {J_IJCV},
  author       = {Luo, Gen and Zhou, Yiyi and Sun, Xiaoshuai and Wu, Yongjian and Gao, Yue and Ji, Rongrong},
  doi          = {10.1007/s11263-023-01871-1},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards language-guided visual recognition via dynamic convolutions},
  volume       = {132},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
