<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---53">MAM - 53</h2>
<ul>
<li><details>
<summary>
(2024). A justifiable investment in AI for healthcare: Aligning
ambition with reality. <em>MAM</em>, <em>34</em>(4), 1–40. (<a
href="https://doi.org/10.1007/s11023-024-09692-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare systems are grappling with critical challenges, including chronic diseases in aging populations, unprecedented health care staffing shortages and turnover, scarce resources, unprecedented demands and wait times, escalating healthcare expenditure, and declining health outcomes. As a result, policymakers and healthcare executives are investing in artificial intelligence (AI) solutions to increase operational efficiency, lower health care costs, and improve patient care. However, current level of investment in developing healthcare AI among members of the global digital health partnership does not seem to yield a high return yet. This is mainly due to underinvestment in the supporting infrastructure necessary to enable the successful implementation of AI. If a healthcare-specific AI winter is to be avoided, it is paramount that this disparity in the level of investment in the development of AI itself and in the development of the necessary supporting system components is evened out.},
  archive      = {J_MAM},
  author       = {Karpathakis, Kassandra and Morley, Jessica and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09692-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Minds Mach.},
  title        = {A justifiable investment in AI for healthcare: Aligning ambition with reality},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for the internal democracy of
political parties. <em>MAM</em>, <em>34</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11023-024-09693-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article argues that AI can enhance the measurement and implementation of democratic processes within political parties, known as Intra-Party Democracy (IPD). It identifies the limitations of traditional methods for measuring IPD, which often rely on formal parameters, self-reported data, and tools like surveys. Such limitations lead to partial data collection, rare updates, and significant resource demands. To address these issues, the article suggests that specific data management and Machine Learning techniques, such as natural language processing and sentiment analysis, can improve the measurement and practice of IPD.},
  archive      = {J_MAM},
  author       = {Novelli, Claudio and Formisano, Giuliano and Juneja, Prathm and Sandri, Giulia and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09693-x},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence for the internal democracy of political parties},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mapping the ethics of generative AI: A comprehensive scoping
review. <em>MAM</em>, <em>34</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s11023-024-09694-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.},
  archive      = {J_MAM},
  author       = {Hagendorff, Thilo},
  doi          = {10.1007/s11023-024-09694-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Minds Mach.},
  title        = {Mapping the ethics of generative AI: A comprehensive scoping review},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fl-IRT-ing with psychometrics to improve NLP bias
measurement. <em>MAM</em>, <em>34</em>(4), 1–34. (<a
href="https://doi.org/10.1007/s11023-024-09695-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To prevent ordinary people from being harmed by natural language processing (NLP) technology, finding ways to measure the extent to which a language model is biased (e.g., regarding gender) has become an active area of research. One popular class of NLP bias measures are bias benchmark datasets—collections of test items that are meant to assess a language model’s preference for stereotypical versus non-stereotypical language. In this paper, we argue that such bias benchmarks should be assessed with models from the psychometric framework of item response theory (IRT). Specifically, we tie an introduction to basic IRT concepts and models with a discussion of how they could be relevant to the evaluation, interpretation and improvement of bias benchmark datasets. Regarding evaluation, IRT provides us with methodological tools for assessing the quality of both individual test items (e.g., the extent to which an item can differentiate highly biased from less biased language models) as well as benchmarks as a whole (e.g., the extent to which the benchmark allows us to assess not only severe but also subtle levels of model bias). Through such diagnostic tools, the quality of benchmark datasets could be improved, for example by deleting or reworking poorly performing items. Finally, in regards to interpretation, we argue that IRT models’ estimates for language model bias are conceptually superior to traditional accuracy-based evaluation metrics, as the former take into account more information than just whether or not a language model provided a biased response.},
  archive      = {J_MAM},
  author       = {Bachmann, Dominik and van der Wal, Oskar and Chvojka, Edita and Zuidema, Willem H. and van Maanen, Leendert and Schulz, Katrin},
  doi          = {10.1007/s11023-024-09695-9},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Minds Mach.},
  title        = {Fl-IRT-ing with psychometrics to improve NLP bias measurement},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beneficent intelligence: A capability approach to modeling
benefit, assistance, and associated moral failures through AI systems.
<em>MAM</em>, <em>34</em>(4), 1–37. (<a
href="https://doi.org/10.1007/s11023-024-09696-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing discourse around AI ethics lacks the language and formalism necessary to capture the diverse ethical concerns that emerge when AI systems interact with individuals. Drawing on Sen and Nussbaum’s capability approach, we present a framework formalizing a network of ethical concepts and entitlements necessary for AI systems to confer meaningful benefit or assistance to stakeholders. Such systems enhance stakeholders’ ability to advance their life plans and well-being while upholding their fundamental rights. We characterize two necessary conditions for morally permissible interactions between AI systems and those impacted by their functioning, and two sufficient conditions for realizing the ideal of meaningful benefit. We then contrast this ideal with several salient failure modes, namely, forms of social interactions that constitute unjustified paternalism, coercion, deception, exploitation and domination. The proliferation of incidents involving AI in high-stakes domains underscores the gravity of these issues and the imperative to take an ethics-led approach to AI systems from their inception.},
  archive      = {J_MAM},
  author       = {London, Alex John and Heidari, Hoda},
  doi          = {10.1007/s11023-024-09696-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-37},
  shortjournal = {Minds Mach.},
  title        = {Beneficent intelligence: A capability approach to modeling benefit, assistance, and associated moral failures through AI systems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The inherent normativity of concepts. <em>MAM</em>,
<em>34</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09697-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept normativity is a prominent subject of inquiry in the philosophical literature on the nature of concepts. Concepts are said to be normative, in that the use of concepts to categorise is associated with an evaluation of the appropriateness of such categorisation measured against some objective external standard. Two broad groups of views have emerged in accounting for the normativity of concepts: a weaker view traces such normativity to the social practice in which the agent using the concept is embedded, while a stronger view traces such normativity to a first-person capacity of reflection. However, both views have drawbacks: the weaker view seems not to do justice to the basic sense of normativity associated with an individual agent using a concept, while the stronger view ties such normativity with the first-person conscious evaluation, which appears to be too strong. Here, we propose a different view of concepts using principles from the Active Inference framework. We reconceive concepts, defining them as Bayesian beliefs—that is, conditional probability distributions—that represent causes and contingencies in the world, their form grounded in the exchange between the agent and its environment. This allows us to present a different view on the source of normativity, with an emphasis on the structure of the agent itself as well as its interaction with the environment. On the Active Inference view, concepts are normative in that they are intrinsically connected to the self-evidencing nature of an agent, whose very structure implies an evaluation of the concepts it employs.},
  archive      = {J_MAM},
  author       = {So, Wing Yi and Friston, Karl J. and Neacsu, Victorita},
  doi          = {10.1007/s11023-024-09697-7},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {The inherent normativity of concepts},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imitation and large language models. <em>MAM</em>,
<em>34</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s11023-024-09698-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of imitation is both ubiquitous and curiously under-analysed in theoretical discussions about the cognitive powers and capacities of machines, and in particular—for what is the focus of this paper—the cognitive capacities of large language models (LLMs). The question whether LLMs understand what they say and what is said to them, for instance, is a disputed one, and it is striking to see this concept of imitation being mobilised here for sometimes contradictory purposes. After illustrating and discussing how this concept is being used in various ways in the context of conversational systems, I draw a sketch of the different associations that the term ‘imitation’ conveys and distinguish two main senses of the notion. The first one is what I call the ‘imitative behaviour’ and the second is what I call the ‘status of imitation’. I then highlight and untangle some conceptual difficulties with these two senses and conclude that neither of these applies to LLMs. Finally, I introduce an appropriate description that I call ‘imitation manufacturing’. All this ultimately helps me to explore a radical negative answer to the question of machine understanding.},
  archive      = {J_MAM},
  author       = {Boisseau, Éloïse},
  doi          = {10.1007/s11023-024-09698-6},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {Imitation and large language models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is unsupervised clustering somehow truer? <em>MAM</em>,
<em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11023-024-09699-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists increasingly approach the world through machine learning techniques, but philosophers of science often question their epistemic status. Some philosophers have argued that the use of unsupervised clustering algorithms is more justified than the use of supervised classification, because supervised classification is more biased, and because (parametric) simplicity plays a different and more interesting role in unsupervised clustering. I call these arguments the No-Bias Argument and the Simplicity-Truth Argument. I show how both arguments are fallacious and how, on the contrary, the use of supervised classification is at least as justified as the use of unsupervised clustering.},
  archive      = {J_MAM},
  author       = {Søgaard, Anders},
  doi          = {10.1007/s11023-024-09699-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Minds Mach.},
  title        = {Is unsupervised clustering somehow truer?},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding with toy surrogate models in machine learning.
<em>MAM</em>, <em>34</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11023-024-09700-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the natural and social sciences, it is common to use toy models—extremely simple and highly idealized representations—to understand complex phenomena. Some of the simple surrogate models used to understand opaque machine learning (ML) models, such as rule lists and sparse decision trees, bear some resemblance to scientific toy models. They allow non-experts to understand how an opaque ML model works globally via a much simpler model that highlights the most relevant features of the input space and their effect on the output. The obvious difference is that the common target of a toy and a full-scale model in the sciences is some phenomenon in the world, while the target of a surrogate model is another model. This essential difference makes toy surrogate models (TSMs) a new object of study for theories of understanding, one that is not easily accommodated under current analyses. This paper provides an account of what it means to understand an opaque ML model globally with the aid of such simple models.},
  archive      = {J_MAM},
  author       = {Páez, Andrés},
  doi          = {10.1007/s11023-024-09700-1},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Minds Mach.},
  title        = {Understanding with toy surrogate models in machine learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence, discrimination, fairness, and other
moral concerns. <em>MAM</em>, <em>34</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s11023-024-09702-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Should the input data of artificial intelligence (AI) systems include factors such as race or sex when these factors may be indicative of morally significant facts? More importantly, is it wrong to rely on the output of AI tools whose input includes factors such as race or sex? And is it wrong to rely on the output of AI systems when it is correlated with factors such as race or sex (whether or not its input includes such factors)? The answers to these questions are controversial. In this paper, I argue for the following claims. First, since factors such as race or sex are not morally significant in themselves, including such factors in the input data, or relying on output that includes such factors or is correlated with them, is neither objectionable (for example, unfair) nor commendable in itself. Second, sometimes (but not always) there are derivative reasons against such actions due to the relationship between factors such as race or sex and facts that are morally significant (ultimately) in themselves. Finally, even if there are such derivative reasons, they are not necessarily decisive since there are sometimes also countervailing reasons. Accordingly, the moral status of the above actions is contingent.},
  archive      = {J_MAM},
  author       = {Segev, Re’em},
  doi          = {10.1007/s11023-024-09702-z},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence, discrimination, fairness, and other moral concerns},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Models of possibilities instead of logic as the basis of
human reasoning. <em>MAM</em>, <em>34</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s11023-024-09662-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theory of mental models and its computer implementations have led to crucial experiments showing that no standard logic—the sentential calculus and all logics that include it—can underlie human reasoning. The theory replaces the logical concept of validity (the conclusion is true in all cases in which the premises are true) with necessity (conclusions describe no more than possibilities to which the premises refer). Many inferences are both necessary and valid. But experiments show that individuals make necessary inferences that are invalid, e.g., Few people ate steak or sole; therefore, few people ate steak. Other crucial experiments show that individuals reject inferences that are not necessary but valid, e.g., He had the anesthetic or felt pain, but not both; therefore, he had the anesthetic or felt pain, or both. Nothing in logic can justify the rejection of a valid inference: a denial of its conclusion is inconsistent with its premises, and inconsistencies yield valid inferences of any conclusions whatsoever including the one denied. So inconsistencies are catastrophic in logic. In contrast, the model theory treats all inferences as defeasible (nonmonotonic), and inconsistencies have the null model, which yields only the null model in conjunction with any other premises. So inconsistences are local. Which allows truth values in natural languages to be much richer than those that occur in the semantics of standard logics; and individuals verify assertions on the basis of both facts and possibilities that did not occur.},
  archive      = {J_MAM},
  author       = {Johnson-Laird, P. N. and Byrne, Ruth M. J. and Khemlani, Sangeet S.},
  doi          = {10.1007/s11023-024-09662-4},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Minds Mach.},
  title        = {Models of possibilities instead of logic as the basis of human reasoning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human autonomy at risk? An analysis of the challenges from
AI. <em>MAM</em>, <em>34</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09665-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomy is a core value that is deeply entrenched in the moral, legal, and political practices of many societies. The development and deployment of artificial intelligence (AI) have raised new questions about AI’s impacts on human autonomy. However, systematic assessments of these impacts are still rare and often held on a case-by-case basis. In this article, I provide a conceptual framework that both ties together seemingly disjoint issues about human autonomy, as well as highlights differences between them. In the first part, I distinguish between distinct concerns that are currently addressed under the umbrella term ‘human autonomy’. In particular, I show how differentiating between autonomy-as-authenticity and autonomy-as-agency helps us to pinpoint separate challenges from AI deployment. Some of these challenges are already well-known (e.g. online manipulation or limitation of freedom), whereas others have received much less attention (e.g. adaptive preference formation). In the second part, I address the different roles AI systems can assume in the context of autonomy. In particular, I differentiate between AI systems taking on agential roles and AI systems being used as tools. I conclude that while there is no ‘silver bullet’ to address concerns about human autonomy, considering its various dimensions can help us to systematically address the associated risks.},
  archive      = {J_MAM},
  author       = {Prunkl, Carina},
  doi          = {10.1007/s11023-024-09665-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Human autonomy at risk? an analysis of the challenges from AI},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A teleological approach to information systems design.
<em>MAM</em>, <em>34</em>(3), 1–35. (<a
href="https://doi.org/10.1007/s11023-024-09673-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the design and production of information systems have seen significant growth. However, these information artefacts often exhibit characteristics that compromise their reliability. This issue appears to stem from the neglect or underestimation of certain crucial aspects in the application of Information Systems Design (ISD). For example, it is frequently difficult to prove when one of these products does not work properly or works incorrectly (falsifiability), their usage is often left to subjective experience and somewhat arbitrary choices (anecdotes), and their functions are often obscure for users as well as designers (explainability). In this paper, we propose an approach that can be used to support the analysis and re-(design) of information systems grounded on a well-known theory of information, namely, teleosemantics. This approach emphasizes the importance of grounding the design and validation process on dependencies between four core components: the producer (or designer), the produced (or used) information system, the consumer (or user), and the design (or use) purpose. We analyze the ambiguities and problems of considering these components separately. We then present some possible ways in which they can be combined through the teleological approach. Also, we debate guidelines to prevent ISD from failing to address critical issues. Finally, we discuss perspectives on applications over real existing information technologies and some implications for explainable AI and ISD.},
  archive      = {J_MAM},
  author       = {Fumagalli, Mattia and Ferrario, Roberta and Guizzardi, Giancarlo},
  doi          = {10.1007/s11023-024-09673-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-35},
  shortjournal = {Minds Mach.},
  title        = {A teleological approach to information systems design},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Find the gap: AI, responsible agency and vulnerability.
<em>MAM</em>, <em>34</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11023-024-09674-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The responsibility gap, commonly described as a core challenge for the effective governance of, and trust in, AI and autonomous systems (AI/AS), is traditionally associated with a failure of the epistemic and/or the control condition of moral responsibility: the ability to know what we are doing and exercise competent control over this doing. Yet these two conditions are a red herring when it comes to understanding the responsibility challenges presented by AI/AS, since evidence from the cognitive sciences shows that individual humans face very similar responsibility challenges with regard to these two conditions. While the problems of epistemic opacity and attenuated behaviour control are not unique to AI/AS technologies (though they can be exacerbated by them), we show that we can learn important lessons for AI/AS development and governance from how philosophers have recently revised the traditional concept of moral responsibility in response to these challenges to responsible human agency from the cognitive sciences. The resulting instrumentalist views of responsibility, which emphasize the forward-looking and flexible role of agency cultivation, hold considerable promise for integrating AI/AS into a healthy moral ecology. We note that there nevertheless is a gap in AI/AS responsibility that has yet to be extensively studied and addressed, one grounded in a relational asymmetry of vulnerability between human agents and sociotechnical systems like AI/AS. In the conclusion of this paper we note that attention to this vulnerability gap must inform and enable future attempts to construct trustworthy AI/AS systems and preserve the conditions for responsible human agency.},
  archive      = {J_MAM},
  author       = {Vallor, Shannon and Vierkant, Tillmann},
  doi          = {10.1007/s11023-024-09674-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Find the gap: AI, responsible agency and vulnerability},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In the craftsman’s garden: AI, alan turing, and stanley
cavell. <em>MAM</em>, <em>34</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11023-024-09676-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is rising skepticism within public discourse about the nature of AI. By skepticism, I mean doubt about what we know about AI. At the same time, some AI speakers are raising the kinds of issues that usually really matter in analysis, such as issues relating to consent and coercion. This essay takes up the question of whether we should analyze a conversation differently because it is between a human and AI instead of between two humans and, if so, why. When is it okay, for instance, to read the phrases “please stop” or “please respect my boundaries” as meaning something other than what those phrases ordinarily mean – and what makes it so? If we ignore denials of consent, or put them in scare quotes, we should have a good reason. This essay focuses on two thinkers, Alan Turing and Stanley Cavell, who in different ways answer the question of whether it matters that a speaker is a machine. It proposes that Cavell’s work on the problem of other minds, in particular Cavell’s story in The Claim of Reason of an automaton whom he imagines meeting in a craftsman’s garden, may be especially helpful in thinking about how to analyze what AI has to say.},
  archive      = {J_MAM},
  author       = {O’Connor, Marie Theresa},
  doi          = {10.1007/s11023-024-09676-y},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {In the craftsman’s garden: AI, alan turing, and stanley cavell},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The hierarchical correspondence view of levels: A case study
in cognitive science. <em>MAM</em>, <em>34</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09678-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a general conception of levels in philosophy which says that the world is arrayed into a hierarchy of levels and that there are different modes of analysis that correspond to each level of this hierarchy, what can be labelled the ‘Hierarchical Correspondence View of Levels” (or HCL). The trouble is that despite its considerable lineage and general status in philosophy of science and metaphysics the HCL has largely escaped analysis in specific domains of inquiry. The goal of this paper is to take up a recent call to domain-specificity by examining the role of the HCL in cognitive science. I argue that the HCL is, in fact, a conception of levels that has been employed in cognitive science and that cognitive scientists should avoid its use where possible. The argument is that the HCL is problematic when applied to cognitive science specifically because it fails to distinguish two important kinds of shifts used when analysing information processing systems: shifts in grain and shifts in analysis. I conclude by proposing a revised version of the HCL which accommodates the distinction.},
  archive      = {J_MAM},
  author       = {Kersten, Luke},
  doi          = {10.1007/s11023-024-09678-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {The hierarchical correspondence view of levels: A case study in cognitive science},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The new mechanistic approach and cognitive ontology—or: What
role do (neural) mechanisms play in cognitive ontology? <em>MAM</em>,
<em>34</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11023-024-09679-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive ontology has become a popular topic in philosophy, cognitive psychology, and cognitive neuroscience. At its center is the question of which cognitive capacities should be included in the ontology of cognitive psychology and cognitive neuroscience. One common strategy for answering this question is to look at brain structures and determine the cognitive capacities for which they are responsible. Some authors interpret this strategy as a search for neural mechanisms, as understood by the so-called new mechanistic approach. In this article, I will show that this new mechanistic answer is confronted with what I call the triviality problem. A discussion of this problem will show that one cannot derive a meaningful cognitive ontology from neural mechanisms alone. Nonetheless, neural mechanisms play a crucial role in the discovery of a cognitive ontology because they are epistemic proxies for best systematizations.},
  archive      = {J_MAM},
  author       = {Krickel, Beate},
  doi          = {10.1007/s11023-024-09679-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {The new mechanistic approach and cognitive Ontology—Or: What role do (Neural) mechanisms play in cognitive ontology?},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sociotechnical system perspective on AI. <em>MAM</em>,
<em>34</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11023-024-09680-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Kudina, Olya and van de Poel, Ibo},
  doi          = {10.1007/s11023-024-09680-2},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Minds Mach.},
  title        = {A sociotechnical system perspective on AI},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experts or authorities? The strange case of the presumed
epistemic superiority of artificial intelligence systems. <em>MAM</em>,
<em>34</em>(3), 1–27. (<a
href="https://doi.org/10.1007/s11023-024-09681-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high predictive accuracy of contemporary machine learning-based AI systems has led some scholars to argue that, in certain cases, we should grant them epistemic expertise and authority over humans. This approach suggests that humans would have the epistemic obligation of relying on the predictions of a highly accurate AI system. Contrary to this view, in this work we claim that it is not possible to endow AI systems with a genuine account of epistemic expertise. In fact, relying on accounts of expertise and authority from virtue epistemology, we show that epistemic expertise requires a relation with understanding that AI systems do not satisfy and intellectual abilities that these systems do not manifest. Further, following the Distribution Cognition theory and adapting an account by Croce on the virtues of collective epistemic agents to the case of human-AI interactions we show that, if an AI system is successfully appropriated by a human agent, a hybrid epistemic agent emerges, which can become both an epistemic expert and an authority. Consequently, we claim that the aforementioned hybrid agent is the appropriate object of a discourse around trust in AI and the epistemic obligations that stem from its epistemic superiority.},
  archive      = {J_MAM},
  author       = {Ferrario, Andrea and Facchini, Alessandro and Termine, Alberto},
  doi          = {10.1007/s11023-024-09681-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Minds Mach.},
  title        = {Experts or authorities? the strange case of the presumed epistemic superiority of artificial intelligence systems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability and interpretability in science and deep
learning. <em>MAM</em>, <em>34</em>(3), 1–31. (<a
href="https://doi.org/10.1007/s11023-024-09682-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models—and in particular Deep Neural Network (DNN) models—which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown to be language-independent. It is argued that the high epistemic complexity of DNN models hinders the estimate of their reliability and also their prospect of long term progress. Some potential ways forward are suggested. Thirdly, this article identifies the close relation between a model’s epistemic complexity and its interpretability, as introduced in the context of responsible AI. This clarifies in which sense—and to what extent—the lack of understanding of a model (black-box problem) impacts its interpretability in a way that is independent of individual skills. It also clarifies how interpretability is a precondition for a plausible assessment of the reliability of any model, which cannot be based on statistical analysis alone. This article focuses on the comparison between traditional scientific models and DNN models. However, Random Forest (RF) and Logistic Regression (LR) models are also briefly considered.},
  archive      = {J_MAM},
  author       = {Scorzato, Luigi},
  doi          = {10.1007/s11023-024-09682-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Minds Mach.},
  title        = {Reliability and interpretability in science and deep learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Submarine cables and the risks to digital sovereignty.
<em>MAM</em>, <em>34</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11023-024-09683-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The international network of submarine cables plays a crucial role in facilitating global telecommunications connectivity, carrying over 99% of all internet traffic. However, submarine cables challenge digital sovereignty due to their ownership structure, cross-jurisdictional nature, and vulnerabilities to malicious actors. In this article, we assess these challenges, current policy initiatives designed to mitigate them, and the limitations of these initiatives. The nature of submarine cables curtails a state’s ability to regulate the infrastructure on which it relies, reduces its data security, and threatens its ability to provide telecommunication services. States currently address these challenges through regulatory controls over submarine cables and associated companies, investing in the development of additional cable infrastructure, and implementing physical protection measures for the cables themselves. Despite these efforts, the effectiveness of current mechanisms is hindered by significant obstacles arising from technical limitations and a lack of international coordination on regulation. We conclude by noting how these obstacles lead to gaps in states’ policies and point towards how they could be improved to create a proactive approach to submarine cable governance that defends states’ digital sovereignty.},
  archive      = {J_MAM},
  author       = {Ganz, Abra and Camellini, Martina and Hine, Emmie and Novelli, Claudio and Roberts, Huw and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09683-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Submarine cables and the risks to digital sovereignty},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “The human must remain the central focus”: Subjective
fairness perceptions in automated decision-making. <em>MAM</em>,
<em>34</em>(3), 1–37. (<a
href="https://doi.org/10.1007/s11023-024-09684-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of algorithms in allocating resources and services in both private industry and public administration has sparked discussions about their consequences for inequality and fairness in contemporary societies. Previous research has shown that the use of automated decision-making (ADM) tools in high-stakes scenarios like the legal justice system might lead to adverse societal outcomes, such as systematic discrimination. Scholars have since proposed a variety of metrics to counteract and mitigate biases in ADM processes. While these metrics focus on technical fairness notions, they do not consider how members of the public, as most affected subjects by algorithmic decisions, perceive fairness in ADM. To shed light on subjective fairness perceptions of individuals, this study analyzes individuals’ answers to open-ended fairness questions about hypothetical ADM scenarios that were embedded in the German Internet Panel (Wave 54, July 2021), a probability-based longitudinal online survey. Respondents evaluated the fairness of vignettes describing the use of ADM tools across different contexts. Subsequently, they explained their fairness evaluation providing a textual answer. Using qualitative content analysis, we inductively coded those answers (N = 3697). Based on their individual understanding of fairness, respondents addressed a wide range of aspects related to fairness in ADM which is reflected in the 23 codes we identified. We subsumed those codes under four overarching themes: Human elements in decision-making, Shortcomings of the data, Social impact of AI, and Properties of AI. Our codes and themes provide a valuable resource for understanding which factors influence public fairness perceptions about ADM.},
  archive      = {J_MAM},
  author       = {Szafran, Daria and Bach, Ruben L.},
  doi          = {10.1007/s11023-024-09684-y},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Minds Mach.},
  title        = {“The human must remain the central focus”: Subjective fairness perceptions in automated decision-making},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a responsible fairness analysis: From binary to
multiclass and multigroup assessment in graph neural network-based user
modeling tasks. <em>MAM</em>, <em>34</em>(3), 1–34. (<a
href="https://doi.org/10.1007/s11023-024-09685-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User modeling is a key topic in many applications, mainly social networks and information retrieval systems. To assess the effectiveness of a user modeling approach, its capability to classify personal characteristics (e.g., the gender, age, or consumption grade of the users) is evaluated. Due to the fact that some of the attributes to predict are multiclass (e.g., age usually encompasses multiple ranges), assessing fairness in user modeling becomes a challenge since most of the related metrics work with binary attributes. As a workaround, the original multiclass attributes are usually binarized to meet standard fairness metrics definitions where both the target class and sensitive attribute (such as gender or age) are binary. However, this alters the original conditions, and fairness is evaluated on classes that differ from those used in the classification. In this article, we extend the definitions of four existing fairness metrics (related to disparate impact and disparate mistreatment) from binary to multiclass scenarios, considering different settings where either the target class or the sensitive attribute includes more than two groups. Our work endeavors to bridge the gap between formal definitions and real use cases in bias detection. The results of the experiments, conducted on four real-world datasets by leveraging two state-of-the-art graph neural network-based models for user modeling, show that the proposed generalization of fairness metrics can lead to a more effective and fine-grained comprehension of disadvantaged sensitive groups and, in some cases, to a better analysis of machine learning models originally deemed to be fair. The source code and the preprocessed datasets are available at the following link: https://github.com/erasmopurif/toward-responsible-fairness-analysis .},
  archive      = {J_MAM},
  author       = {Purificato, Erasmo and Boratto, Ludovico and De Luca, Ernesto William},
  doi          = {10.1007/s11023-024-09685-x},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-34},
  shortjournal = {Minds Mach.},
  title        = {Toward a responsible fairness analysis: From binary to multiclass and multigroup assessment in graph neural network-based user modeling tasks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anthropomorphizing machines: Reality or popular myth?
<em>MAM</em>, <em>34</em>(3), 1–25. (<a
href="https://doi.org/10.1007/s11023-024-09686-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to a widespread view, people often anthropomorphize machines such as certain robots and computer and AI systems by erroneously attributing mental states to them. On this view, people almost irresistibly believe, even if only subconsciously, that machines with certain human-like features really have phenomenal or subjective experiences like sadness, happiness, desire, pain, joy, and distress, even though they lack such feelings. This paper questions this view by critiquing common arguments used to support it and by suggesting an alternative explanation. Even if people’s behavior and language regarding human-like machines suggests they believe those machines really have mental states, it is possible that they do not believe that at all. The paper also briefly discusses potential implications of regarding such anthropomorphism as a popular myth. The exercise illuminates the difficult concept of anthropomorphism, helping to clarify possible human relations with or toward machines that increasingly resemble humans and animals.},
  archive      = {J_MAM},
  author       = {Coghlan, Simon},
  doi          = {10.1007/s11023-024-09686-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Anthropomorphizing machines: Reality or popular myth?},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Toward sociotechnical AI: Mapping
vulnerabilities for machine learning in context. <em>MAM</em>,
<em>34</em>(3), 1–2. (<a
href="https://doi.org/10.1007/s11023-024-09687-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Dobbe, Roel and Wolters, Anouk},
  doi          = {10.1007/s11023-024-09687-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-2},
  shortjournal = {Minds Mach.},
  title        = {Correction to: toward sociotechnical AI: mapping vulnerabilities for machine learning in context},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unfairness in AI anti-corruption tools: Main drivers and
consequences. <em>MAM</em>, <em>34</em>(3), 1–35. (<a
href="https://doi.org/10.1007/s11023-024-09688-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the potential sources and consequences of unfairness in artificial intelligence (AI) predictive tools used for anti-corruption efforts. Using the examples of three AI-based anti-corruption tools from Brazil—risk estimation of corrupt behaviour in public procurement, among public officials, and of female straw candidates in electoral contests—it illustrates how unfairness can emerge at the infrastructural, individual, and institutional levels. The article draws on interviews with law enforcement officials directly involved in the development of anti-corruption tools, as well as academic and grey literature, including official reports and dissertations on the tools used as examples. Potential sources of unfairness include problematic data, statistical learning issues, the personal values and beliefs of developers and users, and the governance and practices within the organisations in which these tools are created and deployed. The findings suggest that the tools analysed were trained using inputs from past anti-corruption procedures and practices and based on common sense assumptions about corruption, which are not necessarily free from unfair disproportionality and discrimination. In designing the ACTs, the developers did not reflect on the risks of unfairness, nor did they prioritise the use of specific technological solutions to identify and mitigate this type of problem. Although the tools analysed do not make automated decisions and only support human action, their algorithms are not open to external scrutiny.},
  archive      = {J_MAM},
  author       = {Odilla, Fernanda},
  doi          = {10.1007/s11023-024-09688-8},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-35},
  shortjournal = {Minds Mach.},
  title        = {Unfairness in AI anti-corruption tools: Main drivers and consequences},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A causal analysis of harm. <em>MAM</em>, <em>34</em>(3),
1–24. (<a href="https://doi.org/10.1007/s11023-024-09689-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As autonomous systems rapidly become ubiquitous, there is a growing need for a legal and regulatory framework that addresses when and how such a system harms someone. There have been several attempts within the philosophy literature to define harm, but none of them has proven capable of dealing with the many examples that have been presented, leading some to suggest that the notion of harm should be abandoned and “replaced by more well-behaved notions”. As harm is generally something that is caused, most of these definitions have involved causality at some level. Yet surprisingly, none of them makes use of causal models and the definitions of actual causality that they can express. In this paper, which is an expanded version of the conference paper Beckers et al. (Adv Neural Inform Process Syst 35:2365–2376, 2022), we formally define a qualitative notion of harm that uses causal models and is based on a well-known definition of actual causality. The key features of our definition are that it is based on contrastive causation and uses a default utility to which the utility of actual outcomes is compared. We show that our definition is able to handle the examples from the literature, and illustrate its importance for reasoning about situations involving autonomous systems.},
  archive      = {J_MAM},
  author       = {Beckers, Sander and Chockler, Hana and Halpern, Joseph Y.},
  doi          = {10.1007/s11023-024-09689-7},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {A causal analysis of harm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measure for measure: Operationalising cognitive realism.
<em>MAM</em>, <em>34</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11023-024-09690-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a measure of realism from within the framework of cognitive structural realism (CSR). It argues that in the context of CSR, realism can be operationalised in terms of balance between accuracy and generality. More specifically, the paper draws on the free energy principle to characterise the measure of realism in terms of the balance between accuracy and generality.},
  archive      = {J_MAM},
  author       = {Beni, Majid D.},
  doi          = {10.1007/s11023-024-09690-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {Measure for measure: Operationalising cognitive realism},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scientific inference with interpretable machine learning:
Analyzing models to learn about real-world phenomena. <em>MAM</em>,
<em>34</em>(3), 1–39. (<a
href="https://doi.org/10.1007/s11023-024-09691-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To learn about real world phenomena, scientists have traditionally used models with clearly interpretable elements. However, modern machine learning (ML) models, while powerful predictors, lack this direct elementwise interpretability (e.g. neural network weights). Interpretable machine learning (IML) offers a solution by analyzing models holistically to derive interpretations. Yet, current IML research is focused on auditing ML models rather than leveraging them for scientific inference. Our work bridges this gap, presenting a framework for designing IML methods—termed ’property descriptors’—that illuminate not just the model, but also the phenomenon it represents. We demonstrate that property descriptors, grounded in statistical learning theory, can effectively reveal relevant properties of the joint probability distribution of the observational data. We identify existing IML methods suited for scientific inference and provide a guide for developing new descriptors with quantified epistemic uncertainty. Our framework empowers scientists to harness ML models for inference, and provides directions for future IML research to support scientific understanding.},
  archive      = {J_MAM},
  author       = {Freiesleben, Timo and König, Gunnar and Molnar, Christoph and Tejero-Cantero, Álvaro},
  doi          = {10.1007/s11023-024-09691-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-39},
  shortjournal = {Minds Mach.},
  title        = {Scientific inference with interpretable machine learning: Analyzing models to learn about real-world phenomena},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI within online discussions: Rational, civil, privileged?
<em>MAM</em>, <em>34</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11023-024-09658-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While early optimists have seen online discussions as potential spaces for deliberation, the reality of many online spaces is characterized by incivility and irrationality. Increasingly, AI tools are considered as a solution to foster deliberative discourse. Against the backdrop of previous research, we show that AI tools for online discussions heavily focus on the deliberative norms of rationality and civility. In the operationalization of those norms for AI tools, the complex deliberative dimensions are simplified, and the focus lies on the detection of argumentative structures in argument mining or verbal markers of supposedly uncivil comments. If the fairness of such tools is considered, the focus lies on data bias and an input–output frame of the problem. We argue that looking beyond bias and analyzing such applications through a sociotechnical frame reveals how they interact with social hierarchies and inequalities, reproducing patterns of exclusion. The current focus on verbal markers of incivility and argument mining risks excluding minority voices and privileges those who have more access to education. Finally, we present a normative argument why examining AI tools for online discourses through a sociotechnical frame is ethically preferable, as ignoring the predicable negative effects we describe would present a form of objectionable indifference.},
  archive      = {J_MAM},
  author       = {Carstens, Jonas Aaron and Friess, Dennis},
  doi          = {10.1007/s11023-024-09658-0},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {AI within online discussions: Rational, civil, privileged?},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards transnational fairness in machine learning: A case
study in disaster response systems. <em>MAM</em>, <em>34</em>(2), 1–26.
(<a href="https://doi.org/10.1007/s11023-024-09663-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on fairness in machine learning (ML) has been largely focusing on individual and group fairness. With the adoption of ML-based technologies as assistive technology in complex societal transformations or crisis situations on a global scale these existing definitions fail to account for algorithmic fairness transnationally. We propose to complement existing perspectives on algorithmic fairness with a notion of transnational algorithmic fairness and take first steps towards an analytical framework. We exemplify the relevance of a transnational fairness assessment in a case study on a disaster response system using images from online social media. In the presented case, ML systems are used as a support tool in categorizing and classifying images from social media after a disaster event as an almost instantly available source of information for coordinating disaster response. We present an empirical analysis assessing the transnational fairness of the application’s outputs-based on national socio-demographic development indicators as potentially discriminatory attributes. In doing so, the paper combines interdisciplinary perspectives from data analytics, ML, digital media studies and media sociology in order to address fairness beyond the technical system. The case study investigated reflects an embedded perspective of peoples’ everyday media use and social media platforms as the producers of sociality and processing data-with relevance far beyond the case of algorithmic fairness in disaster scenarios. Especially in light of the concentration of artificial intelligence (AI) development in the Global North and a perceived hegemonic constellation, we argue that transnational fairness offers a perspective on global injustices in relation to AI development and application that has the potential to substantiate discussions by identifying gaps in data and technology. These analyses ultimately will enable researchers and policy makers to derive actionable insights that could alleviate existing problems with fair use of AI technology and mitigate risks associated with future developments.},
  archive      = {J_MAM},
  author       = {Kozcuer, Cem and Mollen, Anne and Bießmann, Felix},
  doi          = {10.1007/s11023-024-09663-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Minds Mach.},
  title        = {Towards transnational fairness in machine learning: A case study in disaster response systems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reflective artificial intelligence. <em>MAM</em>,
<em>34</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09664-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) technology advances, we increasingly delegate mental tasks to machines. However, today’s AI systems usually do these tasks with an unusual imbalance of insight and understanding: new, deeper insights are present, yet many important qualities that a human mind would have previously brought to the activity are utterly absent. Therefore, it is crucial to ask which features of minds have we replicated, which are missing, and if that matters. One core feature that humans bring to tasks, when dealing with the ambiguity, emergent knowledge, and social context presented by the world, is reflection. Yet this capability is completely missing from current mainstream AI. In this paper we ask what reflective AI might look like. Then, drawing on notions of reflection in complex systems, cognitive science, and agents, we sketch an architecture for reflective AI agents, and highlight ways forward.},
  archive      = {J_MAM},
  author       = {Lewis, Peter R. and Sarkadi, Ştefan},
  doi          = {10.1007/s11023-024-09664-2},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Reflective artificial intelligence},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box testing and auditing of bias in ADM systems.
<em>MAM</em>, <em>34</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s11023-024-09666-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For years, the number of opaque algorithmic decision-making systems (ADM systems) with a large impact on society has been increasing: e.g., systems that compute decisions about future recidivism of criminals, credit worthiness, or the many small decision computing systems within social networks that create rankings, provide recommendations, or filter content. Concerns that such a system makes biased decisions can be difficult to investigate: be it by people affected, NGOs, stakeholders, governmental testing and auditing authorities, or other external parties. Scientific testing and auditing literature rarely focuses on the specific needs for such investigations and suffers from ambiguous terminologies. With this paper, we aim to support this investigation process by collecting, explaining, and categorizing methods of testing for bias, which are applicable to black-box systems, given that inputs and respective outputs can be observed. For this purpose, we provide a taxonomy that can be used to select suitable test methods adapted to the respective situation. This taxonomy takes multiple aspects into account, for example the effort to implement a given test method, its technical requirement (such as the need of ground truth) and social constraints of the investigation, e.g., the protection of business secrets. Furthermore, we analyze which test method can be used in the context of which black box audit concept. It turns out that various factors, such as the type of black box audit or the lack of an oracle, may limit the selection of applicable tests. With the help of this paper, people or organizations who want to test an ADM system for bias can identify which test methods and auditing concepts are applicable and what implications they entail.},
  archive      = {J_MAM},
  author       = {Krafft, Tobias D. and Hauer, Marc P. and Zweig, Katharina},
  doi          = {10.1007/s11023-024-09666-0},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Minds Mach.},
  title        = {Black-box testing and auditing of bias in ADM systems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Toward sociotechnical AI: Mapping vulnerabilities for
machine learning in context. <em>MAM</em>, <em>34</em>(2), 1–51. (<a
href="https://doi.org/10.1007/s11023-024-09668-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an empirical and conceptual account on seeing machine learning models as part of a sociotechnical system to identify relevant vulnerabilities emerging in the context of use. As ML is increasingly adopted in socially sensitive and safety-critical domains, many ML applications end up not delivering on their promises, and contributing to new forms of algorithmic harm. There is still a lack of empirical insights as well as conceptual tools and frameworks to properly understand and design for the impact of ML models in their sociotechnical context. In this paper, we follow a design science research approach to work towards such insights and tools. We center our study in the financial industry, where we first empirically map recently emerging MLOps practices to govern ML applications, and corroborate our insights with recent literature. We then perform an integrative literature research to identify a long list of vulnerabilities that emerge in the sociotechnical context of ML applications, and we theorize these along eight dimensions. We then perform semi-structured interviews in two real-world use cases and across a broad set of relevant actors and organizations, to validate the conceptual dimensions and identify challenges to address sociotechnical vulnerabilities in the design and governance of ML-based systems. The paper proposes a set of guidelines to proactively and integrally address both the dimensions of sociotechnical vulnerability, as well as the challenges identified in the empirical use case research, in the organization of MLOps practices.},
  archive      = {J_MAM},
  author       = {Dobbe, Roel and Wolters, Anouk},
  doi          = {10.1007/s11023-024-09668-y},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-51},
  shortjournal = {Minds Mach.},
  title        = {Toward sociotechnical AI: Mapping vulnerabilities for machine learning in context},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A genealogical approach to algorithmic bias. <em>MAM</em>,
<em>34</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11023-024-09672-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fairness, Accountability, and Transparency (FAccT) literature tends to focus on bias as a problem that requires ex post solutions (e.g. fairness metrics), rather than addressing the underlying social and technical conditions that (re)produce it. In this article, we propose a complementary strategy that uses genealogy as a constructive, epistemic critique to explain algorithmic bias in terms of the conditions that enable it. We focus on XAI feature attributions (Shapley values) and counterfactual approaches as potential tools to gauge these conditions and offer two main contributions. One is constructive: we develop a theoretical framework to classify these approaches according to their relevance for bias as evidence of social disparities. We draw on Pearl’s ladder of causation (Causality: models, reasoning, and inference. Cambridge University Press, Cambridge, 2000, Causality, 2nd edn. Cambridge University Press, Cambridge, 2009. https://doi.org/10.1017/CBO9780511803161 ) to order these XAI approaches concerning their ability to answer fairness-relevant questions and identify fairness-relevant solutions. The other contribution is critical: we evaluate these approaches in terms of their assumptions about the role of protected characteristics in discriminatory outcomes. We achieve this by building on Kohler-Hausmann’s (Northwest Univ Law Rev 113(5):1163–1227, 2019) constructivist theory of discrimination. We derive three recommendations for XAI practitioners to develop and AI policymakers to regulate tools that address algorithmic bias in its conditions and hence mitigate its future occurrence.},
  archive      = {J_MAM},
  author       = {Ziosi, Marta and Watson, David and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09672-2},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Minds Mach.},
  title        = {A genealogical approach to algorithmic bias},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regulation by design: Features, practices, limitations, and
governance implications. <em>MAM</em>, <em>34</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s11023-024-09675-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regulation by design (RBD) is a growing research field that explores, develops, and criticises the regulative function of design. In this article, we provide a qualitative thematic synthesis of the existing literature. The aim is to explore and analyse RBD’s core features, practices, limitations, and related governance implications. To fulfil this aim, we examine the extant literature on RBD in the context of digital technologies. We start by identifying and structuring the core features of RBD, namely the goals, regulators, regulatees, methods, and technologies. Building on that structure, we distinguish among three types of RBD practices: compliance by design, value creation by design, and optimisation by design. We then explore the challenges and limitations of RBD practices, which stem from risks associated with compliance by design, contextual limitations, or methodological uncertainty. Finally, we examine the governance implications of RBD and outline possible future directions of the research field and its practices.},
  archive      = {J_MAM},
  author       = {Prifti, Kostina and Morley, Jessica and Novelli, Claudio and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09675-z},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Regulation by design: Features, practices, limitations, and governance implications},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tool-augmented human creativity. <em>MAM</em>,
<em>34</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11023-024-09677-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creativity is the hallmark of human intelligence. Roli et al. (Frontiers in Ecology and Evolution 9:806283, 2022) state that algorithms cannot achieve human creativity. This paper analyzes cooperation between humans and intelligent algorithmic tools to compensate for algorithms’ limited creativity. The intelligent tools have functionality from the neocortex, the brain’s center for learning, reasoning, planning, and language. The analysis provides four key insights about human-tool cooperation to solve challenging problems. First, no neocortex-based tool without feelings can achieve human creativity. Second, an interactive tool exploring users’ feeling-guided creativity enhances the ability to solve complex problems. Third, user-led abductive reasoning incorporating human creativity is essential to human-tool cooperative problem-solving. Fourth, although stakeholders must take moral responsibility for the adverse impact of tool answers, it is still essential to teach tools moral values to generate trustworthy answers. The analysis concludes that the scientific community should create neocortex-based tools to augment human creativity and enhance problem-solving rather than creating autonomous algorithmic entities with independent but less creative problem-solving.},
  archive      = {J_MAM},
  author       = {Hole, Kjell Jørgen},
  doi          = {10.1007/s11023-024-09677-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Minds Mach.},
  title        = {Tool-augmented human creativity},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a benchmark for scientific understanding in humans
and machines. <em>MAM</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11023-024-09657-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific understanding is a fundamental goal of science. However, there is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of scientific understanding. In this paper, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral conception of understanding, according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion of scientific understanding by considering a set of questions that gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. We suggest building a Scientific Understanding Benchmark (SUB), formed by a set of these tests, allowing for the evaluation and comparison of scientific understanding. Benchmarking plays a crucial role in establishing trust, ensuring quality control, and providing a basis for performance evaluation. By aligning machine and human scientific understanding we can improve their utility, ultimately advancing scientific understanding and helping to discover new insights within machines.},
  archive      = {J_MAM},
  author       = {Barman, Kristian Gonzalez and Caron, Sascha and Claassen, Tom and de Regt, Henk},
  doi          = {10.1007/s11023-024-09657-1},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {Towards a benchmark for scientific understanding in humans and machines},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contentless representationalism? A neglected option between
radical enactivist and predictive processing accounts of representation.
<em>MAM</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09659-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Christias, Dionysis},
  doi          = {10.1007/s11023-024-09659-z},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Contentless representationalism? a neglected option between radical enactivist and predictive processing accounts of representation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epistemology goes AI: A study of GPT-3’s capacity to
generate consistent and coherent ordered sets of propositions on a
single-input-multiple-outputs basis. <em>MAM</em>, <em>34</em>(1), 1–18.
(<a href="https://doi.org/10.1007/s11023-024-09660-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The more we rely on digital assistants, online search engines, and AI systems to revise our system of beliefs and increase our body of knowledge, the less we are able to resort to some independent criterion, unrelated to further digital tools, in order to asses the epistemic reliability of the outputs delivered by them. This raises some important questions to epistemology in general and pressing questions to applied to epistemology in particular. In this paper, we propose an experimental method for the assessment of GPT-3’s capacity to generate consistent and coherent sets of outputs. When several outputs to one and the same input are very repetitive they tend to be consistent with each other, that is they do not contradict each other. But consistency does not make the set of outputs as a whole more informative than the outputs considered individually. We argue that the less informative a set of outputs is, the less coherent it is. We establish a conceptual distinction between consistency and coherence in the light of what some epistemologists refer to as a coherence theories of truth and justification. While much attention has been given to GPT-3’s capacity to produce internally coherent individual outputs, we argue, instead, that more attention should be given to its capacity to produce consistent and coherent outputs generated on a single-input-multiple-outputs basis.},
  archive      = {J_MAM},
  author       = {de Araujo, Marcelo and de Almeida, Guilherme and Nunes, José Luiz},
  doi          = {10.1007/s11023-024-09660-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Minds Mach.},
  title        = {Epistemology goes AI: A study of GPT-3’s capacity to generate consistent and coherent ordered sets of propositions on a single-input-multiple-outputs basis},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gamification, side effects, and praise and blame for
outcomes. <em>MAM</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09661-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Gamification” refers to adding game-like elements to non-game activities so as to encourage participation. Gamification is used in various contexts: apps on phones motivating people to exercise, employers trying to encourage their employees to work harder, social media companies trying to stimulate user engagement, and so on and so forth. Here, I focus on gamification with this property: the game-designer (a company or other organization) creates a “game” in order to encourage the players (the users) to bring about certain outcomes as a side effect of playing the game. The side effect might be good for the user (e.g., improving her health) and/or good for the company or organization behind the game (e.g., advertising their products, increasing their profits, etc.). The “players” of the game may or may not be aware of creating these side effects; and they may or may not approve of/endorse the creation of those side effects. The organizations behind the games, in contrast, are typically directly aiming to create games that have the side effects in question. These aspects of gamification are puzzling and interesting from the point of view of philosophical analyses of agency and responsibility for outcomes. In this paper, I relate these just-mentioned aspects of gamification to some philosophical discussions of responsibility gaps, the ethics of side effects (including the Knobe effect and the doctrine of double effect), and ideas about the relations among different parties’ agency.},
  archive      = {J_MAM},
  author       = {Nyholm, Sven},
  doi          = {10.1007/s11023-024-09661-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Gamification, side effects, and praise and blame for outcomes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). We are building gods: AI as the anthropomorphised authority
of the past. <em>MAM</em>, <em>34</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11023-024-09667-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article argues that large language models (LLMs) should be interpreted as a form of gods. In a theological sense, a god is an immortal being that exists beyond time and space. This is clearly nothing like LLMs. In an anthropological sense, however, a god is rather defined as the personified authority of a group through time—a conceptual tool that molds a collective of ancestors into a unified agent or voice. This is exactly what LLMs are. They are products of vast volumes of data, literally traces of past human (speech) acts, synthesized into a single agency that is (falsely) experienced by users as extra-human. This reconceptualization, I argue, opens up new avenues of critique of LLMs by allowing the mobilization of theoretical resources from centuries of religious critique. For illustration, I draw on the Marxian religious philosophy of Martin Hägglund. From this perspective, the danger of LLMs emerge not only as bias or unpredictability, but as a temptation to abdicate our spiritual and ultimately democratic freedom in favor of what I call a tyranny of the past.},
  archive      = {J_MAM},
  author       = {Öhman, Carl},
  doi          = {10.1007/s11023-024-09667-z},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Minds Mach.},
  title        = {We are building gods: AI as the anthropomorphised authority of the past},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The man behind the curtain: Appropriating fairness in AI.
<em>MAM</em>, <em>34</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09669-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal in this paper is to establish a set of criteria for understanding the meaning and sources of attributing (un)fairness to AI algorithms. To do so, we first establish that (un)fairness, like other normative notions, can be understood in a proper primary sense and in secondary senses derived by analogy. We argue that AI algorithms cannot be said to be (un)fair in the proper sense due to a set of criteria related to normativity and agency. However, we demonstrate how and why AI algorithms can be qualified as (un)fair by analogy and explore the sources of this (un)fairness and the associated problems of responsibility assignment. We conclude that more user-driven AI approaches could alleviate some of these difficulties.},
  archive      = {J_MAM},
  author       = {Korecki, Marcin and Köstner, Guillaume and Martinelli, Emanuele and Carissimo, Cesare},
  doi          = {10.1007/s11023-024-09669-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {The man behind the curtain: Appropriating fairness in AI},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anthropomorphising machines and computerising minds: The
crosswiring of languages between artificial intelligence and brain &amp;
cognitive sciences. <em>MAM</em>, <em>34</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11023-024-09670-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article discusses the process of “conceptual borrowing”, according to which, when a new discipline emerges, it develops its technical vocabulary also by appropriating terms from other neighbouring disciplines. The phenomenon is likened to Carl Schmitt’s observation that modern political concepts have theological roots. The authors argue that, through extensive conceptual borrowing, AI has ended up describing computers anthropomorphically, as computational brains with psychological properties, while brain and cognitive sciences have ended up describing brains and minds computationally and informationally, as biological computers. The crosswiring between the technical languages of these disciplines is not merely metaphorical but can lead to confusion, and damaging assumptions and consequences. The article ends on an optimistic note about the self-adjusting nature of technical meanings in language and the ability to leave misleading conceptual baggage behind when confronted with advancement in understanding and factual knowledge.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano and Nobre, Anna C},
  doi          = {10.1007/s11023-024-09670-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Minds Mach.},
  title        = {Anthropomorphising machines and computerising minds: The crosswiring of languages between artificial intelligence and brain &amp; cognitive sciences},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Philosophical lessons for emotion recognition technology.
<em>MAM</em>, <em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11023-024-09671-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition technology uses artificial intelligence to make inferences about a person’s emotions, on the basis of their facial expressions, body language, tone of voice, or other types of input. Underlying such technology are a variety of assumptions about the manifestation, nature, and value of emotions. To assure the quality and desirability of emotion recognition technology, it is important to critically assess the assumptions embedded in the technology. Within philosophy, there is a long tradition of epistemological, ontological, phenomenological, and ethical reflection on the manifestation, nature, and value of emotions. This article draws from this tradition of philosophy of emotions, in order to challenge the assumptions underlying current emotion recognition technology and to promote a more critical engagement with the concept of emotions in the tech-industry.},
  archive      = {J_MAM},
  author       = {Waelen, Rosalie},
  doi          = {10.1007/s11023-024-09671-3},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Minds Mach.},
  title        = {Philosophical lessons for emotion recognition technology},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pragmatic theory of computational artefacts. <em>MAM</em>,
<em>34</em>(1), 139–170. (<a
href="https://doi.org/10.1007/s11023-023-09650-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some computational phenomena rely essentially on pragmatic considerations, and seem to undermine the independence of the specification from the implementation. These include software development, deviant uses, esoteric languages and recent data-driven applications. To account for them, the interaction between pragmatics, epistemology and ontology in computational artefacts seems essential, indicating the need to recover the role of the language metaphor. We propose a User Levels (ULs) structure as a pragmatic complement to the Levels of Abstraction (LoAs)-based structure defining the ontology and epistemology of computational artefacts. ULs identify a flexible hierarchy in which users bear their own semantic and normative requirements, possibly competing with the logical specification. We formulate a notion of computational act intended in its pragmatic sense, alongside pragmatic versions of implementation and correctness.},
  archive      = {J_MAM},
  author       = {Buda, Alessandro G. and Primiero, Giuseppe},
  doi          = {10.1007/s11023-023-09650-0},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {139-170},
  shortjournal = {Minds Mach.},
  title        = {A pragmatic theory of computational artefacts},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Limits of optimization. <em>MAM</em>, <em>34</em>(1),
117–137. (<a href="https://doi.org/10.1007/s11023-023-09633-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is about finding the best available object with respect to an objective function. Mathematics and quantitative sciences have been highly successful in formulating problems as optimization problems, and constructing clever processes that find optimal objects from sets of objects. As computers have become readily available to most people, optimization and optimized processes play a very broad role in societies. It is not obvious, however, that the optimization processes that work for mathematics and abstract objects should be readily applied to complex and open social systems. In this paper we set forth a framework to understand when optimization is limited, particularly for complex and open social systems.},
  archive      = {J_MAM},
  author       = {Carissimo, Cesare and Korecki, Marcin},
  doi          = {10.1007/s11023-023-09633-1},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {117-137},
  shortjournal = {Minds Mach.},
  title        = {Limits of optimization},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Informational equivalence but computational differences?
Herbert simon on representations in scientific practice. <em>MAM</em>,
<em>34</em>(1), 93–116. (<a
href="https://doi.org/10.1007/s11023-023-09630-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To explain why, in scientific problem solving, a diagram can be “worth ten thousand words,” Jill Larkin and Herbert Simon (1987) relied on a computer model: two representations can be “informationally” equivalent but differ “computationally,” just as the same data can be encoded in a computer in multiple ways, more or less suited to different kinds of processing. The roots of this proposal lay in cognitive psychology, more precisely in the “imagery debate” of the 1970s on whether there are image-like mental representations. Simon (1972, 1978) hoped to solve this debate by thoroughly reducing the differences between forms of mental representations (e.g., between images and sentences) to differences in computational efficiency; to carry out this reduction, he borrowed from computer science the concepts of data type and of data structure. I argue that, in the end, his account amounted to nothing more than characterizing representations by the fast operations on them. This analysis then allows me to assess what Simon’s approach actually achieves when transported from psychology to the study of scientific representations, as in Larkin and Simon (1987): it allows comparing, not representations in and of themselves, but rather the computational roles they play in particular problem-solving processes—that is, representations together with a particular way of using them.},
  archive      = {J_MAM},
  author       = {Waszek, David},
  doi          = {10.1007/s11023-023-09630-4},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {93-116},
  shortjournal = {Minds Mach.},
  title        = {Informational equivalence but computational differences? herbert simon on representations in scientific practice},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three early formal approaches to the verification of
concurrent programs. <em>MAM</em>, <em>34</em>(1), 73–92. (<a
href="https://doi.org/10.1007/s11023-023-09621-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper traces a relatively linear sequence of early research approaches to the formal verification of concurrent programs. It does so forwards and then backwards in time. After briefly outlining the context, the key insights from three distinct approaches from the 1970s are identified (Ashcroft/Manna, Ashcroft (solo) and Owicki). The main technical material in the paper focuses on a specific program taken from the last published of the three pieces of research (Susan Owicki’s): her own verification of her Findpos example is outlined followed by attempts at verifying the same example using the earlier approaches. Reconsidering the prior approaches on the basis of Owicki’s useful example illuminates similarities and differences between the proposals. Along the way, observations about interactions between researchers (and some “blind spots”) are noted.},
  archive      = {J_MAM},
  author       = {Jones, Cliff B.},
  doi          = {10.1007/s11023-023-09621-5},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {73-92},
  shortjournal = {Minds Mach.},
  title        = {Three early formal approaches to the verification of concurrent programs},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From monitors to monitors: A primitive history.
<em>MAM</em>, <em>34</em>(1), 51–71. (<a
href="https://doi.org/10.1007/s11023-023-09632-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computers became multi-component systems in the 1950s, handling the speed differentials efficiently was identified as a major challenge. The desire for better understanding and control of ‘concurrency’ spread into hardware, software, and formalism. This paper examines the way in which the problem emerged and was handled across various computing cultures from 1955 to 1985. In the machinic culture of the late 1950s, system programs called ‘monitors’ were used for directly managing synchronisation. Attempts to reframe synchronisation in the subsequent algorithmic culture pushed the problem to a higher level of abstraction; Dijkstra’s semaphores were a reaction to the algorithms’ complexity. Towards the end of the 1960s, the culture of ‘structured programming’ created a milieu in which Dijkstra, Hoare, and Brinch Hansen (among others) aimed for a concurrency primitive which embodied the new view of programming. Via conditional critical regions and Dijkstra’s ‘secretaries’, the co-produced ‘monitor’ appeared to provide the desired encapsulation. The construct received embodiment in a few programming languages; this paper ends by considering Modula and Concurrent Pascal.},
  archive      = {J_MAM},
  author       = {Astarte, Troy K.},
  doi          = {10.1007/s11023-023-09632-2},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {51-71},
  shortjournal = {Minds Mach.},
  title        = {From monitors to monitors: A primitive history},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). True turing: A bird’s-eye view. <em>MAM</em>,
<em>34</em>(1), 29–49. (<a
href="https://doi.org/10.1007/s11023-023-09634-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alan Turing is often portrayed as a materialist in secondary literature. In the present article, I suggest that Turing was instead an idealist, inspired by Cambridge scholars, Arthur Eddington, Ernest Hobson, James Jeans and John McTaggart. I outline Turing’s developing thoughts and his legacy in the USA to date. Specifically, I contrast Turing’s two notions of computability (both from 1936) and distinguish between Turing’s “machine intelligence” in the UK and the more well-known “artificial intelligence” in the USA. According to my proposed historical interpretation, Turing did not view computations in the real world to be exhaustively and deterministically characterized by his automatic machines from 1936.},
  archive      = {J_MAM},
  author       = {Daylight, Edgar},
  doi          = {10.1007/s11023-023-09634-0},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {29-49},
  shortjournal = {Minds Mach.},
  title        = {True turing: A bird’s-eye view},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leibniz and the stocking frame: Computation, weaving and
knitting in the 17th century. <em>MAM</em>, <em>34</em>(1), 11–28. (<a
href="https://doi.org/10.1007/s11023-023-09623-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comparison made by Ada Lovelace in 1843 between the Analytical Engine and the Jacquard loom is one of the well-known analogies between looms and computation machines. Given the fact that weaving – and textile production in general – is one of the oldest cultural techniques in human history, the question arises whether this was the first time that such a parallel was drawn. As this paper will show, centuries before Lovelace’s analogy, such a comparison was made by Gottfried Wilhelm Leibniz. During the 17th century, Leibniz compared his calculating machines with another textile machine, the stocking frame, a machine which mechanized knitting and which was invented in 1589. During the following centuries, this machine was considered as a technological wonder and as a creation of God, and, during the last decades of the 17th century, Leibniz emphasized the need to consider it and other textile machines mathematically. What, then, were the reasons for the parallel drawn between this machine and Leibniz’s automatic computation machines? And what were the consequences of this analogy concerning the artisanal knowledge embedded in manual textile practices?},
  archive      = {J_MAM},
  author       = {Friedman, Michael},
  doi          = {10.1007/s11023-023-09623-3},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {11-28},
  shortjournal = {Minds Mach.},
  title        = {Leibniz and the stocking frame: Computation, weaving and knitting in the 17th century},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing cultures: Historical and philosophical
perspectives. <em>MAM</em>, <em>34</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s11023-023-09653-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Gastaldi, Juan Luis},
  doi          = {10.1007/s11023-023-09653-x},
  journal      = {Minds and Machines},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {Computing cultures: Historical and philosophical perspectives},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
