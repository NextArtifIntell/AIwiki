<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="air---349">AIR - 349</h2>
<ul>
<li><details>
<summary>
(2024). Neuromorphic computing for modeling neurological and
psychiatric disorders: Implications for drug development. <em>AIR</em>,
<em>57</em>(12), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10948-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of neuromorphic computing, inspired by the structure and function of the human brain, presents a transformative framework for modelling neurological disorders in drug development. This article investigates the implications of applying neuromorphic computing to simulate and comprehend complex neural systems affected by conditions like Alzheimer’s, Parkinson’s, and epilepsy, drawing from extensive literature. It explores the intersection of neuromorphic computing with neurology and pharmaceutical development, emphasizing the significance of understanding neural processes and integrating deep learning techniques. Technical considerations, such as integrating neural circuits into CMOS technology and employing memristive devices for synaptic emulation, are discussed. The review evaluates how neuromorphic computing optimizes drug discovery and improves clinical trials by precisely simulating biological systems. It also examines the role of neuromorphic models in comprehending and simulating neurological disorders, facilitating targeted treatment development. Recent progress in neuromorphic drug discovery is highlighted, indicating the potential for transformative therapeutic interventions. As technology advances, the synergy between neuromorphic computing and neuroscience holds promise for revolutionizing the study of the human brain’s complexities and addressing neurological challenges.},
  archive      = {J_AIR},
  author       = {Raikar, Amisha S. and Andrew, J and Dessai, Pranjali Prabhu and Prabhu, Sweta M. and Jathar, Shounak and Prabhu, Aishwarya and Naik, Mayuri B. and Raikar, Gokuldas Vedant S.},
  doi          = {10.1007/s10462-024-10948-3},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neuromorphic computing for modeling neurological and psychiatric disorders: Implications for drug development},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic literature review on pancreas segmentation from
traditional to non-supervised techniques in abdominal medical images.
<em>AIR</em>, <em>57</em>(12), 1–65. (<a
href="https://doi.org/10.1007/s10462-024-10966-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abdominal organs play a significant role in regulating various functional systems. Any impairment in its functioning can lead to cancerous diseases. Diagnosing these diseases mainly relies on radiologists’ subjective assessment, which varies according to professional abilities and clinical experience. Computer-Aided Diagnosis (CAD) system is designed to assist clinicians in identifying various pathological changes. Hence, automatic pancreas segmentation is a vital input to the CAD system in the diagnosis of cancer at its early stages. Automatic segmentation is achieved through traditional methods like atlas-based and statistical models, and nowadays, it is achieved through artificial intelligence approaches like machine learning and deep learning using various imaging modalities. This study investigates and analyses the various state-of-the-art multi-organ and pancreas segmentation approaches to identify the research gaps and future perspectives for the research community. The objective is achieved by framing the research questions using the PICOC framework and then selecting 140 research articles using a systematic process through the Covidence tool to conclude the answers to the respective questions. The literature search has been conducted on five databases of original studies published from 2003 to 2023. Initially, the literature analysis is presented in terms of publication, and the comparative analysis of the current study is presented with existing review studies. Then, existing studies are analyzed, focusing on semi-automatic and automatic multi-organ segmentation and pancreas segmentation, using various learning methods. Finally, the various critical issues, the research gaps and the future perspectives of segmentation methods based on published evidence are summarized.},
  archive      = {J_AIR},
  author       = {Jain, Suchi and Sikka, Geeta and Dhir, Renu},
  doi          = {10.1007/s10462-024-10966-1},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review on pancreas segmentation from traditional to non-supervised techniques in abdominal medical images},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of computer vision-based personal
protective equipment compliance in industry practice: Advancements,
challenges and future directions. <em>AIR</em>, <em>57</em>(12), 1–28.
(<a href="https://doi.org/10.1007/s10462-024-10978-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computerized compliance of Personal Protective Equipment (PPE) is an emerging topic in academic literature that aims to enhance workplace safety through the automation of compliance and prevention of PPE misuse (which currently relies on manual employee supervision and reporting). Although trends in the scientific literature indicate a high potential for solving the compliance problem by employing computer vision (CV) techniques, the practice has revealed a series of barriers that limit their wider applications. This article aims to contribute to the advancement of CV-based PPE compliance by providing a comparative review of high-level approaches, algorithms, datasets, and technologies used in the literature. The systematic review highlights industry-specific challenges, environmental variations, and computational costs related to the real-time management of PPE compliance. The issues of employee identification and identity management are also discussed, along with ethical and cybersecurity concerns. Through the concept of CV-based PPE Compliance 4.0, which encapsulates PPE, human, and company spatio-temporal variabilities, this study provides guidelines for future research directions for addressing the identified barriers. The further advancements and adoption of CV-based solutions for PPE compliance will require simultaneously addressing human identification, pose estimation, object recognition and tracking, necessitating the development of corresponding public datasets.},
  archive      = {J_AIR},
  author       = {Vukicevic, Arso M. and Petrovic, Milos and Milosevic, Pavle and Peulic, Aleksandar and Jovanovic, Kosta and Novakovic, Aleksandar},
  doi          = {10.1007/s10462-024-10978-x},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of computer vision-based personal protective equipment compliance in industry practice: Advancements, challenges and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A concise review towards a novel target specific
multi-source unsupervised transfer learning technique for GDP estimation
using CO2 emission data. <em>AIR</em>, <em>57</em>(12), 1–33. (<a
href="https://doi.org/10.1007/s10462-024-10858-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though economic growths of most of the nations have seen exponential rise due to industrialization, it has also caused proportional increase in their carbon emissions. This paper exploits this proportionate relationship of carbon emission with GDP to predict the per-capita GDP of those nations whose GDP values are missing in the world bank database. The reason behind the same was, those countries were either war-torn or politically isolated/unstable. To achieve the objective of predicting the missing GDP values of those countries from their carbon emissions, this paper exploits the non-linear relationship among the carbon emissions from solid fuels, liquid fuels, and gaseous fuels. It is so because even the differential utilization of these fuels impact economy differently. Use of traditional solid fuel for cooking points toward energy poverty, and access to clean cooking gas indicates higher living standard. However, the available data from the war-torn or isolated countries are very little, and hence insufficient for building a robust predictive machine learning model. So, this paper employs multi-source unsupervised transfer learning to precisely estimate the missing per-capita GDP of those nations. It suitably enlarges the training domains for the prediction models to be more robust. We empirically evaluate the proposed methodology for different regression techniques to estimate the missing GDP values of eleven different nations belonging to diverse strata of economies viz. developed economies, developing, and/or least developing economies. Proposed methodology profoundly improves the prediction preciseness of these regression techniques in estimating the missing per-capita GDP of the considered nations.},
  archive      = {J_AIR},
  author       = {Kumar, Sandeep and Muhuri, Pranab K.},
  doi          = {10.1007/s10462-024-10858-4},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A concise review towards a novel target specific multi-source unsupervised transfer learning technique for GDP estimation using CO2 emission data},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advances in text-guided 3D editing: A survey. <em>AIR</em>,
<em>57</em>(12), 1–61. (<a
href="https://doi.org/10.1007/s10462-024-10937-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 3D Artificial Intelligence Generated Content (AIGC), compared with generating 3D assets from scratch, editing extant 3D assets satisfies user prompts, allowing the creation of diverse and high-quality 3D assets in a time and labor-saving manner. More recently, text-guided 3D editing that modifies 3D assets guided by text prompts is user-friendly and practical, which evokes a surge in research within this field. In this survey, we comprehensively investigate recent literature on text-guided 3D editing in an attempt to answer two questions: What are the methodologies of existing text-guided 3D editing? How has current progress in text-guided 3D editing gone so far? Specifically, we focus on text-guided 3D editing methods published in the past 4 years, delving deeply into their frameworks and principles. We then present a fundamental taxonomy in terms of the editing strategy, optimization scheme, and 3D representation. Based on the taxonomy, we review recent advances in this field, considering factors such as editing scale, type, granularity, and perspective. In addition, we highlight four applications of text-guided 3D editing, including texturing, style transfer, local editing of scenes, and insertion editing, to exploit further the 3D editing capacities with in-depth comparisons and discussions. Depending on the insights achieved by this survey, we discuss open challenges and future research directions. We hope this survey will help readers gain a deeper understanding of this exciting field and foster further advancements in text-guided 3D editing.},
  archive      = {J_AIR},
  author       = {Lu, Lihua and Li, Ruyang and Zhang, Xiaohui and Wei, Hui and Du, Guoguang and Wang, Binqiang},
  doi          = {10.1007/s10462-024-10937-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in text-guided 3D editing: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep models for multi-view 3D object recognition: A review.
<em>AIR</em>, <em>57</em>(12), 1–71. (<a
href="https://doi.org/10.1007/s10462-024-10941-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper focuses on the progress of deep learning-based methods for multi-view 3D object recognition. It covers the state-of-the-art techniques in this field, specifically those that utilize 3D multi-view data as input representation. The paper provides a comprehensive analysis of the pipeline for deep learning-based multi-view 3D object recognition, including the various techniques employed at each stage. It also presents the latest developments in CNN-based and transformer-based models for multi-view 3D object recognition. The review discusses existing models in detail, including the datasets, camera configurations, view selection strategies, pre-trained CNN architectures, fusion strategies, and recognition performance. Additionally, it examines various computer vision applications that use multi-view classification. Finally, it highlights future directions, factors impacting recognition performance, and trends for the development of multi-view 3D object recognition method.},
  archive      = {J_AIR},
  author       = {Alzahrani, Mona and Usman, Muhammad and Jarraya, Salma Kammoun and Anwar, Saeed and Helmy, Tarek},
  doi          = {10.1007/s10462-024-10941-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-71},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep models for multi-view 3D object recognition: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERTH scheduler: Enhanced red-tailed hawk algorithm for
multi-cost optimization in cloud task scheduling. <em>AIR</em>,
<em>57</em>(12), 1–48. (<a
href="https://doi.org/10.1007/s10462-024-10945-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective task scheduling has become the key to optimizing resource allocation, reducing operation costs, and enhancing the user experience. The complexity and dynamics of cloud computing environments require task scheduling algorithms that can flexibly respond to multiple computing demands and changing resource states. Therefore, we propose an enhanced Red-tailed Hawk algorithm (named ERTH) based on multiple elite policies and chaotic mapping, while applying this approach in conjunction with the proposed scheduling model to optimize the efficiency of task scheduling in cloud computing environments. We apply the ERTH algorithm to a real cloud computing environment and conduct a comparison with the original RTH and other conventional algorithms. The proposed ERTH algorithm has better convergence speed and stability in most cases of small and large-scale tasks and performs better in minimizing the task completion time and system load cost. Specifically, our experiments show that the ERTH algorithm reduces the total system cost by 34.8% and 36.4% relative to the traditional algorithm for tasks of different sizes. Further, evaluations in the IEEE Congress on Evolutionary Computation (CEC) benchmark test sets show that the ERTH algorithm outperforms the traditional or emerging algorithms in several performance metrics such as mean, standard deviation, etc. The proposal and validation of the ERTH algorithm are of great significance in promoting the application of intelligent optimization algorithms in cloud computing.},
  archive      = {J_AIR},
  author       = {Qin, Xinqi and Li, Shaobo and Tong, Jian and Xie, Cankun and Zhang, Xingxing and Wu, Fengbin and Xie, Qun and Ling, Yihong and Lin, Guangzheng},
  doi          = {10.1007/s10462-024-10945-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {ERTH scheduler: Enhanced red-tailed hawk algorithm for multi-cost optimization in cloud task scheduling},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech based detection of alzheimer’s disease: A survey of
AI techniques, datasets and challenges. <em>AIR</em>, <em>57</em>(12),
1–43. (<a href="https://doi.org/10.1007/s10462-024-10961-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a growing global concern, exacerbated by an aging population and the high costs associated with traditional detection methods. Recent research has identified speech data as valuable clinical information for AD detection, given its association with the progressive degeneration of brain cells and subsequent impacts on memory, cognition, and language abilities. The ongoing demographic shift toward an aging global population underscores the critical need for affordable and easily available methods for early AD detection and intervention. To address this major challenge, substantial research has recently focused on investigating speech data, aiming to develop efficient and affordable diagnostic tools that align with the demands of our aging society. This paper presents an in-depth review of studies from 2018–2023 utilizing speech for AD detection. Following the PRISMA protocol and a two-stage selection process, we identified 85 publications for analysis. In contrast to previous literature reviews, this paper places a strong emphasis on conducting a rigorous comparative analysis of various Artificial Intelligence (AI) based techniques, categorizing them meticulously based on underlying algorithms. We perform an exhaustive evaluation of research papers leveraging common benchmark datasets, specifically ADReSS and ADReSSo, to assess their performance. In contrast to previous literature reviews, this work makes a significant contribution by overcoming the limitations posed by the absence of standardized tasks and commonly accepted benchmark datasets for comparing different studies. The analysis reveals the dominance of deep learning models, particularly those leveraging pre-trained models like BERT, in AD detection. The integration of acoustic and linguistic features often achieves accuracies above 85%. Despite these advancements, challenges persist in data scarcity, standardization, privacy, and model interpretability. Future directions include improving multilingual recognition, exploring emerging multimodal approaches, and enhancing ASR systems for AD patients. By identifying these key challenges and suggesting future research directions, our review serves as a valuable resource for advancing AD detection techniques and their practical implementation.},
  archive      = {J_AIR},
  author       = {Ding, Kewen and Chetty, Madhu and Noori Hoshyar, Azadeh and Bhattacharya, Tanusri and Klein, Britt},
  doi          = {10.1007/s10462-024-10961-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Speech based detection of alzheimer’s disease: A survey of AI techniques, datasets and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mises-fisher similarity-based boosted additive angular
margin loss for breast cancer classification. <em>AIR</em>,
<em>57</em>(12), 1–42. (<a
href="https://doi.org/10.1007/s10462-024-10963-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the accuracy of breast cancer diagnosis, current practices rely on biopsies and microscopic examinations. However, this approach is known for being time-consuming, tedious, and costly. While convolutional neural networks (CNNs) have shown promise for their efficiency and high accuracy, training them effectively becomes challenging in real-world learning scenarios such as class imbalance, small-scale datasets, and label noises. Angular margin-based softmax losses, which concentrate on the angle between features and classifiers embedded in cosine similarity at the classification layer, aim to regulate feature representation learning. Nevertheless, the cosine similarity’s lack of a heavy tail impedes its ability to compactly regulate intra-class feature distribution, limiting generalization performance. Moreover, these losses are constrained to target classes when margin penalties are applied, which may not always optimize effectiveness. Addressing these hurdles, we introduce an innovative approach termed MF-BAM (Mises-Fisher Similarity-based Boosted Additive Angular Margin Loss), which extends beyond traditional cosine similarity and is anchored in the von Mises-Fisher distribution. MF-BAM not only penalizes the angle between deep features and their corresponding target class weights but also considers angles between deep features and weights associated with non-target classes. Through extensive experimentation on the BreaKHis dataset, MF-BAM achieves outstanding accuracies of 99.92%, 99.96%, 100.00%, and 98.05% for magnification levels of ×40, ×100, ×200, and ×400, respectively. Furthermore, additional experiments conducted on the BACH dataset for breast cancer classification, as well as on the LFW and YTF datasets for face recognition, affirm the generalization capability of our proposed loss function.},
  archive      = {J_AIR},
  author       = {Alirezazadeh, P. and Dornaika, F. and Charafeddine, J.},
  doi          = {10.1007/s10462-024-10963-4},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Mises-fisher similarity-based boosted additive angular margin loss for breast cancer classification},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning-based natural language processing: A
systematic literature review. <em>AIR</em>, <em>57</em>(12), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10970-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a decentralized machine learning (ML) framework that allows models to be trained without sharing the participants’ local data. FL thus preserves privacy better than centralized machine learning. Since textual data (such as clinical records, posts in social networks, or search queries) often contain personal information, many natural language processing (NLP) tasks dealing with such data have shifted from the centralized to the FL setting. However, FL is not free from issues, including convergence and security vulnerabilities (due to unreliable or poisoned data introduced into the model), communication and computation bottlenecks, and even privacy attacks orchestrated by honest-but-curious servers. In this paper, we present a systematic literature review (SLR) of NLP applications in FL with a special focus on FL issues and the solutions proposed so far. Our review surveys 36 recent papers published in relevant venues, which are systematically analyzed and compared from multiple perspectives. As a result of the survey, we also identify the most outstanding challenges in the area.},
  archive      = {J_AIR},
  author       = {Khan, Younas and Sánchez, David and Domingo-Ferrer, Josep},
  doi          = {10.1007/s10462-024-10970-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Federated learning-based natural language processing: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital deception: Generative artificial intelligence in
social engineering and phishing. <em>AIR</em>, <em>57</em>(12), 1–23.
(<a href="https://doi.org/10.1007/s10462-024-10973-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of Artificial Intelligence (AI) and Machine Learning (ML) has profound implications for both the utility and security of our digital interactions. This paper investigates the transformative role of Generative AI in Social Engineering (SE) attacks. We conduct a systematic review of social engineering and AI capabilities and use a theory of social engineering to identify three pillars where Generative AI amplifies the impact of SE attacks: Realistic Content Creation, Advanced Targeting and Personalization, and Automated Attack Infrastructure. We integrate these elements into a conceptual model designed to investigate the complex nature of AI-driven SE attacks—the Generative AI Social Engineering Framework. We further explore human implications and potential countermeasures to mitigate these risks. Our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.},
  archive      = {J_AIR},
  author       = {Schmitt, Marc and Flechais, Ivan},
  doi          = {10.1007/s10462-024-10973-2},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Digital deception: Generative artificial intelligence in social engineering and phishing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustworthy human computation: A survey. <em>AIR</em>,
<em>57</em>(12), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10974-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human computation is an approach to solving problems that prove difficult using AI only, and involves the cooperation of many humans. Because human computation requires close engagement with both “human populations as users” and “human populations as driving forces,” establishing mutual trust between AI and humans is an important issue to further the development of human computation. This survey lays the groundwork for the realization of trustworthy human computation. First, the trustworthiness of human computation as computing systems, that is, trust offered by humans to AI, is examined using the RAS (reliability, availability, and serviceability) analogy, which define measures of trustworthiness in conventional computer systems. Next, the social trustworthiness provided by human computation systems to users or participants is discussed from the perspective of AI ethics, including fairness, privacy, and transparency. Then, we consider human–AI collaboration based on two-way trust, in which humans and AI build mutual trust and accomplish difficult tasks through reciprocal collaboration. Finally, future challenges and research directions for realizing trustworthy human computation are discussed.},
  archive      = {J_AIR},
  author       = {Kashima, Hisashi and Oyama, Satoshi and Arai, Hiromi and Mori, Junichiro},
  doi          = {10.1007/s10462-024-10974-1},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Trustworthy human computation: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive investigation of multimodal deep learning
fusion strategies for breast cancer classification. <em>AIR</em>,
<em>57</em>(12), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10984-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In breast cancer research, diverse data types and formats, such as radiological images, clinical records, histological data, and expression analysis, are employed. Given the intricate nature of natural phenomena, relying on the features of a single modality is seldom sufficient for comprehensive analysis. Therefore, it is possible to guarantee medical relevance and achieve improved clinical outcomes by combining several modalities. The presen study carefully maps and reviews 47 primary articles from six well-known digital libraries that were published between 2018 and 2023 for breast cancer classification based on multimodal deep learning fusion (MDLF) techniques. This systematic literature review encompasses various aspects, including the medical modalities combined, the datasets utilized in these studies, the techniques, models, and architectures used in MDLF and it also discusses the advantages and limitations of each approach. The analysis of selected papers has revealed a compelling trend: the emergence of new modalities and combinations that were previously unexplored in the context of breast cancer classification. This exploration has not only expanded the scope of predictive models but also introduced fresh perspectives for addressing diverse targets, ranging from screening to diagnosis and prognosis. The practical advantages of MDLF are evident in its ability to enhance the predictive capabilities of machine learning models, resulting in improved accuracy across diverse applications. The prevalence of deep learning models underscores their success in autonomously discerning complex patterns, offering a substantial departure from traditional machine learning approaches. Furthermore, the paper explores the challenges and future directions in this field, including the need for larger datasets, the use of ensemble learning methods, and the interpretation of multimodal models.},
  archive      = {J_AIR},
  author       = {Nakach, Fatima-Zahrae and Idri, Ali and Goceri, Evgin},
  doi          = {10.1007/s10462-024-10984-z},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive investigation of multimodal deep learning fusion strategies for breast cancer classification},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient propositional system for abductive logic
programming. <em>AIR</em>, <em>57</em>(12), 1–30. (<a
href="https://doi.org/10.1007/s10462-024-10928-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abductive logic programming (ALP) extends logic programming with hypothetical reasoning by means of abducibles, an extension able to handle interesting problems, such as diagnosis, planning, and verification with formal methods. Implementations of this extension have been using Prolog meta-interpreters and Prolog programs with Constraint Handling Rules (CHR). While the latter adds a clean and efficient interface to the host system, it still suffers in performance for large programs. Here, the concern is to obtain a more performant implementation of the SCIFF system following a compiled approach. This paper, as a first step in this long term goal, sets out a propositional ALP system following SCIFF, eliminating the need for CHR and achieving better performance.},
  archive      = {J_AIR},
  author       = {Gavanelli, Marco and Julián-Iranzo, Pascual and Sáenz-Pérez, Fernando},
  doi          = {10.1007/s10462-024-10928-7},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient propositional system for abductive logic programming},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive snow ablation-inspired particle swarm
optimization with its application in geometric optimization.
<em>AIR</em>, <em>57</em>(12), 1–59. (<a
href="https://doi.org/10.1007/s10462-024-10946-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the shortcomings of particle swarm optimization (PSO), such as low execution efficiency and difficulty in overcoming local optima, this paper proposes a multi-strategy PSO method incorporating snow ablation operation (SAO), known as SAO-MPSO. Firstly, Cubic initialization is performed on particles to obtain a good initial environment. Subsequently, SAO and PSO are combined in parallel, and a balanced search mechanism led by multiple sub-populations is devised, significantly improving the search efficiency of overall population. Finally, the degree day method of SAO is introduced, and particles are endowed with memory of environmental changes to prevent premature convergence of PSO, while balancing the exploration and exploitation (ENE) capabilities in later phases. All adaptive parameters are used throughout this method in place of fixed parameters to improve the robustness and adaptability. For a comprehensive analysis of SAO-MPSO, its good ENE ability is verified on CEC 2020 and CEC 2022 and this method is compared with existing improved PSO versions on both test sets. The results show that SAO-MPSO has certain advantages in the comparison of similar improved algorithms. In order to further validate the strength of SAO-MPSO in dealing with nonlinear optimization problems (OPs) with strong constraints, firstly, based on the ball Wang-Ball (BWB) curve, a combined BWB (CBWB) curve is constructed, and a construction method for CBWB curves that satisfy G1 and G2 continuity is derived. Then, with the energy minimization and scale parameters of the CBWB curve as the optimization objective and variables respectively, a shape optimization model that satisfies G2 continuity is established. Finally, three numerical optimization examples based on this model are solved using SAO-MPSO and compared with 10 other methods. The results show that the energy obtained by SAO-MPSO is the smallest, which verifies the effectiveness of this method applied to shape OPs of CBWB curve.},
  archive      = {J_AIR},
  author       = {Hu, Gang and Guo, Yuxuan and Zhao, Weiguo and Houssein, Essam H.},
  doi          = {10.1007/s10462-024-10946-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An adaptive snow ablation-inspired particle swarm optimization with its application in geometric optimization},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surface defect inspection of industrial products with object
detection deep networks: A systematic review. <em>AIR</em>,
<em>57</em>(12), 1–48. (<a
href="https://doi.org/10.1007/s10462-024-10956-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the focal points in industrial product defect detection lies in the utilization of deep learning-based object detection algorithms. With the continuous introduction of these algorithms and their refined models, notable achievements have been attained. However, challenges persist in industrial settings, such as substantial variations in defect scales, the delicate balance between accuracy and speed, and the detection of small objects. Various methods have been proposed to address these challenges and propel the advancement of defect detection. To comprehensively review the latest developments in deep learning-based industrial product defect detection algorithms and foster further progress, this paper encompasses typical datasets and evaluation metrics used in industrial product defect detection, traces the development history of supervised one-stage and two-stage object detection algorithm-based and unsupervised algorithm-based industrial defect detection methods, discusses major challenges, and outlines future directions. It highlights the potential for further improving the accuracy, speed, and reliability of defect detection systems in industrial applications.},
  archive      = {J_AIR},
  author       = {Ma, Yuxin and Yin, Jiaxing and Huang, Feng and Li, Qipeng},
  doi          = {10.1007/s10462-024-10956-3},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Surface defect inspection of industrial products with object detection deep networks: A systematic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The atmospheric boundary layer: A review of current
challenges and a new generation of machine learning techniques.
<em>AIR</em>, <em>57</em>(12), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10962-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric boundary layer (ABL) structure and dynamics are important aspects to consider in human health. The ABL is characterized by a high degree of spatial and temporal variability that hinders their understanding. This paper aims to provide a comprehensive overview of machine learning (ML) methodologies, encompassing deep learning and ensemble approaches, within the scope of ABL research. The goal is to highlight the challenges and opportunities of using ML in turbulence modeling and parameterization in areas such as atmospheric pollution, meteorology, and renewable energy. The review emphasizes the validation of results to ensure their reliability and applicability. ML has proven to be a valuable tool for understanding and predicting how ABL spatial and seasonal variability affects pollutant dispersion and public health. In addition, it has been demonstrated that ML can be used to estimate several variables and parameters, such as ABL height, making it a promising approach to enhance air quality management and urban planning.},
  archive      = {J_AIR},
  author       = {Canché-Cab, Linda and San-Pedro, Liliana and Ali, Bassam and Rivero, Michel and Escalante, Mauricio},
  doi          = {10.1007/s10462-024-10962-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The atmospheric boundary layer: A review of current challenges and a new generation of machine learning techniques},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The survey on the dual nature of xAI challenges in intrusion
detection and their potential for AI innovation. <em>AIR</em>,
<em>57</em>(12), 1–32. (<a
href="https://doi.org/10.1007/s10462-024-10972-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving domain of cybersecurity, the imperative for intrusion detection systems is undeniable; yet, it is increasingly clear that to meet the ever-growing challenges posed by sophisticated threats, intrusion detection itself stands in need of the transformative capabilities offered by the explainable artificial intelligence (xAI). As this concept is still developing, it poses an array of challenges that need addressing. This paper discusses 25 of such challenges of varying research interest, encountered in the domain of xAI, identified in the course of a targeted study. While these challenges may appear as obstacles, they concurrently present as significant research opportunities. These analysed challenges encompass a wide spectrum of concerns spanning the intersection of xAI and cybersecurity. The paper underscores the critical role of xAI in addressing opacity issues within machine learning algorithms and sets the stage for further research and innovation in the quest for transparent and interpretable artificial intelligence that humans are able to trust. In addition to this, by reframing these challenges as opportunities, this study seeks to inspire and guide researchers towards realizing the full potential of xAI in cybersecurity.},
  archive      = {J_AIR},
  author       = {Pawlicki, Marek and Pawlicka, Aleksandra and Kozik, Rafał and Choraś, Michał},
  doi          = {10.1007/s10462-024-10972-3},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The survey on the dual nature of xAI challenges in intrusion detection and their potential for AI innovation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent applications and advances of african vultures
optimization algorithm. <em>AIR</em>, <em>57</em>(12), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10981-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The African Vultures Optimization Algorithm (AVOA) is a recently developed meta-heuristic algorithm inspired by the foraging behavior of African vultures in nature. This algorithm has gained attention due to its simplicity, flexibility, and effectiveness in tackling many optimization problems. The significance of this review lies in its comprehensive examination of the AVOA’s development, core principles, and applications. By analyzing 112 studies, this review highlights the algorithm’s versatility and the growing interest in enhancing its performance for real-world optimization challenges. This review methodically explores the evolution of AVOA, investigating proposed improvements that enhance the algorithm’s ability to adapt to various search geometries in optimization problems. Additionally, it introduces the AVOA solver, detailing its functionality and application in different optimization scenarios. The review demonstrates the AVOA’s effectiveness, particularly its unique weighting mechanism, which mimics vulture behavior during the search process. The findings underscore the algorithm’s robustness, ease of use, and lack of dependence on derivative information. The review also critically evaluates the AVOA’s convergence behavior, identifying its strengths and limitations. In conclusion, the study not only consolidates the existing knowledge on AVOA but also proposes directions for future research, including potential adaptations and enhancements to address its limitations. The insights gained from this review offer valuable guidance for researchers and practitioners seeking to apply or improve the AVOA in various optimization tasks.},
  archive      = {J_AIR},
  author       = {Hussien, Abdelazim G. and Gharehchopogh, Farhad Soleimanian and Bouaouda, Anas and Kumar, Sumit and Hu, Gang},
  doi          = {10.1007/s10462-024-10981-2},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Recent applications and advances of african vultures optimization algorithm},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controllable image synthesis methods, applications and
challenges: A comprehensive survey. <em>AIR</em>, <em>57</em>(12), 1–46.
(<a href="https://doi.org/10.1007/s10462-024-10987-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable Image Synthesis (CIS) is a methodology that allows users to generate desired images or manipulate specific attributes of images by providing precise input conditions or modifying latent representations. In recent years, CIS has attracted considerable attention in the field of image processing, with significant advances in consistency, controllability and harmony. However, several challenges still remain, particularly regarding the fine-grained controllability and interpretability of synthesized images. In this paper, we comprehensively and systematically review the CIS from problem definition, taxonomy and evaluation systems to existing challenges and future research directions. First, the definition of CIS is given, and several representative deep generative models are introduced in detail. Second, the existing CIS methods are divided into three categories according to the different control manners used and discuss the typical work in each category critically. Furthermore, we introduce the public datasets and evaluation metrics commonly used in image synthesis and analyze the representative CIS methods. Finally, we present several open issues and discuss the future research direction of CIS.},
  archive      = {J_AIR},
  author       = {Huang, Shanshan and Li, Qingsong and Liao, Jun and Wang, Shu and Liu, Li and Li, Lian},
  doi          = {10.1007/s10462-024-10987-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Controllable image synthesis methods, applications and challenges: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clarity in complexity: How aggregating explanations resolves
the disagreement problem. <em>AIR</em>, <em>57</em>(12), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10952-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Rashômon Effect, applied in Explainable Machine Learning, refers to the disagreement between the explanations provided by various attribution explainers and to the dissimilarity across multiple explanations generated by a particular explainer for a single instance from the dataset (differences between feature importances and their associated signs and ranks), an undesirable outcome especially in sensitive domains such as healthcare or finance. We propose a method inspired from textual-case based reasoning for aligning explanations from various explainers in order to resolve the disagreement and dissimilarity problems. We iteratively generated a number of 100 explanations for each instance from six popular datasets, using three prevalent feature attribution explainers: LIME, Anchors and SHAP (with the variations Tree SHAP and Kernel SHAP) and consequently applied a global cluster-based aggregation strategy that quantifies alignment and reveals similarities and associations between explanations. We evaluated our method by weighting the $$\:k$$ -NN algorithm with agreed feature overlap explanation weights and compared it to a non-weighted $$\:k$$ -NN predictor, having as task binary classification. Also, we compared the results of the weighted $$\:k$$ -NN algorithm using aggregated feature overlap explanation weights to the weighted $$\:k$$ -NN algorithm using weights produced by a single explanation method (either LIME, SHAP or Anchors). Our global alignment method benefited the most from a hybridization with feature importance scores (information gain), that was essential for acquiring a more accurate estimate of disagreement, for enabling explainers to reach a consensus across multiple explanations and for supporting effective model learning through improved classification performance.},
  archive      = {J_AIR},
  author       = {Mitruț, Oana and Moise, Gabriela and Moldoveanu, Alin and Moldoveanu, Florica and Leordeanu, Marius and Petrescu, Livia},
  doi          = {10.1007/s10462-024-10952-7},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Clarity in complexity: How aggregating explanations resolves the disagreement problem},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tire wear monitoring using feature fusion and CatBoost
classifier. <em>AIR</em>, <em>57</em>(12), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10999-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the critical issue of tire wear is essential for enhancing vehicle safety, performance, and maintenance. Worn-out tires often lead to accidents, underscoring the need for effective monitoring systems. This study is vital for several reasons: safety, as worn tires increase the risk of accidents due to reduced traction and longer braking distances; performance, as uneven tire wear affects vehicle handling and fuel efficiency; maintenance costs, as early detection can prevent more severe damage to suspension and alignment systems; and regulatory compliance, as ensuring tire integrity helps meet safety regulations imposed by transportation authorities. In response, this study systematically evaluates tire conditions at 25%, 50%, 75%, and 100% wear, with an intact tire as a reference, using vibration signals as the primary data source. The analysis employs statistical, histogram, and autoregressive–moving-average (ARMA) feature extraction techniques, followed by feature selection to identify key parameters influencing tire wear. CatBoost is used for feature classification, leveraging its adaptability and efficiency in distinguishing varying wear patterns. Additionally, the study incorporates feature fusion to combine different types of features for a more comprehensive analysis. The proposed methodology not only offers a robust framework for accurately classifying tire wear levels but also holds significant potential for real-time implementation, contributing to proactive maintenance practices, prolonged tire lifespan, and overall vehicular safety.},
  archive      = {J_AIR},
  author       = {Prasshanth, C. V. and Sugumaran, V.},
  doi          = {10.1007/s10462-024-10999-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Tire wear monitoring using feature fusion and CatBoost classifier},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of artificial intelligence methods in bladder
cancer: Segmentation, classification, and detection. <em>AIR</em>,
<em>57</em>(12), 1–30. (<a
href="https://doi.org/10.1007/s10462-024-10953-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and other disruptive technologies can potentially improve healthcare across various disciplines. Its subclasses, artificial neural networks, deep learning, and machine learning, excel in extracting insights from large datasets and improving predictive models to boost their utility and accuracy. Though research in this area is still in its early phases, it holds enormous potential for the diagnosis, prognosis, and treatment of urological diseases, such as bladder cancer. The long-used nomograms and other classic forecasting approaches are being reconsidered considering AI’s capabilities. This review emphasizes the coming integration of artificial intelligence into healthcare settings while critically examining the most recent and significant literature on the subject. This study seeks to define the status of AI and its potential for the future, with a special emphasis on how AI can transform bladder cancer diagnosis and treatment.},
  archive      = {J_AIR},
  author       = {Bashkami, Ayah and Nasayreh, Ahmad and Makhadmeh, Sharif Naser and Gharaibeh, Hasan and Alzahrani, Ahmed Ibrahim and Alwadain, Ayed and Heming, Jia and Ezugwu, Absalom E. and Abualigah, Laith},
  doi          = {10.1007/s10462-024-10953-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of artificial intelligence methods in bladder cancer: Segmentation, classification, and detection},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chronobridge: A novel framework for enhanced temporal and
relational reasoning in temporal knowledge graphs. <em>AIR</em>,
<em>57</em>(12), 1–33. (<a
href="https://doi.org/10.1007/s10462-024-10983-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of predicting entities and relations in Temporal Knowledge Graph (TKG) extrapolation is crucial and has been studied extensively. Mainstream algorithms, such as Gated Recurrent Unit (GRU) models, primarily focus on encoding historical factual features within TKGs, often neglecting the importance of incorporating entities and relational features during decoding. This bias ultimately leads to loss of detail and inadequate prediction accuracy during the inference process. To address this issue, a novel ChronoBridge framework is proposed that features a dual mechanism of a chronological node encoder and a bridged feature fusion decoder. Specifically, the chronological node encoder employs an advanced recursive neural network with an enhanced GRU in an autoregressive manner to model historical KG sequences, thereby accurately capturing entity changes over time and significantly enhancing the model’s ability to identify and encode temporal patterns of facts across the timeline. Meanwhile, the bridged feature fusion decoder utilizes a new variant of GRU and a multilayer perception mechanism during the prediction phase to extract entity and relation features and fuse them for inference, thereby strengthening the reasoning capabilities of the model for future events. Testing on three standard datasets showed significant improvements, with a 25.21% increase in MRR accuracy and a 39.38% enhancement in relation inference. This advancement not only improves the understanding of temporal evolution in knowledge graphs but also sets a foundation for future research and applications of TKG reasoning.},
  archive      = {J_AIR},
  author       = {Liu, Qian and Feng, Siling and Huang, Mengxing and Bhatti, Uzair Aslam},
  doi          = {10.1007/s10462-024-10983-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Chronobridge: A novel framework for enhanced temporal and relational reasoning in temporal knowledge graphs},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks for multi-view learning: A taxonomic
review. <em>AIR</em>, <em>57</em>(12), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10990-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of user-generated content, multi-view learning has become a rapidly growing direction in pattern recognition and data analysis areas. Due to the significant application value of multi-view learning, there has been a continuous emergence of research based on machine learning methods and traditional deep learning paradigms. The core challenge in multi-view learning lies in harnessing both consistent and complementary information to forge a unified, comprehensive representation. However, many multi-view learning tasks are based on graph-structured data, making existing methods unable to effectively mine the information contained in the input multiple data sources. Recently, graph neural networks (GNN) techniques have been widely utilized to deal with non-Euclidean data, such as graphs or manifolds. Thus, it is essential to combine the advantages of the powerful learning capability of GNN models and multi-view data. In this paper, we aim to provide a comprehensive survey of recent research works on GNN-based multi-view learning. In detail, we first provide a taxonomy of GNN-based multi-view learning methods according to the input form of models: multi-relation, multi-attribute and mixed. Then, we introduce the applications of multi-view learning, including recommendation systems, computer vision and so on. Moreover, several public datasets and open-source codes are introduced for implementation. Finally, we analyze the challenges of applying GNN models on various multi-view learning tasks and state new future directions in this field.},
  archive      = {J_AIR},
  author       = {Xiao, Shunxin and Li, Jiacheng and Lu, Jielong and Huang, Sujia and Zeng, Bao and Wang, Shiping},
  doi          = {10.1007/s10462-024-10990-1},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph neural networks for multi-view learning: A taxonomic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence techniques for dynamic security
assessments - a survey. <em>AIR</em>, <em>57</em>(12), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10993-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing uptake of converter-interfaced generation (CIG) is changing power system dynamics, rendering them extremely dependent on fast and complex control systems. Regularly assessing the stability of these systems across a wide range of operating conditions is thus a critical task for ensuring secure operation. However, the simultaneous simulation of both fast and slow (electromechanical) phenomena, along with an increased number of critical operating conditions, pushes traditional dynamic security assessments (DSA) to their limits. While DSA has served its purpose well, it will not be tenable in future electricity systems with thousands of power electronic devices at different voltage levels on the grid. Therefore, reducing both human and computational efforts required for stability studies is more critical than ever. In response to these challenges, several advanced simulation techniques leveraging artificial intelligence (AI) have been proposed in recent years. AI techniques can handle the increased uncertainty and complexity of power systems by capturing the non-linear relationships between the system’s operational conditions and their stability without solving the set of algebraic-differential equations that model the system. Once these relationships are established, system stability can be promptly and accurately evaluated for a wide range of scenarios. While hundreds of research articles confirm that AI techniques are paving the way for fast stability assessments, many questions and issues must still be addressed, especially regarding the pertinence of studying specific types of stability with the existing AI-based methods and their application in real-world scenarios. In this context, this article presents a comprehensive review of AI-based techniques for stability assessments in power systems. Different AI technical implementations, such as learning algorithms and the generation and treatment of input data, are widely discussed and contextualized. Their practical applications, considering the type of stability, system under study, and type of applications, are also addressed. We review the ongoing research efforts and the AI-based techniques put forward thus far for DSA, contextualizing and interrelating them. We also discuss the advantages, limitations, challenges, and future trends of AI techniques for stability studies.},
  archive      = {J_AIR},
  author       = {Cuevas, Miguel and Álvarez-Malebrán, Ricardo and Rahmann, Claudia and Ortiz, Diego and Peña, José and Rozas-Valderrama, Rodigo},
  doi          = {10.1007/s10462-024-10993-y},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence techniques for dynamic security assessments - a survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactuals in fuzzy relational models. <em>AIR</em>,
<em>57</em>(12), 1–19. (<a
href="https://doi.org/10.1007/s10462-024-10996-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the pressing need for explainability in Machine Learning systems, the studies on counterfactual explanations have gained significant interest. This research delves into this timely problem cast in a unique context of relational systems described by fuzzy relational equations. We develop a comprehensive solution to the counterfactual problems encountered in this setting, which is a novel contribution to the field. An underlying optimization problem is formulated, and its gradient-based solution is constructed. We demonstrate that the non-uniqueness of the derived solution is conveniently formalized and quantified by admitting a result coming in the form of information granules of a higher type, namely type-2 or interval-valued fuzzy set. The construction of the solution in this format is realized by invoking the principle of justifiable granularity, another innovative aspect of our research. We also discuss ways of designing fuzzy relations and elaborate on methods of carrying out counterfactual explanations in rule-based models. Illustrative examples are included to present the performance of the method and interpret the obtained results.},
  archive      = {J_AIR},
  author       = {Al-Hmouz, Rami and Pedrycz, Witold and Ammari, Ahmed},
  doi          = {10.1007/s10462-024-10996-9},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Counterfactuals in fuzzy relational models},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of recent approaches to form understanding in
scanned documents. <em>AIR</em>, <em>57</em>(12), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-11000-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive survey of over 100 research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, with particular focus on transformer-based models, which have been shown to improve performance in form understanding tasks by up to 25% in accuracy compared to traditional methods. Our research methodology involves an in-depth analysis of popular documents and trends over the last decade, including 15 state-of-the-art models and 10 benchmark datasets. By examining these works, we offer novel insights into the evolution of this domain. Specifically, we highlight how transformers have revolutionized form-understanding techniques by enhancing the ability to process noisy scanned documents with significant improvements in OCR accuracy. Furthermore, we present an overview of the most relevant datasets, such as FUNSD, CORD, and SROIE, which serve as benchmarks for evaluating the performance of the models. By comparing the capabilities of these models and reporting an average improvement of 10–15% in key form extraction tasks, we aim to provide researchers and practitioners with useful guidance in selecting the most suitable solutions for their form understanding applications.},
  archive      = {J_AIR},
  author       = {Abdallah, Abdelrahman and Eberharter, Daniel and Pfister, Zoe and Jatowt, Adam},
  doi          = {10.1007/s10462-024-11000-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of recent approaches to form understanding in scanned documents},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span
class="math display"><em>p</em>, <em>q</em>, <em>r</em>−</span>
fractional fuzzy sets and their aggregation operators and applications.
<em>AIR</em>, <em>57</em>(12), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10911-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using $$p,q,r-$$ fractional fuzzy sets ( $$p,q,r-$$ FFS) to demonstrate the stability of cryptocurrencies is considered due to the complex and volatile nature of cryptocurrency markets, where traditional models may fall short in capturing nuances and uncertainties. $$p,q,r-$$ FFS provides a flexible framework for modeling cryptocurrency stability by accommodating imprecise data, multidimensional analysis of various market factors, and adaptability to the unique characteristics of the cryptocurrency space, potentially offering a more comprehensive understanding of the factors influencing stability. Existing studies have explored Picture Fuzzy Sets and Spherical Fuzzy Sets, built on membership, neutrality, and non-membership grades. However, these sets can’t reach the maximum value (equal to $$1$$ ) due to grade constraints. For example, when considering $$\wp =(h,\langle \text{0.9,0.8,1.0}\rangle \left|h\in H\right.)$$ , these sets fall short. This is obvious when a decision-maker possesses complete confidence in an alternative, they have the option to assign a value of 1 as the assessment score for that alternative. This signifies that they harbor no doubts or uncertainties regarding the chosen option. To address this, $$p,q,r-$$ Fractional Fuzzy Sets ( $$p,q,r-$$ FFSs) are introduced, using new parameters $$p$$ , $$q$$ , and $$r$$ . These parameters abide by $$p$$ , $$q\ge 1$$ and $$r$$ as the least common multiple of $$p$$ and $$q$$ . We establish operational laws for $$p,q,r-$$ FFSs. Based on these operational laws, we proposed a series of aggregation operators (AOs) to aggregate the information in context of $$p,q,r-$$ fractional fuzzy numbers. Furthermore, we constructed a novel multi-criteria group decision-making (MCGDM) method to deal with real-world decision-making problems. A numerical example is provided to demonstrate the proposed approach.},
  archive      = {J_AIR},
  author       = {Gulistan, Muhammad and Hongbin, Ying and Pedrycz, Witold and Rahim, Muhammad and Amin, Fazli and Khalifa, Hamiden Abd El-Wahed},
  doi          = {10.1007/s10462-024-10911-2},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {$$p,q,r-$$ fractional fuzzy sets and their aggregation operators and applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bio-inspired disease prediction: Harnessing the power of
electric eel foraging optimization algorithm with machine learning for
heart disease prediction. <em>AIR</em>, <em>57</em>(12), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10975-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease is the most significant health problem around the world. Thus, it emphasizes the need for accurate and efficient predictive models for early diagnosis. This study proposes an innovative approach integrating the Electric Eel Foraging Optimization Algorithm (EEFOA) with the Random Forest (RF) algorithm for classifying heart disease prediction. EEFOA draws inspiration from the foraging behaviour of electric eels, a bio-inspired optimization framework capable of effectively exploring complex solutions. The objective is to improve the predictive performance of heart disease diagnosis by integrating optimization and Machine learning methodologies. The experiment uses a heart disease dataset comprising clinical and demographic features of at-risk individuals. Subsequently, EEFOA was applied to optimize the features of the dataset and classification using the RF algorithm, thereby enhancing its predictive performance. The results demonstrate that the Electric Eel Foraging Optimization Algorithm Random Forest (EEFOARF) model outperforms traditional RF and other state-of-the-art classifiers in terms of predictive accuracy, sensitivity, specificity, precision, and Log_Loss, achieving remarkable scores of 96.59%, 95.15%, 98.04%, 98%, and 0.1179, respectively. The proposed methodology has the potential to make a significant contribution, thereby reducing morbidity and mortality rates.},
  archive      = {J_AIR},
  author       = {Narasimhan, Geetha and Victor, Akila},
  doi          = {10.1007/s10462-024-10975-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bio-inspired disease prediction: Harnessing the power of electric eel foraging optimization algorithm with machine learning for heart disease prediction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for measuring the training efficiency of a
neural architecture. <em>AIR</em>, <em>57</em>(12), 1–33. (<a
href="https://doi.org/10.1007/s10462-024-10943-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring Efficiency in neural network system development is an open research problem. This paper presents an experimental framework to measure the training efficiency of a neural architecture. To demonstrate our approach, we analyze the training efficiency of Convolutional Neural Networks and Bayesian equivalents on the MNIST and CIFAR-10 tasks. Our results show that training efficiency decays as training progresses and varies across different stopping criteria for a given neural model and learning task. We also find a non-linear relationship between training stopping criteria, training Efficiency, model size, and training Efficiency. Furthermore, we illustrate the potential confounding effects of overtraining on measuring the training efficiency of a neural architecture. Regarding relative training efficiency across different architectures, our results indicate that CNNs are more efficient than BCNNs on both datasets. More generally, as a learning task becomes more complex, the relative difference in training efficiency between different architectures becomes more pronounced.},
  archive      = {J_AIR},
  author       = {Cueto-Mendoza, Eduardo and Kelleher, John},
  doi          = {10.1007/s10462-024-10943-8},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A framework for measuring the training efficiency of a neural architecture},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GOG-MBSHO: Multi-strategy fusion binary sea-horse optimizer
with gaussian transfer function for feature selection of cancer gene
expression data. <em>AIR</em>, <em>57</em>(12), 1–57. (<a
href="https://doi.org/10.1007/s10462-024-10954-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer gene expression data has the characteristics of high-dimensional, multi-text and multi-classification. The problem of cancer subtype diagnosis can be solved by selecting the most representative and predictive genes from a large number of gene expression data. Feature selection technology can effectively reduce the dimension of data, which helps analyze the information on cancer gene expression data. A multi-strategy fusion binary sea-horse optimizer based on Gaussian transfer function (GOG-MBSHO) is proposed to solve the feature selection problem of cancer gene expression data. Firstly, the multi-strategy includes golden sine strategy, hippo escape strategy and multiple inertia weight strategies. The sea-horse optimizer with the golden sine strategy does not disrupt the structure of the original algorithm. Embedding the golden sine strategy within the spiral motion of the sea-horse optimizer enhances the movement of the algorithm and improves its global exploration and local exploitation capabilities. The hippo escape strategy is introduced for random selection, which avoids the algorithm from falling into local optima, increases the search diversity, and improves the optimization accuracy of the algorithm. The advantage of multiple inertial weight strategies is that dynamic exploitation and exploration can be carried out to accelerate the convergence speed and improve the performance of the algorithm. Then, the effectiveness of multi-strategy fusion was demonstrated by 15 UCI datasets. The simulation results show that the proposed Gaussian transfer function is better than the commonly used S-type and V-type transfer functions, which can improve the classification accuracy, effectively reduce the number of features, and obtain better fitness value. Finally, comparing with other binary swarm intelligent optimization algorithms on 15 cancer gene expression datasets, it is proved that the proposed GOG1-MBSHO has great advantages in the feature selection of cancer gene expression data.},
  archive      = {J_AIR},
  author       = {Wang, Yu-Cai and Song, Hao-Ming and Wang, Jie-Sheng and Song, Yu-Wei and Qi, Yu-Liang and Ma, Xin-Ru},
  doi          = {10.1007/s10462-024-10954-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {GOG-MBSHO: Multi-strategy fusion binary sea-horse optimizer with gaussian transfer function for feature selection of cancer gene expression data},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on feature extraction and learning techniques for
link prediction in homogeneous and heterogeneous complex networks.
<em>AIR</em>, <em>57</em>(12), 1–93. (<a
href="https://doi.org/10.1007/s10462-024-10998-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex networks are commonly observed in several real-world areas, such as social, biological, and technical systems, where they exhibit complicated patterns of connectedness and organised clusters. These networks have intricate topological characteristics that frequently elude conventional characterization. Link prediction in complex networks, like data flow in telecommunications networks, protein interactions in biological systems, and social media interactions on platforms like Facebook, etc., is an essential element of network analytics and presents fresh research challenges. Consequently, there is a growing emphasis in research on creating new link prediction methods for different network applications. This survey investigates several strategies related to link prediction, ranging from feature extraction based to feature learning based techniques, with a specific focus on their utilisation in dynamic and developing network topologies. Furthermore, this paper emphasises on a wide variety of feature learning techniques that go beyond basic feature extraction and matrix factorization. It includes advanced learning-based algorithms and neural network techniques specifically designed for link prediction. The study also presents evaluation results of different link prediction techniques on homogeneous and heterogeneous network datasets, and provides a thorough examination of existing methods and potential areas for further investigation.},
  archive      = {J_AIR},
  author       = {Kapoor, Puneet and Kaushal, Sakshi and Kumar, Harish and Kanwar, Kushal},
  doi          = {10.1007/s10462-024-10998-7},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-93},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on feature extraction and learning techniques for link prediction in homogeneous and heterogeneous complex networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of deep causal models and their industrial
applications. <em>AIR</em>, <em>57</em>(11), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10886-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of causality assumes a paramount position within the realm of human cognition. Over the past few decades, there has been significant advancement in the domain of causal effect estimation across various disciplines, including but not limited to computer science, medicine, economics, and industrial applications. Given the continous advancements in deep learning methodologies, there has been a notable surge in its utilization for the estimation of causal effects using counterfactual data. Typically, deep causal models map the characteristics of covariates to a representation space and then design various objective functions to estimate counterfactual data unbiasedly. Different from the existing surveys on causal models in machine learning, this review mainly focuses on the overview of the deep causal models based on neural networks, and its core contributions are as follows: (1) we cast insight on a comprehensive overview of deep causal models from both timeline of development and method classification perspectives; (2) we outline some typical applications of causal effect estimation to industry; (3) we also endeavor to present a detailed categorization and analysis on relevant datasets, source codes and experiments.},
  archive      = {J_AIR},
  author       = {Li, Zongyu and Guo, Xiaobo and Qiang, Siwei},
  doi          = {10.1007/s10462-024-10886-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of deep causal models and their industrial applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable deep learning approach for advanced persistent
threats (APTs) detection in cybersecurity: A review. <em>AIR</em>,
<em>57</em>(11), 1–47. (<a
href="https://doi.org/10.1007/s10462-024-10890-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Advanced Persistent Threat (APT) attacks on network systems have increased through sophisticated fraud tactics. Traditional Intrusion Detection Systems (IDSs) suffer from low detection accuracy, high false-positive rates, and difficulty identifying unknown attacks such as remote-to-local (R2L) and user-to-root (U2R) attacks. This paper addresses these challenges by providing a foundational discussion of APTs and the limitations of existing detection methods. It then pivots to explore the novel integration of deep learning techniques and Explainable Artificial Intelligence (XAI) to improve APT detection. This paper aims to fill the gaps in the current research by providing a thorough analysis of how XAI methods, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), can make black-box models more transparent and interpretable. The objective is to demonstrate the necessity of explainability in APT detection and propose solutions that enhance the trustworthiness and effectiveness of these models. It offers a critical analysis of existing approaches, highlights their strengths and limitations, and identifies open issues that require further research. This paper also suggests future research directions to combat evolving threats, paving the way for more effective and reliable cybersecurity solutions. Overall, this paper emphasizes the importance of explainability in enhancing the performance and trustworthiness of cybersecurity systems.},
  archive      = {J_AIR},
  author       = {Mutalib, Noor Hazlina Abdul and Sabri, Aznul Qalid Md and Wahab, Ainuddin Wahid Abdul and Abdullah, Erma Rahayu Mohd Faizal and AlDahoul, Nouar},
  doi          = {10.1007/s10462-024-10890-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Explainable deep learning approach for advanced persistent threats (APTs) detection in cybersecurity: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of aspect-based sentiment analysis:
Domains, methods, and trends. <em>AIR</em>, <em>57</em>(11), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10906-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a fine-grained type of sentiment analysis that identifies aspects and their associated opinions from a given text. With the surge of digital opinionated text data, ABSA gained increasing popularity for its ability to mine more detailed and targeted insights. Many review papers on ABSA subtasks and solution methodologies exist, however, few focus on trends over time or systemic issues relating to research application domains, datasets, and solution approaches. To fill the gap, this paper presents a systematic literature review (SLR) of ABSA studies with a focus on trends and high-level relationships among these fundamental components. This review is one of the largest SLRs on ABSA. To our knowledge, it is also the first to systematically examine the interrelations among ABSA research and data distribution across domains, as well as trends in solution paradigms and approaches. Our sample includes 727 primary studies screened from 8550 search results without time constraints via an innovative automatic filtering process. Our quantitative analysis not only identifies trends in nearly two decades of ABSA research development but also unveils a systemic lack of dataset and domain diversity as well as domain mismatch that may hinder the development of future ABSA research. We discuss these findings and their implications and propose suggestions for future research.},
  archive      = {J_AIR},
  author       = {Hua, Yan Cathy and Denny, Paul and Wicker, Jörg and Taskova, Katerina},
  doi          = {10.1007/s10462-024-10906-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of aspect-based sentiment analysis: Domains, methods, and trends},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient network clustering approach using
graph-boosting and nonnegative matrix factorization. <em>AIR</em>,
<em>57</em>(11), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10912-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network clustering is a critical task in data analysis, aimed at uncovering the underlying structure and patterns within complex networks. Traditional clustering methods often struggle with large-scale and noisy data, leading to suboptimal results. Also, the efficiency of positive samples in network clustering depends on the carefully constructed data augmentation, and the pre-training process of the model deals with large-scale data. To address these issues, in this paper, we introduce an efficient network clustering approach that leverages Graph-Boosting and Nonnegative Matrix Factorization to enhance clustering performance (GBNMF). Our algorithm addresses the limitations of traditional clustering techniques by incorporating the strengths of graph-boosting, which iteratively improves the quality of clusters, and Nonnegative Matrix Factorization (NMF), which effectively captures latent structures within the data. We validate our algorithm through extensive experiments on various benchmark network datasets, demonstrating significant improvements in clustering accuracy and robustness. The proposed algorithm not only achieves superior clustering results but also exhibits remarkable computational efficiency, making it a valuable tool for large-scale network analysis applications.},
  archive      = {J_AIR},
  author       = {Tang, Ji and Xu, Xiaoru and Wang, Teng and Rezaeipanah, Amin},
  doi          = {10.1007/s10462-024-10912-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient network clustering approach using graph-boosting and nonnegative matrix factorization},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable generative AI (GenXAI): A survey,
conceptualization, and research agenda. <em>AIR</em>, <em>57</em>(11),
1–38. (<a href="https://doi.org/10.1007/s10462-024-10916-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI (GenAI) represents a shift from AI’s ability to “recognize” to its ability to “generate” solutions for a wide range of tasks. As generated solutions and applications grow more complex and multi-faceted, new needs, objectives, and possibilities for explainability (XAI) have emerged. This work elaborates on why XAI has gained importance with the rise of GenAI and the challenges it poses for explainability research. We also highlight new and emerging criteria that explanations should meet, such as verifiability, interactivity, security, and cost considerations. To achieve this, we focus on surveying existing literature. Additionally, we provide a taxonomy of relevant dimensions to better characterize existing XAI mechanisms and methods for GenAI. We explore various approaches to ensure XAI, ranging from training data to prompting. Our paper provides a concise technical background of GenAI for non-technical readers, focusing on text and images to help them understand new or adapted XAI techniques for GenAI. However, due to the extensive body of work on GenAI, we chose not to delve into detailed aspects of XAI related to the evaluation and usage of explanations. Consequently, the manuscript appeals to both technical experts and professionals from other fields, such as social scientists and information systems researchers. Our research roadmap outlines over ten directions for future investigation.},
  archive      = {J_AIR},
  author       = {Schneider, Johannes},
  doi          = {10.1007/s10462-024-10916-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Explainable generative AI (GenXAI): A survey, conceptualization, and research agenda},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph pooling in graph neural networks: Methods and their
applications in omics studies. <em>AIR</em>, <em>57</em>(11), 1–55. (<a
href="https://doi.org/10.1007/s10462-024-10918-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) process the graph-structured data using neural networks and have proven successful in various graph processing tasks. Currently, graph pooling operators have emerged as crucial components that bridge the gap between node representation learning and diverse graph-level tasks by transforming node representations into graph representations. Given the rapid growth and widespread adoption of graph pooling, this review aims to summarize the existing graph pooling operators for GNNs and their representative applications in omics. Specifically, we first present a comprehensive taxonomy of existing graph pooling algorithms, expanding the categorization for both global and hierarchical pooling operators, and for the first time reviewing the inverse operation of graph pooling, named unpooling. Next, we describe the general evaluation framework for graph pooling operators, encompassing three fundamental aspects: experimental setup, ablation analysis, and model interpretation. We also discuss open issues that significantly influence the design of graph pooling operators, including complexity, connectivity, adaptability, additional loss, and attention mechanisms. Finally, we summarize bioinformatics applications of graph pooling operators in omics, including graphs of gene interaction, medical images, and protein structures for drug discovery and disease diagnosis. Furthermore, we showcase the impact of graph pooling operators on research in specific real-world domains, with a focus on prediction performance and model interpretability. This review provides methodological insights in machine learning based graph modeling and related omics research, as well as an ongoing resource by gathering related papers and code in a dedicated GitHub repository ( https://github.com/Hou-WJ/Graph-Pooling-Operators-and-Bioinformatics-Applications ).},
  archive      = {J_AIR},
  author       = {Wang, Yan and Hou, Wenju and Sheng, Nan and Zhao, Ziqi and Liu, Jialin and Huang, Lan and Wang, Juexin},
  doi          = {10.1007/s10462-024-10918-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph pooling in graph neural networks: Methods and their applications in omics studies},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ameliorated fick’s law algorithm based multi-threshold
medical image segmentation. <em>AIR</em>, <em>57</em>(11), 1–75. (<a
href="https://doi.org/10.1007/s10462-024-10919-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a critical and demanding step in medical image processing, which provides a solid foundation for subsequent medical image data extraction and analysis. Multi-threshold image segmentation, one of the most commonly used and specialized image segmentation techniques, limits its application to medical images because it requires demanding computational performance and is difficult to produce satisfactory segmentation results. To overcome the above problems, an ameliorated Fick&#39;s law algorithm (MsFLA) for multi-threshold image segmentation is developed in this paper. First, an optimized sine–cosine strategy is introduced to extend the molecular diffusion process to alleviate the problem of easily falling into local optima, thus improving the convergence accuracy of the Fick&#39;s law algorithm (FLA). Secondly, the introduction of local minimal value avoidance enriches the individual molecular information and enhances the local search ability, thus improving computational accuracy. In addition, the optimal neighborhood learning strategy is added to ensure a more careful and reasonable reliance on the optimal solution, thus reducing the chance of convergence of a local solution. The efficient optimization capability of MsFLA is comprehensively validated by comparing MsFLA with the original FLA and other algorithms in 23 classical benchmark functions. Finally, MsFLA is applied to image segmentation of grayscale images of COVID-19 and brain and color images of Lung and Colon cancer histopathology by using Cross entropy to validate its segmentation capability. The experimental results show that the MsFLA obtains the best segmentation results in three medical image cases compared to other comparison algorithms, which indicates that MsFLA can effectively solve the multi-threshold medical image segmentation problem.},
  archive      = {J_AIR},
  author       = {Hu, Gang and Zhao, Feng and Hussien, Abdelazim G. and Zhong, Jingyu and Houssein, Essam H.},
  doi          = {10.1007/s10462-024-10919-8},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-75},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ameliorated fick’s law algorithm based multi-threshold medical image segmentation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models in medical and healthcare fields:
Applications, advances, and challenges. <em>AIR</em>, <em>57</em>(11),
1–48. (<a href="https://doi.org/10.1007/s10462-024-10921-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) are increasingly recognized for their advanced language capabilities, offering significant assistance in diverse areas like medical communication, patient data optimization, and surgical planning. Our survey meticulously searched for papers with keywords such as “medical,” “clinical,” “healthcare,” and “LLMs” across various databases, including ACM and Google Scholar. It sought to delve into the latest trends and applications of LLMs in healthcare, analyzing 175 relevant publications to support both practitioners and researchers in the field. We have compiled 56 experimental datasets, various evaluation methods and reviewed cutting-edge LLMs across tasks. Our comprehensive analysis of LLMs in healthcare applications, including medical question-answering, dialogue summarization, electronic health record generation, scientific research, medical education, medical product safety monitoring, clinical health reasoning, and clinical decision support. Furthermore, we have identified the challenges, including data security, inaccurate information, fairness and bias, plagiarism, copyrights, and accountability, and the potential solutions, namely de-identification framework, references,counterfactually fair prompting,opening and ending control codes, and establishing normative standards,to address these open issues,respectively. The findings of this survey exert a profound impact on spurring innovation in practical applications and addressing inherent challenges within the academic and medical communities.},
  archive      = {J_AIR},
  author       = {Wang, Dandan and Zhang, Shiqing},
  doi          = {10.1007/s10462-024-10921-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Large language models in medical and healthcare fields: Applications, advances, and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced moth-flame optimizer with quality enhancement
and directional crossover: Optimizing classic engineering problems.
<em>AIR</em>, <em>57</em>(11), 1–56. (<a
href="https://doi.org/10.1007/s10462-024-10923-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular meta-heuristic algorithm, the Moth-Flame Optimization (MFO) algorithm has garnered significant interest owing to its high flexibility and straightforward implementation. However, when addressing engineering constraint problems with specific parameters, MFO also exhibits limitations such as fast convergence and a tendency to converge to local optima. In order to address these challenges, this paper introduces an enhanced version of the MFO, EQDXMFO. EQDXMFO integrates a Quality Enhancement (EQ) strategy and a Directional Crossover (DX) mechanism, fortifying the algorithm’s search dynamics. Specifically, the DX mechanism is designed to augment the population’s diversity, enhancing the algorithm’s exploratory potential. Concurrently, the EQ strategy is employed to elevate the solution quality, which in turn refines the convergence precision of the algorithm. To verify the effectiveness of EQDXMFO, experiments are carried out on the test set of the IEEE CEC2017. A total of 5 classical algorithms, five excellent MFO variants, and seven state-of-the-art algorithms are selected for comparison, which confirm the significant advantages of EQDXMFO. Next, EQDXMFO is applied to five complex engineering constraint problems, demonstrating that EQDXMFO can optimize realistic problems. The comprehensive analysis shows that EQDXMFO has strong optimization capabilities and provides methods for research on other complex real-world problems.},
  archive      = {J_AIR},
  author       = {Yu, Helong and Quan, Jiale and Han, Yongqi and Heidari, Ali Asghar and Chen, Huiling},
  doi          = {10.1007/s10462-024-10923-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An enhanced moth-flame optimizer with quality enhancement and directional crossover: Optimizing classic engineering problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic sleep stage classification using deep learning:
Signals, data representation, and neural networks. <em>AIR</em>,
<em>57</em>(11), 1–68. (<a
href="https://doi.org/10.1007/s10462-024-10926-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical practice, sleep stage classification (SSC) is a crucial step for physicians in sleep assessment and sleep disorder diagnosis. However, traditional sleep stage classification relies on manual work by sleep experts, which is time-consuming and labor-intensive. Faced with this obstacle, computer-aided diagnosis (CAD) has the potential to become an intelligent assistant tool for sleep experts, aiding doctors in the assessment and decision-making process. In fact, in recent years, CAD supported by artificial intelligence, especially deep learning (DL) techniques, has been widely applied in SSC. DL offers higher accuracy and lower costs, making a significant impact. In this paper, we will systematically review SSC research based on DL methods (DL-SSC). We explores DL-SSC from several important perspectives, including signal and data representation, data preprocessing, deep learning models, and performance evaluation. Specifically, this paper addresses three main questions: (1) What signals can DL-SSC use? (2) What are the various methods to represent these signals? (3) What are the effective DL models? Through addressing on these questions, this paper will provide a comprehensive overview of DL-SSC.},
  archive      = {J_AIR},
  author       = {Liu, Peng and Qian, Wei and Zhang, Hua and Zhu, Yabin and Hong, Qi and Li, Qiang and Yao, Yudong},
  doi          = {10.1007/s10462-024-10926-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-68},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic sleep stage classification using deep learning: Signals, data representation, and neural networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for surgical workflow analysis: A survey of
progresses, limitations, and trends. <em>AIR</em>, <em>57</em>(11),
1–44. (<a href="https://doi.org/10.1007/s10462-024-10929-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic surgical workflow analysis, which aims to recognize the ongoing surgical events in videos, is fundamental for developing context-aware computer-assisted systems. This paper reviews representative surgical workflow recognition algorithms based on deep learning, outlining their merits, limitations, and future research directions. The literature survey was performed on three large bibliographic databases, covering 67 lary sources, which were comparatively analyzed in terms of spatial feature modeling, spatio-temporal feature modeling, input pre-processing, regularization and post-processing algorithms, as well as learning strategies. Then, common public datasets and evaluation metrics for surgical workflow recognition are also described in detail. Finally, we discuss all literature from different perspectives, and point out the challenges, possible solutions and future trends. The need for more diverse and larger datasets, the potential of unsupervised and semi-supervised learning approaches, comprehensive and equitable metrics, establishing complete regulatory and data standards, and interoperability will be key challenges in translating models to clinical operating rooms. And we propose that surgical activity anticipation and employing large language model as training assistant are interesting research directions in surgical workflow analysis.},
  archive      = {J_AIR},
  author       = {Li, Yunlong and Zhao, Zijian and Li, Renbo and Li, Feng},
  doi          = {10.1007/s10462-024-10929-6},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for surgical workflow analysis: A survey of progresses, limitations, and trends},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review of medical image processing using quantum-enabled
algorithms. <em>AIR</em>, <em>57</em>(11), 1–52. (<a
href="https://doi.org/10.1007/s10462-024-10932-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and reliable storage, analysis, and transmission of medical images are imperative for accurate diagnosis, treatment, and management of various diseases. Since quantum computing can revolutionize big data analytics by providing faster solutions and security tactics, numerous studies in this field have focused on the use of quantum and quantum-inspired algorithms to enhance the performance of traditional medical image processing approaches. This review aims to provide readers with a succinct yet adequate compendium of the advances in medical image processing combined with quantum behaviors for disease diagnosis and medical image security. Some open challenges are outlined, identifying the performance limitations of current quantum technology in their applications, while addressing the short-, medium-, and long-term development plans of this field in designing future quantum healthcare systems. We hope that this review will provide full guidance for upcoming researchers interested in this area and will stimulate further appetite of experts already active in this area aimed at the pursuit of more advanced quantum paradigms in medical image processing applications.},
  archive      = {J_AIR},
  author       = {Yan, Fei and Huang, Hesheng and Pedrycz, Witold and Hirota, Kaoru},
  doi          = {10.1007/s10462-024-10932-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Review of medical image processing using quantum-enabled algorithms},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of video-based human action recognition in team
sports. <em>AIR</em>, <em>57</em>(11), 1–55. (<a
href="https://doi.org/10.1007/s10462-024-10934-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few decades, numerous studies have focused on identifying and recognizing human actions using machine learning and computer vision techniques. Video-based human action recognition (HAR) aims to detect actions from video sequences automatically. This can cover simple gestures to complex actions involving multiple people interacting with objects. Actions in team sports exhibit a different nature compared to other sports, since they tend to occur at a faster pace and involve more human-human interactions. As a result, research has typically not focused on the challenges of HAR in team sports. This paper comprehensively summarises HAR-related research and applications with specific focus on team sports such as football (soccer), basketball and Australian rules football. Key datasets used for HAR-related team sports research are explored. Finally, common challenges and future work are discussed, and possible research directions identified.},
  archive      = {J_AIR},
  author       = {Yin, Hongwei and Sinnott, Richard O. and Jayaputera, Glenn T.},
  doi          = {10.1007/s10462-024-10934-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of video-based human action recognition in team sports},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big data applications: Overview, challenges and future.
<em>AIR</em>, <em>57</em>(11), 1–49. (<a
href="https://doi.org/10.1007/s10462-024-10938-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data (i.e., social big data, vehicular big data, healthcare big data etc) points to massive and complex data, that require special technologies and approaches for storage, processing, and analysis. Similarly, big data applications are software and systems utilizing large and complex datasets to extract insights, support decision-making, and address diverse business and societal challenges. Recently, the significance of big data applications has grown immensely for organizations across diverse sectors as they increasingly rely on insights derived from data. The increasing reliance on data insights has rendered traditional technologies and platforms inefficient due to scalability limitations and performance issues. This study contributes by identifying key domains impacted by big data, examining its effect on decision-making, addressing inherent complexities and opportunities, exploring core technologies, and offering solutions for potential concerns. Additionally, it conducts a comparative analysis to demonstrate the superiority of this research. These contributions provide valuable insights into the evolving landscape shaped by big data applications.},
  archive      = {J_AIR},
  author       = {Badshah, Afzal and Daud, Ali and Alharbey, Riad and Banjar, Ameen and Bukhari, Amal and Alshemaimri, Bader},
  doi          = {10.1007/s10462-024-10938-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Big data applications: Overview, challenges and future},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid approach for bengali sentence validation.
<em>AIR</em>, <em>57</em>(11), 1–31. (<a
href="https://doi.org/10.1007/s10462-024-10795-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bengali is the official language of Bangladesh and is widely used in Bangladesh and West Bengal in India. Due to the growing accessibility of the internet and smart devices, the use of digital text material and documents in Bengali is growing with time. An automated Bengali Sentence Validation System is proposed in this study to effectively determine the correctness of sentences in such extensively available Bengali content. As far as we know, no substantial work has been done in the field of Bengali Sentence Validation utilizing deep learning approaches. Due to the lack of linguistic resources, sophisticated Natural Language Processing tools, and benchmark datasets, developing an automated Sentence Validation System for a limited-resource language like Bengali is challenging. Additionally, Bengali Sentences come in two morphological varieties (Sadhu-bhasha and Cholito-bhasha), making the validation process more challenging. The proposed automated Bengali Sentence Validation system contains the CNN-BiLSTM hybrid classifier model. As of now, there is no standard dataset for Bengali sentence validation. Due to the lack of a standard dataset, we collected Bengali sentences from different sources in Bangladesh and developed a Bengali Sentence Validation (BSV) Dataset with around 5000 labelled sentences arranged into two categories such as correct and incorrect. Experimental results demonstrate that the proposed system outperformed other classifier models and existing approaches for Bengali Sentence Validation and is able to categorize a wide range of Bengali sentences based on their correctness. The system’s F1 score for the Bengali Sentence Validation is 98%.},
  archive      = {J_AIR},
  author       = {Sikder, Juel and Chakraborty, Prosenjit and Das, Utpol Kanti and Dhar, Krity},
  doi          = {10.1007/s10462-024-10795-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A hybrid approach for bengali sentence validation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the kepler optimization algorithm with chaotic
maps: Comprehensive performance evaluation and engineering applications.
<em>AIR</em>, <em>57</em>(11), 1–44. (<a
href="https://doi.org/10.1007/s10462-024-10857-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kepler Optimisation Algorithm (KOA) is a recently proposed algorithm that is inspired by Kepler’s laws to predict the positions and velocities of planets at a given time. However, although promising, KOA can encounter challenges such as convergence to sub-optimal solutions or slow convergence speed. This paper proposes an improvement to KOA by integrating chaotic maps to solve complex engineering problems. The improved algorithm, named Chaotic Kepler Optimization Algorithm (CKOA), is characterized by a better ability to avoid local minima and to reach globally optimal solutions thanks to a dynamic diversification strategy based on chaotic maps. To confirm the effectiveness of the suggested approach, in-depth statistical analyses were carried out using the CEC2020 and CEC2022 benchmarks. These analyses included mean and standard deviation of fitness, convergence curves, Wilcoxon tests, as well as population diversity assessments. The experimental results, which compare CKOA not only to the original KOA but also to eight other recent optimizers, show that the proposed algorithm performs better in terms of convergence speed and solution quality. In addition, CKOA has been successfully tested on three complex engineering problems, confirming its robustness and practical effectiveness. These results make CKOA a powerful optimisation tool in a variety of complex real-world contexts. After final acceptance, the source code will be uploaded to the Github account: nawal.elghouate@usmba.ac.ma.},
  archive      = {J_AIR},
  author       = {El Ghouate, Nawal and Bencherqui, Ahmed and Mansouri, Hanaa and Maloufy, Ahmed El and Tahiri, Mohamed Amine and Karmouni, Hicham and Sayyouri, Mhamed and Askar, S. S. and Abouhawwash, Mohamed},
  doi          = {10.1007/s10462-024-10857-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improving the kepler optimization algorithm with chaotic maps: Comprehensive performance evaluation and engineering applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adjusted graph based semi-supervised embedded feature
selection. <em>AIR</em>, <em>57</em>(11), 1–34. (<a
href="https://doi.org/10.1007/s10462-024-10868-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised feature selection has aroused continuous attention in processing high-dimensional data with most unlabeled and fewer data samples. Many graph-based models perform on a pre-defined graph, which is separated from the procedure of feature selection, making the model hard to select the discriminative features. To address this issue, we exploit a self-adjusted graph for semi-supervised embedded feature selection method (SAGFS), which learns an optimal sparse similarity graph to replace the pre-defined graph to alleviate the effect of data noise. SAGFS allows the learned graph itself to be adjusted according to the local geometric structure of the data and the procedure of selecting features to select the most representative features. Besides that, we introduce $$l_{2,p}$$ -norm to constrain the projection matrix for efficient feature selection. An efficient alternating optimization algorithm is presented, together with analyses on its convergence. Systematical experiments on several publicly datasets are performed to analyze the proposed model from several aspects, and demonstrate that our approaches outperform other comparison methods.},
  archive      = {J_AIR},
  author       = {Zhu, Jianyong and Zheng, Jiaying and Zhou, Zhenchen and Ding, Qiong and Nie, Feiping},
  doi          = {10.1007/s10462-024-10868-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Self-adjusted graph based semi-supervised embedded feature selection},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformers-based architectures for stroke segmentation: A
review. <em>AIR</em>, <em>57</em>(11), 1–35. (<a
href="https://doi.org/10.1007/s10462-024-10900-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes. The emergence of deep learning methodologies has transformed the landscape of medical image analysis. Recently, Transformers, initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis. This comprehensive review aims to provide an in-depth exploration of the cutting-edge Transformer-based architectures applied in the context of stroke segmentation. It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation. Subsequently, the review delves into the fundamental ideas of Transformers, offering detailed insights into their architectural intricacies and the underlying mechanisms that empower them to effectively capture complex spatial information within medical images. The existing literature is systematically categorized and analyzed, discussing various approaches that leverage Transformers for stroke segmentation. A critical assessment is provided, highlighting the strengths and limitations of these methods, including considerations of performance and computational efficiency. Additionally, this review explores potential avenues for future research and development.},
  archive      = {J_AIR},
  author       = {Zafari-Ghadim, Yalda and Rashed, Essam A. and Mohamed, Amr and Mabrok, Mohamed},
  doi          = {10.1007/s10462-024-10900-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transformers-based architectures for stroke segmentation: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel ML-MCDM-based decision support system for evaluating
autonomous vehicle integration scenarios in geneva’s public
transportation. <em>AIR</em>, <em>57</em>(11), 1–64. (<a
href="https://doi.org/10.1007/s10462-024-10917-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel decision-support system (DSS) to assist decision-makers in the ULTIMO project with integrating Autonomous Vehicles (AVs) in Geneva, Switzerland. Specifically, it aids in selecting the best scenario for incorporating AVs into Geneva’s public transportation system. The proposed DSS is architected on a combined integrated framework that includes a machine learning (ML) algorithm, random forest (RF) algorithm, and three novel multi-criteria decision-making (MCDM) algorithms: (1) Modified E-ARWEN (ME-ARWEN) for selecting the best scenario with high sensitivity; (2) Compromiser—Positive, Neutral, Negative (Compromiser-PNN) for extracting weights from stakeholders, considering their preferences and potential conflicts; and (3) Collective Weight Processor (CWP) for deriving weights from expert opinions. Besides the main objective, this article also aims to: (1) Address the gap in practical DSS software within AV-related studies by providing Python codes of the DSS; (2) Develop a highly sensitive and comprehensive MCDM framework to address the project’s needs; and (3) Employ Artificial Intelligence within the DSS to optimize outputs. By the application of the proposed DSS, four scenarios were evaluated: (1) Full integration of AVs; (2) Partial integration; (3) Pilot project in limited areas; and (4) Delayed integration. The analysis identified partial integration as the best scenario for integrating AVs. Furthermore, comprehensive analyses conducted to validate the DSS outputs demonstrated the reliability of the results.},
  archive      = {J_AIR},
  author       = {Zakeri, Shervin and Konstantas, Dimitri and Sorooshian, Shahryar and Chatterjee, Prasenjit},
  doi          = {10.1007/s10462-024-10917-w},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel ML-MCDM-based decision support system for evaluating autonomous vehicle integration scenarios in geneva’s public transportation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of graph neural network applications in
mechanics-related domains. <em>AIR</em>, <em>57</em>(11), 1–48. (<a
href="https://doi.org/10.1007/s10462-024-10931-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanics-related tasks often present unique challenges in achieving accurate geometric and physical representations, particularly for non-uniform structures. Graph neural networks (GNNs) have emerged as a promising tool to tackle these challenges by adeptly learning from graph data with irregular underlying structures. Consequently, recent years have witnessed a surge in complex mechanics-related applications inspired by the advancements of GNNs. Despite this process, there is a notable absence of a systematic review addressing the recent advancement of GNNs in solving mechanics-related tasks. To bridge this gap, this review article aims to provide an in-depth overview of the GNN applications in mechanics-related domains while identifying key challenges and outlining potential future research directions. In this review article, we begin by introducing the fundamental algorithms of GNNs that are widely employed in mechanics-related applications. We provide a concise explanation of their underlying principles to establish a solid understanding that will serve as a basis for exploring the applications of GNNs in mechanics-related domains. The scope of this paper is intended to cover the categorisation of literature into solid mechanics, fluid mechanics, and interdisciplinary mechanics-related domains, providing a comprehensive summary of graph representation methodologies, GNN architectures, and further discussions in their respective subdomains. Additionally, open data and source codes relevant to these applications are summarised for the convenience of future researchers. This article promotes an interdisciplinary integration of GNNs and mechanics and provides a guide for researchers interested in applying GNNs to solve complex mechanics-related tasks.},
  archive      = {J_AIR},
  author       = {Zhao, Yingxue and Li, Haoran and Zhou, Haosu and Attar, Hamid Reza and Pfaff, Tobias and Li, Nan},
  doi          = {10.1007/s10462-024-10931-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of graph neural network applications in mechanics-related domains},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of artificial intelligence in the new generation
of underwater humanoid welding robots: A review. <em>AIR</em>,
<em>57</em>(11), 1–32. (<a
href="https://doi.org/10.1007/s10462-024-10940-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater welding robots play a crucial role in addressing challenges such as low efficiency, suboptimal performance, and high risks associated with underwater welding operations. These robots face a dual challenge encompassing both hardware deployment and software algorithms. Recent years have seen significant interest in humanoid robots and artificial intelligence (AI) technologies, which hold promise as breakthrough solutions for advancing underwater welding capabilities. Firstly, this review delves into the hardware platforms envisioned for future underwater humanoid welding robots (UHWR), encompassing both underwater apparatus and terrestrial support equipment. Secondly, it provides an extensive overview of AI applications in underwater welding scenarios, particularly focusing on their implementation in UHWR. This includes detailed discussions on multi-sensor calibration, vision-based three-dimensional (3D) reconstruction, extraction of weld features, decision-making for weld repairs, robot trajectory planning, and motion planning for dual-arm robots. Through comparative analysis within the text, it becomes evident that AI significantly enhances capabilities such as underwater multi-sensor calibration, vision-based 3D reconstruction, and weld feature extraction. Moreover, AI shows substantial potential in tasks like underwater image enhancement, decision-making processes, robot trajectory planning, and dual-arm robot motion planning. Looking ahead, the development trajectory for AI in UHWR emphasizes multifunctional models, edge computing in compact models, and advanced decision-making technologies in expansive models.},
  archive      = {J_AIR},
  author       = {Chi, Peng and Wang, Zhenmin and Liao, Haipeng and Li, Ting and Wu, Xiangmiao and Zhang, Qin},
  doi          = {10.1007/s10462-024-10940-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence in the new generation of underwater humanoid welding robots: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computationally efficient deep learning models for diabetic
retinopathy detection: A systematic literature review. <em>AIR</em>,
<em>57</em>(11), 1–44. (<a
href="https://doi.org/10.1007/s10462-024-10942-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy, often resulting from conditions like diabetes and hypertension, is a leading cause of blindness globally. With diabetes affecting millions worldwide and anticipated to rise significantly, early detection becomes paramount. The survey scrutinizes existing literature, revealing a noticeable absence of consideration for computational complexity aspects in deep learning models. Notably, most researchers concentrate on employing deep learning models, and there is a lack of comprehensive surveys on the role of vision transformers in enhancing the efficiency of these models for DR detection. This study stands out by presenting a systematic review, exclusively considering 84 papers published in reputable academic journals to ensure a focus on mature research. The distinctive feature of this Systematic Literature Review (SLR) lies in its thorough investigation of computationally efficient approaches and models for DR detection. It sheds light on the incorporation of vision transformers into deep learning models, highlighting their significant contribution to improving accuracy. Moreover, the research outlines clear objectives related to the identified problem, giving rise to specific research questions. Following an assessment of relevant literature, data is extracted from digital archives. Additionally, in light of the results obtained from this SLR, a taxonomy for the detection of diabetic retinopathy has been presented. The study also highlights key research challenges and proposes potential avenues for further investigation in the field of detecting diabetic retinopathy.},
  archive      = {J_AIR},
  author       = {Haq, Nazeef Ul and Waheed, Talha and Ishaq, Kashif and Hassan, Muhammad Awais and Safie, Nurhizam and Elias, Nur Fazidah and Shoaib, Muhammad},
  doi          = {10.1007/s10462-024-10942-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computationally efficient deep learning models for diabetic retinopathy detection: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of deep learning techniques for plant
diseases. <em>AIR</em>, <em>57</em>(11), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10944-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is one of the most crucial sectors, meeting the fundamental food needs of humanity. Plant diseases increase food economic and food security concerns for countries and disrupt their agricultural planning. Traditional methods for detecting plant diseases require a lot of labor and time. Consequently, many researchers and institutions strive to address these issues using advanced technological methods. Deep learning-based plant disease detection offers considerable progress and hope compared to classical methods. When trained with large and high-quality datasets, these technologies robustly detect diseases on plant leaves in early stages. This study systematically reviews the application of deep learning techniques in plant disease detection by analyzing 160 research articles from 2020 to 2024. The studies are examined in three different areas: classification, detection, and segmentation of diseases on plant leaves, while also thoroughly reviewing publicly available datasets. This systematic review offers a comprehensive assessment of the current literature, detailing the most popular deep learning architectures, the most frequently studied plant diseases, datasets, encountered challenges, and various perspectives. It provides new insights for researchers working in the agricultural sector. Moreover, it addresses the major challenges in the field of disease detection in agriculture. Thus, this study offers valuable information and a suitable solution based on deep learning applications for agricultural sustainability.},
  archive      = {J_AIR},
  author       = {Pacal, Ishak and Kunduracioglu, Ismail and Alma, Mehmet Hakki and Deveci, Muhammet and Kadry, Seifedine and Nedoma, Jan and Slany, Vlastimil and Martinek, Radek},
  doi          = {10.1007/s10462-024-10944-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of deep learning techniques for plant diseases},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost optimization in edge computing: A survey. <em>AIR</em>,
<em>57</em>(11), 1–83. (<a
href="https://doi.org/10.1007/s10462-024-10947-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge computing paradigm is becoming increasingly commercialized due to the widespread adoption of wireless communication technologies and the growing demand for compute-intensive mobile applications. Edge computing complements the cloud computing model by deploying computation, storage, and network resources to the edge locations of wireless access networks, empowering end devices to run resource-intensive applications. In order to promote the commercialization of edge computing, it is important to explore effective ways to reduce the cost of edge computing networks. This paper provides a comprehensive review of the research findings in recent years, offering a clear perspective on the research dynamics. This paper first recalls the architectural framework of edge computing. Then, the main optimization objectives and optimization methods are comprehensively described. Mainstream mathematical models for cost reduction are then shown in depth. The paper also discusses the methods used to evaluate the effectiveness. Then, typical examples of typical application scenarios for edge computing networks are examined in depth. Finally, the paper identifies some unresolved issues. We expect future research to make more attempts in these directions.},
  archive      = {J_AIR},
  author       = {Cao, Liming and Huo, Tao and Li, Shaobo and Zhang, Xingxing and Chen, Yanchi and Lin, Guangzheng and Wu, Fengbin and Ling, Yihong and Zhou, Yaxin and Xie, Qun},
  doi          = {10.1007/s10462-024-10947-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-83},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Cost optimization in edge computing: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero shot plant disease classification with semantic
attributes. <em>AIR</em>, <em>57</em>(11), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10950-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving field of plant disease detection, the number and complexity of crop diseases are increasing, made worse by factors like climate change. Addressing these challenges requires robust and efficient methodologies capable of early and accurate disease identification. This paper explores the integration of advanced deep learning techniques, including pre-trained models, zero-shot learning, and semantic attributes to enhance the effectiveness of plant disease detection systems. High level features extracted from the images by these pretrained models capture crucial patterns, while domain-specific semantic attributes, such as leaf texture and color variations, enhance the understanding. Incorporating zero-shot learning enables adaptation to new and unseen diseases using semantic descriptions. Experimental validation across diverse plant species and disease types underscores the approach’s reliability in real-world agricultural scenarios. Our approach has demonstrated superior performance with plant village dataset, showing a significant improvement in accuracy and generalization. These results underscore the potential of our method to revolutionize plant disease detection and management in agricultural practices.},
  archive      = {J_AIR},
  author       = {Kumar, Pranav and Mathew, Jimson and Sanodiya, Rakesh Kumar and Setty, Thanush and Bhaskarla, Bhanu Prakash},
  doi          = {10.1007/s10462-024-10950-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Zero shot plant disease classification with semantic attributes},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards attributed graph clustering using enhanced graph and
reconstructed graph structure. <em>AIR</em>, <em>57</em>(11), 1–31. (<a
href="https://doi.org/10.1007/s10462-024-10958-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering, leveraging both structural and attribute information, is crucial in various real-world applications. However, current approaches face challenges stemming from the sparsity of graphs and sensitivity to noise in Graph Convolutional Networks (GCNs). Moreover, GCN-based methods are often designed based on the assumption of homophilic graph and ignore heterophilic graph. To address these, we propose a graph clustering method that consists of four phases: graph enhance, graph reconstruction, graph refine, and dual-guidance supervisor module. An enhanced graph module is defined by an auxiliary graph to consider distant relationships in the topology structure to alleviate the limitations of sparse graphs. The graph reconstruction phase includes the creation and integration of homophily and heterophily graphs to achieve graph-agnostic. In graph refine, the auxiliary graph is iteratively improved to enhance the generalization of the representation. In this phase, a subspace clustering module is applied to convert attribute-based embeddings into relationship-based representations. Finally, the extracted graphs are fed to a dual-guidance supervisor module to obtain the final clustering result. Experimental validation on several benchmark datasets demonstrates the efficiency of our model. Meanwhile, the findings offer significant advancements in attributed graph clustering, promising improved applicability in various domains.},
  archive      = {J_AIR},
  author       = {Yang, Xuejin and Xie, Cong and Zhou, Kemin and Song, Shaoyun and Yang, Junsheng and Li, Bin},
  doi          = {10.1007/s10462-024-10958-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Towards attributed graph clustering using enhanced graph and reconstructed graph structure},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-enhanced power quality management in distribution
systems: Implementing a dual-phase UPQC control with adaptive neural
networks and optimized PI controllers. <em>AIR</em>, <em>57</em>(11),
1–48. (<a href="https://doi.org/10.1007/s10462-024-10959-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of electrical distribution, managing power quality is critical due to its significant impact on infrastructure and customer satisfaction. Addressing issues such as voltage sags and swells, along with current and voltage harmonics, is imperative. The innovative approach proposed in this paper centers on a dual-phase control strategy using a Universal Power Quality Conditioner that integrates series and parallel compensations to rectify these disturbances simultaneously. Our methodology introduces a hybrid control scheme that employs adaptive dynamic neural networks (ADNN), a sinusoidal tracking filter (STF), and a proportional-integral (PI) controller optimized via an improved krill herd (IKH) algorithm. The first phase utilizes the ADNN-based adaptive integrated estimator for quick and accurate disturbance detection and estimation. Subsequently, the second phase employs the STF, omitting the Low Pass Filter and employing a Phase Locking Loop to generate precise reference voltages and currents for the series and parallel active filters based on dynamic load and source conditions. This advanced control mechanism not only enhances system efficacy but also reduces the need for extensive computational resources. Furthermore, the performance of both series and parallel inverters is finely tuned through a PI controller optimized with the IKH algorithm, improving the DC link voltage regulation. Our extensive testing under various conditions, including voltage imbalances and harmonic disturbances, demonstrates the robustness of the proposed solution in both transient and steady-state scenarios.},
  archive      = {J_AIR},
  author       = {Singh, Arvind R. and Dashtdar, Masoud and Bajaj, Mohit and Garmsiri, Reza and Blazek, Vojtech and Prokop, Lukas and Misak, Stanislav},
  doi          = {10.1007/s10462-024-10959-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-enhanced power quality management in distribution systems: Implementing a dual-phase UPQC control with adaptive neural networks and optimized PI controllers},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Over-the-air upgrading for enhancing security of intelligent
connected vehicles: A survey. <em>AIR</em>, <em>57</em>(11), 1–58. (<a
href="https://doi.org/10.1007/s10462-024-10968-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous improvement in the connectivity, automation and autonomy levels of Intelligent Connected Vehicles (ICVs) significantly increases the probability of potential security threats. Over-the-Air (OTA) is a promising technique for upgrading features of ICVs and enhancing their reliability and security against environmental disturbances as well as malicious attacks. To better understand the potential security risks and possible countermeasures, we survey research works in ICV security during OTA from cloud upgrade, terminal upgrade, and object upgrade. We also summarize existing methods in OTA upgrading techniques and systematically investigate the overall framework of OTA upgrading methods from the perspectives of Software-Over-the-Air (SOTA) and Firmware-Over-the-Air (FOTA).We further discuss possible mitigation strategies and open issues yet to be resolved in this research direction. This survey shows that OTA provides a powerful technique for upgrading the ICV features and improving ICV security.},
  archive      = {J_AIR},
  author       = {Li, Beibei and Hu, Wei and Da, Lemei and Wu, Yibing and Wang, Xinxin and Li, Yiwei and Yuan, Chaoxuan},
  doi          = {10.1007/s10462-024-10968-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Over-the-air upgrading for enhancing security of intelligent connected vehicles: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved multi-strategy adaptive grey wolf optimization for
practical engineering applications and high-dimensional problem solving.
<em>AIR</em>, <em>57</em>(10), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10821-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Grey Wolf Optimization (GWO) is a highly effective meta-heuristic algorithm leveraging swarm intelligence to tackle real-world optimization problems. However, when confronted with large-scale problems, GWO encounters hurdles in convergence speed and problem-solving capabilities. To address this, we propose an Improved Adaptive Grey Wolf Optimization (IAGWO), which significantly enhances exploration of the search space through refined search mechanisms and adaptive strategy. Primarily, we introduce the incorporation of velocity and the Inverse Multiquadratic Function (IMF) into the search mechanism. This integration not only accelerates convergence speed but also maintains accuracy. Secondly, we implement an adaptive strategy for population updates, enhancing the algorithm&#39;s search and optimization capabilities dynamically. The efficacy of our proposed IAGWO is demonstrated through comparative experiments conducted on benchmark test sets, including CEC 2017, CEC 2020, CEC 2022, and CEC 2013 large-scale global optimization suites. At CEC2017, CEC 2020 (10/20 dimensions), CEC 2022 (10/20 dimensions), and CEC 2013, respectively, it outperformed other comparative algorithms by 88.2%, 91.5%, 85.4%, 96.2%, 97.4%, and 97.2%. Results affirm that our algorithm surpasses state-of-the-art approaches in addressing large-scale problems. Moreover, we showcase the broad application potential of the algorithm by successfully solving 19 real-world engineering challenges.},
  archive      = {J_AIR},
  author       = {Yu, Mingyang and Xu, Jing and Liang, Weiyun and Qiu, Yu and Bao, Sixu and Tang, Lin},
  doi          = {10.1007/s10462-024-10821-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved multi-strategy adaptive grey wolf optimization for practical engineering applications and high-dimensional problem solving},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on knowledge-enhanced multimodal learning.
<em>AIR</em>, <em>57</em>(10), 1–80. (<a
href="https://doi.org/10.1007/s10462-024-10825-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. At the same time, knowledge graphs enhance explainability, fairness and validity of decision making, issues of outermost importance for such complex implementations. The current survey aims to unify the fields of VL representation learning and knowledge graphs, and provides a taxonomy and analysis of knowledge-enhanced VL models.},
  archive      = {J_AIR},
  author       = {Lymperaiou, Maria and Stamou, Giorgos},
  doi          = {10.1007/s10462-024-10825-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on knowledge-enhanced multimodal learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systematic exploration and in-depth analysis of ChatGPT
architectures progression. <em>AIR</em>, <em>57</em>(10), 1–24. (<a
href="https://doi.org/10.1007/s10462-024-10832-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast evolution of artificial intelligence frameworks has resulted in the creation of increasingly sophisticated large language models (LLM), ChatGPT being the most famous one. This study paper dives into this LLM with a case study of ChatGPT’s architecture and provides a thorough comparative analysis of its numerous versions, tracking its history from its conception to its most recent incarnations. This research intends to give a full knowledge of the model’s history by investigating the underlying mechanisms and enhancements provided in each edition. The comparative analysis covers key aspects such as model size, training data, fine-tuning techniques, and performance metrics. Furthermore, this study evaluates the limits of ChatGPT in its many incarnations. These limitations include common sense reasoning difficulties, biased replies, verbosity, sensitivity to input wording, and others. Each constraint is investigated for potential remedies and workarounds. This research article also provides a complete analysis of the ChatGPT architecture and its progress through multiple iterations. It gives vital insights for academics, developers, and users wanting to harness the promise of ChatGPT while managing its restrictions by exploring both the model’s strengths and limitations. The distinctiveness of this paper rests in its comprehensive assessment of ChatGPT’s architectural development and its practical strategy for resolving the myriad difficulties in producing cohesive and contextually relevant replies.},
  archive      = {J_AIR},
  author       = {Banik, Debajyoty and Pati, Natasha and Sharma, Atul},
  doi          = {10.1007/s10462-024-10832-0},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Systematic exploration and in-depth analysis of ChatGPT architectures progression},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New covering techniques and applications utilizing
multigranulation fuzzy rough sets. <em>AIR</em>, <em>57</em>(10), 1–44.
(<a href="https://doi.org/10.1007/s10462-024-10860-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to conduct an in-depth study of Zhan’s methodology pertaining to the covering of multigranulation fuzzy rough sets ( $$\hbox {C}_{{MG}}$$ FRSs), we build two families: the family of fuzzy $$\beta $$ -minimum descriptions and the family of $$\beta $$ -maximum descriptions. Subsequently, utilizing these notions, we proceed to develop two variations of covering via optimistic (pessimistic) multigranuation rough set samples ( $$\hbox {CO(P)}_{{MG}}$$ FRS). The axiomatic properties are examined. In this study, we examine four models of covering using variable precision multigranulation fuzzy rough sets ( $$\hbox {CVP}_{{MG}}$$ FRSs). We proceed with analyzing the features of these models. Interconnections between these planned plans are also elucidated. This study explores algorithms that aim to identify innovative strategies for addressing multiattribute group decision-making problems (MAGDM) and multicriteria group decision-making problems (MCGDM). The test examples have been elucidated to provide an inclusive grasp of the efficacy of the offered samples. Ultimately, the distinctions between our methodologies and the preexisting research have been demonstrated.},
  archive      = {J_AIR},
  author       = {Atef, Mohammed and Liu, Sifeng and Moslem, Sarbast and Pamucar, Dragan},
  doi          = {10.1007/s10462-024-10860-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {New covering techniques and applications utilizing multigranulation fuzzy rough sets},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human–computer interaction using artificial
intelligence-based expert prioritization and neuro quantum fuzzy picture
rough sets for identity management choices of non-fungible tokens in the
metaverse. <em>AIR</em>, <em>57</em>(10), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10875-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Necessary improvements should be made to increase the effectiveness of non-fungible tokens on the Metaverse platform without having extra costs. For the purpose of handing this process more efficiently, there is a need to determine the most important factors for a more successful integration of non-fungible tokens into this platform. Accordingly, this study aims to determine the appropriate the identity management choices of non-fungible tokens in the Metaverse. There are three different stages in the proposed novel fuzzy decision-making model. The first stage includes prioritizing the expert choices with artificial intelligence-based decision-making methodology. Secondly, the criteria sets for managing non-fungible tokens are weighted by using Quantum picture fuzzy rough sets-based M-SWARA methodology. Finally, the identity management choices regarding non-fungible tokens in the Metaverse are ranked with Quantum picture fuzzy rough sets oriented VIKOR. The main contribution of this study is that artificial intelligence methodology is integrated to the fuzzy decision-making modelling to differentiate the experts. With the help of this situation, it can be possible to create clusters for the experts. Hence, the opinions of experts outside this group may be excluded from the scope. It has been determined that security must be ensured first to increase the use of non-fungible tokens on the Metaverse platform. Similarly, technological infrastructure must also be sufficient to achieve this objective. Moreover, biometrics for unique identification has the best ranking performance among the alternatives. Privacy with authentication plays also critical role for the effectiveness of this process.},
  archive      = {J_AIR},
  author       = {Kou, Gang and Dinçer, Hasan and Pamucar, Dragan and Yüksel, Serhat and Deveci, Muhammet and Olaru, Gabriela Oana and Eti, Serkan},
  doi          = {10.1007/s10462-024-10875-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Human–computer interaction using artificial intelligence-based expert prioritization and neuro quantum fuzzy picture rough sets for identity management choices of non-fungible tokens in the metaverse},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSP: Self-supervised pertaining technique for classification
of shoulder implants in x-ray medical images: A broad experimental
study. <em>AIR</em>, <em>57</em>(10), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10878-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple pathologic conditions can lead to a diseased and symptomatic glenohumeral joint for which total shoulder arthroplasty (TSA) replacement may be indicated. The long-term survival of implants is limited. With the increasing incidence of joint replacement surgery, it can be anticipated that joint replacement revision surgery will become more common. It can be challenging at times to retrieve the manufacturer of the in situ implant. Therefore, certain systems facilitated by AI techniques such as deep learning (DL) can help correctly identify the implanted prosthesis. Correct identification of implants in revision surgery can help reduce perioperative complications and complications. DL was used in this study to categorise different implants based on X-ray images into four classes (as a first case study of the small dataset): Cofield, Depuy, Tornier, and Zimmer. Imbalanced and small public datasets for shoulder implants can lead to poor performance of DL model training. Most of the methods in the literature have adopted the idea of transfer learning (TL) from ImageNet models. This type of TL has been proven ineffective due to some concerns regarding the contrast between features learnt from natural images (ImageNet: colour images) and shoulder implants in X-ray images (greyscale images). To address that, a new TL approach (self-supervised pertaining (SSP)) is proposed to resolve the issue of small datasets. The SSP approach is based on training the DL models (ImageNet models) on a large number of unlabelled greyscale medical images in the domain to update the features. The models are then trained on a small labelled data set of X-ray images of shoulder implants. The SSP shows excellent results in five ImageNet models, including MobilNetV2, DarkNet19, Xception, InceptionResNetV2, and EfficientNet with precision of 96.69%, 95.45%, 98.76%, 98.35%, and 96.6%, respectively. Furthermore, it has been shown that different domains of TL (such as ImageNet) do not significantly affect the performance of shoulder implants in X-ray images. A lightweight model trained from scratch achieves 96.6% accuracy, which is similar to using standard ImageNet models. The features extracted by the DL models are used to train several ML classifiers that show outstanding performance by obtaining an accuracy of 99.20% with Xception+SVM. Finally, extended experimentation has been carried out to elucidate our approach’s real effectiveness in dealing with different medical imaging scenarios. Specifically, five different datasets are trained and tested with and without the proposed SSP, including the shoulder X-ray with an accuracy of 99.47% and CT brain stroke with an accuracy of 98.60%.},
  archive      = {J_AIR},
  author       = {Alzubaidi, Laith and Fadhel, Mohammed A. and Hollman, Freek and Salhi, Asma and Santamaria, Jose and Duan, Ye and Gupta, Ashish and Cutbush, Kenneth and Abbosh, Amin and Gu, Yuantong},
  doi          = {10.1007/s10462-024-10878-0},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SSP: self-supervised pertaining technique for classification of shoulder implants in x-ray medical images: a broad experimental study},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating the metaverse: Unraveling the impact of
artificial intelligence—a comprehensive review and gap analysis.
<em>AIR</em>, <em>57</em>(10), 1–58. (<a
href="https://doi.org/10.1007/s10462-024-10881-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the burgeoning interest in the Metaverse—a virtual reality-driven immersive digital world—this study delves into the pivotal role of AI in shaping its functionalities and elevating user engagement. Focused on recent advancements, prevailing challenges, and potential future developments, our research draws from a comprehensive analysis grounded in meticulous methodology. The study, informed by credible sources including SD, Scopus, IEEE, and WoS, encompasses 846 retrieved studies. Through a rigorous selection process, 54 research papers were identified as relevant, forming the basis for a specific taxonomy of AI in the Metaverse. Our examination spans diverse dimensions of the Metaverse, encompassing augmented reality, virtual reality, mixed reality, Blockchain, Agent Systems, Intelligent NPCs, Societal and Educational Impact, HCI and Systems Design, and Technical Aspects. Emphasizing the necessity of adopting trustworthy AI in the Metaverse, our findings underscore its potential to enhance user experience, safeguard privacy, and promote responsible technology use. This paper not only sheds light on the scholarly interest in the Metaverse but also explores its impact on human behavior, education, societal norms, and community dynamics. Serving as a foundation for future development and responsible implementation of the Metaverse concept, our research identifies and addresses seven open issues, providing indispensable insights for subsequent studies on the integration of AI in the Metaverse.},
  archive      = {J_AIR},
  author       = {Fadhel, Mohammed A. and Duhaim, Ali M. and Albahri, A. S. and Al-Qaysi, Z. T. and Aktham, M. A. and Chyad, M. A. and Abd-Alaziz, Wael and Albahri, O. S. and Alamoodi, A.H. and Alzubaidi, Laith and Gupta, Ashish and Gu, Yuantong},
  doi          = {10.1007/s10462-024-10881-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Navigating the metaverse: Unraveling the impact of artificial intelligence—a comprehensive review and gap analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facial image analysis for automated suicide risk detection
with deep neural networks. <em>AIR</em>, <em>57</em>(10), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10882-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately assessing suicide risk is a critical concern in mental health care. Traditional methods, which often rely on self-reporting and clinical interviews, are limited by their subjective nature and may overlook non-verbal cues. This study introduces an innovative approach to suicide risk assessment using facial image analysis. The Suicidal Visual Indicators Prediction (SVIP) Framework leverages EfficientNetb0 and ResNet architectures, enhanced through Bayesian optimization techniques, to detect nuanced facial expressions indicating mental state. The models’ interpretability is improved using GRADCAM, Occlusion Sensitivity, and LIME, which highlight significant facial regions for predictions. Using datasets DB1 and DB2, which consist of full and cropped facial images from social media profiles of individuals with known suicide outcomes, the method achieved 67.93% accuracy with EfficientNetb0 on DB1 and up to 76.6% accuracy with a Bayesian-optimized Support Vector Machine model using ResNet18 features on DB2. This approach provides a less intrusive, accessible alternative to video-based methods and demonstrates the substantial potential for early detection and intervention in mental health care.},
  archive      = {J_AIR},
  author       = {Rashed, Amr E. Eldin and Atwa, Ahmed E. Mansour and Ahmed, Ali and Badawy, Mahmoud and Elhosseini, Mostafa A. and Bahgat, Waleed M.},
  doi          = {10.1007/s10462-024-10882-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Facial image analysis for automated suicide risk detection with deep neural networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling imbalanced medical datasets: Review of a decade of
research. <em>AIR</em>, <em>57</em>(10), 1–57. (<a
href="https://doi.org/10.1007/s10462-024-10884-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and medical diagnostic studies often struggle with the issue of class imbalance in medical datasets, complicating accurate disease prediction and undermining diagnostic tools. Despite ongoing research efforts, specific characteristics of medical data frequently remain overlooked. This article comprehensively reviews advances in addressing imbalanced medical datasets over the past decade, offering a novel classification of approaches into preprocessing, learning levels, and combined techniques. We present a detailed evaluation of the medical datasets and metrics used, synthesizing the outcomes of previous research to reflect on the effectiveness of the methodologies despite methodological constraints. Our review identifies key research trends and offers speculative insights and research trajectories to enhance diagnostic performance. Additionally, we establish a consensus on best practices to mitigate persistent methodological issues, assisting the development of generalizable, reliable, and consistent results in medical diagnostics.},
  archive      = {J_AIR},
  author       = {Salmi, Mabrouka and Atif, Dalia and Oliva, Diego and Abraham, Ajith and Ventura, Sebastian},
  doi          = {10.1007/s10462-024-10884-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Handling imbalanced medical datasets: Review of a decade of research},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive review of tubule formation in histopathology
images: Advancement in tubule and tumor detection techniques.
<em>AIR</em>, <em>57</em>(10), 1–116. (<a
href="https://doi.org/10.1007/s10462-024-10887-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer, the earliest documented cancer in history, stands as a foremost cause of mortality, accounting for 684,996 deaths globally in 2020 (15.5% of all female cancer cases). Irrespective of socioeconomic factors, geographic locations, race, or ethnicity, breast cancer ranks as the most frequently diagnosed cancer in women. The standard grading for breast cancer utilizes the Nottingham Histopathology Grading (NHG) system, which considers three crucial features: mitotic counts, nuclear pleomorphism, and tubule formation. Comprehensive reviews on features, for example, mitotic count and nuclear pleomorphism have been available thus far. Nevertheless, a thorough investigation specifically focusing on tubule formation aligned with the NHG system is currently lacking. Motivated by this gap, the present study aims to unravel tubule formation in histopathology images via a comprehensive review of detection approaches involving tubule and tumor features. Without temporal constraints, a structured methodology is established in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, resulting in 12 articles for tubule detection and 67 included articles for tumor detection. Despite the primary focus on breast cancer, the structured search string extends beyond this domain to encompass any cancer type utilizing histopathology images as input, focusing on tubule and tumor detection. This broadened scope is essential. Insights from approaches in tubule and tumor detection for various cancers can be assimilated, integrated, and contributed to an enhanced understanding of tubule formation in breast histopathology images. This study compiles evidence-based analyses into a cohesive document, offering comprehensive information to a diverse audience, including newcomers, experienced researchers, and stakeholders interested in the subject matter.},
  archive      = {J_AIR},
  author       = {Siet, Joseph Jiun Wen and Tan, Xiao Jian and Cheor, Wai Loon and Ab Rahman, Khairul Shakir and Cheng, Ee Meng and Wan Muhamad, Wan Zuki Azman and Yip, Sook Yee},
  doi          = {10.1007/s10462-024-10887-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-116},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of tubule formation in histopathology images: Advancement in tubule and tumor detection techniques},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models (LLMs): Survey, technical frameworks,
and future challenges. <em>AIR</em>, <em>57</em>(10), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10888-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has significantly impacted various fields. Large language models (LLMs) like GPT-4, BARD, PaLM, Megatron-Turing NLG, Jurassic-1 Jumbo etc., have contributed to our understanding and application of AI in these domains, along with natural language processing (NLP) techniques. This work provides a comprehensive overview of LLMs in the context of language modeling, word embeddings, and deep learning. It examines the application of LLMs in diverse fields including text generation, vision-language models, personalized learning, biomedicine, and code generation. The paper offers a detailed introduction and background on LLMs, facilitating a clear understanding of their fundamental ideas and concepts. Key language modeling architectures are also discussed, alongside a survey of recent works employing LLM methods for various downstream tasks across different domains. Additionally, it assesses the limitations of current approaches and highlights the need for new methodologies and potential directions for significant advancements in this field.},
  archive      = {J_AIR},
  author       = {Kumar, Pranjal},
  doi          = {10.1007/s10462-024-10888-y},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Large language models (LLMs): Survey, technical frameworks, and future challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). The interactive fusion of characters and lexical
information for chinese named entity recognition. <em>AIR</em>,
<em>57</em>(10), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10891-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have demonstrated that incorporating lexical information into characters can effectively improve the performance of Chinese Named Entity Recognition (CNER). However, we argue that previous studies have not extensively explored the interactive relationship between characters and lexical information, and have only used the lexical information to enhance character-level representation. To address this limitation, we propose an interactive fusion approach that integrates characters and lexical information for CNER. Specifically, we first design graph attention networks to initially fuse character and lexical information within an interactive graph structure. Additionally, by introducing methods such as feedforward neural networks, residual connections, and layer normalization, the fusion effect of the graph attention network is further enhanced. Finally, concatenating and reducing dimensionality of character feature vectors and lexical feature vectors to achieve secondary fusion, thereby obtaining a more comprehensive feature representation. Experimental results on multiple datasets demonstrate that our proposed model outperforms other models that fuse lexical information. Particularly, on the CCKS2020 and Ontonotes datasets, our model achieves higher F1 scores than previous state-of-the-art models. The code is available via the link: https://github.com/wangye0523/The-interactive-fusion-of-characters-and-lexical-information-for-Chinese-named-entity-recognition .},
  archive      = {J_AIR},
  author       = {Wang, Ye and Wang, Zheng and Yu, Hong and Wang, Guoyin and Lei, Dajiang},
  doi          = {10.1007/s10462-024-10891-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The interactive fusion of characters and lexical information for chinese named entity recognition},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent characteristic objects method (INCOME): A data
knowledge-based multi-criteria decision analysis. <em>AIR</em>,
<em>57</em>(10), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-10892-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-criteria decision analysis (MCDA) methods are vital in assessing decision variants under multiple conditions. However, involving domain experts in developing decision models can be challenging and costly, necessitating more scalable and independent solutions. This paper introduces the intelligent characteristic objects method (INCOME), which combines the k-Nearest Neighbor (kNN) algorithm and the COMET method to create a theoretical decision-maker for comparing characteristic objects (COs). INCOME overcomes limitations of classical MCDA methods, such as the TOPSIS approach, which struggles with complex functions and non-monotonic modeling. INCOME influences data-based knowledge to provide a robust framework for assessing decision options. The integration of the COMET method and kNN algorithm enables improved modeling of decision functions based on evaluated data, increasing the flexibility and independence of the INCOME approach. A case study assessing gas power plants based on four criteria is presented to validate the performance of the INCOME method. The results demonstrate high correlations with the reference model and slightly higher classical approaches like TOPSIS and TOPSIS-COMET. However, INCOME exhibits greater stability and flexibility by utilizing all available data instead of relying on limited expert knowledge. The proposed INCOME approach offers several advantages, including creating a continuous decision model, resistance to the Rank-Reversal phenomenon, and the potential for replacing domain experts with artificial experts. This study highlights the effectiveness of INCOME in Multi-Criteria Decision Analysis. It suggests future research directions, such as parameter selection and testing in different decision-making problems.},
  archive      = {J_AIR},
  author       = {Kizielewicz, Bartłomiej and Shekhovtsov, Andrii and Więckowski, Jakub and Wątróbski, Jarosław and Sałabun, Wojciech},
  doi          = {10.1007/s10462-024-10892-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent characteristic objects method (INCOME): A data knowledge-based multi-criteria decision analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based authentication for insider threat
detection in critical infrastructure. <em>AIR</em>, <em>57</em>(10),
1–35. (<a href="https://doi.org/10.1007/s10462-024-10893-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s cyber environment, threats such as data breaches, cyberattacks, and unauthorized access threaten national security, critical infrastructure, and financial stability. This research addresses the challenging task of protecting critical infrastructure from insider threats because of the high level of trust and access these individuals typically receive. Insiders may obtain a system administrator’s password through close observation or by deploying software to gather the information. To solve this issue, an innovative artificial intelligence-based methodology is proposed to identify a user by their password’s keystroke dynamics. This paper also introduces a new Gabor Filter Matrix Transformation method to transform numerical values into images by revealing the behavioral pattern of password typing. A siamese neural network (SNN) with the branches of convolutional neural networks is utilized for image comparison, aiming to detect unauthorized attempts to access critical infrastructure systems. The network analyzes the unique features of a user’s password timestamps transformed into images and compares them with previously submitted user passwords. The obtained results indicate that transforming the numerical values of keystroke dynamics into images and training an SNN leads to a lower equal error rate (EER) and higher user authentication accuracy than those previously reported in other studies. The methodology is validated on publicly available keystroke dynamics collections, the CMU and GREYC-NISLAB datasets, which collectively comprise over 30,000 password samples. It achieves the lowest EER value of 0.04545 compared to state-of-the-art methods for transforming non-image data into images. The paper concludes with a discussion of findings and potential future directions.},
  archive      = {J_AIR},
  author       = {Budžys, Arnoldas and Kurasova, Olga and Medvedev, Viktor},
  doi          = {10.1007/s10462-024-10893-1},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning-based authentication for insider threat detection in critical infrastructure},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From rule-based models to deep learning transformers
architectures for natural language processing and sign language
translation systems: Survey, taxonomy and performance evaluation.
<em>AIR</em>, <em>57</em>(10), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10895-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing Deaf and Hard of Hearing population worldwide and the persistent shortage of certified sign language interpreters, there is a pressing need for an efficient, signs-driven, integrated end-to-end translation system, from sign to gloss to text and vice-versa. There has been a wealth of research on machine translations and related reviews. However, there are few works on sign language machine translation considering the particularity of the language being continuous and dynamic. This paper aims to address this void, providing a retrospective analysis of the temporal evolution of sign language machine translation algorithms and a taxonomy of the Transformers architectures, the most used approach in language translation. We also present the requirements of a real-time Quality-of-Service sign language machine translation system underpinned by accurate deep learning algorithms. We propose future research directions for sign language translation systems.},
  archive      = {J_AIR},
  author       = {Shahin, Nada and Ismail, Leila},
  doi          = {10.1007/s10462-024-10895-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {From rule-based models to deep learning transformers architectures for natural language processing and sign language translation systems: Survey, taxonomy and performance evaluation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of artificial intelligence in coal mine
ultra-deep roadway engineering—a review. <em>AIR</em>, <em>57</em>(10),
1–56. (<a href="https://doi.org/10.1007/s10462-024-10898-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep integration of computer field and coal mining field is the only way to coal mine intellectualization. A variety of artificial intelligence tools have been applied in open-pit and shallow coal mines. However, with the geometric increase of coal demand, the contradiction between supply and demand is becoming more and more serious, and the exploitation of resources from shallow layer (&gt; 600 m) has become an inevitable trend. Well then, as a new engineering scene, the harsh conditions of “three high and one disturbance” seriously threaten the safety of personnel. The superposition of complex mining environment makes the number of input factors increase sharply, which leads to the application of artificial intelligence in deep coal mine roadway engineering. The guidance is not mature, the construction of various databases is missing, and there are still some problems in universality and applicability. To this end, this paper starts with the introduction of the operating characteristics of various artificial intelligence tools, and conducts a comprehensive study of the relevant high-level articles published in top journals. It systematically sorts out the research progress that has successfully solved the five directions of rock mechanics strength, surrounding rock stability, rock-burst, roof fall risks and micro-seismic events. While objectively evaluating the comprehensive performance of different tools, it also expounds its own views on key research and results. Literature review shows that whether as a development tool or a comparative model, the application of ANN in the field of coal mining is more than 98%, and it performs extremely well in the direction of surrounding rock stability and roof fall risk, with an accuracy rate of more than 90%. As the most mature direction of AI tools application, rock mechanical strength has experienced the development process from “SVM → ANN → DL → XGBoost → RF”. The dataset is from small samples (&lt; 100) to big samples (&gt; 1000), and the R2 of tree-based models can be stabilized at more than 95%. The research on rock-burst prediction mainly focuses on field factors and micro-seismic monitoring data. Whether it is a small sample or a large-scale data model, the accuracy of BN remains above 85%. The prediction and evaluation of micro-seismic events is a new direction in recent years. The image processing and application of CNN is extremely important. The signal recognition and classification accounts for more than 90%, and the research potential of source location needs to be further explored. In general, the nature of the rock itself is the first choice for almost all influencing factors. At the same time, the update iteration of monitoring methods (micro-seismic, ground sound, separation, deformation, etc.) expands the development of the database, making it possible to obtain the data due to threat to life and cost of equipment, which is very difficult to obtain before. In the process of parameter selection at the input end, the method of combining lithology conditions, geological environment and monitoring data will gradually become the first choice for research. Finally, in the follow-up work collation and on-the-spot investigation, it mainly focuses on the existing problems of deep coal mines, explores the application potential of artificial intelligence in deep coal mine roadway engineering, puts forward the possible research focus and challenging problems in the future, and gives its own opinions.},
  archive      = {J_AIR},
  author       = {Yu, Bingbing and Wang, Bo and Zhang, Yuantong},
  doi          = {10.1007/s10462-024-10898-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence in coal mine ultra-deep roadway engineering—a review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive review of deep learning-based models for
heart disease prediction. <em>AIR</em>, <em>57</em>(10), 1–50. (<a
href="https://doi.org/10.1007/s10462-024-10899-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease (HD) is one of the leading causes of death in humans, posing a heavy burden on society, families, and patients. Real-time prediction of HD can reduce mortality rates and is crucial for timely intervention and treatment of HD. Deep learning (DL)-related methods have higher accuracy and real-time performance in predicting HD. In this study, we comprehensively compared and evaluated the contributions and limitations of DL algorithms, extended deep learning (ETDL) algorithms, and integrated deep learning (integrated DL) algorithms that combine DL with other technologies for predicting HD. The articles considered span the period from 2018 to 2023, and after rigorous screening, 64 articles were selected for preliminary research. A systematic literature review of real-time HDP will provide future researchers with a comprehensive understanding of existing deep learning methods and related integrated technologies in the healthcare industry. Furthermore, it discusses popular datasets employed in deploying numerous prediction models. Additionally, it reveals existing open problems or challenges encountered by previous researchers. Notably, the most prevalent challenge is the scarcity of large discrete datasets, followed by the need for further improvement of existing models.},
  archive      = {J_AIR},
  author       = {Zhou, Chunjie and Dai, Pengfei and Hou, Aihua and Zhang, Zhenxing and Liu, Li and Li, Ali and Wang, Fusheng},
  doi          = {10.1007/s10462-024-10899-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of deep learning-based models for heart disease prediction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multi-criteria decision making for electric
vehicles: A hybrid approach based on RANCOM and ESP-SPOTIS.
<em>AIR</em>, <em>57</em>(10), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10901-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s globalized technological area, aligning decisions with customer preferences is crucial yet challenging due to the complexities and uncertainties involved. Multi-Criteria Decision Analysis (MCDA) serves as a vital tool for constructing support systems that cater to customer-centric trends. While existing MCDA methods vary in their calculation concepts, some prioritize ideal solutions, while others accommodate personalized preferences within dynamic decision contexts. Moreover, determining the relevance of criteria based on expert knowledge adds another layer of personalization to the evaluation process, further individualizing decision-making. However, current decision models often fail to integrate these concepts, leaving a gap in how recommendations can be enhanced when both are combined. To address these challenges, this paper introduces an innovative approach integrating Ranking Comparison and Expected Solution Point Stable Preference Ordering Towards Ideal Solution methods. This hybrid model incorporates personalization into multi-criteria evaluation, catering to individual preferences. By representing customer preferences through two distinct measures, the proposed approach ensures personalized recommendations aligned with decision-makers’ needs. The efficacy of the hybrid model was validated through its application to the electric vehicle selection problem. The verification process highlighted potential disparities compared to other multi-criteria approaches, establishing a consumer preference-based Decision Support System approach for more precise and personalized selection recommendations.},
  archive      = {J_AIR},
  author       = {Więckowski, Jakub and Wątróbski, Jarosław and Shkurina, Anna and Sałabun, Wojciech},
  doi          = {10.1007/s10462-024-10901-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive multi-criteria decision making for electric vehicles: A hybrid approach based on RANCOM and ESP-SPOTIS},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for literature reviews:
Opportunities and challenges. <em>AIR</em>, <em>57</em>(10), 1–49. (<a
href="https://doi.org/10.1007/s10462-024-10902-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
  archive      = {J_AIR},
  author       = {Bolaños, Francisco and Salatino, Angelo and Osborne, Francesco and Motta, Enrico},
  doi          = {10.1007/s10462-024-10902-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence for literature reviews: Opportunities and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrasting linguistic patterns in human and LLM-generated
news text. <em>AIR</em>, <em>57</em>(10), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10903-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct a quantitative analysis contrasting human-written English news text with comparable large language model (LLM) output from six different LLMs that cover three different families and four sizes in total. Our analysis spans several measurable linguistic dimensions, including morphological, syntactic, psychometric, and sociolinguistic aspects. The results reveal various measurable differences between human and AI-generated texts. Human texts exhibit more scattered sentence length distributions, more variety of vocabulary, a distinct use of dependency and constituent types, shorter constituents, and more optimized dependency distances. Humans tend to exhibit stronger negative emotions (such as fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of these models increasing as their size grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns. The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one. Differences between LLMs and humans are larger than between LLMs.},
  archive      = {J_AIR},
  author       = {Muñoz-Ortiz, Alberto and Gómez-Rodríguez, Carlos and Vilares, David},
  doi          = {10.1007/s10462-024-10903-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Contrasting linguistic patterns in human and LLM-generated news text},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence-based expert weighted quantum
picture fuzzy rough sets and recommendation system for metaverse
investment decision-making priorities. <em>AIR</em>, <em>57</em>(10),
1–45. (<a href="https://doi.org/10.1007/s10462-024-10905-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There should be some improvements to increase the performance of Metaverse investments. However, businesses need to focus on the most important actions to provide cost effectiveness in this process. In summary, a new study is needed in which a priority analysis is made for the performance indicators of Metaverse investments. Accordingly, this study aims to evaluate the main determinants of the performance of the metaverse investments. Within this context, a novel model is created that has four different stages. The first stage is related to the prioritizing the experts with artificial intelligence-based decision-making method. Secondly, missing evaluations are estimated by expert recommendation system. Thirdly, the criteria are weighted with Quantum picture fuzzy rough sets-based (QPFR) M-Step-wise Weight Assessment Ratio Analysis (SWARA). Finally, investment decision-making priorities are ranked by QPFR VIKOR (Vlse Kriterijumska Optimizacija Kompromisno Resenje). The main contribution of this study is the integration of the artificial intelligence methodology to the fuzzy decision-making approach for the purpose of computing the weights of the decision makers. Owing to this condition, the evaluations of these people are examined according to their qualifications. This situation has a positive contribution to make more effective evaluations. Organizational effectiveness is found to be the most important factor in improving the performance of metaverse investments. Similarly, it is also identified that it is important for businesses to ensure technological improvements in the development of Metaverse investments. On the other side, the ranking results indicate that regulatory framework is the most critical alternative in this regard.},
  archive      = {J_AIR},
  author       = {Kou, Gang and Dinçer, Hasan and Pamucar, Dragan and Yüksel, Serhat and Deveci, Muhammet and Eti, Serkan},
  doi          = {10.1007/s10462-024-10905-0},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence-based expert weighted quantum picture fuzzy rough sets and recommendation system for metaverse investment decision-making priorities},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Securing data and preserving privacy in cloud IoT-based
technologies an analysis of assessing threats and developing effective
safeguard. <em>AIR</em>, <em>57</em>(10), 1–46. (<a
href="https://doi.org/10.1007/s10462-024-10908-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is a powerful technology adopted in various industries. Applications of the IoT aim to enhance automation, productivity, and user comfort in a cloud and distributive computing environment. Cloud computing automatically stores and analyzes the large amounts of data generated by IoT-based applications. Cloud computing has become a crucial component of the information age through easier data administration and storage. Currently, government agencies, commercial enterprises, and end users are rapidly migrating their data to cloud environments. This may require end-user authentication, greater safety, and data recovery in the event of an attack. A few issues were discovered by authors after analysis and assessments of various aspects of the published research papers. The research analysis reveals that the existing methods need to be further improved to address the contemporary dangers related to data security and privacy. Based on the research reports, it can be stated that safe end-to-end data transmission in a cloud-IoT environment requires modifications and advancements in the design of reliable protocols. Upcoming technologies like blockchain, machine learning, fog, and edge computing mitigate data over the cloud to some level. The study provides a thorough analysis of security threats including their categorization, and potential countermeasures to safeguard our cloud-IoT data. Additionally, the authors have summarized cutting-edge approaches like machine learning and blockchain technologies being used in the data security privacy areas. Further, this paper discusses the existing problems related to data privacy and security in the cloud-IoT environment in today’s world and their possible future directions.},
  archive      = {J_AIR},
  author       = {Pathak, Mayank and Mishra, Kamta Nath and Singh, Satya Prakash},
  doi          = {10.1007/s10462-024-10908-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Securing data and preserving privacy in cloud IoT-based technologies an analysis of assessing threats and developing effective safeguard},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary computation for unmanned aerial vehicle path
planning: A survey. <em>AIR</em>, <em>57</em>(10), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10913-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) path planning aims to find the optimal flight path from the start point to the destination point for each aerial vehicle. With the rapid development of UAV technology, UAVs are required to tackle missions in increasingly complex environments. Consequently, UAV path planning encounters more challenges, causing traditional deterministic algorithms to struggle to find the optimal path within a certain time. Evolutionary computation (EC) is a series of nature-inspired methodologies and algorithms, which have shown effectiveness and efficiency in solving many complex optimization problems in real-world applications. Recently, EC algorithms have been effectively applied in UAV path planning and have shown encouraging performance in obtaining high-quality solutions. Therefore, it is crucial to review the related research experience and literature in the field of using EC for UAV path planning. This paper presents a comprehensive survey to showcase the existing studies on EC in UAV path planning, especially in complex environments. The paper first proposes a novel taxonomy to categorize the relevant studies into three different categories according to the complex environmental properties of the application scenarios. These environmental properties include complex search space, complex time control, and complex optimization objectives. Then, the EC algorithms for UAV path planning in these complex environments are further systematically surveyed as constrained search space and large-scale search space in complex search space, dynamic UAV path planning and multi-UAV concurrent path planning in complex time control, and expensive objective and multiple objectives in complex optimization objectives. Finally, some potential future research directions for applying EC algorithms to UAV path planning are presented and discussed.},
  archive      = {J_AIR},
  author       = {Jiang, Yi and Xu, Xin-Xin and Zheng, Min-Yi and Zhan, Zhi-Hui},
  doi          = {10.1007/s10462-024-10913-0},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evolutionary computation for unmanned aerial vehicle path planning: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of alzheimer’s disease using pre-trained deep
learning models through transfer learning: A review. <em>AIR</em>,
<em>57</em>(10), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10914-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the progress in image processing and Artificial Intelligence (AI), it is now possible to develop automated tool for the early detection and diagnosis of Alzheimer’s Disease (AD). Handcrafted techniques developed so far, lack generality, leading to the development of deep learning (DL) techniques, which can extract more relevant features. To cater for the limited labelled datasets and requirement in terms of high computational power, transfer learning models can be adopted as a baseline. In recent years, considerable research efforts have been devoted to developing machine learning-based techniques for AD detection and classification using medical imaging data. This survey paper comprehensively reviews the existing literature on various methodologies and approaches employed for AD detection and classification, with a focus on neuroimaging techniques such as structural MRI, PET, and fMRI. The main objective of this survey is to analyse the different transfer learning models that can be used for the deployment of deep convolution neural network for AD detection and classification. The phases involved in the development namely image capture, pre-processing, feature extraction and selection are also discussed in the view of shedding light on the different phases and challenges that need to be addressed. The research perspectives may provide research directions on the development of automated applications for AD detection and classification.},
  archive      = {J_AIR},
  author       = {Heenaye-Mamode Khan, Maleika and Reesaul, Pushtika and Auzine, Muhammad Muzzammil and Taylor, Amelia},
  doi          = {10.1007/s10462-024-10914-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Detection of alzheimer’s disease using pre-trained deep learning models through transfer learning: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot adaptation of multi-modal foundation models: A
survey. <em>AIR</em>, <em>57</em>(10), 1–30. (<a
href="https://doi.org/10.1007/s10462-024-10915-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: (1) prompt-based methods, (2) adapter-based methods, and (3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: (1) adaptive domain generalization, (2) adaptive model selection, and (3) adaptive knowledge utilization.Kindly check and confirm the edit made in the title.The title is correct.},
  archive      = {J_AIR},
  author       = {Liu, Fan and Zhang, Tianshu and Dai, Wenwen and Zhang, Chuanyi and Cai, Wenwen and Zhou, Xiaocong and Chen, Delong},
  doi          = {10.1007/s10462-024-10915-y},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Few-shot adaptation of multi-modal foundation models: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring meta-heuristics for partitional clustering:
Methods, metrics, datasets, and challenges. <em>AIR</em>,
<em>57</em>(10), 1–60. (<a
href="https://doi.org/10.1007/s10462-024-10920-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitional clustering is a type of clustering that can organize the data into non-overlapping groups or clusters. This technique has diverse applications across the different various domains like image processing, pattern recognition, data mining, rule-based systems, customer segmentation, image segmentation, and anomaly detection, etc. Hence, this survey aims to identify the key concepts and approaches in partitional clustering. Further, it also highlights its widespread applicability including major advantages and challenges. Partitional clustering faces challenges like selecting the optimal number of clusters, local optima, sensitivity to initial centroids, etc. Therefore, this survey describes the clustering problems as partitional clustering, dynamic clustering, automatic clustering, and fuzzy clustering. The objective of this survey is to identify the meta-heuristic algorithms for the aforementioned clustering. Further, the meta-heuristic algorithms are also categorised into simple meta-heuristic algorithms, improved meta-heuristic algorithms, and hybrid meta-heuristic algorithms. Hence, this work also focuses on the adoption of new meta-heuristic algorithms, improving existing methods and novel techniques that enhance clustering performance and robustness, making partitional clustering a critical tool for data analysis and machine learning. This survey also highlights the different objective functions and benchmark datasets adopted for measuring the effectiveness of clustering algorithms. Before the literature survey, several research questions are formulated to ensure the effectiveness and efficiency of the survey such as what are the various meta-heuristic techniques available for clustering problems? How to handle automatic data clustering? What are the main reasons for hybridizing clustering algorithms? The survey identifies shortcomings associated with existing algorithms and clustering problems and highlights the active area of research in the clustering field to overcome these limitations and improve performance.},
  archive      = {J_AIR},
  author       = {Kaur, Arvinder and Kumar, Yugal and Sidhu, Jagpreet},
  doi          = {10.1007/s10462-024-10920-1},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring meta-heuristics for partitional clustering: Methods, metrics, datasets, and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization through meta-learning: A survey.
<em>AIR</em>, <em>57</em>(10), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10922-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Additionally, we present a decision graph to assist readers in navigating the taxonomy based on data availability and domain shifts, enabling them to select and develop a proper model tailored to their specific problem requirements. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions.},
  archive      = {J_AIR},
  author       = {Khoee, Arsham Gholamzadeh and Yu, Yinan and Feldt, Robert},
  doi          = {10.1007/s10462-024-10922-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Domain generalization through meta-learning: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to learn for few-shot continual active learning.
<em>AIR</em>, <em>57</em>(10), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10924-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in continual learning are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We exploit meta-learning and propose a method, called Meta-Continual Active Learning. This method sequentially queries the most informative examples from a pool of unlabeled data for annotation to enhance task-specific performance and tackles continual learning problems through a meta-objective. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to avoid memory over-fitting caused by experience replay and sample queries, thereby ensuring generalization. We conduct extensive experiments on benchmark text classification datasets from diverse domains to validate the feasibility and effectiveness of meta-continual active learning. We also analyze the impact of different active learning strategies on various meta continual learning models. The experimental results demonstrate that introducing randomness into sample selection is the best default strategy for maintaining generalization in meta-continual learning framework.},
  archive      = {J_AIR},
  author       = {Ho, Stella and Liu, Ming and Gao, Shang and Gao, Longxiang},
  doi          = {10.1007/s10462-024-10924-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learning to learn for few-shot continual active learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic literature review for load balancing and task
scheduling techniques in cloud computing. <em>AIR</em>, <em>57</em>(10),
1–63. (<a href="https://doi.org/10.1007/s10462-024-10925-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is an emerging technology composed of several key components that work together to create a seamless network of interconnected devices. These interconnected devices, such as sensors, routers, smartphones, and smart appliances, are the foundation of the Internet of Everything (IoE). Huge volumes of data generated by IoE devices are processed and accumulated in the cloud, allowing for real-time analysis and insights. As a result, there is a dire need for load-balancing and task-scheduling techniques in cloud computing. The primary objective of these techniques is to divide the workload evenly across all available resources and handle other issues like reducing execution time and response time, increasing throughput and fault detection. This systematic literature review (SLR) aims to analyze various technologies comprising optimization and machine learning algorithms used for load balancing and task-scheduling problems in a cloud computing environment. To analyze the load-balancing patterns and task-scheduling techniques, we opted for a representative set of 63 research articles written in English from 2014 to 2024 that has been selected using suitable exclusion-inclusion criteria. The SLR aims to minimize bias and increase objectivity by designing research questions about the topic. We have focused on the technologies used, the merits-demerits of diverse technologies, gaps within the research, insights into tools, forthcoming opportunities, performance metrics, and an in-depth investigation into ML-based optimization techniques.},
  archive      = {J_AIR},
  author       = {Devi, Nisha and Dalal, Sandeep and Solanki, Kamna and Dalal, Surjeet and Lilhore, Umesh Kumar and Simaiya, Sarita and Nuristani, Nasratullah},
  doi          = {10.1007/s10462-024-10925-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review for load balancing and task scheduling techniques in cloud computing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AL-MobileNet: A novel model for 2D gesture recognition in
intelligent cockpit based on multi-modal data. <em>AIR</em>,
<em>57</em>(10), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10930-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the degree of automotive intelligence increases, gesture recognition is gaining more attention in human-vehicle interaction. However, existing gesture recognition methods are computationally intensive and perform poorly in multi-modal sensor scenarios. This paper proposes a novel network structure, AL-MobileNet (MobileNet with Attention and Lightweight Modules), which can quickly and accurately estimate 2D gestures in RGB and infrared (IR) images. The innovations of this paper are as follows: Firstly, to enhance multi-modal data, we created a synthetic IR dataset based on real 2D gestures and employed a coarse-to-fine training approach. Secondly, to speed up the model&#39;s computation on edge devices, we introduced a new lightweight computational module called the Split Channel Attention Block (SCAB). Thirdly, to ensure the model maintains accuracy in large datasets, we incorporated auxiliary networks and Angle-Weighted Loss (AWL) into the backbone network. Experiments show that AL-MobileNet requires only 0.4 GFLOPs of computational power and 1.2 million parameters. This makes it 1.5 times faster than MobileNet and allows for quick execution on edge devices. AL-MobileNet achieved a running speed of up to 28 FPS on the Ambarella CV28. On both general datasets and our dataset, our algorithm achieved an average PCK0.2 score of 0.95. This indicates that the algorithm can quickly generate accurate 2D gestures. The demonstration of the algorithm can be reviewed in gesturebaolong.},
  archive      = {J_AIR},
  author       = {Wang, Bin and Yu, Liwen and Zhang, Bo},
  doi          = {10.1007/s10462-024-10930-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AL-MobileNet: A novel model for 2D gesture recognition in intelligent cockpit based on multi-modal data},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based drone simulators: Survey,
practice, and challenge. <em>AIR</em>, <em>57</em>(10), 1–47. (<a
href="https://doi.org/10.1007/s10462-024-10933-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, machine learning has been very useful in solving diverse tasks with drones, such as autonomous navigation, visual surveillance, communication, disaster management, and agriculture. Among these machine learning, two representative paradigms have been widely utilized in such applications: supervised learning and reinforcement learning. Researchers prefer to use supervised learning, mostly based on convolutional neural networks, because of its robustness and ease of use but yet data labeling is laborious and time-consuming. On the other hand, when traditional reinforcement learning is combined with the deep neural network, it can be a very powerful tool to solve high-dimensional input problems such as image and video. Along with the fast development of reinforcement learning, many researchers utilize reinforcement learning in drone applications, and it often outperforms supervised learning. However, it usually requires the agent to explore the environment on a trial-and-error basis which is high cost and unrealistic in the real environment. Recent advances in simulated environments can allow an agent to learn by itself to overcome these drawbacks, although the gap between the real environment and the simulator has to be minimized in the end. In this sense, a realistic and reliable simulator is essential for reinforcement learning training. This paper investigates various drone simulators that work with diverse reinforcement learning architectures. The characteristics of the reinforcement learning-based drone simulators are analyzed and compared for the researchers who would like to employ them for their projects. Finally, we shed light on some challenges and potential directions for future drone simulators.},
  archive      = {J_AIR},
  author       = {Chan, Jun Hoong and Liu, Kai and Chen, Yu and Sagar, A. S. M. Sharifuzzaman and Kim, Yong-Guk},
  doi          = {10.1007/s10462-024-10933-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reinforcement learning-based drone simulators: Survey, practice, and challenge},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning and machine learning techniques for head pose
estimation: A survey. <em>AIR</em>, <em>57</em>(10), 1–66. (<a
href="https://doi.org/10.1007/s10462-024-10936-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation (HPE) has been extensively investigated over the past decade due to its wide range of applications across several domains of artificial intelligence (AI), resulting in progressive improvements in accuracy. The problem becomes more challenging when the application requires full-range angles, particularly in unconstrained environments, making HPE an active research topic. This paper presents a comprehensive survey of recent AI-based HPE tasks in digital images. We also propose a novel taxonomy based on the main steps to implement each method, broadly dividing these steps into eleven categories under four groups. Moreover, we provide the pros and cons of ten categories of the overall system. Finally, this survey sheds some light on the public datasets, available codes, and future research directions, aiding readers and aspiring researchers in identifying robust methods that exhibit a strong baseline within the subcategory for further exploration in this fascinating area. The review compared and analyzed 113 articles published between 2018 and 2024, distributing 70.5% deep learning, 24.1% machine learning, and 5.4% hybrid approaches. Furthermore, it included 101 articles related to datasets, definitions, and other elements for AI-based HPE systems published over the last two decades. To the best of our knowledge, this is the first paper that aims to survey HPE strategies based on artificial intelligence, with detailed explanations of the main steps to implement each method. A regularly updated project page is provided: ( github ).},
  archive      = {J_AIR},
  author       = {Algabri, Redhwan and Abdu, Ahmed and Lee, Sungon},
  doi          = {10.1007/s10462-024-10936-7},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning and machine learning techniques for head pose estimation: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advances in remote sensing based soil moisture retrieval:
Applications, techniques, scales and challenges for combining machine
learning and physical models. <em>AIR</em>, <em>57</em>(9), 1–22. (<a
href="https://doi.org/10.1007/s10462-024-10734-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil Moisture (SM) monitoring is crucial for various applications in agriculture, hydrology, and climate science. Remote Sensing (RS) offers a powerful tool for large-scale SM retrieval. This paper explores the advancements in RS techniques for SM estimation. We discuss the applications of these techniques, along with the advantages and limitations of traditional physical models and data-driven Machine Learning (ML) based approaches. The paper emphasizes the potential of combining ML and physical models to leverage the strengths of both approaches. We explore the challenges associated with this integration and future research directions to improve the accuracy, scalability, and robustness of RS-based SM retrieval. Finally, the paper also discusses a few issues such as input data selection, data availability, ML complexity, the need for public datasets for benchmarking, and analysis.},
  archive      = {J_AIR},
  author       = {Abbes, Ali Ben and Jarray, Noureddine and Farah, Imed Riadh},
  doi          = {10.1007/s10462-024-10734-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in remote sensing based soil moisture retrieval: Applications, techniques, scales and challenges for combining machine learning and physical models},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization for semantic segmentation: A survey.
<em>AIR</em>, <em>57</em>(9), 1–30. (<a
href="https://doi.org/10.1007/s10462-024-10817-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have proven explicit contributions in making autonomous driving cars and related tasks such as semantic segmentation, motion tracking, object detection, sensor fusion, and planning. However, in challenging situations, DNNs are not generalizable because of the inherent domain shift due to the nature of training under the i.i.d. assumption. The goal of semantic segmentation is to preserve information from a given image into multiple meaningful categories for visual understanding. Particularly for semantic segmentation, pixel-wise annotation is extremely costly and not always feasible. Domain generalization for semantic segmentation aims to learn pixel-level semantic labels from multiple source domains and generalize to predict pixel-level semantic labels on multiple unseen target domains. In this survey, for the first time, we present a comprehensive review of DG for semantic segmentation. we present a comprehensive summary of recent works related to domain generalization in semantic segmentation, which establishes the importance of generalizing to new environments of segmentation models. Although domain adaptation has gained more attention in segmentation tasks than domain generalization, it is still worth unveiling new trends that are adopted from domain generalization methods in semantic segmentation. We cover most of the recent and dominant DG methods in the context of semantic segmentation and also provide some other related applications. We conclude this survey by highlighting the future directions in this area.},
  archive      = {J_AIR},
  author       = {Rafi, Taki Hasan and Mahjabin, Ratul and Ghosh, Emon and Ko, Young-Woong and Lee, Jeong-Gun},
  doi          = {10.1007/s10462-024-10817-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Domain generalization for semantic segmentation: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated multi-attributive border approximation area
comparison (MABAC) method for evaluating resilience and knowledge
sharing of suppliers in pythagorean fuzzy environment. <em>AIR</em>,
<em>57</em>(9), 1–31. (<a
href="https://doi.org/10.1007/s10462-024-10830-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to determine the resilience scores of suppliers by considering the importance of knowledge sharing and incorporating a set of criteria that affect the supply chain’s resilience, along with knowledge sharing. In order to achieve this goal, we propose a new multi-criteria decision-making (MCDM) method named MULTIMABAC to evaluate suppliers’ resilience. This method is based on the multi-attributive border approximation area comparison (MABAC) method. The new method consists of criteria weighting, expert weighting, and supplier evaluation. In the criteria weighting section, we extend the BWM (Best–Worst method) using Pythagorean fuzzy sets (PFSs). The weight of experts is determined by incorporating the average concept into the MABAC method. The resilience score of suppliers is calculated using an extended MABAC method with ideal positive and negative solutions. All indices are aggregated using the ordinal priority approach (OPA) to obtain a unique ranking. Additionally, we extend the MULTIMABAC method using PFSs to address uncertainty in supply chain decision-making. Results and sensitivity analysis demonstrate that the proposed method effectively facilitates knowledge sharing in selecting resilient suppliers. Therefore, organizations can use the proposed method to evaluate and choose resilient suppliers. Moreover, the MULTIMABAC method demonstrates robustness to variations in the weights of its constituent parameters.},
  archive      = {J_AIR},
  author       = {Jafari, Mostafa and Naghdi Khanachah, Shayan},
  doi          = {10.1007/s10462-024-10830-2},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An integrated multi-attributive border approximation area comparison (MABAC) method for evaluating resilience and knowledge sharing of suppliers in pythagorean fuzzy environment},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidence generalization-based discounting method: Assigning
unreliable information to partial ignorance. <em>AIR</em>,
<em>57</em>(9), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-10833-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflict management is an important topic when dealing with unreliable sources information fusion in Dempster–Shafer theory. Discounting unreliable bodies of evidence has proven to be effective to decrease conflict. Based on the generalization of belief functions, a new generalization-based discounting method is proposed. When resolving conflicts with the same degree, our method can realize less information loss in comparison with other discounting methods. By simulating the process of resolving conflicts of randomly generated bodies of evidence, using entropy measurements and binary conflict as evaluation index, we show our method’s rationality and superiority. Finally, the hyperparameters of the conflict metrics are generated and generalization-based discounting is applied to classify real-world datasets. The improved classification performance further illustrates the usefulness of the method.},
  archive      = {J_AIR},
  author       = {Hu, Qiying and Zhou, Qianli and Li, Zhen and Deng, Yong and Cheong, Kang Hao},
  doi          = {10.1007/s10462-024-10833-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evidence generalization-based discounting method: Assigning unreliable information to partial ignorance},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting named entity recognition in food computing:
Enhancing performance and robustness. <em>AIR</em>, <em>57</em>(9),
1–34. (<a href="https://doi.org/10.1007/s10462-024-10834-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-evolving domain of food computing, named entity recognition (NER) presents transformative potential that extends far beyond mere word tagging in recipes. Its implications encompass intelligent recipe recommendations, health analysis, and personalization. Nevertheless, existing NER models in food computing encounter challenges stemming from variations in recipe input standards, limited annotations, and dataset quality. This article addresses the specific problem of ingredient NER and introduces two innovative models: SINERA, an efficient and robust model, and SINERAS, a semi-supervised variant that leverages a Gaussian Mixture Model (GMM) to learn from untagged ingredient list entries. To mitigate issues associated with data quality and availability in food computing, we introduce the ARTI dataset, a diverse and comprehensive repository of ingredient lines. Additionally, we identify and tackle a pervasive challenge—spurious correlations between entity positions and predictions. To address this, we propose a set of data augmentation rules tailored for food NER. Extensive evaluations conducted on the ARTI dataset and a revised TASTEset dataset underscore the performance of our models. They outperform several state-of-the-art benchmarks and rival the BERT model while maintaining smaller parameter sizes and reduced training times.},
  archive      = {J_AIR},
  author       = {Akujuobi, Uchenna and Liu, Shuhong and Besold, Tarek R.},
  doi          = {10.1007/s10462-024-10834-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting named entity recognition in food computing: Enhancing performance and robustness},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective multi-modal adaptive contextual feature
information fusion method for chinese long text classification.
<em>AIR</em>, <em>57</em>(9), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10835-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese long text classification plays a vital role in Natural Language Processing. Compared to Chinese short texts, Chinese long texts contain more complex semantic feature information. Furthermore, the distribution of these semantic features is uneven due to the varying lengths of the texts. Current research on Chinese long text classification models primarily focuses on enhancing text semantic features and representing Chinese long texts as graph-structured data. Nonetheless, these methods are still susceptible to noise information and tend to overlook the deep semantic information in long texts. To address the above challenges, this study proposes a novel and effective method called MACFM, which introduces a deep feature information mining method and an adaptive modal feature information fusion strategy to learn the semantic features of Chinese long texts thoroughly. First, we present the DCAM module to capture complex semantic features in Chinese long texts, allowing the model to learn detailed high-level representation features. Then, we explore the relationships between word vectors and text graphs, enabling the model to capture abundant semantic information and text positional information from the graph. Finally, we develop the AMFM module to effectively combine different modal feature representations and eliminate the unrelated noise information. The experimental results on five Chinese long text datasets show that our method significantly improves the accuracy of Chinese long text classification tasks. Furthermore, the generalization experiments on five English datasets and the visualized results demonstrate the effectiveness and interpretability of the MACFM model.},
  archive      = {J_AIR},
  author       = {Xu, Yangshuyi and Liu, Guangzhong and Zhang, Lin and Shen, Xiang and Luo, Sizhe},
  doi          = {10.1007/s10462-024-10835-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An effective multi-modal adaptive contextual feature information fusion method for chinese long text classification},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An in-depth survey of the artificial gorilla troops
optimizer: Outcomes, variations, and applications. <em>AIR</em>,
<em>57</em>(9), 1–78. (<a
href="https://doi.org/10.1007/s10462-024-10838-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recently developed algorithm inspired by natural processes, known as the Artificial Gorilla Troops Optimizer (GTO), boasts a straightforward structure, unique stabilizing features, and notably high effectiveness. Its primary objective is to efficiently find solutions for a wide array of challenges, whether they involve constraints or not. The GTO takes its inspiration from the behavior of Gorilla Troops in the natural world. To emulate the impact of gorillas at each stage of the search process, the GTO employs a flexible weighting mechanism rooted in its concept. Its exceptional qualities, including its independence from derivatives, lack of parameters, user-friendliness, adaptability, and simplicity, have resulted in its rapid adoption for addressing various optimization challenges. This review is dedicated to the examination and discussion of the foundational research that forms the basis of the GTO. It delves into the evolution of this algorithm, drawing insights from 112 research studies that highlight its effectiveness. Additionally, it explores proposed enhancements to the GTO’s behavior, with a specific focus on aligning the geometry of the search area with real-world optimization problems. The review also introduces the GTO solver, providing details about its identification and organization, and demonstrates its application in various optimization scenarios. Furthermore, it provides a critical assessment of the convergence behavior while addressing the primary limitation of the GTO. In conclusion, this review summarizes the key findings of the study and suggests potential avenues for future advancements and adaptations related to the GTO.},
  archive      = {J_AIR},
  author       = {Hussien, Abdelazim G. and Bouaouda, Anas and Alzaqebah, Abdullah and Kumar, Sumit and Hu, Gang and Jia, Heming},
  doi          = {10.1007/s10462-024-10838-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-78},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An in-depth survey of the artificial gorilla troops optimizer: Outcomes, variations, and applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way decision in machine learning tasks: A systematic
review. <em>AIR</em>, <em>57</em>(9), 1–56. (<a
href="https://doi.org/10.1007/s10462-024-10845-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we survey the applications of Three-way decision theory (TWD) in machine learning (ML), focusing in particular on four tasks: weakly supervised learning and multi-source data management, missing data management, uncertainty quantification in classification, and uncertainty quantification in clustering. For each of these four tasks we present the results of a systematic review of the literature, by which we report on the main characteristics of the current state of the art, as well as on the quality of reporting and reproducibility level of the works found in the literature. To this aim, we discuss the main benefits, limitations and issues found in the reviewed articles, and we give clear indications and directions for quality improvement that are informed by validation, reporting, and reproducibility standards, guidelines and best practice that have recently emerged in the ML field. Finally, we discuss about the more promising and relevant directions for future research in regard to TWD.},
  archive      = {J_AIR},
  author       = {Campagner, Andrea and Milella, Frida and Ciucci, Davide and Cabitza, Federico},
  doi          = {10.1007/s10462-024-10845-9},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Three-way decision in machine learning tasks: A systematic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models in healthcare: From a systematic
review on medical examinations to a comparative analysis on fundamentals
of robotic surgery online test. <em>AIR</em>, <em>57</em>(9), 1–54. (<a
href="https://doi.org/10.1007/s10462-024-10849-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have the intrinsic potential to acquire medical knowledge. Several studies assessing LLMs on medical examinations have been published. However, there is no reported evidence on tests related to robot-assisted surgery. The aims of this study were to perform the first systematic review of LLMs on medical examinations and to establish whether ChatGPT, GPT-4, and Bard can pass the Fundamentals of Robotic Surgery (FRS) didactic test. A literature search was performed on PubMed, Web of Science, Scopus, and arXiv following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach. A total of 45 studies were analyzed. GPT-4 passed several national qualifying examinations with questions in English, Chinese, and Japanese using zero-shot and few-shot learning. Med-PaLM 2 obtained similar scores on the United States Medical Licensing Examination with more refined prompt engineering techniques. Five different 2023 releases of ChatGPT, one of GPT-4, and one of Bard were tested on FRS. Seven attempts were performed with each release. The pass score was 79.5%. ChatGPT achieved a mean score of 64.6%, 65.6%, 75.0%, 78.9%, and 72.7% respectively from the first to the fifth tested release on FRS vs 91.5% of GPT-4 and 79.5% of Bard. GPT-4 outperformed ChatGPT and Bard in all corresponding attempts with a statistically significant difference for ChatGPT (p &lt; 0.001), but not Bard (p = 0.002). Our findings agree with other studies included in this systematic review. We highlighted the potential and challenges of LLMs to transform the education of healthcare professionals in the different stages of learning, by assisting teachers in the preparation of teaching contents, and trainees in the acquisition of knowledge, up to becoming an assessment framework of leaners.},
  archive      = {J_AIR},
  author       = {Moglia, Andrea and Georgiou, Konstantinos and Cerveri, Pietro and Mainardi, Luca and Satava, Richard M. and Cuschieri, Alfred},
  doi          = {10.1007/s10462-024-10849-5},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Large language models in healthcare: From a systematic review on medical examinations to a comparative analysis on fundamentals of robotic surgery online test},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of evaluation approaches for explainable AI with
applications in cardiology. <em>AIR</em>, <em>57</em>(9), 1–44. (<a
href="https://doi.org/10.1007/s10462-024-10852-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (XAI) elucidates the decision-making process of complex AI models and is important in building trust in model predictions. XAI explanations themselves require evaluation as to accuracy and reasonableness and in the context of use of the underlying AI model. This review details the evaluation of XAI in cardiac AI applications and has found that, of the studies examined, 37% evaluated XAI quality using literature results, 11% used clinicians as domain-experts, 11% used proxies or statistical analysis, with the remaining 43% not assessing the XAI used at all. We aim to inspire additional studies within healthcare, urging researchers not only to apply XAI methods but to systematically assess the resulting explanations, as a step towards developing trustworthy and safe models.},
  archive      = {J_AIR},
  author       = {Salih, Ahmed M. and Galazzo, Ilaria Boscolo and Gkontra, Polyxeni and Rauseo, Elisa and Lee, Aaron Mark and Lekadir, Karim and Radeva, Petia and Petersen, Steffen E. and Menegaz, Gloria},
  doi          = {10.1007/s10462-024-10852-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of evaluation approaches for explainable AI with applications in cardiology},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistage transfer learning for medical images.
<em>AIR</em>, <em>57</em>(9), 1–47. (<a
href="https://doi.org/10.1007/s10462-024-10855-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is revolutionizing various domains and significantly impacting medical image analysis. Despite notable progress, numerous challenges remain, necessitating the refinement of deep learning algorithms for optimal performance in medical image analysis. This paper explores the growing demand for precise and robust medical image analysis by focusing on an advanced deep learning technique, multistage transfer learning. Over the past decade, multistage transfer learning has emerged as a pivotal strategy, particularly in overcoming challenges associated with limited medical data and model generalization. However, the absence of well-compiled literature capturing this development remains a notable gap in the field. This exhaustive investigation endeavors to address this gap by providing a foundational understanding of how multistage transfer learning approaches confront the unique challenges posed by insufficient medical image datasets. The paper offers a detailed analysis of various multistage transfer learning types, architectures, methodologies, and strategies deployed in medical image analysis. Additionally, it delves into intrinsic challenges within this framework, providing a comprehensive overview of the current state while outlining potential directions for advancing methodologies in future research. This paper underscores the transformative potential of multistage transfer learning in medical image analysis, providing valuable guidance to researchers and healthcare professionals.},
  archive      = {J_AIR},
  author       = {Ayana, Gelan and Dese, Kokeb and Abagaro, Ahmed Mohammed and Jeong, Kwangcheol Casey and Yoon, Soon-Do and Choe, Se-woon},
  doi          = {10.1007/s10462-024-10855-7},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multistage transfer learning for medical images},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the data barrier: A review of deep learning
techniques for democratizing AI with small datasets. <em>AIR</em>,
<em>57</em>(9), 1–61. (<a
href="https://doi.org/10.1007/s10462-024-10859-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Justifiably, while big data is the primary interest of research and public discourse, it is essential to acknowledge that small data remains prevalent. The same technological and societal forces that generate big datasets also produce a more significant number of small datasets. Contrary to the notion that more data is inherently superior, real-world constraints such as budget limitations and increased analytical complexity present critical challenges. Quality versus quantity trade-offs necessitate strategic decision-making, where small data often leads to quicker, more accurate, and cost-effective insights. Concentrating AI research, particularly in deep learning (DL), on big datasets exacerbates AI inequality, as tech giants such as Meta, Amazon, Apple, Netflix and Google (MAANG) can easily lead AI research due to their access to vast datasets, creating a barrier for small and mid-sized enterprises that lack similar access. This article addresses this imbalance by exploring DL techniques optimized for small datasets, offering a comprehensive review of historic and state-of-the-art DL models developed specifically for small datasets. This study aims to highlight the feasibility and benefits of these approaches, promoting a more inclusive and equitable AI landscape. Through a PRISMA-based literature search, 175+ relevant articles are identified and subsequently analysed based on various attributes, such as publisher, country, utilization of small dataset technique, dataset size, and performance. This article also delves into current DL models and highlights open research problems, offering recommendations for future investigations. Additionally, the article highlights the importance of developing DL models that effectively utilize small datasets, particularly in domains where data acquisition is difficult and expensive.},
  archive      = {J_AIR},
  author       = {Rather, Ishfaq Hussain and Kumar, Sushil and Gandomi, Amir H.},
  doi          = {10.1007/s10462-024-10859-3},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Breaking the data barrier: A review of deep learning techniques for democratizing AI with small datasets},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Parallel intelligence in three decades: A historical review
and future perspective on ACP and cyber-physical-social systems.
<em>AIR</em>, <em>57</em>(9), 1–35. (<a
href="https://doi.org/10.1007/s10462-024-10861-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in human-in-the-loop or human-centric research have sparked a new wave of scientific exploration. These studies have enhanced the understanding of complex social systems and contributed to more sustainable artificial intelligence (AI) ecosystems. However, the incorporation of human or social factors increases system complexity, making traditional approaches inadequate for managing these complex systems and necessitating a novel operational paradigm. Over decades of work, a mature and comprehensive theory of parallel intelligence (PI) has been established. Rooted in cyber-physical-social systems (CPSS), PI adapts flexibly to various situations within complex systems through the ACP framework (Artificial systems, Computational experiments, and Parallel execution), ensuring system reliability. This paper provides a detailed review and a novel perspective on PI, beginning with the historical and philosophical origins of CPSS and proceeding to present both the fundamental framework and technological implementations of PI. PI-based Industry 5.0 is highlighted, where three pillars are adopted to help realize the supposed vision. Additionally, the paper outlines applications of PI in multiple fields, such as transportation, healthcare, manufacturing, and agriculture, and discusses the opportunities and challenges for imaginative intelligence. The continuous exploration of PI is expected to eventually facilitate the realization of “6S”-based (safe, secure, sustainable, sensitive, service, and smart) parallel ecosystems.},
  archive      = {J_AIR},
  author       = {Wang, Xingxia and Yang, Jing and Liu, Yuhang and Wang, Yutong and Wang, Fei-Yue and Kang, Mengzhen and Tian, Yonglin and Rudas, Imre and Li, Lingxi and Fanti, Maria Pia and Alrifaee, Bassam and Deveci, Muhammet and Mishra, Deepak and Khan, Muhammad Khurram and Chen, Long and Reffye, Philippe De},
  doi          = {10.1007/s10462-024-10861-9},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Parallel intelligence in three decades: A historical review and future perspective on ACP and cyber-physical-social systems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A brief review of hypernetworks in deep learning.
<em>AIR</em>, <em>57</em>(9), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10862-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypernetworks, or hypernets for short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning. Despite their success across different problem settings, there is currently no comprehensive review available to inform researchers about the latest developments and to assist in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example of training deep neural networks using hypernets and propose categorizing hypernets based on five design criteria: inputs, outputs, variability of inputs and outputs, and the architecture of hypernets. We also review applications of hypernets across different deep learning problem settings, followed by a discussion of general scenarios where hypernets can be effectively employed. Finally, we discuss the challenges and future directions that remain underexplored in the field of hypernets. We believe that hypernetworks have the potential to revolutionize the field of deep learning. They offer a new way to design and train neural networks, and they have the potential to improve the performance of deep learning models on a variety of tasks. Through this review, we aim to inspire further advancements in deep learning through hypernetworks.},
  archive      = {J_AIR},
  author       = {Chauhan, Vinod Kumar and Zhou, Jiandong and Lu, Ping and Molaei, Soheila and Clifton, David A.},
  doi          = {10.1007/s10462-024-10862-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A brief review of hypernetworks in deep learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey: Knowledge graph entity alignment research based on
graph embedding. <em>AIR</em>, <em>57</em>(9), 1–58. (<a
href="https://doi.org/10.1007/s10462-024-10866-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment (EA) aims to automatically match entities in different knowledge graphs, which is beneficial to the development of knowledge-driven applications. Representation learning has powerful feature capture capability and it is widely used in the field of natural language processing. Compared with traditional EA methods, EA methods based on representation learning have better performance and efficiency. Hence, we summarize and analyze the representative EA approaches based on representation learning in this paper. We present the problem description and data preprocessing for EA and other related fundamental knowledge. We propose a new EA framework for the latest models, which includes information aggregation module, entity alignment module, and post-alignment module. Based on these three modules, the various technologies are described in detail. In the experimental part, we first explore the effect of EA direction on model performance. Then, we classify the models into different categories in terms of alignment inference strategy, noise filtering strategy, and whether additional information is utilized. To ensure fairness, we perform the comparative analysis of the performance of the models within the categories separately on different datasets. We investigate both unimodal and multimodal EA. Finally, we present future research perspectives based on the shortcomings of existing EA methods.},
  archive      = {J_AIR},
  author       = {Zhu, Beibei and Wang, Ruolin and Wang, Junyi and Shao, Fei and Wang, Kerun},
  doi          = {10.1007/s10462-024-10866-4},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey: Knowledge graph entity alignment research based on graph embedding},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascaded cross-modal transformer for audio–textual
classification. <em>AIR</em>, <em>57</em>(9), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10869-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech classification tasks often require powerful language understanding models to grasp useful features, which becomes problematic when limited training data is available. To attain superior classification performance, we propose to harness the inherent value of multimodal representations by transcribing speech using automatic speech recognition models and translating the transcripts into different languages via pretrained translation models. We thus obtain an audio–textual (multimodal) representation for each data sample. Subsequently, we combine language-specific Bidirectional Encoder Representations from Transformers with Wav2Vec2.0 audio features via a novel cascaded cross-modal transformer (CCMT). Our model is based on two cascaded transformer blocks. The first one combines text-specific features from distinct languages, while the second one combines acoustic features with multilingual features previously learned by the first transformer block. We employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge. CCMT was declared the winning solution, obtaining an unweighted average recall of 65.41% and 85.87% for complaint and request detection, respectively. Moreover, we applied our framework on the Speech Commands v2 and HVB dialog data sets, surpassing previous studies reporting results on these benchmarks. Our code is freely available for download at: https://github.com/ristea/ccmt .},
  archive      = {J_AIR},
  author       = {Ristea, Nicolae-Cătălin and Anghel, Andrei and Ionescu, Radu Tudor},
  doi          = {10.1007/s10462-024-10869-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Cascaded cross-modal transformer for audio–textual classification},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intuitionistic fuzzy three-way decision method based on data
envelopment analysis. <em>AIR</em>, <em>57</em>(9), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10870-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful mathematical tool for tackling uncertain decision problems, three-way decision has garnered substantial attention since its inception. However, real-world decision problems are inherently complex, and decision-makers often exhibit characteristics of incomplete rationality. Traditional three-way decision models, which rely on functions or relationships, face challenges when confronted with multi-output problems. In response to this challenge, this paper introduces an intuitionistic fuzzy three-way decision model grounded in data envelopment analysis (DEA). Initially, we propose an input–output correlation degree that integrates hesitancy information and serves as a procedural indicator for benefit scores. Subsequently, the traditional DEA is extended to accommodate the intuitionistic fuzzy environment and utilized to construct a comprehensive loss function. Furthermore, a novel intuitionistic fuzzy three-way decision model is developed, incorporating three dimensions: optimism, neutrality, and pessimism, and corresponding decision rules and algorithms are provided. Finally, the effectiveness of the proposed model is rigorously validated through a series of experiments and comparative analyses. The model offers a pioneering approach to address uncertain multi-input–output decision problems, effectively integrating decision-maker’s risk preferences within an intuitionistic fuzzy environment.},
  archive      = {J_AIR},
  author       = {Xin, Xian-wei and Yu, Xiao and Li, Tao and Ma, Yuan-yuan and Xue, Zhan-ao and Wang, Chen-yang},
  doi          = {10.1007/s10462-024-10870-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intuitionistic fuzzy three-way decision method based on data envelopment analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic survey of fuzzy deep learning for uncertain
medical data. <em>AIR</em>, <em>57</em>(9), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10871-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent medical industry is in a rapid stage of development around the world, followed by are the expanding market size and basic theories of intelligent medical diagnosis and decision-making. Deep learning models have achieved good practical results in medical domain. However, traditional deep learning is almost calculated and developed by crisp values, while imprecise, uncertain, and vague medical data is common in the process of diagnosis and treatment. It is important and significant to review the contributions of fuzzy deep learning for uncertain medical data, because fuzzy deep learning that originated from fuzzy sets, can effectively deal with uncertain and inaccurate information, providing new viewpoints for alleviating the presence of noise, artifact or high dimensional unstructured information in uncertain medical data. Therefore, taking focus on the intersection of both different fuzzy deep learning models and several types of uncertain medical data, the paper first constructs four types of frameworks of fuzzy deep learning models used for uncertain medical data, and investigates the status from three aspects: fuzzy deep learning models, uncertain medical data and application scenarios. Then the performance evaluation metrics of fuzzy deep learning models are analyzed in details. This work has some original points: (1) four types of frameworks of applying fuzzy deep learning models for uncertain medical data are first proposed. (2) Seven fuzzy deep learning models, five types of uncertain medical data, and five application scenarios are reviewed in details, respectively. (3) The advantages, challenges, and future research directions of fuzzy deep learning for uncertain medical data are critically analyzed, providing valuable suggestions for further deep research.},
  archive      = {J_AIR},
  author       = {Zheng, Yuanhang and Xu, Zeshui and Wu, Tong and Yi, Zhang},
  doi          = {10.1007/s10462-024-10871-7},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic survey of fuzzy deep learning for uncertain medical data},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain, artificial intelligence, and healthcare: The
tripod of future—a narrative review. <em>AIR</em>, <em>57</em>(9),
1–123. (<a href="https://doi.org/10.1007/s10462-024-10873-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of blockchain and artificial intelligence (AI) marks a paradigm shift in healthcare, addressing critical challenges in securing electronic health records (EHRs), ensuring data privacy, and facilitating secure data transmission. This study provides a comprehensive analysis of the adoption of blockchain and AI within healthcare, spotlighting their role in fortifying security and transparency leading the trajectory for a promising future in the realm of healthcare. Our study, employing the PRISMA model, scrutinized 402 relevant articles, employing a narrative analysis to explore the fusion of blockchain and AI in healthcare. The review includes the architecture of AI and blockchain, examines AI applications with and without blockchain integration, and elucidates the interdependency between AI and blockchain. The major findings include: (i) it protects data transfer, and digital records, and provides security; (ii) enhances EHR security and COVID-19 data transmission, thereby bolstering healthcare efficiency and reliability through precise assessment metrics; (iii) addresses challenges like data security, privacy, and decentralized computing, forming a robust tripod. The fusion of blockchain and AI revolutionize healthcare by securing EHRs, and enhancing privacy, and security. Private blockchain adoption reflects the sector’s commitment to data security, leading to improved efficiency and accessibility. This convergence promises enhanced disease identification, response, and overall healthcare efficacy, and addresses key sector challenges. Further exploration of advanced AI features integrated with blockchain promises to enhance outcomes, shaping the future of global healthcare delivery with guaranteed data security, privacy, and innovation.},
  archive      = {J_AIR},
  author       = {Bathula, Archana and Gupta, Suneet K. and Merugu, Suresh and Saba, Luca and Khanna, Narendra N. and Laird, John R. and Sanagala, Skandha S. and Singh, Rajesh and Garg, Deepak and Fouda, Mostafa M. and Suri, Jasjit S.},
  doi          = {10.1007/s10462-024-10873-5},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-123},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Blockchain, artificial intelligence, and healthcare: The tripod of future—a narrative review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI meets physics: A comprehensive survey. <em>AIR</em>,
<em>57</em>(9), 1–85. (<a
href="https://doi.org/10.1007/s10462-024-10874-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncovering the mechanisms of physics is driving a new paradigm in artificial intelligence (AI) discovery. Today, physics has enabled us to understand the AI paradigm in a wide range of matter, energy, and space-time scales through data, knowledge, priors, and laws. At the same time, the AI paradigm also draws on and introduces the knowledge and laws of physics to promote its own development. Then this new paradigm of using physical science to inspire AI is the physical science of artificial intelligence (PhysicsScience4AI, PS4AI). Although AI has become the driving force for development in various fields, there is still a “black box” phenomenon that is difficult to explain in the field of AI deep learning. This article will briefly review the connection between relevant physics disciplines (classical mechanics, electromagnetism, statistical physics, quantum mechanics) and AI. It will focus on discussing the mechanisms of physics disciplines and how they inspire the AI deep learning paradigm, and briefly introduce some related work on how AI solves physics problems. PS4AI is a new research field. At the end of the article, we summarize the challenges facing the new physics-inspired AI paradigm and look forward to the next generation of artificial intelligence technology. This article aims to provide a brief review of research related to physics-inspired AI deep algorithms and to stimulate future research and exploration by elucidating recent advances in physics.},
  archive      = {J_AIR},
  author       = {Jiao, Licheng and Song, Xue and You, Chao and Liu, Xu and Li, Lingling and Chen, Puhua and Tang, Xu and Feng, Zhixi and Liu, Fang and Guo, Yuwei and Yang, Shuyuan and Li, Yangyang and Zhang, Xiangrong and Ma, Wenping and Wang, Shuang and Bai, Jing and Hou, Biao},
  doi          = {10.1007/s10462-024-10874-4},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-85},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI meets physics: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of deep learning models and online healthcare
databases for electronic health records and their use for health
prediction. <em>AIR</em>, <em>57</em>(9), 1–26. (<a
href="https://doi.org/10.1007/s10462-024-10876-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental obstacle to healthcare transformation continues to be the acquisition of knowledge and insightful data from complex, high dimensional, and heterogeneous biological data. As technology has improved, a wide variety of data sources, including omics data, imaging data, and health records, have been available for use in healthcare research contexts. Electronic health records (EHRs), which are digitalized versions of medical records, have given researchers a significant chance to create computational methods for analyzing healthcare data. EHR systems typically keep track of all the data relating to a patient’s medical history, including clinical notes, demographic background, and diagnosis details. EHR data can offer valuable insights and support doctors in making better decisions related to disease and diagnostic forecasts. As a result, several academics use deep learning to forecast diseases and track health trajectories in EHR. Recent advances in deep learning technology have produced innovative and practical paradigms for building end-to-end learning models. However, scholars have limited access to online HER databases, and there is an inherent need to address this issue. This research examines deep learning models, their architectures, and readily accessible EHR online databases. The goal of this paper is to examine how various architectures, models, and databases differ in terms of features and usability. It is anticipated that the outcomes of this review will lead to the development of more robust deep learning models that facilitate medical decision-making processes based on EHR data and inform efforts to support the selection of architectures, models, and databases for specific research purposes.},
  archive      = {J_AIR},
  author       = {Nasarudin, Nurul Athirah and Al Jasmi, Fatma and Sinnott, Richard O. and Zaki, Nazar and Al Ashwal, Hany and Mohamed, Elfadil A. and Mohamad, Mohd Saberi},
  doi          = {10.1007/s10462-024-10876-2},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of deep learning models and online healthcare databases for electronic health records and their use for health prediction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey of deep learning-based lightweight
object detection models for edge devices. <em>AIR</em>, <em>57</em>(9),
1–49. (<a href="https://doi.org/10.1007/s10462-024-10877-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study concentrates on deep learning-based lightweight object detection models on edge devices. Designing such lightweight object recognition models is more difficult than ever due to the growing demand for accurate, quick, and low-latency models for various edge devices. The most recent deep learning-based lightweight object detection methods are comprehensively described in this work. Information on the lightweight backbone architectures used by these object detectors has been listed. The training and inference processes concerning to deep learning applications on edge devices is being discussed. To raise readers’ awareness of this developing domain, a variety of applications for deep learning-based lightweight object detectors and related utilities have been offered. Designing potent, lightweight object detectors based on deep learning has been suggested as a counter to such problems. On well-known datasets such as MS-COCO and PASCAL-VOC, we thoroughly examine the performance of certain conventional deep learning-based lightweight object detectors.},
  archive      = {J_AIR},
  author       = {Mittal, Payal},
  doi          = {10.1007/s10462-024-10877-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of deep learning-based lightweight object detection models for edge devices},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning-based bee recognition and tracking for
advancing insect behavior research. <em>AIR</em>, <em>57</em>(9), 1–22.
(<a href="https://doi.org/10.1007/s10462-024-10879-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of insect behavior, particularly that of honey bees, has a broad scope and significance. Tracking bee flying patterns grants much helpful information about bee behavior. However, tracking a small yet fast-moving object, such as a bee, is difficult. Hence, we present artificial intelligence, machine-learning-based bee recognition, and tracking systems to assist the researcher in studying the bee’s behavior. To develop a machine learning system, a labeled database is required for model training. To address this, we implemented an automated system for analyzing and labeling bee videos. This labeled database served as the foundation for two distinct bee-tracking solutions. The first solution (planar bee tracking system) tracked individual bees in closed mazes using a neural network. The second solution (spatial bee tracking system) utilized a neural network and a tracking algorithm to identify and track flying bees in open environments. Both systems tackle the challenge of tracking small-bodied creatures with rapid and diverse movement patterns. Although we applied these systems to entomological cognition research in this paper, their relevance extends to general insect research and developing tracking solutions for small organisms with swift movements. We present the complete architecture and detailed methodologies to facilitate the utilization of these models in future research endeavors. Our approach is a simple and inexpensive method that contributes to the growing number of image-analysis tools used for tracking animal movement, with future potential applications under less sterile field conditions. The tools presented in this paper could assist the study of movement ecology, specifically in insects, by providing accurate movement specifications. Following the movement of pollinators or natural enemies, for example, greatly contributes to the study of pollination or biological control, respectively, in natural and agro-ecosystems.},
  archive      = {J_AIR},
  author       = {Rozenbaum, Erez and Shrot, Tammar and Daltrophe, Hadassa and Kunya, Yehuda and Shafir, Sharoni},
  doi          = {10.1007/s10462-024-10879-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning-based bee recognition and tracking for advancing insect behavior research},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deciphering arabic question: A dedicated survey on arabic
question analysis methods, challenges, limitations and future pathways.
<em>AIR</em>, <em>57</em>(9), 1–37. (<a
href="https://doi.org/10.1007/s10462-024-10880-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey reviews different research on question analysis, including other comparative studies of question analysis approaches and an evaluation of the questions by different NLP techniques that are used in question interpretation and categorization. Among these key findings noted includes the assessment of deep learning models such as M-BiGRU-CNN and M-TF-IDF, which come with high precision and accuracy when applied with the effectiveness of use in dealing with the complexities involved in a language. Some of the most mature machine learning algorithms, for example, SVM or logistic regression, remain powerful models, especially on the classification task, meaning that the latter continues to be relevant. This study further underlines the applicability of rule-based or hybrid methodologies in certain linguistic situations, and it must be said that custom design solutions are required. We could recommend, on this basis, directing future work towards the integration of these hybrid systems and towards the definition of more general methodologies of evaluation that are in line with the constant evolution of NLP technologies. It revealed that the underlying challenges and barriers in the domain are very complex syntactic and dialectic variations, unavailability of software tools, very critical standardization in Arabic datasets, benchmark creation, handling of translated data, and the integration of Large Language Models (LLMs). The paper discusses the lack of identity and processing of such structures through online systems for comparison. This comprehensive review highlights not only the diversified potential for the capabilities of NLP techniques in refining question analysis but also the potential way of great promises for further enhancements and improvements in this progressive domain.},
  archive      = {J_AIR},
  author       = {Essam, Mariam and Deif, Mohanad A. and Elgohary, Rania},
  doi          = {10.1007/s10462-024-10880-6},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deciphering arabic question: A dedicated survey on arabic question analysis methods, challenges, limitations and future pathways},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-based methods for detecting and classifying age-related
macular degeneration: A comprehensive review. <em>AIR</em>,
<em>57</em>(9), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-10883-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the advancements and achievements of artificial intelligence (AI) in computer vision (CV), particularly in the context of diagnosing and grading age-related macular degeneration (AMD), one of the most common leading causes of blindness and low vision that impact millions of patients globally. Integrating AI in biomedical engineering and healthcare has significantly enhanced the understanding and development of the CV application to mimic human problem-solving abilities. By leveraging AI-based models, ophthalmologists can improve the accuracy and speed of disease diagnosis, enabling early treatment and mitigating the severity of the conditions. This paper presents a comprehensive analysis of many studies on AMD published between 2014 and 2024, with more than 80% published after 2020. Various methodologies and techniques are examined, particularly emphasizing utilizing different retinal imaging modalities like color fundus photography and optical coherence tomography (OCT), where 66% of the studies used OCT datasets. This review aims to compare the efficacy of these AI-based approaches, including machine learning and deep learning, in detecting and diagnosing different stages and grades of AMD based on the evaluation of different performance metrics using different private and public datasets. In addition, this paper introduces some suggested AI solutions for future work.},
  archive      = {J_AIR},
  author       = {El-Den, Niveen Nasr and Elsharkawy, Mohamed and Saleh, Ibrahim and Ghazal, Mohammed and Khalil, Ashraf and Haq, Mohammad Z. and Sewelam, Ashraf and Mahdi, Hani and El-Baz, Ayman},
  doi          = {10.1007/s10462-024-10883-3},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-based methods for detecting and classifying age-related macular degeneration: A comprehensive review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Link prediction for hypothesis generation: An active
curriculum learning infused temporal graph-based approach. <em>AIR</em>,
<em>57</em>(9), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10885-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few years Literature-based Discovery (LBD) has regained popularity as a means to enhance the scientific research process. The resurgent interest has spurred the development of supervised and semi-supervised machine learning models aimed at making previously implicit connections between scientific concepts/entities within often extensive bodies of literature explicit—i.e., suggesting novel scientific hypotheses. In doing so, understanding the temporally evolving interactions between these entities can provide valuable information for predicting the future development of entity relationships. However, existing methods often underutilize the latent information embedded in the temporal aspects of the interaction data. Motivated by applications in the food domain—where we aim to connect nutritional information with health-related benefits—we address the hypothesis-generation problem using a temporal graph-based approach. Given that hypothesis generation involves predicting future (i.e., still to be discovered) entity connections, in our view the ability to capture the dynamic evolution of connections over time is pivotal for a robust model. To address this, we introduce THiGER, a novel batch contrastive temporal node-pair embedding method. THiGER excels in providing a more expressive node-pair encoding by effectively harnessing node-pair relationships. Furthermore, we present THiGER-A, an incremental training approach that incorporates an active curriculum learning strategy to mitigate label bias arising from unobserved connections. By progressively training on increasingly challenging and high-utility samples, our approach significantly enhances the performance of the embedding model. Empirical validation of our proposed method demonstrates its effectiveness on established temporal-graph benchmark datasets, as well as on real-world datasets within the food domain.},
  archive      = {J_AIR},
  author       = {Akujuobi, Uchenna and Kumari, Priyadarshini and Choi, Jihun and Badreddine, Samy and Maruyama, Kana and Palaniappan, Sucheendra K. and Besold, Tarek R.},
  doi          = {10.1007/s10462-024-10885-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Link prediction for hypothesis generation: An active curriculum learning infused temporal graph-based approach},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the applications of neural ordinary differential
equations in medical image analysis. <em>AIR</em>, <em>57</em>(9), 1–32.
(<a href="https://doi.org/10.1007/s10462-024-10894-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image analysis tasks are characterized by high-noise, volumetric, and multi-modality, posing challenges for the model that attempts to learn robust features from the input images. Over the last decade, deep neural networks (DNNs) have achieved enormous success in medical image analysis tasks, which can be attributed to their powerful feature representation capability. Despite the promising results reported in numerous literature, DNNs are also criticized for several pivotal limits, with one of the limitations is lack of safety. Safety plays an important role in the applications of DNNs during clinical practice, helping the model defend against potential attacks and preventing the model from silent failure prediction. The recently proposed neural ordinary differential equation (NODE), a continuous model bridging the gap between DNNs and ODE, provides a significant advantage in ensuring the model’s safety. Among the variants of NODE, the neural memory ordinary differential equation (nmODE) owns the global attractor theoretically, exhibiting superiority in prompting the model’s performance and robustness during applications. While NODE and its variants have been widely used in medical image analysis tasks, there is a lack of a comprehensive review of their applications, hindering the in-depth understanding of NODE’s working principle and its potential applications. To mitigate this limitation, this paper thoroughly reviews the literature on the applications of NODE in medical image analysis from the following five aspects: segmentation, reconstruction, registration, disease prediction, and data generation. We also summarize both the strengths and downsides of the applications of NODE, followed by the possible research directions. To the best of our knowledge, this is the first review regards the applications of NODE in the field of medical image analysis. We hope this review can draw the researchers’ attention to the great potential of NODE and its variants in medical image analysis.},
  archive      = {J_AIR},
  author       = {Niu, Hao and Zhou, Yuxiang and Yan, Xiaohao and Wu, Jun and Shen, Yuncheng and Yi, Zhang and Hu, Junjie},
  doi          = {10.1007/s10462-024-10894-0},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {On the applications of neural ordinary differential equations in medical image analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards trustworthy LLMs: A review on debiasing and
dehallucinating in large language models. <em>AIR</em>, <em>57</em>(9),
1–50. (<a href="https://doi.org/10.1007/s10462-024-10896-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large language models (LLMs) have attracted considerable attention due to their remarkable capabilities. However, LLMs’ generation of biased or hallucinatory content raised significant concerns, posing major challenges for their practical application. Many studies have dedicated efforts to address these critical issues, adopting various approaches to mitigate bias and hallucinations in LLM-generated content. Remarkably, no review papers have synthesized insights on these two primary problems. Addressing this gap, this paper aims to conduct a simultaneous and dual-focused review of the current landscape of research. The discussions encompass widely used and newly proposed benchmarks and evaluation methods on bias and hallucination in LLMs. This paper also investigates advanced mitigation methods and present a taxonomy based on different mitigation strategies. Moreover, a comparative analysis of the sources, mitigation methods, and evaluation methods for bias and hallucination is included. In the end, this paper provides a synthesis of current research trends and suggests potential directions for future research to address bias and hallucination in LLMs, considering the ongoing challenges in this field.},
  archive      = {J_AIR},
  author       = {Lin, Zichao and Guan, Shuyan and Zhang, Wending and Zhang, Huiyan and Li, Yugang and Zhang, Huaping},
  doi          = {10.1007/s10462-024-10896-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Towards trustworthy LLMs: A review on debiasing and dehallucinating in large language models},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAN based augmentation using a hybrid loss function for
dermoscopy images. <em>AIR</em>, <em>57</em>(9), 1–19. (<a
href="https://doi.org/10.1007/s10462-024-10897-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dermatology is the most appropriate field to utilize pattern recognition-based automated techniques for objective, accurate, and rapid diagnosis because diagnosis mainly relies on visual examinations of skin lesions. Recent approaches utilizing deep learning techniques have shown remarkable results in this field. However, they necessitate a substantial quantity of images and the availability of dermoscopy images is often limited. Also, even if enough images are available, their labeling requires expert knowledge and is time-consuming. To overcome these issues, an efficient augmentation approach is needed to expand training datasets from input images. Therefore, in this work, a generative adversarial network has been developed using a new hybrid loss function constructed with traditional loss functions to enhance the generation power of the architecture. Also, the effect of the proposed approach and different generative network-based augmentations, which have been used with dermoscopy images in the literature, on the classification of skin lesions has been investigated. Therefore, the main contributions of this work are: (i) introducing a new generative model for the augmentation of dermoscopy images; (ii) presenting the effect of the proposed model on the classification of the images; (iii) comparative evaluations of the effectiveness of different generative network-based augmentations in the classification of seven forms of skin lesions. The classification accuracy when the proposed augmentation is used is 93.12%, which is higher than its counterparts. Experimental results indicate the significance of augmentation techniques in the classification of skin lesions and the efficiency of the proposed structure in improving the classification accuracy.},
  archive      = {J_AIR},
  author       = {Goceri, Evgin},
  doi          = {10.1007/s10462-024-10897-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {GAN based augmentation using a hybrid loss function for dermoscopy images},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of the impact of dimensionality reduction and
data splitting on classification performance in credit risk assessment.
<em>AIR</em>, <em>57</em>(9), 1–23. (<a
href="https://doi.org/10.1007/s10462-024-10904-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment (CRA) plays an important role in credit decision-making process of financial institutions. Today, developing big data analysis and machine learning methods have marked a new era in credit risk estimation. In recent years, using machine learning methods in credit risk estimation has emerged as an alternative method for financial institutions. The past demographic and financial data of the person whose CRA will be performed is important for creating an automatic artificial intelligence credit score prediction model based on machine learning. It is also necessary to use features correctly to create accurate machine learning models. This article aims to investigate the effects of dimensionality reduction and data splitting steps on the performance of classification algorithms widely used in the literature. In our study, dimensionality reduction was performed using Principal Component Analysis (PCA). Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), Naive Bayes (NB) algorithms were chosen for classification. Percentage splitting (PER, 66–34%) and k-fold (k = 10) cross-validation techniques were used when dividing the data set into training and test data. The results obtained were evaluated with accuracy, recall, F1 score, precision, and AUC metrics. German data set was used in this study. The effect of data splitting and dimension reduction on the classification of CRA systems was examined. The highest ACC in PER and CV data splitting was obtained with the RF algorithm. Using data splitting methods and PCA, the highest accuracy was observed with RF and the highest AUC with NB, with 13 PCs in which 80% of the variance was obtained. As a result, the data set consisting of a total of 20 features, expressed by 13 PCs, achieved similar or higher success than the results obtained from the original data set.},
  archive      = {J_AIR},
  author       = {Bulut, Cem and Arslan, Emel},
  doi          = {10.1007/s10462-024-10904-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comparison of the impact of dimensionality reduction and data splitting on classification performance in credit risk assessment},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RPL-based attack detection approaches in IoT networks:
Review and taxonomy. <em>AIR</em>, <em>57</em>(9), 1–56. (<a
href="https://doi.org/10.1007/s10462-024-10907-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Routing Protocol for Low-Power and Lossy Networks (RPL) plays a crucial role in the Internet of Things (IoT) and Wireless Sensor Networks. However, ensuring the RPL protocol’s security is paramount due to its susceptibility to various attacks. These attacks disrupt data transmission and can substantially damage network topology by depleting critical resources. This paper presents a comprehensive survey addressing several key components in response to this challenge. Firstly, it categorizes potential attacks targeting the RPL protocol based on their impact on network performance and explores effective mechanisms to secure the protocol against them. The study identifies the most destructive and problematic threats affecting RPL functionality. Furthermore, it provides valuable insights into the security challenges of the RPL protocol and discusses their real-world implications for deploying and maintaining IoT and sensor networks. To underscore the uniqueness of the survey, we offer a qualitative comparison with other surveys in the same field. While this study acknowledges certain limitations, such as intentionally focusing only on reviewing RPL-specific attacks, it is a valuable reference for future researchers seeking to comprehend and mitigate attacks targeting RPL. It also suggests areas for further research in this domain.},
  archive      = {J_AIR},
  author       = {Alfriehat, Nadia and Anbar, Mohammed and Aladaileh, Mohammed and Hasbullah, Iznan and Shurbaji, Tamarah A. and Karuppayah, Shankar and Almomani, Ammar},
  doi          = {10.1007/s10462-024-10907-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {RPL-based attack detection approaches in IoT networks: Review and taxonomy},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forensic research of satellite images forgery: A
comprehensive survey. <em>AIR</em>, <em>57</em>(9), 1–40. (<a
href="https://doi.org/10.1007/s10462-024-10909-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery is a significant and attractive area in remote sensing applications, widely applied in monitoring, managing, and tracking natural disasters. Due to the proliferation of commercial satellites, there is an increasing availability of high-resolution satellite images. However, the ubiquity of image editing tools and the advancement of image processing technologies have made satellite image forgery relatively easy, allowing for the arbitrary addition, removal, or modification of target objects. In recent years, satellite image forgery has caused significant negative effects and potential threats to the nation, society, and individuals, drawing the attention of many scholars. Although forensics of satellite image tampering is an emerging research field that offers new insights into the field of information security, there has been a scarcity of comprehensive surveys in this area. This paper aims to fill this gap and investigates recent advances in satellite image forensics, focusing on tampering strategies and forensic methodologies. First, we discuss the concept of satellite images, the definition of satellite image forgery from global and local perspectives, and the datasets commonly used for satellite image forensics. We then detail each tampering detection and localization method, including their characteristics, advantages, disadvantages, and performance in detection or localization across various notable datasets. We also compare some representative forensic networks using evaluation metrics and public datasets. Finally, the anticipated future directions for satellite image forgery forensics are discussed.},
  archive      = {J_AIR},
  author       = {Ding, Xiangling and Nie, Yuchen and Yao, Jizhou and Tang, Jia and Lang, Yubo},
  doi          = {10.1007/s10462-024-10909-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Forensic research of satellite images forgery: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep dive into RNA: A systematic literature review on RNA
structure prediction using machine learning methods. <em>AIR</em>,
<em>57</em>(9), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10910-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of non-coding RNAs (ncRNAs) has expanded our comprehension of RNAs’ inherent nature and capabilities. The intricate three-dimensional structures assumed by RNAs dictate their specific functions and molecular interactions. However, the limited number of mapped structures, partly due to experimental constraints of methods such as nuclear magnetic resonance (NMR), highlights the importance of in silico prediction solutions. This is particularly crucial in potential applications in therapeutic drug discovery. In this context, machine learning (ML) methods have emerged as prominent candidates, having previously demonstrated prowess in solving complex challenges across various domains. This review focuses on analyzing the development of ML-based solutions for RNA structure prediction, specifically oriented toward recent advancements in the deep learning (DL) domain. A systematic analysis of 33 works reveals insights into the representation of RNA structures, secondary structure motifs, and tertiary interactions. The review highlights current trends in ML methods used for RNA structure prediction, demonstrates the growing research involvement in this field, and summarizes the most valuable findings.},
  archive      = {J_AIR},
  author       = {Budnik, Michał and Wawrzyniak, Jakub and Grala, Łukasz and Kadziński, Miłosz and Szóstak, Natalia},
  doi          = {10.1007/s10462-024-10910-3},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep dive into RNA: A systematic literature review on RNA structure prediction using machine learning methods},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks for text classification: A survey.
<em>AIR</em>, <em>57</em>(8), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-10808-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchmarks. Note that we present a comprehensive comparison between different techniques and identify the pros and cons of various evaluation metrics in this survey.},
  archive      = {J_AIR},
  author       = {Wang, Kunze and Ding, Yihao and Han, Soyeon Caren},
  doi          = {10.1007/s10462-024-10808-0},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph neural networks for text classification: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From single to universal: Tiny lesion detection in medical
imaging. <em>AIR</em>, <em>57</em>(8), 1–42. (<a
href="https://doi.org/10.1007/s10462-024-10762-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automatic detection of tiny lesions in medical imaging plays a critical role in comprehensive cancer diagnosis, staging, treatment, follow-up, and prognosis. Numerous deep learning (DL) based studies have focused on single lesions, providing highly sensitive identification and precise localization. On the other hand, some recent studies have started to concentrate on whole-body lesions, as they could provide systemic clinical support. This paper presents a single-to-universal review of DL studies on tiny lesion detection in medical imaging, with a particular emphasis on detection models and techniques, as well as the data-related aspects such as modality, dimension, and dataset. A wide range of tasks are covered, including traditional single lesion detection tasks such as lung nodules, breast masses, thyroid nodules, and diseased lymph nodes, as well as the emerging task of universal lesion detection. Moreover, the paper provides in-depth analysis and discussion, with the hope of inspiring future research endeavors.},
  archive      = {J_AIR},
  author       = {Zhang, Yi and Mao, Yiji and Lu, Xuanyu and Zou, Xingyu and Huang, Hao and Li, Xinyang and Li, Jiayue and Zhang, Haixian},
  doi          = {10.1007/s10462-024-10762-x},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {From single to universal: Tiny lesion detection in medical imaging},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous wireless network selection using feed forward
double hierarchy linguistic neural network. <em>AIR</em>,
<em>57</em>(8), 1–18. (<a
href="https://doi.org/10.1007/s10462-024-10826-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network selection in heterogeneous wireless networks (HWNs) is a complex issue that requires a thorough understanding of service features and user preferences. This is because the various wireless access technologies have varying capabilities and limitations, and the best network for a voice, video, and data service depends on a variety of factors. For selecting the optimal network in HWNs, varying factors such as the user’s position, accessible network resources, service quality requirements, and user preferences must be considered. The classical decision making procedure is very difficult and uncertain to select the desirable HWNs for voice, video, and data. Therefore, we develop a novel decision making model based on feed-forward neural networks under the double hierarchy linguistic information for the selection of the best HWNs for voice, video, and data. In this article, we introduce a novel feed-forward double hierarchy linguistic neural network using the Hamacher t-norm and t-conorm. Further, the feed-forward double hierarchy linguistic neural network applies to the decision making model for the selection of the best HWNs for voice, video, and data. In this novel decision making model, we first take the given data about HWNs and use the converting function to convert the given data into a double hierarchy linguistic term set. We calculate the hidden layer and output layer information by using Hamacher aggregation operations. Finally, we use the sigmoid activation function on the output layer information to decide on the best HWNs for voice, video, and data according to ranking. The proposed approach is compared with other existing models of decision making and the results of the comparison show that the proposed technique is applicable and reliable for the decision support model.},
  archive      = {J_AIR},
  author       = {Abdullah, Saleem and Ullah, Ihsan and Ghani, Fazal},
  doi          = {10.1007/s10462-024-10826-y},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-18},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Heterogeneous wireless network selection using feed forward double hierarchy linguistic neural network},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive study on modern optimization techniques for
engineering applications. <em>AIR</em>, <em>57</em>(8), 1–52. (<a
href="https://doi.org/10.1007/s10462-024-10829-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid industrialization has fueled the need for effective optimization solutions, which has led to the widespread use of meta-heuristic algorithms. Among the repertoire of over 600, over 300 new methodologies have been developed in the last ten years. This increase highlights the need for a sophisticated grasp of these novel methods. The use of biological and natural phenomena to inform meta-heuristic optimization strategies has seen a paradigm shift in recent years. The observed trend indicates an increasing acknowledgement of the effectiveness of bio-inspired methodologies in tackling intricate engineering problems, providing solutions that exhibit rapid convergence rates and unmatched fitness scores. This study thoroughly examines the latest advancements in bio-inspired optimisation techniques. This work investigates each method’s unique characteristics, optimization properties, and operational paradigms to determine how revolutionary these approaches could be for problem-solving paradigms. Additionally, extensive comparative analyses against conventional benchmarks, such as metrics such as search history, trajectory plots, and fitness functions, are conducted to elucidate the superiority of these new approaches. Our findings demonstrate the revolutionary potential of bio-inspired optimizers and provide new directions for future research to refine and expand upon these intriguing methodologies. Our survey could be a lighthouse, guiding scientists towards innovative solutions rooted in various natural mechanisms.},
  archive      = {J_AIR},
  author       = {Selvarajan, Shitharth},
  doi          = {10.1007/s10462-024-10829-9},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive study on modern optimization techniques for engineering applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence in optical lens design.
<em>AIR</em>, <em>57</em>(8), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10842-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional optical design entails arduous, iterative stages that significantly rely on the intuition and experience of lens designers. Starting-point design selection has always been the major hurdle for most optical design problem, and different designers might produce different final lens designs even if using the same initial specification. Lens designers typically choose designs from existing lens databases, analyse relevant lens structures, or explore patent literature and technical publications. With increased processing capability, producing automated lens designs using Artificial Intelligence (AI) approaches is becoming a viable alternative. Therefore, it is noteworthy that a comprehensive review addressing the latest advancements in using AI for starting-point design is still lacking. Herein, we highlight the gap at the confluence of applied AI and optical lens design, by presenting a comprehensive review of the current literature with an emphasis on using various AI approaches to generate starting-point designs for refractive optical systems, discuss the limitations, and suggest a potential alternate approach for further research.},
  archive      = {J_AIR},
  author       = {Yow, Ai Ping and Wong, Damon and Zhang, Yueqian and Menke, Christoph and Wolleschensky, Ralf and Török, Peter},
  doi          = {10.1007/s10462-024-10842-y},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence in optical lens design},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review of few-shot learning application in CSI human
sensing. <em>AIR</em>, <em>57</em>(8), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10812-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi sensing has garnered increasing interest for its significant advantages, primarily leveraging Wi-Fi signal fluctuations induced by human activities and advanced neural network algorithms. However, its application faces challenges due to limited generalizability, necessitating frequent data recollection and neural network retraining for adaptation to new environments. To address these limitations, some researchers introduced few-shot learning into Wi-Fi sensing applications because it offers a promising solution with its ability to achieve remarkable performance in novel scenarios using minimal training samples. Despite its potential, a comprehensive review of its applications within this domain remains absent. This study endeavors to fill this gap by exploring prominent Wi-Fi sensing applications that incorporate few-shot learning, aiming to delineate their key features. We categorize few-shot learning approaches into three distinct methodologies: transfer learning, metric learning, and meta-learning, based on their neural network training strategies. Through this classification, we examine representative systems from an application perspective and elucidate the principles of few-shot learning implementation. These systems are evaluated in terms of learning methodology, data modality, and recognition accuracy. Finally, this paper highlights the challenges and future directions for few-shot learning in Channel State Information (CSI) based human sensing, providing a valuable resource for researchers in the field of Wi-Fi human sensing leveraging few-shot learning.},
  archive      = {J_AIR},
  author       = {Wang, Zhengjie and Li, Jianhang and Wang, Wenchao and Dong, Zhaolei and Zhang, Qingwei and Guo, Yinjing},
  doi          = {10.1007/s10462-024-10812-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Review of few-shot learning application in CSI human sensing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State-of-the-art review on the use of AI-enhanced
computational mechanics in geotechnical engineering. <em>AIR</em>,
<em>57</em>(8), 1–58. (<a
href="https://doi.org/10.1007/s10462-024-10836-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant uncertainties can be found in the modelling of geotechnical materials. This can be attributed to the complex behaviour of soils and rocks amidst construction processes. Over the past decades, the field has increasingly embraced the application of artificial intelligence methodologies, thus recognising their suitability in forecasting non-linear relationships intrinsic to materials. This review offers a critical evaluation AI methodologies incorporated in computational mechanics for geotechnical engineering. The analysis categorises four pivotal areas: physical properties, mechanical properties, constitutive models, and other characteristics relevant to geotechnical materials. Among the various methodologies analysed, ANNs stand out as the most commonly used strategy, while other methods such as SVMs, LSTMs, and CNNs also see a significant level of application. The most widely used AI algorithms are Artificial Neural Networks (ANN), Random Forest (RF), and Support Vector Machines (SVM), representing 35%, 19%, and 17% respectively. The most extensive AI application is in the domain of mechanical properties, accounting for 59%, followed by other applications at 16%. The efficacy of AI applications is intrinsically linked to the type of datasets employed, the selected model input. This study also outlines future research directions emphasising the need to integrate physically guided and adaptive learning mechanisms to enhance the reliability and adaptability in addressing multi-scale and multi-physics coupled mechanics problems in geotechnics.},
  archive      = {J_AIR},
  author       = {Liu, Hongchen and Su, Huaizhi and Sun, Lizhi and Dias-da-Costa, Daniel},
  doi          = {10.1007/s10462-024-10836-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {State-of-the-art review on the use of AI-enhanced computational mechanics in geotechnical engineering},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for lungs cancer detection: A review.
<em>AIR</em>, <em>57</em>(8), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10807-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although lung cancer has been recognized to be the deadliest type of cancer, a good prognosis and efficient treatment depend on early detection. Medical practitioners’ burden is reduced by deep learning techniques, especially Deep Convolutional Neural Networks (DCNN), which are essential in automating the diagnosis and classification of diseases. In this study, we use a variety of medical imaging modalities, including X-rays, WSI, CT scans, and MRI, to thoroughly investigate the use of deep learning techniques in the field of lung cancer diagnosis and classification. This study conducts a comprehensive Systematic Literature Review (SLR) using deep learning techniques for lung cancer research, providing a comprehensive overview of the methodology, cutting-edge developments, quality assessments, and customized deep learning approaches. It presents data from reputable journals and concentrates on the years 2015–2024. Deep learning techniques solve the difficulty of manually identifying and selecting abstract features from lung cancer images. This study includes a wide range of deep learning methods for classifying lung cancer but focuses especially on the most popular method, the Convolutional Neural Network (CNN). CNN can achieve maximum accuracy because of its multi-layer structure, automatic learning of weights, and capacity to communicate local weights. Various algorithms are shown with performance measures like precision, accuracy, specificity, sensitivity, and AUC; CNN consistently shows the greatest accuracy. The findings highlight the important contributions of DCNN in improving lung cancer detection and classification, making them an invaluable resource for researchers looking to gain a greater knowledge of deep learning’s function in medical applications.},
  archive      = {J_AIR},
  author       = {Javed, Rabia and Abbas, Tahir and Khan, Ali Haider and Daud, Ali and Bukhari, Amal and Alharbey, Riad},
  doi          = {10.1007/s10462-024-10807-1},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for lungs cancer detection: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An experimental evaluation of siamese neural networks for
robot localization using omnidirectional imaging in indoor environments.
<em>AIR</em>, <em>57</em>(8), 1–26. (<a
href="https://doi.org/10.1007/s10462-024-10840-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to address the localization problem using omnidirectional images captured by a catadioptric vision system mounted on the robot. For this purpose, we explore the potential of Siamese Neural Networks for modeling indoor environments using panoramic images as the unique source of information. Siamese Neural Networks are characterized by their ability to generate a similarity function between two input data, in this case, between two panoramic images. In this study, Siamese Neural Networks composed of two Convolutional Neural Networks (CNNs) are used. The output of each CNN is a descriptor which is used to characterize each image. The dissimilarity of the images is computed by measuring the distance between these descriptors. This fact makes Siamese Neural Networks particularly suitable to perform image retrieval tasks. First, we evaluate an initial task strongly related to localization that consists in detecting whether two images have been captured in the same or in different rooms. Next, we assess Siamese Neural Networks in the context of a global localization problem. The results outperform previous techniques for solving the localization task using the COLD-Freiburg dataset, in a variety of lighting conditions, specially when using images captured in cloudy and night conditions.},
  archive      = {J_AIR},
  author       = {Cabrera, Juan José and Román, Vicente and Gil, Arturo and Reinoso, Oscar and Payá, Luis},
  doi          = {10.1007/s10462-024-10840-0},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An experimental evaluation of siamese neural networks for robot localization using omnidirectional imaging in indoor environments},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation and countermeasures of adversarial examples on
vision: A survey. <em>AIR</em>, <em>57</em>(8), 1–48. (<a
href="https://doi.org/10.1007/s10462-024-10841-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have found that deep learning models are vulnerable to adversarial examples, demonstrating that applying a certain imperceptible perturbation on clean examples can effectively deceive the well-trained and high-accuracy deep learning models. Moreover, the adversarial examples can achieve a considerable level of certainty with the attacked label. In contrast, human could barely discern the difference between clean and adversarial examples, which raised tremendous concern about robust and trustworthy deep learning techniques. In this survey, we reviewed the existence, generation, and countermeasures of adversarial examples in Computer Vision, to provide comprehensive coverage of the field with an intuitive understanding of the mechanisms and summarized the strengths, weaknesses, and major challenges. We hope this effort will ignite further interest in the community to solve current challenges and explore this fundamental area.},
  archive      = {J_AIR},
  author       = {Liu, Jiangfan and Li, Yishan and Guo, Yanming and Liu, Yu and Tang, Jun and Nie, Ying},
  doi          = {10.1007/s10462-024-10841-z},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generation and countermeasures of adversarial examples on vision: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of digital twins and their application in
cybersecurity based on artificial intelligence. <em>AIR</em>,
<em>57</em>(8), 1–65. (<a
href="https://doi.org/10.1007/s10462-024-10805-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential of digital twin technology is yet to be fully realised due to its diversity and untapped potential. Digital twins enable systems’ analysis, design, optimisation, and evolution to be performed digitally or in conjunction with a cyber-physical approach to improve speed, accuracy, and efficiency over traditional engineering methods. Industry 4.0, factories of the future, and digital twins continue to benefit from the technology and provide enhanced efficiency within existing systems. Due to the lack of information and security standards associated with the transition to cyber digitisation, cybercriminals have been able to take advantage of the situation. Access to a digital twin of a product or service is equivalent to threatening the entire collection. There is a robust interaction between digital twins and artificial intelligence tools, which leads to strong interaction between these technologies, so it can be used to improve the cybersecurity of these digital platforms based on their integration with these technologies. This study aims to investigate the role of artificial intelligence in providing cybersecurity for digital twin versions of various industries, as well as the risks associated with these versions. In addition, this research serves as a road map for researchers and others interested in cybersecurity and digital security.},
  archive      = {J_AIR},
  author       = {Homaei, Mohammadhossein and Mogollón-Gutiérrez, Óscar and Sancho, José Carlos and Ávila, Mar and Caro, Andrés},
  doi          = {10.1007/s10462-024-10805-3},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of digital twins and their application in cybersecurity based on artificial intelligence},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating metaheuristics and artificial intelligence for
healthcare: Basics, challenging and future directions. <em>AIR</em>,
<em>57</em>(8), 1–46. (<a
href="https://doi.org/10.1007/s10462-024-10822-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and rapid disease detection is necessary to manage health problems early. Rapid increases in data amount and dimensionality caused challenges in many disciplines, with the primary issues being high computing costs, memory costs, and low accuracy performance. These issues will arise since Machine Learning (ML) classifiers are mostly used in these fields. However, noisy and irrelevant features have an impact on ML accuracy. Therefore, to choose the best subset of features and decrease the dimensionality of the data, Metaheuristics (MHs) optimization algorithms are applied to Feature Selection (FS) using various modalities of medical imaging or disease datasets with different dimensions. The review starts by giving a general overview of the many approaches to AI algorithms, followed by a general overview of the various MH algorithms for healthcare applications, an analysis of MHs boosted AI for healthcare applications, and using a wide range of research databases as a data source for access to numerous field publications. The final section of this review discusses the problems and challenges facing healthcare application development.},
  archive      = {J_AIR},
  author       = {Houssein, Essam H. and Saber, Eman and Ali, Abdelmgeid A. and Wazery, Yaser M.},
  doi          = {10.1007/s10462-024-10822-2},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Integrating metaheuristics and artificial intelligence for healthcare: Basics, challenging and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Role of artificial intelligence in the crime prediction and
pattern analysis studies published over the last decade: A scientometric
analysis. <em>AIR</em>, <em>57</em>(8), 1–35. (<a
href="https://doi.org/10.1007/s10462-024-10823-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime is the intentional commission of an act usually suspected as socially detrimental and specifically defined, forbidden, and punishable under criminal law. Developing a society that is less susceptible to criminal acts makes crime prediction and pattern analysis (CPPA) a paramount topic for academic research interest. With the innovation in technology and rapid expansion of Artificial Intelligence (AI), the research in the field of CPPA has evolved radically to predict crime efficiently. While the number of publications is expanding substantially, we believe there is a dearth of thorough scientometric analysis for this topic. This work intends to analyze research conducted in the last decade using Scopus data and a scientometric technique, emphasizing citation trends and intriguing journals, nations, institutions, their collaborations, authors, and co-authorship networks in CPPA research. Furthermore, three field plots have been staged to visualize numerous associations between country, journal, keyword, and author. Besides, a comprehensive keyword analysis is carried out to visualize the CPPA research carried out with AI amalgamation. A total of five clusters have been identified depicting several AI methods used by the researchers in CPPA and the evolution of research trends over time from various perspectives.},
  archive      = {J_AIR},
  author       = {Kaur, Manpreet and Saini, Munish},
  doi          = {10.1007/s10462-024-10823-1},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Role of artificial intelligence in the crime prediction and pattern analysis studies published over the last decade: A scientometric analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on emotion detection by using deep learning
techniques. <em>AIR</em>, <em>57</em>(8), 1–80. (<a
href="https://doi.org/10.1007/s10462-024-10831-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the growth of Internet with its numerous potential applications and diverse fields, artificial intelligence (AI) and sentiment analysis (SA) have become significant and popular research areas. Additionally, it was a key technology that contributed to the Fourth Industrial Revolution (IR 4.0). The subset of AI known as emotion recognition systems facilitates communication between IR 4.0 and IR 5.0. Nowadays users of social media, digital marketing, and e-commerce sites are increasing day by day resulting in massive amounts of unstructured data. Medical, marketing, public safety, education, human resources, business, and other industries also use the emotion recognition system widely. Hence it provides a large amount of textual data to extract the emotions from them. The paper presents a systematic literature review of the existing literature published between 2013 to 2023 in text-based emotion detection. This review scrupulously summarized 330 research papers from different conferences, journals, workshops, and dissertations. This paper explores different approaches, methods, different deep learning models, key aspects, description of datasets, evaluation techniques, Future prospects of deep learning, challenges in existing studies and presents limitations and practical implications.},
  archive      = {J_AIR},
  author       = {Chutia, Tulika and Baruah, Nomi},
  doi          = {10.1007/s10462-024-10831-1},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on emotion detection by using deep learning techniques},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SRGAN-enhanced unsafe operation detection and classification
of heavy construction machinery using cascade learning. <em>AIR</em>,
<em>57</em>(8), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10839-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the inherently hazardous construction industry, where injuries are frequent, the unsafe operation of heavy construction machinery significantly contributes to the injury and accident rates. To reduce these risks, this study introduces a novel framework for detecting and classifying these unsafe operations for five types of construction machinery. Utilizing a cascade learning architecture, the approach employs a Super-Resolution Generative Adversarial Network (SRGAN), Real-Time Detection Transformers (RT-DETR), self-DIstillation with NO labels (DINOv2), and Dilated Neighborhood Attention Transformer (DiNAT) models. The study focuses on enhancing the detection and classification of unsafe operations in construction machinery through upscaling low-resolution surveillance footage and creating detailed high-resolution inputs for the RT-DETR model. This enhancement, by leveraging temporal information, significantly improves object detection and classification accuracy. The performance of the cascaded pipeline yielded an average detection and first-level classification precision of 96%, a second-level classification accuracy of 98.83%, and a third-level classification accuracy of 98.25%, among other metrics. The cascaded integration of these models presents a well-rounded solution for near-real-time surveillance in dynamic construction environments, advancing surveillance technologies and significantly contributing to safety management within the industry.},
  archive      = {J_AIR},
  author       = {Kim, Bubryur and An, Eui-Jung and Kim, Sungho and Sri Preethaa, K. R. and Lee, Dong-Eun and Lukacs, R. R.},
  doi          = {10.1007/s10462-024-10839-7},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SRGAN-enhanced unsafe operation detection and classification of heavy construction machinery using cascade learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards the automation of systematic reviews using natural
language processing, machine learning, and deep learning: A
comprehensive review. <em>AIR</em>, <em>57</em>(8), 1–60. (<a
href="https://doi.org/10.1007/s10462-024-10844-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic reviews (SRs) constitute a critical foundation for evidence-based decision-making and policy formulation across various disciplines, particularly in healthcare and beyond. However, the inherently rigorous and structured nature of the SR process renders it laborious for human reviewers. Moreover, the exponential growth in daily published literature exacerbates the challenge, as SRs risk missing out on incorporating recent studies that could potentially influence research outcomes. This pressing need to streamline and enhance the efficiency of SRs has prompted significant interest in leveraging Artificial Intelligence (AI) techniques to automate various stages of the SR process. This review paper provides a comprehensive overview of the current AI methods employed for SR automation, a subject area that has not been exhaustively covered in previous literature. Through an extensive analysis of 52 related works and an original online survey, the primary AI techniques and their applications in automating key SR stages, such as search, screening, data extraction, and risk of bias assessment, are identified. The survey results offer practical insights into the current practices, experiences, opinions, and expectations of SR practitioners and researchers regarding future SR automation. Synthesis of the literature review and survey findings highlights gaps and challenges in the current landscape of SR automation using AI techniques. Based on these insights, potential future directions are discussed. This review aims to equip researchers and practitioners with a foundational understanding of the basic concepts, primary methodologies, and recent advancements in AI-driven SR automation while guiding computer scientists in exploring novel techniques to invigorate further and advance this field.},
  archive      = {J_AIR},
  author       = {Ofori-Boateng, Regina and Aceves-Martins, Magaly and Wiratunga, Nirmalie and Moreno-Garcia, Carlos Francisco},
  doi          = {10.1007/s10462-024-10844-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of implementing security and privacy in
federated learning. <em>AIR</em>, <em>57</em>(8), 1–66. (<a
href="https://doi.org/10.1007/s10462-024-10846-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has received a great deal of research attention recently,with privacy protection becoming a key factor in the development of artificial intelligence. Federated learning is a special kind of distributed learning framework, which allows multiple users to participate in model training while ensuring that their privacy is not compromised; however, this paradigm is still vulnerable to security and privacy threats from various attackers. This paper focuses on the security and privacy threats related to federated learning. First, we analyse the current research and development status of federated learning through use of the CiteSpace literature search tool. Next, we describe the basic concepts and threat models, and then analyse the security and privacy vulnerabilities within current federated learning architectures. Finally, the directions of development in this area are further discussed in the context of current advanced defence solutions, for which we provide a summary and comparison.},
  archive      = {J_AIR},
  author       = {Hu, Kai and Gong, Sheng and Zhang, Qi and Seng, Chaowen and Xia, Min and Jiang, Shanshan},
  doi          = {10.1007/s10462-024-10846-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An overview of implementing security and privacy in federated learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessment of bio-medical waste disposal techniques using
interval-valued q-rung orthopair fuzzy soft set based EDAS method.
<em>AIR</em>, <em>57</em>(8), 1–75. (<a
href="https://doi.org/10.1007/s10462-024-10750-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting an optimum technique for disposing of biomedical waste is a frequently observed obstacle in multi-attribute group decision-making (MAGDM) problems. The MAGDM is commonly applied to tackle decision-making states originated by obscurity and vagueness. The interval-valued q-rung orthopair fuzzy soft set is a novel variant of fuzzy sets. The main objective of this study is to introduce the interval-valued q-rung orthopair fuzzy soft Einstein-ordered weighted and Einstein hybrid weighted aggregation operators. Based on developed aggregation operators, a novel decision-making approach, the Evaluation based on the Distance from the Average Solution introduced to solve the MAGDM problem. The execution of the proposed approach demonstrates the significant impact of determining the most effective strategy to handle biomedical waste. Our proposed approach&#39;s practicality is confirmed by a case study focusing on selecting the most effective technique for Biomedical Waste (BMW) treatment. This study shows that autoclaving is the most effective method for the disposal of BMW. Comparative and sensitivity analysis confirms the consistency and effectiveness of our methodology. The comparative study indicates the effects of the proposed strategy are more feasible and realistic than the prevailing techniques.},
  archive      = {J_AIR},
  author       = {Zulqarnain, Rana Muhammad and Naveed, Hamza and Askar, Sameh and Deveci, Muhammet and Siddique, Imran and Castillo, Oscar},
  doi          = {10.1007/s10462-024-10750-1},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-75},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Assessment of bio-medical waste disposal techniques using interval-valued q-rung orthopair fuzzy soft set based EDAS method},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning for biometric recognition: A survey.
<em>AIR</em>, <em>57</em>(8), 1–40. (<a
href="https://doi.org/10.1007/s10462-024-10847-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning (DL) has achieved great success in biometric recognition. The application of DL has also led to a high demand for biometric data. However, as people attach more importance to privacy protection, biometric data have become increasingly difficult to obtain and access, leading to significant limitations in the development and application of DL-based biometric recognition. Federated learning (FL), a distributed learning technique with privacy protection, provides a potential solution to this problem. Several researchers have attempted to integrate FL into biometric recognition. These studies have shown that the introduction of FL not only solves the conflict between privacy and accessibility of biometric data but also improves the accuracy and generalizability of local recognition systems. Therefore, the combination of FL and biometric recognition techniques has become a new research hotspot. In this survey, we comprehensively review the latest advances regarding the application of FL to biometric recognition, biometric presentation attack detection and the related fields to provide new researchers with a quick and systematic overview of this emerging cross-disciplinary field. This paper also summarizes the future opportunities and challenges of this field. To our knowledge, this is the first survey that systematically organizes and analyses federated biometric recognition and related fields to provide suggestions and references for future research.},
  archive      = {J_AIR},
  author       = {Guo, Jian and Mu, Hengyu and Liu, Xingli and Ren, Hengyi and Han, Chong},
  doi          = {10.1007/s10462-024-10847-7},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Federated learning for biometric recognition: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fire identification based on novel dense generative
adversarial networks. <em>AIR</em>, <em>57</em>(8), 1–20. (<a
href="https://doi.org/10.1007/s10462-024-10848-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing death rates, damage to properties, and loss of trees can be caused by fires. In Australia and the United States of America, many fire incidents are reported annually. Due to that, both governments struggle from the devastation beyond plants, buildings, and infrastructure. A lot of people have lost their properties and land. Various innovations in fire detection technologies have been implemented to minimize the impacts of fires on the economy and lives. Some of these solutions are costly, while others lack accuracy. In this article, a novel deep-learning model to detect fires is presented. This model is based on new Novel Dense Generative Adversarial Networks (NDGANs) and image preprocessing technologies for fire detection through a continuous monitoring system. This system produces alarms if a fire or smoke is detected. The proposed approach was trained and tested on five datasets. This system was evaluated using four performance quantities, which are accuracy, sensitivity, dice, and F-score, and attained 98.87%, 97.64%, 98.82%, and 98.69% for the considered quantities, respectively. In addition, the proposed method was compared with other developed approaches and outperformed these methods. The presented New Dense Generative Adversarial Networks technology is useful in fire detection as shown from the conducted simulation experiments on MATLAB.},
  archive      = {J_AIR},
  author       = {Shawly, Tawfeeq and Alsheikhy, Ahmed A.},
  doi          = {10.1007/s10462-024-10848-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fire identification based on novel dense generative adversarial networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Versatile time-window sliding machine learning techniques
for stock market forecasting. <em>AIR</em>, <em>57</em>(8), 1–20. (<a
href="https://doi.org/10.1007/s10462-024-10851-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stock market which is a critical instrument in the modern financial system consistently attracts a significant number of individuals and financial institutions. The amalgamation of machine learning with stock forecasting has seen increased interest due to the rising prominence of machine learning. Among numerous machine learning models, the long short-term memory (LSTM) is favored by many researchers for its superior performance in long time series. This paper presents an innovative approach that integrates LSTM with versatile sliding window techniques to enhance prediction results and training performance. Moreover, the attached experiments incorporate convolutional filters and combined bivariate performance measures, which are invaluable methodologies for enhancing stock forecasting systems. These significant contributions are expected to be beneficial for researchers operating within this domain.},
  archive      = {J_AIR},
  author       = {Zhan, Zeqiye and Kim, Song-Kyoo},
  doi          = {10.1007/s10462-024-10851-x},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Versatile time-window sliding machine learning techniques for stock market forecasting},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise suppression zeroing neural network for online solving
the time-varying inverse kinematics problem of four-wheel mobile
manipulators with external disturbances. <em>AIR</em>, <em>57</em>(8),
1–24. (<a href="https://doi.org/10.1007/s10462-024-10804-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel noise suppression zeroing neural network (NSZNN) is presented for the trajectory tracking problem on a four Mecanum wheeled mobile manipulator (FMWMM) by solving its time-varying inverse kinematics (TVIK) problem. The holistic kinematic model of the FMWMM is developed, which can receive synergistic control of the mobile manipulator. Different from the situation without external interference addressed in our previous work, this paper considers a variety of common time-varying interferences by studying the basic principles of various noises, and proves the NSZNN model’s of the validity and superiority, which solves the TVIK problem of the FMWMM with external disturbances through theoretical analyses. Compared with the existing gradient neural network (GNN) and the traditional zeroing neural network (ZNN), the most representative hybrid noise is selected to conduct a large number of experiments to substantiate the high efficiency and robustness of the NSZNN model. Finally, the NSZNN model is verified on the FMWMM via a robot operating system (ROS) by a successful execution of the trajectory tracking task.},
  archive      = {J_AIR},
  author       = {Sun, Zhongbo and Zhou, Yanpeng and Tang, Shijun and Luo, Jun and Zhao, Bo},
  doi          = {10.1007/s10462-024-10804-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Noise suppression zeroing neural network for online solving the time-varying inverse kinematics problem of four-wheel mobile manipulators with external disturbances},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based models for combating rumours on
microblogging platforms: A review. <em>AIR</em>, <em>57</em>(8), 1–69.
(<a href="https://doi.org/10.1007/s10462-024-10837-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable success of Transformer-based embeddings in natural language tasks has sparked interest among researchers in applying them to classify rumours on social media, particularly microblogging platforms. Unlike traditional word embedding methods, Transformers excel at capturing a word’s contextual meaning by considering words from both the left and right of a word, resulting in superior text representations ideal for tasks like rumour detection on microblogging platforms. This survey aims to provide a thorough and well-organized overview and analysis of existing research on implementing Transformer-based models for rumour detection on microblogging platforms. The scope of this study is to offer a comprehensive understanding of this topic by systematically examining and organizing the existing literature. We start by discussing the fundamental reasons and significance of automating rumour detection on microblogging platforms. Emphasizing the critical role of text embedding in converting textual data into numerical representations, we review current approaches to implement Transformer models for rumour detection on microblogging platforms. Furthermore, we present a novel taxonomy that covers a wide array of techniques and approaches employed in the deployment of Transformer-based models for identifying misinformation on microblogging platforms. Additionally, we highlight the challenges associated with this field and propose potential avenues for future research. Drawing insights from the surveyed articles, we anticipate that promising results will continue to emerge as the challenges outlined in this study are addressed. We hope that our efforts will stimulate further interest in harnessing the capabilities of Transformer models to combat the spread of rumours on microblogging platforms.},
  archive      = {J_AIR},
  author       = {Anggrainingsih, Rini and Hassan, Ghulam Mubashar and Datta, Amitava},
  doi          = {10.1007/s10462-024-10837-9},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-69},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transformer-based models for combating rumours on microblogging platforms: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small data challenges for intelligent prognostics and health
management: A review. <em>AIR</em>, <em>57</em>(8), 1–52. (<a
href="https://doi.org/10.1007/s10462-024-10820-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognostics and health management (PHM) is critical for enhancing equipment reliability and reducing maintenance costs, and research on intelligent PHM has made significant progress driven by big data and deep learning techniques in recent years. However, complex working conditions and high-cost data collection inherent in real-world scenarios pose small-data challenges for the application of these methods. Given the urgent need for data-efficient PHM techniques in academia and industry, this paper aims to explore the fundamental concepts, ongoing research, and future trajectories of small data challenges in the PHM domain. This survey first elucidates the definition, causes, and impacts of small data on PHM tasks, and then analyzes the current mainstream approaches to solving small data problems, including data augmentation, transfer learning, and few-shot learning techniques, each of which has its advantages and disadvantages. In addition, this survey summarizes benchmark datasets and experimental paradigms to facilitate fair evaluations of diverse methodologies under small data conditions. Finally, some promising directions are pointed out to inspire future research.},
  archive      = {J_AIR},
  author       = {Li, Chuanjiang and Li, Shaobo and Feng, Yixiong and Gryllias, Konstantinos and Gu, Fengshou and Pecht, Michael},
  doi          = {10.1007/s10462-024-10820-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Small data challenges for intelligent prognostics and health management: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geological adaptive intelligent control of earth pressure
balance shield machine based on deep reinforcement learning.
<em>AIR</em>, <em>57</em>(8), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10850-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific and precise control of tunnelling parameters is of utmost importance during the construction of shield machines. Given the complexity of the working environment, manual operation is highly prone to causing safety accidents. Therefore, achieving intelligent control of the shield machine is crucial. Based on this, this paper proposes a geological adaptive intelligent control method of earth pressure balance shield machine using the Deep Deterministic Policy Gradient (DDPG) algorithm as the framework, with Actor-Critic as the basis. Firstly, DDPG agent is constructed to replace the screw conveyor control system as the main body of strategy implementation. Secondly, an environmental model is established by utilizing the mechanism model between the sealed cabin pressure and the screw conveyor speed. The real-time sealed cabin pressure, target pressure, and pressure error serve as the state space, while the screw conveyor speed is used as the action space. A combined reward function is set based on safety and accuracy. Finally, the Actor network interacts with the environment under the supervision of the reward function and Critic network. Successful training is achieved when the cumulative reward value is maximized, resulting in the output of optimal control strategy. In this paper, the method dynamically regulates the screw conveyor speed by interacting with the geological environment, to realize the precise control of the sealed cabin pressure and ensure the dynamic balance between sealed cabin pressure and excavation face pressure. The test results show that this method has a good control effect on the sealed cabin pressure under various geological conditions, and can complete 72 kinds of soil transition tasks. It has strong soil adaptability and can respond well to the dynamic changes of soil conditions. This approach enhances the intelligence of the shield machine, mitigating inaccuracies attributed to human operation, which provides a guarantee of safe shield machine operation, whilst exhibiting valuable engineering applications.},
  archive      = {J_AIR},
  author       = {Liu, Xuanyu and Zhang, Wenshuai and Shao, Cheng and Wang, Yudong and Cong, Qiumei},
  doi          = {10.1007/s10462-024-10850-y},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Geological adaptive intelligent control of earth pressure balance shield machine based on deep reinforcement learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge transfer in lifelong machine learning: A
systematic literature review. <em>AIR</em>, <em>57</em>(8), 1–55. (<a
href="https://doi.org/10.1007/s10462-024-10853-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong Machine Learning (LML) denotes a scenario involving multiple sequential tasks, each accompanied by its respective dataset, in order to solve specific learning problems. In this context, the focus of LML techniques is on utilizing already acquired knowledge to adapt to new tasks efficiently. Essentially, LML concerns about facing new tasks while exploiting the knowledge previously gathered from earlier tasks not only to help in adapting to new tasks but also to enrich the understanding of past ones. By understanding this concept, one can better grasp one of the major obstacles in LML, known as Knowledge Transfer (KT). This systematic literature review aims to explore state-of-the-art KT techniques within LML and assess the evaluation metrics and commonly utilized datasets in this field, thereby keeping the LML research community updated with the latest developments. From an initial pool of 417 articles from four distinguished databases, 30 were deemed highly pertinent for the information extraction phase. The analysis recognizes four primary KT techniques: Replay, Regularization, Parameter Isolation, and Hybrid. This study delves into the characteristics of these techniques across both neural network (NN) and non-neural network (non-NN) frameworks, highlighting their distinct advantages that have captured researchers’ interest. It was found that the majority of the studies focused on supervised learning within an NN modelling framework, particularly employing Parameter Isolation and Hybrid for KT. The paper concludes by pinpointing research opportunities, including investigating non-NN models for Replay and exploring applications outside of computer vision (CV).},
  archive      = {J_AIR},
  author       = {Khodaee, Pouya and Viktor, Herna L. and Michalowski, Wojtek},
  doi          = {10.1007/s10462-024-10853-9},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Knowledge transfer in lifelong machine learning: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable artificial intelligence (XAI) in finance: A
systematic literature review. <em>AIR</em>, <em>57</em>(8), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10854-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the range of decisions made by Artificial Intelligence (AI) expands, the need for Explainable AI (XAI) becomes increasingly critical. The reasoning behind the specific outcomes of complex and opaque financial models requires a thorough justification to improve risk assessment, minimise the loss of trust, and promote a more resilient and trustworthy financial ecosystem. This Systematic Literature Review (SLR) identifies 138 relevant articles from 2005 to 2022 and highlights empirical examples demonstrating XAI&#39;s potential benefits in the financial industry. We classified the articles according to the financial tasks addressed by AI using XAI, the variation in XAI methods between applications and tasks, and the development and application of new XAI methods. The most popular financial tasks addressed by the AI using XAI were credit management, stock price predictions, and fraud detection. The three most commonly employed AI black-box techniques in finance whose explainability was evaluated were Artificial Neural Networks (ANN), Extreme Gradient Boosting (XGBoost), and Random Forest. Most of the examined publications utilise feature importance, Shapley additive explanations (SHAP), and rule-based methods. In addition, they employ explainability frameworks that integrate multiple XAI techniques. We also concisely define the existing challenges, requirements, and unresolved issues in applying XAI in the financial sector.},
  archive      = {J_AIR},
  author       = {Černevičienė, Jurgita and Kabašinskas, Audrius},
  doi          = {10.1007/s10462-024-10854-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Explainable artificial intelligence (XAI) in finance: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive unified defense framework for tackling adversarial
audio attacks. <em>AIR</em>, <em>57</em>(8), 1–22. (<a
href="https://doi.org/10.1007/s10462-024-10863-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks aimed at subverting recognition systems have laid bare significant security vulnerabilities inherent in deep neural networks. In the automatic speech recognition (ASR) domain, prevailing defense mechanisms have primarily centered on pre-processing procedures to mitigate adversarial threats stemming from such attacks. However, despite their initial success, these methods have shown surprising vulnerabilities when confronted with robust and adaptive adversarial attacks. This paper proposes an adaptive unified defense framework tailored to address the challenges posed by robust audio adversarial examples. The framework comprises two pivotal components: (1) a unified pre-processing mechanism is designed to disrupt the continuity and transferability of adversarial attacks. Its objective is to thwart the consistent operation of adversarial examples across different systems or conditions, thereby enhancing the robustness of the defense. (2) an adaptive ASR transcription method is proposed to further bolster our defense strategy. Empirical experiments conducted using two benchmark audio datasets within a state-of-the-art ASR system affirm the effectiveness of our adaptive defense framework. It achieves an impressive 100% accuracy rate against representative audio attacks and consistently outperforms other state-of-the-art defense techniques, achieving an accuracy rate of 98.5% even when faced with various challenging adaptive adversarial attacks.},
  archive      = {J_AIR},
  author       = {Du, Xia and Zhang, Qi and Zhu, Jiajie and Liu, Xiaoyuan},
  doi          = {10.1007/s10462-024-10863-7},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive unified defense framework for tackling adversarial audio attacks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI’s effect on innovation capacity in the context of
industry 5.0: A scoping review. <em>AIR</em>, <em>57</em>(8), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10864-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic literature about innovation conveys innovation strategy the leading and starting role to generate business growth due to technology development and more effective managerial practices. The advent of Artificial Intelligence (AI) however reverts this paradigm in the context of Industry 5.0. The focus is moving from “how innovation fosters AI” to “how AI fosters innovation”. Therefore, our research question can be stated as follows: What factors influence the effect of AI on Innovation Capacity in the context of Industry 5.0? To address this question we conduct a scoping review of a vast body of literature spanning engineering, human sciences, and management science. We conduct a keyword-based literature search completed by bibliographic analysis, then classify the resulting 333 works into 3 classes and 15 clusters which we critically analyze. We extract 3 hypotheses setting associations between 4 factors: company age, AI maturity, manufacturing strategy, and innovation capacity. The review uncovers several debates and research gaps left unsolved by the existing literature. In particular, it raises the debate whether the Industry5.0 promise can be achieved while Artificial General Intelligence (AGI) remains out of reach. It explores diverging possible futures driven toward social manufacturing or mass customization. Finally, it discusses alternative AI policies and their incidence on open and internal innovation. We conclude that the effect of AI on innovation capacity can be synergic, deceptive, or substitutive depending on the alignment of the uncovered factors. Moreover, we identify a set of 12 indicators enabling us to measure these factors to predict AI’s effect on innovation capacity. These findings provide researchers with a new understanding of the interplay between artificial intelligence and human intelligence. They provide practitioners with decision metrics for a successful transition to Industry 5.0.},
  archive      = {J_AIR},
  author       = {Bécue, Adrien and Gama, Joao and Brito, Pedro Quelhas},
  doi          = {10.1007/s10462-024-10864-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI’s effect on innovation capacity in the context of industry 5.0: A scoping review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive review of data-driven approaches for
forecasting production from unconventional reservoirs: Best practices
and future directions. <em>AIR</em>, <em>57</em>(8), 1–40. (<a
href="https://doi.org/10.1007/s10462-024-10865-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of well production from unconventional reservoirs is a complex problem given an incomplete understanding of physics despite large amounts of data. Recently, Data Analytics Techniques (DAT) have emerged as an effective approach for production forecasting for unconventional reservoirs. In some of these approaches, DAT are combined with physics-based models to capture the essential physical mechanisms of fluid flow in porous media, while leveraging the power of data-driven methods to account for uncertainties and heterogeneities. Here, we provide an overview of the applications and performance of DAT for production forecasting of unconventional reservoirs examining and comparing predictive models using different algorithms, validation benchmarks, input data, number of wells, and formation types. We also discuss the strengths and limitations of each model, as well as the challenges and opportunities for future research in this field. Our analysis shows that machine learning (ML) based models can achieve satisfactory performance in forecasting production from unconventional reservoirs. We measure the performance of the models using two dimensionless metrics: mean absolute percentage error (MAPE) and coefficient of determination (R2). The predicted and actual production data show a high degree of agreement, as most of the models have a low error rate and a strong correlation. Specifically, ~ 65% of the models have MAPE less than 20%, and more than 80% of the models have R2 higher than 0.6. Therefore, we expect that DAT can improve the reliability and robustness of production forecasting for unconventional resources. However, we also identify some areas for future improvement, such as developing new ML algorithms, combining DAT with physics-based models, and establishing multi-perspective approaches for comparing model performance.},
  archive      = {J_AIR},
  author       = {Rahmanifard, Hamid and Gates, Ian},
  doi          = {10.1007/s10462-024-10865-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of data-driven approaches for forecasting production from unconventional reservoirs: Best practices and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient artificial intelligence approaches for medical
image processing in healthcare: Comprehensive review, taxonomy, and
analysis. <em>AIR</em>, <em>57</em>(8), 1–139. (<a
href="https://doi.org/10.1007/s10462-024-10814-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In healthcare, medical practitioners employ various imaging techniques such as CT, X-ray, PET, and MRI to diagnose patients, emphasizing the crucial need for early disease detection to enhance survival rates. Medical Image Analysis (MIA) has undergone a transformative shift with the integration of Artificial Intelligence (AI) techniques such as Machine Learning (ML) and Deep Learning (DL), promising advanced diagnostics and improved healthcare outcomes. Despite these advancements, a comprehensive understanding of the efficiency metrics, computational complexities, interpretability, and scalability of AI based approaches in MIA is essential for practical feasibility in real-world healthcare environments. Existing studies exploring AI applications in MIA lack a consolidated review covering the major MIA stages and specifically focused on evaluating the efficiency of AI based approaches. The absence of a structured framework limits decision-making for researchers, practitioners, and policymakers in selecting and implementing optimal AI approaches in healthcare. Furthermore, the lack of standardized evaluation metrics complicates methodology comparison, hindering the development of efficient approaches. This article addresses these challenges through a comprehensive review, taxonomy, and analysis of existing AI-based MIA approaches in healthcare. The taxonomy covers major image processing stages, classifying AI approaches for each stage based on method and further analyzing them based on image origin, objective, method, dataset, and evaluation metrics to reveal their strengths and weaknesses. Additionally, comparative analysis conducted to evaluate the efficiency of AI based MIA approaches over five publically available datasets: ISIC 2018, CVC-Clinic, 2018 DSB, DRIVE, and EM in terms of accuracy, precision, Recall, F-measure, mIoU, and specificity. The popular public datasets and evaluation metrics are briefly described and analyzed. The resulting taxonomy provides a structured framework for understanding the AI landscape in healthcare, facilitating evidence-based decision-making and guiding future research efforts toward the development of efficient and scalable AI approaches to meet current healthcare needs.},
  archive      = {J_AIR},
  author       = {Alnaggar, Omar Abdullah Murshed Farhan and Jagadale, Basavaraj N. and Saif, Mufeed Ahmed Naji and Ghaleb, Osamah A. M. and Ahmed, Ammar A. Q. and Aqlan, Hesham Abdo Ahmed and Al-Ariki, Hasib Daowd Esmail},
  doi          = {10.1007/s10462-024-10814-2},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-139},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Efficient artificial intelligence approaches for medical image processing in healthcare: Comprehensive review, taxonomy, and analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of artificial intelligence in turbomachinery
aerodynamics: Progresses and challenges. <em>AIR</em>, <em>57</em>(8),
1–46. (<a href="https://doi.org/10.1007/s10462-024-10867-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turbomachinery plays a vital role in energy conversion systems, with aerodynamic issues being integral to its entire lifecycle, spanning the period of design, validation, and maintenance. Conventionally, the reliance on skilled aerodynamic engineers has been pivotal in the successful development of turbomachines. However, in the current era of burgeoning artificial intelligence (AI) technology, researchers are increasingly turning to AI to replace human expertise and decision-making in these aerodynamic issues and to solve previously intractable aerodynamic problems. This paper presents a systematic literature review of the latest advancements in applying AI to turbomachinery aerodynamics, encompassing the design, validation, and maintenance of compressors and turbines. It underscores how AI is revolutionizing the research paradigm of turbomachinery aerodynamics. AI’s powerful learning capability facilitates more precise and convenient aerodynamic analyses and inspires innovative aerodynamic design ideas that go beyond the capabilities of classical design techniques. Additionally, AI’s autonomous decision-making capability can be employed for aerodynamic optimization and active flow control of turbomachines, generating optimal aerodynamic solutions and complex control strategies that surpass human brains. As a main contribution, we provide a detailed exposition of the future intelligent turbomachinery research and development (R &amp;D) system, along with highlighting potential challenges such as physics embedding, interactive 3D design optimization, and real-time prognoses. It is anticipated that harnessing AI’s full potential will lead to a comprehensive AI-based turbomachinery R &amp;D system in the future.},
  archive      = {J_AIR},
  author       = {Zou, Zhengping and Xu, Pengcheng and Chen, Yiming and Yao, Lichao and Fu, Chao},
  doi          = {10.1007/s10462-024-10867-3},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence in turbomachinery aerodynamics: Progresses and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive analysis of artificial intelligence techniques
for gynaecological cancer: Symptoms identification, prognosis and
prediction. <em>AIR</em>, <em>57</em>(8), 1–44. (<a
href="https://doi.org/10.1007/s10462-024-10872-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gynaecological cancers encompass a spectrum of malignancies affecting the female reproductive system, comprising the cervix, uterus, ovaries, vulva, vagina, and fallopian tubes. The significant health threat posed by these cancers worldwide highlight the crucial need for techniques for early detection and prediction of gynaecological cancers. Preferred reporting items for systematic reviews and Meta-Analysis guidelines are used to select the articles published from 2013 up to 2023 on the Web of Science, Scopus, Google Scholar, PubMed, Excerpta Medical Database, and encompass AI technique for the early detection and prediction of gynaecological cancers. Based on the study of different articles on gynaecological cancer, the results are also compared using various quality parameters such as prediction rate, accuracy, sensitivity, specificity, the area under curve precision, recall, and F1-score. This work highlights the impact of gynaecological cancer on women belonging to different age groups and regions of the world. A detailed categorization of the traditional techniques like physical-radiological, bio-physical and bio-chemical used to detect gynaecological cancer by health organizations is also presented in the study. Besides, this work also explores the methodology used by different researchers in which AI plays a crucial role in identifying cancer symptoms at earlier stages. The paper also investigates the pivotal study years, highlighting the periods when the highest number of research articles on gynaecological cancer are published. The challenges faced by researchers while performing AI-based research on gynaecological cancers are also highlighted in this work. The features and representations such as Magnetic Resonance Imaging (MRI), ultrasound, pap smear, pathological, etc., which proficient the AI algorithms in early detection of gynaecological cancer are also explored. This comprehensive review contributes to the understanding of the role of AI in improving the detection and prognosis of gynaecological cancers, and provides insights for future research directions and clinical applications. AI has the potential to substantially reduce mortality rates linked to gynaecological cancer in the future by enabling earlier identification, individualised risk assessment, and improved treatment techniques. This would ultimately improve patient outcomes and raise the standard of healthcare for all individuals.},
  archive      = {J_AIR},
  author       = {Gandotra, Sonam and Kumar, Yogesh and Modi, Nandini and Choi, Jaeyoung and Shafi, Jana and Ijaz, Muhammad Fazal},
  doi          = {10.1007/s10462-024-10872-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive analysis of artificial intelligence techniques for gynaecological cancer: Symptoms identification, prognosis and prediction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Projection generalized correntropy twin support vector
regression. <em>AIR</em>, <em>57</em>(8), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10856-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A projection generalized maximum correntropy twin support vector regression algorithm is proposed. The generalized correntropy function is added into the loss function of adaptive filtering, maximizing which can suppress the interference of noise or outliers.Considering the fact that single-shift projection twin support vector regression cannot observe local information of samples, a complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) combined with wavelet soft threshold denoising is used to assign weights to samples. The CEEMDAN is used to decompose the original data, calculate the Pearson correlation coefficient between the mode functions and the original data. The mode with low correlation is filtered by wavelet based algorithm with soft-threshold to get the reconstructed samples after noise reduction. Smaller weights will be assigned to reconstructed samples with significant differences from the original data, while larger weights will be assigned to reconstructed samples with smaller differences. Similarly, the empirical risk term in the cost function is also assigned calculated weights to improve the robustness. Due to the use of empirical mode decomposition, the proposed method is particularly suitable for processing non-stationary data. Experimental results on artificial and UCI datasets verified the effectiveness of the algorithm.},
  archive      = {J_AIR},
  author       = {Wang, Zhongyi and Yang, Yonghui and Wang, Luyao},
  doi          = {10.1007/s10462-024-10856-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Projection generalized correntropy twin support vector regression},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rotation invariance and equivariance in 3D deep learning: A
survey. <em>AIR</em>, <em>57</em>(7), 1–52. (<a
href="https://doi.org/10.1007/s10462-024-10741-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) in 3D scenes show a strong capability of extracting high-level semantic features and significantly promote research in the 3D field. 3D shapes and scenes often exhibit complicated transformation symmetries, where rotation is a challenging and necessary subject. To this end, many rotation invariant and equivariant methods have been proposed. In this survey, we systematically organize and comprehensively overview all methods. First, we rewrite the previous definition of rotation invariance and equivariance by classifying them into weak and strong categories. Second, we provide a unified theoretical framework to analyze these methods, especially weak rotation invariant and equivariant ones that are seldom analyzed theoretically. We then divide existing methods into two main categories, i.e., rotation invariant ones and rotation equivariant ones, which are further subclassified in terms of manipulating input ways and basic equivariant block structures, respectively. In each subcategory, their common essence is highlighted, a couple of representative methods are analyzed, and insightful comments on their pros and cons are given. Furthermore, we deliver a general overview of relevant applications and datasets for two popular tasks of 3D semantic understanding and molecule-related. Finally, we provide several open problems and future research directions based on challenges and difficulties in ongoing research.},
  archive      = {J_AIR},
  author       = {Fei, Jiajun and Deng, Zhidong},
  doi          = {10.1007/s10462-024-10741-2},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Rotation invariance and equivariance in 3D deep learning: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on the chicken swarm optimization
algorithm and its applications: State-of-the-art and research
challenges. <em>AIR</em>, <em>57</em>(7), 1–63. (<a
href="https://doi.org/10.1007/s10462-024-10786-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of optimization theory and the algorithms that are generated from it has increased along with science and technology&#39;s continued advancement. Numerous issues in daily life can be categorized as combinatorial optimization issues. Swarm intelligence optimization algorithms have been successful in machine learning, process control, and engineering prediction throughout the years and have been shown to be efficient in handling combinatorial optimization issues. An intelligent optimization system called the chicken swarm optimization algorithm (CSO) mimics the organic behavior of flocks of chickens. In the benchmark problem&#39;s optimization process as the objective function, it outperforms several popular intelligent optimization methods like PSO. The concept and advancement of the flock optimization algorithm, the comparison with other meta-heuristic algorithms, and the development trend are reviewed in order to further enhance the search performance of the algorithm and quicken the research and application process of the algorithm. The fundamental algorithm model is first described, and the enhanced chicken swarm optimization algorithm based on algorithm parameters, chaos and quantum optimization, learning strategy, and population diversity is then categorized and summarized using both domestic and international literature. The use of group optimization algorithms in the areas of feature extraction, image processing, robotic engineering, wireless sensor networks, and power. Second, it is evaluated in terms of benefits, drawbacks, and application in comparison to other meta-heuristic algorithms. Finally, the direction of flock optimization algorithm research and development is anticipated.},
  archive      = {J_AIR},
  author       = {Chen, Binhe and Cao, Li and Chen, Changzu and Chen, Yaodan and Yue, Yinggao},
  doi          = {10.1007/s10462-024-10786-3},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on the chicken swarm optimization algorithm and its applications: State-of-the-art and research challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic YOLO for small underwater object detection.
<em>AIR</em>, <em>57</em>(7), 1–23. (<a
href="https://doi.org/10.1007/s10462-024-10788-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practical application of object detection inevitably encounters challenges posed by small objects. In underwater object detection, a crucial method for marine exploration, the presence of small objects in underwater environments significantly hampers the performance of detection. In this paper, a dynamic YOLO detector is proposed as a solution to alleviate this problem. Specifically, a light-weight backbone network is first constructed based on deformable convolution v3, with some specialized designs for small object detection. Secondly, a unified feature fusion framework based on channel-wise, scale-wise, and spatial-aware attention is proposed to fuse feature maps from different scales. This is particularly critical for detecting small objects since it allows us to fully exploit the enhanced capabilities offered by our proposed backbone network. Finally, a simple but effective detection head is designed to handle the conflict between classification and localization by disentangling and aligning the two tasks. Extensive experiments are conducted on benchmark datasets to demonstrate the effectiveness of the proposed model. Without bells and whistles, dynamic YOLO outperforms the recent state-of-the-art methods by a large margin of $$+\,0.8$$ AP and $$+\,1.8$$ $$\text {AP}_{S}$$ on the DUO dataset. Experimental results on Pascal VOC and MS COCO datasets also demonstrate the superiority of the proposed method. At last, ablation studies are conducted on DUO dataset to validate the effectiveness and efficiency of each design in dynamic YOLO. Source code will be available at https://github.com/chenjie04/Dynamic-YOLO .},
  archive      = {J_AIR},
  author       = {Chen, Jie and Er, Meng Joo},
  doi          = {10.1007/s10462-024-10788-1},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dynamic YOLO for small underwater object detection},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new deep neural network for forecasting: Deep dendritic
artificial neural network. <em>AIR</em>, <em>57</em>(7), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10790-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep artificial neural networks have become a good alternative to classical forecasting methods in solving forecasting problems. Popular deep neural networks classically use additive aggregation functions in their cell structures. It is available in the literature that the use of multiplicative aggregation functions in shallow artificial neural networks produces successful results for the forecasting problem. A type of high-order shallow artificial neural network that uses multiplicative aggregation functions is the dendritic neuron model artificial neural network, which has successful forecasting performance. In this study, the transformation of the dendritic neuron model turned into a multi-output architecture. A new dendritic cell based on the multi-output dendritic neuron model and a new deep artificial neural network is proposed. The training of this new deep dendritic artificial neural network is carried out with the differential evolution algorithm. The forecasting performance of the deep dendritic artificial neural network is compared with basic classical forecasting methods and some recent shallow and deep artificial neural networks over stock market time series. As a result, it has been observed that deep dendritic artificial neural network produces very successful forecasting results for the forecasting problem.},
  archive      = {J_AIR},
  author       = {Egrioglu, Erol and Bas, Eren},
  doi          = {10.1007/s10462-024-10790-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A new deep neural network for forecasting: Deep dendritic artificial neural network},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equilibrium optimizer-based harmony search algorithm with
nonlinear dynamic domains and its application to real-world optimization
problems. <em>AIR</em>, <em>57</em>(7), 1–89. (<a
href="https://doi.org/10.1007/s10462-024-10793-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harmony Search (HS) algorithm is a swarm intelligence algorithm inspired by musical improvisation. Although HS has been applied to various engineering problems, it faces challenges such as getting trapped in local optima, slow convergence speed, and low optimization accuracy when applied to complex problems. To address these issues, this paper proposes an improved version of HS called Equilibrium Optimization-based Harmony Search Algorithm with Nonlinear Dynamic Domains (EO-HS-NDD). EO-HS-NDD integrates multiple leadership-guided strategies from the Equilibrium Optimizer (EO) algorithm, using harmony memory considering disharmony and historical harmony memory, while leveraging the hidden guidance direction information from the Equilibrium Optimizer. Additionally, the algorithm designs a nonlinear dynamic convergence domain to adaptively adjust the search space size and accelerate convergence speed. Furthermore, to balance exploration and exploitation capabilities, appropriate adaptive adjustments are made to Harmony Memory Considering Rate (HMCR) and Pitch Adjustment Rate (PAR). Experimental validation on the CEC2017 test function set demonstrates that EO-HS-NDD outperforms HS and nine other HS variants in terms of robustness, convergence speed, and optimization accuracy. Comparisons with advanced versions of the Differential Evolution (DE) algorithm also indicate that EO-HS-NDD exhibits superior solving capabilities. Moreover, EO-HS-NDD is applied to solve 15 real-world optimization problems from CEC2020 and compared with advanced algorithms from the CEC2020 competition. The experimental results show that EO-HS-NDD performs well in solving real-world optimization problems.},
  archive      = {J_AIR},
  author       = {Wang, Jinglin and Ouyang, Haibin and Li, Steven and Ding, Weiping and Gao, Liqun},
  doi          = {10.1007/s10462-024-10793-4},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-89},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Equilibrium optimizer-based harmony search algorithm with nonlinear dynamic domains and its application to real-world optimization problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent analysis of android application privacy policy
and permission consistency. <em>AIR</em>, <em>57</em>(7), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10798-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of mobile devices, mobile applications bring a lot of convenience to people’s lives. The abuse of mobile device permissions is prone to the risk of privacy leakage. The existing detection technology can detect the inconsistency between the declared authority and the actual use authority. But using the third-party privacy policy as the analysis basis for SDK permissions will result in a large set of extracted declaration permissions, which will lead to identifying risky applications as normal applications during consistency comparison. The prevailing approach involves utilizing models based on TextCNN to extract information from privacy policies. However, the training of TextCNN relies on large-scale annotated datasets, leading to high costs. This paper uses BERT as the word vector extraction model to obtain private phrases from the privacy policy. And then we use cosine similarity to automatically filter permission phrase samples, reducing the workload of manual labeling. On the other hand, existing methods do not support the analysis of Chinese privacy policies. In order to solve the problem of consistency judgment between Chinese privacy policy and permission usage, we implement a BERT-based Android privacy policy and permission usage consistency analysis engine. The engine first uses static analysis to obtain the permission list of Android applications, and then combines the BERT model to achieve consistency analysis. After functional and speed testing, we found that the engine can successfully run the consistency analysis function of Chinese declaration permissions and usage permissions, and it is better than the existing detection methods.},
  archive      = {J_AIR},
  author       = {Tu, Tengfei and Zhang, Hua and Gong, Bei and Du, Daizhong and Wen, Qiaoyan},
  doi          = {10.1007/s10462-024-10798-z},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent analysis of android application privacy policy and permission consistency},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced adaptive-convergence in harris’ hawks optimization
algorithm. <em>AIR</em>, <em>57</em>(7), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10802-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel enhanced adaptive-convergence in Harris’ hawks optimization algorithm (EAHHO). In EAHHO, considering that Harris’ hawks will adopt different perching strategies and chasing styles according to the value of the escaping energy parameter E, nonlinear adaptive-convergence factor a is designed and adjusted to enhance the convergence and robustness of the algorithm. Moreover, the convergence and stability of EAHHO are proved mathematically by using the Markov chain theory and Lyapunov stability theory respectively. Moreover, numerical simulation results of 14 HHOs with different nonlinear convergence factors on 23 benchmark functions show that the nonlinear convergence factor of EAHHO is applicable to challenging problems with unknown search spaces, and the comparisons with the selected well-established algorithms on 56 test functions demonstrate that EAHHO performs competitively and effectively. Finally, the experiment results show that EAHHO algorithm also has a good performance to solve the optimization problems with relatively high dimensions and graph size of Internet of Vehicles routing problem.},
  archive      = {J_AIR},
  author       = {Mao, Mingxuan and Gui, Diyu},
  doi          = {10.1007/s10462-024-10802-6},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhanced adaptive-convergence in harris’ hawks optimization algorithm},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the capabilities of explainable few-shot learning
in remote sensing. <em>AIR</em>, <em>57</em>(7), 1–55. (<a
href="https://doi.org/10.1007/s10462-024-10803-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements have significantly improved the efficiency and effectiveness of deep learning methods for image-based remote sensing tasks. However, the requirement for large amounts of labeled data can limit the applicability of deep neural networks to existing remote sensing datasets. To overcome this challenge, few-shot learning has emerged as a valuable approach for enabling learning with limited data. While previous research has evaluated the effectiveness of few-shot learning methods on satellite-based datasets, little attention has been paid to exploring the applications of these methods to datasets obtained from Unmanned Aerial Vehicles (UAVs), which are increasingly used in remote sensing studies. In this review, we provide an up-to-date overview of both existing and newly proposed few-shot classification techniques, along with appropriate datasets that are used for both satellite-based and UAV-based data. We demonstrate few-shot learning can effectively handle the diverse perspectives in remote sensing data. As an example application, we evaluate state-of-the-art approaches on a UAV disaster scene dataset, yielding promising results. Furthermore, we highlight the significance of incorporating explainable AI (XAI) techniques into few-shot models. In remote sensing, where decisions based on model predictions can have significant consequences, such as in natural disaster response or environmental monitoring, the transparency provided by XAI is crucial. Techniques like attention maps and prototype analysis can help clarify the decision-making processes of these complex models, enhancing their reliability. We identify key challenges including developing flexible few-shot methods to handle diverse remote sensing data effectively. This review aims to equip researchers with an improved understanding of few-shot learning’s capabilities and limitations in remote sensing, while pointing out open issues to guide progress in efficient, reliable and interpretable data-efficient techniques.},
  archive      = {J_AIR},
  author       = {Lee, Gao Yu and Dam, Tanmoy and Ferdaus, Md. Meftahul and Poenar, Daniel Puiu and Duong, Vu N.},
  doi          = {10.1007/s10462-024-10803-5},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Unlocking the capabilities of explainable few-shot learning in remote sensing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Plant disease management: A fine-tuned enhanced CNN approach
with mobile app integration for early detection and classification.
<em>AIR</em>, <em>57</em>(7), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10809-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Farmers face the formidable challenge of meeting the increasing demands of a rapidly growing global population for agricultural products, while plant diseases continue to wreak havoc on food production. Despite substantial investments in disease management, agriculturists are increasingly turning to advanced technology for more efficient disease control. This paper addresses this critical issue through an exploration of a deep learning-based approach to disease detection. Utilizing an optimized Convolutional Neural Network (E-CNN) architecture, the study concentrates on the early detection of prevalent leaf diseases in Apple, Corn, and Potato crops under various conditions. The research conducts a thorough performance analysis, emphasizing the impact of hyperparameters on plant disease detection across these three distinct crops. Multiple machine learning and pre-trained deep learning models are considered, comparing their performance after fine-tuning their parameters. Additionally, the study investigates the influence of data augmentation on detection accuracy. The experimental results underscore the effectiveness of our fine-tuned enhanced CNN model, achieving an impressive 98.17% accuracy in fungal classes. This research aims to pave the way for more efficient plant disease management and, ultimately, to enhance agricultural productivity in the face of mounting global challenges. To improve accessibility for farmers, the developed model seamlessly integrates with a mobile application, offering immediate results upon image upload or capture. In case of a detected disease, the application provides detailed information on the disease, its causes, and available treatment options.},
  archive      = {J_AIR},
  author       = {Iftikhar, Mudassir and Kandhro, Irfan Ali and Kausar, Neha and Kehar, Asadullah and Uddin, Mueen and Dandoush, Abdulhalim},
  doi          = {10.1007/s10462-024-10809-z},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Plant disease management: A fine-tuned enhanced CNN approach with mobile app integration for early detection and classification},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An experimental evaluation of deep reinforcement learning
algorithms for HVAC control. <em>AIR</em>, <em>57</em>(7), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10819-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heating, ventilation, and air conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers’ robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.},
  archive      = {J_AIR},
  author       = {Manjavacas, Antonio and Campoy-Nieves, Alejandro and Jiménez-Raboso, Javier and Molina-Solana, Miguel and Gómez-Romero, Juan},
  doi          = {10.1007/s10462-024-10819-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An experimental evaluation of deep reinforcement learning algorithms for HVAC control},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal vision-based human action recognition using deep
learning: A review. <em>AIR</em>, <em>57</em>(7), 1–85. (<a
href="https://doi.org/10.1007/s10462-024-10730-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based Human Action Recognition (HAR) is a hot topic in computer vision. Recently, deep-based HAR has shown promising results. HAR using a single data modality is a common approach; however, the fusion of different data sources essentially conveys complementary information and improves the results. This paper comprehensively reviews deep-based HAR methods using multiple visual data modalities. The main contribution of this paper is categorizing existing methods into four levels, which provides an in-depth and comparable analysis of approaches in various aspects. So, at the first level, proposed methods are categorized based on the employed modalities. At the second level, methods categorized in the first level are classified based on the employment of complete modalities or working with missing modalities at the test time. At the third level, complete and missing modality branches are categorized based on existing approaches. Finally, similar frameworks in the third category are grouped together. In addition, a comprehensive comparison is provided for publicly available benchmark datasets, which helps to compare and choose suitable datasets for a task or to develop new datasets. This paper also compares the performance of state-of-the-art methods on benchmark datasets. The review concludes by highlighting several future directions.},
  archive      = {J_AIR},
  author       = {Shafizadegan, Fatemeh and Naghsh-Nilchi, Ahmad R. and Shabaninia, Elham},
  doi          = {10.1007/s10462-024-10730-5},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-85},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multimodal vision-based human action recognition using deep learning: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: From understanding diseases to drug design:
Can artificial intelligence bridge the gap? <em>AIR</em>,
<em>57</em>(7), 1–2. (<a
href="https://doi.org/10.1007/s10462-024-10765-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIR},
  author       = {Pushkaran, Anju Choorakottayil and Arabi, Alya A.},
  doi          = {10.1007/s10462-024-10765-8},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-2},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Correction to: from understanding diseases to drug design: can artificial intelligence bridge the gap?},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multifaceted survey on privacy preservation of federated
learning: Progress, challenges, and opportunities. <em>AIR</em>,
<em>57</em>(7), 1–67. (<a
href="https://doi.org/10.1007/s10462-024-10766-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) refers to a system of training and stabilizing local machine learning models at the global level by aggregating the learning gradients of the models. It reduces the concern of sharing the private data of participating entities for statistical analysis to be carried out at the server. It allows participating entities called clients or users to infer useful information from their raw data. As a consequence, the need to share their confidential information with any other entity or the central entity called server is eliminated. FL can be clearly interpreted as a privacy-preserving version of traditional machine learning and deep learning algorithms. However, despite this being an efficient distributed training scheme, the client’s sensitive information can still be exposed to various security threats from the shared parameters. Since data has always been a major priority for any user or organization, this article is primarily concerned with discussing the significant problems and issues relevant to the preservation of data privacy and the viability and feasibility of several proposed solutions in the FL context. In this work, we conduct a detailed study on FL, the categorization of FL, the challenges of FL, and various attacks that can be executed to disclose the users’ sensitive data used during learning. In this survey, we review and compare different privacy solutions for FL to prevent data leakage and discuss secret sharing (SS)-based security solutions for FL proposed by various researchers in concise form. We also briefly discuss quantum federated learning (QFL) and privacy-preservation techniques in QFL. In addition to these, a comparison and contrast of several survey works on FL is included in this work. We highlight the major applications based on FL. We discuss certain future directions pertaining to the open issues in the field of FL and finally conclude our work.},
  archive      = {J_AIR},
  author       = {Saha, Sanchita and Hota, Ashlesha and Chattopadhyay, Arup Kumar and Nag, Amitava and Nandi, Sukumar},
  doi          = {10.1007/s10462-024-10766-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A multifaceted survey on privacy preservation of federated learning: Progress, challenges, and opportunities},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing trustworthy deep learning for image classification
against evasion attacks: A systematic literature review. <em>AIR</em>,
<em>57</em>(7), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10777-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving field of Deep Learning (DL), the trustworthiness of models is essential for their effective application in critical domains like healthcare and autonomous systems. Trustworthiness in DL encompasses aspects such as reliability, fairness, and transparency, which are crucial for its real-world impact and acceptance. However, the development of trustworthy DL models faces significant challenges. This is notably due to adversarial examples, a sophisticated form of evasion attack in adversarial machine learning (AML), which subtly alter inputs to deceive these models and pose a major threat to their safety and reliability. The current body of research primarily focuses on defensive measures, such as enhancing the robustness of models or implementing explainable AI techniques. However, this approach often neglects to address the fundamental vulnerabilities that adversaries exploit. As a result, the field tends to concentrate more on counteracting measures rather than gaining an in-depth understanding of the vulnerabilities and attack strategies inherent in DL systems. This gap in comprehensive understanding impedes the formulation of effective defense mechanisms. This research aims to shift the focus from predominantly defensive strategies toward a more extensive comprehension of adversarial techniques and the innate vulnerabilities of DL models. We undertake this by conducting a thorough systematic literature review, encompassing 49 diverse studies from the previous decade. Our findings reveal the key characteristics of adversarial examples that enable their success against image classification-based DL models. Building on these insights, we propose the Transferable Pretrained Adversarial Deep Learning framework (TPre-ADL). This conceptual model aims to rectify the deficiencies in current defense strategies by incorporating the analyzed traits of adversarial examples, potentially enhancing the robustness and trustworthiness of DL models.},
  archive      = {J_AIR},
  author       = {Akhtom, Dua’a Mkhiemir and Singh, Manmeet Mahinderjit and XinYing, Chew},
  doi          = {10.1007/s10462-024-10777-4},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing trustworthy deep learning for image classification against evasion attacks: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive assessment of artificial intelligence
applications for cancer diagnosis. <em>AIR</em>, <em>57</em>(7), 1–52.
(<a href="https://doi.org/10.1007/s10462-024-10783-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is being used increasingly to detect fatal diseases such as cancer. The potential reduction in human error, rapid diagnosis, and consistency of judgment are the primary motives for using these applications. Artificial Neural Networks and Convolution Neural Networks are popular AI techniques being increasingly used in diagnosis. Numerous academics have explored and evaluated AI methods used in the detection of various cancer types for comparison and analysis. This study presents a thorough evaluation of the AI techniques used in cancer detection based on extensively researched studies and research trials published on the subject. The manuscript offers a thorough evaluation and comparison of the AI methods applied to the detection of five primary cancer types: breast cancer, lung cancer, colorectal cancer, prostate cancer, skin cancer, and digestive cancer. To determine how well these models compare with medical professionals’ judgments, the opinions of developed models and of experts are compared and provided in this paper.},
  archive      = {J_AIR},
  author       = {Singh, Gaurav and Kamalja, Anushka and Patil, Rohit and Karwa, Ashutosh and Tripathi, Akansha and Chavan, Pallavi},
  doi          = {10.1007/s10462-024-10783-6},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive assessment of artificial intelligence applications for cancer diagnosis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and privacy-preserving collaborative training: A
comprehensive survey. <em>AIR</em>, <em>57</em>(7), 1–64. (<a
href="https://doi.org/10.1007/s10462-024-10797-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing numbers of artificial intelligence systems are employing collaborative machine learning techniques, such as federated learning, to build a shared powerful deep model among participants, while keeping their training data locally. However, concerns about integrity and privacy in such systems have significantly hindered the use of collaborative learning systems. Therefore, numerous efforts have been presented to preserve the model’s integrity and reduce the privacy leakage of training data throughout the training phase of various collaborative learning systems. This survey seeks to provide a systematic and comprehensive evaluation of security and privacy studies in collaborative training, in contrast to prior surveys that only focus on one single collaborative learning system. Our survey begins with an overview of collaborative learning systems from various perspectives. Then, we systematically summarize the integrity and privacy risks of collaborative learning systems. In particular, we describe state-of-the-art integrity attacks (e.g., Byzantine, backdoor, and adversarial attacks) and privacy attacks (e.g., membership, property, and sample inference attacks), as well as the associated countermeasures. We additionally provide an analysis of open problems to motivate possible future studies.},
  archive      = {J_AIR},
  author       = {Yang, Fei and Zhang, Xu and Guo, Shangwei and Chen, Daiyuan and Gan, Yan and Xiang, Tao and Liu, Yang},
  doi          = {10.1007/s10462-024-10797-0},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Robust and privacy-preserving collaborative training: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software advancements in automatic epilepsy diagnosis and
seizure detection: 10-year review. <em>AIR</em>, <em>57</em>(7), 1–66.
(<a href="https://doi.org/10.1007/s10462-024-10799-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a chronic neurological disorder that may be diagnosed and monitored using routine diagnostic tests like Electroencephalography (EEG). However, manual introspection and analysis of EEG signals is presently difficult and repetitive task even for experienced neuro-technologists with high false-positive rates and inter- and intra-rater reliability. Software advancements using Artificial Intelligence (AI) algorithms have the potential to early detect and predict abnormal patterns observed in EEG signals. The present review focuses on systematically reporting software advancements and their implementation using hardware systems in automatic epilepsy diagnosis and seizure detection for the past 10 years. Traditional, hybrid, and end-to-end AI-based pipelines and associated EEG datasets have been discussed. The review summarizes and compares reported articles, datasets, and patents through various subjective and objective parameters in this field. Latest advancements demonstrate that AI-based pipelines can reduce the introspection time by at least 50% without compromising the diagnostic accuracy or abnormal event detection. A significant rise in hardware implementation of software-based pipelines, end-to-end deep learning architectures for real-time analysis, and granted patents has been noticed since 2011. More than twenty-eight datasets have been developed to automatically diagnose epileptic EEG signals from 2001 to 2023. Extensive analysis using explainability tools, cross-dataset generalizations, reproducibility analysis, and ablation experiments can further improve the existing AI-based pipelines in this field. There is a need for the development of standardized protocols for data collection and its AI pipeline for a robust, inter- and intra-rater reliability-free, and real-time automatic epilepsy diagnosis.},
  archive      = {J_AIR},
  author       = {Handa, Palak and Lavanya and Goel, Nidhi and Garg, Neeta},
  doi          = {10.1007/s10462-024-10799-y},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Software advancements in automatic epilepsy diagnosis and seizure detection: 10-year review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big data and predictive analytics: A sytematic review of
applications. <em>AIR</em>, <em>57</em>(7), 1–77. (<a
href="https://doi.org/10.1007/s10462-024-10811-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data involves processing vast amounts of data using advanced techniques. Its potential is harnessed for predictive analytics, a sophisticated branch that anticipates unknown future events by discerning patterns observed in historical data. Various techniques obtained from modeling, data mining, statistics, artificial intelligence, and machine learning are employed to analyze available history to extract discriminative patterns for predictors. This study aims to analyze the main research approaches on Big Data Predictive Analytics (BDPA) based on very up-to-date published articles from 2014 to 2023. In this article, we fully concentrate on predictive analytics using big data mining techniques, where we perform a Systematic Literature Review (SLR) by reviewing 109 articles. Based on the application and content of current studies, we introduce taxonomy including seven major categories of industrial, e-commerce, smart healthcare, smart agriculture, smart city, Information and Communications Technologies (ICT), and weather. The benefits and weaknesses of each approach, potentially important changes, and open issues, in addition to future paths, are discussed. The compiled SLR not only extends on BDPA’s strengths, open issues, and future works but also detects the need for optimizing the insufficient metrics in big data applications, such as timeliness, accuracy, and scalability, which would enable organizations to apply big data to shift from retrospective analytics to prospective predictive if fulfilled.},
  archive      = {J_AIR},
  author       = {Jamarani, Amirhossein and Haddadi, Saeid and Sarvizadeh, Raheleh and Haghi Kashani, Mostafa and Akbari, Mohammad and Moradi, Saeed},
  doi          = {10.1007/s10462-024-10811-5},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-77},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Big data and predictive analytics: A sytematic review of applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The application of evolutionary computation in generative
adversarial networks (GANs): A systematic literature survey.
<em>AIR</em>, <em>57</em>(7), 1–87. (<a
href="https://doi.org/10.1007/s10462-024-10818-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a subfield of deep learning (DL), generative adversarial networks (GANs) have produced impressive generative results by applying deep generative models to create synthetic data and by performing an adversarial training process. Nevertheless, numerous issues related to the instability of training need to be urgently addressed. Evolutionary computation (EC), using the corresponding paradigm of biological evolution, overcomes these problems and improves evolutionary-based GANs’ ability to deal with real-world applications. Therefore, this paper presents a systematic literature survey combining EC and GANs. First, the basic theories of GANs and EC are analyzed and summarized. Second, to provide readers with a comprehensive view, this paper outlines the recent advances in combining EC and GANs after detailed classification and introduces each of them. These classifications include evolutionary GANs and their variants, GANs with evolutionary strategies and differential evolution, GANs combined with neuroevolution, evolutionary GANs related to different optimization problems, and applications of evolutionary GANs. Detailed information on the evaluation metrics, network structures, and comparisons of these models is presented in several tables. Finally, future directions and possible perspectives for further development are discussed.},
  archive      = {J_AIR},
  author       = {Wang, Yong and Zhang, Qian and Wang, Gai-Ge and Cheng, Honglei},
  doi          = {10.1007/s10462-024-10818-y},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-87},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The application of evolutionary computation in generative adversarial networks (GANs): A systematic literature survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of safety and trustworthiness of large language
models through the lens of verification and validation. <em>AIR</em>,
<em>57</em>(7), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10824-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&amp;V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&amp;V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.},
  archive      = {J_AIR},
  author       = {Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and Cai, Kaiwen and Zhang, Yanghao and Wu, Sihao and Xu, Peipei and Wu, Dengyu and Freitas, Andre and Mustafa, Mustafa A.},
  doi          = {10.1007/s10462-024-10824-0},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of safety and trustworthiness of large language models through the lens of verification and validation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustworthy cyber-physical power systems using AI: Dueling
algorithms for PMU anomaly detection and cybersecurity. <em>AIR</em>,
<em>57</em>(7), 1–47. (<a
href="https://doi.org/10.1007/s10462-024-10827-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy systems require radical changes due to the conflicting needs of combating climate change and meeting rising energy demands. These revolutionary decentralization, decarbonization, and digitalization techniques have ushered in a new global energy paradigm. Waves of disruption have been felt across the electricity industry as the digitalization journey in this sector has converged with advances in artificial intelligence (AI). However, there are risks involved. As AI becomes more established, new security threats have emerged. Among the most important is the cyber-physical protection of critical infrastructure, such as the power grid. This article focuses on dueling AI algorithms designed to investigate the trustworthiness of power systems’ cyber-physical security under various scenarios using the phasor measurement units (PMU) use case. Particularly in PMU operations, the focus is on areas that manage sensitive data vital to power system operators’ activities. The initial stage deals with anomaly detection applied to energy systems and PMUs, while the subsequent stage examines adversarial attacks targeting AI models. At this stage, evaluations of the Madry attack, basic iterative method (BIM), momentum iterative method (MIM), and projected gradient descend (PGD) are carried out, which are all powerful adversarial techniques that may compromise anomaly detection methods. The final stage addresses mitigation methods for AI-based cyberattacks. All these three stages represent various uses of AI and constitute the dueling AI algorithm convention that is conceptualised and demonstrated in this work. According to the findings of this study, it is essential to investigate the trade-off between the accuracy of AI-based anomaly detection models and their digital immutability against potential cyberphysical attacks in terms of trustworthiness for the critical infrastructure under consideration.},
  archive      = {J_AIR},
  author       = {Cali, Umit and Catak, Ferhat Ozgur and Halden, Ugur},
  doi          = {10.1007/s10462-024-10827-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Trustworthy cyber-physical power systems using AI: Dueling algorithms for PMU anomaly detection and cybersecurity},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive study to learn the impact of augmented
reality and haptic interaction in ultrasound-guided percutaneous liver
biopsy training and education. <em>AIR</em>, <em>57</em>(7), 1–36. (<a
href="https://doi.org/10.1007/s10462-024-10791-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation based training methods are gaining popularity as they could provide a platform for practitioners to gain hands-on experience without causing ethical issues. By combining augmented reality (AR) and haptics, a training method for percutaneous liver biopsy (PLB) could be developed providing realistic scenarios, and real-time visualization of the human anatomy and needle. Additionally, it could also provide real-time feedback to the practitioner. In this review, we describe the conventional PLB procedure, then discuss AR technology and its application in the field of medicine for image-guided therapies, especially, hepatic biopsy. Next, we summarize the associated devices, models and methods illustrating a few haptic simulators devised for training and gesture assessment. Lastly, we present a few potential approaches to integrate AR and haptic interaction to develop a PLB training simulator by accounting the existing challenges.},
  archive      = {J_AIR},
  author       = {Mangalote, Iffa Afsa Changaai and Aboumarzouk, Omar and Al-Ansari, Abdulla A. and Dakua, Sarada Prasad},
  doi          = {10.1007/s10462-024-10791-6},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive study to learn the impact of augmented reality and haptic interaction in ultrasound-guided percutaneous liver biopsy training and education},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning model using classification for diabetic
retinopathy detection: An overview. <em>AIR</em>, <em>57</em>(7), 1–28.
(<a href="https://doi.org/10.1007/s10462-024-10806-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of diabetic retinopathy is a serious disease for diabetics to minimize their sightlessness risks. The different approaches take a much longer time for a very large training dataset. In classifying to better the accuracy of diabetic retinopathy, a novel technique called MAP Concordance Regressive Camargo’s Index-Based Deep Multilayer Perceptive Learning Classification (MAPCRCI-DMPLC) has been introduced with minimum time consumption. The novel model of MAPCRCI-DMPLC comprises the input layer, hidden layers, and output layer for detecting diabetic retinopathy at an early stage through high accuracy and less moment consumption. The proposed MAPCRCI-DMPLC model collected the retinal fundus images from the dataset as input. After that, we carried out image preprocessing using the MAP-estimated local region filtering-based preprocessing technique in the first hidden layer. In the second hidden layer, Camargo’s index-based ROI extraction is performed to identify the infected region. Then, Concordance Correlative Regression is applied for texture feature extraction. Then the color feature is extracted, beginning the image. The features extracted to the output layer to classify the different levels of DR using the swish activation function through higher accuracy. An investigational assessment using a retinal image dataset on factors such as peak signal-to-noise ratio (PSNR), disease detection accuracy (DDA), false-positive rate (FPR), and disease detection time (DDT), regarding the quantity of retinal fundus images and image dimension. The quantitative and qualitatively analyzed outcome shows a better presentation of our proposed MAPCRCI-DMPLC technique when compared through the five state-of-the-art approaches.},
  archive      = {J_AIR},
  author       = {Muthusamy, Dharmalingam and Palani, Parimala},
  doi          = {10.1007/s10462-024-10806-2},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning model using classification for diabetic retinopathy detection: An overview},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic data navigation: Information value-based sample
selection. <em>AIR</em>, <em>57</em>(7), 1–17. (<a
href="https://doi.org/10.1007/s10462-024-10813-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence represents a rapidly expanding domain, with several industrial applications demonstrating its superiority over traditional techniques. Despite numerous advancements within the subfield of Machine Learning, it encounters persistent challenges, highlighting the importance of ongoing research efforts. Among its primary branches, this study delves into two categories, being Supervised and Reinforcement Learning, particularly addressing the common issue of data selection for training. The inherent variability in informational content among data points is apparent, wherein certain samples offer more valuable information to the neural network than others. However, evaluating the significance of various data points remains a non-trivial task, generating the need for a robust method to effectively prioritize samples. Drawing inspiration from Reinforcement Learning principles, this paper introduces a novel sample prioritization approach, applied to Supervised Learning scenarios, aimed at enhancing classification accuracy through strategic data navigation, while exploring the boundary between Reinforcement and Supervised Learning techniques. We provide a comprehensive description of our methodology while revealing the identification of an optimal prioritization balance and demonstrating its beneficial impact on model performance. Although classification accuracy serves as the primary validation metric, the concept of information density-based prioritization encompasses wider applicability. Additionally, the paper investigates parallels and distinctions between Reinforcement and Supervised Learning methods, declaring that the foundational principle is equally relevant, hence completely adaptable to Supervised Learning with appropriate adjustments due to different learning frameworks. The project page and source code are available at: https://csanadlb.github.io/sl_prioritized_sampling/ .},
  archive      = {J_AIR},
  author       = {Balogh, Csanád L. and Pelenczei, Bálint and Kővári, Bálint and Bécsi, Tamás},
  doi          = {10.1007/s10462-024-10813-3},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-17},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Strategic data navigation: Information value-based sample selection},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Silent no more: A comprehensive review of artificial
intelligence, deep learning, and machine learning in facilitating deaf
and mute communication. <em>AIR</em>, <em>57</em>(7), 1–46. (<a
href="https://doi.org/10.1007/s10462-024-10816-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People who often communicate via sign language are essential to our society and significantly contribute. They struggle with communication mostly because other people, who often do not understand sign language, cannot interact with them. It is necessary to develop a dependable system for automatic sign language recognition. This paper aims to provide a comprehensive review of the advancements in artificial intelligence (AI), deep learning (DL), and machine learning (ML) technologies that have been used to facilitate communication for individuals who are deaf and mute (D–M). This study explores various applications of these technologies, including sign language interpretation, speech recognition, and text-to-speech synthesis. By examining the current state of research and development in AI, ML, and DL for the D–M field, the survey sheds light on the potential and challenges faced in utilizing AI, deep learning, and ML to bridge the communication gap for the D–M community. The findings of this survey will contribute to a greater understanding of the potential impact of these technologies in improving access to communication for individuals who are D–M, thereby aiding in the development of more inclusive and accessible solutions.},
  archive      = {J_AIR},
  author       = {ZainEldin, Hanaa and Gamel, Samah A. and Talaat, Fatma M. and Aljohani, Mansourah and Baghdadi, Nadiah A. and Malki, Amer and Badawy, Mahmoud and Elhosseini, Mostafa A.},
  doi          = {10.1007/s10462-024-10816-0},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Silent no more: A comprehensive review of artificial intelligence, deep learning, and machine learning in facilitating deaf and mute communication},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphs in clusters: A hybrid approach to unsupervised
extractive long document summarization using language models.
<em>AIR</em>, <em>57</em>(7), 1–18. (<a
href="https://doi.org/10.1007/s10462-024-10828-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective summarization of long documents is a challenging task. When addressing this challenge, Graph and Cluster-Based methods stand out as effective unsupervised solutions. Graph-Based Unsupervised methods are widely employed for summarization due to their success in identifying relationships within documents. Cluster-Based methods excel in minimizing redundancy by grouping similar content together before generating a concise summary. Therefore, this paper merges Cluster-Based and Graph-Based methods by applying language models for Unsupervised Extractive Summarization of long documents. The approach simultaneously extracts key information while minimizing redundancy. First, we use BERT-based sentence embeddings to create sentence clusters using k-means clustering and select the optimum number of clusters using the elbow method to ensure that sentences are categorized based on their semantic similarities. Then, the TextRank algorithm is employed within each cluster to rank sentences based on their importance and representativeness. Finally, the total similarity score of the graph is used to rank the clusters and eliminate less important sentence groups. Our method achieves comparable or better summary quality and reduced redundancy compared to both individual Cluster-Based and Graph-Based methods, as well as other supervised and Unsupervised baseline models across diverse datasets.},
  archive      = {J_AIR},
  author       = {Gokhan, Tuba and Price, Malcolm James and Lee, Mark},
  doi          = {10.1007/s10462-024-10828-w},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-18},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graphs in clusters: A hybrid approach to unsupervised extractive long document summarization using language models},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample intelligence-based progressive hedging algorithms for
the stochastic capacitated reliable facility location problem.
<em>AIR</em>, <em>57</em>(6), 1–50. (<a
href="https://doi.org/10.1007/s10462-024-10755-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting facility locations requires significant investment to anticipate and prepare for disruptive events like earthquakes, floods, or labor strikes. In practice, location choices account for facility capacities, which often cannot change during disruptions. When a facility fails, demand transfers to others only if spare capacity exists. Thus, capacitated reliable facility location problems (CRFLP) under uncertainty are more complex than uncapacitated versions. To manage uncertainty and decide effectively, stochastic programming (SP) methods are often employed. Two commonly used SP methods are approximation methods, i.e., Sample Average Approximation (SAA), and decomposition methods, i.e., Progressive Hedging Algorithm (PHA). SAA needs large sample sizes for performance guarantee and turn into computationally intractable. On the other hand, PHA, as an exact method for convex problems, suffers from the need to iteratively solve numerous sub-problems which are computationally costly. In this paper, we developed two novel algorithms integrating SAA and PHA for solving the CRFLP under uncertainty. The developed methods are innovative in that they blend the complementary aspects of PHA and SAA in terms of exactness and computational efficiency, respectively. Further, the developed methods are practical in that they allow the specialist to adjust the tradeoff between the exactness and speed of attaining a solution. We present the effectiveness of the developed integrated approaches, Sampling Based Progressive Hedging Algorithm (SBPHA) and Discarding SBPHA (d-SBPHA), over the pure strategies (i.e. SAA). The validation of the methods is demonstrated through two-stage stochastic CRFLP. Promising results are attained for CRFLP, and the method has great potential to be generalized for SP problems.},
  archive      = {J_AIR},
  author       = {Aydin, Nezir and Murat, Alper and Mordukhovich, Boris S.},
  doi          = {10.1007/s10462-024-10755-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Sample intelligence-based progressive hedging algorithms for the stochastic capacitated reliable facility location problem},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fermatean fuzzy sets and its extensions: A systematic
literature review. <em>AIR</em>, <em>57</em>(6), 1–54. (<a
href="https://doi.org/10.1007/s10462-024-10761-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fermatean Fuzzy Set (FFS) theory emerges as a crucial and prevalent tool in addressing uncertainty across diverse domains. Despite its recognized utility in managing ambiguous information, recent research lacks a comprehensive analysis of key FFS areas, applications, research gaps, and outcomes. This study, conducted through the Scientific Procedures and Rationales for Systematic Literature Reviews (SPAR-4-SLR) protocol, delves into an exploration of the FFS literature, reviewing 135 relevant articles. The documents are meticulously analyzed based on their integrated methodologies, Aggregation Operators (AOs), linguistic sets, and extensions. Additionally, a thematic analysis, facilitated by the Bibliometrix tool, is presented to provide nuanced insights into future research directions and crucial areas within the literature. The study unveils valuable findings, including the integration of linguistic variables with interval-valued FFS, fostering robust environments for dynamic decision-making—a mere glimpse of the potential directions for future research. The gaps and future directions section further articulates recommendations, offering a structured foundation for researchers to enhance their understanding of FFS and chart future studies confidently.},
  archive      = {J_AIR},
  author       = {Büyüközkan, Gülçin and Uztürk, Deniz and Ilıcak, Öykü},
  doi          = {10.1007/s10462-024-10761-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fermatean fuzzy sets and its extensions: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive analysis of digital twins in smart cities: A
4200-paper bibliometric study. <em>AIR</em>, <em>57</em>(6), 1–54. (<a
href="https://doi.org/10.1007/s10462-024-10781-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey paper comprehensively reviews Digital Twin (DT) technology, a virtual representation of a physical object or system, pivotal in Smart Cities for enhanced urban management. It explores DT&#39;s integration with Machine Learning for predictive analysis, IoT for real-time data, and its significant role in Smart City development. Addressing the gap in existing literature, this survey analyzes over 4,220 articles from the Web of Science, focusing on unique aspects like datasets, platforms, and performance metrics. Unlike other studies in the field, this research paper distinguishes itself through its comprehensive and bibliometric approach, analyzing over 4,220 articles and focusing on unique aspects like datasets, platforms, and performance metrics. This approach offers an unparalleled depth of analysis, enhancing the understanding of Digital Twin technology in Smart City development and setting a new benchmark in scholarly research in this domain. The study systematically identifies emerging trends and thematic topics, utilizing tools like VOSviewer for data visualization. Key findings include publication trends, prolific authors, and thematic clusters in research. The paper highlights the importance of DT in various urban applications, discusses challenges and limitations, and presents case studies showcasing successful implementations. Distinguishing from prior studies, it offers detailed insights into emerging trends, future research directions, and the evolving role of policy and governance in DT development, thereby making a substantial contribution to the field.},
  archive      = {J_AIR},
  author       = {El-Agamy, Rasha F. and Sayed, Hanaa A. and AL Akhatatneh, Arwa M. and Aljohani, Mansourah and Elhosseini, Mostafa},
  doi          = {10.1007/s10462-024-10781-8},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive analysis of digital twins in smart cities: A 4200-paper bibliometric study},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New discrete-time zeroing neural network for solving
time-dependent linear equation with boundary constraint. <em>AIR</em>,
<em>57</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10462-024-10746-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, continuous- and discrete-time models of a zeroing neural network (ZNN) have been developed to provide online solutions for the time-dependent linear equation (TDLE) with boundary constraint. This paper presents a novel approach to address the bound-constrained TDLE (BCTDLE) problem by proposing a new discrete-time ZNN (DTZNN) model. The proposed DTZNN model is designed using the Taylor difference formula to discretize the previous continuous-time ZNNN (CTZNN) model. Theoretical analysis indicates the computational property of the proposed DTZNN model, and numerical results further demonstrate its validity. The applicability of the proposed DTZNN model is finally confirmed via its application to the motion planning of a PUMA560 robotic arm.},
  archive      = {J_AIR},
  author       = {Cang, Naimeng and Qiu, Feng and Xue, Shan and Jia, Zehua and Guo, Dongsheng and Zhang, Zhijun and Li, Weibing},
  doi          = {10.1007/s10462-024-10746-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {New discrete-time zeroing neural network for solving time-dependent linear equation with boundary constraint},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving partial differential equations using large-data
models: A literature review. <em>AIR</em>, <em>57</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10462-024-10784-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematics lies at the heart of engineering science and is very important for capturing and modeling of diverse processes. These processes may be naturally-occurring or man-made. One important engineering problem in this regard is the modeling of advanced mathematical problems and their analysis. Partial differential equations (PDEs) are important and useful tools to this end. However, solving complex PDEs for advanced problems requires extensive computational resources and complex techniques. Neural networks provide a way to solve complex PDEs reliably. In this regard, large-data models are new generation of techniques, which have large dependency capturing capabilities. Hence, they can richly model and accurately solve such complex PDEs. Some common large-data models include Convolutional neural networks (CNNs) and their derivatives, transformers, etc. In this literature survey, the mathematical background is introduced. A gentle introduction to the area of solving PDEs using large-data models is given. Various state-of-the-art large-data models for solving PDEs are discussed. Also, the major issues and future scope of the area are identified. Through this literature survey, it is hoped that readers will gain an insight into the area of solving PDEs using large-data models and pursue future research in this interesting area.},
  archive      = {J_AIR},
  author       = {Hafiz, Abdul Mueed and Faiq, Irfan and Hassaballah, M.},
  doi          = {10.1007/s10462-024-10784-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Solving partial differential equations using large-data models: A literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DC-graph: A chunk optimization model based on document
classification and graph learning. <em>AIR</em>, <em>57</em>(6), 1–24.
(<a href="https://doi.org/10.1007/s10462-024-10771-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing machine reading comprehension methods use a fixed stride to chunk long texts, which leads to missing contextual information at the boundaries of the chunks and a lack of communication between the information within each chunk. This paper proposes DC-Graph model, addressing existing issues in terms of reconstructing and supplementing information in long texts. Knowledge graphs contain extensive knowledge, and the semantic relationships between entities exhibit strong logical characteristics, which can assist the model in semantic understanding and reasoning. By categorizing the questions, this paper filters the content of long texts based on categories and reconstructs the content that aligns with the question category, compressing and optimizing the long text to minimize the number of document chunks when inputted into BERT. Additionally, unstructured text is transformed into a structured knowledge graph, and features are extracted using graph convolutional networks. These features are then added as global information to each chunk, aiding answer prediction. Experimental results on the CoQA, QuAC, and TriviaQA datasets demonstrate that our method outperforms both BERT and Recurrent Chunking Mechanisms, which share the same improvement approach, in terms of F1 and EM score. The code is available at (https://github.com/guohaozhang/DC-Graph.git).},
  archive      = {J_AIR},
  author       = {Zhou, Jingjing and Zhang, Guohao and Alfarraj, Osama and Tolba, Amr and Li, Xuefeng and Zhang, Hao},
  doi          = {10.1007/s10462-024-10771-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {DC-graph: A chunk optimization model based on document classification and graph learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey of convergence analysis of beetle
antennae search algorithm and its applications. <em>AIR</em>,
<em>57</em>(6), 1–72. (<a
href="https://doi.org/10.1007/s10462-024-10789-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, swarm intelligence optimization algorithms have been proven to have significant effects in solving combinatorial optimization problems. Introducing the concept of evolutionary computing, which is currently a hot research topic, into swarm intelligence optimization algorithms to form novel swarm intelligence optimization algorithms has proposed a new research direction for better solving combinatorial optimization problems. The longhorn beetle whisker search algorithm is an emerging heuristic algorithm, which originates from the simulation of longhorn beetle foraging behavior. This algorithm simulates the touch strategy required by longhorn beetles during foraging, and achieves efficient search in complex problem spaces through bioheuristic methods. This article reviews the research progress on the search algorithm for longhorn beetles from 2017 to present. Firstly, the basic principle and model structure of the beetle whisker search algorithm were introduced, and its differences and connections with other heuristic algorithms were analyzed. Secondly, this paper summarizes the research achievements of scholars in recent years on the improvement of longhorn whisker search algorithms. Then, the application of the beetle whisker search algorithm in various fields was explored, including function optimization, engineering design, and path planning. Finally, this paper summarizes the research achievements of scholars in recent years on the improvement of the longhorn whisker search algorithm, and proposes future research directions, including algorithm deep learning fusion, processing of multimodal problems, etc. Through this review, readers will have a comprehensive understanding of the research status and prospects of the longhorn whisker search algorithm, providing useful guidance for its application in practical problems.},
  archive      = {J_AIR},
  author       = {Chen, Changzu and Cao, Li and Chen, Yaodan and Chen, Binhe and Yue, Yinggao},
  doi          = {10.1007/s10462-024-10789-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-72},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of convergence analysis of beetle antennae search algorithm and its applications},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on imbalanced learning: Latest research,
applications and future directions. <em>AIR</em>, <em>57</em>(6), 1–51.
(<a href="https://doi.org/10.1007/s10462-024-10759-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced learning constitutes one of the most formidable challenges within data mining and machine learning. Despite continuous research advancement over the past decades, learning from data with an imbalanced class distribution remains a compelling research area. Imbalanced class distributions commonly constrain the practical utility of machine learning and even deep learning models in tangible applications. Numerous recent studies have made substantial progress in the field of imbalanced learning, deepening our understanding of its nature while concurrently unearthing new challenges. Given the field’s rapid evolution, this paper aims to encapsulate the recent breakthroughs in imbalanced learning by providing an in-depth review of extant strategies to confront this issue. Unlike most surveys that primarily address classification tasks in machine learning, we also delve into techniques addressing regression tasks and facets of deep long-tail learning. Furthermore, we explore real-world applications of imbalanced learning, devising a broad spectrum of research applications from management science to engineering, and lastly, discuss newly-emerging issues and challenges necessitating further exploration in the realm of imbalanced learning.},
  archive      = {J_AIR},
  author       = {Chen, Wuxing and Yang, Kaixiang and Yu, Zhiwen and Shi, Yifan and Chen, C. L. Philip},
  doi          = {10.1007/s10462-024-10759-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on imbalanced learning: Latest research, applications and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight diffusion models: A survey. <em>AIR</em>,
<em>57</em>(6), 1–51. (<a
href="https://doi.org/10.1007/s10462-024-10800-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models (DMs) are a type of potential generative models, which have achieved better effects in many fields than traditional methods. DMs consist of two main processes: one is the forward process of gradually adding noise to the original data until pure Gaussian noise; the other is the reverse process of gradually removing noise to generate samples conforming to the target distribution. DMs optimize the application results through the iterative noise processing process. However, this greatly increases the computational and storage costs in the training and inference stages, limiting the wide application of DMs. Therefore, how to effectively reduce the resource consumption of using DMs while giving full play to their good performance has become a valuable and necessary research problem. At present, some research has been devoted to lightweight DMs to solve this problem, but there has been no survey in this area. This paper focuses on lightweight DMs methods in the field of image processing, classifies them according to their processing ideas. Finally, the development prospect of future work is analyzed and discussed. It is hoped that this paper can provide other researchers with strategic ideas to reduce the resource consumption of DMs, thereby promoting the further development of this research direction and providing available models for wider applications.},
  archive      = {J_AIR},
  author       = {Song, Wei and Ma, Wen and Zhang, Ming and Zhang, Yanghao and Zhao, Xiaobing},
  doi          = {10.1007/s10462-024-10800-8},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Lightweight diffusion models: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method for the ethical analysis of brain-inspired AI.
<em>AIR</em>, <em>57</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-10769-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its successes, to date Artificial Intelligence (AI) is still characterized by a number of shortcomings with regards to different application domains and goals. These limitations are arguably both conceptual (e.g., related to the underlying theoretical models, such as symbolic vs.connectionist), and operational (e.g., related to robustness and ability to generalize). Biologically inspired AI, and more specifically brain-inspired AI, promises to provide further biological aspects beyond those that are already traditionally included in AI, making it possible to assess and possibly overcome some of its present shortcomings. This article examines some conceptual, technical, and ethical issues raised by the development and use of brain-inspired AI. Against this background, the paper asks whether there is anything ethically unique about brain-inspired AI. The aim of the paper is to introduce a method that has a heuristic nature and that can be applied to identify and address the ethical issues arising from brain-inspired AI (and from AI more generally). The conclusion resulting from the application of this method is that, compared to traditional AI, brain-inspired AI raises new foundational ethical issues and some new practical ethical issues, and exacerbates some of the issues raised by traditional AI.},
  archive      = {J_AIR},
  author       = {Farisco, Michele and Baldassarre, G. and Cartoni, E. and Leach, A. and Petrovici, M.A. and Rosemann, A. and Salles, A. and Stahl, B. and van Albada, S. J.},
  doi          = {10.1007/s10462-024-10769-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A method for the ethical analysis of brain-inspired AI},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of z number-based fuzzy inference system to
predict bearing capacity of circular foundations. <em>AIR</em>,
<em>57</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-10772-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise bearing capacity prediction of circular foundations is essential in civil engineering design and construction. The bearing capacity is affected by factors such as depth, density of soil, internal angle of friction, cohesion of soil, and foundation radius. In this paper, an innovative perspective on a fuzzy inference system (FIS) was proposed to predict bearing capacity. The uncertainty of fuzzy rules is eliminated by using Z-number theory. The effective parameters, i.e., depth, density of soil, internal angle of friction, cohesion of soil, and foundation radius were considered as inputs to the proposed model. To compare regression and FIS model with Z-based FIS, statistical indices such as the coefficient of determination (R2), root mean square error (RMSE), and variance account for (VAF) were employed. For training and testing Z-FIS, the R2 was (0.977 and 0.971), the RMSE was (1.645 and 1.745), and the VAF was (98.549% and 98.138), whereas for the FIS method, the values were (0.912 and 0.904), (5.962 and 6.76), and (90.12% and 88.49%). It should be mentioned that Z theory decreased the computational time by 89.28% (174.04 s to 18.65 s). The comparison of the statistical indicators of the presented models revealed the superiority of the Z-FIS model over the FIS. Notably, sensitivity analysis revealed that the most effective parameters on bearing capacity are internal angle of friction, depth, and soil density.},
  archive      = {J_AIR},
  author       = {Hosseini, Shahab and Gordan, Behrouz and Kalkan, Erol},
  doi          = {10.1007/s10462-024-10772-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Development of z number-based fuzzy inference system to predict bearing capacity of circular foundations},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Red-billed blue magpie optimizer: A novel metaheuristic
algorithm for 2D/3D UAV path planning and engineering design problems.
<em>AIR</em>, <em>57</em>(6), 1–89. (<a
href="https://doi.org/10.1007/s10462-024-10716-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical optimization, Unmanned Aerial Vehicle (UAV) path planning, and engineering design problems are fundamental to the development of artificial intelligence. Traditional methods show limitations in dealing with these complex nonlinear models. To address these challenges, the swarm intelligence algorithm is introduced as a metaheuristic method and effectively implemented. However, existing technology exhibits drawbacks such as slow convergence speed, low precision, and poor robustness. In this paper, we propose a novel metaheuristic approach called the Red-billed Blue Magpie Optimizer (RBMO), inspired by the cooperative and efficient predation behaviors of red-billed blue magpies. The mathematical model of RBMO was established by simulating the searching, chasing, attacking prey, and food storage behaviors of the red-billed blue magpie. To demonstrate RBMO’s performance, we first conduct qualitative analyses through convergence behavior experiments. Next, RBMO’s numerical optimization capabilities are substantiated using CEC2014 (Dim = 10, 30, 50, and 100) and CEC2017 (Dim = 10, 30, 50, and 100) suites, consistently achieving the best Friedman mean rank. In UAV path planning applications (two-dimensional and three − dimensional), RBMO obtains preferable solutions, demonstrating its effectiveness in solving NP-hard problems. Additionally, in five engineering design problems, RBMO consistently yields the minimum cost, showcasing its advantage in practical problem-solving. We compare our experimental results with three categories of widely recognized algorithms: (1) advanced variants, (2) recently proposed algorithms, and (3) high-performance optimizers, including CEC winners.},
  archive      = {J_AIR},
  author       = {Fu, Shengwei and Li, Ke and Huang, Haisong and Ma, Chi and Fan, Qingsong and Zhu, Yunwei},
  doi          = {10.1007/s10462-024-10716-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-89},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Red-billed blue magpie optimizer: A novel metaheuristic algorithm for 2D/3D UAV path planning and engineering design problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech emotion recognition systems and their security
aspects. <em>AIR</em>, <em>57</em>(6), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10760-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) systems leverage information derived from sound waves produced by humans to identify the concealed emotions in utterances. Since 1996, researchers have placed effort on improving the accuracy of SER systems, their functionalities, and the diversity of emotions that can be identified by the system. Although SER systems have become very popular in a variety of domains in modern life and are highly connected to other systems and types of data, the security of SER systems has not been adequately explored. In this paper, we conduct a comprehensive analysis of potential cyber-attacks aimed at SER systems and the security mechanisms that may prevent such attacks. To do so, we first describe the core principles of SER systems and discuss prior work performed in this area, which was mainly aimed at expanding and improving the existing capabilities of SER systems. Then, we present the SER system ecosystem, describing the dataflow and interactions between each component and entity within SER systems and explore their vulnerabilities, which might be exploited by attackers. Based on the vulnerabilities we identified within the ecosystem, we then review existing cyber-attacks from different domains and discuss their relevance to SER systems. We also introduce potential cyber-attacks targeting SER systems that have not been proposed before. Our analysis showed that only 30% of the attacks can be addressed by existing security mechanisms, leaving SER systems unprotected in the face of the other 70% of potential attacks. Therefore, we also describe various concrete directions that could be explored in order to improve the security of SER systems.},
  archive      = {J_AIR},
  author       = {Gurowiec, Itzik and Nissim, Nir},
  doi          = {10.1007/s10462-024-10760-z},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Speech emotion recognition systems and their security aspects},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of deep learning methods for digitisation of
complex documents and engineering diagrams. <em>AIR</em>,
<em>57</em>(6), 1–37. (<a
href="https://doi.org/10.1007/s10462-024-10779-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a review of deep learning on engineering drawings and diagrams. These are typically complex diagrams, that contain a large number of different shapes, such as text annotations, symbols, and connectivity information (largely lines). Digitising these diagrams essentially means the automatic recognition of all these shapes. Initial digitisation methods were based on traditional approaches, which proved to be challenging as these methods rely heavily on hand-crafted features and heuristics. In the past five years, however, there has been a significant increase in the number of deep learning-based methods proposed for engineering diagram digitalisation. We present a comprehensive and critical evaluation of existing literature that has used deep learning-based methods to automatically process and analyse engineering drawings. Key aspects of the digitisation process such as symbol recognition, text extraction, and connectivity information detection, are presented and thoroughly discussed. The review is presented in the context of a wide range of applications across different industry sectors, such as Oil and Gas, Architectural, Mechanical sectors, amongst others. The paper also outlines several key challenges, namely the lack of datasets, data annotation, evaluation and class imbalance. Finally, the latest development in digitalising engineering drawings are summarised, conclusions are drawn, and future interesting research directions to accelerate research and development in this area are outlined.},
  archive      = {J_AIR},
  author       = {Jamieson, Laura and Francisco Moreno-García, Carlos and Elyan, Eyad},
  doi          = {10.1007/s10462-024-10779-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of deep learning methods for digitisation of complex documents and engineering diagrams},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deepfake video detection: Challenges and opportunities.
<em>AIR</em>, <em>57</em>(6), 1–47. (<a
href="https://doi.org/10.1007/s10462-024-10810-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake videos are a growing social issue. These videos are manipulated by artificial intelligence (AI) techniques (especially deep learning), an emerging societal issue. Malicious individuals misuse deepfake technologies to spread false information, such as fake images, videos, and audio. The development of convincing fake content threatens politics, security, and privacy. The majority of deepfake video detection methods are data-driven. This survey paper aims to thoroughly analyse deepfake video generation and detection. The paper’s main contribution is the classification of the many challenges encountered while detecting deepfake videos. The paper discusses data challenges such as unbalanced datasets and inadequate labelled training data. Training challenges include the need for many computational resources. It also addresses reliability challenges, including overconfidence in detection methods and emerging manipulation approaches. The research emphasises the dominance of deep learning-based methods in detecting deepfakes despite their computational efficiency and generalisation limitations. However, it also acknowledges the drawbacks of these approaches, such as their limited computing efficiency and generalisation. The research also critically evaluates deepfake datasets, emphasising the necessity for good-quality datasets to improve detection methods. The study also indicates major research gaps, guiding future deepfake detection research. This entails developing robust models for real-time detection.},
  archive      = {J_AIR},
  author       = {Kaur, Achhardeep and Noori Hoshyar, Azadeh and Saikrishna, Vidya and Firmin, Selena and Xia, Feng},
  doi          = {10.1007/s10462-024-10810-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deepfake video detection: Challenges and opportunities},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning implementation of image segmentation in
agricultural applications: A comprehensive review. <em>AIR</em>,
<em>57</em>(6), 1–59. (<a
href="https://doi.org/10.1007/s10462-024-10775-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a crucial task in computer vision, which divides a digital image into multiple segments and objects. In agriculture, image segmentation is extensively used for crop and soil monitoring, predicting the best times to sow, fertilize, and harvest, estimating crop yield, and detecting plant diseases. However, image segmentation faces difficulties in agriculture, such as the challenges of disease staging recognition, labeling inconsistency, and changes in plant morphology with the environment. Consequently, we have conducted a comprehensive review of image segmentation techniques based on deep learning, exploring the development and prospects of image segmentation in agriculture. Deep learning-based image segmentation solutions widely used in agriculture are categorized into eight main groups: encoder-decoder structures, multi-scale and pyramid-based methods, dilated convolutional networks, visual attention models, generative adversarial networks, graph neural networks, instance segmentation networks, and transformer-based models. In addition, the applications of image segmentation methods in agriculture are presented, such as plant disease detection, weed identification, crop growth monitoring, crop yield estimation, and counting. Furthermore, a collection of publicly available plant image segmentation datasets has been reviewed, and the evaluation and comparison of performance for image segmentation algorithms have been conducted on benchmark datasets. Finally, there is a discussion of the challenges and future prospects of image segmentation in agriculture.},
  archive      = {J_AIR},
  author       = {Lei, Lian and Yang, Qiliang and Yang, Ling and Shen, Tao and Wang, Ruoxi and Fu, Chengbiao},
  doi          = {10.1007/s10462-024-10775-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning implementation of image segmentation in agricultural applications: A comprehensive review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated group decision-making framework for assessing
S3PRLPs based on MULTIMOORA-WASPAS with q-rung orthopair fuzzy
information. <em>AIR</em>, <em>57</em>(6), 1–59. (<a
href="https://doi.org/10.1007/s10462-024-10782-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable third-party reverse logistics has gradually risen to prominence as a component of contemporary commercial development as a result of the acceleration of global economic integration and the prominent growth of information technology in the logistics industry. In the procedure of sustainable third-party reverse logistics providers (S3PRLPs) selection, indeterminacy and conflict information bring great challenges to decision experts. In view of the significant superiority of q-rung orthopair fuzzy (q-ROF) set in expressing uncertain and vague assessment information, this essay designs a comprehensive assessment framework through merging the best and worst method (BWM), Multiplicative Multi-objective Optimization by Ratio Analysis with Full Multiplicative Form (MULTIMOORA) and weighted aggregated sum product assessment (WASPAS) method to address the S3PRLPs selection issue with entirely unknown weight information under q-ROF setting. Firstly, we present a novel score function for comparing q-ROF numbers after analyzing the inadequacies of previous works. Secondly, the q-ROF Frank interactive weighted average (q-ROFFIWA) and q-ROF Frank interactive weighted geometric (q-ROFFIWG) operators are advanced based on the constructed operations to take into consideration the interactive impact of information fusion procedure. Thirdly, the q-ROF-MULTIMOORA-WASPAS decision framework is built based on novel score function and the developed operators, in which the synthetic weights of the criterion are determined by the modified BWM and entropy weight method to reflect both the subjectivity of the decision expert and the objectivity of the decision information. Ultimately, an empirical example was used to evaluate S3PRLPs to demonstrate the applicability and feasibility of the developed methodology, and a comparative analysis was conducted with other existing methods to highlight its advantages in dealing with complex decision problems. The discussion from the research indicates that the developed methodology can be used to evaluate S3PRLPs and further improve the quality of logistics services for organizations.},
  archive      = {J_AIR},
  author       = {Rong, Yuan and Yu, Liying and Liu, Yi and Peng, Xingdong and Garg, Harish},
  doi          = {10.1007/s10462-024-10782-7},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An integrated group decision-making framework for assessing S3PRLPs based on MULTIMOORA-WASPAS with q-rung orthopair fuzzy information},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of the creeping of AFC based on fuzzy reasoning
and bi-LSTM fusion iteration. <em>AIR</em>, <em>57</em>(6), 1–34. (<a
href="https://doi.org/10.1007/s10462-024-10773-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creeping of Armoured Face Conveyor (AFC) is an engineering problem that needs to be avoided in coal mining production process. In this paper, a method for predicting the creeping accident of AFC based on fuzzy reasoning and Bi-directional Long Short-Term Memory (Bi-LSTM) fusion iteration is put forward. Firstly, through the force analysis of the AFC and the fuzzy correlation analysis method in the actual operation process, the reasons for the creeping of AFC are analyzed; Secondly, according to the propulsion characteristics of the AFC in the time sequence development, the method of the AFC running track based on Bi-LSTM neural network is proposed; Then, on the basis of the virtual transformation of the prediction results, a judgment mechanism for the extent of the creeping of the AFC based on fuzzy evidence reasoning based on fuzzy comprehensive evaluation method and Dempster-Shafer evidence theory (D-S evidence theory) is established; Finally, the analysis on the creeping of 9711 full-mechanized mining face in Kaiyuan Mine under virtual environment after 6 cycles of continuous advancement shows that the extent of creeping of AFC is relatively high and coal mining accidents are likely to occur.},
  archive      = {J_AIR},
  author       = {Li, Suhua and Xie, Jiacheng and Wang, Xuewen and Ge, Fuxiang},
  doi          = {10.1007/s10462-024-10773-8},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Prediction of the creeping of AFC based on fuzzy reasoning and bi-LSTM fusion iteration},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection and defense techniques in federated
learning: A comprehensive review. <em>AIR</em>, <em>57</em>(6), 1–34.
(<a href="https://doi.org/10.1007/s10462-024-10796-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning methods based on a large amount of data have achieved substantial success in numerous fields. However, with increases in regulations for protecting private user data, access to such data has become restricted. To overcome this limitation, federated learning (FL) has been widely utilized for training deep learning models without centralizing data. However, the inaccessibility of FL data and heterogeneity of the client data render difficulty in providing security and protecting the privacy in FL. In addition, the security and privacy anomalies in the corresponding systems significantly hinder the application of FL. Numerous studies have been proposed aiming to maintain the model security and mitigate the leakage of private training data during the FL training phase. Existing surveys categorize FL attacks from a defensive standpoint, but lack the efficiency of pinpointing attack points and implementing timely defenses. In contrast, our survey comprehensively categorizes and summarizes detected anomalies across client, server, and communication perspectives, facilitating easier identification and timely defense measures. Our survey provides an overview of the FL system and briefly introduces the FL security and privacy anomalies. Next, we detail the existing security and privacy anomalies and the methods of detection and defense from the perspectives of the client, server, and communication process. Finally, we address the security and privacy anomalies in non-independent identically distributed cases during FL and summarize the related research progress. This survey aims to provide a systematic and comprehensive review of security and privacy research in FL to help understand the progress and better apply FL in additional scenarios.},
  archive      = {J_AIR},
  author       = {Zhang, Chang and Yang, Shunkun and Mao, Lingfeng and Ning, Huansheng},
  doi          = {10.1007/s10462-024-10796-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Anomaly detection and defense techniques in federated learning: A comprehensive review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable multiplex semi-local random walk based on
influential nodes to improve link prediction in complex networks.
<em>AIR</em>, <em>57</em>(6), 1–32. (<a
href="https://doi.org/10.1007/s10462-024-10801-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the exponential growth of online social networks as complex networks has presented challenges in expanding networks and forging new connections. Link prediction emerges as a crucial technique to anticipate future relationships among users, leveraging the current network state to address this challenge effectively. While link prediction models on monoplex networks have a well-established history, the exploration of similar tasks on multilayer networks has garnered considerable attention. Extracting topological and multimodal features for weighting links can improve link prediction in weighted complex networks. Meanwhile, establishing reliable and trustworthy paths between users is a useful way to create metrics that convert unweighted to weighted similarity. The local random walk is a widely used technique for predicting links in weighted monoplex networks. The aim of this paper is to develop a semi-local random walk over reliable paths to improve link prediction on a multilayer social network as a complex network, which is denoted as Reliable Multiplex semi-Local Random Walk (RMLRW). RMLRW leverages the semi-local random walk technique over reliable paths, integrating intra-layer and inter-layer information from multiplex features to conduct a trustworthy biased random walk for predicting new links within a target layer of multilayer networks. In order to make RMLRW scalable, we develop a semi-local random walk-based network embedding to represent the network in a lower-dimensional space while preserving its original characteristics. Extensive experimental studies on several real-world multilayer networks demonstrate the performance assurance of RMLRW compared to equivalent methods. Specifically, RMLRW improves the average f-measure of the link prediction by 3.2% and 2.5% compared to SEM-Path and MLRW, respectively.},
  archive      = {J_AIR},
  author       = {Li, Shunlei and Tang, Jing and Zhou, Wen and Zhang, Yin and Azam, Muhammad Adeel and Mattos, Leonardo S.},
  doi          = {10.1007/s10462-024-10801-7},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reliable multiplex semi-local random walk based on influential nodes to improve link prediction in complex networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative deep learning for data generation in natural
hazard analysis: Motivations, advances, challenges, and opportunities.
<em>AIR</em>, <em>57</em>(6), 1–91. (<a
href="https://doi.org/10.1007/s10462-024-10764-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining and analysis are critical for preventing or mitigating natural hazards. However, data availability in natural hazard analysis is experiencing unprecedented challenges due to economic, technical, and environmental constraints. Recently, generative deep learning has become an increasingly attractive solution to these challenges, which can augment, impute, or synthesize data based on these learned complex, high-dimensional probability distributions of data. Over the last several years, much research has demonstrated the remarkable capabilities of generative deep learning for addressing data-related problems in natural hazards analysis. Data processed by deep generative models can be utilized to describe the evolution or occurrence of natural hazards and contribute to subsequent natural hazard modeling. Here we present a comprehensive review concerning generative deep learning for data generation in natural hazard analysis. (1) We summarized the limitations associated with data availability in natural hazards analysis and identified the fundamental motivations for employing generative deep learning as a critical response to these challenges. (2) We discuss several deep generative models that have been applied to overcome the problems caused by limited data availability in natural hazards analysis. (3) We analyze advances in utilizing generative deep learning for data generation in natural hazard analysis. (4) We discuss challenges associated with leveraging generative deep learning in natural hazard analysis. (5) We explore further opportunities for leveraging generative deep learning in natural hazard analysis. This comprehensive review provides a detailed roadmap for scholars interested in applying generative models for data generation in natural hazard analysis.},
  archive      = {J_AIR},
  author       = {Ma, Zhengjing and Mei, Gang and Xu, Nengxiong},
  doi          = {10.1007/s10462-024-10764-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-91},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative deep learning for data generation in natural hazard analysis: Motivations, advances, challenges, and opportunities},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel picture fuzzy power partitioned hamy mean operators
with dempster-shafer theory and their applications in MCDM.
<em>AIR</em>, <em>57</em>(6), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10757-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some multi-criteria decision-making (MCDM) scenarios, decision makers must address challenges like handling uncertain and incomplete information, managing biases in criteria values, and assessing interrelationships among criteria based on their partitioning as per their characteristics. To tackle these challenges, a picture fuzzy set (PFS) can be utilized to quantify vague information, Hamy mean (HM) can be used to consider criteria interrelationships while the power average (PA) mitigates any kind of biasness. Also, to overcome the limitations of persistence and invariantess in algebraic operations, Dempster-Shafer theory (DST) is employed. By integrating the conventional HM with the traditional PA under partitioning, this paper first introduced the novel power partitioned Hamy mean ( $$PPtHM^q$$ ) operator. Then, this operator is extended for picture fuzzy numbers (PFNs) with DST and two novel operators are introduced, which are named as picture fuzzy power partitioned Hamy mean $$(PFPPtHM^q_{DST})$$ and picture fuzzy weighted power partitioned Hamy mean $$(PFWPPtHM^q_{DST})$$ with some desirable properties. Moreover, based on these operators, a new method for MCDM in the PFS environment has been designed. The paper illustrates their application in selecting the best hotel among four alternatives ( $$B_1$$ , $$B_2$$ , $$B_3$$ , $$B_4$$ ) based on five criteria, which are partitioned into two sets. Results indicate that the best and worst alternatives under these operators are hotels $$B_1$$ and $$B_4$$ , respectively. Sensitivity analysis explores the impact of granularity parameter variations, and comparative analysis demonstrates the effectiveness of the presented operators. Overall, the study concludes that these operators offer flexibility, generality, and consistency for analyzing MCDM problems in PFS environments.},
  archive      = {J_AIR},
  author       = {Punetha, Tanuja and Komal and Pamucar, Dragan},
  doi          = {10.1007/s10462-024-10757-8},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Novel picture fuzzy power partitioned hamy mean operators with dempster-shafer theory and their applications in MCDM},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). N-ary aggregation operators on function spaces: Perspective
of construction. <em>AIR</em>, <em>57</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10462-024-10753-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For disposing numerous practical application problems involving expert systems, decision-making, image processing, classifications and etc, the investigations on the constructions and basic properties of n-ary aggregation operators (nAAOs) have always been a hot research topic with important research value and significance at theoretical investigations on aggregation operators (AOs). Herein, first, we propose a method for constructing nAAOs on function spaces via a family of known ones defined on a bounded poset, where those function spaces are composed by all fuzzy sets with that bounded poset as the truth values set. This method is different from the existing construction methods of nAAOs on bounded posets and provides a unified way of constructing usual nAAOs (like t-norms, uninorms, overlap functions, etc.) on function spaces via a family of known ones. Second, we present notion of representable nAAOs on function spaces and afford their equivalent characterization. Third, we discuss some vital properties of representable nAAOs on function spaces. Fourth, it is worth noticing that the obtained results cover the cases of nAAOs on function spaces composed of all interval-valued fuzzy sets and type-2 fuzzy sets when underlying bounded poset is taken as the corresponding truth values set, respectively. As a consequence, the theoretical results obtained herein have certain promotion and basic theoretical value for the mining of new potential applications of nAAOs in real problems, especially in expert systems, decision-making, image processing and etc.},
  archive      = {J_AIR},
  author       = {Qiao, Junsheng},
  doi          = {10.1007/s10462-024-10753-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Artif. Intell. Rev.},
  title        = {N-ary aggregation operators on function spaces: Perspective of construction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning search algorithm: Framework and comprehensive
performance for solving optimization problems. <em>AIR</em>,
<em>57</em>(6), 1–85. (<a
href="https://doi.org/10.1007/s10462-024-10767-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the Learning Search Algorithm (LSA) is introduced as an innovative optimization algorithm that draws inspiration from swarm intelligence principles and mimics the social learning behavior observed in humans. The LSA algorithm optimizes the search process by integrating historical experience and real-time social information, enabling it to effectively navigate complex problem spaces. By doing so, it enhances its global development capability and provides efficient solutions to challenging optimization tasks. Additionally, the algorithm improves the collective learning capacity by incorporating teaching and active learning behaviors within the population, leading to improved local development capabilities. Furthermore, a dynamic adaptive control factor is utilized to regulate the algorithm’s global exploration and local development abilities. The proposed algorithm is rigorously evaluated using 40 benchmark test functions from IEEE CEC 2014 and CEC 2020, and compared against nine established evolutionary algorithms as well as 11 recently improved algorithms. The experimental results demonstrate the superiority of the LSA algorithm, as it achieves the top rank in the Friedman rank-sum test, highlighting its power and competitiveness. Moreover, the LSA algorithm is successfully applied to solve six real-world engineering problems and 15 UCI datasets of feature selection problems, showcasing its significant advantages and potential for practical applications in engineering problems and feature selection problems.},
  archive      = {J_AIR},
  author       = {Qu, Chiwen and Peng, Xiaoning and Zeng, Qilan},
  doi          = {10.1007/s10462-024-10767-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-85},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learning search algorithm: Framework and comprehensive performance for solving optimization problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the impact of SEO-based ranking factors for voice
queries through machine learning. <em>AIR</em>, <em>57</em>(6), 1–28.
(<a href="https://doi.org/10.1007/s10462-024-10780-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of voice search is proliferating and expected to grow into the foreseeable future; this is why websites increasingly optimize their content associated with voice-based search to improve their ranking. In this era of rapid growth in voice search technology, it is a topical matter that needs research. Moreover, many predictions about its future excite the subject and require systematic investigation. This research aims to analyze important features that contribute to the SEO of webpages. Therefore, there is a need to examine various ranking factors that improve the ranking of the webpages for voice search queries on the Search Engine Results Page (SERP). This study consists of two phases. The first phase comprises systematic data acquisition and identifying important SEO-based ranking factors. The second phase includes a longitudinal case study to evaluate the impact and significance of identified factors. To achieve this goal, we conduct experiments on methodical combinations of features through machine learning algorithms such as Support Vector Machine, Logistic Regression, Naive Bayes Classifier, K-Nearest Neighbors, Decision Trees and Random Forest. Comparing results for multiple feature designs evaluates the contributing nature of specific features in SEO-based optimization for ranking. Results suggest the importance of the newly identified feature set (FF) outperforms baselines (EF and EFN) by a significant margin. A longitudinal case study on a blog over four months confirms that optimizing these features improves page ranking; therefore, webmasters must optimize these features while preparing the webpage.},
  archive      = {J_AIR},
  author       = {Saeed, Zafar and Aslam, Fozia and Ghafoor, Adnan and Umair, Muhammad and Razzak, Imran},
  doi          = {10.1007/s10462-024-10780-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring the impact of SEO-based ranking factors for voice queries through machine learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient full-size convolutional computing method based
on memristor crossbar. <em>AIR</em>, <em>57</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10462-024-10787-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence systems based on neural networks need to perform a large number of repeated parallel operations quickly. Without hardware acceleration, they cannot achieve effectiveness and availability. Memristor-based neuromorphic computing systems are one of the promising hardware acceleration strategies. In this paper, we propose a full-size convolution algorithm (FSCA) for the memristor crossbar, which can store both the input matrix and the convolution kernel and map the convolution kernel to the entire input matrix in a full parallel method during the computation. This method dramatically increases the convolutional kernel computations in a single operation, and the number of operations no longer increases with the input matrix size. Then a bidirectional pulse control switch integrated with two extra memristors into CMOS devices is designed to effectively suppress the leakage current problem in the row and column directions of the existing memristor crossbar. The spice circuit simulation system is built to verify that the design convolutional computation algorithm can extract the feature map of the entire input matrix after only a few operations in the memristor crossbar-based computational circuit. System-level simulations based on the MNIST classification task verify that the designed algorithm and circuit can effectively implement Gabor filtering, allowing the multilayer neural network to improve the classification task recognition accuracy to 98.25% with a 26.2% reduction in network parameters. In comparison, the network can even effectively immunize various non-idealities of the memristive synaptic within 30%.},
  archive      = {J_AIR},
  author       = {Tan, Jinpei and Shen, Siyuan and Duan, Shukai and Wang, Lidan},
  doi          = {10.1007/s10462-024-10787-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient full-size convolutional computing method based on memristor crossbar},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of machine learning techniques for arabic fake news
detection. <em>AIR</em>, <em>57</em>(6), 1–33. (<a
href="https://doi.org/10.1007/s10462-024-10778-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms have emerged as primary information sources, offering easy access to a wide audience. Consequently, a significant portion of the global population relies on these platforms for updates on current events. However, fraudulent actors exploit social networks to disseminate false information, either for financial gain or to manipulate public opinion. Recognizing the detrimental impact of fake news, researchers have turned their attention to automating its detection. In this paper, we provide a thorough review of fake news detection in Arabic, a low-resource language, to contextualize the current state of research in this domain. In our research methodology, we recall fake news terminology, provide examples for clarity, particularly in Arabic contexts, and explore its impact on public opinion. We discuss the challenges in fake news detection, outline the used datasets, and provide Arabic annotation samples for label assignment. Likewise, preprocessing steps for Arabic language nuances are highlighted. We also explore features from shared tasks and their implications. Lastly, we address open issues, proposing some future research directions like dataset improvement, feature refinement, and increased awareness to combat fake news proliferation. We contend that incorporating our perspective into the examination of fake news aspects, along with suggesting enhancements, sets this survey apart from others currently available.},
  archive      = {J_AIR},
  author       = {Touahri, Ibtissam and Mazroui, Azzeddine},
  doi          = {10.1007/s10462-024-10778-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Survey of machine learning techniques for arabic fake news detection},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical reinforcement learning for handling sparse
rewards in multi-goal navigation. <em>AIR</em>, <em>57</em>(6), 1–23.
(<a href="https://doi.org/10.1007/s10462-024-10794-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has achieved remarkable advancements in navigation tasks in recent years. However, tackling multi-goal navigation tasks with sparse rewards remains a complex and challenging problem due to the long-sequence decision-making involved. Such multi-goal navigation tasks inherently incorporate a hybrid action space, where the robot needs to select a navigation endpoint first before executing primitive actions. To address the problem of multi-goal navigation with sparse rewards, we introduce a novel hierarchical RL framework named Hierarchical RL with Multi-Goal (HRL-MG). The main idea of HRL-MG is to divide and conquer the hybrid action space, splitting long-sequence decisions into short-sequence decisions. The HRL-MG framework is composed of two main modules: a selector and an actuator. The selector employs a temporal abstraction hierarchical architecture designed to specify a desired end goal based on the discrete action space. Conversely, the actuator utilizes a continuous goal-oriented hierarchical architecture developed to enact continuous action sequences to reach the desired end goal specified by the selector. In addition, we incorporate a dynamic goal detection mechanism, grounded in hindsight experience replay, to mitigate the challenges posed by sparse reward landscapes. We validated the algorithm’s efficacy on both the discrete environment Maze_2D and the continuous robotic environment MuJoCo ‘Ant’. The results indicate that HRL-MG significantly outperforms other methods in multi-goal navigation tasks with sparse rewards.},
  archive      = {J_AIR},
  author       = {Yan, Jiangyue and Luo, Biao and Xu, Xiaodong},
  doi          = {10.1007/s10462-024-10794-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Hierarchical reinforcement learning for handling sparse rewards in multi-goal navigation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Methodology and real-world applications of dynamic uncertain
causality graph for clinical diagnosis with explainability and
invariance. <em>AIR</em>, <em>57</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10462-024-10763-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-aided clinical diagnosis is desired in medical care. Existing deep learning models lack explainability and mainly focus on image analysis. The recently developed Dynamic Uncertain Causality Graph (DUCG) approach is causality-driven, explainable, and invariant across different application scenarios, without problems of data collection, labeling, fitting, privacy, bias, generalization, high cost and high energy consumption. Through close collaboration between clinical experts and DUCG technicians, 46 DUCG models covering 54 chief complaints were constructed. Over 1,000 diseases can be diagnosed without triage. Before being applied in real-world, the 46 DUCG models were retrospectively verified by third-party hospitals. The verified diagnostic precisions were no less than 95%, in which the diagnostic precision for every disease including uncommon ones was no less than 80%. After verifications, the 46 DUCG models were applied in the real-world in China. Over one million real diagnosis cases have been performed, with only 17 incorrect diagnoses identified. Due to DUCG’s transparency, the mistakes causing the incorrect diagnoses were found and corrected. The diagnostic abilities of the clinicians who applied DUCG frequently were improved significantly. Following the introduction to the earlier presented DUCG methodology, the recommendation algorithm for potential medical checks is presented and the key idea of DUCG is extracted.},
  archive      = {J_AIR},
  author       = {Zhang, Zhan and Zhang, Qin and Jiao, Yang and Lu, Lin and Ma, Lin and Liu, Aihua and Liu, Xiao and Zhao, Juan and Xue, Yajun and Wei, Bing and Zhang, Mingxia and Gao, Ru and Zhao, Hong and Lu, Jie and Li, Fan and Zhang, Yang and Wang, Yiming and Zhang, Lei and Tian, Fengwei and Hu, Jie and Gou, Xin},
  doi          = {10.1007/s10462-024-10763-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Methodology and real-world applications of dynamic uncertain causality graph for clinical diagnosis with explainability and invariance},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The methods for improving large-scale multi-view clustering
efficiency: A survey. <em>AIR</em>, <em>57</em>(6), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-10785-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diversity and large scale of multi-view data have brought more significant challenges to conventional clustering technology. Recently, multi-view clustering has received widespread attention because it can better use different views’ consensus and complementary information to improve clustering performance. Simultaneously, many researchers have proposed various algorithms to reduce the computational complexity to accommodate the demands of large-scale multi-view clustering. However, the current reviews do not summarize from the perspective of reducing the computational complexity of large-scale multi-view clustering. Therefore, this paper outlines various high-frequency methods used in recent years to reduce the computational complexity of large-scale multi-view clustering, i.e. third-order tensor t-SVD, anchors-based graph construction, matrix blocking, and matrix factorization, and compares the corresponding algorithms based on several open datasets. Finally, the strengths and weaknesses of the current algorithm and the point of improvement are analyzed.},
  archive      = {J_AIR},
  author       = {Yang, Zengbiao and Tan, Yihua},
  doi          = {10.1007/s10462-024-10785-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The methods for improving large-scale multi-view clustering efficiency: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active convolutional neural networks sign language
(ActiveCNN-SL) framework: A paradigm shift in deaf-mute communication.
<em>AIR</em>, <em>57</em>(6), 1–30. (<a
href="https://doi.org/10.1007/s10462-024-10792-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time speech-to-text and text-to-speech technologies have significantly influenced the accessibility of communication for individuals who are deaf or mute. This research aims to assess the efficacy of these technologies in facilitating communication between deaf or mute individuals and those who are neither deaf nor mute. A mixed-method approach will incorporate qualitative and quantitative data collection and analysis techniques. The study will involve participants from deaf or mute and non-deaf or non-mute communities. The research will scrutinize the precision and efficiency of communication using these technologies and evaluate user experience and satisfaction. Furthermore, the study intends to pinpoint potential obstacles and limitations of these technologies and offer suggestions for enhancing their effectiveness in fostering inclusivity. The study proposes an active learning framework for sign language gesture recognition, termed Active Convolutional Neural Networks—Sign Language (ActiveCNN-SL). ActiveCNN-SL aims to minimize the labeled data required for training and augment the accuracy of sign language gesture recognition through iterative human feedback. This proposed framework holds the potential to enhance communication accessibility for deaf and mute individuals and encourage inclusivity across various environments. The proposed framework is trained using two primary datasets: (i) the Sign Language Gesture Images Dataset and (ii) the American Sign Language Letters (ASL)—v1. The framework employs Resnet50 and YoloV.8 to train the datasets. It has demonstrated high performance in terms of precision and accuracy. The ResNet model achieved a remarkable accuracy rate of 99.98% during training, and it also exhibited a validation accuracy of 100%, surpassing the baseline CNN and RNN models. The YOLOv8 model outperformed previous methods on the ASL alphabet dataset, achieving an overall mean average accuracy for all classes of 97.8%.},
  archive      = {J_AIR},
  author       = {ZainEldin, Hanaa and Baghdadi, Nadiah A. and Gamel, Samah A. and Aljohani, Mansourah and Talaat, Fatma M. and Malki, Amer and Badawy, Mahmoud and Elhosseini, Mostafa},
  doi          = {10.1007/s10462-024-10792-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Active convolutional neural networks sign language (ActiveCNN-SL) framework: A paradigm shift in deaf-mute communication},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research trends in deep learning and machine learning for
cloud computing security. <em>AIR</em>, <em>57</em>(5), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10776-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning and machine learning show effectiveness in identifying and addressing cloud security threats. Despite the large number of articles published in this field, there remains a dearth of comprehensive reviews that synthesize the techniques, trends, and challenges of using deep learning and machine learning for cloud computing security. Accordingly, this paper aims to provide the most updated statistics on the development and research in cloud computing security utilizing deep learning and machine learning. Up to the middle of December 2023, 4051 publications were identified after we searched the Scopus database. This paper highlights key trend solutions for cloud computing security utilizing machine learning and deep learning, such as anomaly detection, security automation, and emerging technology&#39;s role. However, challenges such as data privacy, scalability, and explainability, among others, are also identified as challenges of using machine learning and deep learning for cloud security. The findings of this paper reveal that deep learning and machine learning for cloud computing security are emerging research areas. Future research directions may include addressing these challenges when utilizing machine learning and deep learning for cloud security. Additionally, exploring the development of algorithms and techniques that comply with relevant laws and regulations is essential for effective implementation in this domain.},
  archive      = {J_AIR},
  author       = {Alzoubi, Yehia Ibrahim and Mishra, Alok and Topcu, Ahmet Ercan},
  doi          = {10.1007/s10462-024-10776-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Research trends in deep learning and machine learning for cloud computing security},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAV control in autonomous object-goal navigation: A
systematic literature review. <em>AIR</em>, <em>57</em>(5), 1–64. (<a
href="https://doi.org/10.1007/s10462-024-10758-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research interest in autonomous control of unmanned aerial vehicles (UAVs) has increased rapidly over the past decade. They are now widely used in civilian, military, and private areas. Applications include surveillance, search and rescue, and delivery tasks. More broadly, they excel at solving problems where a significant amount of space must be covered and traveled. However, using UAVs to solve navigation problems with full autonomy necessitates the mastering of complex subtasks. A solution that includes control, planning, localization, and mapping remains an open challenge. Object-goal navigation contains the same navigation problems where the main objective is to reach a target object. The search and identification of this target are central to the vehicle’s navigation. This requires an understanding of what it is and where it can be located to move around the scene. This paper presents a systematic literature review on object-goal navigation and its subtasks, using autonomous UAVs. Survey taxonomies were found for the tasks and methods behind navigation and target localization problems using UAVs. The review analyzed 67 articles found between 2011 and 2022. They were found in the ACM, IEEE Xplore, WebOfScience, Science Direct, and Scopus databases. This review revealed essential issues related to autonomous navigation task dependencies. Moreover, it highlighted gaps in UAV development and framework standardization. Open challenges for autonomous UAV control for object-goal navigation must address the research on finding methods for problems. For example, autonomy level and comparison metrics, considering safety, ethics, and legal implications.},
  archive      = {J_AIR},
  author       = {Ayala, Angel and Portela, Leticia and Buarque, Fernando and Fernandes, Bruno J. T. and Cruz, Francisco},
  doi          = {10.1007/s10462-024-10758-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {UAV control in autonomous object-goal navigation: A systematic literature review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective coyote optimization algorithm based on
hybrid elite framework and meta-lamarckian learning strategy for optimal
power flow problem. <em>AIR</em>, <em>57</em>(5), 1–64. (<a
href="https://doi.org/10.1007/s10462-024-10752-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-objective coyote optimization algorithm based on hybrid elite framework and Meta-Lamarckian learning strategy (MOCOA-ML) was proposed to solve the optimal power flow (OPF) problem. MOCOA-ML adds external archives with grid mechanism on the basis of elite non-dominated sorting. It can guarantee the diversity of the population while obtaining the Pareto solution set. When selecting elite coyotes, there is a greater probability to select the elite in sparse areas, which is conducive to the development of sparse areas. In addition, combined with Meta-Lamarckian learning strategy, based on four crossover operators (horizontal crossover operator, longitudinal crossover operator, elite crossover operator and direct crossover operator), the local search method is adaptively selected for optimization, and its convergence performance is improved. First, the simulation is carried out in 20 test functions, and compared with MODA, MOPSO, MOJAYA, NSGA-II, MOEA/D, MOAOS and MOTEO. The experimental results showed that MOCOA-ML achieved the best inverted generational distance value and the best hypervolume value in 11 and 13 test functions, respectively. Then, MOCOA-ML is used to solve the optimal power flow problem. Taking the fuel cost, power loss and total emissions as objective functions, the tests of two-objective and three-objective bechmark problems are carried out on IEEE 30-bus system and IEEE 57-bus system. The results are compared with MOPSO, MOGWO and MSSA algorithms. The experimental results of OPF demonstrate that MOCOA-ML can find competitive solutions and ranks first in six cases. It also shows that the proposed method has obtained a satisfactory uniform Pareto front.},
  archive      = {J_AIR},
  author       = {Zhu, Jun-Hua and Wang, Jie-Sheng and Zhang, Xing-Yue and Wang, Yu-Cai and Song, Hao-Ming and Zheng, Yue and Liu, Xun},
  doi          = {10.1007/s10462-024-10752-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-objective coyote optimization algorithm based on hybrid elite framework and meta-lamarckian learning strategy for optimal power flow problem},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence and edge computing for machine
maintenance-review. <em>AIR</em>, <em>57</em>(5), 1–33. (<a
href="https://doi.org/10.1007/s10462-024-10748-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial internet of things (IIoT) has ushered us into a world where most machine parts are now embedded with sensors that collect data. This huge data reservoir has enhanced data-driven diagnostics and prognoses of machine health. With technologies like cloud or centralized computing, the data could be sent to powerful remote data centers for machine health analysis using artificial intelligence (AI) tools. However, centralized computing has its own challenges, such as privacy issues, long latency, and low availability. To overcome these problems, edge computing technology was embraced. Thus, instead of moving all the data to the remote server, the data can now transition on the edge layer where certain computations are done. Thus, access to the central server is infrequent. Although placing AI on edge devices aids in fast inference, it poses new research problems, as highlighted in this paper. Moreover, the paper discusses studies that use edge computing to develop artificial intelligence-based diagnostic and prognostic techniques for industrial machines. It highlights the locations of data preprocessing, model training, and deployment. After analysis of several works, trends of the field are outlined, and finally, future research directions are elaborated},
  archive      = {J_AIR},
  author       = {Bala, Abubakar and Rashid, Rahimi Zaman Jusoh A. and Ismail, Idris and Oliva, Diego and Muhammad, Noryanti and Sait, Sadiq M. and Al-Utaibi, Khaled A. and Amosa, Temitope Ibrahim and Memon, Kamran Ali},
  doi          = {10.1007/s10462-024-10748-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence and edge computing for machine maintenance-review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated machine learning: Past, present and future.
<em>AIR</em>, <em>57</em>(5), 1–88. (<a
href="https://doi.org/10.1007/s10462-024-10726-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) is a young research area aiming at making high-performance machine learning techniques accessible to a broad set of users. This is achieved by identifying all design choices in creating a machine-learning model and addressing them automatically to generate performance-optimised models. In this article, we provide an extensive overview of the past and present, as well as future perspectives of AutoML. First, we introduce the concept of AutoML, formally define the problems it aims to solve and describe the three components underlying AutoML approaches: the search space, search strategy and performance evaluation. Next, we discuss hyperparameter optimisation (HPO) techniques commonly used in AutoML systems design, followed by providing an overview of the neural architecture search, a particular case of AutoML for automatically generating deep learning models. We further review and compare available AutoML systems. Finally, we provide a list of open challenges and future research directions. Overall, we offer a comprehensive overview for researchers and practitioners in the area of machine learning and provide a basis for further developments in AutoML.},
  archive      = {J_AIR},
  author       = {Baratchi, Mitra and Wang, Can and Limmer, Steffen and van Rijn, Jan N. and Hoos, Holger and Bäck, Thomas and Olhofer, Markus},
  doi          = {10.1007/s10462-024-10726-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-88},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automated machine learning: Past, present and future},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of automatic plankton image recognition: Challenges,
existing solutions and future perspectives. <em>AIR</em>,
<em>57</em>(5), 1–59. (<a
href="https://doi.org/10.1007/s10462-024-10745-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planktonic organisms including phyto-, zoo-, and mixoplankton are key components of aquatic ecosystems and respond quickly to changes in the environment, therefore their monitoring is vital to follow and understand these changes. Advances in imaging technology have enabled novel possibilities to study plankton populations, but the manual classification of images is time consuming and expert-based, making such an approach unsuitable for large-scale application and urging for automatic solutions for the analysis, especially recognizing the plankton species from images. Despite the extensive research done on automatic plankton recognition, the latest cutting-edge methods have not been widely adopted for operational use. In this paper, a comprehensive survey on existing solutions for automatic plankton recognition is presented. First, we identify the most notable challenges that make the development of plankton recognition systems difficult and restrict the deployment of these systems for operational use. Then, we provide a detailed description of solutions found in plankton recognition literature. Finally, we propose a workflow to identify the specific challenges in new datasets and the recommended approaches to address them. Many important challenges remain unsolved including the following: (1) the domain shift between the datasets hindering the development of an imaging instrument independent plankton recognition system, (2) the difficulty to identify and process the images of previously unseen classes and non-plankton particles, and (3) the uncertainty in expert annotations that affects the training of the machine learning models. To build harmonized instrument and location agnostic methods for operational purposes these challenges should be addressed in future research.},
  archive      = {J_AIR},
  author       = {Eerola, Tuomas and Batrakhanov, Daniel and Barazandeh, Nastaran Vatankhah and Kraft, Kaisa and Haraguchi, Lumi and Lensu, Lasse and Suikkanen, Sanna and Seppälä, Jukka and Tamminen, Timo and Kälviäinen, Heikki},
  doi          = {10.1007/s10462-024-10745-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Survey of automatic plankton image recognition: Challenges, existing solutions and future perspectives},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secretary bird optimization algorithm: A new metaheuristic
for solving global optimization problems. <em>AIR</em>, <em>57</em>(5),
1–102. (<a href="https://doi.org/10.1007/s10462-024-10729-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel population-based metaheuristic algorithm called secretary bird optimization algorithm (SBOA), inspired by the survival behavior of secretary birds in their natural environment. Survival for secretary birds involves continuous hunting for prey and evading pursuit from predators. This information is crucial for proposing a new metaheuristic algorithm that utilizes the survival abilities of secretary birds to address real-world optimization problems. The algorithm&#39;s exploration phase simulates secretary birds hunting snakes, while the exploitation phase models their escape from predators. During this phase, secretary birds observe the environment and choose the most suitable way to reach a secure refuge. These two phases are iteratively repeated, subject to termination criteria, to find the optimal solution to the optimization problem. To validate the performance of SBOA, experiments were conducted to assess convergence speed, convergence behavior, and other relevant aspects. Furthermore, we compared SBOA with 15 advanced algorithms using the CEC-2017 and CEC-2022 benchmark suites. All test results consistently demonstrated the outstanding performance of SBOA in terms of solution quality, convergence speed, and stability. Lastly, SBOA was employed to tackle 12 constrained engineering design problems and perform three-dimensional path planning for Unmanned Aerial Vehicles. The results demonstrate that, compared to contrasted optimizers, the proposed SBOA can find better solutions at a faster pace, showcasing its significant potential in addressing real-world optimization problems.},
  archive      = {J_AIR},
  author       = {Fu, Youfa and Liu, Dan and Chen, Jiadui and He, Ling},
  doi          = {10.1007/s10462-024-10729-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-102},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Secretary bird optimization algorithm: A new metaheuristic for solving global optimization problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revolutionizing personalized medicine with generative AI: A
systematic review. <em>AIR</em>, <em>57</em>(5), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10768-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine, targeting treatments to individual genetic and clinical profiles, faces challenges in data collection, costs, and privacy. Generative AI offers a promising solution by creating realistic, privacy-preserving patient data, potentially revolutionizing patient-centric healthcare. This review examines the role of deep generative models (DGMs) in clinical informatics, medical imaging, bioinformatics, and early diagnostics, showcasing their impact on precision medicine. Adhering to PRISMA guidelines, the review analyzes studies from databases such as Scopus and PubMed, focusing on AI&#39;s impact in precision medicine and DGMs&#39; applications in synthetic data generation. DGMs, particularly Generative Adversarial Networks (GANs), have improved synthetic data generation, enhancing accuracy and privacy. However, limitations exist, especially in the accuracy of foundation models like Large Language Models (LLMs) in digital diagnostics. Overcoming data scarcity and ensuring realistic, privacy-safe synthetic data generation are crucial for advancing personalized medicine. Further development of LLMs is essential for improving diagnostic precision. The application of generative AI in personalized medicine is emerging, highlighting the need for more interdisciplinary research to advance this field.},
  archive      = {J_AIR},
  author       = {Ghebrehiwet, Isaias and Zaki, Nazar and Damseh, Rafat and Mohamad, Mohd Saberi},
  doi          = {10.1007/s10462-024-10768-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revolutionizing personalized medicine with generative AI: A systematic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sustainability performance assessment of freight
transportation modes using an integrated decision-making framework based
on m-generalized q-neutrosophic sets. <em>AIR</em>, <em>57</em>(5),
1–38. (<a href="https://doi.org/10.1007/s10462-024-10751-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The freight transport industry is one of the primary sectors responsible for excessive energy consumption and greenhouse gas emissions. Restructuring international and domestic freight transport chains based on sustainability and green transportation is critical for practitioners and policymakers to reduce pressure on the logistics and transportation industries. This study aims to develop a mathematical model for selecting the most appropriate transportation type, and accordingly, the optimal route in transportation operations to improve the sustainability performance of the freight transportation industry. Therefore, the main goal is to choose the most suitable route and transportation type which contributes to create a more eco-friendly and sustainable transportation system. For this purpose, Neutrosophic Number-based Delphi (NN-Delphi), m-Generalized q-Neutrosophic Sets (mGqNSs)-based Stepwise Weight Assessment Ratio Analysis (MGqNS-SWARA) and mGqNSs-based Additive Ratio Assessment (mGqNS-ARAS) are developed and implemented to set the influential criteria, compute the weights of these criteria, and identify the sustainability performance of the freight mode variants, respectively. According to the final results, &quot;Cargo security&quot; and &quot;Accident rates&quot; are the most important criteria with a relative importance score of 0.0237, contributing to the sustainability of load transport modes. Moreover, &quot;Maritime Transport Mode&quot; is identified as the most sustainable transportation type with a relative importance score of 0.7895. Finally, it is revealed that there is a positive relationship between maritime transport and sustainability.},
  archive      = {J_AIR},
  author       = {Görçün, Ömer Faruk and Tirkolaee, Erfan Babaee and Aytekin, Ahmet and Korucuk, Selçuk},
  doi          = {10.1007/s10462-024-10751-0},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Sustainability performance assessment of freight transportation modes using an integrated decision-making framework based on m-generalized q-neutrosophic sets},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey: Federated learning data security and
privacy-preserving in edge-internet of things. <em>AIR</em>,
<em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-10774-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of data generated owing to the rapid development of the Smart Internet of Things is increasing exponentially. Traditional machine learning can no longer meet the requirements for training complex models with large amounts of data. Federated learning, as a new paradigm for training statistical models in distributed edge networks, alleviates integration and training problems in the context of massive and heterogeneous data and security protection for private data. Edge computing processes data at the edge layers of data sources to ensure low-data-delay processing; it provides high-bandwidth communication and a stable network environment, and relieves the pressure of processing massive data using a single node in the cloud center. A combination of edge computing and federated learning can further optimize computing, communication, and data security for the edge-Internet of Things. This review investigated the development status of federated learning and expounded on its basic principles. Then, in view of the security attacks and privacy leakage problems of federated learning in the edge Internet of things, relevant work was investigated from cryptographic technologies (such as secure multi-party computation, homomorphic encryption and secret sharing), perturbation schemes (such as differential privacy), adversarial training and other privacy security protection measures. Finally, challenges and future research directions for the integration of edge computing and federated learning are discussed.},
  archive      = {J_AIR},
  author       = {Li, Haiao and Ge, Lina and Tian, Lei},
  doi          = {10.1007/s10462-024-10774-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Survey: Federated learning data security and privacy-preserving in edge-internet of things},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive review of artificial intelligence models for
screening major retinal diseases. <em>AIR</em>, <em>57</em>(5), 1–93.
(<a href="https://doi.org/10.1007/s10462-024-10736-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a systematic survey of artificial intelligence (AI) models that have been proposed over the past decade to screen retinal diseases, which can cause severe visual impairments or even blindness. The paper covers both the clinical and technical perspectives of using AI models in hosipitals to aid ophthalmologists in promptly identifying retinal diseases in their early stages. Moreover, this paper also evaluates various methods for identifying structural abnormalities and diagnosing retinal diseases, and it identifies future research directions based on a critical analysis of the existing literature. This comprehensive study, which reviews both the conventional and state-of-the-art methods to screen retinopathy across different modalities, is unique in its scope. Additionally, this paper serves as a helpful guide for researchers who want to work in the field of retinal image analysis in the future.},
  archive      = {J_AIR},
  author       = {Hassan, Bilal and Raja, Hina and Hassan, Taimur and Akram, Muhammad Usman and Raja, Hira and Abd-alrazaq, Alaa A. and Yousefi, Siamak and Werghi, Naoufel},
  doi          = {10.1007/s10462-024-10736-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-93},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of artificial intelligence models for screening major retinal diseases},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified crayfish optimization algorithm for solving
multiple engineering application problems. <em>AIR</em>, <em>57</em>(5),
1–57. (<a href="https://doi.org/10.1007/s10462-024-10738-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crayfish Optimization Algorithm (COA) is innovative and easy to implement, but the crayfish search efficiency decreases in the later stage of the algorithm, and the algorithm is easy to fall into local optimum. To solve these problems, this paper proposes an modified crayfish optimization algorithm (MCOA). Based on the survival habits of crayfish, MCOA proposes an environmental renewal mechanism that uses water quality factors to guide crayfish to seek a better environment. In addition, integrating a learning strategy based on ghost antagonism into MCOA enhances its ability to evade local optimality. To evaluate the performance of MCOA, tests were performed using the IEEE CEC2020 benchmark function and experiments were conducted using four constraint engineering problems and feature selection problems. For constrained engineering problems, MCOA is improved by 11.16%, 1.46%, 0.08% and 0.24%, respectively, compared with COA. For feature selection problems, the average fitness value and accuracy are improved by 55.23% and 10.85%, respectively. MCOA shows better optimization performance in solving complex spatial and practical application problems. The combination of the environment updating mechanism and the learning strategy based on ghost antagonism significantly improves the performance of MCOA. This discovery has important implications for the development of the field of optimization.},
  archive      = {J_AIR},
  author       = {Jia, Heming and Zhou, Xuelian and Zhang, Jinrui and Abualigah, Laith and Yildiz, Ali Riza and Hussien, Abdelazim G.},
  doi          = {10.1007/s10462-024-10738-x},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Modified crayfish optimization algorithm for solving multiple engineering application problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development and trending of deep learning methods for wind
power predictions. <em>AIR</em>, <em>57</em>(5), 1–49. (<a
href="https://doi.org/10.1007/s10462-024-10728-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing data availability in wind power production processes due to advanced sensing technologies, data-driven models have become prevalent in studying wind power prediction (WPP) methods. Deep learning models have gained popularity in recent years due to their ability of handling high-dimensional input, automating data feature engineering, and providing high flexibility in modeling. However, with a large volume of deep learning based WPP studies developed in recent literature, it is important to survey the existing developments and their contributions in solving the issue of wind power uncertainty. This paper revisits deep learning-based wind power prediction studies from two perspectives, deep learning-enabled WPP formulations and developed deep learning methods. The advancement of WPP formulations is summarized from the following perspectives, the considered input and output designs as well as the performance evaluation metrics. The technical aspect review of deep learning leveraged in WPPs focuses on its advancement in feature processing and prediction model development. To derive a more insightful conclusion on the so-far development, over 140 recent deep learning-based WPP studies have been covered. Meanwhile, we have also conducted a comparative study on a set of deep models widely used in WPP studies and recently developed in the machine learning community. Results show that DLinear obtains more than 2% improvements by benchmarking a set of strong deep learning models. Potential research directions for WPPs, which can bring profound impacts, are also highlighted.},
  archive      = {J_AIR},
  author       = {Liu, Hong and Zhang, Zijun},
  doi          = {10.1007/s10462-024-10728-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Development and trending of deep learning methods for wind power predictions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stable mapping of nmODE. <em>AIR</em>, <em>57</em>(5),
1–23. (<a href="https://doi.org/10.1007/s10462-024-10749-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks pose significant challenges to the reliability and performance of neural networks. Despite the development of several defense mechanisms targeting various types of adversarial perturbations, only a few manage to strike a balance between theoretical soundness and practical efficacy. nmODE (neural memory ordinary differential equation) is a recently proposed model with several intriguing properties. By delving into the rare attribute of global attractors inherent in nmODE, this paper unveils its stable mapping, thereby conferring certified defense capabilities upon it. Moreover, a novel quantitative approach is proposed, establishing a mathematical link between perturbations and nmODE’s defense proficiency. Additionally, a training technique termed as nmODE+ is put forward, enhancing the defense capability of nmODE without imposing additional training burdens. Extensive experiments demonstrate nmODE’s resilience to various perturbations, showcasing its seamless integration with neural networks and existing defense mechanisms. These findings offer valuable insights into leveraging differential equations for robust neural network security.},
  archive      = {J_AIR},
  author       = {Luo, Haiying and He, Tao and Yi, Zhang},
  doi          = {10.1007/s10462-024-10749-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A stable mapping of nmODE},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diabetic retinopathy detection using supervised and
unsupervised deep learning: A review study. <em>AIR</em>,
<em>57</em>(5), 1–66. (<a
href="https://doi.org/10.1007/s10462-024-10770-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severe progression of Diabetes Mellitus (DM) stands out as one of the most significant concerns for healthcare officials worldwide. Diabetic Retinopathy (DR) is a common complication associated with diabetes, particularly affecting individuals between the ages of 18 and 65. As per the findings of the International Diabetes Federation (IDF) report, 35–60% of individuals suffering from DR possess a diabetes history. DR emerges as a leading cause of worldwide visual impairment. Due to the absence of ophthalmologists worldwide, insufficient health resources, and healthcare services, patients cannot get timely eye screening services. Automated computer-aided detection of DR provides a wide range of potential benefits. In contrast to traditional observer-driven techniques, automatic detection allows for a more objective analysis of numerous images in a shorter time. Moreover, Unsupervised Learning (UL) holds a high potential for image classification in healthcare, particularly regarding explainability and interpretability. Many studies on the detection of DR with both supervised and unsupervised Deep Learning (DL) methodologies are available. Surprisingly, none of the reviews presented thus far have highlighted the potential benefits of both supervised and unsupervised DL methods in Medical Imaging for the detection of DR. After a rigorous selection process, 103 articles were retrieved from four diverse and well-known databases (Web of Science, Scopus, ScienceDirect, and IEEE). This review provides a comprehensive summary of both supervised and unsupervised DL methods applied in DR detection, explaining the significant benefits of both techniques and covering aspects such as datasets, pre-processing, segmentation techniques, and supervised and unsupervised DL methods for detection. The insights from this review will aid academics and researchers in medical imaging to make informed decisions and choose the best practices for DR detection.},
  archive      = {J_AIR},
  author       = {Naz, Huma and Ahuja, Neelu Jyothi and Nijhawan, Rahul},
  doi          = {10.1007/s10462-024-10770-x},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Diabetic retinopathy detection using supervised and unsupervised deep learning: A review study},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using machine learning to predict artistic styles: An
analysis of trends and the research agenda. <em>AIR</em>,
<em>57</em>(5), 1–66. (<a
href="https://doi.org/10.1007/s10462-024-10727-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of art, machine learning models have been used to predict artistic styles in paintings. The foregoing is somewhat advantageous for analysts, as these tools can provide more valuable results and help reduce bias in the results and conclusions provided. Therefore, the objective of this research was to examine research trends in the use of machine learning to predict artistic styles from a bibliometric review based on the PRISMA methodology. From the search equations, 268 documents were found, out of which, following the application of inclusion and exclusion criteria, 128 documents were analyzed. Through quantitative analysis, a growing research interest in the subject is evident, progressing from user perception approaches to the utilization of tools like deep learning for art studies. Among the main results, it is possible to identify that one of the most used techniques in the field has been neural networks for pattern recognition. Also, a large part of the research focuses on the use of design software for image creation and manipulation. Finally, it is found that the number of studies focused on contemporary modern art is still limited, this is due to the fact that a large part of the investigations has focused on historical artistic styles.},
  archive      = {J_AIR},
  author       = {Valencia, Jackeline and Pineda, Geraldine García and Pineda, Vanessa García and Valencia-Arias, Alejandro and Arcila-Diaz, Juan and de la Puente, Renata Teodori},
  doi          = {10.1007/s10462-024-10727-0},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Using machine learning to predict artistic styles: An analysis of trends and the research agenda},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying the ethics of AI: A systematic review of tools for
developing and assessing AI-based systems. <em>AIR</em>, <em>57</em>(5),
1–30. (<a href="https://doi.org/10.1007/s10462-024-10740-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI)-based systems and their increasingly common use have made it a ubiquitous technology; Machine Learning algorithms are present in streaming services, social networks, and in the health sector. However, implementing this emerging technology carries significant social and ethical risks and implications. Without ethical development of such systems, there is the potential for this technology to undermine people’s autonomy, privacy, and equity, even affecting human rights. Considering the approaches necessary for ethical development and effective governance of AI, such as ethical principles, guidelines, and technical tools, the question arises regarding the limitations of implementing these measures by the highly technical personnel involved in the process. In this context, we propose the creation of a typology that distinguishes the different stages of the AI life-cycle, the high-level ethical principles that should govern their implementation, and the tools with the potential to foster compliance with these principles, encompassing both technical and conceptual resources. In addition, this typology will include relevant information such as developmental level, related tasks, sectors, and language. Our research is based on a systematic review in which we identified 352 resources and tools. We expect this contribution to be valuable in promoting ethical AI development for developers and leaders who manage these initiatives. The complete typology and the comprehensive list of resources are available for consultation at https://ricardo-ob.github.io/tools4responsibleai .},
  archive      = {J_AIR},
  author       = {Ortega-Bolaños, Ricardo and Bernal-Salcedo, Joshua and Germán Ortiz, Mariana and Galeano Sarmiento, Julian and Ruz, Gonzalo A. and Tabares-Soto, Reinel},
  doi          = {10.1007/s10462-024-10740-3},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applying the ethics of AI: A systematic review of tools for developing and assessing AI-based systems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lob-based deep learning models for stock price trend
prediction: A benchmark study. <em>AIR</em>, <em>57</em>(5), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10715-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation, and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.},
  archive      = {J_AIR},
  author       = {Prata, Matteo and Masi, Giuseppe and Berti, Leonardo and Arrigoni, Viviana and Coletta, Andrea and Cannistraci, Irene and Vyetrenko, Svitlana and Velardi, Paola and Bartolini, Novella},
  doi          = {10.1007/s10462-024-10715-4},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Lob-based deep learning models for stock price trend prediction: A benchmark study},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of nature-inspired algorithms on single-objective
optimization problems from 2019 to 2023. <em>AIR</em>, <em>57</em>(5),
1–51. (<a href="https://doi.org/10.1007/s10462-024-10747-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of nature inspired algorithm (NIA) is a vital area of research that consistently aids in solving optimization problems. One of the metaheuristic algorithm classifications that has drawn attention from researchers in recent decades is NIA. It makes a significant contribution by addressing numerous large-scale problems and achieving the best results. This research aims to identify the optimal NIA for solving single-objective optimization problems. The NIA discovered between 2019 and 2023 is presented in this study with a brief description. About 83 distinct NIAs have been studied in this study in order to address the optimization issues. In order to accomplish this goal, we have taken into consideration eight real-world single-objective optimization problems: the 3-bar truss design problem, the rolling element bearing, the pressure vessel, the cantilever beam, the I beam, the design of a welded beam, and the design of a spring. Based on a comparative study and bibliographic analysis, we have determined that two algorithms—the flow direction algorithm, and prairie dog optimization—give us the best results and optimal solutions for all eight of the engineering problems listed. Lastly, some perspectives on the limitations, difficulties, and future course are provided. In addition to providing future research guidelines, this will assist the novice and emerging researcher in providing a more comprehensive perspective on advanced NIA.},
  archive      = {J_AIR},
  author       = {Rani, Rekha and Jain, Sarika and Garg, Harish},
  doi          = {10.1007/s10462-024-10747-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of nature-inspired algorithms on single-objective optimization problems from 2019 to 2023},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new semi-local centrality for identifying influential
nodes based on local average shortest path with extended neighborhood.
<em>AIR</em>, <em>57</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10725-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the importance of nodes in complex networks is known as the problem of identifying influential nodes and is considered a critical aspect in interacting with these networks. This problem has many applications such as controlling rumors, sickness spreading, and viral marketing, where its importance has been understood by the research society in the last decade. This paper proposes a new semi-local centrality to identify influential nodes in complex networks based on the theory of Local Average Shortest Path with extended Neighborhood concept (LASPN). LASPN focuses on a distributed technique to extract the subgraph associated with each node and apply the average shortest path theory to it. We use the extended neighborhood concept to find the nearest neighbors of each node with low complexity, where this can lead to high efficiency in dealing with large-scale networks. In addition to applying relative changes in the average shortest path, the proposed metric considers the importance of the node itself as well as its nearest neighbors in ranking the nodes. Evaluation of the proposed centrality metric has been done through numerical simulations on several real-world networks. The results based on Kendall&#39;s $$\tau$$ coefficient under the SIR infection spreading model show that LASPN improves the performance by 2.7% compared to the best available equivalent method.},
  archive      = {J_AIR},
  author       = {Xiao, Yi and Chen, Yuan and Zhang, Hongyan and Zhu, Xinghui and Yang, Yimin and Zhu, Xiaoping},
  doi          = {10.1007/s10462-024-10725-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A new semi-local centrality for identifying influential nodes based on local average shortest path with extended neighborhood},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Business chatbots with deep learning technologies:
State-of-the-art, taxonomies, and future research directions.
<em>AIR</em>, <em>57</em>(5), 1–63. (<a
href="https://doi.org/10.1007/s10462-024-10744-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the support of advanced hardware and software technology, Artificial Intelligence (AI) techniques, especially the increasing number of deep learning algorithms, have spawned the popularization of online intelligent services and accelerated the contemporary development and applications of chatbot systems. The promise of providing 24/7 uninterrupted business services and minimizing workforce costs has made business chatbots a hot topic due to the impact of the pandemic. It has attracted considerable attention from academic researchers and business practitioners. However, a thorough technical review of advanced chatbot technologies and their relevance and applications to various business domains is rare in the literature. The main contribution of this review article is the critical analysis of various chatbot development approaches and the underlying deep learning computational methods in the context of some business applications. We first conceptualize current business chatbot architectures and illustrate the technical characteristics of two common structures. Next, we explore the mainstream deep learning technologies in chatbot design from the perspective of computational methods and usages. Then, we propose a new framework to classify chatbot construction architectures and differentiate the traditional retrieval-based and generation-based chatbots in terms of the modern pipeline and end-to-end structures. Finally, we highlight future research directions for business chatbots to enable researchers to devote their efforts to the most promising research topics and commercial scenarios and for practitioners to benefit from realizing the trend in business chatbot development and applications.},
  archive      = {J_AIR},
  author       = {Zhang, Yongxiang and Lau, Raymond Y. K. and David Xu, Jingjun and Rao, Yanghui and Li, Yuefeng},
  doi          = {10.1007/s10462-024-10744-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Business chatbots with deep learning technologies: State-of-the-art, taxonomies, and future research directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position adaptive residual block and knowledge complement
strategy for point cloud analysis. <em>AIR</em>, <em>57</em>(5), 1–19.
(<a href="https://doi.org/10.1007/s10462-024-10754-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the sparsity, irregularity and disorder of the point cloud, the tasks related to it are full of challenges. Exploring local geometric patterns and multi-scale features is effective for point cloud understanding, and promising results have been achieved. In this paper, we present a Position Adaptive Residual Block, namely PARB, for the first time. It can carry out powerful geometric signal description and feature learning. Starting from this module, we propose two extensions. First, a Position Adaptive Residual Network, called PARNet, is derived by utilizing PARB. Second, PARB can be regarded as a plug-and-play module embedded in MLP-based networks, which can remarkably enhance the performance of the backbone. We also introduce an efficient Knowledge Complement Strategy, which is part of the PARNet architecture, to make the framework perform better. Extensive experimental results on challenging benchmarks demonstrate that our PARNet delivers the new state-of-the-art on ShapeNet-Part and achieves competitive performance on ModelNet40.},
  archive      = {J_AIR},
  author       = {Zhang, Shichao and Shen, Hangchi and Duan, Shukai and Wang, Lidan},
  doi          = {10.1007/s10462-024-10754-x},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Position adaptive residual block and knowledge complement strategy for point cloud analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning-based methods for resource
scheduling in cloud computing: A review and future directions.
<em>AIR</em>, <em>57</em>(5), 1–42. (<a
href="https://doi.org/10.1007/s10462-024-10756-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the acceleration of the Internet in Web 2.0, Cloud computing is a new paradigm to offer dynamic, reliable and elastic computing services. Efficient scheduling of resources or optimal allocation of requests is one of the prominent issues in emerging Cloud computing. Considering the growing complexity of Cloud computing, future Cloud systems will require more effective resource management methods. In some complex scenarios with difficulties in directly evaluating the performance of scheduling solutions, classic algorithms (such as heuristics and meta-heuristics) will fail to obtain an effective scheme. Deep reinforcement learning (DRL) is a novel method to solve scheduling problems. Due to the combination of deep learning and reinforcement learning (RL), DRL has achieved considerable performance in current studies. To focus on this direction and analyze the application prospect of DRL in Cloud scheduling, we provide a comprehensive review for DRL-based methods in resource scheduling of Cloud computing. Through the theoretical formulation of scheduling and analysis of RL frameworks, we discuss the advantages of DRL-based methods in Cloud scheduling. We also highlight different challenges and discuss the future directions existing in the DRL-based Cloud scheduling.},
  archive      = {J_AIR},
  author       = {Zhou, Guangyao and Tian, Wenhong and Buyya, Rajkumar and Xue, Ruini and Song, Liang},
  doi          = {10.1007/s10462-024-10756-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep reinforcement learning-based methods for resource scheduling in cloud computing: A review and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-sensitive learning for imbalanced medical data: A
review. <em>AIR</em>, <em>57</em>(4), 1–72. (<a
href="https://doi.org/10.1007/s10462-023-10652-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Machine Learning (ML) in medicine has unlocked many opportunities to harness complex medical data, enhancing patient outcomes and advancing the field. However, the inherent imbalanced distribution of medical data poses a significant challenge, resulting in biased ML models that perform poorly on minority classes. Mitigating the impact of class imbalance has prompted researchers to explore various strategies, wherein Cost-Sensitive Learning (CSL) arises as a promising approach to improve the accuracy and reliability of ML models. This paper presents the first review of CSL for imbalanced medical data. A comprehensive exploration of the existing literature encompassed papers published from January 2010 to December 2022 and sourced from five major digital libraries. A total of 173 papers were selected, analysed, and classified based on key criteria, including publication years, channels and sources, research types, empirical types, medical sub-fields, medical tasks, CSL approaches, strengths and weaknesses of CSL, frequently used datasets and data types, evaluation metrics, and development tools. The results indicate a noteworthy publication rise, particularly since 2020, and a strong preference for CSL direct approaches. Data type analysis unveiled diverse modalities, with medical images prevailing. The underutilisation of cost-related metrics and the prevalence of Python as the primary programming tool are highlighted. The strengths and weaknesses analysis covered three aspects: CSL strategy, CSL approaches, and relevant works. This study serves as a valuable resource for researchers seeking to explore the current state of research, identify strengths and gaps in the existing literature and advance CSL’s application for imbalanced medical data.},
  archive      = {J_AIR},
  author       = {Araf, Imane and Idri, Ali and Chairi, Ikram},
  doi          = {10.1007/s10462-023-10652-8},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-72},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Cost-sensitive learning for imbalanced medical data: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resampling strategies for imbalanced regression: A survey
and empirical analysis. <em>AIR</em>, <em>57</em>(4), 1–42. (<a
href="https://doi.org/10.1007/s10462-024-10724-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced problems can arise in different real-world situations, and to address this, certain strategies in the form of resampling or balancing algorithms are proposed. This issue has largely been studied in the context of classification, and yet, the same problem features in regression tasks, where target values are continuous. This work presents an extensive experimental study comprising various balancing and predictive models, and wich uses metrics to capture important elements for the user and to evaluate the predictive model in an imbalanced regression data context. It also proposes a taxonomy for imbalanced regression approaches based on three crucial criteria: regression model, learning process, and evaluation metrics. The study offers new insights into the use of such strategies, highlighting the advantages they bring to each model’s learning process, and indicating directions for further studies. The code, data and further information related to the experiments performed herein can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression .},
  archive      = {J_AIR},
  author       = {Avelino, Juscimara G. and Cavalcanti, George D. C. and Cruz, Rafael M. O.},
  doi          = {10.1007/s10462-024-10724-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Resampling strategies for imbalanced regression: A survey and empirical analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the integration of artificial intelligence (AI)
and augmented reality (AR) in maritime medicine. <em>AIR</em>,
<em>57</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s10462-024-10735-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This narrative literature review has analyzed the integration of artificial intelligence (AI) and augmented reality (AR) in the field of maritime medicine. A comprehensive search was conducted in academic databases using relevant search terms, resulting in the identification of 257 records. After screening for relevance and quality, a final review was conducted on 17 papers. This review highlights the potential applications and benefits of AI and AR in enhancing medical practices and safety measures for seafarers. The integration of AI and AR technologies in maritime medicine shows promise in providing real-time medical assistance, remote consultations, augmented training, and improved diagnostic capabilities. Additionally, AI-driven predictive models can aid in early detection of health issues and support proactive health management onboard ships. Challenges related to data privacy, connectivity at sea, and the need for regulatory frameworks are also discussed. The data analysis reported in this review contributes to a better understanding of the current state and future potential of AI and AR in maritime medicine and provide insights into opportunities for further research and implementation in the maritime industry.},
  archive      = {J_AIR},
  author       = {Battineni, Gopi and Chintalapudi, Nalini and Ricci, Giovanna and Ruocco, Ciro and Amenta, Francesco},
  doi          = {10.1007/s10462-024-10735-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring the integration of artificial intelligence (AI) and augmented reality (AR) in maritime medicine},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emb-trattunet: A novel edge loss function and
transformer-CNN architecture for multi-classes pneumonia infection
segmentation in low annotation regimes. <em>AIR</em>, <em>57</em>(4),
1–35. (<a href="https://doi.org/10.1007/s10462-024-10717-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary challenges in applying deep learning approaches to medical imaging is the limited availability of data due to various factors. These factors include concerns about data privacy and the requirement for expert radiologists to perform the time-consuming and labor-intensive task of labeling data, particularly for tasks such as segmentation. Consequently, there is a critical need to develop novel approaches for few-shot learning tasks in this domain. In this work, we propose a Novel CNN-Transformer Fusion scheme to segment Multi-classes pneumonia infection from limited CT-scans data. In total, there are three main contributions: (i) CNN-Transformer encoders fusion, which allows to extract and fuse richer features in the encoding phase, which contains: local, global and long-range dependencies features, (ii) Multi-Branches Skip Connection (MBSC) is proposed to extract and fuse richer features from the encoder features then integrate them into the decoder layers, where MBSC blocks extract higher-level features related to the finer details of different infection types, and (iii) a Multi-classes Boundary Aware Cross-Entropy (MBA-CE) Loss function is proposed to deal with fuzzy boundaries, enhance the separability between classes and give more attention to the minority classes. The performance of the proposed approach is evaluated using two evaluation scenarios and compared with different baseline and state-of-the-art segmentation architectures for Multi-classes Covid-19 segmentation. The obtained results show that our approach outperforms the comparison methods in both Ground-Glass Opacity (GGO) and Consolidation segmentation. On the other hand, our approach shows consistent performance when the training data is reduced to half, which proves the efficiency of our approach in few-shot learning. In contrast, the performance of the comparison methods drops in this scenario. Moreover, our approach is able to deal with imbalanced data classes. These advantages prove the effectiveness and efficiency of the proposed EMB-TrAttUnet approach in a pandemic scenario where time is critical to save patient lives.},
  archive      = {J_AIR},
  author       = {Bougourzi, Fares and Dornaika, Fadi and Nakib, Amir and Taleb-Ahmed, Abdelmalik},
  doi          = {10.1007/s10462-024-10717-2},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Emb-trattunet: A novel edge loss function and transformer-CNN architecture for multi-classes pneumonia infection segmentation in low annotation regimes},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling the mysteries of AI chatbots. <em>AIR</em>,
<em>57</em>(4), 1–35. (<a
href="https://doi.org/10.1007/s10462-024-10720-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This primer provides an overview of the rapidly evolving field of generative artificial intelligence, specifically focusing on large language models like ChatGPT (OpenAI) and Bard (Google). Large language models have demonstrated unprecedented capabilities in responding to natural language prompts. The aim of this primer is to demystify the underlying theory and architecture of large language models, providing intuitive explanations for a broader audience. Learners seeking to gain insight into the technical underpinnings of large language models must sift through rapidly growing and fragmented literature on the topic. This primer brings all the main concepts into a single digestible document. Topics covered include text tokenization, vocabulary construction, token embedding, context embedding with attention mechanisms, artificial neural networks, and objective functions in model training. The primer also explores state-of-the-art methods in training large language models to generalize on specific applications and to align with human intentions. Finally, an introduction to the concept of prompt engineering highlights the importance of effective human-machine interaction through natural language in harnessing the full potential of artificial intelligence chatbots. This comprehensive yet accessible primer will benefit students and researchers seeking foundational knowledge and a deeper understanding of the inner workings of existing and emerging artificial intelligence models. The author hopes that the primer will encourage further responsible innovation and informed discussions about these increasingly powerful tools.},
  archive      = {J_AIR},
  author       = {Bridgelall, Raj},
  doi          = {10.1007/s10462-024-10720-7},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Unraveling the mysteries of AI chatbots},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence and multimodal data fusion for smart
healthcare: Topic modeling and bibliometrics. <em>AIR</em>,
<em>57</em>(4), 1–52. (<a
href="https://doi.org/10.1007/s10462-024-10712-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in artificial intelligence (AI) have driven extensive research into developing diverse multimodal data analysis approaches for smart healthcare. There is a scarcity of large-scale analysis of literature in this field based on quantitative approaches. This study performed a bibliometric and topic modeling examination on 683 articles from 2002 to 2022, focusing on research topics and trends, journals, countries/regions, institutions, authors, and scientific collaborations. Results showed that, firstly, the number of articles has grown from 1 in 2002 to 220 in 2022, with a majority being published in interdisciplinary journals that link healthcare and medical research and information technology and AI. Secondly, the significant rise in the quantity of research articles can be attributed to the increasing contribution of scholars from non-English speaking countries/regions and the noteworthy contributions made by authors in the USA and India. Thirdly, researchers show a high interest in diverse research issues, especially, cross-modality magnetic resonance imaging (MRI) for brain tumor analysis, cancer prognosis through multi-dimensional data analysis, and AI-assisted diagnostics and personalization in healthcare, with each topic experiencing a significant increase in research interest. There is an emerging trend towards issues such as applying generative adversarial networks and contrastive learning for multimodal medical image fusion and synthesis and utilizing the combined spatiotemporal resolution of functional MRI and electroencephalogram in a data-centric manner. This study is valuable in enhancing researchers’ and practitioners’ understanding of the present focal points and upcoming trajectories in AI-powered smart healthcare based on multimodal data analysis.},
  archive      = {J_AIR},
  author       = {Chen, Xieling and Xie, Haoran and Tao, Xiaohui and Wang, Fu Lee and Leng, Mingming and Lei, Baiying},
  doi          = {10.1007/s10462-024-10712-7},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence and multimodal data fusion for smart healthcare: Topic modeling and bibliometrics},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot social-aware cooperative planning in pedestrian
environments using attention-based actor-critic. <em>AIR</em>,
<em>57</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s10462-024-10739-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safe and efficient cooperative planning of multiple robots in pedestrian participation environments is promising for applications. In this paper, a novel multi-robot social-aware efficient cooperative planner on the basis of off-policy multi-agent reinforcement learning (MARL) under partial dimension-varying observation and imperfect perception conditions is proposed. We adopt a temporal-spatial graph (TSG)-based social encoder to better extract the importance of social relations between each robot and the pedestrians in its field of view (FOV). Also, we introduce a K-step lookahead reward setting in the multi-robot RL framework to avoid aggressive, intrusive, short-sighted, and unnatural motion decisions generated by robots. Moreover, we improve the traditional centralized critic network with a multi-head global attention module to better aggregate local observation information among different robots to guide the process of the individual policy update. Finally, multi-group experimental results verify the effectiveness of the proposed cooperative motion planner.},
  archive      = {J_AIR},
  author       = {Dong, Lu and He, Zichen and Song, Chunwei and Yuan, Xin and Zhang, Haichao},
  doi          = {10.1007/s10462-024-10739-w},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-robot social-aware cooperative planning in pedestrian environments using attention-based actor-critic},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evidential linguistic ELECTRE method for selection of
emergency shelter sites. <em>AIR</em>, <em>57</em>(4), 1–36. (<a
href="https://doi.org/10.1007/s10462-024-10709-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many circumstances, decisions are based on subjective experience. However, some views can be vague, meaning that policymakers do not know exactly how they should express their opinions. Therefore, it is necessary for researchers to provide scientific decision frameworks, among which the multi-criteria decision making (MCDM) method in the linguistic environment is gradually favored by scholars. A large body of literature reports relevant approaches with regard to linguistic term sets, but existing approaches are insufficient to express the subjective thoughts of policymakers in a complex and uncertain environment. In this paper, we address this problem by introducing the concept of evidential linguistic term set (ELTS). ELTS generalizes many other uncertainty representations under linguistic context, such as fuzzy sets, probabilities, or possibility distributions. Measures on ELTS, such as uncertainty measure, dissimilarity measure and expectation function, provide general frameworks to handle uncertain information. Modeling and reasoning of information expressed by ELTSs are realized by the proposed aggregation operators. Subsequently, this paper presents a novel MCDM approach called evidential linguistic ELECTRE method, and applies it to the case of selection of emergency shelter sites. The findings demonstrate the effectiveness of the proposed method for MCDM problems under linguistic context and highlight the significance of the developed ELTS.},
  archive      = {J_AIR},
  author       = {Fei, Liguo and Liu, Xiaoyu and Zhang, Changping},
  doi          = {10.1007/s10462-024-10709-2},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An evidential linguistic ELECTRE method for selection of emergency shelter sites},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning heralding a new development phase in
molecular dynamics simulations. <em>AIR</em>, <em>57</em>(4), 1–36. (<a
href="https://doi.org/10.1007/s10462-024-10731-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dynamics (MD) simulations are a key computational chemistry technique that provide dynamic insight into the underlying atomic-level processes in the system under study. These insights not only improve our understanding of the molecular world, but also aid in the design of experiments and targeted interventions. Currently, MD is associated with several limitations, the most important of which are: insufficient sampling, inadequate accuracy of the atomistic models, and challenges with proper analysis and interpretation of the obtained trajectories. Although numerous efforts have been made to address these limitations, more effective solutions are still needed. The recent development of artificial intelligence, particularly machine learning (ML), offers exciting opportunities to address the challenges of MD. In this review we aim to familiarize readers with the basics of MD while highlighting its limitations. The main focus is on exploring the integration of deep learning with MD simulations. The advancements made by ML are systematically outlined, including the development of ML-based force fields, techniques for improved conformational space sampling, and innovative methods for trajectory analysis. Additionally, the challenges and implications associated with the integration of ML and artificial intelligence are discussed. While the potential of ML-MD fusion is clearly established, further applications are needed to confirm its superiority over traditional methods. This comprehensive overview of the new perspectives of MD, which ML has opened up, serves as a gentle introduction to the exciting phase of MD development.},
  archive      = {J_AIR},
  author       = {Prašnikar, Eva and Ljubič, Martin and Perdih, Andrej and Borišek, Jure},
  doi          = {10.1007/s10462-024-10731-4},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning heralding a new development phase in molecular dynamics simulations},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New formulation for predicting total dissolved gas
supersaturation in dam reservoir: Application of hybrid artificial
intelligence models based on multiple signal decomposition.
<em>AIR</em>, <em>57</em>(4), 1–37. (<a
href="https://doi.org/10.1007/s10462-024-10707-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Total dissolved gas (TDG) concentration plays an important role in the control of the aquatic life. Elevated TDG can cause gas-bubble trauma in fish (GBT). Therefore, controlling TDG fluctuation has become of great importance for different disciplines of surface water environmental engineering.. Nowadays, direct estimation of TDG is expensive and time-consuming. Hence, this work proposes a new modelling framework for predicting TDG based on the integration of machine learning (ML) models and multiresolution signal decomposition. The proposed ML models were trained and validated using hourly data obtained from four stations at the United States Geological Survey. The dataset are composed from: (i) water temperature (Tw), (ii) barometric pressure (BP), and (iii) discharge (Q), which were used as the input variables for TDG prediction. The modelling strategy is conducted based on two different steps. First, six singles ML model namely: (i) multilayer perceptron neural network, (ii) Gaussian process regression, (iii) random forest regression, (iv) random vector functional link, (v) adaptive boosting, and (vi) Bootstrap aggregating (Bagging), were developed for predicting TDG using Tw, BP, and Q, and their performances were compared. Second, a new framework was introduced based on the combination of empirical mode decomposition (EMD), the variational mode decomposition (VMD), and the empirical wavelet transform (EWT) preprocessing signal decomposition algorithms with ML models for building new hybrid ML models. Hence, the Tw, BP, and Q signals were decomposed to extract the intrinsic mode functions (IMFs) by using the EMD and VMD methods and the multiresolution analysis (MRA) components by using the EWT method. Then after, the IMFs and MRA components were selected and regraded as new input variables for the ML models and used as an integral part thereof. The single and hybrid prediction models were compared using several statistical metrics namely, root mean square error, mean absolute error, coefficient of determination (R2), and Nash–Sutcliffe efficiency (NSE). The single and hybrid models were trained several times with high number of repetitions, depending on the kind of modeling process. The obtained results using single models gave good agreement between the predicted TDG and the situ measured dataset. Overall, the Bagging model performed better than the other five models with R2 and NSE values of 0.906 and 0.902, respectively. However, the extracted IMFs and MRA components using the EMD, VMD and the EWT have contributed to an improvement of the hybrid models’ performances, for which the R2 and NSE were significantly increased reaching the values of 0.996 and 0.995. Experimental results showed the superiority of hybrid models and more importantly the importance of signal decomposition in improving the predictive accuracy of TDG.},
  archive      = {J_AIR},
  author       = {Heddam, Salim and Al-Areeq, Ahmed M. and Tan, Mou Leong and Ahmadianfar, Iman and Halder, Bijay and Demir, Vahdettin and Kilinc, Huseyin Cagan and Abba, Sani I. and Oudah, Atheer Y. and Yaseen, Zaher Mundher},
  doi          = {10.1007/s10462-024-10707-4},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {New formulation for predicting total dissolved gas supersaturation in dam reservoir: Application of hybrid artificial intelligence models based on multiple signal decomposition},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spline magnitude disparity cross correlated deep network for
gait recognition. <em>AIR</em>, <em>57</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s10462-023-10676-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition stands as a pivotal biometric technology in individual identification, yet its real-world implementation faces challenges stemming from intra-subject disparities. The task of extracting consistent features to distinguish among various subjects becomes onerous due to factors such as image noise and magnitude divergence, significantly impacting recognition accuracy. In addressing this hurdle, we introduce a groundbreaking approach known as the Spline Magnitude Disparity Cross-Correlated Deep Network, designed to optimize gait recognition efficiency. Our method, the Spline Magnitude Disparity Cross-Correlated Deep Network, operates through two key steps: B-Spline magnitude disparity deformation (BS-MDD) registration and cross-correlated long-short gait recognition modeling. The BS-MDD algorithm employs free-form deformation to approximate the magnitude divergence in gait input, enhancing viewpoint optimization and contributing to the development of the cross-correlated model. By focusing on preserving high-output recognition gates while eliminating forget gates, our approach achieves a heightened recognition rate. Evaluation on the widely utilized CASIA B dataset showcases the superiority of our proposed method over state-of-the-art alternatives in terms of the true positive rate, false-positive rate, recognition time, and overall recognition rate. Notably, our approach elevates the true positive rate by 5% and reduces the false-positive rate by 4%. These results underscore the high effectiveness of our method, demonstrating its capacity to substantially improve the accuracy of gait recognition in practical applications.”},
  archive      = {J_AIR},
  author       = {Jain, Deepak Kumar and Kumar, Manoj and Abualigah, Laith},
  doi          = {10.1007/s10462-023-10676-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Spline magnitude disparity cross correlated deep network for gait recognition},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level textual-visual alignment and fusion network for
multimodal aspect-based sentiment analysis. <em>AIR</em>,
<em>57</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s10462-023-10685-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Aspect-Based Sentiment Analysis (MABSA) is an essential task in sentiment analysis that has garnered considerable attention in recent years. Typical approaches in MABSA often utilize cross-modal Transformers to capture interactions between textual and visual modalities. However, bridging the semantic gap between modalities spaces and addressing interference from irrelevant visual objects at different scales remains challenging. To tackle these limitations, we present the Multi-level Textual-Visual Alignment and Fusion Network (MTVAF) in this work, which incorporates three auxiliary tasks. Specifically, MTVAF first transforms multi-level image information into image descriptions, facial descriptions, and optical characters. These are then concatenated with the textual input to form a textual+visual input, facilitating comprehensive alignment between visual and textual modalities. Next, both inputs are fed into an integrated text model that incorporates relevant visual representations. Dynamic attention mechanisms are employed to generate visual prompts to control cross-modal fusion. Finally, we align the probability distributions of the textual input space and the textual+visual input space, effectively reducing noise introduced during the alignment process. Experimental results on two MABSA benchmark datasets demonstrate the effectiveness of the proposed MTVAF, showcasing its superior performance compared to state-of-the-art approaches. Our codes are available at https://github.com/MKMaS-GUET/MTVAF .},
  archive      = {J_AIR},
  author       = {Li, You and Ding, Han and Lin, Yuming and Feng, Xinyu and Chang, Liang},
  doi          = {10.1007/s10462-023-10685-z},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-level textual-visual alignment and fusion network for multimodal aspect-based sentiment analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video shot-boundary detection: Issues, challenges and
solutions. <em>AIR</em>, <em>57</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10462-024-10742-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of high data transmission rates and the recent digital multimedia technology, paves the way to access a huge amount of video over the internet, in seconds. Additionally, uploading videos to different websites is no more confined to expert software professionals resulting in duplication of video data which led to exorbitant growth of multimedia information in cyberspace in a short span of time. This necessitates the development of efficient data management techniques including storage, searching and annotation mechanism. Automatic shot boundary detection is considered to be the first and foremost step towards such management. It is a booming area of research gaining attention in the domain of image processing, computer vision and pattern recognition. In this review paper, we present a detailed description of the methods and algorithms of shot boundary detection, reported in the last two decades. This review shows that using multiple features performs well in comparison to using only a single feature in the shot boundary detection problem although it leads to higher complexity. The major sources of disturbance in the boundary detection are the sudden illumination variation and presence of high motion in the video. An adaptive threshold outperforms a single global threshold in the boundary detection problem and the threshold requirement can be avoided through learning based strategies at the cost of larger training data and higher computation time. Moreover the present review includes a critical analysis of relative merits and demerits of existing algorithms and finally opens promising research directions in the area.},
  archive      = {J_AIR},
  author       = {Kar, T. and Kanungo, P. and Mohanty, Sachi Nandan and Groppe, Sven and Groppe, Jinghua},
  doi          = {10.1007/s10462-024-10742-1},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Video shot-boundary detection: Issues, challenges and solutions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A chaotic based image encryption scheme using elliptic curve
cryptography and genetic algorithm. <em>AIR</em>, <em>57</em>(4), 1–31.
(<a href="https://doi.org/10.1007/s10462-024-10719-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of digital communication and data security, image encryption plays a crucial role in safeguarding sensitive information. Protecting sensitive visual data from unauthorized access drives the pursuit of advanced image encryption methods. This paper proposes a novel approach to enhance image encryption by combining the power of a chaotic map, elliptic curve cryptography, and genetic algorithm. The chaotic map, specifically Arnold’s cat map, is employed to introduce chaos and randomness into the encryption process. The proposed image encryption process involves applying Arnold’s cat map for shuffling the pixel positions, followed by elliptic curve cryptography for encrypting the pixel values using public and private keys. Additionally, a genetic algorithm is employed to optimize the key generation process, enhancing the security of the encryption scheme. The combined utilization of these techniques aims to achieve a high level of confidentiality and robustness in image encryption. The algorithm underwent thorough analysis. It achieved a maximum entropy score of 7.99, indicating a high level of randomness and unpredictability in the encrypted data. Additionally, it exhibited near-zero correlation, which suggests strong resistance against statistical attacks. Moreover, the cryptographic range of possible keys was found to be $$2^{511}$$ . This extensive key space makes the algorithm highly resilient against brute force attacks. It took only 0.5634 s to encrypt a moderately sized $$512\times 512$$ pixel image with an 8-bit image on a standard desktop computer with a 2.3 GHz processor and 16 GB of RAM. The experimental findings confirm that the proposed approach is highly effective and efficient in safeguarding sensitive image data from unauthorized access and potential attacks. This scheme has the benefit of allowing us to protect our private image data while it’s being transmitted.},
  archive      = {J_AIR},
  author       = {Kumar, Sanjay and Sharma, Deepmala},
  doi          = {10.1007/s10462-024-10719-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A chaotic based image encryption scheme using elliptic curve cryptography and genetic algorithm},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environmental, social, and governance (ESG) and artificial
intelligence in finance: State-of-the-art and research takeaways.
<em>AIR</em>, <em>57</em>(4), 1–64. (<a
href="https://doi.org/10.1007/s10462-024-10708-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly growing research landscape in finance, encompassing environmental, social, and governance (ESG) topics and associated Artificial Intelligence (AI) applications, presents challenges for both new researchers and seasoned practitioners. This study aims to systematically map the research area, identify knowledge gaps, and examine potential research areas for researchers and practitioners. The investigation focuses on three primary research questions: the main research themes concerning ESG and AI in finance, the evolution of research intensity and interest in these areas, and the application and evolution of AI techniques specifically in research studies within the ESG and AI in finance domain. Eight archetypical research domains were identified: (i) Trading and Investment, (ii) ESG Disclosure, Measurement and Governance, (iii) Firm Governance, (iv) Financial Markets and Instruments, (v) Risk Management, (vi) Forecasting and Valuation, (vii) Data, and (viii) Responsible Use of AI. Distinctive AI techniques were found to be employed across these archetypes. The study contributes to consolidating knowledge on the intersection of ESG, AI, and finance, offering an ontological inquiry and key takeaways for practitioners and researchers. Important insights include the popularity and crowding of the Trading and Investment domain, the growth potential of the Data archetype, and the high potential of Responsible Use of AI, despite its low publication count. By understanding the nuances of different research archetypes, researchers and practitioners can better navigate this complex landscape and contribute to a more sustainable and responsible financial sector.},
  archive      = {J_AIR},
  author       = {Lim, Tristan},
  doi          = {10.1007/s10462-024-10708-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Environmental, social, and governance (ESG) and artificial intelligence in finance: State-of-the-art and research takeaways},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomalous citations detection in academic networks.
<em>AIR</em>, <em>57</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10462-023-10655-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citation network analysis attracts increasing attention from disciplines of complex network analysis and science of science. One big challenge in this regard is that there are unreasonable citations in citation networks, i.e., cited papers are not relevant to the citing paper. Existing research on citation analysis has primarily concentrated on the contents and ignored the complex relations between academic entities. In this paper, we propose a novel research topic, that is, how to detect anomalous citations. To be specific, we first define anomalous citations and propose a unified framework, named ACTION, to detect anomalous citations in a heterogeneous academic network. ACTION is established based on non-negative matrix factorization and network representation learning, which considers not only the relevance of citation contents but also the relationships among academic entities including journals, papers, and authors. To evaluate the performance of ACTION, we construct three anomalous citation datasets. Experimental results demonstrate the effectiveness of the proposed method. Detecting anomalous citations carry profound significance for academic fairness.},
  archive      = {J_AIR},
  author       = {Liu, Jiaying and Bai, Xiaomei and Wang, Mengying and Tuarob, Suppawong and Xia, Feng},
  doi          = {10.1007/s10462-023-10655-5},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Anomalous citations detection in academic networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review on PID, fuzzy and hybrid fuzzy PID controllers for
controlling non-linear dynamic behaviour of chemical plants.
<em>AIR</em>, <em>57</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10743-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The chemical production process is tedious due to the integration of different types of equipment and variables. Designing the controller is crucial in the chemical industry due to the interactive and non-linear system behaviour. An intelligent autonomous controller can improve the operating efficiency of the industry. Although several controllers have been developed, different system failures are frequently reported. Hence, controllers such as proportional integral derivative (PID), fuzzy logic controller (FLC), and hybrid fuzzy PID (F-PID) applied in the chemical industries are critically reviewed in the paper. Initially, the PID controller-based approaches are reviewed for different purposes in the chemical industry. After that, the FLC-based controllers-based papers are reviewed. In order to satisfy the issues in both controllers, the H-PID controllers have been reviewed. This review paper will provide an effective solution for operation control in the chemical industry under different operating conditions.},
  archive      = {J_AIR},
  author       = {Mohindru, Pankaj},
  doi          = {10.1007/s10462-024-10743-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Review on PID, fuzzy and hybrid fuzzy PID controllers for controlling non-linear dynamic behaviour of chemical plants},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D point cloud-based place recognition: A survey.
<em>AIR</em>, <em>57</em>(4), 1–44. (<a
href="https://doi.org/10.1007/s10462-024-10713-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Place recognition is a fundamental topic in computer vision and robotics. It plays a crucial role in simultaneous localization and mapping (SLAM) systems to retrieve scenes from maps and identify previously visited places to correct cumulative errors. Place recognition has long been performed with images, and multiple survey papers exist that analyze image-based methods. Recently, 3D point cloud-based place recognition (3D-PCPR) has become popular due to the widespread use of LiDAR scanners in autonomous driving research. However, there is a lack of survey paper that discusses 3D-PCPR methods. To bridge the gap, we present a comprehensive survey of recent progress in 3D-PCPR. Our survey covers over 180 related works, discussing their strengths and weaknesses, and identifying open problems within this domain. We categorize mainstream approaches into feature-based, projection-based, segment-based, and multimodal-based methods and present an overview of typical datasets, evaluation metrics, performance comparisons, and applications in this field. Finally, we highlight some promising research directions for future exploration in this domain.},
  archive      = {J_AIR},
  author       = {Luo, Kan and Yu, Hongshan and Chen, Xieyuanli and Yang, Zhengeng and Wang, Jingwen and Cheng, Panfei and Mian, Ajmal},
  doi          = {10.1007/s10462-024-10713-6},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {3D point cloud-based place recognition: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). From understanding diseases to drug design: Can artificial
intelligence bridge the gap? <em>AIR</em>, <em>57</em>(4), 1–39. (<a
href="https://doi.org/10.1007/s10462-024-10714-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a transformative technology with significant potential to revolutionize disease understanding and drug design in healthcare. AI serves as a remarkable accelerating tool that bridges the gap between understanding diseases and discovering drugs. Given its capacity in the analysis and interpretation of massive amounts of data, AI is tremendously boosting the power of predictions with impressive accuracies. This allowed AI to pave the way for advancing all key stages of drug development, with the advantage of expediting the drug discovery process and curbing its costs. This is a comprehensive review of the recent advances in AI and its applications in drug discovery and development, starting with disease identification and spanning through the various stages involved in the drug discovery pipeline, including target identification, screening, lead discovery, and clinical trials. In addition, this review discusses the challenges that arise during the implementation of AI at each stage of the discovery process and provides insights into the future prospects of this field.},
  archive      = {J_AIR},
  author       = {Pushkaran, Anju Choorakottayil and Arabi, Alya A.},
  doi          = {10.1007/s10462-024-10714-5},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {From understanding diseases to drug design: Can artificial intelligence bridge the gap?},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive bibliometric and content analysis of
artificial intelligence in language learning: Tracing between the years
2017 and 2023. <em>AIR</em>, <em>57</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10462-023-10643-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising pervasiveness of Artificial Intelligence (AI) has led applied linguists to combine it with language teaching and learning processes. In many cases, such implementation has significantly contributed to the field. The retrospective amount of literature dedicated on the use of AI in language learning (LL) is overwhelming. Thus, the objective of this paper is to map the existing literature on Artificial Intelligence in language learning through bibliometric and content analysis. From the Scopus database, we systematically explored, after keyword refinement, the prevailing literature of AI in LL. After excluding irrelevant articles, we conducted our study with 606 documents published between 2017 and 2023 for further investigation. This review reinforces our understanding by identifying and distilling the relationships between the content, the contributions, and the contributors. The findings of the study show a rising pattern of AI in LL. Along with the metrics of performance analysis, through VOSviewer and R studio (Biblioshiny), our findings uncovered the influential authors, institutions, countries, and the most influential documents in the field. Moreover, we identified 7 clusters and potential areas of related research through keyword analysis. In addition to the bibliographic details, this review aims to elucidate the content of the field. NVivo 14 and Atlas AI were used to perform content analysis to categorize and present the type of AI used in language learning, Language learning factors, and its participants.},
  archive      = {J_AIR},
  author       = {Rahman, Abdur and Raj, Antony and Tomy, Prajeesh and Hameed, Mohamed Sahul},
  doi          = {10.1007/s10462-023-10643-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive bibliometric and content analysis of artificial intelligence in language learning: Tracing between the years 2017 and 2023},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting deep neural networks with geometrical prior
knowledge: A survey. <em>AIR</em>, <em>57</em>(4), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10722-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks achieve state-of-the-art results in many different problem settings by exploiting vast amounts of training data. However, collecting, storing and—in the case of supervised learning—labelling the data is expensive and time-consuming. Additionally, assessing the networks’ generalization abilities or predicting how the inferred output changes under input transformations is complicated since the networks are usually treated as a black box. Both of these problems can be mitigated by incorporating prior knowledge into the neural network. One promising approach, inspired by the success of convolutional neural networks in computer vision tasks, is to incorporate knowledge about symmetric geometrical transformations of the problem to solve that affect the output in a predictable way. This promises an increased data efficiency and more interpretable network outputs. In this survey, we try to give a concise overview about different approaches that incorporate geometrical prior knowledge into neural networks. Additionally, we connect those methods to 3D object detection for autonomous driving, where we expect promising results when applying those methods.},
  archive      = {J_AIR},
  author       = {Rath, Matthias and Condurache, Alexandru Paul},
  doi          = {10.1007/s10462-024-10722-5},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Boosting deep neural networks with geometrical prior knowledge: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of convolutional neural networks in computer
vision. <em>AIR</em>, <em>57</em>(4), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-10721-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, a series of exemplary advances have been made in several areas involving image classification, semantic segmentation, object detection, and image super-resolution reconstruction with the rapid development of deep convolutional neural network (CNN). The CNN has superior features for autonomous learning and expression, and feature extraction from original input data can be realized by means of training CNN models that match practical applications. Due to the rapid progress in deep learning technology, the structure of CNN is becoming more and more complex and diverse. Consequently, it gradually replaces the traditional machine learning methods. This paper presents an elementary understanding of CNN components and their functions, including input layers, convolution layers, pooling layers, activation functions, batch normalization, dropout, fully connected layers, and output layers. On this basis, this paper gives a comprehensive overview of the past and current research status of the applications of CNN models in computer vision fields, e.g., image classification, object detection, and video prediction. In addition, we summarize the challenges and solutions of the deep CNN, and future research directions are also discussed.},
  archive      = {J_AIR},
  author       = {Zhao, Xia and Wang, Limin and Zhang, Yufei and Han, Xuming and Deveci, Muhammet and Parmar, Milan},
  doi          = {10.1007/s10462-024-10721-6},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of convolutional neural networks in computer vision},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lexicographic optimisation approach to promote more recent
features on longitudinal decision-tree-based classifiers: Applications
to the english longitudinal study of ageing. <em>AIR</em>,
<em>57</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s10462-024-10718-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning algorithms rarely cope directly with the temporal information inherent to longitudinal datasets, which have multiple measurements of the same feature across several time points and are often generated by large health studies. In this paper we report on experiments which adapt the feature-selection function of decision tree-based classifiers to consider the temporal information in longitudinal datasets, using a lexicographic optimisation approach. This approach gives higher priority to the usual objective of maximising the information gain ratio, and it favours the selection of features more recently measured as a lower priority objective. Hence, when selecting between features with equivalent information gain ratio, priority is given to more recent measurements of biomedical features in our datasets. To evaluate the proposed approach, we performed experiments with 20 longitudinal datasets created from a human ageing study. The results of these experiments show that, in addition to an improvement in predictive accuracy for random forests, the changed feature-selection function promotes models based on more recent information that is more directly related to the subject’s current biomedical situation and, thus, intuitively more interpretable and actionable.},
  archive      = {J_AIR},
  author       = {Ribeiro, Caio and Freitas, Alex A.},
  doi          = {10.1007/s10462-024-10718-1},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A lexicographic optimisation approach to promote more recent features on longitudinal decision-tree-based classifiers: Applications to the english longitudinal study of ageing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the computational complexity of ethics: Moral
tractability for minds and machines. <em>AIR</em>, <em>57</em>(4), 1–90.
(<a href="https://doi.org/10.1007/s10462-024-10732-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Why should moral philosophers, moral psychologists, and machine ethicists care about computational complexity? Debates on whether artificial intelligence (AI) can or should be used to solve problems in ethical domains have mainly been driven by what AI can or cannot do in terms of human capacities. In this paper, we tackle the problem from the other end by exploring what kind of moral machines are possible based on what computational systems can or cannot do. To do so, we analyze normative ethics through the lens of computational complexity. First, we introduce computational complexity for the uninitiated reader and discuss how the complexity of ethical problems can be framed within Marr’s three levels of analysis. We then study a range of ethical problems based on consequentialism, deontology, and virtue ethics, with the aim of elucidating the complexity associated with the problems themselves (e.g., due to combinatorics, uncertainty, strategic dynamics), the computational methods employed (e.g., probability, logic, learning), and the available resources (e.g., time, knowledge, learning). The results indicate that most problems the normative frameworks pose lead to tractability issues in every category analyzed. Our investigation also provides several insights about the computational nature of normative ethics, including the differences between rule- and outcome-based moral strategies, and the implementation-variance with regard to moral resources. We then discuss the consequences complexity results have for the prospect of moral machines in virtue of the trade-off between optimality and efficiency. Finally, we elucidate how computational complexity can be used to inform both philosophical and cognitive-psychological research on human morality by advancing the moral tractability thesis.},
  archive      = {J_AIR},
  author       = {Stenseke, Jakob},
  doi          = {10.1007/s10462-024-10732-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-90},
  shortjournal = {Artif. Intell. Rev.},
  title        = {On the computational complexity of ethics: Moral tractability for minds and machines},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ideal plastic waste management system based on an
enhanced MCDM technique. <em>AIR</em>, <em>57</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s10462-024-10737-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fermatean probabilistic hesitant fuzzy set (FPHFS) concept is a method that combines Fermatean fuzzy sets and PHFS to provide detailed MCDM problem solutions. The management of solid waste is facing challenges in disposing of environmentally harmful plastic products. The study primarily focuses on plastic waste from the packaging industry, as it is the primary contributor to overall waste generation. The packaging sector has proposed a concept for the recycling of waste at various levels. This study aims to assess the effectiveness of the hybrid analytic hierarchy process and the combinative distance-based assessment (AHP-CODAS) method FPHFS in determining the most suitable plastic waste collection system. The AHP analysis identified the most effective methods for collecting plastic waste for recycling, emphasizing successful segregation for efficient recycling. A deposit and refund system promotes waste collection and a circular economy by facilitating efficient waste management. The research offers valuable insights into selecting packaging waste collection (PWC) technology in uncertain environments and enhances decision-making methods in the field.},
  archive      = {J_AIR},
  author       = {Suvitha, Krishnan and Narayanamoorthy, Samayan and Pamucar, Dragan and Kang, Daekook},
  doi          = {10.1007/s10462-024-10737-y},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An ideal plastic waste management system based on an enhanced MCDM technique},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological deep learning: A review of an emerging paradigm.
<em>AIR</em>, <em>57</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s10462-024-10710-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological deep learning (TDL) is an emerging area that combines the principles of Topological data analysis (TDA) with deep learning techniques. TDA provides insight into data shape; it obtains global descriptions of multi-dimensional data whilst exhibiting robustness to deformation and noise. Such properties are desirable in deep learning pipelines, but they are typically obtained using non-TDA strategies. This is partly caused by the difficulty of combining TDA constructs (e.g. barcode and persistence diagrams) with current deep learning algorithms. Fortunately, we are now witnessing a growth of deep learning applications embracing topologically-guided components. In this survey, we review the nascent field of topological deep learning by first revisiting the core concepts of TDA. We then explore how the use of TDA techniques has evolved over time to support deep learning frameworks, and how they can be integrated into different aspects of deep learning. Furthermore, we touch on TDA usage for analyzing existing deep models; deep topological analytics. Finally, we discuss the challenges and future prospects of topological deep learning.},
  archive      = {J_AIR},
  author       = {Zia, Ali and Khamis, Abdelwahed and Nichols, James and Tayab, Usman Bashir and Hayder, Zeeshan and Rolland, Vivien and Stone, Eric and Petersson, Lars},
  doi          = {10.1007/s10462-024-10710-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Topological deep learning: A review of an emerging paradigm},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence strategies for simulating the
integrated energy systems. <em>AIR</em>, <em>57</em>(4), 1–56. (<a
href="https://doi.org/10.1007/s10462-024-10704-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, the operational impact of Artificial Intelligence (AI) strategies is massively dominating the scientific arena of improving the operation of energy systems and their hybrid integrations. Comprehensively, this paper highlights the firm methodological link of AI strategies with the different defined categories of numerical methods in hypothetically simulating the complex integrated energy systems especially the integration of Renewable Energy Sources (RES). The conducted studies in this paper are related to the bifurcations of the applied numerical simulation methodologies for efficient energy systems and the practical implementations of the optimal operated energy systems considering the integration scenarios of these methodologies with AI strategies. Furthermore, this research reviews innovatively several case studies and practical examples to emphasize the effective contributions of AI strategies in enhancing the computational analysis of numerical simulation methods forming a smart approach for assessing experimental studies that are associated with energy systems. Finally, this paper deeply discusses the concept of integration either in the hybrid controlling strategies combining AI with numerical simulation methods or in combining different energy systems in one hybrid model for reliable operation considering the complexity level.},
  archive      = {J_AIR},
  author       = {Talaat, M. and Tayseer, M. and Farahat, M. A. and Song, Dongran},
  doi          = {10.1007/s10462-024-10704-7},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence strategies for simulating the integrated energy systems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of predictive uncertainty estimation with machine
learning. <em>AIR</em>, <em>57</em>(4), 1–65. (<a
href="https://doi.org/10.1007/s10462-023-10698-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale and shape, random forests, boosting and deep learning algorithms) that are more flexible by nature. The review of the progress in the field, expedites our understanding on how to develop new algorithms tailored to users’ needs, since the latest advancements are based on some fundamental concepts applied to more complex algorithms. We conclude by classifying the material and discussing challenges that are becoming a hot topic of research.},
  archive      = {J_AIR},
  author       = {Tyralis, Hristos and Papacharalampous, Georgia},
  doi          = {10.1007/s10462-023-10698-8},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of predictive uncertainty estimation with machine learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consumer-side fairness in recommender systems: A systematic
survey of methods and evaluation. <em>AIR</em>, <em>57</em>(4), 1–61.
(<a href="https://doi.org/10.1007/s10462-023-10663-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to data volume. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. Data-driven models are susceptible to data bias, materializing in the bias influencing the models’ decision-making. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches. The approaches are further diversified through differing ideas on what constitutes fair and, conversely, discriminatory recommendations. This survey serves as a systematic overview and discussion of the current research on consumer-side fairness in recommender systems. To that end, a novel taxonomy based on high-level fairness definitions is proposed and used to categorize the research and the proposed fairness evaluation metrics. Finally, we highlight some suggestions for the future direction of the field.},
  archive      = {J_AIR},
  author       = {Vassøy, Bjørnar and Langseth, Helge},
  doi          = {10.1007/s10462-023-10663-5},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Consumer-side fairness in recommender systems: A systematic survey of methods and evaluation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Black-winged kite algorithm: A nature-inspired
meta-heuristic for solving benchmark functions and engineering problems.
<em>AIR</em>, <em>57</em>(4), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-10723-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper innovatively proposes the Black Kite Algorithm (BKA), a meta-heuristic optimization algorithm inspired by the migratory and predatory behavior of the black kite. The BKA integrates the Cauchy mutation strategy and the Leader strategy to enhance the global search capability and the convergence speed of the algorithm. This novel combination achieves a good balance between exploring global solutions and utilizing local information. Against the standard test function sets of CEC-2022 and CEC-2017, as well as other complex functions, BKA attained the best performance in 66.7, 72.4 and 77.8% of the cases, respectively. The effectiveness of the algorithm is validated through detailed convergence analysis and statistical comparisons. Moreover, its application in solving five practical engineering design problems demonstrates its practical potential in addressing constrained challenges in the real world and indicates that it has significant competitive strength in comparison with existing optimization techniques. In summary, the BKA has proven its practical value and advantages in solving a variety of complex optimization problems due to its excellent performance. The source code of BKA is publicly available at https://www.mathworks.com/matlabcentral/fileexchange/161401-black-winged-kite-algorithm-bka .},
  archive      = {J_AIR},
  author       = {Wang, Jun and Wang, Wen-chuan and Hu, Xiao-xue and Qiu, Lin and Zang, Hong-fei},
  doi          = {10.1007/s10462-024-10723-4},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Black-winged kite algorithm: A nature-inspired meta-heuristic for solving benchmark functions and engineering problems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning based on balanced stratified
prioritized experience replay for customer credit scoring in
peer-to-peer lending. <em>AIR</em>, <em>57</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10462-023-10697-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep reinforcement learning (DRL) models have been successfully utilised to solve various classification problems. However, these models have never been applied to customer credit scoring in peer-to-peer (P2P) lending. Moreover, the imbalanced class distribution in experience replay, which may affect the performance of DRL models, has rarely been considered. Therefore, this article proposes a novel DRL model, namely a deep Q-network based on a balanced stratified prioritized experience replay (DQN-BSPER) model, for customer credit scoring in P2P lending. Firstly, customer credit scoring is formulated as a discrete-time finite-Markov decision process. Subsequently, a balanced stratified prioritized experience replay technology is presented to optimize the loss function of the deep Q-network model. This technology can not only balance the numbers of minority and majority experience samples in the mini-batch by using stratified sampling technology but also select more important experience samples for replay based on the priority principle. To verify the model performance, four evaluation measures are introduced for the empirical analysis of two real-world customer credit scoring datasets in P2P lending. The experimental results show that the DQN-BSPER model can outperform four benchmark DRL models and seven traditional benchmark classification models. In addition, the DQN-BSPER model with a discount factor γ of 0.1 has excellent credit scoring performance.},
  archive      = {J_AIR},
  author       = {Wang, Yadong and Jia, Yanlin and Fan, Sha and Xiao, Jin},
  doi          = {10.1007/s10462-023-10697-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep reinforcement learning based on balanced stratified prioritized experience replay for customer credit scoring in peer-to-peer lending},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math display"><em>χ</em></span> -linguistic
sets and its application for the linguistic multi-attribute group
decision making. <em>AIR</em>, <em>57</em>(4), 1–40. (<a
href="https://doi.org/10.1007/s10462-023-10695-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information in the real world often contains many properties such as fuzziness, randomness, and approximation. Although existing linguistic collections attempt to solve these problems, with the emergence of more and more constraints and challenges, this information cannot fully express the problem, leading to an increasing demand for methods that can contain multiple uncertain information. In this paper, we comprehensively consider the various characteristics of information including membership degree, credibility and approximation based on rough sets, and propose the concept of $$\chi$$ -linguistic sets ( $$\chi$$ LSs), which depend on original data rather than prior knowledge and effectively solve the problem of incomplete information representation. At the same time, the corresponding theories such as the comparison method and operational rules have also been proposed. Subsequently, we construct a new $$\chi$$ -linguistic VIKOR ( $$\chi$$ LVIKOR) method for multi-attribute group decision making (MAGDM) problem with $$\chi$$ LSs, and apply it to the risk assessment of COVID-19. Through comparative analysis, we discuss the effectiveness and superiority of $$\chi$$ LSs.},
  archive      = {J_AIR},
  author       = {Xian, Sidong and Liu, Mengnan and Xian, Zhiyu and Chai, Jiahui and Lu, Sicong and Qing, Ke},
  doi          = {10.1007/s10462-023-10695-x},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {$$\chi$$ -linguistic sets and its application for the linguistic multi-attribute group decision making},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-way decision method on multi-scale single-valued
neutrosophic decision systems. <em>AIR</em>, <em>57</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10462-024-10733-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a three-way decision (TWD) method on multi-scale single-valued neutrosophic decision systems (MS-SVNDSs). First, to explore the application of single-valued neutrosophic sets (SVNSs) in multi-scale environment, we establish a rough set model of MS-SVNDSs. Then, aiming at the problem of knowledge acquisition in MS-SVNDSs, we present the corresponding optimal scale selection and reduction methods by using evidence theory, a more direct and simpler algorithm is also discussed. For obtaining decision results that are more in line with human cognition, we further provide a novel three-way decision method. Comparative experiments are subsequently conducted to demonstrate the effectiveness of our approach. The experimental results show that our method not only improves the classification accuracy but also raises decision efficiency.},
  archive      = {J_AIR},
  author       = {Yang, Xuan and Zhou, Xianzhong and Huang, Bing and Li, Huaxiong and Wang, Tianxing},
  doi          = {10.1007/s10462-024-10733-2},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A three-way decision method on multi-scale single-valued neutrosophic decision systems},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning applications in environmental
sustainability: A review. <em>AIR</em>, <em>57</em>(4), 1–68. (<a
href="https://doi.org/10.1007/s10462-024-10706-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental sustainability is a worldwide key challenge attracting increasing attention due to climate change, pollution, and biodiversity decline. Reinforcement learning, initially employed in gaming contexts, has been recently applied to real-world domains, including the environmental sustainability realm, where uncertainty challenges strategy learning and adaptation. In this work, we survey the literature to identify the main applications of reinforcement learning in environmental sustainability and the predominant methods employed to address these challenges. We analyzed 181 papers and answered seven research questions, e.g., “How many academic studies have been published from 2003 to 2023 about RL for environmental sustainability?” and “What were the application domains and the methodologies used?”. Our analysis reveals an exponential growth in this field over the past two decades, with a rate of 0.42 in the number of publications (from 2 papers in 2007 to 53 in 2022), a strong interest in sustainability issues related to energy fields, and a preference for single-agent RL approaches to deal with sustainability. Finally, this work provides practitioners with a clear overview of the main challenges and open problems that should be tackled in future research.},
  archive      = {J_AIR},
  author       = {Zuccotto, Maddalena and Castellini, Alberto and Torre, Davide La and Mola, Lapo and Farinelli, Alessandro},
  doi          = {10.1007/s10462-024-10706-5},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-68},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reinforcement learning applications in environmental sustainability: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated multi-polar fuzzy n-soft preference ranking
organization method for enrichment of evaluations of the digitization of
global economy. <em>AIR</em>, <em>57</em>(3), 1–44. (<a
href="https://doi.org/10.1007/s10462-023-10693-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digitization represents the ultimate expression of globalization that has revolutionized every facet of global existence, enhancing connectivity, financial terms, trade opportunities, and public services. To further broaden or fortify this digital realm and advance global progress, various strategies for multi-criteria group decision-making (MCGDM) have been developed. This research article extends the literature study on the sensational MCGDM method known as Preference Ranking Organization Method for Enrichment of Evaluations (PROMETHEE), which shares the burden of selecting the best alternative under many terms and conditions. This article aims to make a positive contribution to the development of the digital world. The main principle of the PROMETHEE technique is to calculate positive and negative flows of the selected options according to the divergence of the alternatives’ scores. It provides us with partial and full rankings of options by determining score degrees, suitable preference functions, and a multi-criteria preference index. To deal with the multi-polarity of the modern world and assign N-ordered levels, this paper proposes the multi-polar fuzzy N-soft (mFNS) PROMETHEE technique. Prior to this, an Analytic Hierarchy Process (AHP) technique is applied to guarantee the credibility of the criterion weights for each criterion. After that, the whole strategy of m-polar fuzzy N-soft PROMETHEE approach to order the selected options is explained, and all the course of action of this remarkable (MCGDM) technique are synchronized in an extensive flowchart, helping us to learn more about the technology keenly. Moreover, the utility of our discussed technology is illustrated by considering two applications of generating electricity through human movement and choosing the best digital currency. The most suitable choice is extracted with the help of the outranked directed graph. The results obtained by mFNS PROMETHEE technique benefits us with the selection of most reliable source to digitized the global economy, in addition, It enables us to rank the alternatives from most preferable to least one, that not only saves time but also produces the better outcomes. Subsequently, the eminence of mFNS PROMETHEE technology is checked by comparability with the prior art. Lastly, the advantages and disadvantages of our proposed technique are supplemented to demonstrate its productivity and shortcomings.},
  archive      = {J_AIR},
  author       = {Akram, Muhammad and Sultan, Maheen and Deveci, Muhammet},
  doi          = {10.1007/s10462-023-10693-z},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An integrated multi-polar fuzzy N-soft preference ranking organization method for enrichment of evaluations of the digitization of global economy},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elk herd optimizer: A novel nature-inspired metaheuristic
algorithm. <em>AIR</em>, <em>57</em>(3), 1–60. (<a
href="https://doi.org/10.1007/s10462-023-10680-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel nature-inspired swarm-based optimization algorithm called elk herd optimizer (EHO). It is inspired by the breeding process of the elk herd. Elks have two main breeding seasons: rutting and calving. In the rutting season, the elk herd splits into different families of various sizes. This division is based on fighting for dominance between bulls, where the stronger bull can form a family with large numbers of harems. In the calving season, each family breeds new calves from its bull and harems. This inspiration is set in an optimization context where the optimization loop consists of three operators: rutting season, calving season, and selection season. During the selection season, all families are merged, including bulls, harems, and calves. The fittest elk herd will be selected for use in the upcoming rutting and calving seasons. In simple words, EHO divides the population into a set of groups, each with one leader and several followers in the rutting season. The number of followers is determined based on the fitness value of its leader group. Each group will generate new solutions based on its leader and followers in the calving season. The members of all groups including leaders, followers, and new solutions are combined and the fittest population is selected in the selection season. The performance of EHO is assessed using 29 benchmark optimization problems utilized in the CEC-2017 special sessions on real-parameter optimization and four traditional real-world engineering design problems. The comparative results were conducted against ten well-established metaheuristic algorithms and showed that the proposed EHO yielded the best results for almost all the benchmark functions used. Statistical testing using Friedman’s test post-hocked by Holm’s test function confirms the superiority of the proposed EHO when compared to other methods. In a nutshell, EHO is an efficient nature-inspired swarm-based optimization algorithm that can be used to tackle several optimization problems.},
  archive      = {J_AIR},
  author       = {Al-Betar, Mohammed Azmi and Awadallah, Mohammed A. and Braik, Malik Shehadeh and Makhadmeh, Sharif and Doush, Iyad Abu},
  doi          = {10.1007/s10462-023-10680-4},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Elk herd optimizer: A novel nature-inspired metaheuristic algorithm},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligent global online learning (GOL) theory
by generalized n-ary fuzzy relation. <em>AIR</em>, <em>57</em>(3), 1–31.
(<a href="https://doi.org/10.1007/s10462-023-10691-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the devastating COVID pandemic, a new global strategy is required to switch from traditional education to blended or online learning method. Nevertheless, there is no adamant theoretical base available for such an important transition or similar situations in the future. On the other hand, educational systems encounter uncertainty as an integral part of multilayered teaching routes. To analyze the interactions among interconnected entities, soft computing methodologies can serve as an efficient tool to manage such systems with uncertain information through incorporating artificial intelligence (AI) techniques for assessing students performances. Nevertheless, the classical binary fuzzy relation and other existing theoretical models are not capable of explaining/configuring uncertain-based datum for multiplex correlations. To fill these gaps, the present study establishes a neoteric AI-base “Global Online Learning (GOL) theory” using the newly developed n-ary relation and n-ary fuzzy relation as the generalization of classical and binary fuzzy relations. Through the enhanced mathematical concepts and intelligent soft computing techniques, the convoluted multilayer relationships of entities can be punctiliously assessed for different values of n. Furthermore, a network-based perspective is proposed as a promising systematic model when systems are imperfect and prone to uncertainty. In the provided graphical context, the n-ary relation represents the hypergraph pattern, while the n-ary fuzzy relation refers to the generalized fuzzy hypergraph model. Fundamental characteristics of n-ary fuzzy relation, including reflexive, symmetric, transitive, composition, t-cut, support and Cartesian product, are systematically provided to extract mathematical interrelated expressions, as well as parametric connection between t-cut and Cartesian product. Based on the n-ary fuzzy relation, the n-ary fuzzy hyperoperation “ $$\circ _{\rho }$$ ” is assigned to construct fuzzy hyperalgebra as the extension of classical algebra with illustrative examples. The relationships between fuzzy hyperalgebra and hyperalgebra are investigated through the notation of $$(\circ _{\rho })_{t}$$ for $$t\in (0,1].$$ With the introduced t-cut methodology, the corresponding hypergraph is derived to simplify the analysis of educational information. The AI-base GOL theory provides a solid gadget for learning data management, e.g., the grading evaluation of online assessments, where the evaluation of components is accomplished on real data in terms of fuzzy n-ary relation, t-cut and support through a graphical attitude. The results indicate that the AI-base GOL theory is a robust platform to meticulously manage and control uncertain-based intercorrelated information. This platform can be converted into a coding gadget for artificial intelligent educational online mega-systems.},
  archive      = {J_AIR},
  author       = {Amini, Abbas and Firouzkouhi, Narjes and Nazari, Marziyeh and Ghareeb, Nader and Cheng, Chun and Davvaz, Bijan},
  doi          = {10.1007/s10462-023-10691-1},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligent global online learning (GOL) theory by generalized n-ary fuzzy relation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating the curse of dimensionality using feature
projection techniques on electroencephalography datasets: An empirical
review. <em>AIR</em>, <em>57</em>(3), 1–28. (<a
href="https://doi.org/10.1007/s10462-024-10711-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is commonly employed to diagnose and monitor brain disorders, however, manual analysis is time-consuming. Hence, researchers nowadays are increasingly leveraging artificial intelligence (AI) techniques for automatic analysis of EEG, involving task-specific feature extraction and classification. Extracting multiple non-linear features from multiple EEG channels enhances the performance of classification, but it also results in high-dimensional features. However, the presence of the &quot;curse of dimensionality&quot; poses challenges for classifiers in AI-based EEG analyzers, leading to overfitting and complexity in classification. Therefore, in this empirical review, the effectiveness of the existing dimensionality reduction techniques to mitigate the curse of dimensionality in EEG feature sets is analyzed. We begin this study, by overviewing the extracted high-dimensional features from EEG signals related to disorders such as schizophrenia, alcoholism, focal seizures, focal seizures with deep features, and depression. Subsequently, 23 reduction techniques were reviewed, which transform the high-dimensional EEG features into a new reduced feature space to improve the classification. The impact of the reduced features was evaluated using traditional AI-based classifiers (support vector machine (SVM) and k-nearest neighbor (k-NN)). 10-fold cross-validation was performed for training and testing, and the performance was evaluated using accuracy, selected features, and dimensionality reduction rate (DRR) metrics. Comprehensive analysis of projection techniques on diverse EEG datasets offers valuable insights to assist researchers in selecting the most suitable technique. The hybrid projection technique termed principal component analysis-based t-distributed stochastic neighbor embedding (PCA+t-SNE) achieved an impressive average accuracy of 93.36%, surpassing the k-NN classifier without reduction techniques.},
  archive      = {J_AIR},
  author       = {Anuragi, Arti and Sisodia, Dilip Singh and Pachori, Ram Bilas},
  doi          = {10.1007/s10462-024-10711-8},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Mitigating the curse of dimensionality using feature projection techniques on electroencephalography datasets: An empirical review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of smart dustbin systems using the IoT and deep
learning. <em>AIR</em>, <em>57</em>(3), 1–48. (<a
href="https://doi.org/10.1007/s10462-023-10646-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With massive population growth and a shift in the urban culture in smart cities, the constant generation of waste continues to create unsanitary living conditions for city dwellers. Overflowing solid waste in the garbage and the rapid generation of non-degradable solid waste produce a slew of infectious illnesses that proliferate throughout the ecosystem. Conventional solid waste management systems have proved to be increasingly harmful in densely populated areas like smart cities. Also, such systems require real-time manual monitoring of garbage, high labor costs, and constant maintenance. Monitoring waste management on a timely basis and reducing labor costs is scarcely possible, realistically, for a municipal corporation. A Smart Dustbin System (SDS) is proposed that is to be implemented in densely populated urban areas to ensure hygiene. This paper undertakes a comprehensive analysis of the application of smart dustbin systems, following an extensive literature review and a discussion of recent research that is expected to help improve waste management systems. A current SDS used in real-time is implemented with the most recent advances from deep learning, computer vision, and the Internet of Things. The smart dustbin system used in day-to-day life minimizes the overloading of bins, lowers labor costs, and saves energy and time. It also helps keep cities clean, lowering the risk of disease transmission. The primary users of the SDS are universities, malls, and high-rise buildings. The evolution of the SDS over the years with various features and technologies is well analyzed. The datasets used for Smart Waste Management and benchmark garbage image datasets are presented under AI perception. The results of the existing works are compared to highlight the potential limitations of these works.},
  archive      = {J_AIR},
  author       = {Arthur, Menaka Pushpa and Shoba, S. and Pandey, Aru},
  doi          = {10.1007/s10462-023-10646-6},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of smart dustbin systems using the IoT and deep learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The application of AI techniques in requirements
classification: A systematic mapping. <em>AIR</em>, <em>57</em>(3),
1–48. (<a href="https://doi.org/10.1007/s10462-023-10667-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Requirement Analysis is the essential sub-field of requirements engineering (RE). From the last decade, numerous automatic techniques are widely exploited in requirements analysis. In this context, requirements identification and classification is challenging for RE community, especially in context of large corpus and app review. As a consequence, several Artificial Intelligence (AI) techniques such as Machine learning (ML), Deep learning (DL) and transfer learning (TL)) have been proposed to reduce the manual efforts of requirement engineer. Although, these approaches reported promising results than traditional automated techniques, but the knowledge of their applicability in real-life and actual use of these approaches is yet incomplete. The main objective of this paper is to systematically investigate and better understand the role of Artificial Intelligence (AI) techniques in identification and classification of software requirements. This study conducted a systematic literature review (SLR) and collect the primary studies on the use of AI techniques in requirements classification. (1) this study found that 60 studies are published that adopted automated techniques in requirements classification. The reported results indicate that transfer learning based approaches extensively used in classification and yielding most accurate results and outperforms the other ML and DL techniques. (2) The data extraction process of SLR indicates that Support Vector Machine (SVM) and Convolutional Neural Network (CNN) are widely used in selected studies. (3) Precision and Recall are the commonly used metrics for evaluating the performance of automated techniques. This paper revealed that while these AI approaches reported promising results in classification. The applicability of these existing techniques in complex and real-world settings has not been reported yet. This SLR calls for the urge for the close alliance between RE and AI techniques to handle the open issues confronted in the development of some real-world automated system.},
  archive      = {J_AIR},
  author       = {Kaur, Kamaljit and Kaur, Parminder},
  doi          = {10.1007/s10462-023-10667-1},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The application of AI techniques in requirements classification: A systematic mapping},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on graph neural networks for predicting synergistic
drug combinations. <em>AIR</em>, <em>57</em>(3), 1–38. (<a
href="https://doi.org/10.1007/s10462-023-10669-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinational therapies with synergistic effects provide a powerful treatment strategy for tackling complex diseases, particularly malignancies. Discovering these synergistic combinations, often involving various compounds and structures, necessitates exploring a vast array of compound pairings. However, practical constraints such as cost, feasibility, and complexity hinder exhaustive in vivo and in vitro experimentation. In recent years, machine learning methods have made significant inroads in pharmacology. Among these, Graph Neural Networks (GNNs) have gained increasing attention in drug discovery due to their ability to represent complex molecular structures as networks, capture vital structural information, and seamlessly handle diverse data types. This review aims to provide a comprehensive overview of various GNN models developed for predicting effective drug combinations, examining the limitations and strengths of different models, and comparing their predictive performance. Additionally, we discuss the datasets used for drug synergism prediction and the extraction of drug-related information as predictive features. By summarizing the state-of-the-art GNN-driven drug combination prediction, this review aims to offer valuable insights into the promising field of computational pharmacotherapy.},
  archive      = {J_AIR},
  author       = {Besharatifard, Milad and Vafaee, Fatemeh},
  doi          = {10.1007/s10462-023-10669-z},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on graph neural networks for predicting synergistic drug combinations},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What can machine vision do for lymphatic histopathology
image analysis: A comprehensive review. <em>AIR</em>, <em>57</em>(3),
1–59. (<a href="https://doi.org/10.1007/s10462-024-10701-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past 10 years, machine vision (MV) algorithms for image analysis have been developing rapidly with computing power. At the same time, histopathological slices can be stored as digital images. Therefore, MV algorithms can provide diagnostic references to doctors. In particular, the continuous improvement of deep learning algorithms has further improved the accuracy of MV in disease detection and diagnosis. This paper reviews the application of image processing techniques based on MV in lymphoma histopathological images in recent years, including segmentation, classification and detection. Finally, the current methods are analyzed, some potential methods are proposed, and further prospects are made.},
  archive      = {J_AIR},
  author       = {Chen, Haoyuan and Li, Xiaoqi and Li, Chen and Rahaman, Md. Mamunur and Li, Xintong and Wu, Jian and Sun, Hongzan and Grzegorzek, Marcin and Li, Xiaoyan},
  doi          = {10.1007/s10462-024-10701-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {What can machine vision do for lymphatic histopathology image analysis: A comprehensive review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for thyroid nodule examination: A technical
review. <em>AIR</em>, <em>57</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s10462-023-10635-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the incidence of thyroid cancer has increased dramatically, resulting in an increased demand for early thyroid nodule examination. Ultrasound (US) imaging is the modality most frequently used to image thyroid nodules; However, the low image resolution, speckle noise, and high variability make it difficult to utilize traditional image processing techniques. Recent advances in deep learning (DL) have increased research into the automated processing of thyroid US images. We review three main image processing tasks for thyroid nodule analysis: classification, segmentation, and detection. We discuss the advantages and limitations of the recently proposed DL techniques as well as the data availability and algorithmic efficacy. In addition, we investigate the remaining obstacles and future potential for automated analysis of thyroid US images.},
  archive      = {J_AIR},
  author       = {Das, Debottama and Iyengar, M. Sriram and Majdi, Mohammad S. and Rodriguez, Jeffrey J. and Alsayed, Mahmoud},
  doi          = {10.1007/s10462-023-10635-9},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for thyroid nodule examination: A technical review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel generalized similarity measure under intuitionistic
fuzzy environment and its applications to criminal investigation.
<em>AIR</em>, <em>57</em>(3), 1–55. (<a
href="https://doi.org/10.1007/s10462-023-10682-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our contemporary world, where crime prevails, the expeditious conduct of criminal investigations stands as an essential pillar of law and order. However, these inquiries often grapple with intricate complexities, particularly uncertainties stemming from the scarcity of reliable evidence, which can significantly hinder progress. To surmount these challenges, the invaluable tools of crime linkage and psychological profiling of offenders have come to the forefront. The advent of Intuitionistic Fuzzy Sets (IFS) has proven pivotal in navigating these uncertain terrains of decision-making, and at the heart of this lies the concept of similarity measure-an indispensable tool for unraveling intricate problems of choice. While a multitude of similarity measures exists for gauging the likeness between IFSs, our study introduces a novel generalized similarity measure firmly rooted in the IFS framework, poised to surpass existing methods with enhanced accuracy and applicability. We then extend the horizon of practicality by employing this pioneering similarity measure in the domain of clustering for crime prediction-a paramount application within the realm of law enforcement. Furthermore, we venture into the domain of psychological profiling, a potent avenue that has the potential to significantly fortify the arsenal of crime investigations. Through the application of our proposed similarity measure, we usher in a new era of efficacy and insight in the pursuit of justice. In sum, this study not only unveils a groundbreaking similarity measure within the context of an Intuitionistic fuzzy environment but also showcases its compelling applications in the arena of criminal investigation, marking a significant stride toward swifter and more informed decisions in the realm of law and order.},
  archive      = {J_AIR},
  author       = {Dutta, Palash and Banik, Abhilash Kangsha},
  doi          = {10.1007/s10462-023-10682-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel generalized similarity measure under intuitionistic fuzzy environment and its applications to criminal investigation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deepfakes: Current and future trends. <em>AIR</em>,
<em>57</em>(3), 1–32. (<a
href="https://doi.org/10.1007/s10462-023-10679-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in Deep Learning (DL), Big Data and image processing have facilitated online disinformation spreading through Deepfakes. This entails severe threats including public opinion manipulation, geopolitical tensions, chaos in financial markets, scams, defamation and identity theft among others. Therefore, it is imperative to develop techniques to prevent, detect, and stop the spreading of deepfake content. Along these lines, the goal of this paper is to present a big picture perspective of the deepfake paradigm, by reviewing current and future trends. First, a compact summary of DL techniques used for deepfakes is presented. Then, a review of the fight between generation and detection techniques is elaborated. Moreover, we delve into the potential that new technologies, such as distributed ledgers and blockchain, can offer with regard to cybersecurity and the fight against digital deception. Two scenarios of application, including online social networks engineering attacks and Internet of Things, are reviewed where main insights and open challenges are tackled. Finally, future trends and research lines are discussed, pointing out potential key agents and technologies.},
  archive      = {J_AIR},
  author       = {Gambín, Ángel Fernández and Yazidi, Anis and Vasilakos, Athanasios and Haugerud, Hårek and Djenouri, Youcef},
  doi          = {10.1007/s10462-023-10679-x},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deepfakes: Current and future trends},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning methods for service placement: A systematic
review. <em>AIR</em>, <em>57</em>(3), 1–64. (<a
href="https://doi.org/10.1007/s10462-023-10684-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of real-time and latency-sensitive applications in the Internet of Everything (IoE), service placement cannot rely on cloud computing alone. In response to this need, several computing paradigms, such as Mobile Edge Computing (MEC), Ultra-dense Edge Computing (UDEC), and Fog Computing (FC), have emerged. These paradigms aim to bring computing resources closer to the end user, reducing delay and wasted backhaul bandwidth. One of the major challenges of these new paradigms is the limitation of edge resources and the dependencies between different service parts. Some solutions, such as microservice architecture, allow different parts of an application to be processed simultaneously. However, due to the ever-increasing number of devices and incoming tasks, the problem of service placement cannot be solved today by relying on rule-based deterministic solutions. In such a dynamic and complex environment, many factors can influence the solution. Optimization and Machine Learning (ML) are two well-known tools that have been used most for service placement. Both methods typically use a cost function. Optimization is usually a way to define the difference between the predicted and actual value, while ML aims to minimize the cost function. In simpler terms, ML aims to minimize the gap between prediction and reality based on historical data. Instead of relying on explicit rules, ML uses prediction based on historical data. Due to the NP-hard nature of the service placement problem, classical optimization methods are not sufficient. Instead, metaheuristic and heuristic methods are widely used. In addition, the ever-changing big data in IoE environments requires the use of specific ML methods. In this systematic review, we present a taxonomy of ML methods for the service placement problem. Our findings show that 96% of applications use a distributed microservice architecture. Also, 51% of the studies are based on on-demand resource estimation methods and 81% are multi-objective. This article also outlines open questions and future research trends. Our literature review shows that one of the most important trends in ML is reinforcement learning, with a 56% share of research.},
  archive      = {J_AIR},
  author       = {Keshavarz Haddadha, Parviz and Rezvani, Mohammad Hossein and MollaMotalebi, Mahdi and Shankar, Achyut},
  doi          = {10.1007/s10462-023-10684-0},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning methods for service placement: A systematic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “Challenges and future in deep learning for sentiment
analysis: A comprehensive review and a proposed novel hybrid approach”.
<em>AIR</em>, <em>57</em>(3), 1–79. (<a
href="https://doi.org/10.1007/s10462-023-10651-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media is used to categorise products or services, but analysing vast comments is time-consuming. Researchers use sentiment analysis via natural language processing, evaluating methods and results conventionally through literature reviews and assessments. However, our approach diverges by offering a thorough analytical perspective with critical analysis, research findings, identified gaps, limitations, challenges and future prospects specific to deep learning-based sentiment analysis in recent times. Furthermore, we provide in-depth investigation into sentiment analysis, categorizing prevalent data, pre-processing methods, text representations, learning models, and applications. We conduct a thorough evaluation of recent advances in deep learning architectures, assessing their pros and cons. Additionally, we offer a meticulous analysis of deep learning methodologies, integrating insights on applied tools, strengths, weaknesses, performance results, research gaps, and a detailed feature-based examination. Furthermore, we present in a thorough discussion of the challenges, drawbacks, and factors contributing to the successful enhancement of accuracy within the realm of sentiment analysis. A critical comparative analysis of our article clearly shows that capsule-based RNN approaches give the best results with an accuracy of 98.02% which is the CNN or RNN-based models. We implemented various advanced deep-learning models across four benchmarks to identify the top performers. Additionally, we introduced the innovative CRDC (Capsule with Deep CNN and Bi structured RNN) model, which demonstrated superior performance compared to other methods. Our proposed approach achieved remarkable accuracy across different databases: IMDB (88.15%), Toxic (98.28%), CrowdFlower (92.34%), and ER (95.48%). Hence, this method holds promise for automated sentiment analysis and potential deployment.},
  archive      = {J_AIR},
  author       = {Islam, Md. Shofiqul and Kabir, Muhammad Nomani and Ghani, Ngahzaifa Ab and Zamli, Kamal Zuhairi and Zulkifli, Nor Saradatul Akmar and Rahman, Md. Mustafizur and Moni, Mohammad Ali},
  doi          = {10.1007/s10462-023-10651-9},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-79},
  shortjournal = {Artif. Intell. Rev.},
  title        = {&quot;Challenges and future in deep learning for sentiment analysis: A comprehensive review and a proposed novel hybrid approach&quot;},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for cancer cell detection: Do we need
dedicated models? <em>AIR</em>, <em>57</em>(3), 1–36. (<a
href="https://doi.org/10.1007/s10462-023-10699-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel concept for a two-step Ki-67/lymphocytes classification cell detection pipeline on Ki-67 stained histopathological slides utilizing commonly available and undedicated, in terms of the medical problem considered, deep learning models. Models used vary in implementation, complexity, and applications, allowing for the use of a dedicated architecture depending on the physician’s needs. Moreover, generic models’ performance was compared with the problem-dedicated one. Experiments highlight that with relatively small training datasets, commonly used architectures for instance segmentation and object detection are competitive with a dedicated model. To ensure generalization and minimize biased sampling, experiments were performed on data derived from two unrelated histopathology laboratories.},
  archive      = {J_AIR},
  author       = {Karol, Michal and Tabakov, Martin and Markowska-Kaczmar, Urszula and Fulawka, Lukasz},
  doi          = {10.1007/s10462-023-10699-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for cancer cell detection: Do we need dedicated models?},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient scheduling model and method for assembly
blocking permutation flow-shop in industrial robotics field.
<em>AIR</em>, <em>57</em>(3), 1–36. (<a
href="https://doi.org/10.1007/s10462-023-10649-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing green and sustainable development strategies has become essential for industrial robot manufacturing companies to fulfill their societal obligations. By enhancing assembly efficiency and minimizing energy consumption in workshops, these enterprises can differentiate themselves in the fiercely competitive market landscape and ultimately bolster their financial gains. Consequently, this study focuses on examining the collaborative assembly challenges associated with three crucial parts: the body, electrical cabinet, and pipeline pack, within the industrial robot manufacturing process. Considering the energy consumption during both active and idle periods of the industrial robot workshop assembly system, this paper presents a multi-stage energy-efficient scheduling model to minimize the total energy consumption. Two classes of heuristic algorithms are proposed to address this model. Our contribution is the restructuring of the existing complex mathematical programming model, based on the structural properties of scheduling sub-problems across multiple stages. This reformation not only effectively reduces the variable scale and eliminates redundant constraints, but also enables the Gurobi solver to tackle large-scale problems. Extensive experimental results indicate that compared to traditional workshop experience, the constructed green scheduling model and algorithm can provide more precise guidance for the assembly process in the workshop. Regarding total energy consumption, the assembly plans obtained through our designed model and algorithm exhibit approximately 3% lower energy consumption than conventional workshop experience-based approaches.},
  archive      = {J_AIR},
  author       = {Kong, Min and Wu, Peng and Zhang, Yajing and Wang, Weizhong and Deveci, Muhammet and Kadry, Seifedine},
  doi          = {10.1007/s10462-023-10649-3},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Energy-efficient scheduling model and method for assembly blocking permutation flow-shop in industrial robotics field},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorization and correlational analysis of quality factors
influencing citation. <em>AIR</em>, <em>57</em>(3), 1–52. (<a
href="https://doi.org/10.1007/s10462-023-10657-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of the scientific publication plays an important role in generating a large number of citations and raising the work&#39;s visibility. According to several studies, the number of citations has been actively used to measure the quality of the publications. Existing studies have identified the document-related factors, author-related factors, journal-related factors, and altmetrics as the factors that influence the citations of an article. However, the majority of the stated indicators for determining the quality of a publication involve factors from the publication that are related to the author or venue of an article but these are not related to the content of the article. The factors related to the quality of publication are ignored by existing literature. The purpose of this research is to identify, categorize, and correlate the quality criteria that influence citations. As a result, a systematic literature review (SLR) is undertaken for factor categorization, and Pearson’s correlation coefficient (PCC) is calculated to quantify the impact of factors on citations. The SLR collects relevant articles from several data sources from 2013 to 2022 and categorizes factors impacting citations. A subset of factors is identified from DBLPV13 dataset and correlation of these factors with citations is studied to observe the impact of these factors on citations. The factors include Readability, Recency, Open Access, Hot topics, Abstract Length, Paper Title Length, and Page Count. Pearson’s correlation is performed to test the impact of aforementioned factors on citations. It can be observed from correlational analysis that Recency, Open Access, Hot topics, Abstract Length, page count have a favorable impact on citations, whereas Readability, Paper title length has a negative relationship with citations. The relationship among the factors is nonlinear therefore Spearman’s Correlation is computed for comparison with existing studies and has been undertaken to validate the empirical and correlational analytic results. The study has contributed by identifying, categorizing, and correlating the quality factors that need to be prioritized. Apart from the broad and more obvious features, it is determined that there is a need to investigate quality-related factors of the article that are related to the contents of the article.},
  archive      = {J_AIR},
  author       = {Khatoon, Asma and Daud, Ali and Amjad, Tehmina},
  doi          = {10.1007/s10462-023-10657-3},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Categorization and correlational analysis of quality factors influencing citation},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised attribute reduction for hybrid data.
<em>AIR</em>, <em>57</em>(3), 1–52. (<a
href="https://doi.org/10.1007/s10462-023-10642-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of labelling data, a lot of partially hybrid data are existed in many practical applications. Uncertainty measure (UM) can supply new viewpoints for analyzing data. They can help us in disclosing the substantive characteristics of data. Although there are some UMs to evaluate the uncertainty of hybrid data, they cannot be trivially transplanted into partially hybrid data. The existing studies often replace missing labels with pseudo-labels, but pseudo-labels are not real labels. When encountering high label error rates, work will be difficult to sustain. In view of the above situation, this paper studies four UMs for partially hybrid data and proposed semi-supervised attribute reduction algorithms. A decision information system with partially labeled hybrid data (p-HIS) is first divided into two decision information systems: one is the decision information system with labeled hybrid data (l-HIS) and the other is the decision information system with unlabeled hybrid data (u-HIS). Then, four degrees of importance on a attribute subset in a p-HIS are defined based on indistinguishable relation, distinguishable relation, dependence function, information entropy and information amount. We discuss the difference and contact among these UMs. They are the weighted sum of l-HIS and u-HIS determined by the missing rate and can be considered as UMs of a p-HIS. Next, numerical experiments and statistical tests on 12 datasets verify the effectiveness of these UMs. Moreover, an adaptive semi-supervised attribute reduction algorithm of a p-HIS is proposed based on the selected important degrees, which can automatically adapt to various missing rates. Finally, the results of experiments and statistical tests on 12 datasets show the proposed algorithm is statistically better than some stat-of-the-art algorithms according to classification accuracy.},
  archive      = {J_AIR},
  author       = {Li, Zhaowen and He, Jiali and Wang, Pei and Wen, Ching-Feng},
  doi          = {10.1007/s10462-023-10642-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Semi-supervised attribute reduction for hybrid data},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disinformation detection using graph neural networks: A
survey. <em>AIR</em>, <em>57</em>(3), 1–45. (<a
href="https://doi.org/10.1007/s10462-024-10702-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation and propagation of disinformation on social media is a growing concern. The widespread dissemination of disinformation can have destructive effects on people’s attitudes and behavior. So, it is essential to detect disinformation as soon as possible. Therefore, the interest in effective detection techniques has grown rapidly in recent years. Major social media and social networking sites are trying to develop robust strategies to detect disinformation and prevent its spread. Machine learning techniques and especially neural networks, have an essential role in this task. In this paper, we review different approaches for automatic disinformation detection, with a focus on methods that leverage graph neural networks (GNNs). GNNs are very suitable tools for detecting disinformation in social networks. Because on the one hand, graphs are the most comprehensive way to model social networks and on the other hand, GNNs are the best tool for processing graph data. We define different forms of disinformation, and examine the features used and the methods presented from different perspectives. We also discuss relevant research areas, open problems, and future research directions for disinformation detection in social media.},
  archive      = {J_AIR},
  author       = {Lakzaei, Batool and Haghir Chehreghani, Mostafa and Bagheri, Alireza},
  doi          = {10.1007/s10462-024-10702-9},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Disinformation detection using graph neural networks: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fashion intelligence in the metaverse: Promise and future
prospects. <em>AIR</em>, <em>57</em>(3), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-10703-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of artificial intelligence (AI) and the constraints on offline activities imposed due to the sudden outbreak of the COVID epidemic, the Metaverse has recently attracted significant research attention from both academia and industrial practitioners. Fashion, as an expression of a consumer’s aesthetics and personality, has enormous economic potential in both the real world and the Metaverse. In this research, we provide a comprehensive survey of two of the most important components of fashion in the Metaverse: virtual digital humans, and tasks related to fashion items. We survey state-of-the-art articles from 2007 to the present and provide a new taxonomy of extant research topics based on these articles. We also highlight the applications of these topics in the Metaverse from the perspectives of designers and consumers. Finally, we describe possible scenes involving fashion in the Metaverse. The current challenges and open issues related to the fashion industry in the Metaverse are also discussed in order to provide guidance for fashion practitioners, and to shed some light on the future development of fashion AI in the Metaverse.},
  archive      = {J_AIR},
  author       = {Mu, Xiangyu and Zhang, Haijun and Shi, Jianyang and Hou, Jie and Ma, Jianghong and Yang, Yimin},
  doi          = {10.1007/s10462-024-10703-8},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fashion intelligence in the metaverse: Promise and future prospects},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlabeled learning algorithms and operations: Overview and
future trends in defense sector. <em>AIR</em>, <em>57</em>(3), 1–41. (<a
href="https://doi.org/10.1007/s10462-023-10692-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the defense sector, artificial intelligence (AI) and machine learning (ML) have been used to analyse and decipher massive volumes of data, namely for target recognition, surveillance, threat detection and cybersecurity, autonomous vehicles and drones guidance, and language translation. However, there are key points that have been identified as barriers or challenges, especially related to data curation. For this reason, and also due to the need for quick response, the defense sector is looking for AI technologies capable of successfully processing and extracting results from huge amounts of unlabelled or very poorly labelled data. This paper presents an in-depth review of AI/ML algorithms for unsupervised or poorly supervised data, and machine learning operations (MLOps) techniques that are suitable for the defense industry. The algorithms are divided according to their nature, meaning that they either focus on techniques, or on applications. Techniques can belong to the supervision spectrum, or focus on explainability. Applications are either focused on text processing or computer vision. MLOps techniques, tools and practices are then discussed, revealing approaches and reporting experiences with the objective of declaring how to make the operationalization of ML integrated systems more efficient. Despite many contributions from several researchers and industry, further efforts are required to construct substantially robust and reliable models and supporting infrastructures for AI systems, which are reliable and suitable for the defense sector. This review brings up-to-date information regarding AI algorithms and MLOps that will be helpful for future research in the field.},
  archive      = {J_AIR},
  author       = {e Oliveira, Eduardo and Rodrigues, Marco and Pereira, João Paulo and Lopes, António M. and Mestric, Ivana Ilic and Bjelogrlic, Sandro},
  doi          = {10.1007/s10462-023-10692-0},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Unlabeled learning algorithms and operations: Overview and future trends in defense sector},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gene selection for high dimensional biological datasets
using hybrid island binary artificial bee colony with chaos game
optimization. <em>AIR</em>, <em>57</em>(3), 1–74. (<a
href="https://doi.org/10.1007/s10462-023-10675-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray technology, as applied to the fields of bioinformatics, biotechnology, and bioengineering, has made remarkable progress in both the treatment and prediction of many biological problems. However, this technology presents a critical challenge due to the size of the numerous genes present in the high-dimensional biological datasets associated with an experiment, which leads to a curse of dimensionality on biological data. Such high dimensionality of real biological data sets not only increases memory requirements and training costs, but also reduces the ability of learning algorithms to generalise. Consequently, multiple feature selection (FS) methods have been proposed by researchers to choose the most significant and precise subset of classified genes from gene expression datasets while maintaining high classification accuracy. In this research work, a novel binary method called iBABC-CGO based on the island model of the artificial bee colony algorithm, combined with the chaos game optimization algorithm and SVM classifier, is suggested for FS problems using gene expression data. Due to the binary nature of FS problems, two distinct transfer functions are employed for converting the continuous search space into a binary one, thus improving the efficiency of the exploration and exploitation phases. The suggested strategy is tested on a variety of biological datasets with different scales and compared to popular metaheuristic-based, filter-based, and hybrid FS methods. Experimental results supplemented with the statistical measures, box plots, Wilcoxon tests, Friedman tests, and radar plots demonstrate that compared to prior methods, the proposed iBABC-CGO exhibit competitive performance in terms of classification accuracy, selection of the most relevant subset of genes, data variability, and convergence rate. The suggested method is also proven to identify unique sets of informative, relevant genes successfully with the highest overall average accuracy in 15 tested biological datasets. Additionally, the biological interpretations of the selected genes by the proposed method are also provided in our research work.},
  archive      = {J_AIR},
  author       = {Nssibi, Maha and Manita, Ghaith and Chhabra, Amit and Mirjalili, Seyedali and Korbaa, Ouajdi},
  doi          = {10.1007/s10462-023-10675-1},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-74},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Gene selection for high dimensional biological datasets using hybrid island binary artificial bee colony with chaos game optimization},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding upper-limb movements via neurocomputational
models of the sensorimotor system and neurorobotics: Where we stand.
<em>AIR</em>, <em>57</em>(3), 1–35. (<a
href="https://doi.org/10.1007/s10462-023-10694-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roboticists and neuroscientists are interested in understanding and reproducing the neural and cognitive mechanisms behind the human ability to interact with unknown and changing environments as well as to learn and execute fine movements. In this paper, we review the system-level neurocomputational models of the human motor system, and we focus on biomimetic models simulating the functional activity of the cerebellum, the basal ganglia, the motor cortex, and the spinal cord, which are the main central nervous system areas involved in the learning, execution, and control of movements. We review the models that have been proposed from the early of 1970s, when the first cerebellar model was realized, up to nowadays, when the embodiment of these models into robots acting in the real world and into software agents acting in a virtual environment has become of paramount importance to close the perception-cognition-action cycle. This review shows that neurocomputational models have contributed to the comprehension and reproduction of neural mechanisms underlying reaching movements, but much remains to be done because a whole model of the central nervous system controlling musculoskeletal robots is still missing.},
  archive      = {J_AIR},
  author       = {Parziale, Antonio and Marcelli, Angelo},
  doi          = {10.1007/s10462-023-10694-y},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Understanding upper-limb movements via neurocomputational models of the sensorimotor system and neurorobotics: Where we stand},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing long-short-term memory models via metaheuristics
for decomposition aided wind energy generation forecasting.
<em>AIR</em>, <em>57</em>(3), 1–57. (<a
href="https://doi.org/10.1007/s10462-023-10678-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power supply from renewable energy is an important part of modern power grids. Robust methods for predicting production are required to balance production and demand to avoid losses. This study proposed an approach that incorporates signal decomposition techniques with Long Short-Term Memory (LSTM) neural networks tuned via a modified metaheuristic algorithm used for wind power generation forecasting. LSTM networks perform notably well when addressing time-series prediction, and further hyperparameter tuning by a modified version of the reptile search algorithm (RSA) can help improve performance. The modified RSA was first evaluated against standard CEC2019 benchmark instances before being applied to the practical challenge. The proposed tuned LSTM model has been tested against two wind production datasets with hourly resolutions. The predictions were executed without and with decomposition for one, two, and three steps ahead. Simulation outcomes have been compared to LSTM networks tuned by other cutting-edge metaheuristics. It was observed that the introduced methodology notably exceed other contenders, as was later confirmed by the statistical analysis. Finally, this study also provides interpretations of the best-performing models on both observed datasets, accompanied by the analysis of the importance and impact each feature has on the predictions.},
  archive      = {J_AIR},
  author       = {Pavlov-Kagadejev, Marijana and Jovanovic, Luka and Bacanin, Nebojsa and Deveci, Muhammet and Zivkovic, Miodrag and Tuba, Milan and Strumberger, Ivana and Pedrycz, Witold},
  doi          = {10.1007/s10462-023-10678-y},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimizing long-short-term memory models via metaheuristics for decomposition aided wind energy generation forecasting},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel metaheuristic inspired by horned lizard defense
tactics. <em>AIR</em>, <em>57</em>(3), 1–65. (<a
href="https://doi.org/10.1007/s10462-023-10653-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces HLOA, a novel metaheuristic optimization algorithm that mathematically mimics crypsis, skin darkening or lightening, blood-squirting, and move-to-escape defense methods. In crypsis behavior, the lizard changes its color by becoming translucent to avoid detection by its predators. The horned lizard can lighten or darken its skin, depending on whether or not it needs to decrease or increase its solar thermal gain. The skin darkening or lightening strategy is modeled by including the stimulating hormone melanophore rate( $$\alpha$$ -MHS) that influences these skin color changes. Further, the move-to-evasion strategy is also mathematically described. The horned lizard’s shooting blood defense mechanism, described as a projectile motion, is also modeled. These strategies balance exploitation and exploration mechanisms for local and global search over the solution space. HLOA performance is benchmarked with sixty-three optimization problems from the literature, testbench problems provided in IEEE CEC- 2017 “Constrained Real-Parameter Optimization”, analyzed for dimensions 10, 30, 50, and 100, as well as testbench functions from IEEE CEC-06 2019 “100-Digit Challenge”. Moreover, three real-world constraint optimization applications from IEEE CEC2020 and two engineering problems, the multiple gravity assist optimization and the optimal power flow problem, are also studied. Wilcoxon and Friedman statistics tests compare the HLOA algorithm results against ten recent bio-inspired algorithms. Wilcoxon shows that HLOA provides the optimal solution for most testbench functions more effectively than competing algorithms. At the same time, the Friedman statistics test ranks the HLOA first, and the n-dimensional analysis shows that it performs better on the constrained optimization problems for dimensions 50 and 100. The source code is free and available from https://www.mathworks.com/matlabcentral/fileexchange/159658-horned-lizard-optimization-algorithm-hloa .},
  archive      = {J_AIR},
  author       = {Peraza-Vázquez, Hernán and Peña-Delgado, Adrián and Merino-Treviño, Marco and Morales-Cepeda, Ana Beatriz and Sinha, Neha},
  doi          = {10.1007/s10462-023-10653-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel metaheuristic inspired by horned lizard defense tactics},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Role of machine learning and deep learning techniques in
EEG-based BCI emotion recognition system: A review. <em>AIR</em>,
<em>57</em>(3), 1–66. (<a
href="https://doi.org/10.1007/s10462-023-10690-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion is a subjective psychophysiological reaction coming from external stimuli which impacts every aspect of our daily lives. Due to the continuing development of non-invasive and portable sensor technologies, such as brain-computer interfaces (BCI), intellectuals from several fields have been interested in emotion recognition techniques. Human emotions can be recognised using a variety of behavioural cues, including gestures and body language, voice, and physiological markers. The first three, however, might be ineffective because people sometimes conceal their genuine emotions either intentionally or unknowingly. More precise and objective emotion recognition can be accomplished using physiological signals. Among other physiological signals, Electroencephalogram (EEG) is more responsive and sensitive to variation in affective states. Various EEG-based emotion recognition methods have recently been introduced. This study reviews EEG-based BCIs for emotion identification and gives an outline of the progress made in this field. A summary of the datasets and techniques utilised to evoke human emotions and various emotion models is also given. We discuss several EEG feature extractions, feature selection/reduction, machine learning, and deep learning algorithms in accordance with standard emotional identification process. We provide an overview of the human brain&#39;s EEG rhythms, which are closely related to emotional states. We also go over a number of EEG-based emotion identification research and compare numerous machine learning and deep learning techniques. In conclusion, this study highlights the applications, challenges and potential areas for future research in identification and classification of human emotional states.},
  archive      = {J_AIR},
  author       = {Samal, Priyadarsini and Hashmi, Mohammad Farukh},
  doi          = {10.1007/s10462-023-10690-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Milestones in speaker recognition. <em>AIR</em>,
<em>57</em>(3), 1–33. (<a
href="https://doi.org/10.1007/s10462-023-10688-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews significant research in the domain of speaker recognition, i.e., the task of determining the speaker’s identity from its speech. Unlike conventional review articles, this document strives to be concise and selective, provide a historical context, and reach a wider audience. In this endeavour, a summary of selected key works of every decade is provided which highlights the theme(s) of research of that period. At first, an overview of the humble beginnings of the 1960s and 70s is provided, followed by the key developments in the 80s and 90s. The prime focus of the research community in the 2000s is then discussed, leading to various non-conventional features, modelling techniques, and hybrid or fusion systems. The developments of the last decade (the 2010s), such as the i-vector-based systems, are then discussed. Modern speaker recognition based on Artificial Intelligence (AI), such as the x-vector system, and refinements of the i-vector-based systems using deep neural networks, are then discussed. The article concludes with a concise discussion of the evolving recent trends and allied research in speaker recognition.},
  archive      = {J_AIR},
  author       = {Sharma, R. and Govind, D. and Mishra, J. and Dubey, A. K. and Deepak, K. T. and Prasanna, S. R. M.},
  doi          = {10.1007/s10462-023-10688-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Milestones in speaker recognition},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient starling murmuration-based secure web service
model for smart city application using DBN. <em>AIR</em>,
<em>57</em>(3), 1–33. (<a
href="https://doi.org/10.1007/s10462-023-10689-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of IoT devices increased internet usage more than smartphones and personal computers. The manual analysis of the Web Service Description Language (WSDL) document is quite expensive and time-consuming, hence this paper proposes a novel deep learning architecture to overcome the issue associated with web service classification. A structural self-organized deep belief network (SSODBN) is used for real-time web service classification in different fields such as Education, Smart electricity, Intelligent road networks, Health and social care, and Sports, water, and gas distribution. The SSODBN architecture utilizes a dropout strategy to minimize the interrelationship between the feature detectors and a regularized reinforced transfer function to eliminate the irrelevant weights. The main advantage offered by the S-DBN architecture is the improved preprocessing with feature selection. The Starling Murmuration Optimizer (SMO) is utilized in this paper to minimize the reconstruction error of the S-DBN architecture. The security of the smart city architecture is mainly improved via the blockchain defined network (BDN) using user-authenticated blocks. The experimental results revealed that the proposed method managed the scalability, latency, and centralization issues with superior data transmission.},
  archive      = {J_AIR},
  author       = {Sheeba, Adlin and Bushra, S. Nikkath and Rajarajeswari, S. and Subasini, C. A.},
  doi          = {10.1007/s10462-023-10689-9},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient starling murmuration-based secure web service model for smart city application using DBN},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for survival analysis: A review. <em>AIR</em>,
<em>57</em>(3), 1–34. (<a
href="https://doi.org/10.1007/s10462-023-10681-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The influx of deep learning (DL) techniques into the field of survival analysis in recent years has led to substantial methodological progress; for instance, learning from unstructured or high-dimensional data such as images, text or omics data. In this work, we conduct a comprehensive systematic review of DL-based methods for time-to-event analysis, characterizing them according to both survival- and DL-related attributes. In summary, the reviewed methods often address only a small subset of tasks relevant to time-to-event data—e.g., single-risk right-censored data—and neglect to incorporate more complex settings. Our findings are summarized in an editable, open-source, interactive table: https://survival-org.github.io/DL4Survival . As this research area is advancing rapidly, we encourage community contribution in order to keep this database up to date.},
  archive      = {J_AIR},
  author       = {Wiegrebe, Simon and Kopper, Philipp and Sonabend, Raphael and Bischl, Bernd and Bender, Andreas},
  doi          = {10.1007/s10462-023-10681-3},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for survival analysis: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection of microarray data using multidimensional
graph neural network and supernode hierarchical clustering.
<em>AIR</em>, <em>57</em>(3), 1–29. (<a
href="https://doi.org/10.1007/s10462-023-10700-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer remains a significant cause of mortality, and the application of microarray technology has opened new avenues for cancer diagnosis and treatment. However, due to the challenges in sample acquisition, the genetic dimension of microarray data surpasses the sample dimension, resulting in high-dimensional small sample data. Effective feature selection is crucial for identifying biomarkers and facilitating further analysis. However, existing methods struggle to fully exploit the interdependencies among genes, such as regulatory networks and pathways, to guide the feature selection process and construct efficient classification models. In this paper, we propose a novel feature selection algorithm and classification model based on graph neural networks to address these challenges. Our proposed method employs a multidimensional graph to capture intricate gene interactions. We leverage link prediction techniques to enhance the graph structure relationships and employ a multidimensional node evaluator alongside a supernode discovery algorithm based on spectral clustering for initial node filtering. Subsequently, a hierarchical graph pooling technique based on downsampling is used to further refine node selection for feature extraction and model building. We evaluate the proposed method on nine publicly available microarray datasets, and the results demonstrate its superiority over both classical and advanced feature selection techniques in various evaluation metrics. This highlights the effectiveness and advancement of our proposed approach in addressing the complexities associated with microarray data analysis and cancer classification.},
  archive      = {J_AIR},
  author       = {Xie, Weidong and Zhang, Shoujia and Wang, Linjie and Yu, Kun and Li, Wei},
  doi          = {10.1007/s10462-023-10700-3},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Feature selection of microarray data using multidimensional graph neural network and supernode hierarchical clustering},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic fruit picking technology: A comprehensive review
of research advances. <em>AIR</em>, <em>57</em>(3), 1–39. (<a
href="https://doi.org/10.1007/s10462-023-10674-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the fruit industry has become an important part of agricultural development, and fruit harvesting is a key stage in the production process. However, picking fruits during the harvest season is always a major challenge. In order to solve the challenges of time-consuming, costly, and inefficient fruit picking, researchers have conducted a lot of studies on automatic fruit picking equipment. Existing picking technologies still require further research and development to improve efficiency and reduce fruit damage. Aiming at the efficient and non-destructive picking of fruits, this paper reviews machine vision and mechanical fruit picking technology and the current research status, including the current application status, equipment structure, working principle, picking process, and experimental results. As a promising tool, machine vision technology has been widely researched and applied due to its low hardware cost and rich visual information. With the development of science and technology, automated fruit picking technology integrates information technology, integrates automatic perception, transmission, control, and operation, etc., saves manpower costs, and continuously promotes the development of modern agriculture in the direction of refinement of equipment technology, automation, and intelligence. Finally, the challenges faced by automated fruit picking are discussed, and future development is looked forward to with a view to contributing to its sustainable development.},
  archive      = {J_AIR},
  author       = {Zhang, Jun and Kang, Ningbo and Qu, Qianjin and Zhou, Lianghuan and Zhang, Hongbo},
  doi          = {10.1007/s10462-023-10674-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic fruit picking technology: A comprehensive review of research advances},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved honey badger algorithm based on elementary function
density factors and mathematical spirals in polar coordinate systema.
<em>AIR</em>, <em>57</em>(3), 1–58. (<a
href="https://doi.org/10.1007/s10462-023-10658-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Honey Badger Algorithm (HBA) is a new swarm intelligence optimization algorithm by simulating the foraging behavior of honey badgers in nature. To further improve its convergence speed and convergence accuracy, an improved HBA based on the density factors with the elementary functions and the mathematical spirals in the polar coordinate system was proposed. The algorithm proposes six density factors for attenuation states based on elementary functions, and introduces mathematical expressions of the polar diameters and angles of seven mathematical spirals (Fibonacci spiral, Butterfly curve, Rose spiral, Cycloid, Archimedean spiral, Hypotrochoid and Cardioid) in the polar coordinate system based on the density factors with the best synthesized effect to replace the foraging strategy of honey badger digging pattern in HBA. By using 23 benchmark test functions, the above improvements are sequentially compared with the original HBA, and the optimization algorithm with the best improvement, α4CycρHBA, is selected to be compared with SOA, MVO, DOA, CDO, MFO, SCA, BA, GWO and FFA. Finally, four engineering design problems (pressure vessel design, three-bar truss design, cantilever beam design and slotted bulkhead design) were solved. The simulation experiments results show that the proposed improved HBA based on the density factors with the elementary functions and the mathematical spirals of the polar coordinate system has the characteristics of balanced exploration and expiration, fast convergence and high accuracy, and is able to solve the function optimization and engineering optimization problems in a better way.},
  archive      = {J_AIR},
  author       = {Zhang, Si-Wen and Wang, Jie-Sheng and Li, Yi-Xuan and Zhang, Shi-Hui and Wang, Yu-Cai and Wang, Xiao-Tian},
  doi          = {10.1007/s10462-023-10658-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved honey badger algorithm based on elementary function density factors and mathematical spirals in polar coordinate systema},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey of artificial intelligence-based
techniques for performance enhancement of solid oxide fuel cells: Test
cases with debates. <em>AIR</em>, <em>57</em>(2), 1–50. (<a
href="https://doi.org/10.1007/s10462-023-10696-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since installing solid oxide fuel cells (SOFCs)-based systems suffers from high expenses, accurate and reliable modeling is heavily demanded to detect any design issue prior to the system establishment. However, such mathematical models comprise certain unknowns that should be properly estimated to effectively describe the actual operation of SOFCs. Accordingly, due to their recent promising achievements, a tremendous number of metaheuristic optimizers (MHOs) have been utilized to handle this task. Hence, this effort targets providing a novel thorough review of the most recent MHOs applied to define the ungiven parameters of SOFCs stacks. Specifically, among over 300 attempts, only 175 articles are reported, where thirty up-to-date MHOs from the last five years are comprehensively illustrated. Particularly, the discussed MHOs are classified according to their behavior into; evolutionary-based, physics-based, swarm-based, and nature-based algorithms. Each is touched with a brief of their inspiration, features, merits, and demerits, along with their results in SOFC parameters determination. Furthermore, an overall platform is constructed where the reader can easily investigate each algorithm individually in terms of its governing factors, besides, the simulation circumstances related to the studied SOFC test cases. Over and above, numerical simulations are also introduced for commercial SOFCs’ stacks to evaluate the proposed MHOs-based methodology. Moreover, the mathematical formulation of various assessment criteria is systematically presented. After all, some perspectives and observations are provided in the conclusion to pave the way for further analyses and innovations.},
  archive      = {J_AIR},
  author       = {Ashraf, Hossam and Draz, Abdelmonem},
  doi          = {10.1007/s10462-023-10696-w},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of artificial intelligence-based techniques for performance enhancement of solid oxide fuel cells: Test cases with debates},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autoencoders and their applications in machine learning: A
survey. <em>AIR</em>, <em>57</em>(2), 1–52. (<a
href="https://doi.org/10.1007/s10462-023-10662-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders have become a hot researched topic in unsupervised learning due to their ability to learn data features and act as a dimensionality reduction method. With rapid evolution of autoencoder methods, there has yet to be a complete study that provides a full autoencoders roadmap for both stimulating technical improvements and orienting research newbies to autoencoders. In this paper, we present a comprehensive survey of autoencoders, starting with an explanation of the principle of conventional autoencoder and their primary development process. We then provide a taxonomy of autoencoders based on their structures and principles and thoroughly analyze and discuss the related models. Furthermore, we review the applications of autoencoders in various fields, including machine vision, natural language processing, complex network, recommender system, speech process, anomaly detection, and others. Lastly, we summarize the limitations of current autoencoder algorithms and discuss the future directions of the field.},
  archive      = {J_AIR},
  author       = {Berahmand, Kamal and Daneshfar, Fatemeh and Salehi, Elaheh Sadat and Li, Yuefeng and Xu, Yue},
  doi          = {10.1007/s10462-023-10662-6},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Autoencoders and their applications in machine learning: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning based computer aided diagnosis of alzheimer’s
disease: A snapshot of last 5 years, gaps, and future directions.
<em>AIR</em>, <em>57</em>(2), 1–62. (<a
href="https://doi.org/10.1007/s10462-023-10644-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease affects around one in every nine persons among the elderly population. Being a neurodegenerative disease, its cure has not been established till date and is managed through supportive care by the health care providers. Thus, early diagnosis of this disease is a crucial step towards its treatment plan. There exist several diagnostic procedures viz., clinical, scans, biomedical, psychological, and others for the disease’s detection. Computer-aided diagnostic techniques aid in the early detection of this disease and in the past, several such mechanisms have been proposed. These techniques utilize machine learning models to develop a disease classification system. However, the focus of these systems has now gradually shifted to the newer deep learning models. In this regards, this article aims in providing a comprehensive review of the present state-of-the-art techniques as a snapshot of the last 5 years. It also summarizes various tools and datasets available for the development of the early diagnostic systems that provide fundamentals of this field to a novice researcher. Finally, we discussed the need for exploring biomarkers, identification and extraction of relevant features, trade-off between traditional machine learning and deep learning models and the essence of multimodal datasets. This enables both medical, engineering researchers and developers to address the identified gaps and develop an effective diagnostic system for the Alzheimer’s disease.},
  archive      = {J_AIR},
  author       = {Bhandarkar, Anish and Naik, Pratham and Vakkund, Kavita and Junjappanavar, Srasthi and Bakare, Savita and Pattar, Santosh},
  doi          = {10.1007/s10462-023-10644-8},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning based computer aided diagnosis of alzheimer’s disease: A snapshot of last 5 years, gaps, and future directions},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on training challenges in generative adversarial
networks for biomedical image analysis. <em>AIR</em>, <em>57</em>(2),
1–62. (<a href="https://doi.org/10.1007/s10462-023-10624-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical image analysis, the applicability of deep learning methods is directly impacted by the quantity of image data available. This is due to deep learning models requiring large image datasets to provide high-level performance. Generative Adversarial Networks (GANs) have been widely utilized to address data limitations through the generation of synthetic biomedical images. GANs consist of two models. The generator, a model that learns how to produce synthetic images based on the feedback it receives. The discriminator, a model that classifies an image as synthetic or real and provides feedback to the generator. Throughout the training process, a GAN can experience several technical challenges that impede the generation of suitable synthetic imagery. First, the mode collapse problem whereby the generator either produces an identical image or produces a uniform image from distinct input features. Second, the non-convergence problem whereby the gradient descent optimizer fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem whereby unstable training behavior occurs due to the discriminator achieving optimal classification performance resulting in no meaningful feedback being provided to the generator. These problems result in the production of synthetic imagery that is blurry, unrealistic, and less diverse. To date, there has been no survey article outlining the impact of these technical challenges in the context of the biomedical imagery domain. This work presents a review and taxonomy based on solutions to the training problems of GANs in the biomedical imaging domain. This survey highlights important challenges and outlines future research directions about the training of GANs in the domain of biomedical imagery.},
  archive      = {J_AIR},
  author       = {Saad, Muhammad Muneeb and O’Reilly, Ruairi and Rehmani, Mubashir Husain},
  doi          = {10.1007/s10462-023-10624-y},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on training challenges in generative adversarial networks for biomedical image analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the trend of recognizing apple leaf disease
detection through machine learning: A comprehensive analysis using
bibliometric techniques. <em>AIR</em>, <em>57</em>(2), 1–26. (<a
href="https://doi.org/10.1007/s10462-023-10628-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study’s foremost objectives were to scrutinize how unexpected weather affects agricultural output and to assess how well AI-based machine learning and deep leaning algorithms work for spotting apple leaf diseases. The researchers carried out a bibliometric study to obtain understanding of the current research trends, citation patterns, ownership and partnership arrangements, publishing patterns, and other parameters related to early identification of apple illnesses. Comprehensive interdisciplinary scientific maps are limited because syndrome recognition is not restricted to any solitary arena of research, despite the fact that there have been many studies on the identification of apple diseases. By employing a scientometric technique and 109 publications from the Scopus database published between 2011 and 2022, this study attempted to assess the condition of the research area and combine knowledge frameworks. To find important journals, authors, nations, articles, and topics, the study used the automated processes of VOSviewer and Biblioshiny software. Patterns and trends were discovered using citation counts, social network analysis, and citation and co-citation studies.},
  archive      = {J_AIR},
  author       = {Bonkra, Anupam and Pathak, Sunil and Kaur, Amandeep and Shah, Mohd Asif},
  doi          = {10.1007/s10462-023-10628-8},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring the trend of recognizing apple leaf disease detection through machine learning: A comprehensive analysis using bibliometric techniques},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DWSR: An architecture optimization framework for adaptive
super-resolution neural networks based on meta-heuristics. <em>AIR</em>,
<em>57</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s10462-023-10648-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advancements in super-resolution neural network optimization, a fundamental challenge remains unresolved: as the number of parameters is reduced, the network’s performance significantly deteriorates. This paper presents a novel framework called the Depthwise Separable Convolution Super-Resolution Neural Network Framework (DWSR) for optimizing super-resolution neural network architectures. The depthwise separable convolutions are introduced to reduce the number of parameters and minimize the impact on the performance of the super-resolution neural network. The proposed framework uses the RUNge Kutta optimizer (RUN) variant (MoBRUN) as the search method. MoBRUN is a multi-objective binary version of RUN, which balances multiple objectives when optimizing the neural network architecture. Experimental results on publicly available datasets indicate that the DWSR framework can reduce the number of parameters of the Residual Dense Network (RDN) model by 22.17% while suffering only a minor decrease of 0.018 in Peak Signal-to-Noise Ratio (PSNR), the framework can reduce the number of parameters of the Enhanced SRGAN (ESRGAN) model by 31.45% while losing only 0.08 PSNR. Additionally, the framework can reduce the number of parameters of the HAT model by 5.38% while losing only 0.02 PSNR.},
  archive      = {J_AIR},
  author       = {Chu, Shu-Chuan and Dou, Zhi-Chao and Pan, Jeng-Shyang and Kong, Lingping and Snášel, Václav and Watada, Junzo},
  doi          = {10.1007/s10462-023-10648-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {DWSR: An architecture optimization framework for adaptive super-resolution neural networks based on meta-heuristics},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lq-norm multiple kernel fusion regression for self-cleansing
sediment transport. <em>AIR</em>, <em>57</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s10462-023-10673-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimental and modeling studies have been conducted to develop an approach for self-cleansing rigid boundary open channel design such as drainage and sewer systems. Self-cleansing experiments in the literature are mostly performed on circular channel cross-section, while a few studies considered self-cleansing sediment transport in small rectangular channels. Experiments in this study were carried out in a rectangular channel with a length of 12.5 m, a width of 0.6 m, a depth of 0.7 m and having an automatic control system for regulating channel slope, discharge and sediment rate. Behind utilizing collected experimental data in this study, existing data in the literature for rectangular channels are used to develop self-cleansing models applicable for channel design. Through the modeling procedure, this study recommends Lq-norm multiple kernel fusion regression (LMKFR) techniques for self-cleansing sediment transport. The LMKFR is a regression technique based on the regularized kernel regression method which benefits from the combination of multiple information sources to improve the performance using the Lq-norm multiple kernel learning framework. The results obtained by LMKFR are compared to support vector regression benchmark and existing conventional regression self-cleansing sediment transport models in the literature for rectangular channels. The superiority of LMKFR is illustrated in an accurate modeling as compared with its alternatives in terms of various statistical error measurement criteria. The encouraging results of LMKFR can be linked to utilization of several kernels which are fused effectively using an Lq-norm prior that captures the intrinsic sparsity of the problem at hand. Promising performance of LMKFR technique in this study suggests it as an effective technique to be examined in similar environmental, hydrological and hydraulic problems.},
  archive      = {J_AIR},
  author       = {Safari, Mir Jafar Sadegh and Rahimzadeh Arashloo, Shervin and Kohandel Gargari, Mehrnoush},
  doi          = {10.1007/s10462-023-10673-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Lq-norm multiple kernel fusion regression for self-cleansing sediment transport},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning team-based navigation: A review of deep
reinforcement learning techniques for multi-agent pathfinding.
<em>AIR</em>, <em>57</em>(2), 1–36. (<a
href="https://doi.org/10.1007/s10462-023-10670-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation indicators and providing comprehensive clarification on these indicators. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified indicators for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.},
  archive      = {J_AIR},
  author       = {Chung, Jaehoon and Fayyad, Jamil and Younes, Younes Al and Najjaran, Homayoun},
  doi          = {10.1007/s10462-023-10670-6},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learning team-based navigation: A review of deep reinforcement learning techniques for multi-agent pathfinding},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel person detection and suspicious activity recognition
using enhanced YOLOv5 and motion feature map. <em>AIR</em>,
<em>57</em>(2), 1–36. (<a
href="https://doi.org/10.1007/s10462-023-10630-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person and suspicious activity detection is a major challenge for image-based surveillance systems. However, the accuracy of person detection is affected by several factors, such as the presence of the person, his trajectory, posture, complex background, and object distortion. In this work, we developed a person-focused dataset that includes various behaviors of students in an educational institution, such as cheating, theft of lab equipment, fights, and threatening situations. This dataset ensures consistent and standardized identification annotations for individuals, making it suitable for detection, tracking, and behavioral analysis of individuals. In addition, we have increased the detection accuracy through an improved architecture called YOLOv5 and introduced an efficient method for detecting global and local anomalous behaviors. This method extracts motion features that accurately describe the person’s movement, speed, and direction. To evaluate the effectiveness of our proposed approach, we validated it against our proposed database and publicly available benchmark datasets. Our method achieves state-of-the-art detection accuracy, namely 96.12%, with an error rate of 6.68% compared to existing methods. The empirical results show a significant improvement in anomalous activity detection. Our paper concludes with a summary and a discussion of possible future research directions.},
  archive      = {J_AIR},
  author       = {Gawande, Ujwalla and Hajari, Kamal and Golhar, Yogesh},
  doi          = {10.1007/s10462-023-10630-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Novel person detection and suspicious activity recognition using enhanced YOLOv5 and motion feature map},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way decisions in generalized intuitionistic fuzzy
environments: Survey and challenges. <em>AIR</em>, <em>57</em>(2), 1–45.
(<a href="https://doi.org/10.1007/s10462-023-10647-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing decision-making under risks is crucial in various fields, and three-way decision (3WD) methods have been extensively utilized and proven to be effective in numerous scenarios. However, traditional methods may not be sufficient when addressing intricate decision-making scenarios characterized by uncertain and ambiguous information. In response to this challenge, the generalized intuitionistic fuzzy set (IFS) theory extends the conventional fuzzy set theory by introducing two pivotal concepts, i.e., membership degrees and non-membership degrees. These concepts offer a more comprehensive means of portraying the relationship between elements and fuzzy concepts, thereby boosting the ability to model complex problems. The generalized IFS theory brings about heightened flexibility and precision in problem-solving, allowing for a more thorough and accurate description of intricate phenomena. Consequently, the generalized IFS theory emerges as a more refined tool for articulating fuzzy phenomena. The paper offers a thorough review of the research advancements made in 3WD methods within the context of generalized intuitionistic fuzzy (IF) environments. First, the paper summarizes fundamental aspects of 3WD methods and the IFS theory. Second, the paper discusses the latest development trends, including the application of these methods in new fields and the development of new hybrid methods. Furthermore, the paper analyzes the strengths and weaknesses of research methods employed in recent years. While these methods have yielded impressive outcomes in decision-making, there are still some limitations and challenges that need to be addressed. Finally, the paper proposes key challenges and future research directions. Overall, the paper offers a comprehensive and insightful review of the latest research progress on 3WD methods in generalized IF environments, which can provide guidance for scholars and engineers in the intelligent decision-making field with situations characterized by various uncertainties.},
  archive      = {J_AIR},
  author       = {Ding, Juanjuan and Zhang, Chao and Li, Deyu and Zhan, Jianming and Li, Wentao and Yao, Yiyu},
  doi          = {10.1007/s10462-023-10647-5},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Three-way decisions in generalized intuitionistic fuzzy environments: Survey and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and prediction of business success: A survey.
<em>AIR</em>, <em>57</em>(2), 1–51. (<a
href="https://doi.org/10.1007/s10462-023-10664-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Businesses are the driving force behind economic systems and are the lifeblood of the community. A business shares striking similarity to a living organism, including birth, infancy, rising, prosperity, and falling. The success of a business is not only important to the owners, but is also critical to the regional/domestic economic system, or even the global economy. Recent years have witnessed many new emerging businesses with tremendous success, such as Google, Apple, Facebook etc., yet millions of businesses also fail or fade out within a rather short period of time. Finding patterns/factors connected to the business rise and fall remains a long lasting question puzzling many economists, entrepreneurs, and government officials. Recent advancement in artificial intelligence, especially machine learning, has lend researchers powers to use data to model and predict business success. However, due to data driven nature of all machine learning methods, existing approaches are rather domain-driven and ad-hoc in their design and validations. In this paper, we propose a systematic review of modeling and prediction of business success. We first outline a triangle framework to showcase three parities connected to the business: Investment-Business-Market (IBM). After that, we align features into three main categories, each of which is focused on modeling a business from a particular perspective, such as sales, management, innovation etc., and further summarize different types of machine learning and deep learning methods for business modeling and prediction. The survey provides a comprehensive review of computational approaches for business performance modeling and prediction.},
  archive      = {J_AIR},
  author       = {Gangwani, Divya and Zhu, Xingquan},
  doi          = {10.1007/s10462-023-10664-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Modeling and prediction of business success: A survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks: A survey on the links between privacy
and security. <em>AIR</em>, <em>57</em>(2), 1–47. (<a
href="https://doi.org/10.1007/s10462-023-10656-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are models that capture the dependencies between graph data by passing messages between graph nodes and they have been widely used to process graph data that contains relational information. Example application areas include social networks, recommendation systems, and life sciences. However, like all neural networks, there are underpinning security and privacy concerns associated with GNN deployments in practice. For example, attackers can perturb a graph’s data to undermine a model’s effectiveness, or they can steal the model’s data and/or parameters, thus threatening the privacy of the model. In this survey, we provide a comprehensive review of recent research efforts on security and/or privacy in GNNs. We also systematically describe the distinctions and relationships between security and privacy, as well as providing an outlook on future directions of research in this area.},
  archive      = {J_AIR},
  author       = {Guan, Faqian and Zhu, Tianqing and Zhou, Wanlei and Choo, Kim-Kwang Raymond},
  doi          = {10.1007/s10462-023-10656-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph neural networks: A survey on the links between privacy and security},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring data mining and machine learning in gynecologic
oncology. <em>AIR</em>, <em>57</em>(2), 1–47. (<a
href="https://doi.org/10.1007/s10462-023-10666-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gynecologic (GYN) malignancies are gaining new and much-needed attention, perpetually fueling literature. Intra-/inter-tumor heterogeneity and “frightened” global distribution by race, ethnicity, and human development index, are pivotal clues to such ubiquitous interest. To advance “precision medicine” and downplay the heavy burden, data mining (DM) is timely in clinical GYN oncology. No consolidated work has been conducted to examine the depth and breadth of DM applicability as an adjunct to GYN oncology, emphasizing machine learning (ML)-based schemes. This systematic literature review (SLR) synthesizes evidence to fill knowledge gaps, flaws, and limitations. We report this SLR in compliance with Kitchenham and Charters’ guidelines. Defined research questions and PICO crafted a search string across five libraries: PubMed, IEEE Xplore, ScienceDirect, SpringerLink, and Google Scholar—over the past decade. Of the 3499 potential records, 181 primary studies were eligible for in-depth analysis. A spike (60.53%) corollary to cervical neoplasms is denoted onward 2019, predominantly featuring empirical solution proposals drawn from cohorts. Medical records led (23.77%, 53 art.). DM-ML in use is primarily built on neural networks (127 art.), appoint classification (73.19%, 172 art.) and diagnoses (42%, 111 art.), all devoted to assessment. Summarized evidence is sufficient to guide and support the clinical utility of DM schemes in GYN oncology. Gaps persist, inculpating the interoperability of single-institute scrutiny. Cross-cohort generalizability is needed to establish evidence while avoiding outcome reporting bias to locally, site-specific trained models. This SLR is exempt from ethics approval as it entails published articles.},
  archive      = {J_AIR},
  author       = {Idlahcen, Ferdaous and Idri, Ali and Goceri, Evgin},
  doi          = {10.1007/s10462-023-10666-2},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring data mining and machine learning in gynecologic oncology},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel group decision-making method for interval-valued
q-rung dual hesitant fuzzy information using extended power average
operator and frank operations. <em>AIR</em>, <em>57</em>(2), 1–47. (<a
href="https://doi.org/10.1007/s10462-023-10665-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper advances the field of multi-attribute group decision making (MAGDM) by proposing a novel framework based on interval-valued q-rung dual hesitant fuzzy sets (IVq-RDHFSs). IVq-RDHFSs, which surpass most existing fuzzy sets, effectively represent complex fuzzy information by describing membership and non-membership degrees through interval value sets. However, prior MAGDM methods based on IVq-RDHFSs have been limited by the functions of operation rules and aggregation operators (AOs). This limitation is addressed through the construction of a new MAGDM framework, leveraging the robust Frank t-norm and t-conorm (FTT) operation and the extended power average (EPA) operator. The proposed framework features the interval-valued q-rung dual hesitant fuzzy Frank weighted extended power average (IVq-RDHFFWEPA) operator to obtain comprehensive evaluation values. The paper also introduces novel techniques for determining the weights of decision-makers and attributes. Practical applications of the proposed method are demonstrated through the assessment of desalination technology selection and rural green eco-tourism projects. Sensitivity and comparison analyses validate the superior functionality, accuracy, and flexibility of this method compared to many state-of-the-art methods. The contributions of this paper are two-fold: it develops efficient measurement techniques for IVq-RDHFSs, such as distance and weight calculation, and it introduces a comprehensive MAGDM method by integrating FTT and EPA under IVq-RDHFSs, which improves the efficiency of solving decision-making problems.},
  archive      = {J_AIR},
  author       = {Xu, Wuhuan and Yao, Zhong and Wang, Jun and Xu, Yuan},
  doi          = {10.1007/s10462-023-10665-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel group decision-making method for interval-valued q-rung dual hesitant fuzzy information using extended power average operator and frank operations},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overview of temporal action detection based on deep
learning. <em>AIR</em>, <em>57</em>(2), 1–77. (<a
href="https://doi.org/10.1007/s10462-023-10650-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Action Detection (TAD) aims to accurately capture each action interval in an untrimmed video and to understand human actions. This paper comprehensively surveys the state-of-the-art techniques and models used for TAD task. Firstly, it conducts comprehensive research on this field through Citespace and comprehensively introduce relevant dataset. Secondly, it summarizes three types of methods, i.e., anchor-based, boundary-based, and query-based, from the design method level. Thirdly, it summarizes three types of supervised learning methods from the level of learning methods, i.e., fully supervised, weakly supervised, and unsupervised. Finally, this paper explores the current problems, and proposes prospects in TAD task.},
  archive      = {J_AIR},
  author       = {Hu, Kai and Shen, Chaowen and Wang, Tianyan and Xu, Keer and Xia, Qingfeng and Xia, Min and Cai, Chengxue},
  doi          = {10.1007/s10462-023-10650-w},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-77},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Overview of temporal action detection based on deep learning},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chinese dialect speech recognition: A comprehensive survey.
<em>AIR</em>, <em>57</em>(2), 1–39. (<a
href="https://doi.org/10.1007/s10462-023-10668-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a multi-ethnic country with a large population, China is endowed with diverse dialects, which brings considerable challenges to speech recognition work. In fact, due to geographical location, population migration, and other factors, the research progress and practical application of Chinese dialect speech recognition are currently at different stages. Therefore, exploring the significant regional heterogeneities in specific recognition approaches and effects, dialect corpus, and other resources is of vital importance for Chinese speech recognition work. Based on this, we first start with the regional classification of dialects and analyze the pivotal acoustic characteristics of dialects, including specific vowels and tones patterns. Secondly, we comprehensively summarize the existing dialect phonetic corpus in China, which is of some assistance in exploring the general construction methods of dialect phonetic corpus. Moreover, we expound on the general process of dialect recognition. Several critical dialect recognition approaches are summarized and introduced in detail, especially the hybrid method of Artificial Neural Network (ANN) combined with the Hidden Markov Model(HMM), as well as the End-to-End (E2E). Thirdly, through the in-depth comparison of their principles, merits, disadvantages, and recognition performance for different dialects, the development trends and challenges in dialect recognition in the future are pointed out. Finally, some application examples of dialect speech recognition are collected and discussed.},
  archive      = {J_AIR},
  author       = {Li, Qiang and Mai, Qianyu and Wang, Mandou and Ma, Mingjuan},
  doi          = {10.1007/s10462-023-10668-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Chinese dialect speech recognition: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformers in health: A systematic review on architectures
for longitudinal data analysis. <em>AIR</em>, <em>57</em>(2), 1–39. (<a
href="https://doi.org/10.1007/s10462-023-10677-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers are state-of-the-art technology to support diverse Natural Language Processing (NLP) tasks, such as language translation and word/sentence predictions. The main advantage of transformers is their ability to obtain high accuracies when processing long sequences since they avoid the vanishing gradient problem and use the attention mechanism to maintain the focus on the information that matters. These features are fostering the use of transformers in other domains beyond NLP. This paper employs a systematic protocol to identify and analyze studies that propose new transformers’ architectures for processing longitudinal health datasets, which are often dense, and specifically focused on physiological, symptoms, functioning, and other daily life data. Our analysis considered 21 of 456 initial papers, collecting evidence to characterize how recent studies modified or extended these architectures to handle longitudinal multifeatured health representations or provide better ways to generate outcomes. Our findings suggest, for example, that the main efforts are focused on methods to integrate multiple vocabularies, encode input data, and represent temporal notions among longitudinal dependencies. We comprehensively discuss these and other findings, addressing major issues that are still open to efficiently deploy transformers architectures for longitudinal multifeatured healthcare data analysis.},
  archive      = {J_AIR},
  author       = {Siebra, Clauirton A. and Kurpicz-Briki, Mascha and Wac, Katarzyna},
  doi          = {10.1007/s10462-023-10677-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transformers in health: A systematic review on architectures for longitudinal data analysis},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent deep learning techniques for energy consumption
forecasting in smart buildings: A review. <em>AIR</em>, <em>57</em>(2),
1–33. (<a href="https://doi.org/10.1007/s10462-023-10660-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urbanization increases electricity demand due to population growth and economic activity. To meet consumer’s demands at all times, it is necessary to predict the future building energy consumption. Power Engineers could exploit the enormous amount of energy-related data from smart meters to plan power sector expansion. Researchers have made many experiments to address the supply and demand imbalance by accurately predicting the energy consumption. This paper presents a comprehensive literature review of forecasting methodologies used by researchers for energy consumption in smart buildings to meet future energy requirements. Different forecasting methods are being explored in both residential and non-residential buildings. The literature is further analyzed based on the dataset, types of load, prediction accuracy, and the evaluation metrics used. This work also focuses on the main challenges in energy forecasting due to load fluctuation, variability in weather, occupant behavior, and grid planning. The identified research gaps and the suitable methodology for prediction addressing the current issues are presented with reference to the available literature. The multivariate analysis in the suggested hybrid model ensures the learning of repeating patterns and features in the data to enhance the prediction accuracy.},
  archive      = {J_AIR},
  author       = {Mathumitha, R. and Rathika, P. and Manimala, K.},
  doi          = {10.1007/s10462-023-10660-8},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent deep learning techniques for energy consumption forecasting in smart buildings: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational autoencoders for 3D data processing.
<em>AIR</em>, <em>57</em>(2), 1–53. (<a
href="https://doi.org/10.1007/s10462-023-10687-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) play an important role in high-dimensional data generation based on their ability to fuse the stochastic data representation with the power of recent deep learning techniques. The main advantages of these types of generators lie in their ability to encode the information with the possibility to decode and generalize new samples. This capability was heavily explored for 2D image processing; however, only limited research focuses on VAEs for 3D data processing. In this article, we provide a thorough review of the latest achievements in 3D data processing using VAEs. These 3D data types are mostly point clouds, meshes, and voxel grids, which are the focus of a wide range of applications, especially in robotics. First, we shortly present the basic autoencoder with the extensions towards the VAE with further subcategories relevant to discrete point cloud processing. Then, the 3D data specific VAEs are presented according to how they operate on spatial data. Finally, a few comprehensive table summarizing the methods, codes, and datasets as well as a citation map is presented for a better understanding of the VAEs applied to 3D data. The structure of the analyzed papers follows a taxonomy, which differentiates the algorithms according to their primary data types and application domains.},
  archive      = {J_AIR},
  author       = {Molnár, Szilárd and Tamás, Levente},
  doi          = {10.1007/s10462-023-10687-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Variational autoencoders for 3D data processing},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portia spider algorithm: An evolutionary computation
approach for engineering application. <em>AIR</em>, <em>57</em>(2),
1–42. (<a href="https://doi.org/10.1007/s10462-023-10683-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Portia spider, a notable member of the jumping spider family (Salticidae), is widely recognized for its intricate hunting strategies and remarkable problem-solving prowess. Several species fall under the “Portia” genus, with habitats spanning regions in Africa, Asia, and Australia. Demonstrating the ability to tackle new challenges, these spiders can learn and adapt their strategies based on prior experiences. This study introduces the Portia Spider Algorithm (PSA), a swarm-based technique inspired by the unique predatory strategies of the Portia spider. We conducted rigorous assessments of PSA performance against 23 classical test functions, 29 CEC2017 test cases, and 5 engineering optimization tasks. To demonstrate the effectiveness of the PSA, outcomes were juxtaposed with those of renowned algorithms. This paper explores the mechanics, advantages, and potential applications of PSA within the vast domain of computational optimization.},
  archive      = {J_AIR},
  author       = {Pham, Vu Hong Son and Nguyen Dang, Nghiep Trinh},
  doi          = {10.1007/s10462-023-10683-1},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Portia spider algorithm: An evolutionary computation approach for engineering application},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scientometric analysis of quantum-inspired metaheuristic
algorithms. <em>AIR</em>, <em>57</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s10462-023-10659-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum algorithms, based on the principles of quantum mechanics, offer significant parallel processing capabilities with a wide range of applications. Nature-inspired stochastic optimization algorithms have long been a research hotspot. The fusion of quantum mechanics with optimization methods can potentially address NP-hard problems more efficiently and exponentially faster. The potential advantages provided by the ground-breaking paradigm have expedited the scientific output of quantum-inspired optimization algorithms locale. Consequently, a pertinent investigation is required to explain how ground-breaking scientific advancements have evolved. The scientometric approach utilizes quantitative and qualitative techniques to analyze research publications to evaluate the structure of scientific knowledge. Henceforth, the current research presents a scientometric and systematic analysis of quantum-inspired metaheuristic algorithms (QiMs) literature from the Scopus database since its inception. The scientometric implications of the article offer a detailed exploration of the publication patterns, keyword co-occurrence network analysis, author co-citation analysis and country collaboration analysis corresponding to each opted category of QiMs. The analysis reveals that QiMs solely account to 26.66% of publication share in quantum computing and have experienced an impressive 42.59% growth rate in the past decade. Notably, power management, adiabatic quantum computation, and vehicle routing are prominent emerging application areas. An extensive systematic literature analysis identifies key insights and research gaps in the QiMs knowledge domain. Overall, the findings of the current article provide scientific cues to researchers and the academic fraternity for identifying the intellectual landscape and latest research trends of QiMs, thereby fostering innovation and informed decision-making.},
  archive      = {J_AIR},
  author       = {Pooja and Sood, Sandeep Kumar},
  doi          = {10.1007/s10462-023-10659-1},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scientometric analysis of quantum-inspired metaheuristic algorithms},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on neural topic models: Methods, applications, and
challenges. <em>AIR</em>, <em>57</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s10462-023-10661-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field—neural topic models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built on NTMs. Finally, we highlight the challenges confronted by NTMs to inspire future research.},
  archive      = {J_AIR},
  author       = {Wu, Xiaobao and Nguyen, Thong and Luu, Anh Tuan},
  doi          = {10.1007/s10462-023-10661-7},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on neural topic models: Methods, applications, and challenges},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence powered metaverse: Analysis,
challenges and future perspectives. <em>AIR</em>, <em>57</em>(2), 1–46.
(<a href="https://doi.org/10.1007/s10462-023-10641-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse, a virtual reality (VR) space where users can interact with each other and digital objects, is rapidly becoming a reality. As this new world evolves, Artificial Intelligence (AI) is playing an increasingly important role in shaping its development. Integrating AI with emerging technologies in the Metaverse creates new possibilities for immersive experiences that were previously impossible. This paper explores how AI is integrated with technologies such as the Internet of Things, blockchain, Natural Language Processing, virtual reality, Augmented Reality, Mixed Reality, and Extended Reality. One potential benefit of using AI in the Metaverse is the ability to create personalized experiences for individual users, based on their behavior and preferences. Another potential benefit of using AI in the Metaverse is the ability to automate repetitive tasks, freeing up time and resources for more complex and creative endeavors. However, there are also challenges associated with using AI in the Metaverse, such as ensuring user privacy and addressing issues of bias and discrimination. By examining the potential benefits and challenges of using AI in the Metaverse, including ethical considerations, we can better prepare for this exciting new era of VR. This paper presents a comprehensive survey of AI and its integration with other emerging technologies in the Metaverse, as the Metaverse continues to evolve and grow, it will be important for developers and researchers to stay up to date with the latest developments in AI and emerging technologies to fully leverage their potential.},
  archive      = {J_AIR},
  author       = {Soliman, Mona M. and Ahmed, Eman and Darwish, Ashraf and Hassanien, Aboul Ella},
  doi          = {10.1007/s10462-023-10641-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence powered metaverse: Analysis, challenges and future perspectives},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient multimodal sentiment analysis in social media
using hybrid optimal multi-scale residual attention network.
<em>AIR</em>, <em>57</em>(2), 1–27. (<a
href="https://doi.org/10.1007/s10462-023-10645-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a key component of many social media analysis projects. Additionally, prior research has concentrated on a single modality in particular, such as text descriptions for visual information. In contrast to standard image databases, social images frequently connect to one another, making sentiment analysis challenging. The majority of methods now in use consider different images individually, rendering them useless for interrelated images. We proposed a hybrid Arithmetic Optimization Algorithm- Hunger Games Search (AOA-HGS)-optimized Ensemble Multi-scale Residual Attention Network (EMRA-Net) technique in this paper to explore the modal correlations including texts, audio, social links, and video for more effective multimodal sentiment analysis. The hybrid AOA-HGS technique learns complementary and comprehensive features. The EMRA-Net uses two segments, including Ensemble Attention CNN (EA-CNN) and Three-scale Residual Attention Convolutional Neural Network (TRA-CNN), to analyze the multimodal sentiments. The loss of spatial domain image texture features can be reduced by adding the Wavelet transform to TRA-CNN. The feature-level fusion technique known as EA-CNN is used to combine visual, audio, and textual information. The proposed method performs significantly better than the existing multimodel sentimental analysis techniques of HALCB, HDF, and MMLatch when evaluated using the Multimodal Emotion Lines Dataset (MELD) and EmoryNLP datasets. Also, even though the size of the training set varies, the proposed method outperformed other techniques in terms of recall, accuracy, F score, and precision and takes less time to compute in both datasets.},
  archive      = {J_AIR},
  author       = {Subbaiah, Bairavel and Murugesan, Kanipriya and Saravanan, Prabakeran and Marudhamuthu, Krishnamurthy},
  doi          = {10.1007/s10462-023-10645-7},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient multimodal sentiment analysis in social media using hybrid optimal multi-scale residual attention network},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonadditive rough set model for long-term clinical
efficacy evaluation of chronic diseases in real-world settings.
<em>AIR</em>, <em>57</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s10462-023-10672-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of clinical effectiveness in real-world settings is at the core of clinical practice progression. In this study, we address a long-term clinical efficacy evaluation decision-making problem with temporal correlation hybrid attribute characteristics. To address this problem, we propose a novel approach that combines a temporal correlation feature rough set model with machine learning techniques and nonadditive measures. Our proposed approach involves several steps. First, over the framework of granular computing, we construct a temporal correlation hybrid information system, the gradient method is employed to characterize the temporal attributes and the similarity between objects is measured using cosine similarity. Second, based on the similarity of gradient and cosine, we construct a composite binary relation of temporal correlation hybrid information, enabling effective classification of this information. Third, we develop a rough set decision model based on the Choquet integral, which describes temporal correlation decision process. We provide the ranking results of decision schemes with temporal correlation features. To demonstrate the practical applications of our approach, we conduct empirical research using an unlabeled dataset consisting of 3094 patients with chronic renal failure (CRF) and 80,139 EHRs from various clinical encounters. These findings offer valuable support for clinical decision-making. Two main innovations are obtained from this study. First, it establishes general theoretical principles and decision-making methods for temporal correlation and hybrid rough sets. Second, it integrates data-driven clinical decision paradigms with traditional medical research paradigms, laying the groundwork for exploring the feasibility of data-driven clinical decision-making in the field.},
  archive      = {J_AIR},
  author       = {Xiaoli, Chu and Juan, Xu and Xiaodong, Chu and Bingzhen, Sun and Yan, Zhang and Kun, Bao and Yanlin, Li},
  doi          = {10.1007/s10462-023-10672-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A nonadditive rough set model for long-term clinical efficacy evaluation of chronic diseases in real-world settings},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Moderately supervised learning: Definition, framework and
generality. <em>AIR</em>, <em>57</em>(2), 1–35. (<a
href="https://doi.org/10.1007/s10462-023-10654-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training dataset is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training dataset is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks under the era of deep learning have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the appropriate solutions for specific SL tasks. Thus, for practitioners in various AI application fields, it is desirable to reveal these details systematically. This article attempts to achieve this goal by expanding the categorization of SL and investigating the sub-type that plays the central role in SL. More specifically, taking into consideration the properties of the transformation from the given labels to easy-to-learn targets, we firstly categorize SL into three narrower sub-types. Then we focus on the moderately supervised learning (MSL) sub-type that concerns the situation where the given labels are ideal, but due to the simplicity in annotation, careful designs are required to transform the given labels into easy-to-learn targets. From the perspectives of the definition, framework and generality, we conceptualize MSL to present a complete fundamental basis to systematically analyse MSL tasks. At meantime, revealing the relation between the conceptualization of MSL and the mathematicians’ vision, this article as well establishes a tutorial for AI application practitioners to refer to viewing a problem to be solved from the mathematicians’ vision.},
  archive      = {J_AIR},
  author       = {Yang, Yongquan},
  doi          = {10.1007/s10462-023-10654-6},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Moderately supervised learning: Definition, framework and generality},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic program bug fixing by focusing on finding the
shortest sequence of changes. <em>AIR</em>, <em>57</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s10462-023-10686-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic bug repair as the last step in program repair has attracted a lot of research attention. Various ideas and techniques have been presented in this field. Recent bug fixing techniques use machine learning and graphs to generate fixes. Despite the promising results of recent approaches, maintaining high speed and accuracy as well as recording a wide range of errors may still be a problem. In this paper, a new approach is presented in the field of automatic bug fixing based on graphs and model checking. For this purpose, we have used the graph transformation and model checking system to create a sequence of edits and produce fixes. Then, using meta-heuristic algorithms, we have selected the best solution and fix from the generated solutions. We use the extracted graphs from the buggy JavaScript code and their corresponding bug-free ones. In evaluating the effectiveness of the proposed method, we implement it in GROOVE, which is a toolbox used to design and check graph transformation systems. Experimental results on identical dataset demonstrate that the proposed method outperforms other related methods in generating fixes. Also, this method covers a wider range of bugs compared to previous methods.},
  archive      = {J_AIR},
  author       = {Yousofvand, Leila and Soleimani, Seyfollah and Rafe, Vahid and Esfandyari, Sajad},
  doi          = {10.1007/s10462-023-10686-y},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic program bug fixing by focusing on finding the shortest sequence of changes},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring aspect-based sentiment quadruple extraction with
implicit aspects, opinions, and ChatGPT: A comprehensive survey.
<em>AIR</em>, <em>57</em>(2), 1–63. (<a
href="https://doi.org/10.1007/s10462-023-10633-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to earlier ABSA studies primarily concentrating on individual sentiment components, recent research has ventured into more complex ABSA tasks encompassing multiple elements, including pair, triplet, and quadruple sentiment analysis. Quadruple sentiment analysis, also called aspect-category-opinion-sentiment quadruple Extraction (ACOSQE), aims to dissect aspect terms, aspect categories, opinion terms, and sentiment polarities while considering implicit sentiment within sentences. Nonetheless, a comprehensive overview of ACOSQE and its corresponding solutions is currently lacking. This is the precise gap that our survey seeks to address. To be more precise, we systematically reclassify all subtasks of ABSA, reorganizing existing research from the perspective of the involved sentiment elements, with a primary focus on the latest advancements in the ACOSQE task. Regarding solutions, our survey offers a comprehensive summary of the state-of-the-art utilization of language models within the ACOSQE task. Additionally, we explore the application of ChatGPT in sentiment analysis. Finally, we review emerging trends and discuss the challenges, providing insights into potential future directions for ACOSQE within the broader context of ABSA.},
  archive      = {J_AIR},
  author       = {Zhang, Hao and Cheah, Yu-N and Alyasiri, Osamah Mohammed and An, Jieyu},
  doi          = {10.1007/s10462-023-10633-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring aspect-based sentiment quadruple extraction with implicit aspects, opinions, and ChatGPT: A comprehensive survey},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UoloNet: Based on multi-tasking enhanced small target
medical segmentation model. <em>AIR</em>, <em>57</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s10462-023-10671-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, UNET (Ronneberger et al. 2015) and its derivative models have been widely used in medical image segmentation with more superficial structures and excellent segmentation results. Due to the lack of modeling for the overall characteristics of the target, the division tasks of minor marks will produce some discrete noise points, resulting in a decline in model accuracy and application effects. We propose a multi-tasking medical image analysis model UoloNet, a YOLO-based (Redmon et al. 2016; Shafiee et al. 2017) object detection branch is added based on UNET. The shared learning of the two tasks through semantic segmentation and object detection has promoted the model’s mastery of the overall characteristics of the target. In the reasoning stage, merging the two functions of object detection and semantic segmentation can effectively remove discrete noise points in the division and enhance the accuracy of semantic segmentation. In the future, the object detection task will be the problem of excessive convergence of semantic segmentation tasks. The model uses CIOU (Zheng et al. 2020) losses instead of IOU losses in YOLO, which further improves the model’s overall accuracy. The effectiveness of the proposed model is verified both in the MRI dataset SEHPI, which we posted and in the public dataset LITS (Christ 2017).},
  archive      = {J_AIR},
  author       = {Zhang, Kejia and Zhang, Lan and Pan, Haiwei},
  doi          = {10.1007/s10462-023-10671-5},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Artif. Intell. Rev.},
  title        = {UoloNet: Based on multi-tasking enhanced small target medical segmentation model},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on scheduling algorithms using fuzzy
systems in distributed environments. <em>AIR</em>, <em>57</em>(1), 1–72.
(<a href="https://doi.org/10.1007/s10462-023-10632-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task scheduling and resource management are critical for improving system performance and enhancing consumer satisfaction in distributed computing environments. The dynamic nature of tasks and the environments creates new challenges for schedulers. To solve this problem, researchers developed fuzzy-based scheduling algorithms. Fuzzy logic is ideal for decision-making processes since it has a low computational complexity and processing power requirement. Motivated by the extensive research efforts in the distributed computing and fuzzy applications, we present a review of high-quality articles related to fuzzy-based scheduling algorithms in grid, cloud, and fog published between 2005 and June 2023. This paper discusses and compares fuzzy-based scheduling schemes based on merits and demerits, evaluation techniques, simulation environments, and important parameters. We begin by introducing distributed environments, and scheduling process followed by their surveys. This study has summarized several domains where fuzzy logic is used in distributed systems. More specifically, the basic concepts of fuzzy inference system and motivations of fuzzy theory in scheduler are addressed smoothly. A fuzzy-based scheduling algorithm employs fuzzy logic in different ways (e.g., calculating fitness functions, assigning tasks to fog/cloud nodes, and clustering tasks or resources). Finally, open challenges and promising future directions in fuzzy-based scheduling are identified and discussed.},
  archive      = {J_AIR},
  author       = {Jalali Khalil Abadi, Zahra and Mansouri, Najme},
  doi          = {10.1007/s10462-023-10632-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-72},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on scheduling algorithms using fuzzy systems in distributed environments},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning models for digital image processing: A review.
<em>AIR</em>, <em>57</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s10462-023-10631-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the domain of image processing, a wide array of methodologies is dedicated to tasks including denoising, enhancement, segmentation, feature extraction, and classification. These techniques collectively address the challenges and opportunities posed by different aspects of image analysis and manipulation, enabling applications across various fields. Each of these methodologies contributes to refining our understanding of images, extracting essential information, and making informed decisions based on visual data. Traditional image processing methods and Deep Learning (DL) models represent two distinct approaches to tackling image analysis tasks. Traditional methods often rely on handcrafted algorithms and heuristics, involving a series of predefined steps to process images. DL models learn feature representations directly from data, allowing them to automatically extract intricate features that traditional methods might miss. In denoising, techniques like Self2Self NN, Denoising CNNs, DFT-Net, and MPR-CNN stand out, offering reduced noise while grappling with challenges of data augmentation and parameter tuning. Image enhancement, facilitated by approaches such as R2R and LE-net, showcases potential for refining visual quality, though complexities in real-world scenes and authenticity persist. Segmentation techniques, including PSPNet and Mask-RCNN, exhibit precision in object isolation, while handling complexities like overlapping objects and robustness concerns. For feature extraction, methods like CNN and HLF-DIP showcase the role of automated recognition in uncovering image attributes, with trade-offs in interpretability and complexity. Classification techniques span from Residual Networks to CNN-LSTM, spotlighting their potential in precise categorization despite challenges in computational demands and interpretability. This review offers a comprehensive understanding of the strengths and limitations across methodologies, paving the way for informed decisions in practical applications. As the field evolves, addressing challenges like computational resources and robustness remains pivotal in maximizing the potential of image processing techniques.},
  archive      = {J_AIR},
  author       = {Archana, R. and Jeevaraj, P. S. Eliahim},
  doi          = {10.1007/s10462-023-10631-z},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning models for digital image processing: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of deep learning based image
segmentation to detect polyp. <em>AIR</em>, <em>57</em>(1), 1–53. (<a
href="https://doi.org/10.1007/s10462-023-10621-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the world’s most common cancers, colorectal cancer is the third most severe form of cancer. Early polyp detection reduces the risk of colorectal cancer, vital for effective treatment. Artificial intelligence methods such as deep learning have emerged as leading techniques for polyp image segmentation that have gained success in advancing medical image diagnosis. This study aims to provide a review of the most recent research studies that have used deep learning methods and models for polyp segmentation. A comprehensive review of deep learning-based image segmentation models is provided based on existing research studies that are essential for polyp segmentation. Convolutional neural networks, encoder–decoder models, recurrent neural networks, attention-based models, and generative models were the most popular deep learning models which play an essential role in detecting and diagnosing polyp at an early stage. Additionally, this study also aims to provide a detailed classification of prominently used polyp image and video datasets. The evaluation metrics for assessing the effectiveness of different methods, models, and techniques are identified and discussed. A statistical analysis of deep learning models based on polyp datasets and performance metrics is presented, with a discussion of future research trends and limitations.},
  archive      = {J_AIR},
  author       = {Gupta, Mayuri and Mishra, Ashish},
  doi          = {10.1007/s10462-023-10621-1},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of deep learning based image segmentation to detect polyp},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational methods for studying relationship between
nutritional status and respiratory viral diseases: A systematic review.
<em>AIR</em>, <em>57</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10462-023-10627-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study sought to identify the computational methods used in studying the relationship between nutritional status and human respiratory viral infections. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guideline, we searched the PubMed database for studies that used computational approaches to investigate the nutritional determinants of respiratory viral infections. Sixty seven (67) studies were selected after screening an initial 1713 search hits against the study eligibility criteria. Our findings revealed that machine learning, neural network (and deep learning), mathematical models, and statistical methods were used by 83.58%, 26.87%, 13.43% and 92.54% respectively while 19.40% of studies use other unconventional methods. Furthermore, 16.42% and 71.64% studies use formerly created datasets and raw data respectively while 11.94% studies were performed without prior datasets. The outcomes of the selected studies showed that task related to prediction, identification, investigation, association and determination were performed by 20.90%, 8.96%, 40.30%, 10.93% and 14.93% of studies respectively. 1.49% each of the studies performed recommendation, risk reduction, management and screening. The findings of this review answered three research questions and provided guidance for some future contributions in the domain of studies related to nutritional status and respiratory viral disease.},
  archive      = {J_AIR},
  author       = {Hussain, Zakir and Borah, Malaya Dutta and Ahmed, Rezaul Karim},
  doi          = {10.1007/s10462-023-10627-9},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computational methods for studying relationship between nutritional status and respiratory viral diseases: A systematic review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach to fuzzy n-soft sets and its application
for identifying and sanctioning cyber harassment on social media
platforms. <em>AIR</em>, <em>57</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10462-023-10640-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel approach to fuzzy N-soft sets for handling cases where membership degree and grade are not related. In the standard model of fuzzy N-soft sets, membership degree and grade are assumed to be directly proportional. This assumption may not hold true in real-world situations, so a more flexible and nuanced approach is necessary. The proposed approach includes two novel algorithms for decision-making problems involving fuzzy N-soft sets. As a result, it is able to be adaptable and sensitive when addressing uncertainties in real-world scenarios, with a particular focus on identifying and sanctioning cyber harassment on social media platforms. Additionally, an innovative perspective and approach to decision-making problems involving fuzzy N-soft sets is introduced by extending an established selection process that prioritizes the dominant parameter, resulting in more precise and dependable outcomes. Our study offers an effective tool for decision-making in various fields, including e-commerce, social media, and product reviews.},
  archive      = {J_AIR},
  author       = {Korkmaz, Esra and Riaz, Muhammad and Deveci, Muhammet and Kadry, Seifedine},
  doi          = {10.1007/s10462-023-10640-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel approach to fuzzy N-soft sets and its application for identifying and sanctioning cyber harassment on social media platforms},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding rate of return dynamics of cryptocurrencies:
An experimental campaign. <em>AIR</em>, <em>57</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10462-023-10629-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cryptocurrencies have been considered as an asset by public investors and received much research attention. It is a volatile asset, thus predicting its prices is not easy due to the dependence on multiple external factors. Machine learning models are becoming popular for cryptocurrency price predictions, while also considering social media data. In this article, we analyze the rate of return of three cryptocurrencies (Bitcoin, Ether, Binance) from an investor point of view. We also consider three traditional external variables: S&amp;P 500 stock market index, gold price, and volatility index. The rate of return prediction is based on three stages. First, we analyze the correlation between the cryptocurrency returns and the traditional external variables. Next, we focus on the influential social media variables (from Twitter, Reddit, and Wikipedia). Later, we use these variables to improve prediction accuracy. Third, we test how the standard time series models (such as ARIMA and SARIMA) and four machine learning models (such as RNN, LSTM, GRU and Bi-LSTM) predict one-day rate of return. Finally, we also analyze the risk of investing in each cryptocurrencies using value risk statistics. Overall, our result shows no correlation between cryptocurrency returns and three traditional external variables. Second, we found that overall LSTM model is the best, GRU is the second-best prediction model, while the impact of the social media variables varies depending on the cryptocurrencies. Finally, we also found that investment in gold offers better returns than cryptocurrency during Covid-19-like situations.},
  archive      = {J_AIR},
  author       = {Koszewski, Krzysztof and Mazumdar, Somnath and Kumar, Anoop S.},
  doi          = {10.1007/s10462-023-10629-7},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Understanding rate of return dynamics of cryptocurrencies: An experimental campaign},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of artificial intelligence technology in the
field of orthopedics: A narrative review. <em>AIR</em>, <em>57</em>(1),
1–52. (<a href="https://doi.org/10.1007/s10462-023-10638-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) was a new interdiscipline of computer technology, mathematic, cybernetics and determinism. These years, AI had obtained a significant development by the improvement of core technology Machine Learning and Deep Learning. With the assistance of AI, profound changes had been brought into the traditional orthopedics. In this paper, we narratively reviewed the latest applications of AI in orthopedic diseases, including the severity evaluation, triage, diagnosis, treatment and rehabilitation. The research point, relevant advantages and disadvantages of the orthopedic AI was also discussed combined with our own research experiences. We aimed to summarize the past achievements and appeal for more attentions and effective applications of AI in the field of orthopedics.},
  archive      = {J_AIR},
  author       = {Liu, Pengran and Zhang, Jiayao and Liu, Songxiang and Huo, Tongtong and He, Jiajun and Xue, Mingdi and Fang, Ying and Wang, Honglin and Xie, Yi and Xie, Mao and Zhang, Dan and Ye, Zhewei},
  doi          = {10.1007/s10462-023-10638-6},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence technology in the field of orthopedics: A narrative review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable scientific discovery with symbolic regression:
A review. <em>AIR</em>, <em>57</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s10462-023-10622-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery tool, achieving significant advances in various application domains ranging from fundamental to applied sciences. In this survey, we present a structured and comprehensive overview of symbolic regression methods, review the adoption of these methods for model discovery in various areas, and assess their effectiveness. We have also grouped state-of-the-art symbolic regression applications in a categorized manner in a living review.},
  archive      = {J_AIR},
  author       = {Makke, Nour and Chawla, Sanjay},
  doi          = {10.1007/s10462-023-10622-0},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Interpretable scientific discovery with symbolic regression: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bartletts principal regressive and arbitrary african buffalo
optimizatized three-dimensional protein structure prediction.
<em>AIR</em>, <em>57</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10462-023-10634-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein sequencing and structure prediction is analytic to farther predict the tertiary structure, realize protein function and design drugs. However, experimental techniques are used to determine the structure of proteins which are time-consuming and expensive, and thus it&#39;s very intense to design significant computational methods for predicting protein structure based on optimization techniques. Existing deep learning-based methods have accomplished exceptional accomplishments of protein structure prediction, but the methods often utilize the features from prior knowledge with less focus on the error rate. To address this issue in this work, a Bartlett’s Principal Regressive and Arbitrary African Buffalo Optimization (BPR-AABO) for performing secondary Protein Structure Prediction is proposed. The BPR-AABO method has one input layer, three hidden layers and output layer for protein structure prediction with higher accuracy and minimal time consumption. In BPR-AABO method, protein data is considered as an input and transmitted to the input layer. Relevant features are extracted from input protein data in hidden layer 1 by applying Bartlett’s specificity test. The extracted features are transmitted to the hidden layer 2 where Principal Component Regression Analysis is applied with the chosen features for protein structure identification. Then, protein structure identification results are transmitted to the hidden layer 3. In that layer, Improved African Buffalo Optimization Model with sigmoid activation function is used for positioning the amino acids to form the protein structure and therefore performing protein structure prediction with higher accuracy and lesser time consumption. Experimental evaluation is carried out on factors such as, prediction accuracy, prediction time and ROC with respect to number of protein data.},
  archive      = {J_AIR},
  author       = {Nallasamy, Varanavasi and Seshiah, Malarvizhi},
  doi          = {10.1007/s10462-023-10634-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bartletts principal regressive and arbitrary african buffalo optimizatized three-dimensional protein structure prediction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acoustic-based LEGO recognition using attention-based
convolutional neural networks. <em>AIR</em>, <em>57</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s10462-023-10625-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the classification of LEGO types using deep learning-based audio classification approaches. The motivation for this investigation is based on the following assumption. If objects of the same shape fall freely from a certain height and hit a fixed plane, the impact sounds will be very similar, so we can distinguish the same types of objects from the others. Applying this idea to LEGO recognition, we collect impact sounds of 200 LEGO objects that fall from a height of about 30cm from a designated plane, and design a CNN-based recognition system that processes the impact sounds to determine the type of LEGO it belongs to. Recognizing that the fall of LEGO results in the main impact sound (i.e., only the sound at the moment of impact) and several subsequent sounds, we examine whether considering only the first impact sound or all sounds brings about better classification accuracies. We propose a compact two-dimensional CNN model, namely LegoNet, which is designed with a frame-level attention module at the input spectrogram and time-distributed fully-connected layers. Our experiments show that free-fall impact sounds can be used efficiently for accurate object recognition, and the proposed LegoNet, with a much smaller size, achieves better accuracy and robustness compared to baseline models. Also, using the whole sequence of impact sounds is more informative for LEGO classification than only considering the first impact sound. Moreover, it is found that utilizing data of specific object postures can help to improve the classifier’s performance in the case of small training data. The proposed approach can be employed as an extra module to build intelligent agents or object classification systems that require a rich understanding of the surrounding physical world.},
  archive      = {J_AIR},
  author       = {Tran, Van-Thuan and Wu, Chia-Yang and Tsai, Wei-Ho},
  doi          = {10.1007/s10462-023-10625-x},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Acoustic-based LEGO recognition using attention-based convolutional neural networks},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Deep reinforcement learning-based air combat maneuver
decision-making: Literature review, implementation tutorial and future
direction. <em>AIR</em>, <em>57</em>(1), 1–39. (<a
href="https://doi.org/10.1007/s10462-023-10620-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, various innovative air combat paradigms that rely on unmanned aerial vehicles (UAVs), i.e., UAV swarm and UAV-manned aircraft cooperation, have received great attention worldwide. During the operation, UAVs are expected to perform agile and safe maneuvers according to the dynamic mission requirement and complicated battlefield environment. Deep reinforcement learning (DRL), which is suitable for sequential decision-making process, provides a powerful solution tool for air combat maneuver decision-making (ACMD), and hundreds of related research papers have been published in the last five years. However, as an emerging topic, there lacks a systematic review and tutorial. For this reason, this paper first provides a comprehensive literature review to help people grasp a whole picture of this field. It starts from the DRL itself and then extents to its application in ACMD. And special attentions are given to the design of reward function, which is the core of DRL-based ACMD. Then, a maneuver decision-making method based on one-to-one dogfight scenarios is proposed to enable UAV to win short-range air combat. The model establishment, program design, training methods and performance evaluation are described in detail. And the associated Python codes are available at gitee.com/wangyyhhh, thus enabling a quick-start for researchers to build their own ACMD applications by slight modifications. Finally, limitations of the considered model, as well as the possible future research direction for intelligent air combat, are also discussed.},
  archive      = {J_AIR},
  author       = {Wang, Xinwei and Wang, Yihui and Su, Xichao and Wang, Lei and Lu, Chen and Peng, Haijun and Liu, Jie},
  doi          = {10.1007/s10462-023-10620-2},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep reinforcement learning-based air combat maneuver decision-making: Literature review, implementation tutorial and future direction},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient approach to attribute reductions of
quantitative dominance-based neighborhood rough sets based on graded
information granules. <em>AIR</em>, <em>57</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10462-023-10639-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower approximations of quantitative dominance-based neighborhood rough sets aim to enhance the consistency of dominance principles by filtering out pairs of objects that do not meet a predefined threshold. In this paper, we propose a novel approach to reflect dominance principles intuitively by defining generalized decisions based on certain decision rules in quantitative dominance-based neighborhood rough sets. Building upon this framework, we construct upward and downward graded information granules that enable the partitioning of the universe. We analyze the properties of the graded information granules and investigate their relationship with approximating qualities. Furthermore, we introduce the concept of importance degree to quantify the uncertainties of graded information granules under different attributes, which exhibits a monotonic behavior with respect to attributes. Subsequently, we design an attribute reduction method and explore an accelerated process by updating the generalized decisions. Finally, we conduct experiments on several public datasets to evaluate the efficiency of our methodology in terms of attribute reductions. The superiority of our proposed method on running time is illustrated by statistical hypothesis with paired t-test. Also the precision accuracy of reduct set is evaluated by rough sets and machine learning. Additionally, we demonstrate how the structures of graded information granules can be revealed by varying the parameters.},
  archive      = {J_AIR},
  author       = {Yang, Shuyun and Shi, Guang},
  doi          = {10.1007/s10462-023-10639-5},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient approach to attribute reductions of quantitative dominance-based neighborhood rough sets based on graded information granules},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The key artificial intelligence technologies in early
childhood education: A review. <em>AIR</em>, <em>57</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10462-023-10637-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) technologies have been applied in various domains, including early childhood education (ECE). Integration of AI educational technology is a recent significant trend in ECE. Currently, there are more and more studies of AI in ECE. To date, there is a lack of survey articles that discuss the studies of AI in ECE. In this paper, we provide an up-to-date and in-depth overview of the key AI technologies in ECE that provides a historical perspective, summarizes the representative works, outlines open questions, discuss the trends and challenges through a detailed bibliometric analysis, and provides insightful recommendations for future research. We mainly discuss the studies that apply AI-based robots and AI technologies to ECE, including improving the social interaction of children with an autism spectrum disorder. This paper significantly contributes to provide an up-to-date and in-depth survey that is suitable as introductory material for beginners to AI in ECE, as well as supplementary material for advanced users.},
  archive      = {J_AIR},
  author       = {Yi, Honghu and Liu, Ting and Lan, Gongjin},
  doi          = {10.1007/s10462-023-10637-7},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The key artificial intelligence technologies in early childhood education: A review},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature subset selection for multi-scale neighborhood
decision information system via mutual information. <em>AIR</em>,
<em>57</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10462-023-10626-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a granular computing model, multi-scale data analysis has attracted considerable attention in last several years. However, most of multi-scale models are hardly to deal with multi-source data, especially in heterogeneous environments. For this reason, we investigate a novel multi-scale model by combining weighted neighborhood rough sets with Wu-Leung model for the first time, and apply it in feature selection for multi-source heterogeneous multi-scale data. First, the multi-scale weighted neighborhood granules obtained and their properties are discussed. Second, the mutual information of multi-source heterogeneous multi-scale features is presented. Based on this, the definition of the redundancy of the features is obtained and a feature subset selection algorithm that simultaneously performs the selection of features and the optimal scale combination is given. Finally, numerical experiments on multi-source heterogeneous multi-scale datasets and heterogeneous multi-scale datasets are conducted to examine the effectiveness and feasibility of the proposed model. The experiments demonstrate that the proposed model can obtain better results on both datasets.},
  archive      = {J_AIR},
  author       = {Zhang, Lujing and Lin, Guoping and Wei, Ling and Kou, Yi},
  doi          = {10.1007/s10462-023-10626-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Feature subset selection for multi-scale neighborhood decision information system via mutual information},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State-of-the-art review of machine learning and optimization
algorithms applications in environmental effects of blasting.
<em>AIR</em>, <em>57</em>(1), 1–54. (<a
href="https://doi.org/10.1007/s10462-023-10636-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technological difficulties related with blasting operations have become increasingly significant. It is crucial to give due consideration to the evaluation of rock fragmentation and the threats posed by environmental effect of blasting (EEB). To address these challenges, numerous scholars have conducted extensive research employing various assessment techniques with the aim of mitigating risks and preventing the emergence of unfavorable EEB. The occurrence of EEB is prevalent during the excavation of hard rock, and it presents significant hazards to personnel safety, equipment integrity, and operational continuity. Therefore, conducting a systematic review of EEB is of utmost importance as it enables a comprehensive understanding of the contributing factors. Such an understanding plays a vital role in advancing EEB prediction and prevention methods. The careful selection of an appropriate EEB assessment method is a crucial aspect of blasting operations. However, there is a lack of comprehensive discussions on the applications of machine learning (ML) and optimization algorithms (OA) in addressing various EEB. Only a limited number of papers have briefly touched upon this topic. Therefore, the primary objective of this paper is to bridge this gap by conducting an analysis of global trends using CiteSpace and VOSviewer software from the year 2000 onwards. It comprehensively explores EEB classification and definition, encompassing air overpressure (AOp), ground vibration, dust, backbreak, flyrock, and rock fragmentation. Furthermore, the paper provides a compendium of the most recent ML and OA prediction techniques used to addresses EEB. Finally, the paper concludes by proposing future directions for exploring innovative approaches that combine data-driven ML techniques with knowledge-based or physics-based methods. Such integration has the potential to mitigate hazards during blasting operations and reduce the likelihood of unfavorable EEB occurrences.},
  archive      = {J_AIR},
  author       = {Zhou, Jian and Zhang, Yulin and Qiu, Yingui},
  doi          = {10.1007/s10462-023-10636-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {State-of-the-art review of machine learning and optimization algorithms applications in environmental effects of blasting},
  volume       = {57},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
