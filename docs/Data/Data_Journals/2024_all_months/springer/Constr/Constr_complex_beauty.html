<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Constr_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="constr---11">Constr - 11</h2>
<ul>
<li><details>
<summary>
(2024). Invariant polydiagonal subspaces of matrices and constraint
programming. <em>Constr</em>, <em>29</em>(3), 300–314. (<a
href="https://doi.org/10.1007/s10601-024-09378-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a polydiagonal subspace of the Euclidean space, certain components of the vectors are equal (synchrony) or opposite (anti-synchrony). Polydiagonal subspaces invariant under a matrix have many applications in graph theory and dynamical systems, especially coupled cell networks. We describe invariant polydiagonal subspaces in terms of coloring vectors. This approach gives an easy formulation of a constraint satisfaction problem for finding invariant polydiagonal subspaces. Solving the resulting problem with existing state-of-the-art constraint solvers greatly outperforms the currently known algorithms.},
  archive      = {J_Constr},
  author       = {Neuberger, John M. and Sieben, Nándor and Swift, James W.},
  doi          = {10.1007/s10601-024-09378-3},
  journal      = {Constraints},
  month        = {12},
  number       = {3},
  pages        = {300-314},
  shortjournal = {Constraints},
  title        = {Invariant polydiagonal subspaces of matrices and constraint programming},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ner4Opt: Named entity recognition for optimization modelling
from natural language. <em>Constr</em>, <em>29</em>(3), 261–299. (<a
href="https://doi.org/10.1007/s10601-024-09376-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving combinatorial optimization problems involves a two-stage process that follows the model-and-run approach. First, a user is responsible for formulating the problem at hand as an optimization model, and then, given the model, a solver is responsible for finding the solution. While optimization technology has enjoyed tremendous theoretical and practical advances, the process has remained unchanged for decades. To date, transforming problem descriptions into optimization models remains a barrier to entry. To alleviate users from the cognitive task of modeling, we study named entity recognition to capture components of optimization models such as the objective, variables, and constraints from free-form natural language text, and coin this problem as Ner4Opt. We show how to solve Ner4Opt using classical techniques based on morphological and grammatical properties and modern methods leveraging pre-trained large language models and fine-tuning transformers architecture with optimization-specific corpora. For best performance, we present their hybridization combined with feature engineering and data augmentation to exploit the language of optimization problems. We improve over the state-of-the-art for annotated linear programming word problems. Large-language models (LLMs) are not yet versatile enough to turn text into optimization models or extract optimization entities. Still, when augmented with Ner4Opt annotations, the compilation accuracy of LLM-generated models improves significantly. We open-source our Ner4Opt library, release our training and fine-tuning procedures, and share our trained artifacts. We identify several next steps and discuss important open problems toward automated modeling.},
  archive      = {J_Constr},
  author       = {Kadıoğlu, Serdar and Pravin Dakle, Parag and Uppuluri, Karthik and Politi, Regina and Raghavan, Preethi and Rallabandi, SaiKrishna and Srinivasamurthy, Ravisutha},
  doi          = {10.1007/s10601-024-09376-5},
  journal      = {Constraints},
  month        = {12},
  number       = {3},
  pages        = {261-299},
  shortjournal = {Constraints},
  title        = {Ner4Opt: Named entity recognition for optimization modelling from natural language},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning and fine-tuning a generic value-selection heuristic
inside a constraint programming solver. <em>Constr</em>, <em>29</em>(3),
234–260. (<a href="https://doi.org/10.1007/s10601-024-09377-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint programming is known for being an efficient approach to solving combinatorial problems. Important design choices in a solver are the branching heuristics, designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. Although several generic variable-selection heuristics are available in the literature, the options for value-selection heuristics are more scarce. We propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a tailored reward signal, and a heterogeneous graph neural network. Experiments on graph coloring, maximum independent set, maximum cut, and minimum vertex cover problems show that this framework competes with the well-known impact-based and activity-based search heuristics and can find solutions close to optimality without requiring a large number of backtracks. Additionally, we observe that fine-tuning a model with a different problem class can accelerate the learning process.},
  archive      = {J_Constr},
  author       = {Marty, Tom and Boisvert, Léo and François, Tristan and Tessier, Pierre and Gautier, Louis and Rousseau, Louis-Martin and Cappart, Quentin},
  doi          = {10.1007/s10601-024-09377-4},
  journal      = {Constraints},
  month        = {12},
  number       = {3},
  pages        = {234-260},
  shortjournal = {Constraints},
  title        = {Learning and fine-tuning a generic value-selection heuristic inside a constraint programming solver},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing aperiodic tiling rhythmic canons via SAT models.
<em>Constr</em>, <em>29</em>(3), 215–233. (<a
href="https://doi.org/10.1007/s10601-024-09375-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mathematical Music theory, the Aperiodic Tiling Complements Problem consists in finding all the possible aperiodic complements of a given rhythm A. The complexity of this problem depends on the size of the period n of the canon and on the cardinality of the given rhythm A. The current state-of-the-art algorithms can solve instances with n smaller than $$\varvec{180}$$ . In this paper, we propose an ILP formulation and a SAT Encoding to solve this mathemusical problem, and we use the Maplesat solver to enumerate all the aperiodic complements. We then enhance the SAT model in two different ways. First, we enforce the SAT model with a set of clauses that retrieves the solutions up to translation. Second, we propose a decomposition of the solution space that allows to parallelize the resolution of the problem. We validate our different models using several different periods and rhythms and we compute for the first time the complete list of aperiodic tiling complements of standard Vuza rhythms for canons with period $$\varvec{n} = \varvec{\left\{ 180, 420, 900 \right\} }$$ .},
  archive      = {J_Constr},
  author       = {Auricchio, Gennaro and Ferrarini, Luca and Gualandi, Stefano and Lanzarotto, Greta and Pernazza, Ludovico},
  doi          = {10.1007/s10601-024-09375-6},
  journal      = {Constraints},
  month        = {12},
  number       = {3},
  pages        = {215-233},
  shortjournal = {Constraints},
  title        = {Computing aperiodic tiling rhythmic canons via SAT models},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constraint propagation on GPU: A case study for the
cumulative constraint. <em>Constr</em>, <em>29</em>(1), 192–214. (<a
href="https://doi.org/10.1007/s10601-024-09371-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cumulative constraint is a foundamental global constraint, which naturally arises in a variety of problems related to scheduling with limited resources. Since its introduction, numerous propagation algorithms have been proposed, offering different tradeoffs between computational complexity and filtering power. Such diversity allows the resolution of a wide range of applications. Motivated by the impressive computational power that modern Graphical Processing Units (GPUs) provide, this paper explores the use of GPUs for the propagation of the Cumulative constraint. The paper describes the development of a GPU-Acceletated Propagator (GAP), motivates the design choices, and provides solutions for several design challenges. The implementation is evaluated in comparison with state-of-the-art constraint solvers on different benchmarks from the literature. The results suggest that our approach is competitive, providing strong filtering in a reasonable amount of time.},
  archive      = {J_Constr},
  author       = {Tardivo, Fabio and Dovier, Agostino and Formisano, Andrea and Michel, Laurent and Pontelli, Enrico},
  doi          = {10.1007/s10601-024-09371-w},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {192-214},
  shortjournal = {Constraints},
  title        = {Constraint propagation on GPU: A case study for the cumulative constraint},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Limiting the memory consumption of caching for detecting
subproblem dominance in constraint problems. <em>Constr</em>,
<em>29</em>(1), 152–191. (<a
href="https://doi.org/10.1007/s10601-024-09374-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving constraint satisfaction problems often involves a large amount of redundant exploration stemming from the existence of subproblems whose information can be reused for other subproblems. Subproblem dominance is a general notion of reusability that arises when one subproblem imposes more constraints on the remaining part of the search than another subproblem and allows the search to reuse the solutions of the dominating subproblem for the dominated subproblem. The search can exploit subproblem dominance by storing the subproblems that have been explored in a cache and abandoning the current subproblem whenever the cache contains a subproblem that dominates it. While using caching makes it possible to solve problems where subproblem dominance arises orders of magnitude faster, storing all of these subproblems can require a substantial amount of memory, making it impractical in many cases. This paper analyses the dominance between different subproblems for various constraint problems, revealing that only a relatively small number of subproblems dominate other subproblems. Based on these findings, two types of strategies are proposed for reducing the number of subproblems stored in the cache: limiting the number of subproblems that can be stored in the cache and periodically cleaning up the cache. An experimental evaluation demonstrates that these strategies provide an effective instrument for reducing the memory consumption of caching, allowing it to be used on a larger scale. However, there is a trade-off between saving memory and reducing redundant exploration, as removing subproblems from the cache may prevent dominance from being detected for certain subproblems.},
  archive      = {J_Constr},
  author       = {Medema, Michel and Breeman, Luc and Lazovik, Alexander},
  doi          = {10.1007/s10601-024-09374-7},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {152-191},
  shortjournal = {Constraints},
  title        = {Limiting the memory consumption of caching for detecting subproblem dominance in constraint problems},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception-based constraint solving for sudoku images.
<em>Constr</em>, <em>29</em>(1), 112–151. (<a
href="https://doi.org/10.1007/s10601-024-09372-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of perception-based constraint solving, where part of the problem specification is provided indirectly through an image provided by a user. As a pedagogical example, we use the complete image of a Sudoku grid. While the rules of the puzzle are assumed to be known, the image must be interpreted by a neural network to extract the values in the grid. In this paper, we investigate (1) a hybrid modeling approach combining machine learning and constraint solving for joint inference, knowing that blank cells need to be both predicted as being blank and filled-in to obtain a full solution; (2) the effect of classifier calibration on joint inference; and (3) how to deal with cases where the constraints of the reasoning system are not satisfied. More specifically, in the case of handwritten user errors in the image, a naive approach fails to obtain a feasible solution even if the interpretation is correct. Our framework identifies human mistakes by using a constraint solver and helps the user to correct these mistakes. We evaluate the performance of the proposed techniques on images taken through the Sudoku Assistant Android app, among other datasets. Our experiments show that (1) joint inference can correct classifier mistakes, (2) overall calibration improves the solution quality on all datasets, and (3) estimating and discriminating between user-written and original visual input while reasoning makes for a more robust system, even in the presence of user errors.},
  archive      = {J_Constr},
  author       = {Mulamba, Maxime and Mandi, Jayanta and Mahmutoğulları, Ali İrfan and Guns, Tias},
  doi          = {10.1007/s10601-024-09372-9},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {112-151},
  shortjournal = {Constraints},
  title        = {Perception-based constraint solving for sudoku images},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining diverse sets of patterns with constraint programming
using the pairwise jaccard similarity relaxation. <em>Constr</em>,
<em>29</em>(1), 80–111. (<a
href="https://doi.org/10.1007/s10601-024-09373-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, pattern mining has evolved from a slow-moving, repetitive three-step process to a much more agile and iterative/user-centric mining model. A crucial element of this framework is the capability to rapidly provide a set of diverse patterns to the user. This paper proposes a pattern mining approach based on constraint programming that incorporates a non-redundancy/diversity constraint into closed pattern enumeration. The level of diversity is controlled through a threshold on the maximum pairwise Jaccard similarity of pattern occurrences. We show that the Jaccard measure does not have nice (anti-)monotonicity properties w.r.t. the general-to-specific enumeration. To address this limitation, we propose anti-monotonic lower and upper-bound relaxations of the Jaccard similarity with nice pruning-enabling properties, and connect the final results to the original Jaccard Index. To evaluate the effectiveness of our relaxations, we conduct a comprehensive comparison against several existing pattern mining techniques designed to control redundancy. Experimental results illustrate that our approach provides an effective solution for mining diverse itemsets, showing competitive performance in both runtime and flexibility.},
  archive      = {J_Constr},
  author       = {Hien, Arnold and Aribi, Noureddine and Loudni, Samir and Lebbah, Yahia and Ouali, Abdelkader and Zimmermann, Albrecht},
  doi          = {10.1007/s10601-024-09373-8},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {80-111},
  shortjournal = {Constraints},
  title        = {Mining diverse sets of patterns with constraint programming using the pairwise jaccard similarity relaxation},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Plotting: A case study in lifted planning with constraints.
<em>Constr</em>, <em>29</em>(1), 40–79. (<a
href="https://doi.org/10.1007/s10601-024-09370-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a planning problem based on Plotting, a tile-matching puzzle video game published by Taito in 1989. The objective of this turn-based game is to remove a target number of coloured blocks from a grid by sequentially shooting blocks into the same grid. Plotting features complex transitions after every shot: various blocks are affected directly, while others can be indirectly affected by gravity. We consider modelling and solving Plotting from two perspectives. The puzzle is naturally cast as an AI Planning problem and we first discuss modelling the problem using the Planning Domain Definition Language (PDDL). We find that a model in which planning actions correspond to player actions is inefficient with a grounding-based state-of-the-art planner. However, with a more fine-grained action model, where each change of a block is a planning action, solving performance is dramatically improved. We also describe two lifted constraint models, able to capture the inherent complexities of Plotting and enabling the application of efficient solving approaches from SAT and CP. Our empirical results with these models demonstrates that they can compete with, and often exceed, the performance of the dedicated planning solvers, suggesting that the richer languages available to constraint modelling can be of benefit when considering planning problems with complex changes of state. CP and SAT solvers solved almost all of the largest and most challenging instances within 1 hour, whereas the best planning approach solved approximately 30%. Finally, the flexibility provided by the constraint models allows us to easily curate interesting levels for human players.},
  archive      = {J_Constr},
  author       = {Espasa, Joan and Miguel, Ian and Nightingale, Peter and Salamon, András Z. and Villaret, Mateu},
  doi          = {10.1007/s10601-024-09370-x},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {40-79},
  shortjournal = {Constraints},
  title        = {Plotting: A case study in lifted planning with constraints},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counting QBF solutions at level two. <em>Constr</em>,
<em>29</em>(1), 22–39. (<a
href="https://doi.org/10.1007/s10601-024-09369-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We lift the problem of enumerative solution counting to quantified Boolean formulas (QBFs) at the second quantifier block. In contrast to the well-explored model counting problem for SAT (#SAT), where models are simply assignments to the Boolean variables of a formula, we are now dealing with tree (counter-)models reflecting the dependencies between the variables of the first and the second quantifier block. It turns out that enumerative counting on the second level does not give the complete solution count and more fine-grained view is necessary. We present a level-2 solution counting approach that works for true and false formulas. We implemented the presented approach in a counting tool exploiting state-of-the-art QBF solving technology. We present several kinds of benchmarks for testing our implementation and show that even with this very basic approach of solution enumeration the solution counts of challenging benchmarks can be found.},
  archive      = {J_Constr},
  author       = {Plank, Andreas and Möhle, Sibylle and Seidl, Martina},
  doi          = {10.1007/s10601-024-09369-4},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {22-39},
  shortjournal = {Constraints},
  title        = {Counting QBF solutions at level two},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying constraint programming to minimal lottery designs.
<em>Constr</em>, <em>29</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10601-024-09368-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and deploy a set of constraints for the purpose of calculating minimal sizes of lottery designs. Specifically, we find the minimum number of tickets of size six which are needed to match at least two balls on any draw of size six, whenever there are at most 70 balls.},
  archive      = {J_Constr},
  author       = {Cushing, David and Stewart, David I.},
  doi          = {10.1007/s10601-024-09368-5},
  journal      = {Constraints},
  month        = {6},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Constraints},
  title        = {Applying constraint programming to minimal lottery designs},
  volume       = {29},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
