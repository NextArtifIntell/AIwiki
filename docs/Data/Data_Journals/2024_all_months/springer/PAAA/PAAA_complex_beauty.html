<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PAAA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="paaa---155">PAAA - 155</h2>
<ul>
<li><details>
<summary>
(2024). A novel residual fourier convolution model for brain tumor
segmentation of mr images. <em>PAAA</em>, <em>27</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s10044-024-01312-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging is an essential tool for the early diagnosis of brain tumors. However, it is challenging for the segmentation of the brain tumor of magnetic resonance images due to the most severe problem of blurred boundaries and variable spatial structure. Therefore, combining multiple brain datasets, a novel residual Fourier convolution model with local interpretability is presented to address mentioned above problem in this study. Firstly, an interpretable residual Fourier convolution encoder is constructed by the Fourier transform and its inverse for fast extraction of the spectral features of the brain tumor regions. Furthermore, the dilated-gated attention mechanism is designed to expand the receptive fields and extract blurred irregular boundary features that are closer to the lesion regions. Finally, the encoder-decoder spatial attention fusion mechanism is developed to further extract more fine-grained contextual spatial features from the variable spatial structure of adjacent magnetic resonance slices. Compared to other advanced models, our proposed model has achieved state-of-the-art average segmentation performance by testing on the BraTS2019, Figshare, and TCIA datasets. The average Dice coefficient, sensitivity, MIoU, and PPV respectively reach to 0.892, 87.1%, 0.843, and 91.5%. The proposed segmentation framework can provide more reliable segmentation results for the early diagnosis of brain tumors because of its robust feature extraction ability, interpretability, and generalization ability.},
  archive      = {J_PAAA},
  author       = {Zhu, Haipeng and He, Hong},
  doi          = {10.1007/s10044-024-01312-w},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel residual fourier convolution model for brain tumor segmentation of mr images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on decoupled adaptive graph convolution networks
based on skeleton data for action recognition. <em>PAAA</em>,
<em>27</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01319-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network is apt for feature extraction in terms of non-Euclidian human skeleton data, but its adjacency matrix is fixed and the receptive field is small, which results in bias representation for skeleton intrinsic information. In addition, the operation of mean pooling on spatio-temporal features in classification layer will result in losing information and degrade recognition accuracy. To this end, the Decoupled Adaptive Graph Convolutional Network (DAGCN) is proposed. Specifically, a multi-level adaptive adjacency matrix is designed, which can dynamically obtain the rich correlation information among the skeleton nodes by a non-local adaptive algorithm. Whereafter, a new Residual Multi-scale Temporal Convolution Network (RMTCN) is proposed to fully extract temporal feature of the above decoupled skeleton dada. For the second problem in classification, we decompose the spatio-temporal features into three parts as spatial, temporal, spatio-temporal information, they are averagely pooled respectively, and added together for classification, denoted as STMP (spatio-temporal mean pooling) module. Experimental results show that our algorithm achieves accuracy of 96.5%, 90.6%, 96.4% on NTU-RGB+D60, NTU-RGB+D120 and NW-UCLA data sets respectively.},
  archive      = {J_PAAA},
  author       = {Deng, Haigang and Lin, Guocheng and Li, Chengwei and Wang, Chuanxu},
  doi          = {10.1007/s10044-024-01319-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Research on decoupled adaptive graph convolution networks based on skeleton data for action recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UR-YOLO: An urban road small object detection algorithm.
<em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01324-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The autonomous driving system heavily depends on perception algorithms to gather crucial information about the surrounding urban environment. However, detecting small objects on busy urban roads poses a significant challenge. To overcome this obstacle, we present UR-YOLO (Urban Roads-YOLO), a novel small object detection algorithm tailored for urban roads, which builds upon the enhanced YOLOv9 framework. UR-YOLO comprises three key enhancements. Firstly, to mitigate the high background ratio in small object datasets, we employ SCRConv to replace selected standard convolutions in the backbone network. The reduction in spatial redundancy sharpens the perception of vital features. Secondly, to address the sparse distribution of small objects, we incorporate SPPELANBRA, a refined version of SPPELAN, to enhance the model’s sensitivity towards small objects, thereby improving its overall accuracy. Lastly, to address the issue of overlapping small objects, we upgrade the bounding box loss function by substituting the original SIoU loss with the Inner-MPDIoU loss. It not only improves the detection accuracy for small objects but also accelerates the convergence of the training process. To validate the effectiveness of UR-YOLO, we conducted comprehensive ablation and comparative experiments on the 2023 CICVAC dataset. The experimental results reveal that our proposed improvements have boosted the YOLOv9 model’s mAP, precision, and recall by significant margins of 6.02%, 6.63%, and 4.81% respectively. Furthermore, when compared to prior YOLO series and two-stage detection models, UR-YOLO exhibits superior accuracy, higher frames per second, and greater robustness, making it a robust solution for diverse weather conditions on urban roads. Code is available at https://github.com/Ranghao/UR_YOLO .},
  archive      = {J_PAAA},
  author       = {Wang, Juan and Yang, Hao and Wu, Minghu and Wang, Sheng and Cao, Ye and Hu, Shuyao and Shao, Jixiang and Zeng, Chunyan},
  doi          = {10.1007/s10044-024-01324-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {UR-YOLO: An urban road small object detection algorithm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational inference based adversarial domain adaptation.
<em>PAAA</em>, <em>27</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01325-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods demand enormous amounts of labeled data. Although collecting labeled data is a challenge but it can be accomplished with difficulty. Nevertheless, domain shift or bias may occur due to some conditions. Recollecting data under similar conditions is too expensive or impossible. Domain adaptation is an effective technique to deal with this problem. Among domain adaptation methods, adversarial learning models handled domain shifts well. Adversarial learning’s biggest issue is matching the features to the appropriate classes. In other words, domain adaptation may lead to negative transfer. Utilizing adversarial learning and Variational Auto-Encoder (VAE) cluster-forming capabilities, we propose a method that overcomes these limitations. Our structure attempts to generate a smooth latent representation based on both the target and the source data using a variational auto-encoder. Then the affected data are fed into an adversarial learning component. The component includes an encoder and a discriminator. The discriminator aims to identify the source and target encoders’ features. The target encoder uses inverted domain labels to mislead the discriminator. To emphasize the desired effect of VAE on domain adaptation, we test the model performance without adversarial training. In comparison to the most prevalent adversarial domain adaptation benchmarks, our method yields approving and comparable results.},
  archive      = {J_PAAA},
  author       = {Zonoozi, Mahta Hassan Pour and Seydi, Vahid and Deypir, Mahmood},
  doi          = {10.1007/s10044-024-01325-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Variational inference based adversarial domain adaptation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ShuffleNeMt: Modern lightweight convolutional neural network
architecture. <em>PAAA</em>, <em>27</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01327-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight convolutional neural networks (CNNs) are specifically designed for mobile devices and embedded systems, being aimed at achieving lower resource and faster inference speed. However, these networks are limited in their ability to capture long-range dependencies due to the local nature of convolutional operations. The introduction of self-attention into these CNNs can effectively capture global information, but it comes at the cost of significantly slower inference speed. To solve these problems, we propose a novel lightweight network called as ShuffleNeMt. Our ShuffleNeMt involves modernizing ShuffleNetV2 by incorporating several strategies, including pre-norm residual learning, scaling the residual depth, visual self-attention and non-monotonic activation functions. The visual self-attention is used for only one layer and positioned in the middle of the network. Using this way, ShuffleNeMt not only can achieve efficient resource utilization and fast inference speed, but also can capture long-range spatial dependencies. Extensive experimental results demonstrate the superiority of ShuffleNeMt over the existing lightweight architectures. E.g., On the CIFAR-100 image classification dataset, ShuffleNeMt-1.5 achieved top-1 error rate of $$34.05\%$$ using 2.588M parameters, which is better than the top-1 error rate of $$36.86\%$$ achieved by MobileNetV3-Large-0.8 using 2.809M parameters.},
  archive      = {J_PAAA},
  author       = {Zhu, Meng and Min, Weidong and Han, Qing and Zhan, Guowei and Fu, Qiyan and Li, Jiahao},
  doi          = {10.1007/s10044-024-01327-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ShuffleNeMt: Modern lightweight convolutional neural network architecture},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal SiamFC: Per-clip visual tracking with
siamese non-local 3D convolutional networks and multi-template updating.
<em>PAAA</em>, <em>27</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01328-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese network based approaches show promising results on visual object tracking. These methods typically handle the tracking task by per-frame object detection and thus fail to fully exploit the rich temporal contexts among successive frames, which are important for accurate and robust object tracking. To benefit from the temporal information, in this paper, we investigate a per-clip tracking scheme in the Siamese-based approach and present a novel spatio-temporal SiamFC method for high-performance visual tracking. More specifically, we incorporate a non-local 3D fully convolutional network into a Siamese framework, which allows the model to act directly on the inputs of multiple templates and search video clips and to extract features from both spatial and temporal dimensions, thereby capturing the temporal information encoded in multiple video frames. We then propose a multi-template matching module to learn a representative tracking model using spatio-temporal template features and propagate informative target cues from the template set to the search clip using attention, which facilitate the object searching in clips. During inference, we employ a confident search region cropping and a dynamic multi-template update mechanism for stable and robust per-clip tracking. Experiments on six benchmark datasets show that our spatio-temporal SiamFC achieves competitive performance compared to state-of-the-art while running at approximatively 60 FPS on GPU. Codes are available at https://github.com/liangminstu/STSiamFC .},
  archive      = {J_PAAA},
  author       = {Gui, Yan and Ou, Yiru and Liang, Min and Zhang, Jianming and Chen, Zhihua},
  doi          = {10.1007/s10044-024-01328-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Spatio-temporal SiamFC: Per-clip visual tracking with siamese non-local 3D convolutional networks and multi-template updating},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDC-PP-YOLOE: A lightweight model for detecting and counting
citrus fruit. <em>PAAA</em>, <em>27</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01329-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the citrus orchard environment, accurate counting of the fruit, and the use of lightweight detection methods are the key presteps to automate citrus picking and yield estimations. Most high-precision fruit detection models based on deep learning use complex models with devices that require high quantities of computational resources and memory. Devices with limited resources cannot meet the requirements of these models. Thus, to overcome this problem, we focus on creating a lightweight model with a convolutional neural network. In this research, we propose a lightweight citrus detection model based on the mobile device LDC-PP-YOLOE. LDC-PP-YOLOE is improved based on PP-YOLOE by using localized knowledge distillation and CBAM, with a mAP@0.5 of 88 $$\%$$ , mAP@0.95 of 51.3 $$\%$$ , params of 8 M and speed of 0.34 s, respectively. The performance of LDC-PP-YOLOE was compared against commonly used detectors and LDC-PP-YOLOE’s mAP@0.5 was 2.5, 6.9 and 16.3 $$\%$$ , and was 4.3 $$\%$$ greater than Faster R-CNN, YOLOX-s and PicoDet-L, respectively. LDC-PP-YOLOE achieved an RMSE of 8.63 and an MSE of 5.27 compared to the ground truth on citrus applications. In addition, we used apple and passion fruit datasets to verify the generalization of the model; the mAP@0.5 is improved by 1 and 0.7 $$\%$$ . LDC-PP-YOLOE can be used as a lightweight model to help growers track citrus populations and optimize citrus yields in complex citrus orchard environments with resource-limited equipment. It also provides a solution for lightweight models.},
  archive      = {J_PAAA},
  author       = {Lv, Yibo and Lu, Shenglian and Liu, Xiaoyu and Bao, Jiangchuan and Liu, Binghao and Chen, Ming and Li, Guo},
  doi          = {10.1007/s10044-024-01329-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LDC-PP-YOLOE: A lightweight model for detecting and counting citrus fruit},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFA u-net: A u-net like multi-stage feature analysis network
for medical image segmentation. <em>PAAA</em>, <em>27</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01331-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The U-Net and its extensions have achieved good success in medical image segmentation. However, fine-grained segmentation of the objects at their fuzzy edges, which is commonly found in medical images, is still challenging. In this paper, we propose a U-Net like Multi-Stage Feature Analysis Network (MFA U-Net) for medical image segmentation, which focus on mining the reusability of the images and features from several perspectives. Firstly, a multi-channel dimensional feature extraction module is proposed, where the input image was reused by multiple branches of convolutions with different channels to generate supplement features to the original U shaped network. Next, a cascaded U-shaped network is designed for deeper feature mining and analysis, which enables progressive refinement of the features. In the neck of the cascaded network, a parallel hybrid convolution module is designed that concatenating several types of convolutional methods to enhance the semantic representation ability of the model. In short, by reusing of the input images and detected features in several stages, more effective features were extracted and the segmentation performances were improved. The proposed algorithm was evaluated by three mainstream 2D color medical image segmentation datasets and gets significant improvements compared with the traditional U-Net framework, as well as the latest improved ones. Compared to the baseline network, it gets the improvements of 0.93% (Dice) and 1.45% (IoU) on GlaS, 2.09% (Dice) and 2.87% (IoU) on MoNuSeg, and 0.17% (F1) and 1.72% (SE) on DRIVE.},
  archive      = {J_PAAA},
  author       = {Wang, Yupeng and Wang, Suyu and He, Jian},
  doi          = {10.1007/s10044-024-01331-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MFA U-net: A U-net like multi-stage feature analysis network for medical image segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Methods for calculating gliding-box lacunarity efficiently
on large datasets. <em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01332-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lacunarity has proven to be a useful, multifaceted tool for image analysis in several different scientific fields, from geography to virology, which has lent increasing importance to the lacunarity analysis of large datasets. It can be most reliably calculated with the so-called gliding-box method, but the evaluation process can be exceedingly time-consuming and unviable as this algorithm is not designed to operate on large datasets. Here we introduce two novel methods that can calculate gliding-box lacunarity orders of magnitude faster than the original method without any loss of accuracy. We compare these methods with the original as well as with two already existing optimized methods based on runtime memory usage and complexity. The application of all five methods for both 2D and 3D datasets analysis confirms that each of the four optimized methods are orders of magnitude faster than the original one, but each has its advantages and limitations.},
  archive      = {J_PAAA},
  author       = {Kovács, Bálint Barna H. and Erdélyi, Miklós},
  doi          = {10.1007/s10044-024-01332-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Methods for calculating gliding-box lacunarity efficiently on large datasets},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WSA-YOLOv5s: Improved YOLOv5s based on window self-attention
module for ship detection. <em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01333-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection from visual image (SDVI) plays a significant role in terminal management, cross-border ship detection and marine target tracking. Compared to synthetic aperture radar (SDSAR) based ship detection, SDVI has superior performance on accuracy and speed. WSA-YOLOv5s, a new algorithm with a Window Self-Attention (WSA) module is proposed in this paper to substantially improve the ability of detecting small targets while enhancing the ability of large target recognition marginally. The following fine tunings are made based on the original YOLOv5s network. (1) Perform target data enhancement for the dataset’s imbalanced classification by using the improved mix-up algorithm and focal loss; (2) Replace CSP structure in the original backbone with the WSA module, and add one more interaction layer than the original to increase the exchange of high-level semantic information; (3) Add CBAM module to reduce the influence of background interference factors by using spatial attention and channel attention. Experiments are carried out on the Singapore Maritime Dataset (SMD). The results show that our model is greatly improved in all aspects. And the mAP increases from 73 to 90.5%, while keeping the number of model parameters small.},
  archive      = {J_PAAA},
  author       = {Zhou, Weina and Wang, Hong and Wu, Xintao},
  doi          = {10.1007/s10044-024-01333-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {WSA-YOLOv5s: Improved YOLOv5s based on window self-attention module for ship detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling the unseen: Novel strategies for object detection
beyond known distributions. <em>PAAA</em>, <em>27</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01334-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary machine learning, models often struggle with data distribution variations, severely impacting their out-of-distribution (OOD) generalization and detection capabilities. Current object detection methods, relying on virtual outlier synthesis and class-conditional density estimation, struggle to effectively distinguish OOD samples. They often depend on accurate density estimation and may produce virtual outliers that lack realism, particularly in complex or dynamic environments. Furthermore, previous research has typically addressed covariate and semantic shifts independently, resulting in fragmented solutions that fail to comprehensively tackle OOD generalization. This study introduces a unified approach to enhance OOD generalization in object recognition models, addressing these critical gaps. The strategy involves employing adversarial perturbations on the ID (In-Distribution) dataset to enhance the model’s resilience to distribution shifts, thereby simulating potential real-world scenarios characterized by imperceptible variations. Additionally, the integration of Maximum Mean Discrepancy (MMD) at the object level effectively discriminates between ID and OOD samples by quantifying distributional differences. For precise OOD detection, a K-nearest neighbors (KNN) algorithm is used during inference to measure similarity between samples and their closest neighbors in the training data. Evaluations on benchmark datasets, including PASCAL VOC and BDD100K as ID, with COCO and Open Images subsets as OOD, demonstrate significant improvements in OOD generalization compared to existing methods. These discoveries underscore the framework’s potential to elevate the dependability and flexibility of object recognition systems in practical scenarios, particularly in autonomous vehicles where accurate object detection under diverse conditions is critical for safety. This research contributes to advancing OOD generalization techniques and lays the groundwork for future refinement to address evolving challenges in machine learning applications. The code can be accessed from https://github.com/DeviSPhd/},
  archive      = {J_PAAA},
  author       = {Devi, S. and Dayana, R. and Malarvezhi, P.},
  doi          = {10.1007/s10044-024-01334-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Unveiling the unseen: Novel strategies for object detection beyond known distributions},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DN3MF: Deep neural network for non-negative matrix
factorization towards low rank approximation. <em>PAAA</em>,
<em>27</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01335-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction is one of the most sought-after methodologies to deal with high-dimensional ever-expanding complex datasets. Non-negative matrix factorization (NMF) is one such technique for dimension reduction. Here, a multiple deconstruction multiple reconstruction deep learning model (DN3MF) for NMF targeted towards low rank approximation, has been developed. Non-negative input data has been processed using hierarchical learning to generate part-based sparse and meaningful representation. The novel design of DN3MF ensures the non-negativity requirement of the model. The use of Xavier initialization technique solves the exploding or vanishing gradient problem. The objective function of the model has been designed employing regularization, ensuring the best possible approximation of the input matrix. A novel adaptive learning mechanism has been developed to accomplish the objective of the model. The superior performance of the proposed model has been established by comparing the results obtained by the model with that of six other well-established dimension reduction algorithms on three well-known datasets in terms of preservation of the local structure of data in low rank embedding, and in the context of downstream analyses using classification and clustering. The statistical significance of the results has also been established. The outcome clearly demonstrates DN3MF’s superiority over compared dimension reduction approaches in terms of both statistical and intrinsic property preservation standards. The comparative analysis of all seven dimensionality reduction algorithms including DN3MF with respect to the computational complexity and a pictorial depiction of the convergence analysis for both stages of DN3MF have also been presented.},
  archive      = {J_PAAA},
  author       = {Dutta, Prasun and De, Rajat K.},
  doi          = {10.1007/s10044-024-01335-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DN3MF: Deep neural network for non-negative matrix factorization towards low rank approximation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-BEST subspace clustering: Kernel-friendly block-diagonal
embedded and similarity-preserving transformed subspace clustering.
<em>PAAA</em>, <em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01336-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering methods, employing sparse and low-rank models, have demonstrated efficacy in clustering high-dimensional data. These approaches typically assume the separability of input data into distinct subspaces, a premise that does not hold true in general. Furthermore, prevalent low-rank and sparse methods relying on self-expression exhibit effectiveness primarily with linear structure data, facing limitations in processing datasets with intricate nonlinear structures. While kernel subspace clustering methods excel in handling nonlinear structures, they may compromise similarity information during the reconstruction of original data in kernel space. Additionally, these methods may fall short of attaining an affinity matrix with an optimal block-diagonal property. In response to these challenges, this paper introduces a novel subspace clustering approach named Similarity Preserving Kernel Block Diagonal Representation based Transformed Subspace Clustering (KBD-TSC). KBD-TSC contributes in three key aspects: (1) integration of a kernelized version of transform learning within a subspace clustering framework, introducing a block diagonal representation term to generate an affinity matrix with a block-diagonal structure. (2) Construction and integration of a similarity preserving regularizer into the model by minimizing the discrepancy between inner products of the original data and those of the reconstructed data in kernel space. This facilitates enhanced preservation of similarity information between the original data points. (3) Proposal of KBD-TSC by integrating the block diagonal representation term and similarity preserving regularizer into a kernel self-expressing model. The optimization of the proposed model is efficiently addressed through the alternating direction method of multipliers. This study validates the effectiveness of the proposed KBD-TSC method through experimental results obtained from nine datasets, showcasing its potential in addressing the limitations of existing subspace clustering techniques.},
  archive      = {J_PAAA},
  author       = {Maggu, Jyoti and Goel, Anurag},
  doi          = {10.1007/s10044-024-01336-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {K-BEST subspace clustering: Kernel-friendly block-diagonal embedded and similarity-preserving transformed subspace clustering},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFRENet: Efficient detection of drone image based on
multiscale feature aggregation and receptive field expanded.
<em>PAAA</em>, <em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01337-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of object detection in images captured by drones is witnessing a growing surge in research interest. However, because of the abundance of densely packed small objects in the majority of drone images, efficiently detecting dense small objects and achieving accurate classification remain a formidable challenge. To solve the problems mentioned above, we introduce an effective object detection network for drone images based on Multiscale Feature aggregation and Receptive field Expansion (MFRENet). First, we design an effective module named Receptive Field Expanded Feature Extraction Module (RFEFE), which can improve the model&#39;s perception ability of objects with irregular shapes and varying sizes. Next, we introduce the Multiscale Cross Stage Parallel Feature Fusion Module (MCSPFF), which integrates the RFEFE module, and then add the Shuffle Attention module to enable MCSPFF to obtain more semantic information. Then, we propose the Extended Simplified Spatial Pyramid Pooling-Fast and Feature Enhancement Module (ESimSPP2FE), which is inspired by the attention mechanism and enhances the features of small objects. Finally, we propose a small target detection head specially used to detect small targets, which enhances the detection ability of our model. Comprehensive experiments are performed on the VisDrone2021-DET dataset, and the proposed model is compared with the baseline YOLOv8m. The experimental results demonstrate that, in comparison to YOLOv8m, the proposed model achieves improvements of 1.9 and 2.7% in mAP and AP50, respectively. The code is available at https://github.com/chenhao-123-sudo/MFRENet-achive .},
  archive      = {J_PAAA},
  author       = {Chen, Hao and Yang, Wenzhu and Zhou, Guoyu and Zhang, Guodong and Nian, Zhaoyu},
  doi          = {10.1007/s10044-024-01337-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MFRENet: Efficient detection of drone image based on multiscale feature aggregation and receptive field expanded},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOv7-GCM: A detection algorithm for creek waste based on
improved YOLOv7 model. <em>PAAA</em>, <em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01338-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the cleanliness of creek environments, quadruped robots can be utilized to detect for creek waste. The continuous changes in the water environment significantly reduce the accuracy of image detection when using quadruped robots for image acquisition. In order to improve the accuracy of quadruped robots in waste detection, this article proposed a detection model called YOLOv7-GCM model for creek waste. The model integrated a global attention mechanism (GAM) into the YOLOv7 model, which achieved accurate waste detection in ever-changing backgrounds and underwater conditions. A content-aware reassembly of features (CARAFE) replaced a up-sampling of the YOLOv7 model to achieve more accurate and efficient feature reconstruction. A minimum point distance intersection over union (MPDIOU) loss function replaced the CIOU loss function of the YOLOv7 model to more accurately measure the similarity between target boxes and predictive boxes. After the aforementioned improvements, the YOLOv7-GCM model was obtained. A quadruped robot to patrol the creek and collect images of creek waste. Finally, the YOLOv7-GCM model was trained on the creek waste dataset. The outcomes of the experiment show that the precision rate of the YOLOv7-GCM model has increased by 4.2% and the mean average precision (mAP@0.5) has accumulated by 2.1%. The YOLOv7-GCM model provides a new method for identifying creek waste, which may help promote efficient waste management.},
  archive      = {J_PAAA},
  author       = {Qin, Jianhua and Zhou, Honglan and Yi, Huaian and Ma, Luyao and Nie, Jianhan and Huang, Tingting},
  doi          = {10.1007/s10044-024-01338-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {YOLOv7-GCM: A detection algorithm for creek waste based on improved YOLOv7 model},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instruction-aligned hierarchical waypoint planner for
vision-and-language navigation in continuous environments.
<em>PAAA</em>, <em>27</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01339-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing agents to follow language instructions is a compelling yet challenging research topic. Recently, vision-and-language navigation in continuous environments has been proposed to explore the multi-modal pattern analysis and mapless navigation abilities of intelligent agents. However, current waypoint-based methods still have shortcomings, such as the coupled decision process and the possible shortest path-instruction misalignment. To address these challenges, we propose an instruction-aligned hierarchical waypoint planner (IA-HWP) that ensures fine-grained waypoint prediction and enhances instruction alignment. Our HWP architecture decouples waypoint planning into a coarse view selection phase and a refined waypoint location phase, effectively improving waypoint quality and enabling specialized training supervision for different phases. In terms of instruction-aligned model design, we introduce the global action-vision co-grounding and local text-vision co-grounding modules to explicitly improve the understanding of visual landmarks and actions, thereby enhancing the alignment between instructions and trajectories. In terms of instruction-aligned model optimization, we employ reference-waypoint-oriented supervision and direction-aware loss to optimize the model for enhanced instruction following and waypoint execution capabilities. Experiments on the standard benchmark demonstrate the effectiveness of our approach, with improved success rate compared to existing methods.},
  archive      = {J_PAAA},
  author       = {He, Zongtao and Wang, Naijia and Wang, Liuyi and Liu, Chengju and Chen, Qijun},
  doi          = {10.1007/s10044-024-01339-z},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Instruction-aligned hierarchical waypoint planner for vision-and-language navigation in continuous environments},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced pseudo-labeling approach in mixing-based text data
augmentation method. <em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01340-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text augmentation methods facilitate an increase in the amount of training data, without having to collect new training data, by generating transformed versions of real datasets. Among such methods, mixing-based approaches, which swap words between two or more sentences, are widely applied owing to their simplicity and noteworthy performance. However, existing mixing-based approaches do not consider the importance of manipulated words during the pseudo-labeling process because they utilize a naive linear interpolation method. Thus, this paper proposes an advanced mixing-based text augmentation approach based on artificial intelligence methods that explicitly reflect the importance of manipulated words in the pseudo-labeling process. In addition, to avoid overdependence on the pseudo-labeling quality in the training process, the difference between the original label and prediction is also reflected in the loss function. Experimental results indicate that the performance of the proposed method is significantly higher than that of existing approaches.},
  archive      = {J_PAAA},
  author       = {Park, Jungmin and Lee, Younghoon},
  doi          = {10.1007/s10044-024-01340-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Advanced pseudo-labeling approach in mixing-based text data augmentation method},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hidden markov models with multivariate bounded asymmetric
student’s t-mixture model emissions. <em>PAAA</em>, <em>27</em>(4),
1–17. (<a href="https://doi.org/10.1007/s10044-024-01341-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) are popular methods for continuous sequential data modeling and classification tasks. In such applications, the observation emission densities of the HMM hidden states are generally continuous, can vary from one model to the other, and are typically modeled by elliptically contoured distributions, namely Gaussians or Student’s t-distributions. In this context, this paper proposes a novel HMM with Bounded Asymmetric Student’s t-Mixture Model (BASMM) emissions. Our new BASMMHMM is introduced in the light of the added robustness guaranteed by the BASMM in comparison to other popular emission distributions such as the Gaussian Mixture Model (GMM). In fact, GMMs generally have a limited performance with outliers in the data sets (observations) that the HMM is fitted to. Also, GMMs cannot sufficiently model skewed populations, which are typical in many fields, such as financial or signal processing-related data sets. An excellent alternative to solve this problem is found in Student’s t-mixture models. They have similar behaviour and shape to GMMs, but with heavier tails. This allows to have more tolerance towards data sets that span extensive ranges and include outliers. Asymmetry and bounded support are also important features that can further extend the model’s flexibility and fit the imperfections of real-world data. This leads us to explore the effectiveness of the BASMM as an observation emission distribution in HMMs, hence the proposed BASMMHMM. We will also demonstrate the improved robustness of our model by presenting the results of three different experiments: occupancy estimation, stock price prediction, and human activity recognition.},
  archive      = {J_PAAA},
  author       = {Bouarada, Ons and Azam, Muhammad and Amayri, Manar and Bouguila, Nizar},
  doi          = {10.1007/s10044-024-01341-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Hidden markov models with multivariate bounded asymmetric student’s t-mixture model emissions},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CP decomposition-based algorithms for completion problem of
motion capture data. <em>PAAA</em>, <em>27</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01342-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion capture (MoCap) technology is an essential tool for recording and analyzing movements of objects or humans. However, MoCap systems frequently encounter the challenge of missing data, stemming from mismatched markers, occlusion, or equipment limitations. Recovery of these missing data is imperative to maintain the reliability and integrity of MoCap recordings. This paper introduces a novel application of the tensor framework for MoCap data completion. We propose three completion algorithms based on the canonical polyadic (CP) decomposition of tensors. The first algorithm utilizes CP decomposition to capture the low-rank structure of the tensor. However, relying only on low-rank assumptions may be insufficient to deal with complex motion data. Thus, we propose two modified CP decompositions that incorporate additional information, SmoothCP and SparseCP decompositions. SmoothCP integrates piecewise smoothness prior, while SparseCP incorporates sparsity prior, each aiming to improve the accuracy and robustness of MoCap data recovery. To compare and evaluate the merit of the proposed algorithms over other tensor completion methods in terms of several evaluation metrics, we conduct numerical experiments with different MoCap sequences from the CMU motion capture dataset.},
  archive      = {J_PAAA},
  author       = {Mohaoui, Souad and Dmytryshyn, Andrii},
  doi          = {10.1007/s10044-024-01342-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CP decomposition-based algorithms for completion problem of motion capture data},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kfd-net: A knowledge fusion decision method for
post-processing brain glioma MRI segmentation. <em>PAAA</em>,
<em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01343-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic segmentation of brain glioma in MRI images is of great significance for clinical diagnosis and treatment planning. However, achieving precise segmentation requires effective post-processing of the segmentation results. Current post-processing methods fail to differentiate processing based on the glioma category, limiting the improvement of MRI segmentation accuracy. This paper proposes a novel knowledge fusion decision method for post-processing brain glioma MRI segmentation. The method takes grading information and the area ratio from the initial segmentation as input, performs fuzzy reasoning based on formulated rules, and generates decision coefficients for different segmentation regions. To address class imbalance in the segmentation network, a Boundary Region Voxel Dynamic Weighted Loss Function is introduced. On the BraTS2019 validation set, our method achieves DSC values of 0.756, 0.990, and 0.805 for ET, WT, and TC regions, respectively, along with HD values of 4.02mm, 10.73mm, and 9.52mm. Compared to state-of-the-art methods, our proposed approach demonstrates superior segmentation performance. Validation on the BraTS2020 dataset further confirms the stability and reliability of our method.},
  archive      = {J_PAAA},
  author       = {Wang, Guizeng and Lu, Huimin and Li, Niya and Xue, Han and Sang, Pengcheng},
  doi          = {10.1007/s10044-024-01343-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Kfd-net: A knowledge fusion decision method for post-processing brain glioma MRI segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multimodal deep learning approach for hurricane tack
forecast based on encoder-decoder framework. <em>PAAA</em>,
<em>27</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01344-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical Cyclones (TC), also known as typhoons or hurricanes, rank among the most devastating meteorological events worldwide, necessitating precise forecasts to mitigate their impact on life and property. Traditional TC track forecasting methods often depend on limited data sources, resulting in significant challenges in accuracy for long-term predictions. To address these challenges, we introduce a novel forecasting framework named HuCL (Hurricane technology with CNN and LSTM), which harnesses the power of advanced computing and deep learning technologies. This paper details the development and application of the HuCL framework, which integrates Convolutional Neural Networks (CNNs) and Long Short-Term Memory networks (LSTMs) within an encoder-decoder architecture to process and analyze multimodal data, including trajectory data and atmospheric reanalysis maps. This integration enables the HuCL model to capture and utilize complex, implicit relationships within the data, significantly enhancing forecast precision over traditional unimodal methods. Our empirical results demonstrate that HuCL achieves substantial improvements in forecasting accuracy for TC tracks in the Pacific Northwest from 2014 and 2018, underscoring its potential to transform TC prediction methodologies. By leveraging multiple data modalities, the HuCL framework sets a new standard in the predictive accuracy and operational utility of TC forecasting systems, presenting a significant advancement in the field.},
  archive      = {J_PAAA},
  author       = {Wang, Wennan and Lu, Jiadong and Zhu, Linkai and Dai, Shugeng and Song, Shiyang},
  doi          = {10.1007/s10044-024-01344-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A multimodal deep learning approach for hurricane tack forecast based on encoder-decoder framework},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-supervised hierarchical classifier based on local
information. <em>PAAA</em>, <em>27</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01345-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of labeled data is a common problem in supervised classification and in particular in hierarchical classification. Therefore, in this work a semi-supervised hierarchical classifier based on local information (SSHC-BLI) is proposed in order to take advantage of labeled and unlabeled data to perform classification tasks. SSHC-BLI is a semi-supervised learning algorithm for hierarchical classification, which tries to pseudo-label each unlabeled instance using the labels of its labeled neighbors, also, it uses a similarity function to determine whether the unlabeled instance is similar to its labeled neighbors to be pseudo-labeled; in this way, the heuristic function similarity of an instance with a set of instances is proposed. SSHC-BLI was tested in several datasets from different fields, including: artificial, functional genomics and text; also, it was compared against a supervised hierarchical classifier and  two state of the art methods, showing in most cases superior performance with statistical significance in exact match and Matthews correlation coefficient.},
  archive      = {J_PAAA},
  author       = {Serrano-Pérez, Jonathan and Sucar, L. Enrique},
  doi          = {10.1007/s10044-024-01345-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A semi-supervised hierarchical classifier based on local information},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new diffusion method for blind image denoising.
<em>PAAA</em>, <em>27</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01346-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a significant task in computer vision. Previous studies have mostly concentrated on removing noise with specific levels. The blind image denoising approach has recently gained more popularity due to its adaptability. Nonetheless, existing deep learning methods only train networks to learn the direct projection from noisy images to clean ones, which limits their denoising performance. This paper proposes a novel perspective for blind denoising by converting the static image denoising problem into a dynamic process inspired by the diffusion model. To achieve this, we present a new method that views a noisy image as a mid-state of a Gaussian diffusion process. Specifically, the image noise is separated into multiple sub-level noises through the diffusion process, and subsequently eliminated in a sequential manner. Furthermore, we propose a diffusion denoising network that comprises a Feature Extraction Module for extracting image features and a Diffusion Noise Estimation Module for estimating the sub-level noises. Our experiments demonstrate that our proposed method outperforms existing methods and achieves state-of-the-art results in blind additive white Gaussian noise and real-world image denoising.},
  archive      = {J_PAAA},
  author       = {Zhu, Yonggui and Chen, Yaling},
  doi          = {10.1007/s10044-024-01346-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new diffusion method for blind image denoising},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An fMRI-based visual decoding framework combined with
two-stage learning and asynchronous iterative update strategy.
<em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01347-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing visual stimulus information from evoked brain activity is a significant task of visual decoding of the human brain. However, due to the limited size of published functional magnetic resonance imaging (fMRI) datasets, it is difficult to adequately train a complex network with a large number of parameters. Furthermore, the dimensions of fMRI data in existing datasets are extremely high, and the signal-to-noise ratio of the data is relatively low. To address these issues, we design an fMRI-based visual decoding framework that incorporates additional self-supervised training on the encoder and decoder to alleviate the problem of insufficient model training due to limited datasets. Furthermore, we propose a iterative Two Part and Two Stage learning method involving a teacher (supervised)-student (self-supervised) setup and encoder-decoder asynchronous updates strategy. This approach allows the encoder and decoder to mutually reinforce and iteratively update each other under the guidance of a teacher model. The analysis of the ablation experiments demonstrates that the proposed framework can effectively improve the reconstruction accuracy. The experimental results show that the proposed method achieves better visual reconstruction from evoked brain activity of the human brain and that its reconstruction accuracy is superior to that of existing methods.},
  archive      = {J_PAAA},
  author       = {Zhang, Yuanyuan and Li, Xinyu and Liu, Baolin},
  doi          = {10.1007/s10044-024-01347-z},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An fMRI-based visual decoding framework combined with two-stage learning and asynchronous iterative update strategy},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive aquila optimizer for centralized mapping and
exploration. <em>PAAA</em>, <em>27</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01348-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Variable Aquila Optimization Algorithm, designed especially for Multi-Robot Search in uncharted territory, is introduced in this paper. The framework, called Coordinated Multi-robot Exploration Variable Aquila Optimizer (CME-VAO), combines a modified version of the Aquila Optimizer (AO) swarm-based approach with deterministic Coordinated Multi-agent Exploration (CME). With the introduction of stochastic parameters, this integration dynamically adapts the traditional Aquila optimization technique to increase exploration rates. Deterministic CME, which assesses nearby cells surrounding the agents to ascertain cost and utility values, is the first step in the CME-VAO architecture. Exploration efficiency is then further enhanced via Variable Aquila optimization. The effectiveness of the suggested method was confirmed using comprehensive simulations conducted in various environmental settings. Some newer algorithms, like CME-Aquila and CME-Grey Wolf Optimizer (CME-GWO), were used to compare the results. Things like exploration time, failed runs, and area coverage in different conditions were taken into account. We used many simulations with different environmental conditions to find the average coverage percentage and exploration time so that we could compare them statistically with CME-AO and CME-GWO. Results highlight the unique benefit of the suggested algorithm, exhibiting enhanced map exploration with much shorter execution times and few fail runs. All things considered, the CME-VAO architecture significantly improves map navigation in crowded areas in less time during exploration. This puts the suggested methodology in a highly advantageous position for on-board use in dynamic contexts where extended convergence periods or failures of traditional optimization methods are possible.},
  archive      = {J_PAAA},
  author       = {Gul, Faiza and Mir, Imran and Abualigah, Laith},
  doi          = {10.1007/s10044-024-01348-y},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive aquila optimizer for centralized mapping and exploration},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fedadkd: Heterogeneous federated learning via adaptive
knowledge distillation. <em>PAAA</em>, <em>27</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01350-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning framework. It facilitates collaborative modeling among participants without sharing raw user data, providing a feasible solution to address data silos in a secure and privacy-preserving manner. However, data heterogeneity is a major challenge of federated learning as it greatly impacts both the convergence speed during model training and the accuracy of predictions. To address this issue, recent federated learning algorithms have incorporated knowledge distillation as a approach of information sharing. Nevertheless, the majority of current approaches only employ logits averaging to combine participants’ knowledge, which can have a detrimental impact on the accuracy of the global model, particularly when certain local models show poor performance. besides, some methods rely on public datasets, thereby compromising the privacy-preserving principle of federated learning. To address the aforementioned concerns, we introduces a new federated learning algorithm named Federated Adaptive Knowledge Distillation (FedAdKD), which utilizes knowledge distillation. After the server completes the aggregation of the global model, FedAdKD dynamically allocates distillation weights based on the individual local model’s loss on the public dataset, and performs knowledge distillation on the global model. As the use of public datasets violates the privacy-preserving principle of federated learning, we also proposes the incorporation of generative models to generate data that adheres to the original image distribution. By employing federated learning on the client side to train diffusion models, data is generated that adheres to the original image distribution while maintaining privacy. This generated data is then utilized as the distillation dataset. Experimental findings validate the effectiveness of FedAdKD in addressing the obstacles presented by data heterogeneity. FedAdKD not only mitigates the decline in global model accuracy caused by subpar local models and minimizes knowledge forgetting resulting from direct model aggregation but also improves the generalization capacity of the global model.},
  archive      = {J_PAAA},
  author       = {Song, Yalin and Liu, Hang and Zhao, Shuai and Jin, Haozhe and Yu, Junyang and Liu, Yanhong and Zhai, Rui and Wang, Longge},
  doi          = {10.1007/s10044-024-01350-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fedadkd: Heterogeneous federated learning via adaptive knowledge distillation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AF-DETR: Efficient UAV small object detector via
assemble-and-fusion mechanism. <em>PAAA</em>, <em>27</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01349-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of deep learning networks, object detection technologies for unmanned aerial vehicle (UAV) have demonstrated outstanding performance in many application scenarios. However, current small object detection approaches overwhelmingly disregard sparse feature interactions and global context modeling, resulting in incomplete utilization and even loss of semantic information of small objects. Therefore, this study provides an advanced Assemble-and-Fusion mechanism used in DEtection TRansformer (AF-DETR), in which the aggregated global semantics are allocated across layers to augment fine-grained feature learning for small instances. Meanwhile, an adaptive context broadcasting module is designed to effectively integrate contextual information in the decoder, thus ensuring accurate detection of small objects. First, the last four stage features selected from the backbone are sent into the intra-scale feature interaction module, which performs self-attention operation on feature map of the last scale. Second, a fixed fusion module aligns and aggregates multi-scale representations prior to dissemination across layers. Features of adjoining levels then undergo transformation and consolidation within convolutional module. Finally, an enhanced adaptive context broadcasting module is introduced within the decoding MLP to incorporate aggregated semantics into individual tokens for broadcasting contextual information. Our AF-DETR achieves 49.5 $$\%$$ mAP50 and 29.5 $$\%$$ mAP50-95 on VisDrone2021 dataset, and impressive mAP50 results of 67.7% and 70.7% are achieved under RGB and Infrared modalities on the DroneVehicle dataset respectively. Extensive evaluations manifest consistent performance gains attained by our approach over state-of-the-art methods under various metrics, validated across multiple UAV perception benchmarks containing small objects under practical complex conditions.},
  archive      = {J_PAAA},
  author       = {Ren, Lingfei and Lei, Huan and Li, Zhongxu and Yang, Wenyuan},
  doi          = {10.1007/s10044-024-01349-x},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {AF-DETR: Efficient UAV small object detector via assemble-and-fusion mechanism},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global point cloud registration network for large
transformations. <em>PAAA</em>, <em>27</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01351-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional registration is an established yet challenging problem that is key in many different applications, such as mapping the environment for autonomous vehicles, or modeling people for avatar creation, among others. Registration refers to the process of mapping multiple data into the same coordinate system by means of matching correspondences and transformation estimation. Novel proposals exploit the benefits of deep learning architectures for this purpose, as they learn the best features for the data, providing better matches and hence results. However, the state of the art is usually focused on cases of relatively small transformations, although in certain applications and in a real and practical environment, large transformations are very common. In this paper, we present ReLaTo (Registration for Large Transformations), an architecture that addresses the cases where large transformations happen while maintaining good performance for local transformations. This proposal uses a novel Softmax pooling layer to find correspondences in a bilateral consensus manner between two point sets, sampling the most confident matches. These matches estimate a coarse and global registration using weighted Singular Value Decomposition (SVD). A target-guided denoising step is applied to both the obtained matches and latent features to estimate the final fine registration considering the local geometry. All these steps are carried out following an end-to-end approach, which has been shown to perform better than 7 state-of-the-art registration methods in two datasets commonly used for this task (ModelNet40 and the Karlsruhe Institute of Technology and Toyota Technological Institute dataset, KITTI), especially in the case of large transformations.},
  archive      = {J_PAAA},
  author       = {Cuevas-Velasquez, Hanz and Galan-Cuenca, Alejandro and Gallego, Antonio Javier and Saval-Calvo, Marcelo and Fisher, Robert B.},
  doi          = {10.1007/s10044-024-01351-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Global point cloud registration network for large transformations},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text kernel expansion for real-time scene text detection.
<em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01352-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding textual information from natural images is fundamental for artificial intelligence systems to comprehend and interact with the environment. The precise detection of text is crucial for achieving these objectives. In this work, we propose a real-time arbitrary-shaped scene text detector named Text Kernel Expansion (TKE). TKE employs a segmentation module to segment text kernels, and then models them as control points. By employing a regression-based network, TKE refines those control points through an expansion procedure, avoiding the need for complex pixel-level post-processing and ensuring both efficiency and excellent performance. Additionally, we propose an Optimal Bipartite Graph Matching Loss that measures the matching error between the refined control points and the labeled vertices, which efficiently minimizes the global matching distance. Comprehensive testing on four standard benchmarks confirms that our method strikes an effective balance between accuracy and efficiency. The code of our proposed method can be found in: https://github.com/TankosTao/TKE.git . All related datasets are openly valuable and can be downloaded through our Github link.},
  archive      = {J_PAAA},
  author       = {He, Tao and Huang, Sheng and Tang, Wenhao and Liu, Bo},
  doi          = {10.1007/s10044-024-01352-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Text kernel expansion for real-time scene text detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved multi object tracking with locality sensitive
hashing. <em>PAAA</em>, <em>27</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s10044-024-01353-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is one of the most advanced applications of computer vision algorithms. While various tracking approaches have been previously developed, they often use many approximations and assumptions to enable real-time performance within the resource constraints in terms of memory, time and computational requirements. In order to address these limitations, we investigate the bottlenecks of existing tracking frameworks and propose a solution to enhance tracking efficiency. The proposed method uses Locality Sensitive Hashing (LSH) to efficiently store and retrieve nearest neighbours and then utilizes a bipartite cost matching based on the predicted positions, size, aspect ratio, appearance description, and uncertainty in motion estimation. The LSH algorithm helps reduce the dimensionality of the data while preserving their relative distances. LSH hashes the features in constant time and facilitates rapid nearest neighbour retrieval by considering features falling into the same hash buckets as similar. The effectiveness of the method was evaluated on the MOT benchmark dataset and achieved Multiple Object Tracker Accuracy (MOTA) of 67.1% (train) and 62.7% (test). Furthermore, our framework exhibits the highest Multiple Object Tracker Precision (MOTP), mostly tracked objects, and the lowest values for mostly lost objects and identity switches among the state-of-the-art trackers. The incorporation of LSH implementation reduced identity switches by approximately 7% and fragmentation by around 13%. We used the framework for real-time tracking applications on edge devices for an industry partner. We found that the LSH integration resulted in a notable reduction in track ID switching, with only a marginal increase in computation.},
  archive      = {J_PAAA},
  author       = {Chemmanam, Ajai John and Jose, Bijoy and Moopan, Asif},
  doi          = {10.1007/s10044-024-01353-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improved multi object tracking with locality sensitive hashing},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast consistent grid-based clustering algorithm.
<em>PAAA</em>, <em>27</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01354-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast consistent grid-based algorithm that estimates the number of clusters for observations in $${{\mathbb {R}}}^d$$ and, besides, constructs an approximation for the clusters. Consistency is proved under certain conditions. The time complexity of the algorithm can be made linear retaining the consistency. Numerical experiments confirm high computational efficiency of the new algorithm and its ability to process large datasets.},
  archive      = {J_PAAA},
  author       = {Tarasenko, Anton S. and Berikov, Vladimir B. and Pestunov, Igor A. and Rylov, Sergey A. and Ruzankin, Pavel S.},
  doi          = {10.1007/s10044-024-01354-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A fast consistent grid-based clustering algorithm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel robust generalized eigenvalue proximal support
vector machine for pattern classification. <em>PAAA</em>,
<em>27</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10044-024-01355-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By minimizing the p-order of $$L_{2}$$ -norm distance of the objection function of the improved generalized eigenvalue proximal support vector machine (IGEPSVM), the $$L_{2,p}$$ -LIGEPSVM is proposed in this paper. Firstly, the solution of the $$L_{2,p}$$ -LIGEPSVM is demonstrated to be related to the minimum eigenvalue of the correlation matrix, and an improved inverse power method is devised to solve the $$L_{2,p}$$ -LIGEPSVM. Compared with IGEPSVM, $$L_{2,p}$$ -LIGEPSVM not only retains the advantages of linear IGEPSVM, but also overcomes the shortcomings of exaggeration of outliers based on squared Frobenius-norm distance metrics. Furthermore, the main improvements of $$L_{2,p}$$ -LIGEPSVM over IGEPSVM are the robustness and learning efficiency in solving outlier problems. Finally, in order to illustrate the effectiveness and accuracy of the proposed algorithms, five other relevant algorithms are tested on the artificial and the UCI datasets. The classification accuracy of the two algorithms of $$L_{2,p}$$ -LIGEPSVM on the Artificial, Australian, Cancer and Sonar datasets are (83%, 84.2%), (98.70%, 99.25%), (98.84%, 98.77%) and (95.65%, 95.70%), respectitvely, and they are all higher than other related 5 algorithms. Experimental results show that $$L_{2,p}$$ -LIGEPSVM has some obvious advantages, such as not sensitive to outliers, resistant to noise, high computational efficiency and effective to classification. It is worth mentioning that we introduce the application of the proposed method and elaborate on its economic value.},
  archive      = {J_PAAA},
  author       = {Xiong, Weizhi and Yu, Guolin and Ma, Jun and Liu, Sheng},
  doi          = {10.1007/s10044-024-01355-z},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel robust generalized eigenvalue proximal support vector machine for pattern classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spoken language understanding via graph contrastive learning
on the context-aware graph convolutional network. <em>PAAA</em>,
<em>27</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01362-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spoken language understanding system is a crucial component of a dialogue system whose task is to comprehend the user’s verbal expressions and perform the corresponding tasks accordingly. Contextual spoken language understanding (contextual SLU) is an extremely critical issue on this field as it helps the system to understand the user’s verbal expressions more accurately, thus improving the system’s performance and accuracy. The aim of this paper is to enhance the effectiveness of contextual SLU analysis. Context-based language unit systems are mainly concerned with effectively integrating dialog context information. Current approaches usually use the same contextual information to guide the slot filling of all tokens, which may introduce irrelevant information and lead to comprehension bias and ambiguity. To solve this problem, we apply the principle of graph contrastive learning based on the graph convolutional network to enhance the model’s ability to aggregate contextual information. Simultaneously, applying graph contrastive learning can enhance the model’s effectiveness by strengthening its intention. More precisely, graph convolutional networks can consider contextual information and automatically aggregate contextual information, allowing the model to no longer rely on traditionally designed heuristic aggregation functions. The contrastive learning module utilizes the principle of contrastive learning to achieve the effect of intention enhancement, which can learn deeper semantic information and contextual relationships, and improve the model&#39;s effectiveness in three key tasks: slot filling, dialogue action recognition, and intention detection. Experiments on a synthetic dialogue dataset show that our model achieves state-of-the-art performance and significantly outperforms other previous approaches (Slot F1 values + 1.03% on Sim-M, + 2.32% on Sim-R; Act F1 values + 0.26% on Sim-M, + 0.56% on Sim-R; Frame Acc values + 3.15% on Sim-M, + 1.62% on Sim-R). The code is available at: https://github.com/caoze1228/ACIUGCL-CSLU .},
  archive      = {J_PAAA},
  author       = {Cao, Ze and Liu, Jian-Wei},
  doi          = {10.1007/s10044-024-01362-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Spoken language understanding via graph contrastive learning on the context-aware graph convolutional network},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HSWmark: Robust and reversible image watermarking with
improved capacity using quad-tree segmentation. <em>PAAA</em>,
<em>27</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01370-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a HSWmark (Histogram Shifting Watermark) based robust and reversible image watermarking scheme using Quad tree (QT) segmentation with improved capacity and minimum distortion. QT segmentation divides the input image into several sized non-overlapping blocks, and each block is used to embed payload data. Preprocessing is used to avoid overflow and underflow problems while embedding bits into pixels. Histograms of each block are generated and its maximum point and zero point are used for embedding. Payload embedding is performed using the Histogram Shifting (HS) method. Side information containing the details about QT segmentation structure, payload length, and compressed location map is stored in a few rows of cover images using Least Significant Bits replacement technique. At the extraction end, side information is extracted from the received image and payload data are retrieved back along with recovered cover images. Performance of the proposed scheme is assessed in terms of parameters Peak Signal to Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Embedding Capacity (EC), Bit Rate and Bit Error Rate (BER). Parameter values are also compared with some existing techniques. Simulation results shows that attained PSNR is above 47 dB for all test images along with SSIM values higher than 0.979. Proposed technique is able to provide the maximum EC of 102,215 bits with 0.389 bit rate. Thus, the presented technique provides better visual quality of images with improved capacity. Proposed scheme is also tested for its robustness against different attacks and achieved PSNR is above 40 dB with SSIM value higher than 0.88 for all test images. Performance is also compared with the existing technique and from results it is clear that it provides better BER, PSNR and SSIM as compared to the existing techniques.},
  archive      = {J_PAAA},
  author       = {Dwivedi, Ranjana and Srivastava, Vinay Kumar},
  doi          = {10.1007/s10044-024-01370-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {HSWmark: Robust and reversible image watermarking with improved capacity using quad-tree segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel bayesian probabilistic distance clustering
algorithm. <em>PAAA</em>, <em>27</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01356-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Tortora et al. (SN Comput Sci 1:65, 2020) introduced two probabilistic d-clustering algorithms based on the multivariate Gaussian distribution and multivariate Student-t distributions, which exhibit superior performance compared to probabilistic d-clustering and k-means algorithms. However, these proposed algorithms may need help when the variances of individual clusters are heterogeneous. This paper presents a unified Bayesian approach to Gaussian probabilistic distance clustering to address this issue, employing the Gibbs posterior. We derived a closed-form posterior distribution for each unknown parameter using this approach. The effectiveness of the extended method was demonstrated through two numerical examples, including one simulation study and one real data analysis based on three datasets. The proposed method was further compared with conventional methods, demonstrating its superior accuracy. Simulation studies and real data analyses indicate that in many cases, mainly when there is correlation, overlap, or a data variance greater than one, as well as when overlap alone exists, the Bayesian Gaussian probabilistic distance clustering algorithm outperforms the Gaussian probabilistic distance clustering algorithm.},
  archive      = {J_PAAA},
  author       = {Tabibi Gilani, Morteza and Zarei, Reza and Tabibi Gilani, Niloofar},
  doi          = {10.1007/s10044-024-01356-y},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel bayesian probabilistic distance clustering algorithm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting interactions using bayesian additive regression
trees. <em>PAAA</em>, <em>27</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01357-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian additive regression trees (BART) is an ensemble prediction method that is able to model complex, highly non-linear relationships in continuous and discrete data. Previous methods have utilized BART’s flexible, sum-of-trees model to perform variable selection that is non-parametric with respect to the functional form of the outcome. In this paper we introduce a method to detect interactions in a similar manner. Additionally, we present a BART-based pipeline for variable and interaction selection to build interpretable machine learning models that improve inference over existing methods named BARTselect. Simulation results demonstrate that our method is able to detect a high proportion of true interactions across a range of data settings. We conclude with two example data settings, examining interactions in medical expenditure data and newborn birth weight data.},
  archive      = {J_PAAA},
  author       = {Marvald, Joshua and Love, Tanzy},
  doi          = {10.1007/s10044-024-01357-x},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Detecting interactions using bayesian additive regression trees},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-supervised crowd counting method based on patch
crowds statistics. <em>PAAA</em>, <em>27</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01359-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has been widely applied in various fields including social security, urban planning, and intelligent monitoring. A series of excellent fully-supervised crowd counting methods have emerged and achieve great performance. Nevertheless, all of the fully-supervised methods heavily depend on large quantities of annotated crowd density maps. Collecting and annotating crowd images is time-consuming and expensive, especially for highly dense crowds. In contrast, unlabeled crowd images can be acquired without having to make a great effort. However, it is challenging to effectively exploit unlabeled data for crowd counting. To this end, we propose a semi-supervised crowd counting method that aims to optimize the crowd counting models via exploiting large amounts of unlabeled crowd images. Firstly, we design an effective proxy task based on image patch counts statistics. Then, we present an end-to-end iterative learning strategy to train our semi-supervised framework. To prove the effectiveness of our semi-supervised method, we conducted various experiments on three benchmark crowd counting datasets. Experimental results demonstrate that our semi-supervised algorithm achieves competitive performance compared with state-of-the-art semi-supervised crowd counting approaches. Furthermore, experimental results show that our method performs well on cross-dataset.},
  archive      = {J_PAAA},
  author       = {Peng, Sifan and Yin, Baoqun and Xia, Yinfeng and Yang, Qianqian and Wang, Luyang},
  doi          = {10.1007/s10044-024-01359-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A semi-supervised crowd counting method based on patch crowds statistics},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One new family of smooth semi-supervised support vector
classifier based on fourier series approximation technique.
<em>PAAA</em>, <em>27</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01365-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new family of smooth semi-supervised support vector machines based on Fourier series approximation technique (FS-S4VMs) for classification. The semi-supervised support vector machine (S3VM) is the powerful tool for coping with quantities of unlabeled data in the real world. However, the symmetric hinge loss function of S3VM is not smooth. It will decrease the classification accuracy and endure heavy burden calculation. To deal with this problem, the Fourier series approximation smooth technique for replacing the non-smooth item have been investigated. One novel family of FS-S4VMs classifier is derived. Thus, one fast SAGA algorithm for solving non-convex FS-S4VMs can be utilized to decrease the computing scale. The nonlinear case and convergence analysis are presents as well. To attest how the new FS-S4VMs can be implemented into practice, experiments on synthetic and real data sets are accessed. In the final, theoretical explanation and simulation comparisons illustrate the FS-S4VMs enhance the robustness of S3VM, and increase the classification accuracy at about 1%–3% than other smooth techniques.},
  archive      = {J_PAAA},
  author       = {Wang, En and Mei, Yang},
  doi          = {10.1007/s10044-024-01365-x},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {One new family of smooth semi-supervised support vector classifier based on fourier series approximation technique},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphfingerprint: Graph embedding of graphs with almost
constant sub-structures. <em>PAAA</em>, <em>27</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s10044-024-01366-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some machine learning applications, graphs tend to be composed of a large number of tiny almost constant sub-structures. The current embedding methods are not prepared for this type of graphs and thus, their representational power tends to be very low. Our aim is to define a new graph embedding that considers this specific type of graphs. We present GraphFingerprint, which is a new embedding method that specifically considers the fact that graphs are composed of millions of almost constant sub-structures. The three-dimensional characterisation of a chemical metal-oxide nanocompound easily fits in these types of graphs, which nodes are atoms and edges are their bonds. Our graph embedding method has been used to predict the toxicity of these nanocompounds, achieving a high accuracy compared to other embedding methods. The representational power of the current embedding methods do not properly satisfy the requirements of some machine learning applications based on graphs, for this reason, a new embedding method has been defined and heuristically demonstrated that achieves good accuracy.},
  archive      = {J_PAAA},
  author       = {Serratosa, Francesc},
  doi          = {10.1007/s10044-024-01366-w},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Graphfingerprint: Graph embedding of graphs with almost constant sub-structures},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerated multi-kernel sparse stochastic optimization
classifier algorithm for explainable prediction. <em>PAAA</em>,
<em>27</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01367-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For classification problems, training accurate and sparse models in limited time has been a longstanding challenge. When a large number of irrelevant and redundant features are collected in datasets, this problem becomes more difficult. Feature selection is often used to solve this problem. However, feature selection suffers from high time complexity and a trade-off between the number of selected features and predictive errors. As a solution to this problem, we propose an accelerated multi-kernel sparse stochastic optimization classifier (AMSSOC) algorithm. which reconstructs $$\ell _{1}$$ -norm support vector classifier by introducing the $$\ell _{0}$$ -norm function. AMSSOC minimizes the number of features by using an approximate $$\ell _{0}$$ -norm function to the support vector classifier. Meanwhile, the novel column-wise multi-kernel matrix is integrated into the classifier to select key features and obtain the explainable prediction. Moreover, the AMSSOC algorithm employs the Nesterov’s method to accelerate the training speed of the classifier. Our main purpose is to find the feature subset as small as possible for improving the prediction accuracy and explainability. As shown in experimental results, the proposed AMSSOC outperforms the existing four classification methods on thirteen real-world and two real image datasets. At the same time, critical feature subsets are discovered, which play an important role for explainable prediction in practical applications. Also, the achieved outcomes demonstrate the superiority of the proposed algorithm.},
  archive      = {J_PAAA},
  author       = {Chen, Zhirui and Zhang, Zhiwang and Li, Shuqing and Cao, Jie},
  doi          = {10.1007/s10044-024-01367-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Accelerated multi-kernel sparse stochastic optimization classifier algorithm for explainable prediction},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on distance measurement of vehicles in front of
campus patrol vehicles based on monocular vision. <em>PAAA</em>,
<em>27</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01368-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology of intelligent vehicle perception system based on vision sensor is becoming more and more mature, and many distance measurement methods based on monocular vision have been proposed. However, less attention has been paid to the experiment and application of the distance measurement of the vehicle in front of the campus intelligent patrol vehicle. In this paper, two monocular visual distance measurement methods based on the width of the target detection object and the distance between the target detection object and the ground contact point are proposed. Two distance measurement models are proposed according to the principle of camera imaging model and the principle of coordinate system transformation. Then, two kinds of distance measurement experiments are designed and compared. The experimental results show that the measurement method based on the contact point between the target and the ground is poor, but the absolute error is less than 0.67 m. The overall error of distance measurement method based on license plate width is the smallest, and the absolute error is less than 0.15 m. The distance measurement error of the detection box based on the width of the vehicle body is large, and the absolute error is kept within 0.31 m. The measurement accuracy of the two methods for the distance measurement of the vehicle in front of the campus patrol vehicle meets the detection requirements. This work is significant to the research on the distance measurement of the vehicle in front of campus patrol vehicles and it is of great significance to enhance the safety of the campus patrol vehicle.},
  archive      = {J_PAAA},
  author       = {Zheng, Lei and Liu, Lei and Lu, Jingyu and Tian, Jie and Cheng, Yong and Yin, Wei},
  doi          = {10.1007/s10044-024-01368-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Research on distance measurement of vehicles in front of campus patrol vehicles based on monocular vision},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion for human compound emotion recognition: A
fusion of facial expression texture and action unit data. <em>PAAA</em>,
<em>27</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01369-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of facial expressions is a challenging task in computer vision because of the complexity associated with individual facial features and social differences. Early studies classified human facial expressions into six basic categories which are anger, disgust, fear, happiness, sadness and surprise. The neutral expression is also taken into account. Furthermore, compound emotions are explored on human faces which are the representations of the expressions that entail the combination of more than a single basic facial expression. Including at least two expression categories, one is considered as the dominating expression and the other as the complementary expression. In this way, the categorization of compound facial expressions is done. In this study, a novel approach is proposed to recognize compound facial expressions. The main contribution of this paper is the proposed fusion of deep texture and geometric features. The texture features are the deep textures obtained from a deep learning model. The iCV-MEFED dataset is employed. It includes compound facial expressions consisting of all the combinations of basic facial expressions in the sense of dominating and complementary expressions. Therefore, 50 distinct classes of facial expressions are presented. The previous studies carried out on this dataset report high rates of misclassification due to the challenge of the complexity of facial expressions and correlations among the compound expressions. The proposed approach obtained encouraging results and has shown significant improvements in the recognition accuracy of compound facial expressions on the iCV-MEFED dataset.},
  archive      = {J_PAAA},
  author       = {Jiddah, Salman Mohammed and Yurtkan, Kamil},
  doi          = {10.1007/s10044-024-01369-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Feature fusion for human compound emotion recognition: A fusion of facial expression texture and action unit data},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware focal alignment network for micro-video
multi-label classification. <em>PAAA</em>, <em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01376-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-videos have gained immense popularity in recent years due to their concise and interactive format, which aligns well with the fast-paced nature of modern digital consumption. However, this brevity often results in significant semantic shifts within a short timeframe, making it challenging to accurately uncover their context for more precise categorization. To address this issue, a context-aware focal alignment network (CAFANET) for micro-video multi-label classification is proposed. We first implemented a temporal scaling feature extraction approach to achieve a hierarchical representation enriched with segment-based details. We then introduce a context-aware focal alignment attention (CAFAA) mechanism, and this innovative component dynamically adjusts its focus based on the unique characteristics of each segment, effectively bridging the gap between local details with global contextual awareness. Furthermore, we finally fuse these aligned features with the global contexts to obtain the final feature representations, describing the overall information for subsequent classification. Experimental results on a real-world micro-videos multi-label dataset demonstrated the effectiveness of our proposed method in comparison to several state-of-the-art approaches.},
  archive      = {J_PAAA},
  author       = {Yuan, Bin and Yao, Weiheng and Jing, Peiguang and Zhang, Jing and Tsang, Kim Fung and Wang, Shuqiang},
  doi          = {10.1007/s10044-024-01376-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Context-aware focal alignment network for micro-video multi-label classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STBA: Span-based tagging scheme with biaffine attention for
enhanced aspect sentiment triplet extraction. <em>PAAA</em>,
<em>27</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01377-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fine-grained sentiment analysis, a burgeoning and exceptionally challenging subtask is Aspect Sentiment Triplets Extraction (ASTE). This task aims to extract aspect terms, and opinion terms, and determine their corresponding sentiment polarity from a given sentence. Most previous research has focused primarily on creating novel tagging schemes that enable the model to extract sentiment triplets in a seamless, end-to-end manner. However, almost all of these methods have certain drawbacks: first, they overly rely on the assumption that each word has a single function; second, they treat each opinion term or target term as an isolated set of words. As a result, they perform poorly in handling complex ASTE tasks. To address the issues, we propose a span-based tagging scheme with biaffine attention to enhance the extraction of aspect/opinion triplets. During the triplet extraction process, we introduced layer normalization to improve training stability and model generalization and transformed the triplet extraction task into a classification problem. In addition, to enhance the correlations between individual word pairs, we integrated a biaffine attention mechanism to capture the probability distribution of relationships between different spans and to aggregate semantic information within sentences. Demonstrating the effectiveness of the STBA approach. Experiments on the first four benchmark datasets from SemEval Challenges show that our proposed method outperforms strong baselines.},
  archive      = {J_PAAA},
  author       = {Xiao, Xin and Gao, Bin and Su, Zelong and Li, Linlin and Li, Yutong and Liu, Shutian and Liu, Zhengjun},
  doi          = {10.1007/s10044-024-01377-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {STBA: Span-based tagging scheme with biaffine attention for enhanced aspect sentiment triplet extraction},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced bayesian gaussian hidden markov mixture clustering
for improved knowledge discovery. <em>PAAA</em>, <em>27</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s10044-024-01374-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden Markov model (HMM) is widely utilized in natural language processing, speech recognition, autonomous vehicular systems, and healthcare for tasks such as clustering, pattern recognition, predictive modeling, anomaly detection, and time-series forecasting. However, HMMs can be sensitive to initial states, compromising clustering reliability. To address this issue, we propose an innovative integration of an HMM with hybrid distance metric learning and a modified Bayesian Gaussian mixture model (BGMM) to enhance clustering performance and robustness. A significant challenge in HMM applications is determining the optimal number of hidden states. We address this using a k-fold cross-validation strategy. Implementing our Bayesian Gaussian Hidden Markov Mixture Clustering Model (BGH2MCM) on five diverse datasets, we categorize the observed data sequences according to underlying hidden state sequences. This approach yields superior outcomes to conventional techniques such as K-means, agglomerative clustering, density-based spatial clustering of applications with noise (DBSCAN), and the BGMM. We evaluate the efficiency of our model using silhouette, Davies–Bouldin, and Calinski–Harabasz scores, accuracy metrics, and computation time. Our results demonstrate that the BGH2MCM consistently achieves better clustering quality and computational efficiency, showing an average computation time 23% lower than agglomerative clustering with HMM, 22% less than DBSCAN with HMM, and 14% lower than K-means with the HMM and a BGMM-HMM across all datasets. This study highlights the potential of our BGH2MCM to improve data mining and knowledge discovery practices from complex, real-world datasets.},
  archive      = {J_PAAA},
  author       = {Ganesan, Anusha and Paul, Anand and Kim, Sungho},
  doi          = {10.1007/s10044-024-01374-w},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhanced bayesian gaussian hidden markov mixture clustering for improved knowledge discovery},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demsasa: Micro-video scene classification based on denoising
multi-shots association self-attention. <em>PAAA</em>, <em>27</em>(4),
1–9. (<a href="https://doi.org/10.1007/s10044-024-01378-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to segmentation and splicing in micro-videos when user upload videos to platform, the content of different shots in the same scene is discontinuous, which leads to the problem of large content differences between different shots. At the same time, due to the low resolution of the shooting equipment or jitter and other factors, the video has noise information. In view of the above problems, the conventional and serialized scene feature learning in micro-video cannot learn the content difference and correlation between different shots, which will weaken the semantic representation of scene features. Therefore, this paper proposes a micro-video scene classification method based on De-noising Multi-shots Association Self-attention (DeMsASa) model. In this method, the shot boundary detection algorithm segments micro- video firstly, and then the semantic representation of the multi-shots video scene is learned by de-noising, association between video frames in the same shot and the association modeling between different shots. Experiments results show that the classification performance of the proposed method is superior to the existing micro-video scene classification methods.},
  archive      = {J_PAAA},
  author       = {Gong, Rui and Zhang, Yu and Zhang, Yanhui and Liu, Yue and Guo, Jie and Nie, Xiushan},
  doi          = {10.1007/s10044-024-01378-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Demsasa: Micro-video scene classification based on denoising multi-shots association self-attention},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Steel surface defect detection based on sparse global
attention transformer. <em>PAAA</em>, <em>27</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01375-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of surface defects in steel is a fundamental technique for verifying the quality of the material. Despite the widespread use of transformer-based detection methods in defect detection, the precision and speed are still far from satisfactory according to industry standards. A detection model based on sparse global attention is presented in this paper. We proposed a simple sparse sliding-window attention, which localizes self attention for each pixel to its near neighbors. By adjusting dilation values, a larger receptive field can be obtained to improve the detection effect of large-size defects. Then we use the Content-Aware ReAssembly of FEatures (CARAFE) feature upsampling operator, which can aggregate contextual information in the large receptive field and generate features in the predefined region in the way of content-aware to improve the effect of feature fusion. Finally, the EIOU loss is introduced to solve the problem of scale consistency of the bounding box. Through ablation experiments, we analyze the effect of different dilation values on object detection performance. The proposed algorithm achieves the Mean Average Precision (mAP) of 83.7% on the NEU-DET dataset. Through experimentation with the aluminum defect dataset, we have demonstrated that our approach is applicable to other types of defects.},
  archive      = {J_PAAA},
  author       = {Li, Yinghao and Han, Zhiyong and Wang, Wenmeng and Xu, Heping and Wei, Yongpeng and Zai, Guangjun},
  doi          = {10.1007/s10044-024-01375-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Steel surface defect detection based on sparse global attention transformer},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-complexity arrays of patch signature for efficient
ancient coin retrieval. <em>PAAA</em>, <em>27</em>(3), 1–30. (<a
href="https://doi.org/10.1007/s10044-024-01284-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new recognition framework for ancient coins struck from the same die. It is called Low-complexity Arrays of Patch Signatures. To overcome the problem of illumination conditions we use multi-light energy maps which are a light-independent, 2.5D representation of the coin. The coin recognition is based on a local texture analysis of the energy maps. Descriptors of patches, tailored to coin images via the properties provided by the energy map, are matched against a database using a system of associative arrays. The system of associative arrays used for the matching is a generalization of the Low-complexity Arrays of Contour Signatures. Hence, the matching is very efficient and nearly at constant time. Due to the lack of available data, we present two new data sets of artificial and real ancient coins respectively. Theoretical insights for the framework are discussed and various experiments demonstrate the promising efficiency of our method.},
  archive      = {J_PAAA},
  author       = {Lardeux, Florian and Gomez-Krämer, Petra and Marchand, Sylvain},
  doi          = {10.1007/s10044-024-01284-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Low-complexity arrays of patch signature for efficient ancient coin retrieval},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot learning for COVID-19 chest x-ray classification
with imbalanced data: An inter vs. Intra domain study. <em>PAAA</em>,
<em>27</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01285-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image datasets are essential for training models used in computer-aided diagnosis, treatment planning, and medical research. However, some challenges are associated with these datasets, including variability in data distribution, data scarcity, and transfer learning issues when using models pre-trained from generic images. This work studies the effect of these challenges at the intra- and inter-domain level in few-shot learning scenarios with severe data imbalance. For this, we propose a methodology based on Siamese neural networks in which a series of techniques are integrated to mitigate the effects of data scarcity and distribution imbalance. Specifically, different initialization and data augmentation methods are analyzed, and four adaptations to Siamese networks of solutions to deal with imbalanced data are introduced, including data balancing and weighted loss, both separately and combined, and with a different balance of pairing ratios. Moreover, we also assess the inference process considering four classifiers, namely Histogram, kNN, SVM, and Random Forest. Evaluation is performed on three chest X-ray datasets with annotated cases of both positive and negative COVID-19 diagnoses. The accuracy of each technique proposed for the Siamese architecture is analyzed separately. The results are compared to those obtained using equivalent methods on a state-of-the-art CNN, achieving an average F1 improvement of up to 3.6%, and up to 5.6% of F1 for intra-domain cases. We conclude that the introduced techniques offer promising improvements over the baseline in almost all cases and that the technique selection may vary depending on the amount of data available and the level of imbalance.},
  archive      = {J_PAAA},
  author       = {Galán-Cuenca, Alejandro and Gallego, Antonio Javier and Saval-Calvo, Marcelo and Pertusa, Antonio},
  doi          = {10.1007/s10044-024-01285-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Few-shot learning for COVID-19 chest X-ray classification with imbalanced data: An inter vs. intra domain study},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-dimensional extension of the slope chain code:
Analyzing the tortuosity of the flagellar beat of human sperm.
<em>PAAA</em>, <em>27</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01286-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of 3D image processing, accurately representing the geometric nuances of line curves is crucial. Building upon the foundation set by the slope chain code, which adeptly represents intricate two-dimensional curves using an array capturing the exterior angles at each vertex, this study introduces an innovative 3D encoding method tailored for polygonal curves. This 3D encoding employs parallel slope and torsion chains, ensuring invariance to common transformations like translations, rotations, and uniform scaling, while also demonstrating robustness against mirror imaging and variable starting points. A hallmark feature of this method is its ability to compute tortuosity, a descriptor of curve complexity or winding nature. By applying this technique to biomedical engineering, we delved into the flagellar beat patterns of human sperm. These insights underscore the versatility of our 3D encoding across diverse computer vision applications.},
  archive      = {J_PAAA},
  author       = {Bribiesca-Sánchez, Andrés and Guzmán, Adolfo and Montoya, Fernando and Díaz-Guerrero, Dan S. and Hernández, Haydeé O. and Hernández-Herrera, Paul and Darszon, Alberto and Corkidi, Gabriel and Bribiesca, Ernesto},
  doi          = {10.1007/s10044-024-01286-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A three-dimensional extension of the slope chain code: Analyzing the tortuosity of the flagellar beat of human sperm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-mol: A novel contrastive learning framework for molecular
property prediction with 3D information. <em>PAAA</em>, <em>27</em>(3),
1–14. (<a href="https://doi.org/10.1007/s10044-024-01287-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction, crucial for early drug candidate screening and optimization, has seen advancements with deep learning-based methods. While deep learning-based methods have advanced considerably, they often fall short in fully leveraging 3D spatial information. Specifically, current molecular encoding techniques tend to inadequately extract spatial information, leading to ambiguous representations where a single one might represent multiple distinct molecules. Moreover, existing molecular modeling methods focus predominantly on the most stable 3D conformations, neglecting other viable conformations present in reality. To address these issues, we propose 3D-Mol, a novel approach designed for more accurate spatial structure representation. It deconstructs molecules into three hierarchical graphs to better extract geometric information. Additionally, 3D-Mol leverages contrastive learning for pretraining on 20 million unlabeled data, treating their conformations with identical topological structures as weighted positive pairs and contrasting ones as negatives, based on the similarity of their 3D conformation descriptors and fingerprints. We compare 3D-Mol with various state-of-the-art baselines on 7 benchmarks and demonstrate our outstanding performance.},
  archive      = {J_PAAA},
  author       = {Kuang, Taojie and Ren, Yiming and Ren, Zhixiang},
  doi          = {10.1007/s10044-024-01287-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {3D-mol: A novel contrastive learning framework for molecular property prediction with 3D information},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoking-YOLOv8: A novel smoking detection algorithm for
chemical plant personnel. <em>PAAA</em>, <em>27</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01288-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to address the challenges of detecting smoking behavior among workers in chemical plant environments. Smoking behavior is difficult to discern in images, with the cigarette occupying only a small pixel area, compounded by the complex background of chemical plants. Traditional models struggle to accurately capture smoking features, leading to feature loss, reduced recognition accuracy, and issues like false positives and missed detections. To overcome these challenges, we have developed a smoking behavior recognition method based on the YOLOv8 model, named Smoking-YOLOv8. Our approach introduces an SD attention mechanism that focuses on the smoking areas within images. By aggregating information from different positions through weighted averaging, it effectively manages long-distance dependencies and suppresses irrelevant background noise, thereby enhancing detection performance. Furthermore, we utilize Wise-IoU as the regression loss for bounding boxes, along with a rational gradient distribution strategy that prioritizes samples of average quality to improve the model’s precision in localization. Finally, the introduction of SPPCSPC and PConv modules in the neck section of the network allows for multi-faceted feature extraction from images, reducing redundant computation and memory access, and effectively extracting spatial features to balance computational load and optimize network architecture. Experimental results on a custom dataset of smoking behavior in chemical plants show that our model outperforms the standard YOLOv8 model in mean Average Precision (mAP@0.5) by 6.18%, surpassing other mainstream models in overall performance.},
  archive      = {J_PAAA},
  author       = {Wang, Zhong and Liu, Yi and Lei, Lanfang and Shi, Peibei},
  doi          = {10.1007/s10044-024-01288-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Smoking-YOLOv8: A novel smoking detection algorithm for chemical plant personnel},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EdgeNet: A low-power image recognition model based on small
sample information. <em>PAAA</em>, <em>27</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01289-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep convolutional neural networks that rely on large datasets typically require images with high resolution and deep neural network models trained and called upon to improve accuracy of image recognition and classification. It is needed to use lightweight model to adapt to such low-power devices. However, lightweight small models are limited in their ability to classify and recognize small-sized images with low-resolution and are constrained by the number of parameters in the model and unable to perform deep-level feature extraction, since the low-resolution indicates small sample information. In the intelligent interaction in digital media, capturing, storing, transmitting, and computing high-resolution, high-precision images incur high power consumption and operating costs. When deploying an image recognition system on the client-side of IoT devices, it is difficult to meet the hardware requirements of high storage space and fast computation speed. It is also challenging to directly use high-resolution image data for model fine-tuning and training, and the size and parameter updates of the model are also limited by the storage and operating capacity of the hardware facilities. We proposed a low-power image recognition framework consists data pre-processing part and lightweight modeling architecture part. The data pre-processing method for image data based on an Auto-Encoder that filters R, G, B color channel data using a resolution filter to realize data compression, that is Downscaling large input data to a smaller size, thus to address the limitations of low-power deep learning model deployment and training. Based on the resolution filter, a channel normalization method is proposed to perform batch normalization on each channel dimension to encode the original image data at the same size and improve the mean squared error discrimination of the image data. And the lightweight model uses a depth-separable convolutional neural network and two kinds of blocks: one with batch normalization and the other without, EdgeNet. The architecture makes it possible to deploy more suitable for IoT device. The proposed framework achieves only a small precision loss within permission, but improves the forward inference speed of the model, and reduce the memory storage to 8.7 MB.},
  archive      = {J_PAAA},
  author       = {Bao, Weiyue and Zhang, Hong and Ding, Yaoyao and Shen, Fangzhou and Li, Liujun},
  doi          = {10.1007/s10044-024-01289-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {EdgeNet: A low-power image recognition model based on small sample information},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine grained dual level attention mechanisms with spacial
context information fusion for object detection. <em>PAAA</em>,
<em>27</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01290-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For channel and spatial feature map C×W×H in object detection task, its information fusion usually relies on attention mechanism, that is, all C channels and the entire space W×H are all compressed respectively via average/max pooling, and then their attention weight masks are obtained based on correlation calculation. This coarse-grained global operation ignores the differences among multiple channels and diverse spatial regions, resulting in inaccurate attention weights. In addition, how to mine the contextual information in the space W×H is also a challenge for object recognition and localization. To this end, we propose a Fine-Grained Dual Level Attention Mechanism joint Spacial Context Information Fusion module for object detection (FGDLAM&amp;SCIF). It is a cascaded structure, firstly, we subdivide the feature space W×H into n (optimized as n = 4 in experiments) subspaces and construct a global adaptive pooling and one-dimensional convolution algorithm to effectively extract the feature channel weights on each subspace respectively. Secondly, the C feature channels are divided into n (n = 4) sub-channels, and then a multi-scale module is constructed in the feature space W×H to mine context information. Finally, row and column coding is used to fuse them orthogonally to obtain enhanced features. This module is embeddable, which can be transplanted into any object detection network, such as YOLOv4/v5, PPYOLOE, YOLOX and MobileNet, ResNet as well. Experiments are conducted on the MS COCO 2017 and Pascal VOC 2007 datasets to verify its effectiveness and good portability.},
  archive      = {J_PAAA},
  author       = {Deng, Haigang and Wang, Chuanxu and Li, Chengwei and Hao, Zhang},
  doi          = {10.1007/s10044-024-01290-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fine grained dual level attention mechanisms with spacial context information fusion for object detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank tensor completion via nonlocal self-similarity
regularization and orthogonal transformed tensor schatten-p norm.
<em>PAAA</em>, <em>27</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01291-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank tensor completion (LRTC) has become more and more popular in the field of tensor completion. Because solving the tensor rank minimization is NP-hard, extensive surrogate norms of tensor rank have been proposed successively. Among these norms, the innovative nonconvex orthogonal transformed tensor Schatten-p norm (OTT $$S_{p}$$ ) can better capture the low-rank property of tensor than most competitive norms. However, the OTT $$S_{p}$$ method solely depends on the global low-rank prior and ignores the importance of the nonlocal similar structures, which play a significant role in the tensor data processing. In this paper, to address the defect of the OTT $$S_{p}$$ method, we propose a novel LRTC model based on nonlocal self-similarity (NSS) regularization, which combines NSS regularization with the OTT $$S_{p}$$ . As a nonlocal prior, NSS can preserve the nonlocal similar details, so the introduction of NSS regularization contributes to promoting the final inpainting performance. Therefore, our proposed model is capable of further conserving nonlocal self-similarities based on the global low-rankness. Moreover, the alternating direction method of multipliers is adopted to solve our proposed model. Experimental results on color images, grey-scale videos, and multispectral images demonstrate the superiority of our proposed method compared with other existing state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Liu, Jiahui and Zhu, Yulian and Tian, Jialue},
  doi          = {10.1007/s10044-024-01291-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Low-rank tensor completion via nonlocal self-similarity regularization and orthogonal transformed tensor schatten-p norm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep graph kernel-based time series classification
algorithm. <em>PAAA</em>, <em>27</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01292-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data are sequences of values that are obtained by sampling a signal at a fixed frequency, and time series classification algorithms distinguish time series into different categories. Among many time series classification algorithms, subseries-based algorithms have received widespread attention because of their high accuracy and low computational complexity. However, subseries-based algorithms consider the similarity of subseries only by shape and ignore semantic similarity. Therefore, the purpose of this paper is to determine how to solve the problem that subseries-based time series classification algorithms ignore the semantic similarity between subseries. To address this issue, we introduce the deep graph kernel technique to capture the semantic similarity between subseries. To verify the performance of the method, we test the proposed algorithm on publicly available datasets from the UCR repository and the experimental results prove that the deep graph kernel has an important role in enhancing the accuracy of the algorithm and that the proposed algorithm performs quite well in terms of accuracy and has a considerable advantage over other representative algorithms.},
  archive      = {J_PAAA},
  author       = {Yu, Mengping and Huang, Huan and Hou, Rui and Ma, Xiaoxuan and Yuan, Shuai},
  doi          = {10.1007/s10044-024-01292-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A deep graph kernel-based time series classification algorithm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative binary pattern descriptor for face
recognition. <em>PAAA</em>, <em>27</em>(3), 1–25. (<a
href="https://doi.org/10.1007/s10044-024-01293-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among several local descriptors invented in literature, the local binary pattern (LBP) is the prolific one. Despite its advantages like low computational complexity and monotonic gray invariance property, there are various demerits are observed in LBP and these are limited spatial patch, high dimension feature, noisy thresholding function and un-affective in harsh illumination variations. To overcome these issues presented work introduces the novel local descriptor called as discriminative binary pattern (DBP). Precisely two descriptors are introduced under DBP so-called Radial orthogonal binary pattern (ROBP) and radial variance binary pattern (RVBP). In former proposed descriptor, for neighborhood comparison, the center pixel is replaced by mean of medians computed from [orthogonal pixels + center pixel] of two 3 × 3 pixel window, formed from radius S1 and S2 of the 5 × 5 image patch. In latter proposed descriptor, the radial variances generated from 8 pair of two pixels are utilized for comparison with their mean value. In case of the both proposed descriptors, the sub-region wise histograms are extracted and fused to develop the entire feature size. Further the feature length of ROBP and RVBP are merged to form the size of the DBP descriptor. The compression is conducted by principal component analysis (PCA) and Fishers linear discriminant analysis). For matching support vector machines is used. Experiments conducted on 8 benchmark datasets reveals the effectiveness of the proposed DBP as compared to the other state of art benchmark methods.},
  archive      = {J_PAAA},
  author       = {Karanwal, Shekhar},
  doi          = {10.1007/s10044-024-01293-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Discriminative binary pattern descriptor for face recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting person ReID feature extraction via dynamic
convolution. <em>PAAA</em>, <em>27</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01294-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extraction of discriminative features is crucial in person re-identification (ReID) which aims to match a query image of a person to her/his images, captured by different cameras. The conventional deep feature extraction methods on ReID employ CNNs with static convolutional kernels, where the kernel parameters are optimized during the training and remain constant in the inference. This approach limits the network&#39;s ability to model complex contents and decreases performance, particularly when dealing with occlusions or pose changes. In this work, to improve the performance without a significant increase in parameter size, we present a novel approach by utilizing a channel fusion-based dynamic convolution backbone network, which enables the kernels to change adaptively based on the input image, within two existing ReID network architectures. We replace the backbone network of two ReID methods to investigate the effect of dynamic convolution on both simple and complex networks. The first one called Baseline, is a simpler network with fewer layers, while the second, CaceNet represents a more complex architecture with higher performance. Evaluation results demonstrate that both of the designed dynamic networks improve identification accuracy compared to the static counterparts. A significant increase in accuracy is reported under occlusion tested on Occluded-DukeMTMC. Moreover, our approach achieves a performance comparable to the state-of-the-art on Market1501, DukeMTMC-reID, and CUHK03 with a limited computational load. These findings validate the effectiveness of the dynamic convolution in enhancing the person ReID networks and push the boundaries of performance in this domain.},
  archive      = {J_PAAA},
  author       = {Akbaba, Elif Ecem and Gurkan, Filiz and Gunsel, Bilge},
  doi          = {10.1007/s10044-024-01294-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Boosting person ReID feature extraction via dynamic convolution},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual model knowledge distillation for industrial anomaly
detection. <em>PAAA</em>, <em>27</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01295-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection holds significant importance in large-scale industrial manufacturing. Recent methods have capitalized on the benefits of employing a classifier pretrained on natural images to extract representative features from specific layers, which are subsequently processed using various techniques. Notably, memory bank-based methods, which have demonstrated exceptional accuracy, often incur a trade-off in terms of latency, posing a challenge in real-time industrial applications where prompt anomaly detection and response are crucial. Indeed, alternative approaches such as knowledge distillation and normalized flow have demonstrated promising performance in unsupervised anomaly detection while maintaining low latency. In this paper, we aim to revisit the concept of knowledge distillation in the context of unsupervised anomaly detection, emphasizing the significance of feature selection. By employing distinctive features and leveraging different models, we intend to highlight the importance of carefully selecting and utilizing relevant features specifically tailored for the task of anomaly detection. This article presents a novel approach for anomaly detection, which employs dual model knowledge distillation and incorporates various types of semantic information by leveraging high and low-level semantic information.},
  archive      = {J_PAAA},
  author       = {Thomine, Simon and Snoussi, Hichem},
  doi          = {10.1007/s10044-024-01295-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual model knowledge distillation for industrial anomaly detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A judicious way to restore random impulse noise using
iterative weighted total variation diffusion technique. <em>PAAA</em>,
<em>27</em>(3), 1–24. (<a
href="https://doi.org/10.1007/s10044-024-01296-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various types of pixel candidates are available in the literature to replace impulse noise after effective detection. However, using them in the correct location and preserving the signal content, structural similarity, and image details is a task that draws attention, especially in a highly corrupted image. Non-linear Diffusion-based restoration is an efficient solution since it can iteratively update corrupted pixels without diffusing the edge. This work assigns the iterative weighted total variation diffusion technique only for the possibly noisy pixels in high noise ratio processing windows where the windows are pre-classified as low or high noise ratio by a custom CNN classifier. The work, called as CNN-based locally adapting filter (CNN-LAF), can achieve a high structural similarity of .9167 by maintaining a PSNR of 24.01 dB at a 0.8 noise ratio.},
  archive      = {J_PAAA},
  author       = {Pritamdas, Keisham},
  doi          = {10.1007/s10044-024-01296-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A judicious way to restore random impulse noise using iterative weighted total variation diffusion technique},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computer-aided diagnosis of alzheimer’s disease and
neurocognitive disorders with multimodal bi-vision transformer (BiViT).
<em>PAAA</em>, <em>27</em>(3), 1–35. (<a
href="https://doi.org/10.1007/s10044-024-01297-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive disorders affect various cognitive functions that can have a substantial impact on individual’s daily life. Alzheimer’s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer’s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.},
  archive      = {J_PAAA},
  author       = {Shah, S. Muhammad Ahmed Hassan and Khan, Muhammad Qasim and Rizwan, Atif and Jan, Sana Ullah and Samee, Nagwan Abdel and Jamjoom, Mona M.},
  doi          = {10.1007/s10044-024-01297-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-35},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Computer-aided diagnosis of alzheimer’s disease and neurocognitive disorders with multimodal bi-vision transformer (BiViT)},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel two-stage omni-supervised face clustering algorithm.
<em>PAAA</em>, <em>27</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01298-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face clustering has applications in organizing personal photo album, video understanding and automatic labeling of data for semi-supervised learning. Many existing methods cannot cluster millions of faces. They are either too slow, inaccurate, or need a lot memory. In our paper, we proposed a two stage unsupervised clustering algorithm which can cluster millions of faces in minutes. A rough clustering using greedy Transitive Closure (TC) algorithm to separate the easy to locate clusters, then a more precise non-greedy clustering algorithm is used to split the clusters into smaller clusters. We also developed a set of omni-supervised transformations that can produce multiple embeddings using a single trained model as if there are multiple models trained. These embeddings are combined using simple averaging and normalization. We carried out extensive experiments with multiple datasets of different sizes comparing with existing state of the art clustering algorithms to show that our clustering algorithm is robust to differences between datasets, efficient and outperforms existing methods. We also carried out further analysis on number of singleton clusters and variations of our model using different non-greedy clustering algorithms. We did trained our semi-supervised model using the cluster labels and shown that our clustering algorithm is effective for semi-supervised learning.},
  archive      = {J_PAAA},
  author       = {Tan, Sing Kuang and Wang, Xiu},
  doi          = {10.1007/s10044-024-01298-5},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel two-stage omni-supervised face clustering algorithm},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring methods for the generation of visual
counterfactuals in the latent space. <em>PAAA</em>, <em>27</em>(3), 1–9.
(<a href="https://doi.org/10.1007/s10044-024-01299-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of eXplainable Artificial Intelligence (XAI), the generation of counterfactuals is a promising method for human-interpretable explanations. A counterfactual explanation describes a causal situation in the form: “If X had not occurred, Y would not have occurred”. In this work, we study the generation of visual counterfactuals in the latent space for deep learning image classification models. We explore how to adapt the training environment to facilitate the generation of counterfactuals, combining ideas coming from different fields such as multitasking or generative learning, with the aim of developing more interpretable models. We study well-known counterfactual methods and how to apply them in the latent space. Furthermore, we propose a new way of generating counterfactuals working in the latent space and compare it with the other studied approaches, achieving competitive results.},
  archive      = {J_PAAA},
  author       = {Morales, David and Cuéllar, Manuel P. and Morales, Diego P.},
  doi          = {10.1007/s10044-024-01299-4},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exploring methods for the generation of visual counterfactuals in the latent space},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved classification diagnosis approach for cervical
images based on deep neural networks. <em>PAAA</em>, <em>27</em>(3),
1–14. (<a href="https://doi.org/10.1007/s10044-024-01300-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to enhance the speed and performance of cervical diagnosis, we propose an improved Residual Network (ResNet) by combining pyramid convolution with depth-wise separable convolution to obtain the high-quality cervical classification. Since most of cervical images from patients are not in the center of colposcopy images, we devise the segmentation and extraction algorithm of the center movement of the region of interest (ROI), which will further enhance the classification performance. Extensive experiments indicate that our model can not only achieve the lightweight network model, but also fulfil the classification prediction, such as for three-classification of cervical lesions, the classification accuracy is as high as 91.29 $$\%$$ , the precision is 89.70 $$\%$$ , the sensitivity is 88.75 $$\%$$ , the specificity is 94.98 $$\%$$ , the rate of missed diagnosis is 11.25 $$\%$$ and the rate of misdiagnosis is 5.02 $$\%$$ . Finally, after dividing the colposcopy images into four categories, it is shown that our results are still better than those obtained from many previous works as far as the cervical image classification is concerned. The current work can not only assist doctors to quickly diagnose cervical diseases, but also the classification performance can meet some clinical requirements in practice.},
  archive      = {J_PAAA},
  author       = {Wang, Juan and Zhao, Mengying and Xia, Chengyi},
  doi          = {10.1007/s10044-024-01300-0},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An improved classification diagnosis approach for cervical images based on deep neural networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SRU-net: A novel spatiotemporal attention network for sclera
segmentation and recognition. <em>PAAA</em>, <em>27</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01301-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting sclera images for effective recognition under non-cooperative conditions poses a significant challenge due to the prevalent noise. While U-Net-based methods have shown success, their limitations in accurately segmenting objects with varying shapes necessitate innovative approaches. This paper introduces the spatiotemporal residual encoding and decoding network (SRU-Net), featuring multi-spatiotemporal feature integration (Ms-FI) modules and attention-pool mechanisms to enhance segmentation accuracy and robustness. Ms-FI modules within SRU-Net’s encoders and decoders identify salient feature regions and prune responses, while attention-pool modules improve segmentation robustness. To assess the proposed SRU-Net, we conducted experiments using six datasets, employing precision, recall, and F1-score metrics. The experimental results demonstrate the superiority of SRU-Net over state-of-the-art methods. Specifically, SRU-Net achieves F1-score values of 94.58%, 98.31%, 98.49%, 97.52%, 95.3%, 97.47%, and 93.11% for MSD, MASD, SVBPI, MASD+MSD, UBIRIS.v1, UBIRIS.v2, and MICHE, respectively. Further evaluation in recognition tasks, with metrics such as AUC, EER, VER@0.1%FAR, and VER@1%FAR considered for the six datasets. The proposed pipeline, comprising SRU-Net and auto encoders (AE), outperforms previous research for all datasets. Particularly noteworthy is the comparison of EER, where SRU-Net + AE exhibits the best recognition results, achieving an EER of 9.42%, 3.81%, and 5.73% for MSD, MASD, and MICHE datasets, respectively.},
  archive      = {J_PAAA},
  author       = {Mashayekhbakhsh, Tara and Meshgini, Saeed and Rezaii, Tohid Yousefi and Makouei, Somayeh},
  doi          = {10.1007/s10044-024-01301-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SRU-net: A novel spatiotemporal attention network for sclera segmentation and recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical global and local transformer for pain
estimation with facial expression videos. <em>PAAA</em>, <em>27</em>(3),
1–13. (<a href="https://doi.org/10.1007/s10044-024-01302-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic pain intensity estimation from facial expression analysis has important applications in medical and healthcare areas. Most of the existing works tend to directly transfer the typical face recognition models to pain estimation task, which may not obtain good performances because the facial expression of pain is spontaneous with subtle facial variations. Pain estimation from facial video is still challenging because it relies on modeling semantic parts and extraction of fine-grained and dynamic features. In this study, we propose a hierarchical global and local transformer (HGLT) model for pain estimation from facial expression videos. HGLT model consists of an image frame embedding subnetwork and a temporal embedding subnetwork for extraction of spatio-temporal features. In the frame embedding subnetwork, we propose a multi-head local attention mechanism to extract the local fine-grained features related to micro variations of pain, followed by a hierarchical self-attention pooling to integrate the global and local features. In the temporal embedding subnetwork, a transformer encoder with temporal attention is proposed to model the temporal relationships of video frames and capture the dynamic facial variations. A correlation loss is proposed to alleviate the problem of long-tailed imbalance in the distribution of pain intensities. Our proposed method is tested with UNBC-McMaster Shoulder Pain, BioVid Heart Pain dataset, and DAiSEE dataset. Experimental results indicate that our method achieves competitive performances compared with the state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Liu, Hongrui and Xu, Haochen and Qiu, Jinheng and Wu, Shizhe and Liu, Manhua},
  doi          = {10.1007/s10044-024-01302-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Hierarchical global and local transformer for pain estimation with facial expression videos},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconstructed semantic relative distance and global and
local attention fusion network for aspect-based sentiment analysis.
<em>PAAA</em>, <em>27</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01303-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis aims to analyze the sentiment tendencies towards a specific aspect within a given sentence. As a fine-grained sentiment classification task, it plays an integral role in detecting users’ comments. Recent studies have used relational labels in dependency trees to focus on aspect items in local contexts. However, opinion words in context are affected by irrelevant dependency labels, which can interfere with their accurate evaluation. Moreover, the combination of feature sequences with long and short-distance dependencies has not been thoroughly explored. To this end, we propose a reconstructed semantic relative distance and global and local attention fusion network (RAGN), which can extract syntactic and semantic features and fully fusing feature vectors from multiple modules. Firstly, the dependency distance in the context dynamic weights layer is replaced with the reconstructed semantic relative distance, which is recalculated based on the relational labels in a syntactic dependency tree rooted in aspects. Secondly, a global and local attention fusion network captures long-distance dependencies and emphasizes parts of sentences with salient sequence features. Ultimately, combining the aspect sentiment classification task (ASC) and the aspect entity recognition task (AER) and utilizing AER as an auxiliary task facilitates the final classification of ASC. Experimental results on three publicly available datasets verify the superiority, effectiveness, and robustness of the proposed model.},
  archive      = {J_PAAA},
  author       = {Huan, Hai and Chen, Yindi and He, Zichen},
  doi          = {10.1007/s10044-024-01303-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Reconstructed semantic relative distance and global and local attention fusion network for aspect-based sentiment analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general seeds-counting pipeline using deep-learning model.
<em>PAAA</em>, <em>27</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01304-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel Seeds-Counting pipeline harnessing deep learning algorithms to facilitate the automation of yield prediction prior to harvesting, a crucial component of the breeding process. Unlike existing methods that often cater to a single seed species or those with similar shapes, our approach is capable of accurately estimating the number of seeds across a diverse range of species. The pipeline incorporates a classification network for seed image categorization, along with object detection models specifically tailored to accommodate the morphologies of different seeds. By integrating a seed classifier, three distinct seed detectors, and post-processing filters, our method not only showcases exceptional accuracy but also exhibits robust generalization capabilities across various conditions. Demonstrating an error rate of less than 2% in the test set and achieving accuracy rates exceeding 97% in the extended set, the proposed pipeline offers a viable and efficient solution for high-throughput phenotyping and precision agriculture, effectively overcoming the challenges posed by the diverse morphologies of seeds.},
  archive      = {J_PAAA},
  author       = {Pun, Zeonlung and Tian, Xinyu and Gao, Shan},
  doi          = {10.1007/s10044-024-01304-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A general seeds-counting pipeline using deep-learning model},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSU-net: The multi-scale supervised u-net for image splicing
forgery localization. <em>PAAA</em>, <em>27</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01305-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing forgery, that is, copying some parts of an image into another image, is one of the frequently used tampering methods in image forgery. As a research hotspot in recent years, deep learning has been used in image forgery detection. However, current deep learning methods have two drawbacks: first, they are too simple in feature fusion; second, they rely only on a single cross-entropy loss as the loss function, leading to models prone to overfitting. To address these issues, a image splicing forgery localization method based on multi-scale supervised U-shaped network, named MSU-Net, is proposed in this paper. First, a triple-stream feature extraction module is designed, which combines the noise view and edge information of the input image to extract semantic-related and semantic-agnostic features. Second, a feature hierarchical fusion mechanism is proposed that introduces a channel attention mechanism layer by layer to perceive multi-level manipulation trajectories, avoiding the loss of information in semantic-related and semantic-agnostic shallow features during the convolution process. Finally, a strategy for multi-scale supervision is developed, a boundary artifact localization module is designed to compute the edge loss, and a contrastive learning module is introduced to compute the contrastive loss. Through extensive experiments on several public datasets, MSU-Net demonstrates high accuracy in localizing tampered regions and outperforms state-of-the-art methods. Additional attack experiments show that MSU-Net exhibits good robustness against Gaussian blur, Gaussian noise, and JPEG compression attacks. Besides, MSU-Net is superior in terms of model complexity and localization speed.},
  archive      = {J_PAAA},
  author       = {Yu, Hao and Su, Lichao and Dai, Chenwei and Wang, Jinli},
  doi          = {10.1007/s10044-024-01305-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSU-net: The multi-scale supervised U-net for image splicing forgery localization},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal generative explainers using counterfactual inference:
A case study on the morpho-MNIST dataset. <em>PAAA</em>, <em>27</em>(3),
1–14. (<a href="https://doi.org/10.1007/s10044-024-01306-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose leveraging causal generative learning as an interpretable tool for explaining image classifiers. Specifically, we present a generative counterfactual inference approach to study the influence of visual features (pixels) as well as causal factors through generative learning. To this end, we first uncover the most influential pixels on a classifier’s decision by computing both Shapely and contrastive explanations for counterfactual images with different attribute values. We then establish a Monte Carlo mechanism using the generator of a causal generative model in order to adapt Shapley explainers to produce feature importances for the human-interpretable attributes of a causal dataset. This method is applied to the case where a classifier has been trained exclusively on the images of the causal dataset. Finally, we present optimization methods for creating counterfactual explanations of classifiers by means of counterfactual inference, proposing straightforward approaches for both differentiable and arbitrary classifiers. We exploit the Morpho-MNIST causal dataset as a case study for exploring our proposed methods for generating counterfactual explanations. However, our methods are applicable also to other causal datasets containing image data. We employ visual explanation methods from the OmnixAI open source toolkit to compare them with our proposed methods. By employing quantitative metrics to measure the interpretability of counterfactual explanations, we find that our proposed methods of counterfactual explanation offer more interpretable explanations compared to those generated from OmnixAI. This finding suggests that our methods are well-suited for generating highly interpretable counterfactual explanations on causal datasets.},
  archive      = {J_PAAA},
  author       = {Taylor-Melanson, Will and Sadeghi, Zahra and Matwin, Stan},
  doi          = {10.1007/s10044-024-01306-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Causal generative explainers using counterfactual inference: A case study on the morpho-MNIST dataset},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SMRU-net: Skin disease image segmentation using
channel-space separate attention with depthwise separable convolutions.
<em>PAAA</em>, <em>27</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01307-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin disease image segmentation faces two major challenges: the complex and varied lesion morphology and the presence of interfering image backgrounds. To address these difficulties in skin disease image segmentation, we propose a Residual U-Net architecture with Channel-Space Separate Attention based on depthwise separable convolutions. The multi-scale residual U-Net modules in the encoder efficiently capture multi-scale texture information in lesions and backgrounds within a single stage, overcoming the limitations of U-Net in extracting just local features. The introduction of ConvMixer Block for global contextual modeling contributes to suppress complex background interference and enhances the overall understanding of lesion morphology. Additionally, we employ a Channel-Space Separate Attention mechanism with depthwise separable convolutions(CSSA-DSC) for feature fusion, effectively addressing the limited expressiveness issue associated with U-Net’s direct skip-connection concatenation. Experimental results on the PH2, ISIC 2017, and ISIC 2018 datasets demonstrate our method’s strong multi-scale modeling and feature expression capabilities.},
  archive      = {J_PAAA},
  author       = {Liu, Shangwang and Wang, Peixia and Lin, Yinghai and Zhou, Bingyan},
  doi          = {10.1007/s10044-024-01307-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SMRU-net: Skin disease image segmentation using channel-space separate attention with depthwise separable convolutions},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on IbPRIA 2023.
<em>PAAA</em>, <em>27</em>(3), 1–2. (<a
href="https://doi.org/10.1007/s10044-024-01308-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PAAA},
  author       = {Gallego, Antonio Javier and Marín-Jiménez, Manuel J. and Justo, Raquel and Oliveira, Hélder and Pertusa, Antonio},
  doi          = {10.1007/s10044-024-01308-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-2},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Guest editorial: Special issue on IbPRIA 2023},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of dynamic bi-clustering based on improved
genetic algorithm for microarray data. <em>PAAA</em>, <em>27</em>(3),
1–22. (<a href="https://doi.org/10.1007/s10044-024-01309-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nature of microarray data, the analysis of genes/features for disease diagnosis is a challenging task. Generally, the data comes in the form of a 2D matrix, where the row represents the genes and the column indicates the various conditions. Bi-clustering is an emerging technique that can efficiently reveal patterns of genes. It can perform simultaneously with a subset of genes and conditions. Inspired by this, dynamic bi-clustering based on an improved genetic algorithm (GA) is proposed. The chromosomes are efficiently designed. In addition, the fitness function is derived by considering multiple conflicting objectives to measure the quality of a cluster. A novel mutation is designed by the correlation technique. The crossover and mutation rates are dynamically changed. The obtained outcomes of the proposed approach are compared with the various existing approaches, such as traditional GA, the dynamic dame parallel GA, the evolutionary local search algorithm, bi-phase evolutionary searching, and the evolutionary bi-clustering algorithm. Further, statistical tests such as the analysis of variance and Friedman test are executed to show the significance of the proposed model. A biological analysis is also performed.},
  archive      = {J_PAAA},
  author       = {Ram, Pintu Kumar and Kuila, Pratyay},
  doi          = {10.1007/s10044-024-01309-5},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Optimization of dynamic bi-clustering based on improved genetic algorithm for microarray data},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrackYOLO: Towards efficient dam crack detection for
underwater scenes. <em>PAAA</em>, <em>27</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01310-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack is one of the main factors threatening the safety of the dam. Automatic image object detection is the main way of underwater dam crack detection. However, the traditional methods have problems with low crack detection speed, high false alarm rate, and poor robustness. In addition, the existing methods cannot get a satsifying detection result with a high detection speed. To solve these problems, we propose an efficient dam crack detection method for underwater scenes, called CrackYOLO. Firstly, to better integrate the multi-scale features without incurring excessive computational costs, we propose a feature fusion module in CrackYOLO. Next, we re-design the skip-connection in the network to get better features, compressing the overall model parameters. Then, we propose a feature extraction module called Res2C3, which combines semantic and location information. After that, we proposed a BCAtt to make features focus on both channel and location information. Finally, according to the characteristics of dam underwater crack images, we use a genetic algorithm to select the best value of hyperparameters of the model. The experimental results show that the proposed method detects underwater dam cracks robustly with less computational cost. Our CrackYOLO can get 94.3% mAP[0.5] and 151 FPS in underwater crack detection task which can achieve a real-time detection in practice.},
  archive      = {J_PAAA},
  author       = {Shi, Pengfei and Shao, Shen and Fan, Xinnan and Xin, Yuanxue and Zhou, Zhongkai and Cao, Pengfei and Li, Xinyu and Zhu, Sisi},
  doi          = {10.1007/s10044-024-01310-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CrackYOLO: Towards efficient dam crack detection for underwater scenes},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted least squares twin support vector machine based on
density peaks. <em>PAAA</em>, <em>27</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01311-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The least-squares twin support vector machine integrates all samples equally into the quadratic programming problem to calculate the optimal classification hyperplane, and does not distinguish the noise points in the samples, which causes the model to be sensitive to noise points and affected by the overlapping samples of positive and negative classes, and reduces the classification accuracy. To address the above problems, this paper proposes a weighted least squares twin support vector machine based on density peaks. Firstly, the algorithm combines the idea of density peaks to construct a new density weighting strategy, which gives a suitable weight value to this sample through the local density of the sample as well as the relative distance together to highlight the importance of the local center and reduce the influence of noise on the model; secondly, the separability between classes is defined according to the local density matrix, which reduces the influence of positive and negative class overlapping samples on the model and enhances the inter-class separability of the model; finally, an extensive weighting strategy is used in the model to assign weight values to both classes of samples to improve the robustness of the model to cross samples. The comparison experiments on the artificial dataset and the UCI dataset show that the algorithm in this paper can assign appropriate weights to different samples to improve the classification accuracy, while the experiments on the MNIST dataset demonstrate the effectiveness of the algorithm in this paper for real classification problems.},
  archive      = {J_PAAA},
  author       = {Lv, Li and He, Zhipeng and Chen, Juan and Duan, Fayang and Qiu, Shenyu and Pan, Jeng-Shyang},
  doi          = {10.1007/s10044-024-01311-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Weighted least squares twin support vector machine based on density peaks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of convolutional neural network-based layer operator
for image denoising using fourth-order PDE. <em>PAAA</em>,
<em>27</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01313-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images often suffer from speckle noise, which complicates processing. Conventional denoising methods are not effective in preserving the details of the images. This study proposes a novel approach for noise reduction in images, preserving complex structures. The process uses a diffusion network based on a fourth-order filtering partial differential equation (PDE). The PDE is transformed into a system of differential equations through finite difference methods, and a filtering layer operator is defined based on the discretized PDE and M-layer convolutional neural network (CNN). The CNN calculates denoising filters for the image at layer $$k-1$$ , which are then applied to the diffusion network layer operator to update the appearance. Our proposed method has been tested on both BSD68 and ultrasound image datasets and produced improved denoising results compared to traditional methods.},
  archive      = {J_PAAA},
  author       = {Lakra, Mahima},
  doi          = {10.1007/s10044-024-01313-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Design of convolutional neural network-based layer operator for image denoising using fourth-order PDE},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lsf-rdd: A local sensing feature network for road damage
detection. <em>PAAA</em>, <em>27</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01314-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential object detection application, road damage detection aims to identify and mark road damage. Timely maintenance of detected damage can improve road safety. However, the proportions of damage area of the image is very diffident for the variety of the road damage textures and shapes. Additionally it is a challenge to localize the road damage accurately for the blurring of the damaged regions caused by the external environmental factors. In this study, we propose a Road Damage Detector with a Local Sensing Feature Network (LSF-RDD), which constructs a Local Sensing Feature Network (LSF-Net) as a neck to fuse multi-scale features extracted from the backbone network and can focus on the location of the damaged area. First, the CSP-Darknet53 backbone network extracts the feature maps of three scales layer-by-layer from the input images. Second, these three feature maps are input into LSF-Net for multi-scale feature fusion to generate three local feature representations. LSF-Net comprises four interconnected blocks, enabling top-down and bottom-up feature fusion. Feature maps from the backbone perform multi-scale feature fusion through connections between different blocks. Finally, three local feature representations are sent into the detection head for detection. Experiments show that LSF-RDD performs well on the adopted datasets, especially on the China_motorbike dataset of RDD2022, with mAP@0.5 reaching 94.4%.},
  archive      = {J_PAAA},
  author       = {He, Qihan and Li, Zhongxu and Yang, Wenyuan},
  doi          = {10.1007/s10044-024-01314-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Lsf-rdd: A local sensing feature network for road damage detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight weld defect recognition algorithm based on
convolutional neural networks. <em>PAAA</em>, <em>27</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01315-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a lightweight weld defect-recognition algorithm based on a convolutional neural network that is appropriate for weld defect recognition in industrial welding. Specifically, the developed scheme relies on the original SqueezeNet model. However, we improve the fire module to reduce the model’s parameter cardinality, introduce the ECA module to strengthen the learning of feature channels and improve the feature extraction ability of the overall model. The experimental results highlight that our algorithm’s average recognition rate on the overall defects of welding depressions, welding holes, and welding burrs reaches 97.50%. Note that although our model requires substantially fewer parameters, its recognition effect is significantly improved. Our algorithm’s feasibility is verified on the test data and challenged against current weld defect identification algorithms, demonstrating its enhanced identification role and application prospect.},
  archive      = {J_PAAA},
  author       = {Zhao, Wenjie and Li, Dan and Xu, Feihu},
  doi          = {10.1007/s10044-024-01315-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A lightweight weld defect recognition algorithm based on convolutional neural networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-homogeneous nearest neighbor-driven discriminant graph
coupled nonnegative matrix factorization for low-resolution image
recognition. <em>PAAA</em>, <em>27</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01316-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coupled nonnegative matrix factorization method can utilize the information in high-resolution images to assist in the extraction of local semantic features of low-resolution (LR) images. However, since the supervised information is not fully considered in the existing methods, the discriminative performance of the extracted features is limited. In this paper, k-homogeneous nearest neighbor-driven discriminant graph coupled nonnegative matrix factorization (KHNNDG-CNMF) is proposed for low-resolution image recognition (LRIR). In the proposed approach, a k-homogeneous nearest neighbor-driven discriminant graph (KHNNDG) is constructed, which is a discriminant graph matrix constructed within the geometrical nearest neighbors of k homogeneous samples. In discriminant graph construction methods, the two problems of insufficient utilization of data information in local neighborhoods existing in the previous neighbor graph and the inability to reflect the real data distribution in the local neighborhood can be improved. According to the geometrical spatial distribution of samples, the KHNNDG can adaptively reflect the relationship between intra-class samples and inter-class samples, and relatively few hyperparameters are required. Therefore, the coupled nonnegative matrix factorization algorithm combined with the KHNNDG embedding regularized term can more accurately and effectively utilize the local discriminativeness of the original space to constrain the coupled features. Furthermore, we further strengthen the class separability among features by incorporating a consistent projection constraint from features to labels. The proposed algorithm performs experiments on 6 image databases. Three different LR images are involved in the experiment. Compared to the best-performing comparative method, the proposed method shows an average improvement of approximately 2.36% in recognition performance. Particularly in tasks involving extremely LR levels ( $$8\times 7$$ ), the proposed method achieves an average improvement of 3.03%. Ablation experiments show that each module in the method can improve the recognition performance of LR images.},
  archive      = {J_PAAA},
  author       = {Pei, Jihong and Chen, Yebin and Zhao, Yang and Yang, Xuan},
  doi          = {10.1007/s10044-024-01316-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {K-homogeneous nearest neighbor-driven discriminant graph coupled nonnegative matrix factorization for low-resolution image recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based supervised contrastive learning on
fine-grained image classification. <em>PAAA</em>, <em>27</em>(3), 1–12.
(<a href="https://doi.org/10.1007/s10044-024-01317-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of fine-grained image classification performance caused by intra-class diversity and inter-class similarity in fine-grained images, we propose an Attention-based Supervised Contrastive (ASC) algorithm for fine-grained image classification. The method involves three stages: firstly, local parts are generated by a multi-attention module for constructing contrastive objectives to filter useless background information; an attention-based supervised contrastive framework is introduced to pre-train an encoder network and learn generalized features by pulling positive pairs closer while pushing negatives apart. Finally, we use cross-entropy to fine-tune the model pre-trained in the second stage to obtain classification results. Comprehensive experiments on CUB-200-2011, FGVC-Aircraft, and Stanford Cars datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PAAA},
  author       = {Li, Qian and Wu, Weining},
  doi          = {10.1007/s10044-024-01317-5},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Attention-based supervised contrastive learning on fine-grained image classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilinear principal component analysis-based tensor
decomposition for fabric weave pattern recognition from high-dimensional
streaming data. <em>PAAA</em>, <em>27</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01318-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern textile industry integrates video sensors with automated fabric reeling systems for real-time fabric weave pattern inspection. This automation system lessens the human-vision-based cognitive load and improves fabric weave pattern inspection work. However, this automation system poses a unique challenge, particularly when dealing with high-dimensional streaming data from highly precision digital microscope cameras. The complexity arises from the continuous acquisition and management of such high-dimensional streaming video data. Considering the challenges posed by dimensionality reduction in high-dimensional data, this study employs multilinear principal component analysis (MPCA)-based tensor decomposition, a statistical technique designed to effectively reduce high-dimensional datasets into low-dimensional features. This paper proposes an innovative method for fabric weave pattern recognition (FWPR) by leveraging MPCA-based tensor decomposition to extract low-dimensional features from the high-dimensional fabric’s surface texture descriptor tensor (STDT). This proposed method replicates fabric pattern monitoring in automated fabric reeling systems by integrating a digital microscope camera to capture high-dimensional streaming video data from fabric surface texture features. Subsequently high-dimensional video data is converted into sequential image frames representing different fabric weave patterns. These image frames are processed with local binary pattern (LBP) and gray-level co-occurrence matrix (GLCM) methods to aggregate fabric’s surface pattern features and construct the high-dimensional STDT. This STDT is subsequently decomposed into low-dimensional features by leveraging MPCA, resulting in an impressive 99.99% reduction in dimension. A supervised machine learning method utilizes the extracted low-dimensional features to enable FWPR, demonstrating superiority of the proposed method over the benchmark methods in evaluation.},
  archive      = {J_PAAA},
  author       = {Al Mamun, Abdullah and Islam, Md Imranul and Shohag, Md Abu Sayeed and Al-Kouz, Wael and Noor, KM Abdun},
  doi          = {10.1007/s10044-024-01318-4},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multilinear principal component analysis-based tensor decomposition for fabric weave pattern recognition from high-dimensional streaming data},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSMF-SPC: Multimodal sentiment analysis model with effective
context semantic modality fusion and sentiment polarity correction.
<em>PAAA</em>, <em>27</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01320-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis focuses on the fusion of multiple modalities. However, modality representation learning is a key step for better modality fusion, so how to fully learn the sentiment information of non-text modalities is a problem worth exploring. In addition, how to further improve the accuracy of sentiment polarity prediction is also a work to be studied. To solve the above problems, we propose a multimodal sentiment analysis model with effective context semantic modality fusion and sentiment polarity correction (CSMF-SPC). Firstly, we design a low-rank multimodal fusion network based on context semantic modality (CSM-LRMFN). CSM-LRMFN uses the bi-directional long short-term memory network to extract the context semantic features of non-text modalities, and the BERT to extract the features of text modality. Then, CSM-LRMFN adopts a low-rank multimodal fusion method to fully extract the interaction information among modalities with contextual semantics. Different from previous studies, to improve the accuracy of sentiment polarity prediction, we design a weight self-adjusting sentiment polarity penalty loss function, which makes the model learn more sentiment features that are conducive to model prediction through backpropagation. Finally, a series of comparative experiments are conducted on the CMU-MOSI and CMU-MOSEI datasets. Compared with the current representative models, CSMF-SPC achieves better experimental results. Among them, the Acc-2 (including zero) metric is increased by 1.41% and 1.58% on the word-aligned and unaligned CMU-MOSI datasets respectively; it is improved by 1.50% and 2.14% respectively on the CMU-MOSEI dataset, which indicates that the improvement of CSMF-SPC is effective.},
  archive      = {J_PAAA},
  author       = {Li, Yuqiang and Weng, Wenxuan and Liu, Chun and Li, Lin},
  doi          = {10.1007/s10044-024-01320-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CSMF-SPC: Multimodal sentiment analysis model with effective context semantic modality fusion and sentiment polarity correction},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging two-level deep learning classifiers for 2D shape
recognition to automatically solve geometry math word problems.
<em>PAAA</em>, <em>27</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01321-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mathematics, closed-domain systems for Question Answering (QA) have shown a distinct advantage over open-domain systems, primarily due to their focused use of supporting knowledge bases. This advantage is particularly salient in the era of online and hybrid tutoring, where automatic QA systems have become vital in addressing complex mathematical problems. This paper focuses on the challenge of geometric shape recognition in math word problems (MWPs) accompanied by figures that aid in the solution process. Existing systems rely on manually inputted shape information, which is less efficient. In this work, a novel customized two-layer deep learning model ‘2DGeoShapeNet’ for 2D geometric shape recognition has been developed. At the first level, it recognizes images in broad categories such as circles, quadrilaterals, or triangles. At the second level, the subtypes of quadrilaterals and triangles are detected. The proposed 2D shape detection model is trained and tested on a newly created integrated dataset, ‘GeoCQT’ (Circle, Quadrilateral, and Triangle), consisting of 6K+ images. The proposed deep learning technique achieved 93.98% accuracy on the ‘GeoCQT’ dataset. The performance of the proposed techniques is also evaluated on other geometry math word problem solver datasets such as GeoS, Geometry3K, GeoQA, PGDP5K, and PGPS9K. The proposed technique is compared with the already-published work that employed traditional image processing techniques for 2D shape detection. Findings highlight the superiority of two-level deep learning classifiers in detecting geometric shapes, marking a significant advancement in automated geometry problem-solving.},
  archive      = {J_PAAA},
  author       = {Boob, Archana and Radke, Mansi},
  doi          = {10.1007/s10044-024-01321-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Leveraging two-level deep learning classifiers for 2D shape recognition to automatically solve geometry math word problems},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of rare events in the operation of household
equipment using co-evolving time series. <em>PAAA</em>, <em>27</em>(3),
1–8. (<a href="https://doi.org/10.1007/s10044-024-01322-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a probabilistic approach to predict rare events by exploiting coevolving time series. The probability of a failure is calculated based on the weighted autologistic regression of these time series, accounting for specific characteristics of failures such as data imbalance. We estimate the model parameters using the maximum likelihood of the Bernoulli process. By incorporating the temporal behaviors of the various phenomena underlying the occurrence of failures and the nature of the data, we improve the prediction of rare events. Evaluations on both synthetic and real datasets demonstrate that our approach outperforms existing methods in predicting home equipment failures.},
  archive      = {J_PAAA},
  author       = {Mecheri, Hadia and Benamirouche, Islam and Fass, Feriel and Ziou, Djemel and Kadri, Nassima},
  doi          = {10.1007/s10044-024-01322-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-8},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Prediction of rare events in the operation of household equipment using co-evolving time series},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small object detection based on YOLOv8 in UAV perspective.
<em>PAAA</em>, <em>27</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01323-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) image object detection is a challenging task, primarily due to various factors such as multi-scale objects, a high proportion of small objects, significant overlap between objects, poor image quality, and complex and dynamic scenes. To address these challenges, several improvements were made to the YOLOv8 model. Firstly, by pruning the feature mapping layers responsible for detecting large objects in the YOLOv8 model, significant reduction in computational resources was achieved, rendering the model more lightweight. Simultaneously, a detection head fused with self-attention was introduced simultaneously to enhance the detection capability for small objects. Secondly, the introduction of space depth convolution in place of the original convolutional striding and pooling operations facilitates more effective preservation of details in low-resolution images and small objects. Lastly, a multi-level feature fusion module was designed to merge feature maps from different network layers, enhancing the network&#39;s representation capability. Results on the Visdrone dataset demonstrate that the proposed model achieved a significant 4.7% improvement in mAP50 compared to YOLOv8, while reducing the parameter count to only 39% of the original model. Moreover, transfer experiments on the TT100k dataset showed a 3.2% increase in mAP50, validating the effectiveness of the improved model for small object detection tasks in UAV images. Our code is made available at https://github.com/Wtgonw/Imporved-yolov8 .},
  archive      = {J_PAAA},
  author       = {Ning, Tao and Wu, Wantong and Zhang, Jin},
  doi          = {10.1007/s10044-024-01323-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Small object detection based on YOLOv8 in UAV perspective},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust 3D unique descriptor for 3D object detection.
<em>PAAA</em>, <em>27</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01326-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object recognition techniques based on local surface features are widely used for robust recognition. This paper proposes a 3D object recognition technique named 3DU using local features computed based on the uniqueness of keypoints. The technique first transforms 3D keypoints into another 3D space using Local Reference Frame. This transformation helps to find a list of probable matched keypoints of a query keypoint. Further, the proposed uniqueness-based descriptor rejects false matches to obtain the best match from the list. The proposed technique is validated by experiments on the Bologna dataset and achieved 100% recognition rate. In real-time scenarios, scenes obtained by an RGBD camera primarily consist of point density variation, cluttered surfaces, and occlusions. Most of the 3D descriptors have not been validated on such scenes in literature. We have analyzed 3DU and top-rated techniques on three RGBD datasets (dataset proposed in this paper, Challenge and Willow datasets). The results obtained by experiments on the proposed dataset show that the top-rated techniques have failed to handle RGBD data and 3DU has outperformed all compared techniques. The inferior performance of all techniques on complex datasets such as Challenge and Willow has elicited a need to develop robust training-free recognition techniques. The proposed dataset and code of the proposed technique 3DU are openly available in Mendeley (anonymously). http://dx.doi.org/10.17632/rfvzy9jn5v.1 .},
  archive      = {J_PAAA},
  author       = {Joshi, Piyush and Rastegarpanah, Alireza and Stolkin, Rustam},
  doi          = {10.1007/s10044-024-01326-4},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A robust 3D unique descriptor for 3D object detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intrinsic k-means clustering over homogeneous manifolds.
<em>PAAA</em>, <em>27</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01330-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The original K-means algorithm is widely applied for clustering in Euclidean spaces. Nevertheless, due to the non-flat characteristics of the Riemannian manifold, standard Euclidean K-means algorithms yield inferior results on such data. To address this issue, this paper presents an intrinsic K-means clustering algorithm on homogeneous manifolds based on the geodesic distance. It allows the development of K-means-based methods for frequently occurring non-vector spaces in robotics, such as directional vector modelling $$\mathbb {S}^2$$ and pose estimation $$\mathbb {S}^3$$ . First, the Riemannian metric of the homogeneous manifold is delivered; on this basis, the intrinsic K-means is proposed using Karcher mean, and its convergence is proved. Then, differences between the proposed algorithm and four projection-based algorithms, such as embedding projection, stereographic projection, central projection and logarithmic projection, are discussed by investigating their distance preservation on manifolds. Finally, to evaluate the effectiveness of the proposed algorithm, it is compared with the projection-based algorithms on $$\mathbb {S}^n$$ . The results show that the intrinsic K-means achieves better clustering results, where the clustering accuracy of the proposed method is improved by 47% and 27% on average on artificial $$\mathbb {S}^2$$ and $$\mathbb {S}^3$$ datasets, respectively. Meanwhile, the noise immunity of the proposed algorithm becomes more evident with the noise ratio increase.},
  archive      = {J_PAAA},
  author       = {Tan, Chao and Zhao, Huan and Ding, Han},
  doi          = {10.1007/s10044-024-01330-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Intrinsic K-means clustering over homogeneous manifolds},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the limitations of self-supervised learning
for tabular anomaly detection. <em>PAAA</em>, <em>27</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s10044-023-01208-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network’s representation can recover performance.},
  archive      = {J_PAAA},
  author       = {Mai, Kimberly T. and Davies, Toby and Griffin, Lewis D.},
  doi          = {10.1007/s10044-023-01208-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Understanding the limitations of self-supervised learning for tabular anomaly detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning sample representativeness for class-imbalanced
multi-label classification. <em>PAAA</em>, <em>27</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01209-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common problem that often occurs in multi-label image classification. In multi-label datasets, the co-occurrence of labels presents a unique set of difficulties, making it hard for traditional methods to produce satisfactory results, particularly on tail classes. Based on previous research and our investigation, we have found that the number of labels presents in a given sample can influence classification results. Nevertheless, it is worth noting that certain samples within the tail classes exhibit resistance to this influence, which is a critical aspect in the context of class-imbalanced multi-label classification. In this paper, we term these samples as representative samples. Highlighting representative samples during training can effectively address the above issues. Specifically, we propose a new method to learn sample representativeness, which is named Representativeness-Emphasizing Loss (REL). First, we use a new re-weighting form to rebalance the weights based on sample representativeness. Then, a modified focal loss dynamically assigns tailored parameters for each class in each sample to further emphasize the sample representativeness. Extensive experiments on two class-imbalanced datasets show that models trained with this new loss function achieve comparable performance to existing methods.},
  archive      = {J_PAAA},
  author       = {Zhang, Yu and Cao, Sichen and Mi, Siya and Bian, Yali},
  doi          = {10.1007/s10044-024-01209-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning sample representativeness for class-imbalanced multi-label classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wise-SrNet: A novel architecture for enhancing image
classification by learning spatial resolution of feature maps.
<em>PAAA</em>, <em>27</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01211-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges, since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increased the number of models’ weights. ResNet and the next deep convolutional models used the global average pooling layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution while not increasing computational cost. We have evaluated our method using three different datasets they are Intel Image Classification Challenge, MIT Indoors Scenes, and a part of the ImageNet dataset. We investigated the implementation of our architecture on several models of the Inception, ResNet, and DenseNet families. Applying our architecture has revealed a significant effect on increasing convergence speed and accuracy. Our experiments on images with 224224 resolution increased the Top-1 accuracy between 2 to 8% on different datasets and models. Running our models on 512512 resolution images of the MIT Indoors Scenes dataset showed a notable result of improving the Top-1 accuracy within 3 to 26%. We will also demonstrate the GAP layer’s disadvantage when the input images are large and the number of classes is not few. In this circumstance, our proposed architecture can do a great help in enhancing classification results. The code is shared at https://github.com/mr7495/image-classification-spatial .},
  archive      = {J_PAAA},
  author       = {Rahimzadeh, Mohammad and Parvin, Soroush and Askari, Amirali and Safi, Elnaz and Mohammadi, Mohammad Reza},
  doi          = {10.1007/s10044-024-01211-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Wise-SrNet: A novel architecture for enhancing image classification by learning spatial resolution of feature maps},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial–temporal attention with graph and general neural
network-based sign language recognition. <em>PAAA</em>, <em>27</em>(2),
1–17. (<a href="https://doi.org/10.1007/s10044-024-01229-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic sign language recognition (SLR) stands as a vital aspect within the realms of human–computer interaction and computer vision, facilitating the conversion of hand signs utilized by individuals with significant hearing and speech impairments into equivalent text or voice. Researchers have recently used hand skeleton joint information instead of the image pixel due to light illumination and complex background-bound problems. However, besides the hand information, body motion and facial gestures play an essential role in expressing sign language emotion. Also, a few researchers have been working to develop an SLR system by taking a multi-gesture dataset, but their performance accuracy and time complexity are not sufficient. In light of these limitations, we introduce a spatial and temporal attention model amalgamated with a general neural network designed for the SLR system. The main idea of our architecture is first to construct a fully connected graph to project the skeleton information. We employ self-attention mechanisms to extract insights from node and edge features across spatial and temporal domains. Our architecture bifurcates into three branches: a graph-based spatial branch, a graph-based temporal branch, and a general neural network branch, which collectively synergize to contribute to the final feature integration. Specifically, the spatial branch discerns spatial dependencies, while the temporal branch amplifies temporal dependencies embedded within the sequential hand skeleton data. Further, the general neural network branch enhances the architecture’s generalization capabilities, bolstering its robustness. In our evaluation, utilizing the Mexican Sign Language (MSL), Pakistani Sign Language (PSL) datasets, and American Sign Language Large Video dataset which comprises 3D joint coordinates for face, body, and hands that conducted experiments on individual gestures and their combinations. Impressively, our model demonstrated notable efficacy, achieving an accuracy rate of 99.96% for the MSL dataset, 92.00% for PSL, and 26.00% for the ASLLVD dataset, which includes more than 2700 classes. These exemplary performance metrics, coupled with the model’s computationally efficient profile, underscore its preeminence compared to contemporaneous methodologies in the field.},
  archive      = {J_PAAA},
  author       = {Miah, Abu Saleh Musa and Hasan, Md. Al Mehedi and Okuyama, Yuichi and Tomioka, Yoichi and Shin, Jungpil},
  doi          = {10.1007/s10044-024-01229-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Spatial–temporal attention with graph and general neural network-based sign language recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection using adaptive manta ray foraging
optimization for brain tumor classification. <em>PAAA</em>,
<em>27</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01236-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor is an anomalous growth of glial and neural cells and is considered as one of the primary causes of death worldwide. Therefore, it is essential to identify the tumor as soon as possible for reducing the mortality rate throughout the world. However, the classification of brain tumor is a challenging task due to presence of irrelevant features that cause misclassification during detection. In this research, the adaptive manta ray foraging optimization (AMRFO) is proposed for performing an effective feature selection to avoid the problem of overfitting while performing the classification. The adaptive control parameter strategy is incorporated in the AMRFO for enhancing the search process while selecting the feature subset. The linear intensity distribution information and regularization parameter-based intuitionistic fuzzy C-means algorithm namely LRIFCM is used to perform the segmentation of tumor regions. Next, LeeNET, gray-level co-occurrence matrix, local ternary pattern, histogram of gradients, and shape features are used to extract essential features from the segmented regions. Further, the attention-based long short-term memory (ALSTM) is used to classify the brain tumor types according to the features selected by AMRFO. The datasets utilized in this research study for the evaluation of AMRFO-ALSTM method are BRATS 2017, BRATS 2018, and Figshare brain datasets. Segmentation and classification are the two different evaluations examined for the AMRFO-ALSTM. The structural similarity index measure, Jaccard, dice, accuracy, and sensitivity are utilized during segmentation evaluation, while accuracy, specificity, sensitivity, precision, and F1-score are used during classification evaluation. The existing researches namely, transformer-enhanced convolutional neural network, Chan Vese (CV)-support vector machine, CV-K-nearest neighbor, deep convolutional neural network (DCNN), and salp water optimization with deep belief network are used to compare with the AMRFO-ALSTM. The accuracy of AMRFO-ALSTM for Figshare brain dataset is 99.80 which is a greater achievement when compared to the DCNN.},
  archive      = {J_PAAA},
  author       = {Neetha, K. S. and Narayan, Dayanand Lal},
  doi          = {10.1007/s10044-024-01236-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Feature selection using adaptive manta ray foraging optimization for brain tumor classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Distinguishing between crohn’s disease and
ulcerative colitis using deep learning models with interpretability.
<em>PAAA</em>, <em>27</em>(2), 1. (<a
href="https://doi.org/10.1007/s10044-024-01249-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PAAA},
  author       = {Maurício, José and Domingues, Inês},
  doi          = {10.1007/s10044-024-01249-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Correction to: Distinguishing between crohn’s disease and ulcerative colitis using deep learning models with interpretability},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: FBRNet: A feature fusion and border
refinement network for real-time semantic segmentation. <em>PAAA</em>,
<em>27</em>(2), 1. (<a
href="https://doi.org/10.1007/s10044-024-01250-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PAAA},
  author       = {Qu, ShaoJun and Wang, Zhuo and Wu, Jie and Feng, YueWen},
  doi          = {10.1007/s10044-024-01250-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Correction to: FBRNet: a feature fusion and border refinement network for real-time semantic segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A weakly supervised end-to-end framework for semantic
segmentation of cancerous area in whole slide image. <em>PAAA</em>,
<em>27</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01251-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of pathological image is an indispensable content in the cancerous diagnosis and grading, which is provided to doctors for the location and quantitative analysis of pathologically altered tissue. However, pathological whole slide image (WSI) generally has gigapixel size and huge region-level objective to be segmented. Extracting patches from WSI can address the limitation of computer memory, but the integrity of target is hence affected. Moreover, supervised learning methods require manually annotated labels for training, which is laborious and time-consuming. Thus, we studied a novel weakly supervised learning (WSL)-based end-to-end framework for semantic segmentation of cancerous area in WSI. The proposed framework is based on the block-level segmentation of convolutional neural network (CNN), while CNN is required to integrate the global average pooling layer and single fully connected layer as WSL-CNN. Class activation map and dense conditional random field (DenseCRF) are adapted to realize pixel-level segmentation of the cancerous area in patch, which is incorporated into the classification process of WSL-CNN. The hierarchically double use of DenseCRF effectively improves the precision of semantic segmentation. A region-based annotation method and a flexible method of constructing training dataset are proposed to reduce the workload of annotation. Experiments show that the block-level segmentation of CNNs has better performance than the pixel-level segmentation of fully convolutional networks, ResNet50 is the best one that achieves F1 score of 0.87426, Jaccard score of 0.78079, Recall of 0.94251 and Precision of 0.82182. The proposed framework can effectively refine the block-level prediction as semantic segmentation without pixel-level label. The precision of all tested CNNs get improved in the experiments, with WSL-ResNet50 achieving F1 score of 0.90630, Jaccard score of 0.83230, Recall of 0.92051 and Precision of 0.89789. We propose a complete end-to-end framework, including the specific structure of neural network, the construction of training dataset, the prediction method using neural network and the post-processing. CNN-like architectures can be widely transplanted into this framework to realize semantic segmentation, solving the problem of insufficient label of large-scale medical image to a certain extent.},
  archive      = {J_PAAA},
  author       = {Feng, Yanbo and Hafiane, Adel and Laurent, Hélène},
  doi          = {10.1007/s10044-024-01251-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A weakly supervised end-to-end framework for semantic segmentation of cancerous area in whole slide image},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CABF-YOLO: A precise and efficient deep learning method for
defect detection on strip steel surface. <em>PAAA</em>, <em>27</em>(2),
1–15. (<a href="https://doi.org/10.1007/s10044-024-01252-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have gained widespread usage in defect detection systems. However, existing methods are not satisfied for large-scale applications on surface defect detection of strip steel. In this paper, we propose a precise and efficient detection model, named CABF-YOLO, based on the YOLOX for strip steel surface defects. Firstly, we introduce the Triplet Convolutional Coordinate Attention (TCCA) module in the backbone of the YOLOX. By factorizing the pooling operation, the TCCA module can accurately capture cross-channel features to identify the location information of defects. Secondly, we design a novel Bidirectional Fusion (BF) strategy in the neck of the YOLOX. The BF strategy enhances the fusion of low-level and high-level semantic information to obtain fine-grained information. Lastly, the original bounding box loss function is replaced by the EIoU loss function. In the EIoU loss function, the penalty term is redefined to consider the overlap area, central point, and side length of the required regressions to accelerate the convergence rate and localization accuracy. On the benchmark NEU-DET dataset and GC10-DET dataset, the experimental results show that the CABF-YOLO achieves superior performance compared with other comparison models and satisfies the real-time detection requirement of industrial production.},
  archive      = {J_PAAA},
  author       = {Zhou, Qiqi and Wang, Haichao},
  doi          = {10.1007/s10044-024-01252-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CABF-YOLO: A precise and efficient deep learning method for defect detection on strip steel surface},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel venus’ visible image processing neoteric workflow
for improved planetary surface feature analysis. <em>PAAA</em>,
<em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01253-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a novel methodology that comprises of end-to-end Venus’ visible image processing neoteric workflow. The visible raw image is denoised using Tri-State median filter with background dark subtraction, and then enhanced using Contrast Limited Adaptive Histogram Equalization. The multi-modal image registration technique is developed using Segmented Affine Scale Invariant Feature Transform and Motion Smoothness Constraint outlier removal for co-registration of Venus’ visible and radar image. A novel image fusion algorithm using guided filter is developed to merge multi-modal Visible-Radar Venus’ image pair for generating the fused image. The Venus’ visible image quality assessment is performed at each processing step, and results are quantified and visualized. In addition, fuzzy color-coded segmentation map is generated for crucial information retrieval about Venus’ surface feature characteristics. It is found that Venus’ fused image clearly demarked planetary morphological features and validated with publicly available Venus’ radar nomenclature map.},
  archive      = {J_PAAA},
  author       = {Misra, Indranil and Rohil, Mukesh Kumar and Moorthi, SManthira and Dhar, Debajyoti},
  doi          = {10.1007/s10044-024-01253-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel venus’ visible image processing neoteric workflow for improved planetary surface feature analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tiny polyp detection from endoscopic video frames using
vision transformers. <em>PAAA</em>, <em>27</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01254-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques can be effective in helping doctors diagnose gastrointestinal polyps. Currently, processing video frame sequences containing a large amount of spurious noise in polyp detection suffers from elevated recall and mean average precision. Moreover, the mean average precision is also low when the polyp target in the video frame has large-scale variability. Therefore, we propose a tiny polyp detection from endoscopic video frames using Vision Transformers, named TPolyp. The proposed method uses a cross-stage Swin Transformer as a multi-scale feature extractor to extract deep feature representations of data samples, improves the bidirectional sampling feature pyramid, and integrates the prediction heads of multiple channel self-attention mechanisms. This approach focuses more on the feature information of the tiny object detection task than convolutional neural networks and retains relatively deeper semantic information. It additionally improves feature expression and discriminability without increasing the computational complexity. Experimental results show that TPolyp improves detection accuracy by 7%, recall by 7.3%, and average accuracy by 7.5% compared to the YOLOv5 model, and has better tiny object detection in scenarios with blurry artifacts.},
  archive      = {J_PAAA},
  author       = {Liu, Entong and He, Bishi and Zhu, Darong and Chen, Yuanjiao and Xu, Zhe},
  doi          = {10.1007/s10044-024-01254-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Tiny polyp detection from endoscopic video frames using vision transformers},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint face normalization and representation learning for
face recognition. <em>PAAA</em>, <em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01255-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity-independent factors, such as variations of pose, expression, illumination, etc., are the key challenges in face recognition. To avoid the effects of these factors, existing face recognition methods usually adopt two approaches: pose-invariant face feature extracting and face normalization before feature extraction. Contrary to these, we propose a single deep model jointly performing face normalization and representation learning tasks for face recognition, named normalization and reconstruction general adversarial network (NRGAN). First, the unified NRGAN model can boost the performance of the two tasks for each other. Second, NRGAN can synthesize normalized face images without the requirement of paired data, which makes our method have better generalization ability to the uncontrolled environment. Third, a factor-invariant identity disentanglement training strategy is introduced to decouple the identity feature representation from other factors without using any of these factors’ labels. Extensive experiment results on four currently popular face datasets demonstrate the effectiveness of NRGAN on both normalized face synthesis and face recognition tasks.},
  archive      = {J_PAAA},
  author       = {Liu, Yanfei and Chen, Junhua and Li, Yuanqian and Wu, Tianshu and Wen, Hao},
  doi          = {10.1007/s10044-024-01255-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Joint face normalization and representation learning for face recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DA-ResNet: Dual-stream ResNet with attention mechanism for
classroom video summary. <em>PAAA</em>, <em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01256-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to generate both diverse and representative video summary for massive videos. In this paper, a convolution neural network based on dual-stream attention mechanism(DA-ResNet) is designed to obtain candidate summary sequences for classroom scenes. DA-ResNet constructs a dual stream input of image frame sequence and optical flow frame sequence to enhance the expression ability. The network also embeds the attention mechanism into ResNet. On the other hand, the final video summary is obtained by removing redundant frames with the improved hash clustering algorithm. In this process, preprocessing is performed first to reduce computational complexity. And then hash clustering is used to retain the frame with the highest entropy value in each class, removing other similar frames. To verify its effectiveness in classroom scenes, we also created ClassVideo, a real dataset consisting of 45 videos from the normal teaching environment of our school. The results of the experiments show the competitiveness of the proposed method DA-ResNet outperforms the existing methods by about 8% in terms of the F-measure. Besides, the visual results also demonstrate its ability to produce classroom video summaries that are very close to the human preferences.},
  archive      = {J_PAAA},
  author       = {Wu, Yuxiang and Wang, Xiaoyan and Chen, Tianpan and Dou, Yan},
  doi          = {10.1007/s10044-024-01256-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DA-ResNet: Dual-stream ResNet with attention mechanism for classroom video summary},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cotton crop classification using satellite images with score
level fusion based hybrid model. <em>PAAA</em>, <em>27</em>(2), 1–22.
(<a href="https://doi.org/10.1007/s10044-024-01257-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate cotton images are significant component for surveiling cotton development and its precise control. A suitable technique for charting the distribution of cotton at the county or field level must be available to researchers and production managers. The classification of cotton remote sensing models at the county level has significant implications for precision farming, land management, and government decision-making. This work aims to develop a novel cotton crop classification model using satellite images based on soil behaviour. It includes phases like preprocessing, segmentation, feature extraction, and classification. Here, preprocessing is carried out by Gaussian filtering to improve the quality of the input image. Then Modified Deep Joint Segmentation method is employed for the segmentation process. The features such as wide dynamic range vegetation index, simple ratio, Green Chlorophyll index, Transformed vegetation index, and Green leaf area index are extracted for classifying the input. The hybrid Improved CNN (ICNN) and Bidirectional Gated recurrent Unit (Bi-GRU) have used for classification purposes, which is computed by the improved score level fusion. The suggested new hybrid optimization model known as the Battle Royale assisted Butterfly optimization algorithm (BRABOA) is used for adjusting the hidden neuron count of both the ICNN and Bi-GRU classifiers for improving the accuracy. At last, the efficiency of the suggested model is then evaluated to other schemes using a variety of metrics. The suggested HC + BRABOA method obtains a maximum accuracy of (0.95) over conventional methods at a learning percentage of 90% for classifying cotton crops using satellite images.},
  archive      = {J_PAAA},
  author       = {Kaur, Amandeep and Singla, Geetanjali and Singh, Manjinder and Mittal, Amit and Mittal, Ruchi and Malik, Varun},
  doi          = {10.1007/s10044-024-01257-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Cotton crop classification using satellite images with score level fusion based hybrid model},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Saliency information and mosaic based data augmentation
method for densely occluded object recognition. <em>PAAA</em>,
<em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01258-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation methods are crucial to improve the accuracy of densely occluded object recognition in the scene where the quantity and diversity of training images are insufficient. However, the current methods that use regional dropping and mixing strategies suffer from the problem of missing foreground objects and redundant background features, which can lead to densely occluded object recognition issues in classification or detection tasks. Herein, saliency information and mosaic based data augmentation method for densely occluded object recognition is proposed, which utilizes saliency information as prior knowledge to supervise the mosaic process of training images containing densely occluded objects. And the method uses fogging processing and class label mixing to construct new augmented images, in order to improve the accuracy of image classification and object recognition tasks by augmenting the quantity and diversity of training images. Extensive experiments on different classification datasets with various CNN architectures prove the effectiveness of our method.},
  archive      = {J_PAAA},
  author       = {Tong, Ying and Luo, Xiangfeng and Ma, Liyan and Xie, Shaorong and Yang, Wenbin and Guo, Yinsai},
  doi          = {10.1007/s10044-024-01258-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Saliency information and mosaic based data augmentation method for densely occluded object recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene text detection using structured information and an
end-to-end trainable generative adversarial networks. <em>PAAA</em>,
<em>27</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01259-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection poses a considerable challenge due to the diverse nature of text appearance, backgrounds, and orientations. Enhancing robustness, accuracy, and efficiency in this context is vital for several applications, such as optical character recognition, image understanding, and autonomous vehicles. This paper explores the integration of generative adversarial network (GAN) and network variational autoencoder (VAE) to create a robust and potent text detection network. The proposed architecture comprises three interconnected modules: the VAE module, the GAN module, and the text detection module. In this framework, the VAE module plays a pivotal role in generating diverse and variable text regions. Subsequently, the GAN module refines and enhances these regions, ensuring heightened realism and accuracy. Then, the text detection module takes charge of identifying text regions in the input image via assigning confidence scores to each region. The comprehensive training of the entire network involves minimizing a joint loss function that encompasses the VAE loss, the GAN loss, and the text detection loss. The VAE loss ensures diversity in generated text regions and the GAN loss guarantees realism and accuracy, while the text detection loss ensures high-precision identification of text regions. The proposed method employs an encoder-decoder structure within the VAE module and a generator-discriminator structure in the GAN module. Rigorous testing on diverse datasets including Total-Text, CTW1500, ICDAR 2015, ICDAR 2017, ReCTS, TD500, COCO-Text, SynthText, Street View Text, and KIAST Scene Text demonstrates the superior performance of the proposed method compared to existing approaches.},
  archive      = {J_PAAA},
  author       = {Naveen, Palanichamy and Hassaballah, Mahmoud},
  doi          = {10.1007/s10044-024-01259-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Scene text detection using structured information and an end-to-end trainable generative adversarial networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The limitations of differentiable architecture search.
<em>PAAA</em>, <em>27</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01260-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we will provide a detailed explanation of the limitations behind differentiable architecture search (DARTS). Algorithms based on the DARTS paradigm tend to converge towards degenerate solutions. A degenerate solution corresponds to an architecture with a shallow graph containing mainly skip connections. We have identified 6 sources of errors that could explain this phenomenon. Some of these errors can only be partially eliminated. Therefore, we will propose an innovative solution to remove degenerate solutions from the search space. We will demonstrate the validity of our approach through experiments conducted on the CIFAR10 and CIFAR100 databases. Our code is available at the following link: https://scm.univ-tours.fr/projetspublics/lifat/darts_ibpria_sparcity},
  archive      = {J_PAAA},
  author       = {Guillaume, Lacharme and Hubert, Cardot and Christophe, Lente and Nicolas, Monmarche},
  doi          = {10.1007/s10044-024-01260-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {The limitations of differentiable architecture search},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient clustering algorithm based on searching
popularity peaks. <em>PAAA</em>, <em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01261-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address some deficiencies of the density peak clustering algorithm, namely sensitivity to density kernels and challenges with large density differences across clusters, we propose a popularity peak clustering algorithm that is based on a more robust notion of density called popularity. The popularity of a sample is computed according to the number, similarity and popularity of points that have the sample in their k-nearest neighbors. The popularity concept has some properties that help in handling challenges like identifying cluster centers in sparse regions and handling situations with large density differences across clusters. Moreover, in the density peak clustering algorithm, the strategy of assigning non-center points to the same cluster as their nearest higher-density neighbor can cause error propagation. To address this issue, we also propose a new popularity-based label assignment strategy. Our results demonstrate that the proposed algorithm can recognize clusters regardless of their densities and overlap degree and can often outperform the existing density peak clustering algorithms.},
  archive      = {J_PAAA},
  author       = {Motallebi, Hassan and Malakoutifar, Najmeh},
  doi          = {10.1007/s10044-024-01261-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An efficient clustering algorithm based on searching popularity peaks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focalize k-NN: An imputation algorithm for time series
datasets. <em>PAAA</em>, <em>27</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01262-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective use of time series data is crucial in business decision-making. Temporal data reveals temporal trends and patterns, enabling decision-makers to make informed decisions and prevent potential problems. However, missing values in time series data can interfere with the analysis and lead to inaccurate conclusions. Thus, our work proposes a Focalize K-NN method that leverages time series properties to perform missing data imputation. This approach shows the benefits of taking advantage of correlated features and temporal lags to improve the performance of the traditional K-NN imputer. A similar approach could be employed in other methods. We tested this approach with two datasets, various parameter and feature combinations, and observed that it is beneficial in scenarios with disjoint missing patterns. Our findings demonstrate the effectiveness of Focalize K-NN for imputing missing values in time series data. The more noticeable benefits of our methods occur when there is a high percentage of missing data. However, as the amount of missing data increases, so does the error.},
  archive      = {J_PAAA},
  author       = {Almeida, Ana and Brás, Susana and Sargento, Susana and Pinto, Filipe Cabral},
  doi          = {10.1007/s10044-024-01262-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Focalize K-NN: An imputation algorithm for time series datasets},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal trajectory data modeling for fishing gear
classification. <em>PAAA</em>, <em>27</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01263-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {International Organizations urge the protection of our oceans and their ecosystems due to their immeasurable importance to humankind. Since illegal fishing activities, commonly known as IUU fishing, cause irreparable damage to these ecosystems, concerned organisms are pushing to detect and combat IUU fishing practices. The automatic identification system allows to locate the position and trajectory of fishing vessels. In this study we address the task of detecting vessels’ fishing gears based on the trajectory behavior defined by GPS position data, a useful task to prevent the proliferation of IUU fishing practices. We present a new database including trajectories that span 7 different fishing gears and analyze these as in a time sequence analysis problem. We leverage from feature extraction techniques from the online signature verification domain to model vessel trajectories, and extract relevant information in the form of both local and global feature sets. We show how, based on these sets of features, the kinematics of vessels according to different fishing gears can be effectively classified using common supervised learning algorithms with accuracies up to $$90\%$$ . Furthermore, motivated by the concerns raised by several organizations on the adverse impact of bottom trawling on marine biodiversity, we present a binary classification experiment in which we were able to distinguish this kind of fishing gear with an accuracy of $$99\%$$ . We also illustrate in an ablation study the relevance of factors such as data availability and the sampling period to perform fishing gear classification. Compared to existing works, we highlight these factors, especially the importance of using sampling periods in the order of minutes instead of hours.},
  archive      = {J_PAAA},
  author       = {Rodriguez-Albala, Juan Manuel and Peña, Alejandro and Melzi, Pietro and Morales, Aythami and Tolosana, Ruben and Fierrez, Julian and Vera-Rodriguez, Ruben and Ortega-Garcia, Javier},
  doi          = {10.1007/s10044-024-01263-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Spatio-temporal trajectory data modeling for fishing gear classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-recall calibration monitoring for stereo cameras.
<em>PAAA</em>, <em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01264-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cameras are the prevalent sensors used for perception in autonomous robotic systems, but their initial calibration may degrade over time due to dynamic factors. This may lead to a failure of downstream tasks, such as simultaneous localization and mapping (SLAM) or object recognition. Hence, a computationally lightweight process that detects the decalibration is of interest. We describe a modification of StOCaMo, an online calibration monitoring procedure for a stereoscopic system. The method uses robust kernel correlation based on epipolar constraints; it validates extrinsic calibration parameters on a single frame with no temporal tracking. In this paper, we present a modified StOCaMo with an improved recall rate on small decalibrations through a confirmation technique based on resampled variance. With fixed parameters learned on a realistic synthetic dataset from CARLA, StOCaMo and its proposed modification were tested on multiple sequences from two real-world datasets: KITTI and EuRoC MAV. The modification improved the recall of StOCaMo by 25 % (to 91 % and 82 %, respectively), and the accuracy by 12 % (to 94.7 % and 87.5 %, respectively), while labeling at most one-third of the input data as uninformative. The upgraded method achieved the rank correlation between StOCaMo V-index and downstream SLAM error of 0.78 (Spearman).},
  archive      = {J_PAAA},
  author       = {Moravec, Jaroslav and Šára, Radim},
  doi          = {10.1007/s10044-024-01264-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {High-recall calibration monitoring for stereo cameras},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatio-temporal model for violence detection based on
spatial and temporal attention modules and 2D CNNs. <em>PAAA</em>,
<em>27</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01265-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Violence detection is a difficult task because it involves analyzing video clips from multiple security cameras, which are located in various places and operate continuously. When violent crimes occur, a system should be able to reliably detect them in real-time and immediately alert a surveillance team. Currently, researchers employ deep learning models to detect violent behavior. Notably, a large number of deep learning approaches are based on extracting spatio-temporal information from a video by exploiting either 3D Convolutional Neural Networks (CNNs) or multi-stream networks. Despite their success, these techniques require a lot of parameters than 2D CNNs and have high computational complexity. Therefore, we present a simple spatio-temporal attention mechanism combined with a 2D CNN for an effective violence detection system. We propose a Squeeze Temporal Attention block that allows a 2D CNN to learn spatiotemporal features in videos. This effective block uses squeeze and temporal attention modules to summarize a video stream into three channels. In addition, we introduce spatial attention and feature fusion modules to improve the performance of the proposed system. The spatial attention module, Entropy Spatial Module, utilizes an entropy filter and frame differences to focus on spatial regions of the video with more movement. The fusion module parallelizes two dense layers with a 2D CNN to effectively enhance the classifier&#39;s performance. As a result, our proposed model achieves improved performance results in terms of accuracy when compared to Long Short-Term Memory, multi-stream networks, and current 3D CNNs.},
  archive      = {J_PAAA},
  author       = {Mahmoodi, Javad and Nezamabadi-pour, Hossein},
  doi          = {10.1007/s10044-024-01265-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A spatio-temporal model for violence detection based on spatial and temporal attention modules and 2D CNNs},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFDiff: Multiscale feature diffusion model for segmentation
of 3D intracranial aneurysm from CT images. <em>PAAA</em>,
<em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01266-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial aneurysm is a common life-threatening disease, and the rupture of an intracranial aneurysm carries a high risk of morbidity and mortality. Due to their small size in images, it remains a challenging task to accurately extract the intracranial aneurysms in CT images. In this paper, we propose a multi-scale feature diffusion model, named as MFDiff in short, for segmentation of 3D intracranial aneurysm. The proposed MFDiff includes a feature extraction module and a diffusion model. The feature extraction module is designed to extract features of the original image, and the features act as conditional priors to guide the diffusion model to gradually generate segmentation maps. The diffusion model takes a structure similar to U-Net as backbone, and there is a residual multi-scale feature fusion attention module (RMFA) in the diffusion model, which can adapt to intracranial aneurysms of different size due to multi-scale features. A local CT image dataset is employed for experiment, there are both ruptured and unruptured intracranial aneurysms in the images, and the size of intracranial aneurysms is various, even less than 3 mm. Compared with other popular methods, such as U-Net, GLIA-Net, UNETR++ , LinTransUNet, Swin UNETR, the proposed MFDiff shows better performance in intracranial aneurysm segmentation, the segmentation precision is 82.91% when the aneurysms of just size larger than 3 mm are taken into account, and the precision is 75.53% when considering aneurysms of all size.},
  archive      = {J_PAAA},
  author       = {Pei, Xinyu and Ren, Yande and Tang, Yueshan and Wang, Yuanquan and Zhang, Lei and Wei, Jin and Zhao, Di},
  doi          = {10.1007/s10044-024-01266-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MFDiff: Multiscale feature diffusion model for segmentation of 3D intracranial aneurysm from CT images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-free fire detection using the spatial–temporal
attention transform of the YOLO backbone. <em>PAAA</em>, <em>27</em>(2),
1–13. (<a href="https://doi.org/10.1007/s10044-024-01267-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional fire detection approaches typically relied on distinct models to address the varying characteristics of fires and smoke, particularly under different day and night conditions. Additionally, the distinction between wildfires and urban fires, which is influenced by camera shooting distances, often requires the use of separate detection algorithms. To address this, we introduce a novel domain-free (day, night, urban, and forest) fire detection algorithm within YOLOv5, incorporating linear attention for spatial attention extraction and gated temporary pooling (GTP) for temporal attention extraction. This study utilizes YOLOv5 as a base framework, and deviates from the conventional approach of modifying only pre-processing and downstream tasks. Within the dynamic attention block of GTP, our method extracts fire-related features by effectively discerning fires from background, while considering the spatiotemporal characteristics of fire flames and smoke. Despite the compact model size, our proposed approach significantly outperforms state-of-the-art methods for still image and continuous video datasets, demonstrating the effectiveness of our approach. This performance holds true for various fire types, locations, and under day and night conditions. This study marks a significant advancement in domain-free fire detection, offering a unified solution capable of addressing the diverse challenges presented by different fire scenarios and lighting conditions.},
  archive      = {J_PAAA},
  author       = {Kim, Sangwon and Jang, In-su and Ko, Byoung Chul},
  doi          = {10.1007/s10044-024-01267-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Domain-free fire detection using the spatial–temporal attention transform of the YOLO backbone},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block-wise imputation EM algorithm in multi-source scenario:
ADNI case. <em>PAAA</em>, <em>27</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01268-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is the most common form of dementia and the early detection is essential to prevent its proliferation. Real data available has been of paramount importance in order to achieve progress in the automatic detection despite presenting two major challenges: Multi-source observations containing Magnetic resonance (MRI), Positron emission tomography (PET) and Cerebrospinal fluid data (CSF); and also missing values within all these sources. Most machine learning techniques perform this predictive task by using a single data modality. Nevertheless, the integration of all these sources of evidence could possibly bring a higher performance at different stages of disease progression. The Expectation Maximization (EM) algorithm has been successfully employed to handle missing values, but it is not designed for typical Machine Learning scenarios where an imputation model is created over training data and subsequently applied on a testing set. In this work, we propose EMreg-KNN, a novel supervised and multi-source imputation algorithm. Based on the EM algorithm, EMreg-KNN builds a regression ensemble model for the imputation of future data thus allowing the further utilization of any vector-based Machine Learning method to automatically assess the Alzheimer’s disease diagnosis. Using the ADNI database, the proposed method achieves significant improvements on F1, AUC and Accuracy measures over classical imputation methods for this database using four classification algorithms. Considering these classifiers in four different classification scenarios, our algorithm is experimentally superior in terms of the F measure, in nearly 82% of the cases under evaluation.},
  archive      = {J_PAAA},
  author       = {Campos, Sergio and Zamora, Juan and Allende, Héctor},
  doi          = {10.1007/s10044-024-01268-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Block-wise imputation EM algorithm in multi-source scenario: ADNI case},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cross-domain transferability of black-box
adversarial attacks on speaker recognition systems using linearized
backpropagation. <em>PAAA</em>, <em>27</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s10044-024-01269-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker recognition system (SRS) serves as the gatekeeper for secure access, using the unique vocal characteristics of individuals for identification and verification. SRS can be found several biometric security applications such as in banks, autonomous cars, military, and smart devices. However, as technology advances, so do the threats to these models. With the rise of adversarial attacks, these models have been put to the test. Adversarial machine learning (AML) techniques have been utilized to exploit vulnerabilities in SRS, threatening their reliability and security. In this study, we concentrate on transferability in AML within the realm of SRS. Transferability refers to the capability of adversarial examples generated for one model to outsmart another model. Our research centers on enhancing the transferability of adversarial attacks in SRS. Our innovative approach involves strategically skipping non-linear activation functions during the backpropagation process to achieve this goal. The proposed method yields promising results in enhancing the transferability of adversarial examples across diverse SRS architectures, parameters, features, and datasets. To validate the effectiveness of our proposed method, we conduct an evaluation using the state-of-the-art FoolHD attack, an attack designed specifically for exploiting SRS. By implementing our method in various scenarios, including cross-architecture, cross-parameter, cross-feature, and cross-dataset settings, we demonstrate its resilience and versatility. To evaluate the performance of the proposed method in improving transferability, we have introduced three novel metrics: enhanced transferability, relative transferability, and effort in enhancing transferability. Our experiments demonstrate a significant boost in the transferability of adversarial examples in SRS. This research contributes to the growing body of knowledge on AML for SRS and emphasizes the urgency of developing robust defenses to safeguard these critical biometric systems.},
  archive      = {J_PAAA},
  author       = {Patel, Umang and Bhilare, Shruti and Hati, Avik},
  doi          = {10.1007/s10044-024-01269-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing cross-domain transferability of black-box adversarial attacks on speaker recognition systems using linearized backpropagation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proxemics-net++: Classification of human interactions in
still images. <em>PAAA</em>, <em>27</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01270-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human interaction recognition (HIR) is a significant challenge in computer vision that focuses on identifying human interactions in images and videos. HIR presents a great complexity due to factors such as pose diversity, varying scene conditions, or the presence of multiple individuals. Recent research has explored different approaches to address it, with an increasing emphasis on human pose estimation. In this work, we propose Proxemics-Net++, an extension of the Proxemics-Net model, capable of addressing the problem of recognizing human interactions in images through two different tasks: the identification of the types of “touch codes” or proxemics and the identification of the type of social relationship between pairs. To achieve this, we use RGB and body pose information together with the state-of-the-art deep learning architecture, ConvNeXt, as the backbone. We performed an ablative analysis to understand how the combination of RGB and body pose information affects these two tasks. Experimental results show that body pose information contributes significantly to proxemic recognition (first task) as it allows to improve the existing state of the art, while its contribution in the classification of social relations (second task) is limited due to the ambiguity of labelling in this problem, resulting in RGB information being more influential in this task.},
  archive      = {J_PAAA},
  author       = {Jiménez-Velasco, Isabel and Zafra-Palma, Jorge and Muñoz-Salinas, Rafael and Marín-Jiménez, Manuel J.},
  doi          = {10.1007/s10044-024-01270-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Proxemics-net++: Classification of human interactions in still images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAC: A meta-learning approach for feature learning and
recombination. <em>PAAA</em>, <em>27</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01271-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization-based meta-learning aims to learn a meta-initialization that can adapt quickly a new unseen task within a few gradient updates. Model Agnostic Meta-Learning (MAML) is a benchmark meta-learning algorithm comprising two optimization loops. The outer loop leads to the meta initialization and the inner loop is dedicated to learning a new task quickly. ANIL (almost no inner loop) algorithm emphasized that adaptation to new tasks reuses the meta-initialization features instead of rapidly learning changes in representations. This obviates the need for rapid learning. In this work, we propose that contrary to ANIL, learning new features may be needed during meta-testing. A new unseen task from a non-similar distribution would necessitate rapid learning in addition to the reuse and recombination of existing features. We invoke the width-depth duality of neural networks, wherein we increase the width of the network by adding additional connection units (ACUs). The ACUs enable the learning of new atomic features in the meta-testing task, and the associated increased width facilitates information propagation in the forward pass. The newly learned features combine with existing features in the last layer for meta-learning. Experimental results confirm our observations. The proposed MAC method outperformed the existing ANIL algorithm for non-similar task distribution by $$\approx$$ 12% (5-shot task setting).},
  archive      = {J_PAAA},
  author       = {Tiwari, Sambhavi and Gogoi, Manas and Verma, Shekhar and Singh, Krishna Pratap},
  doi          = {10.1007/s10044-024-01271-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MAC: A meta-learning approach for feature learning and recombination},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aka-net: Anchor free-based object detection network for
surveillance video transmission in the IOT edge computing environment.
<em>PAAA</em>, <em>27</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01272-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing use of wireless surveillance cameras in (Internet of things) IoT applications the need to address storage capacity and transmission bandwidth challenges becomes crucial. The majority of successive frames from surveillance cameras contain redundant and irrelevant information, leading to increased transmission burden. Existing video pre-processing techniques often focus on reducing the number of frames without considering accuracy and fail to effectively handle both spatial and temporal redundancies simultaneously. To address these issues, an anchor-free key action point network (AKA-Net) is proposed for video pre-processing in the IoT-edge computing environment. The oriented Features from Accelerated Segment Test (FAST) and rotated Binary Robust Independent Elementary Features (BRIEF) (ORB) feature descriptor is employed to remove duplicate frames, leading to more compact and efficient video representation. The AKA-Net&#39;s major contributions include its powerful representation capabilities achieved through the bottleneck module in the information-transferring backbone network, which effectively captures multi-scale features. The information-transferring module helps to improve the performance of the object detection algorithm for video pre-processing by fusing the complementary information from different scales. This allows the algorithm to detect objects of different sizes more accurately, making it highly effective for real-time video pre-processing tasks. Then, the key action point selection module that utilizes the self-attention mechanism is introduced to accurately select informative key action points. This enables efficient network transmission with lower bandwidth requirements, while maintaining high accuracy and low latency. It treats every pixel within the feature map as a temporal-spatial point and leverages self-attention to identify and select the most relevant keypoints. Experiments show that the proposed AKA-Net outperforms existing methods in terms of compression ratio of 54.2% and accuracy with a rate of 96.7%. By addressing spatial and temporal redundancies and optimizing key action point selection, AKA-Net offers a significant advancement in video pre-processing for smart surveillance systems, benefiting various IoT applications.},
  archive      = {J_PAAA},
  author       = {Sambandam Raju, Preethi and Arumugam Rajendran, Revathi and Mahalingam, Murugan},
  doi          = {10.1007/s10044-024-01272-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Aka-net: Anchor free-based object detection network for surveillance video transmission in the IOT edge computing environment},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep bharatanatyam pose recognition: A wavelet multi head
progressive attention. <em>PAAA</em>, <em>27</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01273-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose identification from 2D video sequences is extremely challenging under the influence of recording artifacts such as lighting, sensor motion, unpredictable subject movements and many more. In this work, the objective is to recognize rhythmic human poses from independently sourced online videos of an Indian classical dance form, Bharatanatyam. The data set (BOICDVD22) consists of internet-sourced video frames of 5 different songs from 10 dancers that are labelled into the corresponding lyrical classes. Inferencing and achieving a decent accuracy on the models trained with this multi-sourced online data is a challenging task. The past works focused on the creation of a miniature offline non-shareable ICD dataset with standard deep learning models which resulted in unsatisfactory performance. Recently, attention-based feature learning has been driving the performance of deep learning models. The most suitable attention mechanism for online data is wavelet-based attention. Though successful, wavelet-based feature learning is applied across one layer and is dependent on global average pooling (GAP) in both channel and spatial dimensions. The current generation of wavelet attention has resulted in unbalanced spatial attention across all the video frames. To overcome this unbalanced attention and induce human-like attention this work proposes to replace the GAP wavelet channel or spatial at a particular layer in the backbone architecture with wavelet multi-head progressive attention (WMHPA). It enhances the attention mechanism as well as decreases information loss because of no GAP. Progressiveness in attention enables the WMHPA to evenly distribute attention features across all the video frames. The results show the highest possible accuracy on the dance data set due to multi-resolution attention across the entire network. The WMHPA validates against state-of-the-art on our ICD as well as benchmarked person re-identification action datasets.},
  archive      = {J_PAAA},
  author       = {Kumar, D. Anil and Kishore, P. V. V. and Sravani, K.},
  doi          = {10.1007/s10044-024-01273-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Deep bharatanatyam pose recognition: A wavelet multi head progressive attention},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BNPSIW: BRBS-based NSST-PZMs domain statistical image
watermarking. <em>PAAA</em>, <em>27</em>(2), 1–27. (<a
href="https://doi.org/10.1007/s10044-024-01274-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustness, imperceptibility, and watermark capacity are three indispensable and contradictory properties for any image watermarking systems. It is a challenging work to achieve the balance among the three important properties. In this paper, by using bivariate Birnbaum–Saunders (BRBS) distribution model, we present a statistical image watermark scheme in nonsubsampled shearlet transform (NSST)-pseudo Zernike moments (PZMs) magnitude hybrid domain. The whole watermarking algorithm includes two parts: watermark embedding and extraction. NSST is firstly performed on host image to obtain the frequency subbands, and the NSST subbands are divided into non overlapping blocks. Then, the significant high-entropy NSST domain blocks are selected. Meanwhile, for each selected NSST coefficient block, PZMs are calculated to obtain the NSST-PZMs amplitude. Finally, watermark signals are inserted into the amplitude hybrid domain of NSST-PZMs. In order to decode accurately watermark signal, the statistical characteristics of NSST-PZMs magnitudes are analyzed in detail. Then, NSST-PZMs magnitudes are described statistically by BRBS distribution, which can simultaneously capture the marginal distribution and strong dependencies of NSST-PZMs magnitudes. Also, BRBS statistical model parameters are estimated accurately by modified closed-form maximum likelihood estimator (MML). Finally, a statistical watermark decoder based on BRBS distribution and maximum likelihood (ML) decision rule is developed in NSST-PZMS magnitude hybrid domain. Extensive experimental results show the superiority of the proposed image watermark decoder over some state-of-the-art statistical watermarking methods and deep learning approaches.},
  archive      = {J_PAAA},
  author       = {Niu, Panpan and He, Yinghong and Guo, Wei and Wang, Xiangyang},
  doi          = {10.1007/s10044-024-01274-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Pattern Anal. Appl.},
  title        = {BNPSIW: BRBS-based NSST-PZMs domain statistical image watermarking},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complex event recognition and anomaly detection with event
behavior model. <em>PAAA</em>, <em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01275-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of complex event processing refers to the process of tracking and analyzing a set of related events and drawing conclusions from them. For such systems, complex event recognition is essential. The object of complex event recognition is to recognize meaningful events or patterns and construct processing rules to respond to them. Researchers have conducted numerous studies on the recognition of complex event patterns by using recognition languages or models. However, the completeness of the process in complex event recognition has rarely been discussed. Although the reality of the event is uncertain, the structure for modeling and explaining complex event interactions of contingent information remains unclear. In this study, we focused on developing a general framework for addressing these problems and demonstrating the applicability of model-based approaches to represent spatio-temporal dimensions and causality in complex event recognition. In this paper, we propose an event behavior model for complex event recognition from a process perspective. The developed model could detect and explain anomalies associated with complex events. An experiment was conducted to evaluate the model performance. The results revealed that temporal operations within overlapping events were crucial to event pattern recognition.},
  archive      = {J_PAAA},
  author       = {Liu, Min-Chang and Hsu, Fang-Rong and Huang, Chua-Huang},
  doi          = {10.1007/s10044-024-01275-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Complex event recognition and anomaly detection with event behavior model},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote sensing image location based on improved yolov7
target detection. <em>PAAA</em>, <em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01276-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target detection, as a core issue in the field of computer vision, is widely applied in many key areas such as face recognition, license plate recognition, security protection, and driverless driving. Although its detection speed and accuracy continue to break records, there are still many challenges and difficulties in target detection of remote sensing images, which require further in-depth research and exploration. Remote sensing images can be regarded as a &quot;three-dimensional data cube&quot;, with more complex background information, dense and small object targets, and more severe weather interference factors. These factors lead to large positioning errors and low detection accuracy in the target detection process of remote sensing images. An improved YOLOv7 object detection model is proposed to address the problem of high false negative rate for dense and small objects in remote sensing images. Firstly, the GAM attention mechanism is introduced, and a global scheduling mechanism is proposed to improve the performance of deep neural networks by reducing information reduction and expanding global interaction representations, thus enhancing the network&#39;s sensitivity to targets. Secondly, the loss function CIoU in the original Yolov7 network model is replaced by SIoU, aiming to optimize the loss function, reduce losses, and improve the generalization of the network. Finally, the model is tested on the public available RSOD remote sensing dataset, and its generalization is verified on the Okahublot FloW-Img sub-dataset. The results showed that the accuracy (MAP@0.5) of detecting objects improved by 1.7 percentage points and 1.5 percentage points respectively for the improved Yolov7 network model compared to the original model, effectively improves the accuracy of detecting small targets in remote sensing images and solves the problem of leakage detection of small targets in remote sensing images.},
  archive      = {J_PAAA},
  author       = {Li, Cui and Wang, Jiao},
  doi          = {10.1007/s10044-024-01276-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Remote sensing image location based on improved yolov7 target detection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ErfReLU: Adaptive activation function for deep neural
network. <em>PAAA</em>, <em>27</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01277-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has found that the activation function (AF) plays a significant role in introducing non-linearity to enhance the performance of deep learning networks. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhances the outcomes is still in its early stages. In this paper, a novel activation function ‘ErfReLU’ has been developed based on the erf function and ReLU. This function leverages the advantages of both the Rectified Linear Unit (ReLU) and the error function (erf). A comprehensive overview of activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf is also presented. Lastly, comparative performance analysis of 9 trainable activation functions namely Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf with the proposed one has been performed. These activation functions are used in MobileNet, VGG16, and ResNet models and their performance is evaluated on benchmark datasets such as CIFAR-10, MNIST, and FMNIST.},
  archive      = {J_PAAA},
  author       = {Rajanand, Ashish and Singh, Pradeep},
  doi          = {10.1007/s10044-024-01277-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ErfReLU: Adaptive activation function for deep neural network},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stacked convolutional neural network framework with
multi-scale attention mechanism for text-independent voiceprint
recognition. <em>PAAA</em>, <em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01278-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-utterance speaker identification is a difficult area of study in natural language processing (NLP). Most cutting-edge experimental approaches for speech processing make use of convolutional neural networks (CNNs) and deep neural networks and analyse data in a unidirectional stream of time. In the past, approaches for identifying speakers that utilised CNNs often made use of highly dense or vast layers, leading to a large number of factors and significant computational expenses. In this article, we provide a novel multi-scale attention-focused 1-dimensional convolutional neural network (MSA-CNN) for recognising speakers that combines L1 and L2 norms. The multi-scale convolutional training architecture was developed to autonomously extract multi-scale characteristics of raw audio data by employing a variety of filter banks. In order for the multi-scale system to emphasis on important speaker feature characteristics in varying settings, a novel attention mechanism was built. In the end, it was combined and applied to the suggested multi-layered convolutional neural network framework to identify the speakers&#39; labels. The recommended network model was tested on a number of standard voice databases and real time recorded corpus. The findings from the experiments demonstrate that our methodology outperformed a baseline CNN scheme (without an attention mechanism) in addition to conventional speaker identification techniques involving feature engineering, achieving an accuracy rate of 97.94% across numerous databases as well as distortion constraints.},
  archive      = {J_PAAA},
  author       = {Karthikeyan, V. and Suja Priyadharsini, S.},
  doi          = {10.1007/s10044-024-01278-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A stacked convolutional neural network framework with multi-scale attention mechanism for text-independent voiceprint recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised group-based crowd dynamic behavior detection
and tracking in online video sequences. <em>PAAA</em>, <em>27</em>(2),
1–17. (<a href="https://doi.org/10.1007/s10044-024-01279-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of video sequences of public places is an important topic in video surveillance systems. Due to the high probability of occurring abnormal behavior in crowded scene, the main purpose of many surveillance systems is to monitor the crowd movement, and detection of abnormalities. To speed up this process and also for error reduction, it is highly important to use automated and intelligent tools in surveillance systems, as an alternative to the human operator. This study presents an unsupervised and online algorithm for analysis of dynamic crowd behavior, which uses the proposed features, with the capability to analyze crowds over time and reveal different behaviors of the crowd groups. In the proposed algorithm, prominent points are initially tracked. These key points are processed by the proposed system that includes removing the fixed points, employing proposed features of the moving points, automated determination of neighborhood, the similarity of the invariant neighbors. Group clustering is done automatically and the classification stage is conducted without the training phase. The dynamic behavior of the crowd is examined using the features and the extracted group properties and different states in the scene are diagnosed by dynamic thresholding. Experimental evaluation of the proposed method on several databases shows that it is performed properly in video sequences and it is able to detect various abnormal behaviors in the crowd scenes.},
  archive      = {J_PAAA},
  author       = {Ghorbanpour, Atefeh and Nahvi, Manoochehr},
  doi          = {10.1007/s10044-024-01279-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Unsupervised group-based crowd dynamic behavior detection and tracking in online video sequences},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early detection of alzheimer’s disease using squeeze and
excitation network with local binary pattern descriptor. <em>PAAA</em>,
<em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01280-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a degenerative brain disease that impairs memory, thinking skills, and the ability to perform even the most basic tasks. The primary challenge in this domain is accurate early stage disease detection. When the disease is detected at an early stage, medical professionals can prescribe medications to reduce brain shrinkage. Although the disease may not be curable, these interventions can extend the patient’s life by slowing down the rate of shrinkage. The four cognitive states of the human brain are cognitive normal (CN), mild cognitive impairment convertible (MCIc), mild cognitive impairment non-convertible (MCInc), and Alzheimer’s disease (AD). Mild cognitive impairment convertible (MCIc) is the early stage of Alzheimer’s disease. Individuals with MCIc will develop Alzheimer’s disease for a few years. However, it is difficult to detect this state through medical investigations. The mild cognitive impairment non-convertible state (MCInc) is the state immediately before MCIc. MCInc is a common condition in people of all ages, where minor memory issues arise as a result of normal aging. Early detection of AD can be claimed if and only if the transition from MCInc to MCIc is complete. Deep learning algorithms can be promising techniques for identifying the progression stage of a disease using magnetic resonance imaging. In this study, a novel deep learning algorithm was proposed to improve the classification accuracy of MCIc vs. MCInc. This study utilized the advantages of local binary patterns along with squeeze and excitation networks (SENet). Without the squeeze and excitation network, the classification accuracy of MCIc versus MCInc was 82%. The classification accuracy improved by 86% with the use of SENet. The experimental results show that the proposed model achieves better performance for MCInc vs. MCIc classification in terms of accuracy, precision, recall, F1 score, and ROC.},
  archive      = {J_PAAA},
  author       = {Francis, Ambily and Pandian, S. Immanuel Alex and Sagayam, K. Martin and Dang, Lam and Anitha, J. and Dinh, Linh and Pomplun, Marc and Dang, Hien},
  doi          = {10.1007/s10044-024-01280-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Early detection of alzheimer’s disease using squeeze and excitation network with local binary pattern descriptor},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PMG-DETR: Fast convergence of DETR with position-sensitive
multi-scale attention and grouped queries. <em>PAAA</em>,
<em>27</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01281-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed DETR successfully applied the Transformer to object detection and achieved impressive results. However, the learned object queries often explore the entire image to match the corresponding regions, resulting in slow convergence of DETR. Additionally, DETR only uses single-scale features from the final stage of the backbone network, leading to poor performance in small object detection. To address these issues, we propose an effective training strategy for improving the DETR framework, named PMG-DETR. We achieve this by using Position-sensitive Multi-scale attention and Grouped queries. First, to better fuse the multi-scale features, we propose a Position-sensitive Multi-scale attention. By incorporating a spatial sampling strategy into deformable attention, we can further improve the performance of small object detection. Second, we extend the attention mechanism by introducing a novel positional encoding scheme. Finally, we propose a grouping strategy for object queries, where queries are grouped at the decoder side for a more precise inclusion of regions of interest and to accelerate DETR convergence. Extensive experiments on the COCO dataset show that PMG-DETR can achieve better performance compared to DETR, e.g., AP 47.8 $$\%$$ using ResNet50 as backbone trained in 50 epochs. We perform ablation studies on the COCO dataset to validate the effectiveness of the proposed PMG-DETR.},
  archive      = {J_PAAA},
  author       = {Cui, Shuming and Deng, Hongwei},
  doi          = {10.1007/s10044-024-01281-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PMG-DETR: Fast convergence of DETR with position-sensitive multi-scale attention and grouped queries},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Study of distinguishable method for mixed images with
similar background. <em>PAAA</em>, <em>27</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01282-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To distinguish a certain object from mixed images with similar background, this paper proposes an image feature texture extraction method for distinguishing mixed images in similar backgrounds. Taking a mixed image of soybean and weed as an example, the method can accurately and quickly distinguish soybean and weed. In this paper, various fuzzy signals are first studied, the uncertainty classification and its impact are analyzed, a method for fuzzy signal processing is proposed. Secondly, this paper uses function packet to extract the texture features of the target, texture decomposition can obtain more detailed and rich target textures. Finally, using feature matching algorithm to determine the similarity between two feature vectors in an image, soybean recognition is completed, thereby removing weeds from the mixture of soybeans and weeds. Compared with the relevant performance of existing extraction methods, the accuracy of the proposed method is achieved more than 95%. It not only has fast processing speed, but also has adaptation to environment. This research has special practical significance and broad practical application prospects, provides important theoretical references and practical significance for fuzzy feature extraction.},
  archive      = {J_PAAA},
  author       = {Zhu, Yuyu and Wang, Wenjing and Wu, QingE and Xiao, Na and Zhang, Yangyang},
  doi          = {10.1007/s10044-024-01282-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Study of distinguishable method for mixed images with similar background},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital fingerprint indexing using synthetic binary indexes.
<em>PAAA</em>, <em>27</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01283-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprint identification is an important issue for people recognition when using Automatic Fingerprint Identification Systems (AFIS). The size of fingerprint databases has increased with the growing use of AFIS for identification at border control, visa issuance and other procedures around the world. Fingerprint indexing algorithms are used to reduce the fingerprint search space, speed up the identification processing time and also improve the accuracy of the identification result. In this paper, we propose a new binary fingerprint indexing method based on synthetic indexes to address this problem on large databases. Two fundamental properties are considered for these synthetic indexes: discriminancy and representativeness. A biometric database is then structured considering synthetic indexes for each fingerprint template, which guaranties to have a fixed number of indexes for the database during the enrollment and identification processes. We compare the proposed algorithm with the classical Minutiae Cylinder Code (MCC) indexing method, which is one of the best methods in the State of the art. In order to evaluate the proposed method, we use all Fingerprint Verification Competition (FVC) datasets from 2000 to 2006 databases separately and combined to confirm the accuracy of our algorithm for real applications. The proposed method achieves a high hit rate (more than 98%) for a low value of penetration rate (less than 5%) compared to existing methods in the literature.},
  archive      = {J_PAAA},
  author       = {Falade, Joannes and Cremer, Sandra and Rosenberger, Christophe},
  doi          = {10.1007/s10044-024-01283-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Digital fingerprint indexing using synthetic binary indexes},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Distinguishing between crohn’s disease and ulcerative
colitis using deep learning models with interpretability. <em>PAAA</em>,
<em>27</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10044-023-01206-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crohn’s disease and ulcerative colitis are two chronic diseases that cause inflammation in the tissues of the entire gastrointestinal tract and are described by the term inflammatory bowel disease. Gastroenterologists find it difficult to evaluate endoscopic images to recognise the characteristics of the two chronic diseases. Therefore, this work aims to build a dataset with images of Crohn’s disease and ulcerative colitis (collected from the public datasets LIMUC, HyperKvasir and CrohnIPI) and train deep learning models (five CNNs and six ViTs) to develop a tool capable of helping doctors to distinguish the type of inflammatory bowel disease. In addition, as these architectures will be too heavy to work in a hospital context, in this work, we are looking to use knowledge distillation to create lighter and simpler architectures with the same precision as the pre-trained architectures used in this study. During this process, it is important to evaluate and interpret the pre-trained architectures before the distillation process, and the architectures resulting from knowledge distillation to ensure that we can maintain performance and that the information learnt by both architectures are similar. It is concluded that is possible to reduce 25x the number of parameters while maintaining good performance and reducing the inference time by 5.32 s. Allied with this, through the interpretability of the models was concluded that both before and after the knowledge distillation are possible to identify ulcers, bleeding situations, and lesions caused by the inflammation of the disease.},
  archive      = {J_PAAA},
  author       = {Maurício, José and Domingues, Inês},
  doi          = {10.1007/s10044-023-01206-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Distinguishing between crohn’s disease and ulcerative colitis using deep learning models with interpretability},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FBRNet: A feature fusion and border refinement network for
real-time semantic segmentation. <em>PAAA</em>, <em>27</em>(1), 1–18.
(<a href="https://doi.org/10.1007/s10044-023-01207-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing semantic segmentation networks perform well in accuracy by spending much computation. However, for practical applications, not only high segmentation accuracy but also high inference speed is required. To solve the problem of the difficult balance between accuracy and speed, we propose a new real-time semantic segmentation network (FBRNet). To extract multi-scale semantic information more quickly, we propose a lightly weighted reinforced atrous spatial pyramid pooling module (arASPP) based on the attention mechanism, which can extract richer and more advanced features with less computation than the original ASPP. To eliminate the semantic gap between high- and low-level features, we propose a new feature fusion module (CSFM), in which a shuffling mechanism is introduced to enhance robustness, and a parallel contextual information enhancement module and detail information enhancement module are built to facilitate the information exchange between high- and low-level features, achieving the effect of improving the model feature representation. Finally, we also introduce high-level features, fusing Laplace convolution and spatial attention mechanisms, and design the edge feature reinforcement module (LABRM) to eliminate the noise of low-level features and compensate for the model’s segmentation effect target boundary. In the Cityscapes validation set and test set, FBRNet achieves 77.63% and 75.3% mIoU, and 101.9 FPS on a single tesla-T4 GPU, also achieved 72.4% mIoU and 89.8 FPS on the CamVid dataset and 55.2% mIoU and 100.8 FPS on the BDD100K dataset, which is a better balance of accuracy and speed compared with existing networks. The code is available at https://github.com/little5570/FBRNet.},
  archive      = {J_PAAA},
  author       = {Qu, ShaoJun and Wang, Zhuo and Wu, Jie and Feng, YueWen},
  doi          = {10.1007/s10044-023-01207-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {FBRNet: A feature fusion and border refinement network for real-time semantic segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear dimensionality reduction with q-gaussian
distribution. <em>PAAA</em>, <em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01210-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the dimensionality reduction has become more important as the number of dimensions of data used in various tasks such as regression and classification has increased. As popular nonlinear dimensionality reduction methods, t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP) have been proposed. However, the former outputs only one low-dimensional space determined by the t-distribution and the latter is difficult to control the distribution of distance between each pair of samples in low-dimensional space. To tackle these issues, we propose novel t-SNE and UMAP extended by q-Gaussian distribution, called q-Gaussian-distributed stochastic neighbor embedding (q-SNE) and q-Gaussian-distributed uniform manifold approximation and projection (q-UMAP). The q-Gaussian distribution is a probability distribution derived by maximizing the tsallis entropy by escort distribution with mean and variance, and a generalized version of Gaussian distribution with a hyperparameter q. Since the shape of the q-Gaussian distribution can be tuned smoothly by the hyperparameter q, q-SNE and q-UMAP can in- tuitively derive different embedding spaces. To show the quality of the proposed method, we compared the visualization of the low-dimensional embedding space and the classification accuracy by k-NN in the low-dimensional space. Empirical results on MNIST, COIL-20, OliverttiFaces and FashionMNIST demonstrate that the q-SNE and q-UMAP can derive better embedding spaces than t-SNE and UMAP.},
  archive      = {J_PAAA},
  author       = {Abe, Motoshi and Nomura, Yuichiro and Kurita, Takio},
  doi          = {10.1007/s10044-024-01210-1},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Nonlinear dimensionality reduction with q-gaussian distribution},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D orientation field transform. <em>PAAA</em>,
<em>27</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01212-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vascular structure enhancement is very useful in image processing and computer vision. The enhancement of the presence of the structures like tubular networks in given images can improve image-dependent diagnostics and can also facilitate tasks like segmentation. The two-dimensional (2D) orientation field transform has been proved to be effective at enhancing 2D contours and curves in images by means of top-down processing. It, however, has no counterpart in 3D images due to the extremely complicated orientation in 3D against 2D. Given the rising demand and interest in handling 3D images, we experiment with modularising the concept and generalise the algorithm to 3D curves. In this work, we propose a 3D orientation field transform. It is a vascular structure enhancement algorithm that can cleanly enhance images having very low signal-to-noise ratio, and push the limits of 3D image quality that can be enhanced computationally. This work also utilises the benefits of modularity and offers several combinative options that each yield moderately better enhancement results in different scenarios. In principle, the proposed 3D orientation field transform can naturally tackle any number of dimensions. As a special case, it is also ideal for 2D images, owning a simpler methodology compared to the previous 2D orientation field transform. The concise structure of the proposed 3D orientation field transform also allows it to be mixed with other enhancement algorithms, and as a preliminary filter to other tasks like segmentation and detection. The effectiveness of the proposed method is demonstrated with synthetic 3D images and real-world transmission electron microscopy tomograms ranging from 2D curve enhancement to, the more important and interesting, 3D ones. Extensive experiments and comparisons with existing related methods also demonstrate the excellent performance of the proposed 3D orientation field transform.},
  archive      = {J_PAAA},
  author       = {Yeung, Wai-Tsun and Cai, Xiaohao and Liang, Zizhen and Kang, Byung-Ho},
  doi          = {10.1007/s10044-024-01212-z},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {3D orientation field transform},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big topic modeling based on a two-level hierarchical latent
beta-liouville allocation for large-scale data and parameter streaming.
<em>PAAA</em>, <em>27</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01213-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension to the standard symmetric latent Dirichlet allocation topic model, we implement asymmetric Beta-Liouville as a conjugate prior to the multinomial and therefore propose the maximum a posteriori for latent Beta-Liouville allocation as an alternative to maximum likelihood estimator for models such as probabilistic latent semantic indexing, unigrams, and mixture of unigrams. Since most Bayesian posteriors, for complex models, are intractable in general, we propose a point estimate (the mode) that offers a much tractable solution. The maximum a posteriori hypotheses using point estimates are much easier than full Bayesian analysis that integrates over the entire parameter space. We show that the proposed maximum a posteriori reduces the three-level hierarchical latent Beta-Liouville allocation to two-level topic mixture as we marginalize out the latent variables. In each document, the maximum a posteriori provides a soft assignment and constructs dense expectation–maximization probabilities over each word (responsibilities) for accurate estimates. For simplicity, we present a stochastic at word-level online expectation–maximization algorithm as an optimization method for maximum a posteriori latent Beta-Liouville allocation estimation whose unnormalized reparameterization is equivalent to a stochastic collapsed variational Bayes. This implicit connection between the collapsed space and expectation–maximization-based maximum a posteriori latent Beta-Liouville allocation shows its flexibility and helps in providing alternative to model selection. We characterize efficiency in the proposed approach for its ability to simultaneously stream both large-scale data and parameters seamlessly. The performance of the model using predictive perplexities as evaluation method shows the robustness of the proposed technique with text document datasets.},
  archive      = {J_PAAA},
  author       = {Ihou, Koffi Eddy and Bouguila, Nizar},
  doi          = {10.1007/s10044-024-01213-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Big topic modeling based on a two-level hierarchical latent beta-liouville allocation for large-scale data and parameter streaming},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive spectral graph wavelets for collaborative
filtering. <em>PAAA</em>, <em>27</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01214-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering is a popular approach in recommender systems, whose objective is to provide personalized item suggestions to potential users based on their purchase or browsing history. However, personalized recommendations require considerable amount of behavioral data on users, which is usually unavailable for new users, giving rise to the cold-start problem. To help alleviate this challenging problem, we introduce a spectral graph wavelet collaborative filtering framework for implicit feedback data, where users, items and their interactions are represented as a bipartite graph. Specifically, we first propose an adaptive transfer function by leveraging a power transform with the goal of stabilizing the variance of graph frequencies in the spectral domain. Then, we design a deep recommendation model for efficient learning of low-dimensional embeddings of users and items using spectral graph wavelets in an end-to-end fashion. In addition to capturing the graph’s local and global structures, our approach yields localization of graph signals in both spatial and spectral domains and hence not only learns discriminative representations of users and items, but also promotes the recommendation quality. The effectiveness of our proposed model is demonstrated through extensive experiments on real-world benchmark datasets, achieving better recommendation performance compared with strong baseline methods.},
  archive      = {J_PAAA},
  author       = {Alshareet, Osama and Ben Hamza, A.},
  doi          = {10.1007/s10044-024-01214-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive spectral graph wavelets for collaborative filtering},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information theory divergences in principal component
analysis. <em>PAAA</em>, <em>27</em>(1), 1–8. (<a
href="https://doi.org/10.1007/s10044-024-01215-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metric learning area studies methodologies to find the most appropriate distance function for a given dataset. It was shown that dimensionality reduction algorithms are closely related to metric learning because, in addition to obtaining a more compact representation of the data, such methods also implicitly derive a distance function that best represents similarity between a pair of objects in the collection. Principal Component Analysis is a traditional linear dimensionality reduction algorithm that is still widely used by researchers. However, its procedure faithfully represents outliers in the generated space, which can be an undesirable characteristic in pattern recognition applications. With this is mind, it was proposed the replacement of the traditional punctual approach by a contextual one based on the data samples neighborhoods. This approach implements a mapping from the usual feature space to a parametric feature space, where the difference between two samples is defined by the vector whose scalar coordinates are given by the statistical divergence between two probability distributions. It was demonstrated for some divergences that the new approach outperforms several existing dimensionality reduction algorithms in a wide range of datasets. Although, it is important to investigate the framework divergence sensitivity. Experiments using Total Variation, Renyi, Sharma-Mittal and Tsallis divergences are exhibited in this paper and the results evidence the method robustness.},
  archive      = {J_PAAA},
  author       = {Nakao, Eduardo K. and Levada, Alexandre L. M.},
  doi          = {10.1007/s10044-024-01215-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-8},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Information theory divergences in principal component analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning approach to censored regression.
<em>PAAA</em>, <em>27</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10044-024-01216-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In censored regression, the outcomes are a mixture of known values (uncensored) and open intervals (censored), meaning that the outcome is either known with precision or is an unknown value above or below a known threshold. The use of censored data is widespread, and correctly modeling it is essential for many applications. Although the literature on censored regression is vast, deep learning approaches have been less frequently applied. This paper proposes three loss functions for training neural networks on censored data using gradient backpropagation: the tobit likelihood, the censored mean squared error, and the censored mean absolute error. We experimented with three variations in the tobit likelihood that arose from different ways of modeling the standard deviation variable: as a fixed value, a reparametrization, and an estimation using a separate neural network for heteroscedastic data. The tobit model yielded better results, but the other two losses are simpler to implement. Another central idea of our research was that data are often censored and truncated simultaneously. The proposed losses can handle simultaneous censoring and truncation at arbitrary values from above and below.},
  archive      = {J_PAAA},
  author       = {Dănăilă, Vlad-Rareş and Buiu, Cătălin},
  doi          = {10.1007/s10044-024-01216-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A deep learning approach to censored regression},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised fuzzy broad learning system based on
mean-teacher model. <em>PAAA</em>, <em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01217-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy broad learning system (FBLS) is a newly proposed fuzzy system, which introduces Takagi–Sugeno fuzzy model into broad learning system. It has shown that FBLS has better nonlinear fitting ability and faster calculation speed than the most of fuzzy neural networks proposed earlier. At the same time, compared to other fuzzy neural networks, FBLS has fewer rules and lower cost of training time. However, label errors or missing are prone to appear in large-scale dataset, which will greatly reduce the performance of FBLS. Therefore, how to use limited label information to train a powerful classifier is an important challenge. In order to address this problem, we introduce Mean-Teacher model for the fuzzy broad learning system. We use the Mean-Teacher model to rebuild the weights of the output layer of FBLS, and use the Teacher–Student model to train FBLS. The proposed model is an implementation of semi-supervised learning which integrates fuzzy logic and broad learning system in the Mean-Teacher-based knowledge distillation framework. Finally, we have proved the great performance of Mean-Teacher-based fuzzy broad learning system (MT-FBLS) through a large number of experiments.},
  archive      = {J_PAAA},
  author       = {Fan, Zizhu and Huang, Yijing and Xi, Chao and Peng, Cheng and Wang, Shitong},
  doi          = {10.1007/s10044-024-01217-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised fuzzy broad learning system based on mean-teacher model},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of sparsity metrics and evolutionary algorithms
applied for normalization of h&amp;e histological images. <em>PAAA</em>,
<em>27</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01218-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color variations in H&amp;E histological images can impact the segmentation and classification stages of computational systems used for cancer diagnosis. To address these variations, normalization techniques can be applied to adjust the colors of histological images. Estimates of stain color appearance matrices and stain density maps can be employed to carry out these color adjustments. This study explores these estimates by leveraging a significant biological characteristic of stain mixtures, which is represented by a sparsity parameter. Computationally estimating this parameter can be accomplished through various sparsity measures and evolutionary algorithms. Therefore, this study aimed to evaluate the effectiveness of different sparsity measures and algorithms for color normalization of H&amp;E-stained histological images. The results obtained demonstrated that the choice of different sparsity measures significantly impacts the outcomes of normalization. The sparsity metric $$l_{\epsilon }^{0}$$ proved to be the most suitable for it. Conversely, the evolutionary algorithms showed little variations in the conducted quantitative analyses. Regarding the selection of the best evolutionary algorithm, the results indicated that particle swarm optimization with a population size of 250 individuals is the most appropriate choice.},
  archive      = {J_PAAA},
  author       = {Tosta, Thaína A. Azevedo and de Faria, Paulo Rogério and Neves, Leandro Alves and Martins, Alessandro Santana and Kaushal, Chetna and do Nascimento, Marcelo Zanchetta},
  doi          = {10.1007/s10044-024-01218-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Evaluation of sparsity metrics and evolutionary algorithms applied for normalization of H&amp;E histological images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning discriminative local contexts for person
re-identification in vehicle surveillance scenarios. <em>PAAA</em>,
<em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01219-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, person re-identification (Re-ID) has been widely used in intelligent surveillance and security. However, Re-ID faces many challenges in the vehicle surveillance scenario, such as heavy occlusion, misalignment, and similar appearances. Most Re-ID methods focus on learning discriminative global features or dividing regions for local feature learning, which may ignore critical but subtle differences between pedestrians. In this paper, we propose a local context aggregation branch for learning discriminative local contexts at multiple scales, which can supplement the critical detailed information omitted in global features. Specifically, we exploit dilated convolutions to simulate spatial feature pyramid to capture multi-scale spatial contexts efficiently. The essential information that can distinguish different pedestrians is then emphasized. Besides, we construct a Re-ID dataset named BSV for vehicle surveillance scenarios and propose a triplet loss with station constraint enhancement, which utilizes additional valuable station information to construct penalty terms to improve the performance of Re-ID further. Extensive experiments are conducted on the proposed BSV dataset and two standard Re-ID datasets, and the results validate the effectiveness of our method.},
  archive      = {J_PAAA},
  author       = {Lin, Xiangyu and Wang, Jing and Huang, Rufei and Wang, Cheng and Zhang, Huizhen},
  doi          = {10.1007/s10044-024-01219-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning discriminative local contexts for person re-identification in vehicle surveillance scenarios},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective bin model for reversible data hiding in encrypted
images. <em>PAAA</em>, <em>27</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01220-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In tandem with the fast-growing technology, the issue of secure data transmission over the Internet has achieved increasing importance. In digital media, enclosing data in images is one of the most common methods for communicating confidential information. A novel reversible data hiding in the encrypted images scheme based on selective bin models is proposed in this paper. The scheme focuses on enhancing the embedding capacity while ensuring the security of images with the help of encryption and the proposed data hiding process. For data embedding, lossless compression is utilized and the image is classified into three bins. Then, marker bits are assigned to these bins for distinguishing between embeddable and non-embeddable regions. The proposed method shows a satisfactory embedding rate for smooth images as well as complex ones due to its selective bin approach. Also, the method is separable in nature, i.e., data extraction and image recovery can be performed independently. Furthermore, the experimental results demonstrate the strategy’s effectiveness when compared with others.},
  archive      = {J_PAAA},
  author       = {Agarwal, Ruchi and Ahmed, Sara and Kumar, Manoj},
  doi          = {10.1007/s10044-024-01220-z},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Selective bin model for reversible data hiding in encrypted images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epileptic EEG signal classification using an improved
VMD-based convolutional stacked autoencoder. <em>PAAA</em>,
<em>27</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01221-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous techniques have been explored so far for epileptic electroencephalograph (EEG) signal detection and classification. Deep learning-based approaches are in recent demand for data classification with huge features. In this paper, an improved deep learning approach based on convolutional features followed by stacked autoencoder (CSAE) and kernel extreme learning machine (KELM) classifier at the end is proposed for EEG signal classification. The convolutional network extracts initial features by convolution, and after this stage, the features are supplied to stacked autoencoder (SAE) for obtaining final compressed features. These suitable features are then fed to KELM classifier for identifying seizure, seizure-free and healthy EEG signals. The EEG signals are decomposed through chaotic water cycle algorithm-optimised variational mode decomposition (CWCA-OVMD) from which the optimised number of efficient modes is obtained yielding six features like energy, entropy, standard deviation, variance, kurtosis, and skewness. These CWCA-OVMD-based features are then fed to the CSAE for the extraction of relevant features. Once the features are obtained, the KELM classifier is used to classify the EEG signal. The classification results are compared with different deep learning classifiers validating the efficacy of the proposed model. The KELM classifier avoids the choice of hidden neurons in the end layer unlike traditional classifiers which is one of the major advantages.},
  archive      = {J_PAAA},
  author       = {Parija, Sebamai and Dash, Pradipta Kishore and Bisoi, Ranjeeta},
  doi          = {10.1007/s10044-024-01221-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Epileptic EEG signal classification using an improved VMD-based convolutional stacked autoencoder},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A detection method for occluded and overlapped apples under
close-range targets. <em>PAAA</em>, <em>27</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01222-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and rapid identification and location of apples contributes to speeding up automation harvesting. However, in unstructured orchard environments, it is common for apples to be overlapped and occluded by branches and leaves, which interferes with apple identification and localization. In order to quickly reconstruct the fruits under overlapping and occlusion conditions, an adaptive radius selection strategy based on random sample consensus algorithm (ARSS-RANSAC) was proposed. Firstly, the edge of apple in the image was obtained by using image preprocessing method. Secondly, an adaptive radius selection strategy was proposed, which is based on fruit shape characteristics. The fruit initial radius was obtained through horizontal or vertical scanning. Then, combined with RANSAC algorithm to select effective contour points by the determined radius, and the circle center coordinates were obtained. Finally, fitting the circle according to the selected valid contour and achieving the recognition and localization of overlapped and occluded apples. 175 apple images with different overlaps and branches and leaves occlusion were applied to verify the effectiveness of algorithm. The evaluation indicators of overlap rate, average false-positive rate, average false-negative rate, and average segmentation error of ARSS-RANSAC were improved compared with the classical Hough transform method. The detection time of a single image was less than 50 ms, which can meet requirements of real-time target detection. The experimental results show that the ARSS-RANSAC algorithm can quickly and accurately identify and locate occluded and overlapped apples and is expected to be applied to harvesting robots of apple and other round fruits.},
  archive      = {J_PAAA},
  author       = {Yuan, Yuhui and Liu, Hubin and Yang, Zengrong and Zheng, Jianhua and Li, Junhui and Zhao, Longlian},
  doi          = {10.1007/s10044-024-01222-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A detection method for occluded and overlapped apples under close-range targets},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ensemble of learned features and reshaping of fractal
geometry-based descriptors for classification of histological images.
<em>PAAA</em>, <em>27</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01223-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of histology images has been the focus of plenty researchers in computer vision. Recently, the most common approaches for this task consist of applying deep learning through CNN models. However, there are some limitations to the use of CNN in the context of histological image classification such as the need for large datasets and the difficulty to implement a generalized model able to classify different types of histology tissue. In this work, an ensemble model based on handcrafted fractal features and deep learning that consists of combining the classification of the ResNet-50 model and the classification of local and global handcrafted features by applying the sum rule is proposed. Fractal geometry concepts are used to obtain handcrafted local and global features from different histological datasets. The local features are reshaped into a matrix in order to compose a feature image. Four different reshaping procedures are evaluated, wherein each generates a representation model of fractal features which is given as input to a CNN model. Another CNN architecture receives as input the original image. After associating the results of both CNN models with the classification of the handcrafted local and global features using machine learning approaches, accuracy rates that range from 88.45% up to 99.77% on five datasets were obtained. Moreover, the model was able to classify images from datasets with different resolutions and imbalanced classes with few training epochs. In general, the proposed method is able to provide results that are compatible with the state-of-the-art in histology image classification.},
  archive      = {J_PAAA},
  author       = {Roberto, Guilherme Freire and Neves, Leandro Alves and Lumini, Alessandra and Martins, Alessandro Santana and Nascimento, Marcelo Zanchetta do},
  doi          = {10.1007/s10044-024-01223-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An ensemble of learned features and reshaping of fractal geometry-based descriptors for classification of histological images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel model for fall detection and action recognition
combined lightweight 3D-CNN and convolutional LSTM networks.
<em>PAAA</em>, <em>27</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01224-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional convolutional neural networks (3D-CNNs) and full connection long short-term memory networks (FC-LSTMs) have been demonstrated as a kind of powerful non-intrusive approaches in fall detection. However, the feature extration of 3D-CNN-based requires a large-scale dataset. Meanwhile, the deployment of FC-LSTM to expand the input into one-dimension leads to the loss of spatial information. To this end, a novel model combined lightweight 3D-CNN and convolutional long short-term memory (ConvLSTM) networks is proposed in this paper. In this model, a lightweight 3D convolutional neural network with five layers is presented to avoid the phenomenon of over-fitting. To further explore the discrimnative features, the channel- and spatial-wise attention modules are adopted in each layer to improve the detection performance. In addition, the ConvLSTM is presented to extract the long-term spatial–temporal features of 3D tensors. Finally, we verify our model through extensive experiments by comprehensive comparisons with HMDB5, UCF11, URFD, and MCFD. Experimental results on the public benchmarks demonstrate that our method outperforms current state-of-the-art single-stream networks with 62.55 ± 7.99% on HMDB5, 97.28 ± 0.36% on UCF11, 98.06 ± 0.32% on URFD, and 94.84 ± 4.64% on MCFD.},
  archive      = {J_PAAA},
  author       = {Su, Chan and Wei, Jianguo and Lin, Deyu and Kong, Linghe and Guan, Yong Liang},
  doi          = {10.1007/s10044-024-01224-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel model for fall detection and action recognition combined lightweight 3D-CNN and convolutional LSTM networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase analysis simulating the takeda method to obtain a 3D
profile of SARS-CoV-2 cells. <em>PAAA</em>, <em>27</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01225-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a morphologic analysis by means of the construction of 3D models of the SARS-CoV-2 VP (viral particles) with algorithms in Python and Matlab based on the processing of frames. To this aim, we simulate the Takeda method to induce periodicity and apply the Fourier transform to obtain the phase of objects under analysis. To this aim, we analyze several research works focused on infected tissues by SARS-CoV-2 virus culture cells, highlighting the obtained medical images of the virus from microscopy and tomography. We optimize the results by performing image processing (segmentation and periodic noise removal) in order to obtain an accurate ROI (Region of Interest) segmentation containing only information on SARS-CoV-2 cells. We apply our algorithm to these images (3D tomographic medical images) to simulate the Takeda method (which also filters the image), considering the periodicity induced by us in the image to carry out a phase unwrapping process. Finally, we use the image phase to focus on the body, center (RNA, Protein M-N), and spikes (Protein S) of the SARS-CoV-2 cells to identify them as characteristic elements of the SARS-CoV-2 virion morphology to build a 3D model based only in the metadata of clinical studies on cell cultures. The latter results in the construction of a mathematical, physical, biological, and numerical model of the SARS-CoV-2 virion, a tool with volumes, or 3D non-speculative or animated models, based only on medical images (3D tomography) in clinical tests, faithful to the virus.},
  archive      = {J_PAAA},
  author       = {Arriaga-Hernández, Jesús and Cuevas-Otahola, Bolivia and Oliveros-Oliveros, José J. and Morín-Castillo, María M.},
  doi          = {10.1007/s10044-024-01225-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Phase analysis simulating the takeda method to obtain a 3D profile of SARS-CoV-2 cells},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subspace clustering via adaptive-loss regularized
representation learning with latent affinities. <em>PAAA</em>,
<em>27</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01226-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data that lies on several subspaces tend to be highly correlated and contaminated by various noises, and its affinities across different subspaces are not always reliable, which impedes the effectiveness of subspace clustering. To alleviate the deficiencies, we propose a novel subspace learning model via adaptive-loss regularized representation learning with latent affinities (ALRLA). Specifically, the robust least square regression with nonnegative constraint is firstly proposed to generate more interpretable reconstruction coefficients in low-dimensional subspace and specify the weighted self-representation capability with adaptive loss norm for better robustness and discrimination. Moreover, an adaptive latent graph learning regularizer with an initialized affinity approximation is considered to provide more accurate and robust neighborhood assignment for low-dimensional representations. Finally, the objective model is solved by an alternating optimization algorithm, with theoretical analyses on its convergence and computational complexity. Extensive experiments on benchmark databases demonstrate that the ALRLA model can produce clearer structured representation under redundant and noisy data environment. It achieves competing clustering performance compared with the state-of-the-art clustering models.},
  archive      = {J_PAAA},
  author       = {Jiang, Kun and Zhu, Lei and Liu, Zheng and Sun, Qindong},
  doi          = {10.1007/s10044-024-01226-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Subspace clustering via adaptive-loss regularized representation learning with latent affinities},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to track and segment fish without human annotations: A
self-supervised deep learning approach. <em>PAAA</em>, <em>27</em>(1),
1–18. (<a href="https://doi.org/10.1007/s10044-024-01227-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking fish movements and sizes of fish is crucial to understanding their ecology and behaviour. Knowing where fish migrate, how they interact with their environment, and how their size affects their behaviour can help ecologists develop more effective conservation and management strategies to protect fish populations and their habitats. Deep learning is a promising tool to analyse fish ecology from underwater videos. However, training deep neural networks (DNNs) for fish tracking and segmentation requires high-quality labels, which are expensive to obtain. We propose an alternative unsupervised approach that relies on spatial and temporal variations in video data to generate noisy pseudo-ground-truth labels. We train a multi-task DNN using these pseudo-labels. Our framework consists of three stages: (1) an optical flow model generates the pseudo-labels using spatial and temporal consistency between frames, (2) a self-supervised model refines the pseudo-labels incrementally, and (3) a segmentation network uses the refined labels for training. Consequently, we perform extensive experiments to validate our method on three public underwater video datasets and demonstrate its effectiveness for video annotation and segmentation. We also evaluate its robustness to different imaging conditions and discuss its limitations.},
  archive      = {J_PAAA},
  author       = {Saleh, Alzayat and Sheaves, Marcus and Jerry, Dean and Rahimi Azghadi, Mostafa},
  doi          = {10.1007/s10044-024-01227-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {How to track and segment fish without human annotations: A self-supervised deep learning approach},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric k-means clustering-based adaptive unsupervised
colour image segmentation. <em>PAAA</em>, <em>27</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01228-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation focuses at highlighting region of interest within the image, by accumulation of pixels based on given properties. This task resembles to clustering, yet many standard clustering methods fail to meet the basic requirement of image segmentation, that is number of segments is rarely determined automatically. The proposed nonparametric K-means clustering (EAIS) overcomes this limitation and turns out to be particularly suitable for the task of image segmentation. In this paper, we propose a nonparametric K-means clustering approach (EAIS) that automatically and adaptively determines the initialization conditions, i.e. number of clusters, initial cluster centroids, and subsequently segments the image into suitable regions. The proposed approach comprises of five modules that includes deep image reconstruction, intra-histogram individual peak level detection, inter-histogram peak levels association, mutual consensus-oriented cluster seeds merging, and morphological reconstruction-driven spatial post-processing. Deep reconstruction performs image smoothing by reducing the variance and outliers in the colour channel distribution. The proposed approach utilizes image histograms-based global distribution to determine the optimal initialization condition for pixel clustering (image segmentation). Followed by dynamic and optimally devised cluster seeds merging for redundancy reduction and determination of adequate number of cluster seeds for K-means initialization. Finally, morphological reconstruction inducts spatial awareness in the clustered space and enhances the spatial consistency of cluster member’s (pixels). Diverse experimental results on the BSDS500 benchmark validate that our proposed approach is robust to various natural scenarios and comparable to state-of-the-art methods regarding segmentation quality and computational efficiency.},
  archive      = {J_PAAA},
  author       = {Khan, Zubair and Yang, Jie},
  doi          = {10.1007/s10044-024-01228-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Nonparametric K-means clustering-based adaptive unsupervised colour image segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local complex features learned by randomized neural networks
for texture analysis. <em>PAAA</em>, <em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01230-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture is a visual attribute largely used in many problems of image analysis. Many methods that use learning techniques have been proposed for texture discrimination, achieving improved performance over previous handcrafted methods. In this paper, we present a new approach that combines a learning technique and the complex network (CN) theory for texture analysis. This method takes advantage of the representation capacity of CN to model a texture image as a directed network and then uses the topological information of vertices to train a randomized neural network. This neural network has a single hidden layer and uses a fast learning algorithm to learn local CN patterns for texture characterization. Thus, we use the weights of the trained neural network to compose a feature vector. These feature vectors are evaluated in a classification experiment in four widely used image databases. Experimental results show a high classification performance of the proposed method compared to other methods, indicating that our approach can be used in many image analysis problems.},
  archive      = {J_PAAA},
  author       = {Ribas, Lucas C. and Scabini, Leonardo F. S. and de Mesquita Sá Junior, Jarbas Joaci and Bruno, Odemir M.},
  doi          = {10.1007/s10044-024-01230-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Local complex features learned by randomized neural networks for texture analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical contrastive learning and color standardization
for single image sand-dust removal. <em>PAAA</em>, <em>27</em>(1), 1–10.
(<a href="https://doi.org/10.1007/s10044-024-01231-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNN) have demonstrated impressive performance in reconstructing images in challenging environments. However, there is still a blank in the field of CNN-based sandstorm image processing. Existing sandstorm removal algorithms enhance degraded images by using prior knowledge, but often fail to address the issues of color cast, low contrast, and poor recognizability. To bridge the gap, we present a novel end-to-end sand-dust reconstruction network and incorporate hierarchical contrastive regularization and color constraint in the network. Based on contrastive learning, the hierarchical contrastive regularization reconstructs the sand-free image by pulling it closer to ’positive’ pairs while pushing it away from ’negative’ pairs in representation space. Furthermore, considering the specific characteristics of sandstorm images, we introduce the color constraint term as a sub-loss function to balance the hue, saturation, and value of the reconstructed image. Experimental results show that the proposed SdR-Net outperforms state-of-the-arts in both quantitative and qualitative.},
  archive      = {J_PAAA},
  author       = {Si, Yazhong and Xu, Mengjia and Yang, Fan},
  doi          = {10.1007/s10044-024-01231-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Hierarchical contrastive learning and color standardization for single image sand-dust removal},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subdomain adaptation via correlation alignment with entropy
minimization for unsupervised domain adaptation. <em>PAAA</em>,
<em>27</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01232-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is a well-explored domain in transfer learning, finding applications across various real-world scenarios. The central challenge in UDA lies in addressing the domain shift between training (source) and testing (target) data distributions. This study focuses on image classification tasks within UDA, where label spaces are shared, but the target domain lacks labeled samples. Our primary objective revolves around mitigating the domain discrepancies between the source and target domains, ultimately facilitating robust generalization in the target domains. Domain adaptation techniques have traditionally concentrated on the global feature distribution to minimize disparities. However, these methods often need to pay more attention to crucial, domain-specific subdomain information within identical classification categories, challenging achieving the desired performance without fine-grained data. To tackle these challenges, we propose a unified framework, Subdomain Adaptation via Correlation Alignment with Entropy Minimization, for unsupervised domain adaptation. Our approach incorporates three advanced techniques: (1) Local Maximum Mean Discrepancy, which aligns the means of local feature subsets, capturing intrinsic subdomain alignments often missed by global alignment, (2) correlation alignment aimed at minimizing the correlation between domain distributions, and (3) entropy regularization applied to target domains to encourage low-density separation between categories. We validate our proposed methods through rigorous experimental evaluations and ablation studies on standard benchmark datasets. The results consistently demonstrate the superior performance of our approaches compared to existing state-of-the-art domain adaptation methods.},
  archive      = {J_PAAA},
  author       = {Gilo, Obsa and Mathew, Jimson and Mondal, Samrat and Sandoniya, Rakesh Kumar},
  doi          = {10.1007/s10044-024-01232-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Subdomain adaptation via correlation alignment with entropy minimization for unsupervised domain adaptation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redirected transfer learning for robust multi-layer subspace
learning. <em>PAAA</em>, <em>27</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01233-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised transfer learning methods usually exploit the labeled source data to learn a classifier for unlabeled target data with a different but related distribution. However, most of the existing transfer learning methods leverage 0-1 matrix as labels which greatly narrows the flexibility of transfer learning. Another major limitation is that these methods are influenced by the redundant features and noises residing in cross-domain data. To cope with these two issues simultaneously, this paper proposes a redirected transfer learning (RTL) approach for unsupervised transfer learning with a multi-layer subspace learning structure. Specifically, in the first layer, we first learn a robust subspace where data from different domains can be well interlaced. This is made by reconstructing each target sample with the lowest-rank representation of source samples. Besides, imposing the $$L_{2,1}$$ -norm sparsity on the regression term and regularization term brings robustness against noise and works for selecting informative features, respectively. In the second layer, we further introduce a redirected label strategy in which the strict binary labels are relaxed into continuous values for each datum. To handle effectively unknown labels of the target domain, we construct the pseudo-labels iteratively for unlabeled target samples to improve the discriminative ability in classification. The superiority of our method in classification tasks is confirmed on several cross-domain datasets.},
  archive      = {J_PAAA},
  author       = {Bao, Jiaqi and Kudo, Mineichi and Kimura, Keigo and Sun, Lu},
  doi          = {10.1007/s10044-024-01233-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Redirected transfer learning for robust multi-layer subspace learning},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatio-temporal binary grid-based clustering model for
seismicity analysis. <em>PAAA</em>, <em>27</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10044-024-01234-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a spatio-temporal binary grid-based clustering model for determining complex earthquake clusters with different shapes and heterogeneous densities present in a catalog. The 3D occurrence of earthquakes is mapped into a 2D-low memory sparse matrix through a grid mechanism in the binary domain with consideration of spatio-temporal attributes. Then, image-transformation of a non-empty sets binary feature matrix, a clustering strategy is implemented with logical AND operator as similarity measure among the binary vectors. This approach is applied to solve the problem of seismicity declustering which separates the clustering and non-clustering patterns of seismicity for real-world earthquake catalogs of Japan (1972–2020) and Eastern Mediterranean (1966–2020). Results demonstrate that the proposed method has a significant reduction in both computation and memory footprint with few tuning parameters. Background earthquakes have an impression on the homogeneous Poisson process with fair memory-less characteristics in the time domain as evident from graphical and statistical analysis. Overall seismicity and observed background activity both have similar multi-fractal behavior with a deviation of $$\pm 0.04$$ . The comparative analysis is carried out with benchmark declustering models: Gardner–Knopoff, Uhrhammer, Gruenthal window-based method, and Reasenberg’s approach, and superior performance of the proposed method is found in most cases.},
  archive      = {J_PAAA},
  author       = {Vijay, Rahul Kumar and Nanda, Satyasai Jagannath and Sharma, Ashish},
  doi          = {10.1007/s10044-024-01234-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A spatio-temporal binary grid-based clustering model for seismicity analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Segmenting large historical notarial manuscripts into
multi-page deeds. <em>PAAA</em>, <em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01235-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Archives around the world hold vast digitized series of historical manuscript books or “bundles” containing, among others, notarial records also known as “deeds” or “acts”. One of the first steps to provide metadata which describe the contents of those bundles is to segment them into their individual deeds. Even if deeds are often page-aligned, as in the bundles considered in the present work, this is a time-consuming task, often prohibitive given the huge scale of the manuscript series involved. Unlike traditional Layout Analysis methods for page-level segmentation, our approach goes beyond the realm of a single-page image, providing consistent deed detection results on full bundles. This is achieved in two tightly integrated steps: first, we estimate the class-posterior at the page level for the “initial”, “middle”, and “final” classes; then we “decode” these posteriors applying a series of sequentiality consistency constraints to obtain a consistent book segmentation. Experiments are presented for four large historical manuscripts, varying the number of “deeds” used for training. Two metrics are introduced to assess the quality of book segmentation, one of them taking into account the loss of information entailed by segmentation errors. The problem formalization, the metrics and the empirical work significantly extend our previous works on this topic.},
  archive      = {J_PAAA},
  author       = {Prieto, Jose Ramón and Becerra, David and Toselli, Alejandro Hector and Alonso, Carlos and Vidal, Enrique},
  doi          = {10.1007/s10044-024-01235-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Segmenting large historical notarial manuscripts into multi-page deeds},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LFFNet: Lightweight feature-enhanced fusion network for
real-time semantic segmentation of road scenes. <em>PAAA</em>,
<em>27</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01237-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have significantly improved semantic segmentation, but their great performance frequently comes at the expense of expensive computation and protracted inference times, which fall short of the exacting standards of real-world applications. A lightweight feature-enhanced fusion network (LFFNet) for real-time semantic segmentation is proposed. LFFNet is a particular type of asymmetric encoder–decoder structure. In the encoder, A multi-dilation rate fusion module can guarantee the retention of local information while enlarging the appropriate field in the encoder section, which resolves the issue of insufficient feature extraction caused by the variability of target size. In the decoder, different decoding modules are designed for spatial information and semantic information. The attentional feature enhancement module takes advantage of the attention mechanism to feature-optimize the contextual information of the high-level output, and the lightweight multi-scale feature fusion module fuses the features from various stages to aggregate more spatial detail information and contextual semantic information. The experimental findings demonstrate that LFFNet achieves 72.1% mIoU and 67.0% mIoU on Cityscapes and Camvid datasets at 102 FPS and 244 FPS, respectively, with only 0.63M parameters. Note that there is neither pretraining nor pre-processing. Our model can achieve superior segmentation performance with fewer parameters and less computation compared to existing networks.},
  archive      = {J_PAAA},
  author       = {Hu, Xuegang and Feng, Jing and Gong, Juelin},
  doi          = {10.1007/s10044-024-01237-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LFFNet: Lightweight feature-enhanced fusion network for real-time semantic segmentation of road scenes},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying robustness: 3D tree point cloud skeletonization
with smart-tree in noisy domains. <em>PAAA</em>, <em>27</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s10044-024-01238-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting tree skeletons from 3D tree point clouds is challenged by noise and incomplete data. While our prior work (Dobbs et al., in: Iberian conference on pattern recognition and image analysis, Springer, Berlin, pp. 351–362, 2023) introduced a deep learning approach for approximating tree branch medial axes, its robustness against various types of noise has not been thoroughly evaluated. This paper addresses this gap. Specifically, we simulate real-world noise challenges by introducing 3D Perlin noise (to represent subtractive noise) and Gaussian noise (to mimic additive noise). To facilitate this evaluation, we introduce a new synthetic tree point cloud dataset, available at https://github.com/uc-vision/synthetic-trees-II . Our results indicate that our deep learning-based skeletonization method is tolerant to both additive and subtractive noise.},
  archive      = {J_PAAA},
  author       = {Dobbs, Harry and Batchelor, Oliver and Peat, Casey and Atlas, James and Green, Richard},
  doi          = {10.1007/s10044-024-01238-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Quantifying robustness: 3D tree point cloud skeletonization with smart-tree in noisy domains},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
