<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BCYB_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bcyb---17">BCYB - 17</h2>
<ul>
<li><details>
<summary>
(2024). Neuroscientific insights about computer vision models: A
concise review. <em>BCYB</em>, <em>118</em>(5), 331–348. (<a
href="https://doi.org/10.1007/s00422-024-00998-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of biologically-inspired computational models has been the focus of study ever since the artificial neuron was introduced by McCulloch and Pitts in 1943. However, a scrutiny of literature reveals that most attempts to replicate the highly efficient and complex biological visual system have been futile or have met with limited success. The recent state-of the-art computer vision models, such as pre-trained deep neural networks and vision transformers, may not be biologically inspired per se. Nevertheless, certain aspects of biological vision are still found embedded, knowingly or unknowingly, in the architecture and functioning of these models. This paper explores several principles related to visual neuroscience and the biological visual pathway that resonate, in some manner, in the architectural design and functioning of contemporary computer vision models. The findings of this survey can provide useful insights for building futuristic bio-inspired computer vision models. The survey is conducted from a historical perspective, tracing the biological connections of computer vision models starting with the basic artificial neuron to modern technologies such as deep convolutional neural network (CNN) and spiking neural networks (SNN). One spotlight of the survey is a discussion on biologically plausible neural networks and bio-inspired unsupervised learning mechanisms adapted for computer vision tasks in recent times.},
  archive      = {J_BCYB},
  author       = {Susan, Seba},
  doi          = {10.1007/s00422-024-00998-9},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {5},
  pages        = {331-348},
  shortjournal = {Biol. Cybern.},
  title        = {Neuroscientific insights about computer vision models: A concise review},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase response curves and the role of coordinates.
<em>BCYB</em>, <em>118</em>(5), 311–330. (<a
href="https://doi.org/10.1007/s00422-024-00997-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “infinitesimal phase response curve” (PRC) is a common tool used to analyze phase resetting in the natural sciences in general and neuroscience in particular. We make the observation that the PRC with respect to a coordinate v actually depends on the choice of other coordinates. As a consequence, a complete delay embedding reconstruction of the dynamics using v which would allow phase to be computed still does not allow the v PRC to be computed. We give a coordinate-free definition of the PRC making this observation obvious. This leads to an experimental protocol: first collect an appropriate ensemble of measurements by intermittently controlling neuron voltage. Then, for any suitable current carrier dynamic postulated, we show how the ensemble can be used to compute the voltage PRC with that current carrier. The approach extends to many oscillators measured and controlled through a subset of their coordinates.},
  archive      = {J_BCYB},
  author       = {Wilshin, Simon and Kvalheim, Matthew D. and Revzen, Shai},
  doi          = {10.1007/s00422-024-00997-w},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {5},
  pages        = {311-330},
  shortjournal = {Biol. Cybern.},
  title        = {Phase response curves and the role of coordinates},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational analysis of sensory feedback mechanisms in
powerstroke–recovery systems. <em>BCYB</em>, <em>118</em>(5), 277–309.
(<a href="https://doi.org/10.1007/s00422-024-00996-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the raison d’etre of the brain is the survival of the body, there are relatively few theoretical studies of closed-loop rhythmic motor control systems. In this paper we provide a unified framework, based on variational analysis, for investigating the dual goals of performance and robustness in powerstroke–recovery systems. To demonstrate our variational method, we augment two previously published closed-loop motor control models by equipping each model with a performance measure based on the rate of progress of the system relative to a spatially extended external substrate—such as a long strip of seaweed for a feeding task, or progress relative to the ground for a locomotor task. The sensitivity measure quantifies the ability of the system to maintain performance in response to external perturbations, such as an applied load. Motivated by a search for optimal design principles for feedback control achieving the complementary requirements of efficiency and robustness, we discuss the performance–sensitivity patterns of the systems featuring different sensory feedback architectures. In a paradigmatic half-center oscillator-motor system, we observe that the excitation–inhibition property of feedback mechanisms determines the sensitivity pattern while the activation–inactivation property determines the performance pattern. Moreover, we show that the nonlinearity of the sigmoid activation of feedback signals allows the existence of optimal combinations of performance and sensitivity. In a detailed hindlimb locomotor system, we find that a force-dependent feedback can simultaneously optimize both performance and robustness, while length-dependent feedback variations result in significant performance-versus-sensitivity tradeoffs. Thus, this work provides an analytical framework for studying feedback control of oscillations in nonlinear dynamical systems, leading to several insights that have the potential to inform the design of control or rehabilitation systems.},
  archive      = {J_BCYB},
  author       = {Yu, Zhuojun and Thomas, Peter J.},
  doi          = {10.1007/s00422-024-00996-x},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {5},
  pages        = {277-309},
  shortjournal = {Biol. Cybern.},
  title        = {Variational analysis of sensory feedback mechanisms in powerstroke–recovery systems},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can a hebbian-like learning rule be avoiding the curse of
dimensionality in sparse distributed data? <em>BCYB</em>,
<em>118</em>(5), 267–276. (<a
href="https://doi.org/10.1007/s00422-024-00995-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally assumed that the brain uses something akin to sparse distributed representations. These representations, however, are high-dimensional and consequently they affect classification performance of traditional Machine Learning models due to the “curse of dimensionality”. In tasks for which there is a vast amount of labeled data, Deep Networks seem to solve this issue with many layers and a non-Hebbian backpropagation algorithm. The brain, however, seems to be able to solve the problem with few layers. In this work, we hypothesize that this happens by using Hebbian learning. Actually, the Hebbian-like learning rule of Restricted Boltzmann Machines learns the input patterns asymmetrically. It exclusively learns the correlation between non-zero values and ignores the zeros, which represent the vast majority of the input dimensionality. By ignoring the zeros the “curse of dimensionality” problem can be avoided. To test our hypothesis, we generated several sparse datasets and compared the performance of a Restricted Boltzmann Machine classifier with some Backprop-trained networks. The experiments using these codes confirm our initial intuition as the Restricted Boltzmann Machine shows a good generalization performance, while the Neural Networks trained with the backpropagation algorithm overfit the training data.},
  archive      = {J_BCYB},
  author       = {Osório, Maria and Sa-Couto, Luis and Wichert, Andreas},
  doi          = {10.1007/s00422-024-00995-y},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {5},
  pages        = {267-276},
  shortjournal = {Biol. Cybern.},
  title        = {Can a hebbian-like learning rule be avoiding the curse of dimensionality in sparse distributed data?},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Astrocyte-mediated neuronal irregularities and dynamics: The
complexity of the tripartite synapse. <em>BCYB</em>, <em>118</em>(5),
249–266. (<a href="https://doi.org/10.1007/s00422-024-00994-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in recent decades, gaining a comprehensive understanding of brain computations remains a significant challenge in neuroscience. Using computational models is crucial for unraveling this complex phenomenon and is equally indispensable for studying neurological disorders. This endeavor has created many neuronal models that capture brain dynamics at various scales and complexities. However, most existing models do not account for the potential influence of glial cells, particularly astrocytes, on neuronal physiology. This gap persists even with the emerging evidence indicating their critical role in regulating neural network activity, plasticity, and even neurological pathologies. To address this gap, some works proposed models that include neuron–glia interactions. Also, while some literature focuses on sophisticated models of neuron–glia interactions that mimic the complexity of physiological phenomena, there are also existing works that propose simplified models of neural–glial ensembles. Building upon these efforts, we aimed to contribute further to the field by proposing a simplified tripartite synapse model that encompasses the presynaptic neuron, postsynaptic neuron, and astrocyte. We defined the tripartite synapse model based on the Adaptive Exponential Integrate-and-Fire neuron model and a simplified scheme of the astrocyte model previously proposed by Postnov. Through our simulations, we demonstrated how astrocytes can influence neuronal firing behavior by sequentially activating and deactivating different pathways within the tripartite synapse. This modulation by astrocytes can shape neuronal behavior and introduce irregularities in the firing patterns of both presynaptic and postsynaptic neurons through the introduction of new pathways and configurations of relevant parameters.},
  archive      = {J_BCYB},
  author       = {Garcia, Den Whilrex and Jacquir, Sabir},
  doi          = {10.1007/s00422-024-00994-z},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {5},
  pages        = {249-266},
  shortjournal = {Biol. Cybern.},
  title        = {Astrocyte-mediated neuronal irregularities and dynamics: The complexity of the tripartite synapse},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How the brain can be trained to achieve an intermittent
control strategy for stabilizing quiet stance by means of reinforcement
learning. <em>BCYB</em>, <em>118</em>(3), 229–248. (<a
href="https://doi.org/10.1007/s00422-024-00993-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stabilization of human quiet stance is achieved by a combination of the intrinsic elastic properties of ankle muscles and an active closed-loop activation of the ankle muscles, driven by the delayed feedback of the ongoing sway angle and the corresponding angular velocity in a way of a delayed proportional (P) and derivative (D) feedback controller. It has been shown that the active component of the stabilization process is likely to operate in an intermittent manner rather than as a continuous controller: the switching policy is defined in the phase-plane, which is divided in dangerous and safe regions, separated by appropriate switching boundaries. When the state enters a dangerous region, the delayed PD control is activated, and it is switched off when it enters a safe region, leaving the system to evolve freely. In comparison with continuous feedback control, the intermittent mechanism is more robust and capable to better reproduce postural sway patterns in healthy people. However, the superior performance of the intermittent control paradigm as well as its biological plausibility, suggested by experimental evidence of the intermittent activation of the ankle muscles, leaves open the quest of a feasible learning process, by which the brain can identify the appropriate state-dependent switching policy and tune accordingly the P and D parameters. In this work, it is shown how such a goal can be achieved with a reinforcement motor learning paradigm, building upon the evidence that, in general, the basal ganglia are known to play a central role in reinforcement learning for action selection and, in particular, were found to be specifically involved in postural stabilization.},
  archive      = {J_BCYB},
  author       = {Takazawa, Tomoki and Suzuki, Yasuyuki and Nakamura, Akihiro and Matsuo, Risa and Morasso, Pietro and Nomura, Taishin},
  doi          = {10.1007/s00422-024-00993-0},
  journal      = {Biological Cybernetics},
  month        = {8},
  number       = {3},
  pages        = {229-248},
  shortjournal = {Biol. Cybern.},
  title        = {How the brain can be trained to achieve an intermittent control strategy for stabilizing quiet stance by means of reinforcement learning},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural coding of space by time. <em>BCYB</em>,
<em>118</em>(3), 215–227. (<a
href="https://doi.org/10.1007/s00422-024-00992-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intertwining of space and time poses a significant scientific challenge, transcending disciplines from philosophy and physics to neuroscience. Deciphering neural coding, marked by its inherent spatial and temporal dimensions, has proven to be a complex task. In this paper, we present insights into temporal and spatial modes of neural coding and their intricate interplay, drawn from neuroscientific findings. We illustrate the conversion of a purely spatial input into the temporal form of a singular spike train, demonstrating storage, transmission to remote locations, and recall through spike bursts corresponding to Sharp Wave Ripples. Moreover, the converted temporal representation can be transformed back into a spatiotemporal pattern. The principles of the transformation process are illustrated using a simple feed-forward spiking neural network. The frequencies and phases of Subthreshold Membrane potential Oscillations play a pivotal role in this framework. The model offers insights into information multiplexing and phenomena such as stretching or compressing time of spike patterns.},
  archive      = {J_BCYB},
  author       = {Löffler, Hubert and Gupta, Daya Shankar and Bahmer, Andreas},
  doi          = {10.1007/s00422-024-00992-1},
  journal      = {Biological Cybernetics},
  month        = {8},
  number       = {3},
  pages        = {215-227},
  shortjournal = {Biol. Cybern.},
  title        = {Neural coding of space by time},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computational neural model that incorporates both
intrinsic dynamics and sensory feedback in the aplysia feeding network.
<em>BCYB</em>, <em>118</em>(3), 187–213. (<a
href="https://doi.org/10.1007/s00422-024-00991-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the nervous system underlying animal motor control can shed light on how animals can adapt flexibly to a changing environment. We focus on the neural basis of feeding control in Aplysia californica. Using the Synthetic Nervous System framework, we developed a model of Aplysia feeding neural circuitry that balances neurophysiological plausibility and computational complexity. The circuitry includes neurons, synapses, and feedback pathways identified in existing literature. We organized the neurons into three layers and five subnetworks according to their functional roles. Simulation results demonstrate that the circuitry model can capture the intrinsic dynamics at neuronal and network levels. When combined with a simplified peripheral biomechanical model, it is sufficient to mediate three animal-like feeding behaviors (biting, swallowing, and rejection). The kinematic, dynamic, and neural responses of the model also share similar features with animal data. These results emphasize the functional roles of sensory feedback during feeding.},
  archive      = {J_BCYB},
  author       = {Li, Yanjun and Webster-Wood, Victoria A. and Gill, Jeffrey P. and Sutton, Gregory P. and Chiel, Hillel J. and Quinn, Roger D.},
  doi          = {10.1007/s00422-024-00991-2},
  journal      = {Biological Cybernetics},
  month        = {8},
  number       = {3},
  pages        = {187-213},
  shortjournal = {Biol. Cybern.},
  title        = {A computational neural model that incorporates both intrinsic dynamics and sensory feedback in the aplysia feeding network},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full hill-type muscle model of the I1/I3 retractor muscle
complex in aplysia californica. <em>BCYB</em>, <em>118</em>(3), 165–185.
(<a href="https://doi.org/10.1007/s00422-024-00990-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coordination of complex behavior requires knowledge of both neural dynamics and the mechanics of the periphery. The feeding system of Aplysia californica is an excellent model for investigating questions in soft body systems’ neuromechanics because of its experimental tractability. Prior work has attempted to elucidate the mechanical properties of the periphery by using a Hill-type muscle model to characterize the force generation capabilities of the key protractor muscle responsible for moving Aplysia’s grasper anteriorly, the I2 muscle. However, the I1/I3 muscle, which is the main driver of retractions of Aplysia’s grasper, has not been characterized. Because of the importance of the musculature’s properties in generating functional behavior, understanding the properties of muscles like the I1/I3 complex may help to create more realistic simulations of the feeding behavior of Aplysia, which can aid in greater understanding of the neuromechanics of soft-bodied systems. To bridge this gap, in this work, the I1/I3 muscle complex was characterized using force-frequency, length-tension, and force-velocity experiments and showed that a Hill-type model can accurately predict its force-generation properties. Furthermore, the muscle’s peak isometric force and stiffness were found to exceed those of the I2 muscle, and these results were analyzed in the context of prior studies on the I1/I3 complex’s kinematics in vivo.},
  archive      = {J_BCYB},
  author       = {Sukhnandan, Ravesh and Chen, Qianxue and Shen, Jiayi and Pao, Samantha and Huan, Yu and Sutton, Gregory P. and Gill, Jeffrey P. and Chiel, Hillel J. and Webster-Wood, Victoria A.},
  doi          = {10.1007/s00422-024-00990-3},
  journal      = {Biological Cybernetics},
  month        = {8},
  number       = {3},
  pages        = {165-185},
  shortjournal = {Biol. Cybern.},
  title        = {Full hill-type muscle model of the I1/I3 retractor muscle complex in aplysia californica},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COVID-19 and silent hypoxemia in a minimal closed-loop model
of the respiratory rhythm generator. <em>BCYB</em>, <em>118</em>(3),
145–163. (<a href="https://doi.org/10.1007/s00422-024-00989-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Silent hypoxemia, or “happy hypoxia,” is a puzzling phenomenon in which patients who have contracted COVID-19 exhibit very low oxygen saturation ( $$\text {SaO}_2$$ &lt; 80%) but do not experience discomfort in breathing. The mechanism by which this blunted response to hypoxia occurs is unknown. We have previously shown that a computational model of the respiratory neural network (Diekman et al. in J Neurophysiol 118(4):2194–2215, 2017) can be used to test hypotheses focused on changes in chemosensory inputs to the central pattern generator (CPG). We hypothesize that altered chemosensory function at the level of the carotid bodies and/or the nucleus tractus solitarii are responsible for the blunted response to hypoxia. Here, we use our model to explore this hypothesis by altering the properties of the gain function representing oxygen sensing inputs to the CPG. We then vary other parameters in the model and show that oxygen carrying capacity is the most salient factor for producing silent hypoxemia. We call for clinicians to measure hematocrit as a clinical index of altered physiology in response to COVID-19 infection.},
  archive      = {J_BCYB},
  author       = {Diekman, Casey O. and Thomas, Peter J. and Wilson, Christopher G.},
  doi          = {10.1007/s00422-024-00989-w},
  journal      = {Biological Cybernetics},
  month        = {8},
  number       = {3},
  pages        = {145-163},
  shortjournal = {Biol. Cybern.},
  title        = {COVID-19 and silent hypoxemia in a minimal closed-loop model of the respiratory rhythm generator},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic computational model of the parallel circuit on the
basal ganglia-cortex associated with parkinson’s disease dementia.
<em>BCYB</em>, <em>118</em>(1), 127–143. (<a
href="https://doi.org/10.1007/s00422-024-00988-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cognitive impairment will gradually appear over time in Parkinson&#39;s patients, which is closely related to the basal ganglia-cortex network. This network contains two parallel circuits mediated by putamen and caudate nucleus, respectively. Based on the biophysical mean-field model, we construct a dynamic computational model of the parallel circuit in the basal ganglia-cortex network associated with Parkinson&#39;s disease dementia. The simulated results show that the decrease of power ratio in the prefrontal cortex is mainly caused by dopamine depletion in the caudate nucleus and is less related to that in the putamen, which indicates Parkinson&#39;s disease dementia may be caused by a lesion of the caudate nucleus rather than putamen. Furthermore, the underlying dynamic mechanism behind the decrease of power ratio is investigated by bifurcation analysis, which demonstrates that the decrease of power ratio is due to the change of brain discharge pattern from the limit cycle mode to the point attractor mode. More importantly, the spatiotemporal course of dopamine depletion in Parkinson&#39;s disease patients is well simulated, which states that with the loss of dopaminergic neurons projecting to the striatum, motor dysfunction of Parkinson&#39;s disease is first observed, whereas cognitive impairment occurs after a period of onset of motor dysfunction. These results are helpful to understand the pathogenesis of cognitive impairment and provide insights into the treatment of Parkinson&#39;s disease dementia.},
  archive      = {J_BCYB},
  author       = {Yang, Hao and Yang, XiaoLi and Yan, SiLu},
  doi          = {10.1007/s00422-024-00988-x},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {127-143},
  shortjournal = {Biol. Cybern.},
  title        = {A dynamic computational model of the parallel circuit on the basal ganglia-cortex associated with parkinson’s disease dementia},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling flat-foot limit cycle walkers with compliant
joints based on local stability variation. <em>BCYB</em>,
<em>118</em>(1), 111–126. (<a
href="https://doi.org/10.1007/s00422-024-00987-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates local stability of a four-link limit cycle walking biped with flat feet and compliant ankle joints. Local stability represents the behavior along the solution trajectory between Poincare sections, which can provide detailed information about the evolution of disturbances. The effects of ankle stiffness and foot structure on local stability are studied. In addition, we apply a control strategy based on local stability analysis to the limit cycle walker. Control is applied only in the phases with poor local stability. Simulation results show that the energy consumption is reduced without sacrificing disturbance rejection ability. This study may be helpful in motion control of limit cycle bipedal walking robots with flat feet and ankle stiffness and understanding of human walking principles.},
  archive      = {J_BCYB},
  author       = {Huang, Yan and Gao, Yue and Huang, Qiang and Wang, Qining},
  doi          = {10.1007/s00422-024-00987-y},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {111-126},
  shortjournal = {Biol. Cybern.},
  title        = {Controlling flat-foot limit cycle walkers with compliant joints based on local stability variation},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical modeling and prediction of neuronal dynamics.
<em>BCYB</em>, <em>118</em>(1), 83–110. (<a
href="https://doi.org/10.1007/s00422-024-00986-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical modeling of neuronal dynamics has experienced a fast growth in the last decades thanks to the biophysical formalism introduced by Hodgkin and Huxley in the 1950s. Other types of models (for instance, integrate and fire models), although less realistic, have also contributed to understand neuronal dynamics. However, there is still a vast volume of data that have not been associated with a mathematical model, mainly because data are acquired more rapidly than they can be analyzed or because it is difficult to analyze (for instance, if the number of ionic channels involved is huge). Therefore, developing new methodologies to obtain mathematical or computational models associated with data (even without previous knowledge of the source) can be helpful to make future predictions. Here, we explore the capability of a wavelet neural network to identify neuronal (single-cell) dynamics. We present an optimized computational scheme that trains the ANN with biologically plausible input currents. We obtain successful identification for data generated from four different neuron models when using all variables as inputs of the network. We also show that the empiric model obtained is able to generalize and predict the neuronal dynamics generated by variable input currents different from those used to train the artificial network. In the more realistic situation of using only the voltage and the injected current as input data to train the network, we lose predictive ability but, for low-dimensional models, the results are still satisfactory. We understand our contribution as a first step toward obtaining empiric models from experimental voltage traces.},
  archive      = {J_BCYB},
  author       = {Fisco-Compte, Pau and Aquilué-Llorens, David and Roqueiro, Nestor and Fossas, Enric and Guillamon, Antoni},
  doi          = {10.1007/s00422-024-00986-z},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {83-110},
  shortjournal = {Biol. Cybern.},
  title        = {Empirical modeling and prediction of neuronal dynamics},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stability against fluctuations: A two-dimensional study of
scaling, bifurcations and spontaneous symmetry breaking in stochastic
models of synaptic plasticity. <em>BCYB</em>, <em>118</em>(1), 39–81.
(<a href="https://doi.org/10.1007/s00422-024-00985-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic models of synaptic plasticity must confront the corrosive influence of fluctuations in synaptic strength on patterns of synaptic connectivity. To solve this problem, we have proposed that synapses act as filters, integrating plasticity induction signals and expressing changes in synaptic strength only upon reaching filter threshold. Our earlier analytical study calculated the lifetimes of quasi-stable patterns of synaptic connectivity with synaptic filtering. We showed that the plasticity step size in a stochastic model of spike-timing-dependent plasticity (STDP) acts as a temperature-like parameter, exhibiting a critical value below which neuronal structure formation occurs. The filter threshold scales this temperature-like parameter downwards, cooling the dynamics and enhancing stability. A key step in this calculation was a resetting approximation, essentially reducing the dynamics to one-dimensional processes. Here, we revisit our earlier study to examine this resetting approximation, with the aim of understanding in detail why it works so well by comparing it, and a simpler approximation, to the system’s full dynamics consisting of various embedded two-dimensional processes without resetting. Comparing the full system to the simpler approximation, to our original resetting approximation, and to a one-afferent system, we show that their equilibrium distributions of synaptic strengths and critical plasticity step sizes are all qualitatively similar, and increasingly quantitatively similar as the filter threshold increases. This increasing similarity is due to the decorrelation in changes in synaptic strength between different afferents caused by our STDP model, and the amplification of this decorrelation with larger synaptic filters.},
  archive      = {J_BCYB},
  author       = {Elliott, Terry},
  doi          = {10.1007/s00422-024-00985-0},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {39-81},
  shortjournal = {Biol. Cybern.},
  title        = {Stability against fluctuations: A two-dimensional study of scaling, bifurcations and spontaneous symmetry breaking in stochastic models of synaptic plasticity},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG rhythm separation and time–frequency analysis of fast
multivariate empirical mode decomposition for motor imagery BCI.
<em>BCYB</em>, <em>118</em>(1), 21–37. (<a
href="https://doi.org/10.1007/s00422-024-00984-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery electroencephalogram (EEG) is widely employed in brain–computer interface (BCI) systems. As a time–frequency analysis method for nonlinear and non-stationary signals, multivariate empirical mode decomposition (MEMD) and its noise-assisted version (NA-MEMD) has been widely used in the preprocessing step of BCI systems for separating EEG rhythms corresponding to specific brain activities. However, when applied to multichannel EEG signals, MEMD or NA-MEMD often demonstrate low robustness to noise and high computational complexity. To address these issues, we have explored the advantages of our recently proposed fast multivariate empirical mode decomposition (FMEMD) and its noise-assisted version (NA-FMEMD) for analyzing motor imagery data. We emphasize that FMEMD enables a more accurate estimation of EEG frequency information and exhibits a more noise-robust decomposition performance with improved computational efficiency. Comparative analysis with MEMD on simulation data and real-world EEG validates the above assertions. The joint average frequency measure is employed to automatically select intrinsic mode functions that correspond to specific frequency bands. Thus, FMEMD-based classification architecture is proposed. Using FMEMD as a preprocessing algorithm instead of MEMD can improve the classification accuracy by 2.3% on the BCI Competition IV dataset. On the Physiobank Motor/Mental Imagery dataset and BCI Competition IV Dataset 2a, FMEMD-based architecture also attained a comparable performance to complex algorithms. The results indicate that FMEMD proficiently extracts feature information from small benchmark datasets while mitigating dimensionality constraints resulting from computational complexity. Hence, FMEMD or NA-FMEMD can be a powerful time–frequency preprocessing method for BCI.},
  archive      = {J_BCYB},
  author       = {Jiao, Yang and Zheng, Qian and Qiao, Dan and Lang, Xun and Xie, Lei and Pan, Yi},
  doi          = {10.1007/s00422-024-00984-1},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {21-37},
  shortjournal = {Biol. Cybern.},
  title        = {EEG rhythm separation and time–frequency analysis of fast multivariate empirical mode decomposition for motor imagery BCI},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fluctuation–response relations for integrate-and-fire models
with an absolute refractory period. <em>BCYB</em>, <em>118</em>(1),
7–19. (<a href="https://doi.org/10.1007/s00422-023-00982-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of relating the spontaneous fluctuations of a stochastic integrate-and-fire (IF) model to the response of the instantaneous firing rate to time-dependent stimulation if the IF model is endowed with a non-vanishing refractory period and a finite (stereotypical) spike shape. This seemingly harmless addition to the model is shown to complicate the analysis put forward by Lindner Phys. Rev. Lett. (2022), i.e., the incorporation of the reset into the model equation, the Rice-like averaging of the stochastic differential equation, and the application of the Furutsu–Novikov theorem. We derive a still exact (although more complicated) fluctuation–response relation (FRR) for an IF model with refractory state and a white Gaussian background noise. We also briefly discuss an approximation for the case of a colored Gaussian noise and conclude with a summary and outlook on open problems.},
  archive      = {J_BCYB},
  author       = {Puttkammer, Friedrich and Lindner, Benjamin},
  doi          = {10.1007/s00422-023-00982-9},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {7-19},
  shortjournal = {Biol. Cybern.},
  title        = {Fluctuation–response relations for integrate-and-fire models with an absolute refractory period},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What have we learned about artificial intelligence from
studying the brain? <em>BCYB</em>, <em>118</em>(1), 1–5. (<a
href="https://doi.org/10.1007/s00422-024-00983-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience and artificial intelligence (AI) share a long, intertwined history. It has been argued that discoveries in neuroscience were (and continue to be) instrumental in driving the development of new AI technology. Scrutinizing these historical claims yields a more nuanced story, where AI researchers were loosely inspired by the brain, but ideas flowed mostly in the other direction.},
  archive      = {J_BCYB},
  author       = {Gershman, Samuel J.},
  doi          = {10.1007/s00422-024-00983-2},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Biol. Cybern.},
  title        = {What have we learned about artificial intelligence from studying the brain?},
  volume       = {118},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
