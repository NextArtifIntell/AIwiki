<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui---22">JMUI - 22</h2>
<ul>
<li><details>
<summary>
(2024). HaM3D: Generalized XR-based multimodal HRI framework with
haptic feedback for industry 4.0. <em>JMUI</em>, <em>18</em>(4),
331–349. (<a href="https://doi.org/10.1007/s12193-024-00443-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current generation of Industry 4.0 involves the integration of automation and data in manufacturing, while the upcoming generation, Industry 5.0, aims to enable seamless interaction between humans, robots, and machines for sustainable and personalized product and service creation. This necessitates manufacturing components according to customer wishes, leading to frequent changes in the production line. In this context, automation proves unsustainable and could be addressed through Human–Robot Cooperation. In this paper, an Extended Reality (XR)-based Human–Robot Interaction with haptic feedback has been proposed to assist users in remotely performing tasks. The haptic feedback, generated using the Artificial Potential Field method, aids users in controlling robots without collisions. This paper introduces a multimodal XR-based framework for effective communication between humans and robots, utilizing vision models to provide information about the remote side to local users. This seamless communication is facilitated through multiple modalities such as haptic, speech, visual, and text. The system’s effectiveness through pick-and-place activities and adaptation to it for complex industry use cases such as peg in the hole and welding are further evaluated. The results demonstrate the system’s efficacy in two specific human–robot collaborative scenarios in industries: arc welding and assembly tasks.},
  archive      = {J_JMUI},
  author       = {Raj, Subin and Beri, Nirbhay and Patel, Dishank Sureshkumar and Sinha, Yashaswi and Chakrabarti, Amaresh and Biswas, Pradipta},
  doi          = {10.1007/s12193-024-00443-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {331-349},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {HaM3D: Generalized XR-based multimodal HRI framework with haptic feedback for industry 4.0},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: Comparing head-mounted and handheld augmented
reality for guided assembly. <em>JMUI</em>, <em>18</em>(4), 329. (<a
href="https://doi.org/10.1007/s12193-024-00441-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Leins, Nicolas and Gonnermann-Müller, Jana and Teichmann, Malte},
  doi          = {10.1007/s12193-024-00441-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {329},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Correction: Comparing head-mounted and handheld augmented reality for guided assembly},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Comparing head-mounted and handheld augmented reality for
guided assembly. <em>JMUI</em>, <em>18</em>(4), 313–328. (<a
href="https://doi.org/10.1007/s12193-024-00440-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different Augmented Reality (AR) displays are becoming more commonly used for work since AR promises benefits by offering support, e.g., with additional information or hints. However, most research compares AR with traditional work support, like paper-based or web-based instructions. Since various AR technologies offer device-specific advantages and disadvantages, different AR technologies are more or less suitable to offer support without overwhelming or distracting the worker. Research, therefore, needs to derive empirical results from comparing different AR displays to derive concrete recommendations for action on the use and design of AR for specific contexts. To address this research gap, this experimental study investigates the effect of video-see-through head-mounted AR (Varjo XR-3) vs. handheld AR (Apple iPad) on performance (time and committed failure), motivation, and cognitive load for guided assembly. The study results reveal that both AR displays can successfully guide people in guided assembly tasks. On a descriptive level, the head-mounted AR device reveals slightly better results in terms of time and committed failures. Notably, the impact of technical restrictions on the study results was still evident. Accordingly, further investigation of device-specific differences is of continuing importance.},
  archive      = {J_JMUI},
  author       = {Leins, Nicolas and Gonnermann-Müller, Jana and Teichmann, Malte},
  doi          = {10.1007/s12193-024-00440-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {313-328},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Comparing head-mounted and handheld augmented reality for guided assembly},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed interaction: Evaluating user interactions for object
manipulations in virtual space. <em>JMUI</em>, <em>18</em>(4), 297–311.
(<a href="https://doi.org/10.1007/s12193-024-00431-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an evaluation of a potential new interaction mode in virtual reality (VR) to determine whether it provides any positive impact in terms of how users interact with content. We evaluated the user experiences for 3D object manipulation across three modes of interaction. Interaction using controllers and gestures are used as baselines from which to gauge the potential value of the new mode of interaction, where a single controller and gestures are combined. This paper reports on a user study that captures quantitative and qualitative data related to a variety of object manipulation tasks in a Virtual Environment (VE). We investigated the impact of this new interaction mode with 40 participants across a number of interaction tasks, with the quantitative evaluation indicating that generally, the mixed mode of interaction resulted in task completion times consistently faster than gesture-based interaction and, in some cases, faster than with the use of controllers alone. A qualitative evaluation of the user experience indicated potential application areas for the new mode of interaction.},
  archive      = {J_JMUI},
  author       = {Lee, Yemon and Connor, Andy M. and Marks, Stefan},
  doi          = {10.1007/s12193-024-00431-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {297-311},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Mixed interaction: Evaluating user interactions for object manipulations in virtual space},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards the development of an automated robotic storyteller:
Comparing approaches for emotional story annotation for non-verbal
expression via body language. <em>JMUI</em>, <em>18</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s12193-024-00429-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is a long-established tradition and listening to stories is still a popular leisure activity. Caused by technization, storytelling media expands, e.g., to social robots acting as multi-modal storytellers, using different multimodal behaviours such as facial expressions or body postures. With the overarching goal to automate robotic storytelling, we have been annotating stories with emotion labels which the robot can use to automatically adapt its behavior. With it, three different approaches are compared in two studies in this paper: 1) manual labels by human annotators (MA), 2) software-based word-sensitive annotation using the Linguistic Inquiry and Word Count program (LIWC), and 3) a machine learning based approach (ML). In an online study showing videos of a storytelling robot, the annotations were validated, with LIWC and MA achieving the best, and ML the worst results. In a laboratory user study, the three versions of the story were compared regarding transportation and cognitive absorption, revealing no significant differences but a positive trend towards MA. On this empirical basis, the Automated Robotic Storyteller was implemented using manual annotations. Future iterations should include other robots and modalities, fewer emotion labels and their probabilities.},
  archive      = {J_JMUI},
  author       = {Steinhaeusser, Sophia C. and Zehe, Albin and Schnetter, Peggy and Hotho, Andreas and Lugrin, Birgit},
  doi          = {10.1007/s12193-024-00429-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Towards the development of an automated robotic storyteller: Comparing approaches for emotional story annotation for non-verbal expression via body language},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-vehicle nudging for increased adaptive cruise control
use: A field study. <em>JMUI</em>, <em>18</em>(2), 257–271. (<a
href="https://doi.org/10.1007/s12193-024-00434-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Close following to lead vehicles is associated with increased risk of rear-end crashes in road traffic. One way to reduce instances of close following is through increased use of the Advanced Driver Assistance System (ADAS) Adaptive Cruise Control (ACC), which is designed to adjust vehicle speed to maintain a safe time headway. Since the activation of ACC is driver-initiated, there is a need to influence the propensity of drivers to use the function. This research aimed to explore whether in-vehicle nudging interventions could be effective for this purpose. A field trial was conducted to consecutively assess the effects of two nudges on drivers’ utilization of ACC, compared to baseline usage. Exposing the participants (n = 49) to the first ambient design nudge resulted in a 46% increase in ACC usage on average. Following the introduction of the second nudge (a competitive leaderboard nudge), the average increase among participants (n = 48) during the complete treatment period reached 61%. The changes in ACC utilization varied between individual drivers, highlighting the need to monitor behavioral outcomes of nudges and adapt them when needed. In conclusion, this research shows that utilizing in-vehicle nudging is a promising approach to increase the use of vehicle functions contributing to improved traffic safety.},
  archive      = {J_JMUI},
  author       = {Gustavsson, Pär and Ljung Aust, Mikael},
  doi          = {10.1007/s12193-024-00434-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {257-271},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {In-vehicle nudging for increased adaptive cruise control use: A field study},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of pedestrian crossing behaviour at unsignalized
intersections using machine learning algorithms: Analysis and
comparison. <em>JMUI</em>, <em>18</em>(2), 239–256. (<a
href="https://doi.org/10.1007/s12193-024-00433-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary safety hazard at unsignalized intersections, particularly in urban areas, is pedestrian-vehicle collisions. Due to its complexity and inattention, pedestrian crossing behaviour has a significant impact on their safety. This study introduces a novel framework to enhance pedestrian safety at unsignalized intersections by developing a predictive model of pedestrian crossing behaviour using machine learning algorithms. While accounting for crossing behaviour as the dependent variable and other independent variables, the analysis prioritises accuracy and internal validity. Important feature scores for the different algorithms were assessed. The model results revealed that the arrival first of a pedestrian or vehicle, pedestrian delay, vehicle speed, pedestrian speed, age, gender, traffic hour, and vehicle category are highly influencing variables for analysing pedestrian behaviour while crossing at unsignalized intersections. This study found that the prediction of pedestrian behaviour based on random forest, extreme gradient boosting and binary logit model achieved 81.72%, 77.19% and 74.95%, respectively. Algorithms, including k-nearest neighbours, artificial neural networks, and support vector machines, have varying classification performance at every step. The findings of this study may be used to support infrastructure-to-vehicle interactions, enabling vehicles to successfully negotiate rolling pedestrian behaviour and improving pedestrian safety.},
  archive      = {J_JMUI},
  author       = {Singh, Dungar and Das, Pritikana and Ghosh, Indrajit},
  doi          = {10.1007/s12193-024-00433-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {239-256},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Prediction of pedestrian crossing behaviour at unsignalized intersections using machine learning algorithms: Analysis and comparison},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truck drivers’ views on the road safety benefits of advanced
driver assistance systems and intelligent transport systems in tanzania.
<em>JMUI</em>, <em>18</em>(2), 229–237. (<a
href="https://doi.org/10.1007/s12193-024-00437-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transportation of goods by road is crucial in Tanzania’s Central and North-South Corridors. However, challenges such as truck congestion and road accidents are affecting the efficiency of these routes. Road crashes are prevalent in low- and middle-income countries, with Africa experiencing an exceptionally high rate. This study examines the opinions of Tanzanian truck drivers on the effectiveness of Advanced Driver Assistance Systems in reducing road safety risks. A discriminant analysis was conducted to assess the awareness and use of these systems among different driver experienced groups. The results highlight the importance of improving infrastructure, ensuring vehicle safety standards, providing comprehensive driver training, and integrating innovative Intelligent Transport Systems to address road safety issues. In conclusion, the study provides valuable insights for policymakers and stakeholders to strengthen road safety measures in Tanzania, facilitating smoother road freight transport operations and promoting economic growth.},
  archive      = {J_JMUI},
  author       = {Chacha, Marwa and Nyaki, Prosper and Cuenen, Ariane and Yasar, Ansar and Wets, Geert},
  doi          = {10.1007/s12193-024-00437-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {229-237},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Truck drivers’ views on the road safety benefits of advanced driver assistance systems and intelligent transport systems in tanzania},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human or robot? Exploring different avatar appearances to
increase perceived security in shared automated vehicles. <em>JMUI</em>,
<em>18</em>(2), 209–228. (<a
href="https://doi.org/10.1007/s12193-024-00436-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared Automated Vehicles (SAVs) promise to make automated mobility accessible to a wide range of people while reducing air pollution and improving traffic flow. In the future, these vehicles will operate with no human driver on board, which poses several challenges that might differ depending on the cultural context and make one-fits-all solutions demanding. A promising substitute for the driver could be Digital Companions (DCs), i.e. conversational agents presented on a screen inside the vehicles. We conducted interviews with Colombian participants and workshops with German and Korean participants and derived two design concepts of DCs as an alternative for the human driver on SAVs: a human-like and a robot-like. We compared these two concepts to a baseline without companion using a scenario-based online questionnaire with participants from Colombia (N = 57), Germany (N = 50), and Korea (N = 29) measuring anxiety, security, trust, risk, control, threat, and user experience. In comparison with the baseline, both DCs are statistically significantly perceived as more positively. While we found a preference for the human-like DC among all participants, this preference is higher among Colombians while Koreans show the highest openness towards the robot-like DC.},
  archive      = {J_JMUI},
  author       = {Schuß, Martina and Pizzoni, Luca and Riener, Andreas},
  doi          = {10.1007/s12193-024-00436-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {209-228},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Human or robot? exploring different avatar appearances to increase perceived security in shared automated vehicles},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What is good? Exploring the applicability of a one item
measure as a proxy for measuring acceptance in driver-vehicle
interaction studies. <em>JMUI</em>, <em>18</em>(2), 195–208. (<a
href="https://doi.org/10.1007/s12193-024-00432-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New driver assistance systems play an important role to rise safety and comfort in todays´ traffic. Those systems should be developed with the needs of the user in mind and tested for the users´ requirements. In this, user acceptance is a central variable of interest – both in scientific and in practical applications of user-centered research on driver assistance systems. In some cases, applied research settings need simplified measurements in order to be efficiently applicable in the driving situations. In the present paper, we explored the applicability and validity of a single-item acceptance measurement (SIAM) for practical study settings covering the attitude towards using new driver assistance systems. To provide a theoretical framing, we tested the one-item measure against the widely used Technology Acceptance Model (TAM) and the van der Laan acceptance scale (VDL) in a driving simulator study. Participants experienced four different complex driving scenarios using a driver assistance system. Acceptance was measured repeatedly throughout the drive. The results supported construct validity for the SIAM, correlating with the VDL. The SIAM further predicted the intention to use the system. Being carefully aware of the psychometric drawbacks of short scales and acknowledging the importance of multi-item scales, the SIAM is promising for efficiently approaching the acceptance of driver assistance systems in applied settings.},
  archive      = {J_JMUI},
  author       = {Buchner, Claudia and Kraus, Johannes and Miller, Linda and Baumann, Martin},
  doi          = {10.1007/s12193-024-00432-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {195-208},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {What is good? exploring the applicability of a one item measure as a proxy for measuring acceptance in driver-vehicle interaction studies},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing driver warning systems for off-road industrial
vehicles using a cyber-physical simulator. <em>JMUI</em>,
<em>18</em>(2), 179–194. (<a
href="https://doi.org/10.1007/s12193-024-00435-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ADAS (Advanced Driver Assistance Systems) are becoming increasingly popular in on-road vehicles due to their safety, productivity, and cost savings. In the same way, off-road vehicles can benefit from ADAS systems to improve the security of drivers and workers in industrial settings. In this work, we study, design, and develop a novel security system to be integrated into industrial vehicles. This system is built to provide one-way Human Computer Interaction, from the computer to the human, so providing, through the interaction with the ADAS system, feedback to drivers about their surroundings, such as nearby workers, and thus helping to avoid collisions and prevent incidents. The study evaluates the quality of different feedback mechanisms, with the goal of designing the ADAS that produces the best User eXperience (UX). These feedback mechanisms are generated by LEDs in different display formats and colors, as well as with haptic feedback. We created a hybrid testbed using a realistic ADAS and a forklift simulator, integrating the system into a physical structure that resembles an industrial vehicle (a forklift) and used a computer-based simulation of a warehouse to gather the information from users. We performed a study with 36 people for the evaluation of the different feedback mechanisms tested, evaluating the results both from an objective point of view based on the results of the simulation, and a subjective point of view through a questionnaire and the stress of the users in each test.},
  archive      = {J_JMUI},
  author       = {Garcia-Carrillo, Dan and Garcia, Roberto and Pañeda, Xabiel G. and Mourao, Filipa and Melendi, David and Corcoba, Victor and Paiva, Sara},
  doi          = {10.1007/s12193-024-00435-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {179-194},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Testing driver warning systems for off-road industrial vehicles using a cyber-physical simulator},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sonically-enhanced in-vehicle air gesture interactions:
Evaluation of different spearcon compression rates. <em>JMUI</em>,
<em>18</em>(2), 159–177. (<a
href="https://doi.org/10.1007/s12193-024-00430-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver distraction is a major contributor to road vehicle crashes, and visual distraction caused by using in-vehicle infotainment systems (IVIS) degrades driving performance and increases crash risk. Air gesture interfaces have been developed to mitigate driver distraction, and using auditory displays showed a decrease in off-road glances and an improved perceived workload. However, the potential of auditory display was not fully investigated. The present paper presents directional research in the design of auditory displays for air-gesture IVIS through a dual-task experiment of driving a simulator and air-gesture menu navigation. Twenty-four participants utilized the in-vehicle air gesture interfaces while driving with four auditory display conditions (text-to-speech, 70% compressed spearcons, 40% compressed spearcson, and no sound). The results showed that 70% spearcon reduced visual distraction and increased menu navigation accuracy but with increased navigation time. 70% spearcon was most preferred by the participants. Driving performance or workload did not show any difference among the conditions. Implications are discussed with the design guidelines for future implementations.},
  archive      = {J_JMUI},
  author       = {Tabbarah, Moustafa and Cao, Yusheng and Fang, Ziming and Li, Lingyu and Jeon, Myounghoon},
  doi          = {10.1007/s12193-024-00430-3},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {159-177},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Sonically-enhanced in-vehicle air gesture interactions: Evaluation of different spearcon compression rates},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on “user-centered advanced driver assistance
systems (UCADAS).” <em>JMUI</em>, <em>18</em>(2), 157–158. (<a
href="https://doi.org/10.1007/s12193-024-00439-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Patel, Ankit R. and Wintersberger, Philipp and Souders, Dustin and Callari, Tiziana C. and Stoll, Tanja},
  doi          = {10.1007/s12193-024-00439-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {2},
  pages        = {157-158},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Special issue on “User-centered advanced driver assistance systems (UCADAS)”},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review of substitutive assistive tools and technologies for
people with visual impairments: Recent advancements and prospects.
<em>JMUI</em>, <em>18</em>(1), 135–156. (<a
href="https://doi.org/10.1007/s12193-023-00427-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of many tools and technologies for people with visual impairment has become a major priority in the field of assistive technology research. However, many of these technology advancements have limitations in terms of the human aspects of the user experience (e.g., usability, learnability, and time to user adaptation) as well as difficulties in translating research prototypes into production. Also, there was no clear distinction between the assistive aids of adults and children, as well as between “partial impairment” and “total blindness”. As a result of these limitations, the produced aids have not gained much popularity and the intended users are still hesitant to utilise them. This paper presents a comprehensive review of substitutive interventions that aid in adapting to vision loss, centred on laboratory research studies to assess user-system interaction and system validation. Depending on the primary cueing feedback signal offered to the user, these technology aids are categorized as visual, haptics, or auditory-based aids. The context of use, cueing feedback signals, and participation of visually impaired people in the evaluation are all considered while discussing these aids. Based on the findings, a set of recommendations is suggested to assist the scientific community in addressing persisting challenges and restrictions faced by both the totally blind and partially sighted people.},
  archive      = {J_JMUI},
  author       = {Muhsin, Zahra J. and Qahwaji, Rami and Ghanchi, Faruque and Al-Taee, Majid},
  doi          = {10.1007/s12193-023-00427-4},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {135-156},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Review of substitutive assistive tools and technologies for people with visual impairments: Recent advancements and prospects},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmented reality and deep learning based system for
assisting assembly process. <em>JMUI</em>, <em>18</em>(1), 119–133. (<a
href="https://doi.org/10.1007/s12193-023-00428-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Industry 4.0, manufacturing entails a rapid change in customer demands which leads to mass customization. The variation in customer requirements leads to small batch sizes and several process variations. Assembly task is one of most important steps in any manufacturing process. A factory floor worker often needs a guidance system due to variations in product or process, to assist them in assembly task. Existing Augmented Reality (AR) based systems use markers for each assembly component for detection which is time consuming and laborious. This paper proposed utilizing state-of-the-art deep learning based object detection technique and employed a regression based mapping technique to obtain the 3D locations of assembly components. Automatic detection of machine parts was followed by a multimodal interface involving both eye gaze and hand tracking to guide the manual assembly process. We proposed eye cursor to guide the user through the task and utilized fingertip distances along with object sizes to detect any error committed during the task. We analyzed the proposed mapping method and found that the mean mapping error was 1.842 cm. We also investigated the effectiveness of the proposed multimodal user interface by conducting two user studies. The first study indicated that the current interface design with eye cursor enabled participants to perform the task significantly faster compared to the interface without eye cursor. The shop floor workers during the second user study reported that the proposed guidance system was comprehendible and easy to use to complete the assembly task. Results showed that the proposed guidance system enabled 11 end users to finish the assembly of one pneumatic cylinder within 55 s with average TLX score less than 25 in a scale of 100 and Cronbach alpha score of 0.8 indicating convergence of learning experience.},
  archive      = {J_JMUI},
  author       = {Raj, Subin and Murthy, L. R. D. and Shanmugam, Thanikai Adhithiyan and Kumar, Gyanig and Chakrabarti, Amaresh and Biswas, Pradipta},
  doi          = {10.1007/s12193-023-00428-3},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {119-133},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Augmented reality and deep learning based system for assisting assembly process},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling the “transactive memory system” in multimodal
multiparty interactions. <em>JMUI</em>, <em>18</em>(1), 103–117. (<a
href="https://doi.org/10.1007/s12193-023-00426-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transactive memory system (TMS) is a team emergent state representing the knowledge of each member about “who knows what” in a team performing a joint task. We present a study to show how the three TMS dimensions Credibility, Specialisation, Coordination, can be modelled as a linear combination of the nonverbal multimodal features displayed by the team performing the joint task. Results indicate that, to some extent, the three dimensions of TMS can be expressed as a linear combination of nonverbal multimodal features. Moreover, the higher the number of modalities (audio, movement, spatial), the better the modelling. Results could be used in future work to design human-centered computing applications able to automatically estimate TMS from teams’ behavioural patterns, to provide feedback and help teams’ interactions.},
  archive      = {J_JMUI},
  author       = {Biancardi, Beatrice and Mancini, Maurizio and Ravenet, Brian and Varni, Giovanna},
  doi          = {10.1007/s12193-023-00426-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {103-117},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Modelling the “transactive memory system” in multimodal multiparty interactions},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study on the attention of people with low vision to
accessibility guidance signs. <em>JMUI</em>, <em>18</em>(1), 87–101. (<a
href="https://doi.org/10.1007/s12193-023-00417-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The function of accessibility guide signs is to convey information to users. The key to designing accessibility guide signs is to improve the efficiency with which they convey information. In this paper, 16 subjects were recruited to study their attentional status when faced with different forms of accessibility sign design by setting up two sets of comparison tests. The subjects watched six videos containing different sign designs with different lighting effects to compare their attention to the different sign designs. We collected the participants&#39; eye-movement, EEG, and HRV data during the experiment, and the PSSUQ questionnaire was administered. The data showed that subjects could quickly attend to the processed signs but did not show significant differences in brain responses. Among the study variables, there were significant differences in the effects of different light frequencies on subjects&#39; attention. Study results suggest that designers can consider the existing sign designs for public places and add richer visual information to the designs, thus improving the efficiency of information transmission.},
  archive      = {J_JMUI},
  author       = {Jiang, Weitao and Zhang, Bingxin and Sun, Ruiqi and Zhang, Dong and Hu, Shan},
  doi          = {10.1007/s12193-023-00417-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {87-101},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A study on the attention of people with low vision to accessibility guidance signs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing alternative modalities in the context of
multimodal human–robot interaction. <em>JMUI</em>, <em>18</em>(1),
69–85. (<a href="https://doi.org/10.1007/s12193-023-00421-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of interactive technology, alternative input modalities are often used, instead of conventional ones, to create intuitive, efficient, and user-friendly avenues of controlling and collaborating with robots. Researchers have examined the efficacy of natural interaction modalities such as gesture or voice in single and dual-task scenarios. These investigations have aimed to address the potential of the modalities on diverse applications encompassing activities like online shopping, precision agriculture, and mechanical component assembly, which involve tasks like object pointing and selection. This article aims to address the impact on user performance in a practical human–robot interaction application where a fixed-base robot is controlled through the utilization of natural alternative modalities. We explored this by investigating the impact of single-task and dual-task conditions on user performance for object picking and dropping. We undertook two user studies—one focusing on single-task scenarios, employing a fixed-base robot for object picking and dropping and the other encompassing dual-task conditions, utilizing a mobile robot for a driving scenario. We measured task completion times and estimated cognitive workload through the NASA Task Load Index (TLX), which offers a subjective, multidimensional scale measuring the perceived cognitive workload of a user. The studies revealed that the ranking of completion times for the alternative modalities remained consistent across both single-task and dual-task scenarios. However, the ranking based on perceived cognitive load was different. In the single-task study, the gesture-based modality resulted the highest TLX score, contrasting with the dual-task study, where the highest TLX score was associated with the eye gaze-based modality. Likewise, the speech-based modality achieved a lower TLX score compared to eye gaze and gesture in the single-task study, but its TLX score in the dual-task study was between gesture and eye gaze. These outcomes suggest that the efficacy of alternative modalities is contingent not only on user preferences but also on the specific situational context.},
  archive      = {J_JMUI},
  author       = {Saren, Suprakas and Mukhopadhyay, Abhishek and Ghose, Debasish and Biswas, Pradipta},
  doi          = {10.1007/s12193-023-00421-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {69-85},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Comparing alternative modalities in the context of multimodal human–robot interaction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal exploration in elementary music classroom.
<em>JMUI</em>, <em>18</em>(1), 55–68. (<a
href="https://doi.org/10.1007/s12193-023-00420-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vibrotactile feedback can support and enhance various musical learning processes of both children and adults, thus improving their musical experience. However, until now, to the best knowledge of the authors, only a few studies have focused on the auditory-tactile music perception in childhood. In this paper, we evaluate a bimodal auditory-tactile teaching approach for elementary school children. The experiment was designed based on the two-group-pretest-posttest design. Children allocated to the experimental group were subjected to auditory-tactile music training, while those allocated to the control-group completed the same training sessions stimulated by audio-only signals. Both groups received common, especially designed musical activities, divided into sixteen sessions. In order to evaluate the experimental procedure, children completed an auditory ability test in three different timepoints: before, half-way through, and at the end of the study. Data from 60 children were analyzed. The main aim of the study was to explore beat sensitivity in simple and complex music metrical structures and examine whether auditory-tactile stimulation could enhance the children’s ability to recognize beat within various musical contexts. Results demonstrate notable performance improvements within the experimental group, suggesting that a multimodal music experience can improve beat induction abilities of both simple and complex meters for elementary school children.},
  archive      = {J_JMUI},
  author       = {Papadogianni, Martha and Altinsoy, Ercan and Andreopoulou, Areti},
  doi          = {10.1007/s12193-023-00420-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {55-68},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Multimodal exploration in elementary music classroom},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hearing loss prevention at loud music events via real-time
visuo-haptic feedback. <em>JMUI</em>, <em>18</em>(1), 43–53. (<a
href="https://doi.org/10.1007/s12193-023-00419-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hearing loss is becoming a global problem, partly as a consequence of exposure to loud music. People may be unaware about the harmful sound levels and consequent damages caused by loud music at venues such as discotheques or festivals. Earplugs are effective in reducing the risk of noise-induced hearing loss but have been shown to be an insufficient prevention strategy. Thus, when it is not possible to lower the volume of the sound source, a viable solution to the problem is to relocate to quieter locations from time to time. In this context, this study introduces a bracelet device with the goal of warning users when the music sound level is too loud in their specific location, via haptic, visual or visuo-haptic feedback. The bracelet embeds a microphone, a microcontroller, an LED strip and four vibration motors. We performed a user study where thirteen participants were asked to react to the three kinds of feedback during a simulated disco club event where the volume of music pieces varied to reach a loud intensity. Results showed that participants never missed the above threshold notification via all types of feedback, but visual feedback led to the slowest reaction times and was deemed the least effective. In line with the findings reported in the hearing loss prevention literature, the perceived usefulness of the proposed device was highly dependent on participants’ subjective approach to the topic of hearing risks at loud music events as well as their willingness to take action regarding its prevention. Ultimately, our study shows how technology, no matter how effective, may not be able to cope with these kinds of cultural issues concerning hearing loss prevention. Educational strategies may represent a more effective solution to the real problem of changing people’s attitudes and motivations to want to protect their hearing.},
  archive      = {J_JMUI},
  author       = {Turchet, Luca and Luiten, Simone and Treub, Tjebbe and van der Burgt, Marloes and Siani, Costanza and Boem, Alberto},
  doi          = {10.1007/s12193-023-00419-4},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {43-53},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Hearing loss prevention at loud music events via real-time visuo-haptic feedback},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A social robot as your reading companion: Exploring the
relationships between gaze patterns and knowledge gains. <em>JMUI</em>,
<em>18</em>(1), 21–41. (<a
href="https://doi.org/10.1007/s12193-023-00418-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent tutoring systems have been widely used in educational activities over the past 20 years. With significantly less effort put into writing or reading assistance, the majority of intelligent tutoring systems focus on mathematics or problem solving activities. However, with the development of E-reading-centered E-education, how to improve students’ learning performance during reading has become increasingly important. Therefore, in this paper, we take a first step in the direction of an adaptive intelligent tutoring system by investigating how different reading strategies relate to knowledge gain based on gaze features and how an embodied social robot affects gaze patterns and reading strategies. The findings showed that different knowledge gains have significant differences in scanning methods and reading depth, and that the feedback given by social robots significantly affects participants’ gaze patterns during the whole reading process. To automatically differentiate between two levels of knowledge gain, several prediction experiments based on various reading strategy-related gaze features were carried out. The results demonstrate that saccades are the best predictors of knowledge gain, with the best model having an average accuracy of 74.2%. Finally, real-time simulation experiments were conducted with sixty participants using the leave-one-out method, and an accurate prediction of the level of knowledge gain of 71. 5% was achieved.},
  archive      = {J_JMUI},
  author       = {Liu, Xuan and Ma, Jiachen and Wang, Qiang},
  doi          = {10.1007/s12193-023-00418-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {21-41},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A social robot as your reading companion: Exploring the relationships between gaze patterns and knowledge gains},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pegasos: A framework for the creation of direct mobile
coaching feedback systems. <em>JMUI</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12193-023-00411-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedback is essential for athletes in order to improve their sport performance. Feedback systems try to provide athletes and coaches not only with visualisations of acquired data, but moreover, with insights into—possibly—invisible aspects of their performance. With the widespread adoption of smartphones and the increase in their capabilities, their use as a device for applications of feedback systems is becoming increasingly popular. However, developing mobile feedback systems requires a high level of expertise from researchers and practitioners. The Direct Mobile Coaching model is a design-paradigm for mobile feedback systems. In order to reduce programming efforts, PEGASOS, a framework for creating feedback systems implementing the so-called Direct Mobile Coaching model, is introduced. The paper compares this framework with state-of-the-art research with regard to their ability of providing different variants feedback and offering multimodality to users.},
  archive      = {J_JMUI},
  author       = {Dobiasch, Martin and Oppl, Stefan and Stöckl, Michael and Baca, Arnold},
  doi          = {10.1007/s12193-023-00411-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Pegasos: A framework for the creation of direct mobile coaching feedback systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
