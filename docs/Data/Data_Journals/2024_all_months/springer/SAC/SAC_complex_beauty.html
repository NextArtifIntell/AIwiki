<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---210">SAC - 210</h2>
<ul>
<li><details>
<summary>
(2024). Hidden markov models for multivariate panel data.
<em>SAC</em>, <em>34</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10462-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While advances continue to be made in model-based clustering, challenges persist in modeling various data types such as panel data. Multivariate panel data present difficulties for clustering algorithms because they are often plagued by missing data and dropouts, presenting issues for estimation algorithms. This research presents a family of hidden Markov models that compensate for the issues that arise in panel data. A modified expectation–maximization algorithm capable of handling missing not at random data and dropout is presented and used to perform model estimation.},
  archive      = {J_SAC},
  author       = {Neal, Mackenzie R. and Sochaniwsky, Alexa A. and McNicholas, Paul D.},
  doi          = {10.1007/s11222-024-10462-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Hidden markov models for multivariate panel data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural gradient hybrid variational inference with
application to deep mixed models. <em>SAC</em>, <em>34</em>(6), 1–17.
(<a href="https://doi.org/10.1007/s11222-024-10488-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic models with global parameters and latent variables are common, and for which variational inference (VI) is popular. However, existing methods are often either slow or inaccurate in high dimensions. We suggest a fast and accurate VI method for this case that employs a well-defined natural gradient variational optimization that targets the joint posterior of the global parameters and latent variables. It is a hybrid method, where at each step the global parameters are updated using the natural gradient and the latent variables are generated from their conditional posterior. A fast to compute expression for the Tikhonov damped Fisher information matrix is used, along with the re-parameterization trick, to provide a stable natural gradient. We apply the approach to deep mixed models, which are an emerging class of Bayesian neural networks with random output layer coefficients to allow for heterogeneity. A range of simulations show that using the natural gradient is substantially more efficient than using the ordinary gradient, and that the approach is faster and more accurate than two cutting-edge natural gradient VI methods. In a financial application we show that accounting for industry level heterogeneity using the deep mixed model improves the accuracy of asset pricing models. MATLAB code to implement the method and replicate the results can be found at https://github.com/WeibenZhang07/NG-HVI},
  archive      = {J_SAC},
  author       = {Zhang, Weiben and Smith, Michael and Maneesoonthorn, Worapree and Loaiza-Maya, Rubén},
  doi          = {10.1007/s11222-024-10488-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Natural gradient hybrid variational inference with application to deep mixed models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed hypothesis testing for large dimensional
two-sample mean vectors. <em>SAC</em>, <em>34</em>(6), 1–31. (<a
href="https://doi.org/10.1007/s11222-024-10489-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the big data era has brought massive datasets to the forefront of academic and industrial discussions. Due to the high communication cost and long calculation time, traditional statistical methods may be difficult to process data centrally on a single server. A robust distributed system can effectively mitigate communication costs and enhance computational efficiency. However, the classical two-sample hypothesis testing problem in statistical analysis has not yet been fully developed within a distributed system framework. This paper explores the challenges of performing two-sample mean tests in a distributed framework, especially in the presence of unequal covariance matrices. By distributing samples across various nodes, we introduce two distributed test statistics: the blockwise linear two-sample test and the distributed two-sample test. Even though the sample size of each node is less than the dimension, the proposed test statistics maintain robust statistical properties. Both statistics are designed to enhance communication efficiency and reduce communication costs compared to the full-sample statistic. Simulation experiments and empirical analyses further confirm the favorable statistical properties of the proposed test statistics.},
  archive      = {J_SAC},
  author       = {Yan, Lu and Hu, Jiang and Wu, Lixiu},
  doi          = {10.1007/s11222-024-10489-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Distributed hypothesis testing for large dimensional two-sample mean vectors},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shrinkage for extreme partial least-squares. <em>SAC</em>,
<em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10490-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on dimension-reduction techniques for modelling conditional extreme values. Specifically, we investigate the idea that extreme values of a response variable can be explained by nonlinear functions derived from linear projections of an input random vector. In this context, the estimation of projection directions is examined, as approached by the extreme partial least squares (EPLS) method—an adaptation of the original partial least squares (PLS) method tailored to the extreme-value framework. Further, a novel interpretation of EPLS directions as maximum likelihood estimators is introduced, utilizing the von Mises–Fisher distribution applied to hyperballs. The dimension reduction process is enhanced through the Bayesian paradigm, enabling the incorporation of prior information into the projection direction estimation. The maximum a posteriori estimator is derived in two specific cases, elucidating it as a regularization or shrinkage of the EPLS estimator. We also establish its asymptotic behavior as the sample size approaches infinity. A simulation data study is conducted in order to assess the practical utility of our proposed method. This clearly demonstrates its effectiveness even in moderate data problems within high-dimensional settings. Furthermore, we provide an illustrative example of the method’s applicability using French farm income data, highlighting its efficacy in real-world scenarios.},
  archive      = {J_SAC},
  author       = {Arbel, Julyan and Girard, Stéphane and Lorenzo, Hadrien},
  doi          = {10.1007/s11222-024-10490-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Shrinkage for extreme partial least-squares},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerated failure time models with error-prone response
and nonlinear covariates. <em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10491-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a specific application of survival analysis, one of main interests in medical studies aims to analyze the patients’ survival time of a specific cancer. Typically, gene expressions are treated as covariates to characterize the survival time. In the framework of survival analysis, the accelerated failure time model in the parametric form is perhaps a common approach. However, gene expressions are possibly nonlinear and the survival time as well as censoring status are subject to measurement error. In this paper, we aim to tackle those complex features simultaneously. We first correct for measurement error in survival time and censoring status, and use them to develop a corrected Buckley–James estimator. After that, we use the boosting algorithm with the cubic spline estimation method to iteratively recover nonlinear relationship between covariates and survival time. Theoretically, we justify the validity of measurement error correction and estimation procedure. Numerical studies show that the proposed method improves the performance of estimation and is able to capture informative covariates. The methodology is primarily used to analyze the breast cancer data provided by the Netherlands Cancer Institute for research.},
  archive      = {J_SAC},
  author       = {Chen, Li-Pang},
  doi          = {10.1007/s11222-024-10491-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Accelerated failure time models with error-prone response and nonlinear covariates},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonconvex dantzig selector and its parallel computing
algorithm. <em>SAC</em>, <em>34</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10492-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dantzig selector is a popular $$\ell _1$$ -type variable selection method widely used across various research fields. However, $$\ell _1$$ -type methods may not perform well for variable selection without complex irrepresentable conditions. In this article, we introduce a nonconvex Dantzig selector for ultrahigh-dimensional linear models. We begin by demonstrating that the oracle estimator serves as a local optimum for the nonconvex Dantzig selector. In addition, we propose a one-step local linear approximation estimator, called the Dantzig-LLA estimator, for the nonconvex Dantzig selector, and establish its strong oracle property. The proposed regularization method avoids the restrictive conditions imposed by $$\ell _1$$ regularization methods to guarantee the model selection consistency. Furthermore, we propose an efficient and parallelizable computing algorithm based on feature-splitting to address the computational challenges associated with the nonconvex Dantzig selector in high-dimensional settings. A comprehensive numerical study is conducted to evaluate the performance of the nonconvex Dantzig selector and the computing efficiency of the feature-splitting algorithm. The results demonstrate that the Dantzig selector with nonconvex penalty outperforms the $$\ell _1$$ penalty-based selector, and the feature-splitting algorithm performs well in high-dimensional settings where linear programming solver may fail. Finally, we generalize the concept of nonconvex Dantzig selector to deal with more general loss functions.},
  archive      = {J_SAC},
  author       = {Wen, Jiawei and Yang, Songshan and Zhao, Delin},
  doi          = {10.1007/s11222-024-10492-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Nonconvex dantzig selector and its parallel computing algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tuning diagonal scale matrices for HMC. <em>SAC</em>,
<em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10494-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared. The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark. Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives. Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies. The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice.},
  archive      = {J_SAC},
  author       = {Tran, Jimmy Huy and Kleppe, Tore Selland},
  doi          = {10.1007/s11222-024-10494-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Tuning diagonal scale matrices for HMC},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: The COR criterion for optimal subset
selection in distributed estimation. <em>SAC</em>, <em>34</em>(6), 1.
(<a href="https://doi.org/10.1007/s11222-024-10496-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Guo, Guangbao and Song, Haoyue and Zhu, Lixing},
  doi          = {10.1007/s11222-024-10496-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: The COR criterion for optimal subset selection in distributed estimation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving tree probability estimation with stochastic
optimization and variance reduction. <em>SAC</em>, <em>34</em>(6), 1–18.
(<a href="https://doi.org/10.1007/s11222-024-10498-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability estimation of tree topologies is one of the fundamental tasks in phylogenetic inference. The recently proposed subsplit Bayesian networks (SBNs) provide a powerful probabilistic graphical model for tree topology probability estimation by properly leveraging the hierarchical structure of phylogenetic trees. However, the expectation maximization method currently used for learning SBN parameters does not scale up to large data sets. In this paper, we introduce several computationally efficient methods for training SBNs and show that variance reduction could be the key for better performance. Furthermore, we also introduce the variance reduction technique to improve the optimization of SBN parameters for variational Bayesian phylogenetic inference. Extensive synthetic and real data experiments demonstrate that our methods outperform previous baseline methods on the tasks of tree topology probability estimation as well as Bayesian phylogenetic inference using SBNs.},
  archive      = {J_SAC},
  author       = {Xie, Tianyu and Yuan, Musu and Deng, Minghua and Zhang, Cheng},
  doi          = {10.1007/s11222-024-10498-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Improving tree probability estimation with stochastic optimization and variance reduction},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential model identification with reversible jump
ensemble data assimilation method. <em>SAC</em>, <em>34</em>(6), 1–15.
(<a href="https://doi.org/10.1007/s11222-024-10499-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data assimilation (DA) schemes, the form representing the processes in the evolution models are pre-determined except some parameters to be estimated. In some applications, such as the contaminant solute transport model and the gas reservoir model, the modes in the equations within the evolution model cannot be predetermined from the outset and may change with the time. We propose a framework of sequential DA method named Reversible Jump Ensemble Filter (RJEnF) to identify the governing modes of the evolution model over time. The main idea is to introduce the Reversible Jump Markov Chain Monte Carlo (RJMCMC) method to the DA schemes to fit the situation where the modes of the evolution model are unknown and the dimension of the parameters is changing. Our framework allows us to identify the modes in the evolution model and their changes, as well as estimate the parameters and states of the dynamic system. Numerical experiments are conducted and the results show that our framework can effectively identify the underlying evolution models and increase the predictive accuracy of DA methods.},
  archive      = {J_SAC},
  author       = {Huan, Yue and Lin, Hai Xiang},
  doi          = {10.1007/s11222-024-10499-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Sequential model identification with reversible jump ensemble data assimilation method},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural networks for variable selection of higher-order
nonparametric spatial autoregressive model. <em>SAC</em>,
<em>34</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10500-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network technology has been receiving increasing attention and being applied in numerous fields due to its strong prediction performance and generalization ability. This paper examines the variable selection problem on the higher-order nonparametric spatial autoregressive model with nonparametric endogenous effects, using an efficient deep learning method. To achieve simultaneous parameter learning and variable selection, the deep learning method applies the concept of the Lasso penalty. In different data distribution scenarios, this method demonstrates strong competitiveness when compared to some variable selection methods. Simulation experiments and real data analysis reflect the effectiveness of this method.},
  archive      = {J_SAC},
  author       = {Li, Jie and Song, Yunquan and Jian, Ling},
  doi          = {10.1007/s11222-024-10500-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Deep neural networks for variable selection of higher-order nonparametric spatial autoregressive model},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Independence test via mutual information in the presence of
measurement errors. <em>SAC</em>, <em>34</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10502-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among existing methods for independence test, mutual information (MI) has great popularity as it is invariant to monotone transformations and enjoys higher power in detecting nonlinear associations. In this paper, we propose a novel MI-based independence test in the presence of measurement errors. The conditional density functions involved in MI are estimated using a novel deconvolution double kernel method. The convergence rates of these estimates are derived under the assumption that the measurement errors are either ordinary or super smooth. In addition, the asymptotic behaviors of the resultant estimate of MI are established under both the null and alternative hypotheses. Extensive simulation studies and an application to the low-resolution observations of source stars dataset confirm the superior numerical performances of the proposed methods.},
  archive      = {J_SAC},
  author       = {Fan, Guoliang and Zhang, Xilin and Zhu, Liping},
  doi          = {10.1007/s11222-024-10502-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Independence test via mutual information in the presence of measurement errors},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised learning via ensembles of diverse functional
representations: The functional voting classifier. <em>SAC</em>,
<em>34</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s11222-024-10503-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Classifier (FVC) is proposed to demonstrate how different functional representations leading to augmented diversity can increase predictive accuracy. Many real-world datasets from several domains are used to display that the FVC can significantly enhance performance compared to individual models. The framework presented provides a foundation for voting ensembles with functional data and can stimulate a highly encouraging line of research in the FDA context.},
  archive      = {J_SAC},
  author       = {Riccio, Donato and Maturo, Fabrizio and Romano, Elvira},
  doi          = {10.1007/s11222-024-10503-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Supervised learning via ensembles of diverse functional representations: The functional voting classifier},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDGM: An approach to estimate the graphical model of
interval-valued data. <em>SAC</em>, <em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10504-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models describe the conditional dependence structure among random variables via vertices and edges and have attracted increasing attention in recent years. However, when the variable is interval-valued instead of a scalar, it remains unclear how the graphical model can be estimated since interval-valued data impose additional complexity, including the lower bound should not be greater than the upper bound and each interval is itself a two-dimensional object. In this paper, we propose an algorithm, named the interval-valued data graphical model (IDGM), to realize such estimation, extending the graphical model concept to interval-valued data modeling. To address the complexity of interval-valued data, we apply the midpoints and log-ranges transformation to engage the center and range information of an interval. Then, we identify the network structure based on a variant $$ 2 \times 2 $$ block-wise sparsity graphical lasso that incorporates the penalty term of the precision matrix. The numerical simulations along with two real-world applications in the fields of macroeconomics and finance show the advantages of IDGM over the competing methods and demonstrate the effectiveness of IDGM in graphical model estimation for interval-valued data.},
  archive      = {J_SAC},
  author       = {Wu, Qiying and Wang, Huiwen and Lu, Shan},
  doi          = {10.1007/s11222-024-10504-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {IDGM: An approach to estimate the graphical model of interval-valued data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Core-elements for large-scale least squares estimation.
<em>SAC</em>, <em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10505-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coresets approach, also called subsampling or subset selection, aims to select a subsample as a surrogate for the observed sample and has found extensive application in large-scale data analysis. Existing coresets methods construct the subsample using a subset of rows from the predictor matrix. Such methods can be significantly inefficient when the predictor matrix is sparse or numerically sparse. To overcome this limitation, we develop a novel element-wise subset selection approach, called core-elements, for large-scale least squares estimation. We provide a deterministic algorithm to construct the core-elements estimator, only requiring an $$O(\textrm{nnz}(X)+rp^2)$$ computational cost, where X is an $$n\times p$$ predictor matrix, r is the number of elements selected from each column of X, and $$\textrm{nnz}(\cdot )$$ denotes the number of non-zero elements. Theoretically, we show that the proposed estimator is unbiased and approximately minimizes an upper bound of the estimation variance. We also provide an approximation guarantee by deriving a coresets-like finite sample bound for the proposed estimator. To handle potential outliers in the data, we further combine core-elements with the median-of-means procedure, resulting in an efficient and robust estimator with theoretical consistency guarantees. Numerical studies on various synthetic and real-world datasets demonstrate the proposed method’s superior performance compared to mainstream competitors.},
  archive      = {J_SAC},
  author       = {Li, Mengyu and Yu, Jun and Li, Tao and Meng, Cheng},
  doi          = {10.1007/s11222-024-10505-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Core-elements for large-scale least squares estimation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Support vector machine in big data: Smoothing strategy and
adaptive distributed inference. <em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10506-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is a powerful binary classification tool, but the growing size of modern data is bringing challenges to it. First, the non-smoothness of hinge loss poses difficulties in large-scale computation. Second, the existing large-scale distributed algorithms heavily rely on uniformity and randomness conditions, which are frequently violated in practice. To solve these issues, we first construct a convolution smoothing SVM, which enjoys a smooth and convex objective function. Then a distributed SVM is developed, in which the estimator can be calculated conveniently by minimizing a pilot sample-based distributed surrogate loss. In particular, it can be adaptive when the uniformity or randomness condition is violated. The established theoretical results and numerical experiments on both synthetic and real data all confirm the proposed methods.},
  archive      = {J_SAC},
  author       = {Wang, Kangning and Liu, Jin and Sun, Xiaofei},
  doi          = {10.1007/s11222-024-10506-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Support vector machine in big data: Smoothing strategy and adaptive distributed inference},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive sufficient sparse clustering by controlling false
discovery. <em>SAC</em>, <em>34</em>(6), 1–36. (<a
href="https://doi.org/10.1007/s11222-024-10507-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse clustering divides samples into distinct groups while simultaneously reducing dimensionality in ultra-high dimensional unsupervised learning. To achieve sparse clustering that can screening out noise variables, an adaptive sparse clustering framework based on sufficient variable screening, abbreviated as adaptive sufficient sparse clustering (ASSC), is developed by controlling false discovery. Without any specific model, ASSC employs a composite hypothesis testing procedure that leverages conditional marginal correlations of variables across distinct groups, aiming to pinpoint sufficient variables via sparse clustering and promoting the performance of sparse clustering in ultra-high dimensionality. To control false discoveries of the hypothesis testing procedure at a predetermined level, ASSC provides the adaptive threshold for identifying conditional marginal dependency, which ensures to accuracy of clustering with high probability. Under mild conditions, the sufficient screening properties and sufficient clustering properties of ASSC are established based on variable screening procedure by controlling false discovery. Numerical studies on synthetic data and real datasets corroborate the performance and flexibility of ASSC, and underscore its potent utility in unsupervised learning.},
  archive      = {J_SAC},
  author       = {Yuan, Zihao and Chen, Jiaqing and Qiu, Han and Wang, Houxiang and Huang, Yangxin},
  doi          = {10.1007/s11222-024-10507-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive sufficient sparse clustering by controlling false discovery},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering validation by distribution hypothesis learning.
<em>SAC</em>, <em>34</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10511-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new clustering validation technique named: “Hypothesis Learning”. We build our method on three concepts: (1) clustering cohesion, (2) clustering dispersion and, (3) hypothesis quality. The first two notions focus on individual cluster quality. We measure them using a classifier estimating the tightness and separation as a likelihood. The third notion evaluates the complexity of learning the clustering partition. Similar to cohesion and dispersion, we get a likelihood value. Next, we aggregate these three measures to find a single index reporting clustering quality. Previous methods from the literature have already used supervised and unsupervised algorithms and stability concepts to validate clustering solutions. Our motivation is not only to improve these methods but to use learning algorithms in a novel manner to learn key clustering concepts such as cohesion and dispersion. Furthermore, we include a technical discussion on how to regularize a classifier to handle overfit, thus explaining the symbiosis between supervised and unsupervised algorithms. In our experimental setup, we tested “Hypothesis Learning” with a fast classifier, K Nearest Neighbour (KNN). However, in the discussion of the method, we explore other classifiers like CART and Random Forest. The experimental results compare our approach with a similar method and many other well-known clustering indexes.},
  archive      = {J_SAC},
  author       = {Bayá, Ariel E. and Larese, Mónica G.},
  doi          = {10.1007/s11222-024-10511-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Clustering validation by distribution hypothesis learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double-loop importance sampling for McKean–vlasov stochastic
differential equation. <em>SAC</em>, <em>34</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s11222-024-10497-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates Monte Carlo (MC) methods to estimate probabilities of rare events associated with the solution to the d-dimensional McKean–Vlasov stochastic differential equation (MV-SDE). MV-SDEs are usually approximated using a stochastic interacting P-particle system, which is a set of P coupled d-dimensional stochastic differential equations (SDEs). Importance sampling (IS) is a common technique for reducing high relative variance of MC estimators of rare-event probabilities. We first derive a zero-variance IS change of measure for the quantity of interest by using stochastic optimal control theory. However, when this change of measure is applied to stochastic particle systems, it yields a $$P \times d$$ -dimensional partial differential control equation (PDE), which is computationally expensive to solve. To address this issue, we use the decoupling approach introduced in (dos Reis et al. 2023), generating a d-dimensional control PDE for a zero-variance estimator of the decoupled SDE. Based on this approach, we develop a computationally efficient double loop MC (DLMC) estimator. We conduct a comprehensive numerical error and work analysis of the DLMC estimator. As a result, we show optimal complexity of $$\mathcal {O}\left( \textrm{TOL}_{\textrm{r}}^{-4}\right) $$ with a significantly reduced constant to achieve a prescribed relative error tolerance $$\textrm{TOL}_{\textrm{r}}$$ . Subsequently, we propose an adaptive DLMC method combined with IS to numerically estimate rare-event probabilities, substantially reducing relative variance and computational runtimes required to achieve a given $$\textrm{TOL}_{\textrm{r}}$$ compared with standard MC estimators in the absence of IS. Numerical experiments are performed on the Kuramoto model from statistical physics.},
  archive      = {J_SAC},
  author       = {Ben Rached, Nadhir and Haji-Ali, Abdul-Lateef and Subbiah Pillai, Shyam Mohan and Tempone, Raúl},
  doi          = {10.1007/s11222-024-10497-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Double-loop importance sampling for McKean–Vlasov stochastic differential equation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal orthogonal designs for experiments with four-level
and two-level factors. <em>SAC</em>, <em>34</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10517-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiments with both four-level and two-level factors have been conducted in many different domains, ranging from engineering to ergonomics. In such experiments, the four-level factors are used in three different ways: as blocking factors, as qualitative factors, or as quantitative factors. However, it was generally not easy for the authors to find an appropriate design for their experiment. A solution to this problem is to use a catalog of designs with a wide range of options. To efficiently use such a catalog, one needs criteria to identify the best designs. We present good criteria for the three different usages of four-level factors, and we apply these criteria to the catalog of Bohyn et al. (J R Stat Soc Ser C: Appl Stat 72:750–769, 2023). Finally, we present tables of four-and-two-level designs that are optimal with respect to the different criteria, for run sizes 16, 32, 64 and 128, with 1, 2, or 3 four-level factors, and up to 20 two-level factors.},
  archive      = {J_SAC},
  author       = {Bohyn, Alexandre and Schoen, Eric D. and Goos, Peter},
  doi          = {10.1007/s11222-024-10517-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Optimal orthogonal designs for experiments with four-level and two-level factors},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Unbiased and multilevel methods for a class of diffusions
partially observed via marked point processes. <em>SAC</em>,
<em>34</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10518-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider the filtering problem associated to partially observed diffusions, with observations following a marked point process. In the model, the data form a point process with observation times that have its intensity driven by a diffusion, with the associated marks also depending upon the diffusion process. We assume that one must resort to time-discretizing the diffusion process and develop particle and multilevel particle filters to recursively approximate the filter. In particular, we prove that our multilevel particle filter can achieve a mean square error (MSE) of $$\mathcal {O}(\epsilon ^2)$$ ( $$\epsilon &gt;0$$ and arbitrary) with a cost of $$\mathcal {O}(\epsilon ^{-2.5})$$ versus using a particle filter which has a cost of $$\mathcal {O}(\epsilon ^{-3})$$ to achieve the same MSE. We then show how this methodology can be extended to give unbiased (that is with no time-discretization error) estimators of the filter, which are proved to have finite variance and with high-probability have finite cost. Finally, we extend our methodology to the problem of online static-parameter estimation.},
  archive      = {J_SAC},
  author       = {Alvarez, Miguel and Jasra, Ajay and Ruzayqat, Hamza},
  doi          = {10.1007/s11222-024-10518-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Unbiased and multilevel methods for a class of diffusions partially observed via marked point processes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust variable selection for partially linear additive
models. <em>SAC</em>, <em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10520-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among semiparametric regression models, partially linear additive models provide a useful tool to include additive nonparametric components as well as a parametric component, when explaining the relationship between the response and a set of explanatory variables. This paper concerns such models under sparsity assumptions for the covariates included in the linear component. Sparse statistical models are easier to interpret than dense ones, since only a small number of the parameters are non–zero. This scenario is common in regression problems, making variable selection an important task. As in other settings, outliers either in the residuals or in the covariates involved in the linear component have a harmful effect. To simultaneously achieve model selection for the parametric component of the model and resistance to outliers, we combine preliminary robust estimators of the additive component, robust linear MM-regression estimators with a penalty such as SCAD on the coefficients in the parametric part. Under mild assumptions, consistency results and rates of convergence for the proposed estimators are derived. A Monte Carlo study is carried out to compare, under different models and contamination schemes, the performance of the robust proposal with its classical counterpart. The numerical results show the advantage of using the robust approach. Through the analysis of a real data set, we also illustrate the benefits of the proposed procedure.},
  archive      = {J_SAC},
  author       = {Boente, Graciela and Martínez, Alejandra Mercedes},
  doi          = {10.1007/s11222-024-10520-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Robust variable selection for partially linear additive models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient estimation of expected information gain in
bayesian experimental design with multi-index monte carlo. <em>SAC</em>,
<em>34</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10522-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expected information gain (EIG) is an important criterion in Ba yesian optimal experimental design. Nested Monte Carlo and M ulti-level Monte Carlo (MLMC) methods have been used to compute EIG. However, in cases where the forward output function is not analytically tractable, even MLMC can not achieve its best rate. In this paper, we use Multi-index Monte Carlo to compute the EIG, which can give $$ O(\varepsilon ^{-2}) $$ computation work. Both theoretical analysis and numerical results are presented.},
  archive      = {J_SAC},
  author       = {Du, Xinting and Wang, Hejin},
  doi          = {10.1007/s11222-024-10522-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Efficient estimation of expected information gain in bayesian experimental design with multi-index monte carlo},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional autoencoder for smoothing and representation
learning. <em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10501-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. Existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. In this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. We design the encoder to employ a projection layer computing the inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermined basis functions. The developed architecture can accommodate both regularly and irregularly spaced data. Our experiments demonstrate that the proposed method outperforms functional principal component analysis in terms of prediction and classification, and maintains superior smoothing ability and better computational efficiency in comparison to the conventional autoencoders under both linear and nonlinear settings.},
  archive      = {J_SAC},
  author       = {Wu, Sidi and Beaulac, Cédric and Cao, Jiguo},
  doi          = {10.1007/s11222-024-10501-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Functional autoencoder for smoothing and representation learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast and accurate numerical method for the left tail of
sums of independent random variables. <em>SAC</em>, <em>34</em>(6),
1–19. (<a href="https://doi.org/10.1007/s11222-024-10514-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a flexible, deterministic numerical method for computing left-tail rare events of sums of non-negative, independent random variables. The method is based on iterative numerical integration of linear convolutions by means of Newtons–Cotes rules. The periodicity properties of convoluted densities combined with the Trapezoidal rule are exploited to produce a robust and efficient method, and the method is flexible in the sense that it can be applied to all kinds of non-negative continuous RVs. We present an error analysis and study the benefits of utilizing Newton–Cotes rules versus the fast Fourier transform (FFT) for numerical integration, showing that although there can be efficiency benefits to using FFT, Newton–Cotes rules tend to preserve the relative error better, and indeed do so at an acceptable computational cost. Numerical studies on problems with both known and unknown rare-event probabilities showcase the method’s performance and support our theoretical findings.},
  archive      = {J_SAC},
  author       = {Ben Rached, Nadhir and Hoel, Håkon and Meo, Johannes Vincent},
  doi          = {10.1007/s11222-024-10514-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A fast and accurate numerical method for the left tail of sums of independent random variables},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online prediction of extreme conditional quantiles via
b-spline interpolation. <em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10515-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme quantiles are critical for understanding the behavior of data in the tail region of a distribution. It is challenging to estimate extreme quantiles, particularly when dealing with limited data in the tail. In such cases, extreme value theory offers a solution by approximating the tail distribution using the Generalized Pareto Distribution (GPD). This allows for the extrapolation beyond the range of observed data, making it a valuable tool for various applications. However, when it comes to conditional cases, where estimation relies on covariates, existing methods may require computationally expensive GPD fitting for different observations. This computational burden becomes even more problematic as the volume of observations increases, sometimes approaching infinity. To address this issue, we propose an interpolation-based algorithm named EMI. EMI facilitates the online prediction of extreme conditional quantiles with finite offline observations. Combining quantile regression and GPD-based extrapolation, EMI is formulated as a bilevel programming problem that is efficiently solvable using classic optimization methods. Once estimates for offline observations are obtained, EMI employs B-spline interpolation for covariate-dependent variables, enabling estimation for online observations with finite GPD fitting. Simulations and real data analysis demonstrate the effectiveness of EMI across various scenarios.},
  archive      = {J_SAC},
  author       = {Li, Zhengpin and Wang, Jian and Hou, Yanxi},
  doi          = {10.1007/s11222-024-10515-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Online prediction of extreme conditional quantiles via B-spline interpolation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stab-GKnock: Controlled variable selection for partially
linear models using generalized knockoffs. <em>SAC</em>, <em>34</em>(6),
1–25. (<a href="https://doi.org/10.1007/s11222-024-10516-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed fixed-X knockoff is a powerful variable selection procedure that controls the false discovery rate (FDR) in any finite-sample setting, yet its theoretical insights are difficult to show beyond Gaussian linear models. In this paper, we make the first attempt to extend the fixed-X knockoff to partially linear models by using generalized knockoff features, and propose a new stability generalized knockoff (Stab-GKnock) procedure by incorporating selection probability as feature importance score. We provide FDR control and power guarantee under some regularity conditions. In addition, we propose a two-stage method under high dimensionality by introducing a new joint feature screening procedure, with guaranteed sure screening property. Extensive simulation studies are conducted to evaluate the finite-sample performance of the proposed method. A real data example is also provided for illustration.},
  archive      = {J_SAC},
  author       = {Su, Han and Sun, Qingyang and Yi, Mengxi and Li, Gaorong and Yuan, Panxu},
  doi          = {10.1007/s11222-024-10516-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Stab-GKnock: Controlled variable selection for partially linear models using generalized knockoffs},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible bayesian modeling for longitudinal binary and
ordinal responses. <em>SAC</em>, <em>34</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10525-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies with binary or ordinal responses are widely encountered in various disciplines, where the primary focus is on the temporal evolution of the probability of each response category. Traditional approaches build from the generalized mixed effects modeling framework. Even amplified with nonparametric priors placed on the fixed or random effects, such models are restrictive due to the implied assumptions on the marginal expectation and covariance structure of the responses. We tackle the problem from a functional data analysis perspective, treating the observations for each subject as realizations from subject-specific stochastic processes at the measured times. We develop the methodology focusing initially on binary responses, for which we assume the stochastic processes have Binomial marginal distributions. Leveraging the logits representation, we model the discrete space processes through continuous space processes. We utilize a hierarchical framework to model the mean and covariance kernel of the continuous space processes nonparametrically and simultaneously through a Gaussian process prior and an Inverse-Wishart process prior, respectively. The prior structure results in flexible inference for the evolution and correlation of binary responses, while allowing for borrowing of strength across all subjects. The modeling approach can be naturally extended to ordinal responses. Here, the continuation-ratio logits factorization of the multinomial distribution is key for efficient modeling and inference, including a practical way of dealing with unbalanced longitudinal data. The methodology is illustrated with synthetic data examples and an analysis of college students’ mental health status data.},
  archive      = {J_SAC},
  author       = {Kang, Jizhou and Kottas, Athanasios},
  doi          = {10.1007/s11222-024-10525-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Flexible bayesian modeling for longitudinal binary and ordinal responses},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementation and analysis of GPU algorithms for vecchia
approximation. <em>SAC</em>, <em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10510-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Processes have become an indispensable part of the spatial statistician’s toolbox but are unsuitable for analyzing large datasets because of the significant time and memory needed to fit the associated model exactly. Vecchia Approximation is widely used to reduce the computational complexity and can be calculated with embarrassingly parallel algorithms. While multi-core software has been developed for Vecchia Approximation, software designed to run on graphics processing units (GPUs) is lacking, despite the tremendous success GPUs have had in statistics and machine learning. We compare three different ways to implement Vecchia Approximation on a GPU: two of which are similar to methods used for other Gaussian Process approximations and one that is new. Our new method exploits the properties of Vecchia Approximation to nearly eliminate thread synchronization and reduce memory access times. We show that our new method outperforms the other two and then compare it to existing multi-core and GPU-accelerated software by fitting Gaussian Process models on various datasets, including a large spatial-temporal dataset of $$n&gt;10^6$$ points collected from an Earth-observing satellite. Our method works on larger datasets and provides higher predictive accuracy than existing GPU methods, and it runs up to 20 times faster than a single-core CPU implementation of Vecchia Approximation.},
  archive      = {J_SAC},
  author       = {James, Zachary and Guinness, Joseph},
  doi          = {10.1007/s11222-024-10510-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Implementation and analysis of GPU algorithms for vecchia approximation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the application of gaussian graphical models to paired
data problems. <em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10513-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are nowadays commonly applied to the comparison of groups sharing the same variables, by jointly learning their independence structures. We consider the case where there are exactly two dependent groups and the association structure is represented by a family of coloured Gaussian graphical models suited to deal with paired data problems. To learn the two dependent graphs, together with their across-graph association structure, we implement a fused graphical lasso penalty. We carry out a comprehensive analysis of this approach, with special attention to the role played by some relevant submodel classes. In this way, we provide a broad set of tools for the application of Gaussian graphical models to paired data problems. These include results useful for the specification of penalty values in order to obtain a path of lasso solutions and an ADMM algorithm that solves the fused graphical lasso optimization problem. Finally, we carry out a simulation study to compare our method with the traditional graphical lasso, and present an application of our method to cancer genomics where it is of interest to compare cancer cells with a control sample from histologically normal tissues adjacent to the tumor. All the methods described in this article are implemented in the R package pdglasso available at https://github.com/savranciati/pdglasso .},
  archive      = {J_SAC},
  author       = {Ranciati, Saverio and Roverato, Alberto},
  doi          = {10.1007/s11222-024-10513-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {On the application of gaussian graphical models to paired data problems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed additive hazards regression analysis of
multi-site current status data without using individual-level data.
<em>SAC</em>, <em>34</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10523-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-site studies, sharing individual-level information across multiple data-contributing sites usually poses a significant risk to data security. Thus, due to privacy constraints, analytical tools without using individual-level data have drawn considerable attention to researchers in recent years. In this work, we consider regression analysis of current status data arising from multi-site cross-sectional studies and develop two distributed estimation methods tailored to the unstratified and stratified additive hazards models, respectively. In particular, instead of utilizing the individual-level data, the proposed methods only require transferring the summary statistics from each site to the analysis center, which achieves the aim of privacy protection. We establish the asymptotic properties of the proposed estimators, including the consistency and asymptotic normality. Specifically, the distributed estimators derived are shown to be asymptotically equivalent to those based on the pooled individual-level data. Simulation studies and an application to a multi-site gonorrhea infection data set demonstrate the proposed methods’ satisfactory performance and practical utility.},
  archive      = {J_SAC},
  author       = {Huang, Peiyao and Li, Shuwei and Song, Xinyuan},
  doi          = {10.1007/s11222-024-10523-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributed additive hazards regression analysis of multi-site current status data without using individual-level data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scale mixtures of multivariate centered skew-normal
distributions. <em>SAC</em>, <em>34</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s11222-024-10512-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most popular approach for modeling multivariate data in many research areas is based on the multivariate normal distribution. However, this approach may not be suitable when dealing with data presenting skewness and/or heavy tail distribution. Thus, the scale mixtures of multivariate skew-normal distributions become an alternative distribution to deal with this scenario. In this context, we introduce the scale mixtures of multivariate centered skew-normal (MCSN) distributions to circumvent some inferential and interpretation problems. Some related properties of this family and Bayesian inference are presented. A Monte Carlo simulation study is carried out to evaluate the parameter recovery. The methodology is illustrated by analyzing a real data set where the new distributions outperform the MCSN distribution.},
  archive      = {J_SAC},
  author       = {de Freitas, João Victor B. and Bondon, Pascal and Azevedo, Caio L. N. and Reisen, Valdério A. and Nobre, Juvêncio S.},
  doi          = {10.1007/s11222-024-10512-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Scale mixtures of multivariate centered skew-normal distributions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The stochastic proximal distance algorithm. <em>SAC</em>,
<em>34</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s11222-024-10524-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic versions of proximal methods have gained much attention in statistics and machine learning. These algorithms tend to admit simple, scalable forms, and enjoy numerical stability via implicit updates. In this work, we propose and analyze a stochastic version of the recently proposed proximal distance algorithm, a class of iterative optimization methods that recover a desired constrained estimation problem as a penalty parameter $$\rho \rightarrow \infty $$ . By uncovering connections to related stochastic proximal methods and interpreting the penalty parameter as the learning rate, we justify heuristics used in practical manifestations of the proximal distance method, establishing their convergence guarantees for the first time. Moreover, we extend recent theoretical devices to establish finite error bounds and a complete characterization of convergence rates regimes. We validate our analysis via a thorough empirical study, also showing that unsurprisingly, the proposed method outpaces batch versions on popular learning tasks.},
  archive      = {J_SAC},
  author       = {Jiang, Haoyu and Xu, Jason},
  doi          = {10.1007/s11222-024-10524-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {The stochastic proximal distance algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The polytope of optimal approximate designs: Extending the
selection of informative experiments. <em>SAC</em>, <em>34</em>(6),
1–18. (<a href="https://doi.org/10.1007/s11222-024-10527-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the problem of constructing an experimental design, optimal for estimating parameters of a given statistical model with respect to a chosen criterion. To address this problem, the literature usually provides a single solution. Often, however, there exists a rich set of optimal designs, and the knowledge of this set can lead to substantially greater freedom to select an appropriate experiment. In this paper, we demonstrate that the set of all optimal approximate designs generally corresponds to a polytope. Particularly important elements of the polytope are its vertices, which we call vertex optimal designs. We prove that the vertex optimal designs possess unique properties, such as small supports, and outline strategies for how they can facilitate the construction of suitable experiments. Moreover, we show that for a variety of situations it is possible to construct the vertex optimal designs with the assistance of a computer, by employing error-free rational-arithmetic calculations. In such cases the vertex optimal designs are exact, often closely related to known combinatorial designs. Using this approach, we were able to determine the polytope of optimal designs for some of the most common multifactor regression models, thereby extending the choice of informative experiments for a large variety of applications.},
  archive      = {J_SAC},
  author       = {Harman, Radoslav and Filová, Lenka and Rosa, Samuel},
  doi          = {10.1007/s11222-024-10527-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {The polytope of optimal approximate designs: Extending the selection of informative experiments},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuity approximation in hybrid bayesian networks
structure learning. <em>SAC</em>, <em>34</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s11222-024-10531-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks have been used to represent the joint distribution of multiple random variables in a flexible yet interpretable manner. One major challenge in learning the structure of a Bayesian network is how to model networks that include a mixture of continuous and discrete random variables, known as hybrid Bayesian networks. This paper overviews the literature on approaches to handle hybrid Bayesian networks. Typically, one of two approaches is taken: either the data are considered to have a joint distribution, designed for a mixture of discrete and continuous variables, or continuous random variables are discretized, resulting in discrete Bayesian networks. This paper proposes a strategy to model all random variables as Gaussian, referred to as Run it As Gaussian (RAG). We demonstrate that RAG results in more reliable estimates of graph structures theoretically and by simulation studies than other strategies. Both strategies are also implemented on a childhood obesity data set. The two different strategies give rise to significant differences in the optimal graph structures, with the results of the simulation study suggesting that our approach is more reliable.},
  archive      = {J_SAC},
  author       = {Zhu, Wanchuang and Nguyen, Ngoc Lan Chi},
  doi          = {10.1007/s11222-024-10531-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Continuity approximation in hybrid bayesian networks structure learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy clustering with barber modularity regularization.
<em>SAC</em>, <em>34</em>(6), 1–34. (<a
href="https://doi.org/10.1007/s11222-024-10495-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new algorithm for the joint clustering of two sets of statistical units $$\mathcal {N}$$ and $$\mathcal {M}$$ which are also equipped with an adjacency structure which is represented by a bipartite network. Our model is based on the fuzzy Partition Around Medoids, and it combines it with techniques for community detection in bipartite complex networks based on Barber modularity maximization. The goal is to produce a partition of $$\mathcal {N}\cup \mathcal {M}$$ into clusters, each of which is also identified by two medoids, one in $$\mathcal {N}$$ and one in $$\mathcal {M}$$ , which represent the typical units in the cluster for each set. Such clusters are optimized so that units in the same cluster both have similar values on their attributes and are likely to be adjacent. We test the algorithm on both simulated and real data, to show how it is able to capture a wide range of different interactions between the distribution of the attributes and the network structure.},
  archive      = {J_SAC},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s11222-024-10495-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-34},
  shortjournal = {Stat. Comput.},
  title        = {Fuzzy clustering with barber modularity regularization},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shrinkage priors via random imaginary data. <em>SAC</em>,
<em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10509-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we focus on the Bayesian variable selection problem in high–dimensional linear regression models. The use of shrinkage priors, when the number of available observations n is less than the number of explanatory variables p, is a well–established technique, which shares great theoretical and empirical properties. The power–expected–posterior (PEP) prior methodology, uses random imaginary (training) data in order to create (usually) objective, fully automatic and compatible priors, with an appealing interpretation. One of the contributions of this work is showing that many common used shrinkage priors can be written as PEP priors, with the power parameters playing the role of the global and local shrinkage parameters. These shrinkage priors differ depending on the choice of hyperprior they use on these parameters. Using data augmentation ideas, we augment the imaginary design matrix, under the PEP prior methodology, in order to create a new class of (independent) shrinkage priors, based on random imaginary data, with appropriately chosen hyperpriors for the power parameters, by borrowing ideas from other scale–mixture shrinkage priors. One of the main advantages of the resulting prior is that its tails are changed depending on the degree of multicollinearity issues we face. In addition, it has an appealing interpretation based on imaginary data coming from a prior predictive distribution and also provides an effective way to establish compatibility of priors among competing models. We check the theoretical properties of our proposed method and explore its behaviour via a simulation study and a real data example.},
  archive      = {J_SAC},
  author       = {Tzoumerkas, G. and Fouskakis, D.},
  doi          = {10.1007/s11222-024-10509-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Shrinkage priors via random imaginary data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast adaptive fourier integration for spectral densities of
gaussian processes. <em>SAC</em>, <em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10519-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from any continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.},
  archive      = {J_SAC},
  author       = {Beckman, Paul G. and Geoga, Christopher J.},
  doi          = {10.1007/s11222-024-10519-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Fast adaptive fourier integration for spectral densities of gaussian processes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric approach for clustering functional
trajectories over time. <em>SAC</em>, <em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10521-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional concurrent, or varying-coefficient, regression models are commonly used in biomedical and clinical settings to investigate how the relation between an outcome and observed covariate varies as a function of another covariate. In this work, we propose a Bayesian nonparametric approach to investigate how clusters of these functional relations evolve over time. Our model clusters individual functional trajectories within and across time periods while flexibly accommodating the evolution of the partitions across time periods with covariates. Motivated by mobile health data collected in a novel, smartphone-based smoking cessation intervention study, we demonstrate how our proposed method can simultaneously cluster functional trajectories, accommodate temporal dependence, and provide insights into the transitions between functional clusters over time.},
  archive      = {J_SAC},
  author       = {Liang, Mingrui and Koslovsky, Matthew D. and Hébert, Emily T. and Kendzor, Darla E. and Vannucci, Marina},
  doi          = {10.1007/s11222-024-10521-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian nonparametric approach for clustering functional trajectories over time},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: Unbiased and multilevel methods for a class of
diffusions partially observed via marked point processes. <em>SAC</em>,
<em>34</em>(6), 1. (<a
href="https://doi.org/10.1007/s11222-024-10530-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Alvarez, Miguel and Jasra, Ajay and Ruzayqat, Hamza},
  doi          = {10.1007/s11222-024-10530-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction: Unbiased and multilevel methods for a class of diffusions partially observed via marked point processes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On periodic <span
class="math display">log <em>G</em><em>A</em><em>R</em><em>C</em><em>H</em></span>
model with empirical application. <em>SAC</em>, <em>34</em>(6), 1–24.
(<a href="https://doi.org/10.1007/s11222-024-10532-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new class of volatility models known as Periodic log-Generalized Autoregressive Conditional Heteroscedastic (P-logGARCH) models, which incorporate periodic variations in the parameters. These parameter variations are particularly relevant when incorporating seasonality into economic decision-making theory. The P-logGARCH formulation demonstrates several desirable properties, including unconstrained positivity of parameters and effective management of extreme values, along with a volatility trend. Specifically, in this paper we investigate the probabilistic structure of this model and establish necessary and sufficient conditions for the existence of stationary solutions in a periodic sense. Additionally, we examine the strong consistency and asymptotic normality of the Generalized Quasi-Maximum Likelihood Estimator (GQMLE) under mild assumptions. To assess the performance of our model, we conduct a Monte Carlo study to examine the finite-sample properties of the GQMLE. Finally, we present empirical evidence by applying the P-log GARCH model to analyze the exchange rates of the Algerian Dinar against the U.S. dollar and the Euro, thereby demonstrating its practical utility.},
  archive      = {J_SAC},
  author       = {Bibi, Abdelouahab and Hamdi, Fayçal},
  doi          = {10.1007/s11222-024-10532-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {On periodic $$\log GARCH$$ model with empirical application},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kullback–leibler divergence based multidimensional robust
universal hypothesis testing. <em>SAC</em>, <em>34</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s11222-024-10533-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ball-type robust universal hypothesis testing (UHT), the null hypothesis is a set of probability distributions constrained by a ball of radius $$r &gt; 0$$ denoted $$B(\mathcal {P}_0,r)$$ based on the cumulative density function of the nominal distribution $$\mathcal {P}_0$$ . A major limitation is that this method is originally designed only for one-dimensional distributions. To overcome this limitation, this paper proposes a new method to deal with multidimensional samples. For this purpose, first of all, new bounds are defined in the multidimensional domain. Later, a new mathematical programming model based on the transformed region of $$B(\mathcal {P}_0,r)$$ , namely empirical multidimensional robust UHT problem based on Kullback–Leibler divergence is proposed for ball-type robust UHT. The power of the new testing method combined with different types of bounds was then demonstrated by a computational study. This method fills the research gap by enabling ball-type robust UHT for multidimensional samples and is flexible in that it can be used with different type of bounds.},
  archive      = {J_SAC},
  author       = {Bahçeci, Ufuk},
  doi          = {10.1007/s11222-024-10533-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Kullback–Leibler divergence based multidimensional robust universal hypothesis testing},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach for parameter estimation of mixture of two
weibull distributions in failure data modeling. <em>SAC</em>,
<em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10534-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture of two 2-parameter Weibull distributions (MixW), as a specialized variant of the mixture of Weibull distributions, serves as an ideal model for heterogeneous data sets within the realms of reliability studies and survival analysis. A principal challenge in dealing with MixW lies in the estimation of parameters. Inspired by the exemplary efficacy of the Quasi-Monte Carlo method in quantile estimation, this paper introduces an innovative approach, which employs the Harrell-Davis and three Sfakianakis and Verginis quantile estimators to enhance the representativeness of the sample, thereby improving the accuracy of parameter estimation. Given the difficulty in deriving analytical expressions for the parameters of MixW and their propensity for convergence to local maxima, this paper adopts the sequential number-theoretic (SNTO) algorithm for the numerical resolution of parameter estimation. The initial optimization region for SNTO is determined via the graphical method of the Weibull probability plot. Simulation studies have demonstrated that our proposed method significantly enhances estimation precision and reduces dependence on the “quality” of the sample. Furthermore, this methodology has been applied to two real data sets that demonstrate the effectiveness of our proposed approach.},
  archive      = {J_SAC},
  author       = {Yan, Tianyu and Fang, Kai-Tai and Yin, Hong},
  doi          = {10.1007/s11222-024-10534-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A novel approach for parameter estimation of mixture of two weibull distributions in failure data modeling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systemic infinitesimal over-dispersion on graphical dynamic
models. <em>SAC</em>, <em>34</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10443-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic models for collections of interacting populations have crucial roles in many scientific fields such as epidemiology, ecology, performance engineering, and queueing theory, to name a few. However, the standard approach to extending an ordinary differential equation model to a Markov chain does not have sufficient flexibility in the mean-variance relationship to match data. To handle that, we develop new approaches using Dirichlet noise to construct collections of independent or dependent noise processes. This permits the modeling of high-frequency variation in transition rates both within and between the populations under study. Our theory is developed in a general framework of time-inhomogeneous Markov processes equipped with a general graphical structure. We demonstrate our approach on a widely analyzed measles dataset, adding Dirichlet noise to a classical Susceptible–Exposed–Infected–Recovered model. Our methodology shows improved statistical fit measured by log-likelihood and provides new insights into the dynamics of this biological system.},
  archive      = {J_SAC},
  author       = {Ning, Ning and Ionides, Edward},
  doi          = {10.1007/s11222-024-10443-3},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Systemic infinitesimal over-dispersion on graphical dynamic models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Analysis of stochastic gradient descent in
continuous time. <em>SAC</em>, <em>34</em>(5), 1–2. (<a
href="https://doi.org/10.1007/s11222-024-10450-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction regarding [Latz 2021, Stat. Comput. 31, 39].},
  archive      = {J_SAC},
  author       = {Latz, Jonas},
  doi          = {10.1007/s11222-024-10450-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-2},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: Analysis of stochastic gradient descent in continuous time},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On weak convergence of quantile-based empirical likelihood
process for ROC curves. <em>SAC</em>, <em>34</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10457-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The empirical likelihood (EL) method possesses desirable qualities such as automatically determining confidence regions and circumventing the need for variance estimation. As an extension, a quantile-based EL (QEL) method is considered, which results in a simpler form. In this paper, we explore the framework of the QEL method. Firstly, we explore the weak convergence of the −2 log empirical likelihood ratio for ROC curves. We also introduce a novel statistic for testing the entire ROC curve and the equality of two distributions. To validate our approach, we conduct simulation studies and analyze real data from hepatitis C patients, comparing our method with existing ones.},
  archive      = {J_SAC},
  author       = {Jiang, Hu and Yiming, Liu and Wang, Zhou},
  doi          = {10.1007/s11222-024-10457-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {On weak convergence of quantile-based empirical likelihood process for ROC curves},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient shapley performance attribution for least-squares
regression. <em>SAC</em>, <em>34</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10459-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the performance of a least-squares regression model, as judged by out-of-sample $$R^2$$ . Shapley values give a fair attribution of the performance of a model to its input features, taking into account interdependencies between features. Evaluating the Shapley values exactly requires solving a number of regression problems that is exponential in the number of features, so a Monte Carlo-type approximation is typically used. We focus on the special case of least-squares regression models, where several tricks can be used to compute and evaluate regression models efficiently. These tricks give a substantial speed up, allowing many more Monte Carlo samples to be evaluated, achieving better accuracy. We refer to our method as least-squares Shapley performance attribution (LS-SPA), and describe our open-source implementation.},
  archive      = {J_SAC},
  author       = {Bell, Logan and Devanathan, Nikhil and Boyd, Stephen},
  doi          = {10.1007/s11222-024-10459-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Efficient shapley performance attribution for least-squares regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classifier-dependent feature selection via greedy methods.
<em>SAC</em>, <em>34</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10460-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to introduce a new approach to feature ranking for classification tasks, called in what follows greedy feature selection. In statistical learning, feature selection is usually realized by means of methods that are independent of the classifier applied to perform the prediction using that reduced number of features. Instead, the greedy feature selection identifies the most important feature at each step and according to the selected classifier. The benefits of such scheme are investigated in terms of model capacity indicators, such as the Vapnik-Chervonenkis dimension or the kernel alignment. This theoretical study proves that the iterative greedy algorithm is able to construct classifiers whose complexity capacity grows at each step. The proposed method is then tested numerically on various datasets and compared to the state-of-the-art techniques. The results show that our iterative scheme is able to truly capture only a few relevant features, and may improve, especially for real and noisy data, the accuracy scores of other techniques. The greedy scheme is also applied to the challenging application of predicting geo-effective manifestations of the active Sun.},
  archive      = {J_SAC},
  author       = {Camattari, Fabiana and Guastavino, Sabrina and Marchetti, Francesco and Piana, Michele and Perracchione, Emma},
  doi          = {10.1007/s11222-024-10460-2},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Classifier-dependent feature selection via greedy methods},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locally sparse and robust partial least squares in
scalar-on-function regression. <em>SAC</em>, <em>34</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10464-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for estimating a scalar-on-function regression model, leveraging a functional partial least squares methodology. Our proposed method involves computing the functional partial least squares components through sparse partial robust M regression, facilitating robust and locally sparse estimations of the regression coefficient function. This strategy delivers a robust decomposition for the functional predictor and regression coefficient functions. After the decomposition, model parameters are estimated using a weighted loss function, incorporating robustness through iterative reweighting of the partial least squares components. The robust decomposition feature of our proposed method enables the robust estimation of model parameters in the scalar-on-function regression model, ensuring reliable predictions in the presence of outliers and leverage points. Moreover, it accurately identifies zero and nonzero sub-regions where the slope function is estimated, even in the presence of outliers and leverage points. We assess our proposed method’s estimation and predictive performance through a series of Monte Carlo experiments and an empirical dataset—that is, data collected in relation to oriented strand board. Compared to existing methods our proposed method performs favorably. Notably, our robust procedure exhibits superior performance in the presence of outliers while maintaining competitiveness in their absence. Our method has been implemented in the robsfplsr package in .},
  archive      = {J_SAC},
  author       = {Gurer, Sude and Shang, Han Lin and Mandal, Abhijit and Beyaztas, Ufuk},
  doi          = {10.1007/s11222-024-10464-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Locally sparse and robust partial least squares in scalar-on-function regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Explainable generalized additive neural
networks with independent neural network training. <em>SAC</em>,
<em>34</em>(5), 1. (<a
href="https://doi.org/10.1007/s11222-024-10461-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Ortega-Fernandez, Ines and Sestelo, Marta and Villanueva, Nora M.},
  doi          = {10.1007/s11222-024-10461-1},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: Explainable generalized additive neural networks with independent neural network training},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A limit formula and a series expansion for the bivariate
normal tail probability. <em>SAC</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10466-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a limit formula for the bivariate Normal tail probability. It only requires the larger threshold to grow indefinitely, but otherwise has no restrictions on how the thresholds grow. The correlation parameter can change and possibly depend on the thresholds. The formula is applicable regardless of Salvage’s condition. Asymptotically, it reduces to Ruben’s formula and Hashorva’s formula under the corresponding conditions, and therefore can be considered a generalisation. Under a mild condition, it satisfies Plackett’s identity on the derivative with respect to the correlation parameter. Motivated by the limit formula, a series expansion is also obtained for the exact tail probability using derivatives of the univariate Mill’s ratio. Under similar conditions for the limit formula, the series converges and its truncated approximation has a small remainder term for large thresholds. To take advantage of this, a simple procedure is developed for the general case by remapping the parameters so that they satisfy the conditions. Examples are presented to illustrate the theoretical findings.},
  archive      = {J_SAC},
  author       = {Au, Siu-Kui},
  doi          = {10.1007/s11222-024-10466-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A limit formula and a series expansion for the bivariate normal tail probability},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mixture of experts regression model for functional
response with functional covariates. <em>SAC</em>, <em>34</em>(5), 1–23.
(<a href="https://doi.org/10.1007/s11222-024-10455-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the fast growth of data that are measured on a continuous scale, functional data analysis has undergone many developments in recent years. Regression models with a functional response involving functional covariates, also called “function-on-function”, are thus becoming very common. Studying this type of model in the presence of heterogeneous data can be particularly useful in various practical situations. We mainly develop in this work a function-on-function Mixture of Experts (FFMoE) regression model. Like most of the inference approach for models on functional data, we use basis expansion (B-splines) both for covariates and parameters. A regularized inference approach is also proposed, it accurately smoothes functional parameters in order to provide interpretable estimators. Numerical studies on simulated data illustrate the good performance of FFMoE as compared with competitors. Usefullness of the proposed model is illustrated on two data sets: the reference Canadian weather data set, in which the precipitations are modeled according to the temperature, and a Cycling data set, in which the developed power is explained by the speed, the cyclist heart rate and the slope of the road.},
  archive      = {J_SAC},
  author       = {Tamo Tchomgui, Jean Steve and Jacques, Julien and Fraysse, Guillaume and Barriac, Vincent and Chretien, Stéphane},
  doi          = {10.1007/s11222-024-10455-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {A mixture of experts regression model for functional response with functional covariates},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse and geometry-aware generalisation of the mutual
information for joint discriminative clustering and feature selection.
<em>SAC</em>, <em>34</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s11222-024-10467-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection in clustering is a hard task which involves simultaneously the discovery of relevant clusters as well as relevant variables with respect to these clusters. While feature selection algorithms are often model-based through optimised model selection or strong assumptions on the data distribution, we introduce a discriminative clustering model trying to maximise a geometry-aware generalisation of the mutual information called GEMINI with a simple $$\ell _1$$ penalty: the Sparse GEMINI. This algorithm avoids the burden of combinatorial feature subset exploration and is easily scalable to high-dimensional data and large amounts of samples while only designing a discriminative clustering model. We demonstrate the performances of Sparse GEMINI on synthetic datasets and large-scale datasets. Our results show that Sparse GEMINI is a competitive algorithm and has the ability to select relevant subsets of variables with respect to the clustering without using relevance criteria or prior hypotheses.},
  archive      = {J_SAC},
  author       = {Ohl, Louis and Mattei, Pierre-Alexandre and Bouveyron, Charles and Leclercq, Mickaël and Droit, Arnaud and Precioso, Frédéric},
  doi          = {10.1007/s11222-024-10467-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Sparse and geometry-aware generalisation of the mutual information for joint discriminative clustering and feature selection},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal designs for nonlinear mixed-effects models using
competitive swarm optimizer with mutated agents. <em>SAC</em>,
<em>34</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10468-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature-inspired meta-heuristic algorithms are increasingly used in many disciplines to tackle challenging optimization problems. Our focus is to apply a newly proposed nature-inspired meta-heuristics algorithm called CSO-MA to solve challenging design problems in biosciences and demonstrate its flexibility to find various types of optimal approximate or exact designs for nonlinear mixed models with one or several interacting factors and with or without random effects. We show that CSO-MA is efficient and can frequently outperform other algorithms either in terms of speed or accuracy. The algorithm, like other meta-heuristic algorithms, is free of technical assumptions and flexible in that it can incorporate cost structure or multiple user-specified constraints, such as, a fixed number of measurements per subject in a longitudinal study. When possible, we confirm some of the CSO-MA generated designs are optimal with theory by developing theory-based innovative plots. Our applications include searching optimal designs to estimate (i) parameters in mixed nonlinear models with correlated random effects, (ii) a function of parameters for a count model in a dose combination study, and (iii) parameters in a HIV dynamic model. In each case, we show the advantages of using a meta-heuristic approach to solve the optimization problem, and the added benefits of the generated designs.},
  archive      = {J_SAC},
  author       = {Cui, Elvis Han and Zhang, Zizhao and Wong, Weng Kee},
  doi          = {10.1007/s11222-024-10468-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Optimal designs for nonlinear mixed-effects models using competitive swarm optimizer with mutated agents},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ForLion: A new algorithm for d-optimal designs under general
parametric statistical models with mixed factors. <em>SAC</em>,
<em>34</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10465-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of designing an experimental plan with both discrete and continuous factors under fairly general parametric statistical models. We propose a new algorithm, named ForLion, to search for locally optimal approximate designs under the D-criterion. The algorithm performs an exhaustive search in a design space with mixed factors while keeping high efficiency and reducing the number of distinct experimental settings. Its optimality is guaranteed by the general equivalence theorem. We present the relevant theoretical results for multinomial logit models (MLM) and generalized linear models (GLM), and demonstrate the superiority of our algorithm over state-of-the-art design algorithms using real-life experiments under MLM and GLM. Our simulation studies show that the ForLion algorithm could reduce the number of experimental settings by 25% or improve the relative efficiency of the designs by 17.5% on average. Our algorithm can help the experimenters reduce the time cost, the usage of experimental devices, and thus the total cost of their experiments while preserving high efficiencies of the designs.},
  archive      = {J_SAC},
  author       = {Huang, Yifei and Li, Keren and Mandal, Abhyuday and Yang, Jie},
  doi          = {10.1007/s11222-024-10465-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {ForLion: A new algorithm for D-optimal designs under general parametric statistical models with mixed factors},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Byzantine-robust and efficient distributed sparsity
learning: A surrogate composite quantile regression approach.
<em>SAC</em>, <em>34</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10470-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed statistical learning has gained significant traction recently, mainly due to the availability of unprecedentedly massive datasets. The objective of distributed statistical learning is to learn models by effectively utilizing data scattered across various machines. However, its performance can be impeded by three significant challenges: arbitrary noises, high dimensionality, and machine failures—the latter being specifically referred to as Byzantine failure. To address the first two challenges, we propose leveraging the potential of composite quantile regression in conjunction with the $$\ell _1$$ penalty. However, this combination introduces a doubly nonsmooth objective function, posing new challenges. In such scenarios, most existing Byzantine-robust methods exhibit slow sublinear convergence rates and fail to achieve near-optimal statistical convergence rates. To fill this gap, we introduce a novel smoothing procedure that effectively handles the nonsmooth aspects. This innovation allows us to develop a Byzantine-robust sparsity learning algorithm that converges provably to the near-optimal convergence rate linearly. Moreover, we establish support recovery guarantees for our proposed methods. We substantiate the effectiveness of our approaches through comprehensive empirical analyses.},
  archive      = {J_SAC},
  author       = {Chen, Canyi and Zhu, Zhengtian},
  doi          = {10.1007/s11222-024-10470-0},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Byzantine-robust and efficient distributed sparsity learning: A surrogate composite quantile regression approach},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mallows-type model averaging estimator for ridge
regression with randomly right censored data. <em>SAC</em>,
<em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10472-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instead of picking up a single ridge parameter in ridge regression, this paper considers a frequentist model averaging approach to appropriately combine the set of ridge estimators with different ridge parameters, when the response is randomly right censored. Within this context, we propose a weighted least squares ridge estimation for unknown regression parameter. A new Mallows-type weight choice criterion is then developed to allocate model weights, where the unknown distribution function of the censoring random variable is replaced by the Kaplan–Meier estimator and the covariance matrix of random errors is substituted by its averaging estimator. Under some mild conditions, we show that when the fitting model is misspecified, the resulting model averaging estimator achieves optimality in terms of minimizing the loss function. Whereas, when the fitting model is correctly specified, the model averaging estimator of the regression parameter is root-n consistent. Additionally, for the weight vector which is obtained by minimizing the new criterion, we establish its rate of convergence to the infeasible optimal weight vector. Simulation results show that our method is better than some existing methods. A real dataset is analyzed for illustration as well.},
  archive      = {J_SAC},
  author       = {Zeng, Jie and Hu, Guozhi and Cheng, Weihu},
  doi          = {10.1007/s11222-024-10472-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A mallows-type model averaging estimator for ridge regression with randomly right censored data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional missing data imputation via undirected
graphical model. <em>SAC</em>, <em>34</em>(5), 1–9. (<a
href="https://doi.org/10.1007/s11222-024-10475-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation is a practical approach in analyzing incomplete data, with multiple imputation by chained equations (MICE) being popularly used. MICE specifies a conditional distribution for each variable to be imputed, but estimating it is inherently a high-dimensional problem for large-scale data. Existing approaches propose to utilize regularized regression models, such as lasso. However, the estimation of them occurs iteratively across all incomplete variables, leading to a considerable increase in computational burden, as demonstrated in our simulation study. To overcome this computational bottleneck, we propose a novel method that estimates the conditional independence structure among variables before the imputation procedure. We extract such information from an undirected graphical model, leveraging the graphical lasso method based on the inverse probability weighting estimator. Our simulation study verifies the proposed method is way faster against the existing methods, while still maintaining comparable imputation performance.},
  archive      = {J_SAC},
  author       = {Lee, Yoonah and Park, Seongoh},
  doi          = {10.1007/s11222-024-10475-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional missing data imputation via undirected graphical model},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Distributed subsampling for multiplicative regression.
<em>SAC</em>, <em>34</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10477-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplicative regression is a useful alternative tool in modeling positive response data. This paper proposes two distributed estimators for multiplicative error model on distributed system with non-randomly distributed massive data. We first present a Poisson subsampling procedure to obtain a subsampling estimator based on the least product relative error (LPRE) loss, which is effective on a distributed system. Theoretically, we justify the subsampling estimator by establishing its convergence rate, asymptotic normality and deriving the optimal subsampling probabilities in terms of the L-optimality criterion. Then, we provide a distributed LPRE estimator based on the Poisson subsampling (DLPRE-P), which is communication-efficient since it needs to transmit a very small subsample from local machines to the central site, which is empirically feasible, together with the gradient of the loss. Practically, due to the use of Newton–Raphson iteration, the Hessian matrix can be computed more robustly using the subsampled data than using one local dataset. We also show that the DLPRE-P estimator is statistically efficient as the global estimator, which is based on putting all the datasets together. Furthermore, we propose a distributed regularized LPRE estimator (DRLPRE-P) to consider the variable selection problem in high dimension. A distributed algorithm based on the alternating direction method of multipliers (ADMM) is developed for implementing the DRLPRE-P. The oracle property holds for DRLPRE-P. Finally, simulation experiments and two real-world data analyses are conducted to illustrate the performance of our methods.},
  archive      = {J_SAC},
  author       = {Li, Xiaoyan and Xia, Xiaochao and Zhang, Zhimin},
  doi          = {10.1007/s11222-024-10477-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Distributed subsampling for multiplicative regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of spatiotemporal changepoints: A generalised
additive model approach. <em>SAC</em>, <em>34</em>(5), 1–9. (<a
href="https://doi.org/10.1007/s11222-024-10478-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of changepoints in spatio-temporal datasets has been receiving increased focus in recent years and is utilised in a wide range of fields. With temporal data observed at different spatial locations, the current approach is typically to use univariate changepoint methods in a marginal sense with the detected changepoint being representative of a single location only. We present a spatio-temporal changepoint method that utilises a generalised additive model (GAM) dependent on the 2D spatial location and the observation time to account for the underlying spatio-temporal process. We use the full likelihood of the GAM in conjunction with the pruned linear exact time (PELT) changepoint search algorithm to detect multiple changepoints across spatial locations in a computationally efficient manner. When compared to a univariate marginal approach our method is shown to perform more efficiently in simulation studies at detecting true changepoints and demonstrates less evidence of overfitting. Furthermore, as the approach explicitly models spatio-temporal dependencies between spatial locations, any changepoints detected are common across the locations. We demonstrate an application of the method to an air quality dataset covering the COVID-19 lockdown in the United Kingdom.},
  archive      = {J_SAC},
  author       = {Hollaway, Michael J. and Killick, Rebecca},
  doi          = {10.1007/s11222-024-10478-6},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Detection of spatiotemporal changepoints: A generalised additive model approach},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). The COR criterion for optimal subset selection in
distributed estimation. <em>SAC</em>, <em>34</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10471-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of selecting an optimal subset in distributed regression is a crucial issue, as each distributed data subset may contain redundant information, which can be attributed to various sources such as outliers, dispersion, inconsistent duplicates, too many independent variables, and excessive data points, among others. Efficient reduction and elimination of this redundancy can help alleviate inconsistency issues for statistical inference. Therefore, it is imperative to track redundancy while measuring and processing data. We develop a criterion for optimal subset selection that is related to Covariance matrices, Observation matrices, and Response vectors (COR). We also derive a novel distributed interval estimation for the proposed criterion and establish the existence of optimal subset length. Finally, numerical experiments are conducted to verify the experimental feasibility of the proposed criterion.},
  archive      = {J_SAC},
  author       = {Guo, Guangbao and Song, Haoyue and Zhu, Lixing},
  doi          = {10.1007/s11222-024-10471-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {The COR criterion for optimal subset selection in distributed estimation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning variational autoencoders via MCMC speed measures.
<em>SAC</em>, <em>34</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10481-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximising an evidence lower bound. There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo methods for constructing variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run metropolis-adjusted Langevin or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical representations arising in hierarchical VAEs.},
  archive      = {J_SAC},
  author       = {Hirt, Marcel and Kreouzis, Vasileios and Dellaportas, Petros},
  doi          = {10.1007/s11222-024-10481-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Learning variational autoencoders via MCMC speed measures},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roughness regularization for functional data analysis with
free knots spline estimation. <em>SAC</em>, <em>34</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11222-024-10474-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, an ever-growing volume of information is recorded, either continuously over time or sporadically, at distinct time intervals. Functional Data Analysis (FDA) stands at the cutting edge of this data revolution, offering a powerful framework for handling and extracting meaningful insights from such complex datasets. The currently proposed FDA methods can often encounter challenges, especially when dealing with curves of varying shapes. This can largely be attributed to the method’s strong dependence on data approximation as a key aspect of the analysis process. In this work, we propose a free knots spline estimation method for functional data with two penalty terms and demonstrate its performance by comparing the results of several clustering methods on simulated and real data.},
  archive      = {J_SAC},
  author       = {De Magistris, Anna and De Simone, Valentina and Romano, Elvira and Toraldo, Gerardo},
  doi          = {10.1007/s11222-024-10474-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Roughness regularization for functional data analysis with free knots spline estimation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AR-ADASYN: Angle radius-adaptive synthetic data generation
approach for imbalanced learning. <em>SAC</em>, <em>34</em>(5), 1–11.
(<a href="https://doi.org/10.1007/s11222-024-10479-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data often leads to biased models favoring the majority class, limiting the representation of the training data and complicating generalization to minority classes. Previous studies have addressed this problem by rebalancing the dataset using resampling methods, such as SMOTE. However, these methods often encounter challenges such as overfitting and information loss. To overcome these limitations, we propose a novel approach called Angle Radius-Adaptive Synthetic Sampling (AR-ADASYN). The proposed method aims to maintain the distribution of minority classes by taking into account both the distance and the angle between existing samples of the minority class. We conducted experiments on 14 datasets composed of real data to compare the classification performance of the proposed method with other oversampling techniques. The results revealed that the proposed method demonstrated superior classification performance compared to the other oversampling techniques. As a result, AR-ADASYN showed potential as a valuable tool for improving the robustness and generalization of machine learning models trained on imbalanced datasets.},
  archive      = {J_SAC},
  author       = {Park, Hyejoon and Kim, Hyunjoong},
  doi          = {10.1007/s11222-024-10479-5},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {AR-ADASYN: Angle radius-adaptive synthetic data generation approach for imbalanced learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach to modeling finite element
discretization error. <em>SAC</em>, <em>34</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10463-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the uncertainty associated with the finite element discretization error is modeled following the Bayesian paradigm. First, a continuous formulation is derived, where a Gaussian process prior over the solution space is updated based on observations from a finite element discretization. To avoid the computation of intractable integrals, a second, finer, discretization is introduced that is assumed sufficiently dense to represent the true solution field. A prior distribution is assumed over the fine discretization, which is then updated based on observations from the coarse discretization. This yields a posterior distribution with a mean that serves as an estimate of the solution, and a covariance that models the uncertainty associated with this estimate. Two particular choices of prior are investigated: a prior defined implicitly by assigning a white noise distribution to the right-hand side term, and a prior whose covariance function is equal to the Green’s function of the partial differential equation. The former yields a posterior distribution with a mean close to the reference solution, but a covariance that contains little information regarding the finite element discretization error. The latter, on the other hand, yields posterior distribution with a mean equal to the coarse finite element solution, and a covariance with a close connection to the discretization error. For both choices of prior a contradiction arises, since the discretization error depends on the right-hand side term, but the posterior covariance does not. We demonstrate how, by rescaling the eigenvalues of the posterior covariance, this independence can be avoided.},
  archive      = {J_SAC},
  author       = {Poot, Anne and Kerfriden, Pierre and Rocha, Iuri and van der Meer, Frans},
  doi          = {10.1007/s11222-024-10463-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian approach to modeling finite element discretization error},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taming numerical imprecision by adapting the KL divergence
to negative probabilities. <em>SAC</em>, <em>34</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10480-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kullback–Leibler (KL) divergence is frequently used in data science. For discrete distributions on large state spaces, approximations of probability vectors may result in a few small negative entries, rendering the KL divergence undefined. We address this problem by introducing a parameterized family of substitute divergence measures, the shifted KL (sKL) divergence measures. Our approach is generic and does not increase the computational overhead. We show that the sKL divergence shares important theoretical properties with the KL divergence and discuss how its shift parameters should be chosen. If Gaussian noise is added to a probability vector, we prove that the average sKL divergence converges to the KL divergence for small enough noise. We also show that our method solves the problem of negative entries in an application from computational oncology, the optimization of Mutual Hazard Networks for cancer progression using tensor-train approximations.},
  archive      = {J_SAC},
  author       = {Pfahler, Simon and Georg, Peter and Schill, Rudolf and Klever, Maren and Grasedyck, Lars and Spang, Rainer and Wettig, Tilo},
  doi          = {10.1007/s11222-024-10480-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Taming numerical imprecision by adapting the KL divergence to negative probabilities},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: R-VGAL: A sequential variational bayes
algorithm for generalised linear mixed models. <em>SAC</em>,
<em>34</em>(5), 1. (<a
href="https://doi.org/10.1007/s11222-024-10469-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Vu, Bao Anh and Gunawan, David and Zammit-Mangion, Andrew},
  doi          = {10.1007/s11222-024-10469-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: R-VGAL: a sequential variational bayes algorithm for generalised linear mixed models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein principal component analysis for circular
measures. <em>SAC</em>, <em>34</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10473-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the 2-Wasserstein space of probability measures supported on the unit-circle, and propose a framework for Principal Component Analysis (PCA) for data living in such a space. We build on a detailed investigation of the optimal transportation problem for measures on the unit-circle which might be of independent interest. In particular, building on previously obtained results, we derive an expression for optimal transport maps in (almost) closed form and propose an alternative definition of the tangent space at an absolutely continuous probability measure, together with fundamental characterizations of the associated exponential and logarithmic maps. PCA is performed by mapping data on the tangent space at the Wasserstein barycentre, which we approximate via an iterative scheme, and for which we establish a sufficient a posteriori condition to assess its convergence. Our methodology is illustrated on several simulated scenarios and a real data analysis of measurements of optical nerve thickness.},
  archive      = {J_SAC},
  author       = {Beraha, Mario and Pegoraro, Matteo},
  doi          = {10.1007/s11222-024-10473-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Wasserstein principal component analysis for circular measures},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new maximum mean discrepancy based two-sample test for
equal distributions in separable metric spaces. <em>SAC</em>,
<em>34</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10483-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel two-sample test for equal distributions in separable metric spaces, utilizing the maximum mean discrepancy (MMD). The test statistic is derived from the decomposition of the total variation of data in the reproducing kernel Hilbert space, and can be regarded as a V-statistic-based estimator of the squared MMD. The paper establishes the asymptotic null and alternative distributions of the test statistic. To approximate the null distribution accurately, a three-cumulant matched chi-squared approximation method is employed. The parameters for this approximation are consistently estimated from the data. Additionally, the paper introduces a new data-adaptive method based on the median absolute deviation to select the kernel width of the Gaussian kernel, and a new permutation test combining two different Gaussian kernel width selection methods, which improve the adaptability of the test to different data sets. Fast implementation of the test using matrix calculation is discussed. Extensive simulation studies and three real data examples are presented to demonstrate the good performance of the proposed test.},
  archive      = {J_SAC},
  author       = {Zhou, Bu and Ong, Zhi Peng and Zhang, Jin-Ting},
  doi          = {10.1007/s11222-024-10483-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A new maximum mean discrepancy based two-sample test for equal distributions in separable metric spaces},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualized causal mediation analysis with continuous
treatment using conditional generative adversarial networks.
<em>SAC</em>, <em>34</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10484-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods used in causal mediation analysis with continuous treatment often focus on estimating average causal effects, limiting their applicability in precision medicine. Machine learning techniques have emerged as a powerful approach for precisely estimating individualized causal effects. This paper proposes a novel method called CGAN-ICMA-CT that leverages Conditional Generative Adversarial Networks (CGANs) to infer individualized causal effects with continuous treatment. We thoroughly investigate the convergence properties of CGAN-ICMA-CT and show that the estimated distribution of our inferential conditional generator converges to the true conditional distribution under mild conditions. We conduct numerical experiments to validate the effectiveness of CGAN-ICMA-CT and compare it with four commonly used methods: linear regression, support vector machine regression, decision tree, and random forest regression. The results demonstrate that CGAN-ICMA-CT outperforms these methods regarding accuracy and precision. Furthermore, we apply the CGAN-ICMA-CT model to the real-world Job Corps dataset, showcasing its practical utility. By utilizing CGAN-ICMA-CT, we estimate the individualized causal effects of the Job Corps program on the number of arrests, providing insights into both direct effects and effects mediated through intermediate variables. Our findings confirm the potential of CGAN-ICMA-CT in advancing individualized causal mediation analysis with continuous treatment in precision medicine settings.},
  archive      = {J_SAC},
  author       = {Huan, Cheng and Song, Xinyuan and Yuan, Hongwei},
  doi          = {10.1007/s11222-024-10484-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Individualized causal mediation analysis with continuous treatment using conditional generative adversarial networks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse bayesian learning using TMB (template model builder).
<em>SAC</em>, <em>34</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10476-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Bayesian Learning, and more specifically the Relevance Vector Machine (RVM), can be used in supervised learning for both classification and regression problems. Such methods are particularly useful when applied to big data in order to find a sparse (in weight space) representation of the model. This paper demonstrates that the Template Model Builder (TMB) is an accurate and flexible computational framework for implementation of sparse Bayesian learning methods.The user of TMB is only required to specify the joint likelihood of the weights and the data, while the Laplace approximation of the marginal likelihood is automatically evaluated to numerical precision. This approximation is in turn used to estimate hyperparameters by maximum marginal likelihood. In order to reduce the computational cost of the Laplace approximation we introduce the notion of an “active set” of weights, and we devise an algorithm for dynamically updating this set until convergence, similar to what is done in other RVM type methods. We implement two different methods using TMB; the RVM and the Probabilistic Feature Selection and Classification Vector Machine method, where the latter also performs feature selection. Experiments based on benchmark data show that our TMB implementation performs comparable to that of the original implementation, but at a lower implementation cost. TMB can also calculate model and prediction uncertainty, by including estimation uncertainty from both latent variables and the hyperparameters. In conclusion, we find that TMB is a flexible tool that facilitates implementation and prototyping of sparse Bayesian methods.},
  archive      = {J_SAC},
  author       = {Helgøy, Ingvild M. and Skaug, Hans J. and Li, Yushu},
  doi          = {10.1007/s11222-024-10476-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Sparse bayesian learning using TMB (Template model builder)},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SB-ETAS: Using simulation based inference for scalable,
likelihood-free inference for the ETAS model of earthquake occurrences.
<em>SAC</em>, <em>34</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10486-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of earthquake catalogs, driven by machine learning-based phase picking and denser seismic networks, calls for the application of a broader range of models to determine whether the new data enhances earthquake forecasting capabilities. Additionally, this growth demands that existing forecasting models efficiently scale to handle the increased data volume. Approximate inference methods such as inlabru, which is based on the Integrated nested Laplace approximation, offer improved computational efficiencies and the ability to perform inference on more complex point-process models compared to traditional MCMC approaches. We present SB-ETAS: a simulation based inference procedure for the epidemic-type aftershock sequence (ETAS) model. This approximate Bayesian method uses sequential neural posterior estimation (SNPE) to learn posterior distributions from simulations, rather than typical MCMC sampling using the likelihood. On synthetic earthquake catalogs, SB-ETAS provides better coverage of ETAS posterior distributions compared with inlabru. Furthermore, we demonstrate that using a simulation based procedure for inference improves the scalability from $$\mathcal {O}(n^2)$$ to $$\mathcal {O}(n\log n)$$ . This makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1981. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 h on a standard laptop, a task that would have taken over 2 weeks using MCMC. Beyond the standard ETAS model, this simulation based framework allows earthquake modellers to define and infer parameters for much more complex models by removing the need to define a likelihood function.},
  archive      = {J_SAC},
  author       = {Stockman, Samuel and Lawson, Daniel J. and Werner, Maximilian J.},
  doi          = {10.1007/s11222-024-10486-6},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {SB-ETAS: Using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New forest-based approaches for sufficient dimension
reduction. <em>SAC</em>, <em>34</em>(5), 1–28. (<a
href="https://doi.org/10.1007/s11222-024-10482-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) primarily aims to reduce the dimensionality of high-dimensional predictor variables while retaining essential information about the responses. Traditional SDR methods typically employ kernel weighting functions, which unfortunately makes them susceptible to the curse of dimensionality. To address this issue, we in this paper propose novel forest-based approaches for SDR that utilize a locally adaptive kernel generated by Mondrian forests. Overall, our work takes the perspective of Mondrian forest as an adaptive weighted kernel technique for SDR problems. In the central mean subspace model, by integrating the methods from Xia et al. (J R Stat Soc Ser B (Stat Methodol) 64(3):363–410, 2002. https://doi.org/10.1111/1467-9868.03411 ) with Mondrian forest weights, we suggest the forest-based outer product of gradients estimation (mf-OPG) and the forest-based minimum average variance estimation (mf-MAVE). Moreover, we substitute the kernels used in nonparametric density function estimations (Xia in Ann Stat 35(6):2654–2690, 2007. https://doi.org/10.1214/009053607000000352 ), targeting the central subspace, with Mondrian forest weights. These techniques are referred to as mf-dOPG and mf-dMAVE, respectively. Under regularity conditions, we establish the asymptotic properties of our forest-based estimators, as well as the convergence of the affiliated algorithms. Through simulation studies and analysis of fully observable data, we demonstrate substantial improvements in computational efficiency and predictive accuracy of our proposals compared with the traditional counterparts.},
  archive      = {J_SAC},
  author       = {Dai, Shuang and Wu, Ping and Yu, Zhou},
  doi          = {10.1007/s11222-024-10482-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {New forest-based approaches for sufficient dimension reduction},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive comparison of goodness-of-fit tests for
logistic regression models. <em>SAC</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10487-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a projection-based test for assessing logistic regression models using the empirical residual marked empirical process and suggest a model-based bootstrap procedure to calculate critical values. We comprehensively compare this test and Stute and Zhu’s test with several commonly used goodness-of-fit (GoF) tests: the Hosmer–Lemeshow test, modified Hosmer–Lemeshow test, Osius–Rojek test, and Stukel test for logistic regression models in terms of type I error control and power performance in small ( $$n=50$$ ), moderate ( $$n=100$$ ), and large ( $$n=500$$ ) sample sizes. We assess the power performance for two commonly encountered situations: nonlinear and interaction departures from the null hypothesis. All tests except the modified Hosmer–Lemeshow test and Osius–Rojek test have the correct size in all sample sizes. The power performance of the projection based test consistently outperforms its competitors. We apply these tests to analyze an AIDS dataset and a cancer dataset. For the former, all tests except the projection-based test do not reject a simple linear function in the logit, which has been illustrated to be deficient in the literature. For the latter dataset, the Hosmer–Lemeshow test, modified Hosmer–Lemeshow test, and Osius–Rojek test fail to detect the quadratic form in the logit, which was detected by the Stukel test, Stute and Zhu’s test, and the projection-based test.},
  archive      = {J_SAC},
  author       = {Liu, Huiling and Li, Xinmin and Chen, Feifei and Härdle, Wolfgang and Liang, Hua},
  doi          = {10.1007/s11222-024-10487-5},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A comprehensive comparison of goodness-of-fit tests for logistic regression models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal confidence interval for the difference between
proportions. <em>SAC</em>, <em>34</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10485-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the probability of the binomial distribution is a basic problem, which appears in almost all introductory statistics courses and is performed frequently in various studies. In some cases, the parameter of interest is a difference between two probabilities, and the current work studies the construction of confidence intervals for this parameter when the sample size is small. Our goal is to find the shortest confidence intervals under the constraint of coverage probability being at least as large as a predetermined level. For the two-sample case, there is no known algorithm that achieves this goal, but different heuristics procedures have been suggested, and the present work aims at finding optimal confidence intervals. In the one-sample case, there is a known algorithm that finds optimal confidence intervals presented by Blyth and Still (J Am Stat Assoc 78(381):108–116, 1983). It is based on solving small and local optimization problems and then using an inversion step to find the global optimum solution. We show that this approach fails in the two-sample case and therefore, in order to find optimal confidence intervals, one needs to solve a global optimization problem, rather than small and local ones, which is computationally much harder. We present and discuss the suitable global optimization problem. Using the Gurobi package we find near-optimal solutions when the sample sizes are smaller than 15, and we compare these solutions to some existing methods, both approximate and exact. We find that the improvement in terms of lengths with respect to the best competitor varies between 1.5 and 5% for different parameters of the problem. Therefore, we recommend the use of the new confidence intervals when both sample sizes are smaller than 15. Tables of the confidence intervals are given in the Excel file in this link ( https://technionmail-my.sharepoint.com/:f:/g/personal/ap_campus_technion_ac_il/El-213Kms51BhQxR8MmQJCYBDfIsvtrK9mQIey1sZnZWIQ?e=hxGunl ).},
  archive      = {J_SAC},
  author       = {Peer, Almog and Azriel, David},
  doi          = {10.1007/s11222-024-10485-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Optimal confidence interval for the difference between proportions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust singular value decomposition with application to
video surveillance background modelling. <em>SAC</em>, <em>34</em>(5),
1–26. (<a href="https://doi.org/10.1007/s11222-024-10493-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional method of computing singular value decomposition (SVD) of a data matrix is based on the least squares principle and is, therefore, very sensitive to the presence of outliers. Hence, the resulting inferences across different applications using the classical SVD are extremely degraded in the presence of data contamination. In particular, background modelling of video surveillance data in the presence of camera tampering cannot be reliably solved by the classical SVD. In this paper, we propose a novel robust singular value decomposition technique based on the popular minimum density power divergence estimator. We have established the theoretical properties of the proposed estimator such as convergence, equivariance and consistency under the high-dimensional regime where both the row and column dimensions of the data matrix approach infinity. We also propose a fast and scalable algorithm based on alternating weighted regression to obtain the estimate. Within the scope of our fairly extensive simulation studies, our method performs better than existing robust SVD algorithms. Finally, we present an application of the proposed method on the video surveillance background modelling problem.},
  archive      = {J_SAC},
  author       = {Roy, Subhrajyoty and Ghosh, Abhik and Basu, Ayanendranath},
  doi          = {10.1007/s11222-024-10493-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Robust singular value decomposition with application to video surveillance background modelling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian cross-validation by parallel markov chain monte
carlo. <em>SAC</em>, <em>34</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10404-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brute force cross-validation (CV) is a method for predictive assessment and model selection that is general and applicable to a wide range of Bayesian models. Naive or ‘brute force’ CV approaches are often too computationally costly for interactive modeling workflows, especially when inference relies on Markov chain Monte Carlo (MCMC). We propose overcoming this limitation using massively parallel MCMC. Using accelerator hardware such as graphics processor units, our approach can be about as fast (in wall clock time) as a single full-data model fit. Parallel CV is flexible because it can easily exploit a wide range data partitioning schemes, such as those designed for non-exchangeable data. It can also accommodate a range of scoring rules. We propose MCMC diagnostics, including a summary of MCMC mixing based on the popular potential scale reduction factor ( $$\widehat{\textrm{R}}$$ ) and MCMC effective sample size ( $$\widehat{\textrm{ESS}}$$ ) measures. We also describe a method for determining whether an $$\widehat{\textrm{R}}$$ diagnostic indicates approximate stationarity of the chains, that may be of more general interest for applications beyond parallel CV. Finally, we show that parallel CV and its diagnostics can be implemented with online algorithms, allowing parallel CV to scale up to very large blocking designs on memory-constrained computing accelerators.},
  archive      = {J_SAC},
  author       = {Cooper, Alex and Vehtari, Aki and Forbes, Catherine and Simpson, Dan and Kennedy, Lauren},
  doi          = {10.1007/s11222-024-10404-w},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian cross-validation by parallel markov chain monte carlo},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of the generalized covariance estimator in
noncausal processes. <em>SAC</em>, <em>34</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10437-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the performance of routinely used optimization algorithms in application to the Generalized Covariance estimator (GCov) for univariate and multivariate mixed causal and noncausal models. The GCov is a semi-parametric estimator with an objective function based on nonlinear autocovariances to identify causal and noncausal orders. When the number and type of nonlinear autocovariances included in the objective function are insufficient/inadequate, or the error density is too close to the Gaussian, identification issues can arise. These issues result in local minima in the objective function, which correspond to parameter values associated with incorrect causal and noncausal orders. Then, depending on the starting point and the optimization algorithm employed, the algorithm can converge to a local minimum. The paper proposes the Simulated Annealing (SA) optimization algorithm as an alternative to conventional numerical optimization methods. The results demonstrate that SA performs well in its application to mixed causal and noncausal models, successfully eliminating the effects of local minima. The proposed approach is illustrated by an empirical study of a bivariate series of commodity prices.},
  archive      = {J_SAC},
  author       = {Cubadda, Gianluca and Giancaterini, Francesco and Hecq, Alain and Jasiak, Joann},
  doi          = {10.1007/s11222-024-10437-1},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimization of the generalized covariance estimator in noncausal processes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing the goodness-of-fit of the stable distributions with
applications to german stock index data and bitcoin cryptocurrency data.
<em>SAC</em>, <em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10441-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier-prone data sets are of immense interest in diverse areas including economics, finance, statistical physics, signal processing, telecommunications and so on. Stable laws (also known as $$\alpha $$ - stable laws) are often found to be useful in modeling outlier-prone data containing important information and exhibiting heavy tailed phenomenon. In this article, an asymptotic distribution of a unbiased and consistent estimator of the stability index $$\alpha $$ is proposed based on jackknife empirical likelihood (JEL) and adjusted JEL method. Next, using the sum-preserving property of stable random variables and exploiting U-statistic theory, we have developed a goodness-of-fit test procedure for $$\alpha $$ -stable distributions where the stability index $$\alpha $$ is specified. Extensive simulation studies are performed in order to assess the finite sample performance of the proposed test. Finally, two appealing real life data examples related to the daily closing price of German Stock Index and Bitcoin cryptocurrency are analysed in detail for illustration purposes.},
  archive      = {J_SAC},
  author       = {Khan, Ruhul Ali and Pal, Ayan and Kundu, Debasis},
  doi          = {10.1007/s11222-024-10441-5},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Testing the goodness-of-fit of the stable distributions with applications to german stock index data and bitcoin cryptocurrency data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jittering and clustering: Strategies for the construction of
robust designs. <em>SAC</em>, <em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10436-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss, and give examples of, methods for randomly implementing some minimax robust designs from the literature. These have the advantage, over their deterministic counterparts, of having bounded maximum loss in large and very rich neighbourhoods of the, almost certainly inexact, response model fitted by the experimenter. Their maximum loss rivals that of the theoretically best possible, but not implementable, minimax designs. The procedures are then extended to more general robust designs. For two-dimensional designs we sample from contractions of Voronoi tessellations, generated by selected basis points, which partition the design space. These ideas are then extended to k-dimensional designs for general k.},
  archive      = {J_SAC},
  author       = {Wiens, Douglas P.},
  doi          = {10.1007/s11222-024-10436-2},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Jittering and clustering: Strategies for the construction of robust designs},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Insufficient gibbs sampling. <em>SAC</em>, <em>34</em>(4),
1–17. (<a href="https://doi.org/10.1007/s11222-024-10423-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some applied scenarios, the availability of complete data is restricted, often due to privacy concerns; only aggregated, robust and inefficient statistics derived from the data are made accessible. These robust statistics are not sufficient, but they demonstrate reduced sensitivity to outliers and offer enhanced data protection due to their higher breakdown point. We consider a parametric framework and propose a method to sample from the posterior distribution of parameters conditioned on various robust and inefficient statistics: specifically, the pairs (median, MAD) or (median, IQR), or a collection of quantiles. Our approach leverages a Gibbs sampler and simulates latent augmented data, which facilitates simulation from the posterior distribution of parameters belonging to specific families of distributions. A by-product of these samples from the joint posterior distribution of parameters and data given the observed statistics is that we can estimate Bayes factors based on observed statistics via bridge sampling. We validate and outline the limitations of the proposed methods through toy examples and an application to real-world income data.},
  archive      = {J_SAC},
  author       = {Luciano, Antoine and Robert, Christian P. and Ryder, Robin J.},
  doi          = {10.1007/s11222-024-10423-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Insufficient gibbs sampling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized fused lasso for grouped data in generalized
linear models. <em>SAC</em>, <em>34</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10433-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized fused Lasso (GFL) is a powerful method based on adjacent relationships or the network structure of data. It is used in a number of research areas, including clustering, discrete smoothing, and spatio-temporal analysis. When applying GFL, the specific optimization method used is an important issue. In generalized linear models, efficient algorithms based on the coordinate descent method have been developed for trend filtering under the binomial and Poisson distributions. However, to apply GFL to other distributions, such as the negative binomial distribution, which is used to deal with overdispersion in the Poisson distribution, or the gamma and inverse Gaussian distributions, which are used for positive continuous data, an algorithm for each individual distribution must be developed. To unify GFL for distributions in the exponential family, this paper proposes a coordinate descent algorithm for generalized linear models. To illustrate the method, a real data example of spatio-temporal analysis is provided.},
  archive      = {J_SAC},
  author       = {Ohishi, Mineaki},
  doi          = {10.1007/s11222-024-10433-5},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Generalized fused lasso for grouped data in generalized linear models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fused lasso nearly-isotonic signal approximation in general
dimensions. <em>SAC</em>, <em>34</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10432-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce and study fused lasso nearly-isotonic signal approximation, which is a combination of fused lasso and generalized nearly-isotonic regression. We show how these three estimators relate to each other and derive solution to a general problem. Our estimator is computationally feasible and provides a trade-off between monotonicity, block sparsity, and goodness-of-fit. Next, we prove that fusion and near-isotonisation in a one-dimensional case can be applied interchangably, and this step-wise procedure gives the solution to the original optimization problem. This property of the estimator is very important, because it provides a direct way to construct a path solution when one of the penalization parameters is fixed. Also, we derive an unbiased estimator of degrees of freedom of the estimator.},
  archive      = {J_SAC},
  author       = {Pastukhov, Vladimir},
  doi          = {10.1007/s11222-024-10432-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Fused lasso nearly-isotonic signal approximation in general dimensions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Type i tobit bayesian additive regression trees for censored
outcome regression. <em>SAC</em>, <em>34</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s11222-024-10434-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censoring occurs when an outcome is unobserved beyond some threshold value. Methods that do not account for censoring produce biased predictions of the unobserved outcome. This paper introduces Type I Tobit Bayesian Additive Regression Tree (TOBART-1) models for censored outcomes. Simulation results and real data applications demonstrate that TOBART-1 produces accurate predictions of censored outcomes. TOBART-1 provides posterior intervals for the conditional expectation and other quantities of interest. The error term distribution can have a large impact on the expectation of the censored outcome. Therefore, the error is flexibly modeled as a Dirichlet process mixture of normal distributions. An R package is available at https://github.com/EoghanONeill/TobitBART .},
  archive      = {J_SAC},
  author       = {O’Neill, Eoghan},
  doi          = {10.1007/s11222-024-10434-4},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Type i tobit bayesian additive regression trees for censored outcome regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified EM-type algorithm to estimate semi-parametric
mixtures of non-parametric regressions. <em>SAC</em>, <em>34</em>(4),
1–22. (<a href="https://doi.org/10.1007/s11222-024-10435-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-parametric Gaussian mixtures of non-parametric regressions (SPGMNRs) are a flexible extension of Gaussian mixtures of linear regressions (GMLRs). The model assumes that the component regression functions (CRFs) are non-parametric functions of the covariate(s) whereas the component mixing proportions and variances are constants. Unfortunately, the model cannot be reliably estimated using traditional methods. A local-likelihood approach for estimating the CRFs requires that we maximize a set of local-likelihood functions. Using the Expectation-Maximization (EM) algorithm to separately maximize each local-likelihood function may lead to label-switching. This is because the posterior probabilities calculated at the local E-step are not guaranteed to be aligned. The consequence of this label-switching is wiggly and non-smooth estimates of the CRFs. In this paper, we propose a unified approach to address label-switching and obtain sensible estimates. The proposed approach has two stages. In the first stage, we propose a model-based approach to address the label-switching problem. We first note that each local-likelihood function is a likelihood function of a Gaussian mixture model (GMM). Next, we reformulate the SPGMNRs model as a mixture of these GMMs. Lastly, using a modified version of the Expectation Conditional Maximization (ECM) algorithm, we estimate the mixture of GMMs. In addition, using the mixing weights of the local GMMs, we can automatically choose the local points where local-likelihood estimation takes place. In the second stage, we propose one-step backfitting estimates of the parametric and non-parametric terms. The effectiveness of the proposed approach is demonstrated on simulated data and real data analysis.},
  archive      = {J_SAC},
  author       = {Skhosana, Sphiwe B. and Millard, Salomon M. and Kanfer, Frans H. J.},
  doi          = {10.1007/s11222-024-10435-3},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {A modified EM-type algorithm to estimate semi-parametric mixtures of non-parametric regressions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group sparse structural smoothing recovery: Model,
statistical properties and algorithm. <em>SAC</em>, <em>34</em>(4),
1–21. (<a href="https://doi.org/10.1007/s11222-024-10438-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an innovative group sparse structural smoothing recovery model and explore its statistical properties. Specifically, the nonconvex group norm and a new inner-group total variation regularizer are established to capture the group and smooth structures of variables. Under a regularization condition, we analyze the statistical properties of the proposed model, including the global recovery bound for estimation error and prediction error. To tackle the proposed nonconvex and nonsmooth problem, an efficient Bregman alternative direction method of multipliers (Bregman ADMM) is designed. We demonstrate that the sequence produced by the proposed Bregman ADMM converges to a stationary point under certain assumptions. Extensive numerical experiments and the application of comparative genomic hybridization data and large-scale breast cancer gene expression data demonstrate that the proposed method improves estimation accuracy and produces better sparsity and smoothness than other group variable selection models.},
  archive      = {J_SAC},
  author       = {Tan, Zuoxun and Yang, Hu},
  doi          = {10.1007/s11222-024-10438-0},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Group sparse structural smoothing recovery: Model, statistical properties and algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multipe imputation with GAM. <em>SAC</em>,
<em>34</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10429-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation of missing values is a key step in data analytics and a standard process in data science. Nonlinear imputation methods come into play whenever the linear relationship between a response and predictors cannot be linearized by transformations of variables, adding interactions, or using, e.g., quadratic terms. Generalized additive models (GAM) and its extension, GAMLSS—where each parameter of the distribution, such as mean, variance, skewness, and kurtosis, can be represented as a function of predictors, are widely used nonlinear methods. However, non-robust methods such as standard GAM’s and GAMLSS’s can be swayed by outliers, leading to outlier-driven imputations. This can apply concerning both representative outliers—those true yet unusual values of your population—and non-representative outliers, which are mere measurement errors. Robust (imputation) methods effectively manage outliers and exhibit resistance to their influence, providing a more reliable approach to dealing with missing data. The innovative solution of the proposed new imputation algorithm tackles three major challenges related to robustness. (1) A robust bootstrap method is employed to handle model uncertainty during the imputation of a random sample. (2) The approach incorporates robust fitting techniques to enhance accuracy. (3) It effectively considers imputation uncertainty in a resilient manner. Furthermore, any complex model for any variable with missingness can be considered and run through the algorithm. For the real-world data sets used and the simulation study conducted, the novel algorithm imputeRobust which includes robust methods for imputation with GAM’s demonstrates superior performance compared to existing imputation methods using GAMLSS. Limitations pertain to the imputation of categorical variables using robust techniques.},
  archive      = {J_SAC},
  author       = {Templ, Matthias},
  doi          = {10.1007/s11222-024-10429-1},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Robust multipe imputation with GAM},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Bias-reduced and variance-corrected asymptotic gaussian
inference about extreme expectiles. <em>SAC</em>, <em>34</em>(4), 1–73.
(<a href="https://doi.org/10.1007/s11222-023-10359-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expectile is a prime candidate for being a standard risk measure in actuarial and financial contexts, for its ability to recover information about probabilities and typical behavior of extreme values, as well as its excellent axiomatic properties. A series of recent papers has focused on expectile estimation at extreme levels, with a view on gathering essential information about low-probability, high-impact events that are of most interest to risk managers. The obtention of accurate confidence intervals for extreme expectiles is paramount in any decision process in which they are involved, but actual inference on these tail risk measures is still a difficult question due to their least squares nature and sensitivity to tail heaviness. This article focuses on asymptotic Gaussian inference about tail expectiles in the challenging context of heavy-tailed observations. We use an in-depth analysis of the proofs of asymptotic normality results for two classes of extreme expectile estimators to derive bias-reduced and variance-corrected Gaussian confidence intervals. These, unlike previous attempts in the literature, are well-rooted in statistical theory and can accommodate underlying distributions that display a wide range of tail behaviors. A large-scale simulation study and three real data analyses confirm the versatility of the proposed technique.},
  archive      = {J_SAC},
  author       = {Daouia, Abdelaati and Stupfler, Gilles and Usseglio-Carleve, Antoine},
  doi          = {10.1007/s11222-023-10359-4},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-73},
  shortjournal = {Stat. Comput.},
  title        = {Bias-reduced and variance-corrected asymptotic gaussian inference about extreme expectiles},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient estimation and correction of selection-induced
bias with order statistics. <em>SAC</em>, <em>34</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10442-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection aims to identify a sufficiently well performing model that is possibly simpler than the most complex model among a pool of candidates. However, the decision-making process itself can inadvertently introduce non-negligible bias when the cross-validation estimates of predictive performance are marred by excessive noise. In finite data regimes, cross-validated estimates can encourage the statistician to select one model over another when it is not actually better for future data. While this bias remains negligible in the case of few models, when the pool of candidates grows, and model selection decisions are compounded (as in step-wise selection), the expected magnitude of selection-induced bias is likely to grow too. This paper introduces an efficient approach to estimate and correct selection-induced bias based on order statistics. Numerical experiments demonstrate the reliability of our approach in estimating both selection-induced bias and over-fitting along compounded model selection decisions, with specific application to forward search. This work represents a light-weight alternative to more computationally expensive approaches to correcting selection-induced bias, such as nested cross-validation and the bootstrap. Our approach rests on several theoretic assumptions, and we provide a diagnostic to help understand when these may not be valid and when to fall back on safer, albeit more computationally expensive approaches. The accompanying code facilitates its practical implementation and fosters further exploration in this area.},
  archive      = {J_SAC},
  author       = {McLatchie, Yann and Vehtari, Aki},
  doi          = {10.1007/s11222-024-10442-4},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Efficient estimation and correction of selection-induced bias with order statistics},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based clustering with missing not at random data.
<em>SAC</em>, <em>34</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s11222-024-10444-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Sportisse, Aude and Marbac, Matthieu and Laporte, Fabien and Celeux, Gilles and Boyer, Claire and Josse, Julie and Biernacki, Christophe},
  doi          = {10.1007/s11222-024-10444-2},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Model-based clustering with missing not at random data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certified coordinate selection for high-dimensional bayesian
inversion with laplace prior. <em>SAC</em>, <em>34</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10445-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider high-dimensional Bayesian inverse problems with arbitrary likelihood and product-form Laplace prior for which we provide a certified approximation of the posterior in the Hellinger distance. The approximate posterior differs from the prior only in a small number of relevant coordinates that contribute the most to the update from the prior to the posterior. We propose and analyze a gradient-based diagnostic to identify these relevant coordinates. Although this diagnostic requires computing an expectation with respect to the posterior, we propose tractable methods for the classical case of a linear forward model with Gaussian likelihood. Our methods can be employed to estimate the diagnostic before solving the Bayesian inverse problem via, e.g., Markov chain Monte Carlo (MCMC) methods. After selecting the coordinates, the approximate posterior can be efficiently inferred since most of its coordinates are only informed by the prior. Moreover, specialized MCMC methods, such as the pseudo-marginal MCMC algorithm, can be used to obtain less correlated samples when sampling the exact posterior. We show the applicability of our method using a 1D signal deblurring problem and a high-dimensional 2D super-resolution problem.},
  archive      = {J_SAC},
  author       = {Flock, Rafael and Dong, Yiqiu and Uribe, Felipe and Zahm, Olivier},
  doi          = {10.1007/s11222-024-10445-1},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Certified coordinate selection for high-dimensional bayesian inversion with laplace prior},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian multilevel model for populations of networks
using exponential-family random graphs. <em>SAC</em>, <em>34</em>(4),
1–15. (<a href="https://doi.org/10.1007/s11222-024-10446-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection of data on populations of networks is becoming increasingly common, where each data point can be seen as a realisation of a network-valued random variable. Moreover, each data point may be accompanied by some additional covariate information and one may be interested in assessing the effect of these covariates on network structure within the population. A canonical example is that of brain networks: a typical neuroimaging study collects one or more brain scans across multiple individuals, each of which can be modelled as a network with nodes corresponding to distinct brain regions and edges corresponding to structural or functional connections between these regions. Most statistical network models, however, were originally proposed to describe a single underlying relational structure, although recent years have seen a drive to extend these models to populations of networks. Here, we describe a model for when the outcome of interest is a network-valued random variable whose distribution is given by an exponential random graph model. To perform inference, we implement an exchange-within-Gibbs MCMC algorithm that generates samples from the doubly-intractable posterior. To illustrate this approach, we use it to assess population-level variations in networks derived from fMRI scans, enabling the inference of age- and intelligence-related differences in the topological structure of the brain’s functional connectivity.},
  archive      = {J_SAC},
  author       = {Lehmann, Brieuc and White, Simon},
  doi          = {10.1007/s11222-024-10446-0},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian multilevel model for populations of networks using exponential-family random graphs},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounded-memory adjusted scores estimation in generalized
linear models with large data sets. <em>SAC</em>, <em>34</em>(4), 1–12.
(<a href="https://doi.org/10.1007/s11222-024-10447-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Zietkiewicz, Patrick and Kosmidis, Ioannis},
  doi          = {10.1007/s11222-024-10447-z},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Bounded-memory adjusted scores estimation in generalized linear models with large data sets},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient workflow for modelling high-dimensional spatial
extremes. <em>SAC</em>, <em>34</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10448-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a comprehensive methodological workflow for Bayesian modelling of high-dimensional spatial extremes that lets us describe both weakening extremal dependence at increasing levels and changes in the type of extremal dependence class as a function of the distance between locations. This is achieved with a latent Gaussian version of the spatial conditional extremes model that allows for computationally efficient inference with R-INLA. Inference is made more robust using a post hoc adjustment method that accounts for possible model misspecification. This added robustness makes it possible to extract more information from the available data during inference using a composite likelihood. The developed methodology is applied to the modelling of extreme hourly precipitation from high-resolution radar data in Norway. Inference is performed quickly, and the resulting model fit successfully captures the main trends in the extremal dependence structure of the data. The post hoc adjustment is found to further improve model performance.},
  archive      = {J_SAC},
  author       = {Vandeskog, Silius M. and Martino, Sara and Huser, Raphaël},
  doi          = {10.1007/s11222-024-10448-y},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {An efficient workflow for modelling high-dimensional spatial extremes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Poisson subsampling-based estimation for
growing-dimensional expectile regression in massive data. <em>SAC</em>,
<em>34</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10449-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective tool for data analysis, expectile regression is widely used in the fields of statistics, econometrics and finance. However, most studies focus on the case where the sample size is not massive and the dimension is low or fixed. This paper studies the parameter estimation and inference for large-scale expectile regression when the number of parameters grows to infinity. Specifically, an inverse probability weighted asymmetric least squares estimator based on Poisson subsampling (ALS-P) is proposed. Theoretically, the convergence rate and asymptotic normality for ALS-P are established. Furthermore, the optimal subsampling probabilities based on the L-optimality criterion are derived. Finally, extensive simulations and two real-world datasets are conducted to illustrate the effectiveness of the proposed methods.},
  archive      = {J_SAC},
  author       = {Li, Xiaoyan and Xia, Xiaochao and Zhang, Zhimin},
  doi          = {10.1007/s11222-024-10449-x},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Poisson subsampling-based estimation for growing-dimensional expectile regression in massive data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient method to simulate diffusion bridges.
<em>SAC</em>, <em>34</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10439-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide a unified approach to simulate diffusion bridges. The proposed method covers a wide range of processes including univariate and multivariate diffusions, and the diffusions can be either time-homogeneous or time-inhomogeneous. We provide a theoretical framework for the proposed method. In particular, using the parametrix representations we show that the approximated probability transition density function converges to that of the true diffusion, which in turn implies the convergence of the approximation. Unlike most of the methods proposed in the literature, our approach does not involve acceptance-rejection mechanics. That is, it is acceptance-rejection free. Extensive numerical examples are provided for illustration and demonstrate the accuracy of the proposed method.},
  archive      = {J_SAC},
  author       = {Chau, H. and Kirkby, J. L. and Nguyen, D. H. and Nguyen, D. and Nguyen, N. and Nguyen, T.},
  doi          = {10.1007/s11222-024-10439-z},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {An efficient method to simulate diffusion bridges},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on the adaptive-ridge algorithm with several
extensions. <em>SAC</em>, <em>34</em>(4), 1–36. (<a
href="https://doi.org/10.1007/s11222-024-10440-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Adaptive Ridge Algorithm is an iterative algorithm designed for variable selection. It is also known under the denomination of Iteratively Reweighted Least-Squares Algorithm in the communities of Compressed Sensing and Sparse Signals Recovery. Besides, it can also be interpreted as an optimization algorithm dedicated to the minimization of possibly nonconvex $$\ell ^q$$ penalized energies (with $$0&lt;q&lt;2$$ ). In the literature, this algorithm can be derived using various mathematical approaches, namely Half Quadratic Minimization, Majorization-Minimization, Alternating Minimization or Local Approximations. In this work, we will show how the Adaptive Ridge Algorithm can be simply derived and analyzed from a single equation, corresponding to a variational reformulation of the $$\ell ^q$$ penalty. We will describe in detail how the Adaptive Ridge Algorithm can be numerically implemented and we will perform a thorough experimental study of its parameters. We will also show how the variational formulation of the $$\ell ^q$$ penalty combined with modern duality principles can be used to design an interesting variant of the Adaptive Ridge Algorithm dedicated to the minimization of quadratic functions over (nonconvex) $$\ell ^q$$ balls.},
  archive      = {J_SAC},
  author       = {Abergel, Rémy and Bouaziz, Olivier and Nuel, Grégory},
  doi          = {10.1007/s11222-024-10440-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-36},
  shortjournal = {Stat. Comput.},
  title        = {A review on the adaptive-ridge algorithm with several extensions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Greedy recursive spectral bisection for modularity-bound
hierarchical divisive community detection. <em>SAC</em>, <em>34</em>(4),
1–18. (<a href="https://doi.org/10.1007/s11222-024-10451-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering techniques depend on the eigenstructure of a similarity matrix to assign data points to clusters, so that points within the same cluster exhibit high similarity and are compared to those in different clusters. This work aimed to develop a spectral method that could be compared to clustering algorithms that represent the current state of the art. This investigation conceived a novel spectral clustering method, as well as five policies that guide its execution, based on spectral graph theory and embodying hierarchical clustering principles. Computational experiments comparing the proposed method with six state-of-the-art algorithms were undertaken in this study to evaluate the clustering methods under scrutiny. The assessment was performed using two evaluation metrics, specifically the adjusted Rand index, and modularity. The obtained results furnish compelling evidence, indicating that the proposed method is competitive and possesses distinctive properties compared to those elucidated in the existing literature. This suggests that our approach stands as a viable alternative, offering a robust choice within the spectrum of available same-purpose tools.},
  archive      = {J_SAC},
  author       = {Cardoso, Douglas O. and da Silva Junior, João Domingos Gomes and Oliveira, Carla Silva and Marques, Celso and de Assis, Laura Silva},
  doi          = {10.1007/s11222-024-10451-3},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Greedy recursive spectral bisection for modularity-bound hierarchical divisive community detection},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian processes for bayesian inverse problems associated
with linear partial differential equations. <em>SAC</em>,
<em>34</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s11222-024-10452-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with the use of Gaussian surrogate models for Bayesian inverse problems associated with linear partial differential equations. A particular focus is on the regime where only a small amount of training data is available. In this regime the type of Gaussian prior used is of critical importance with respect to how well the surrogate model will perform in terms of Bayesian inversion. We extend the framework of Raissi et. al. (2017) to construct PDE-informed Gaussian priors that we then use to construct different approximate posteriors. A number of different numerical experiments illustrate the superiority of the PDE-informed Gaussian priors over more traditional priors.},
  archive      = {J_SAC},
  author       = {Bai, Tianming and Teckentrup, Aretha L. and Zygalakis, Konstantinos C.},
  doi          = {10.1007/s11222-024-10452-2},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Gaussian processes for bayesian inverse problems associated with linear partial differential equations},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible bayesian quantile regression based on the
generalized asymmetric huberised-type distribution. <em>SAC</em>,
<em>34</em>(4), 1–35. (<a
href="https://doi.org/10.1007/s11222-024-10453-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the robustness and flexibility of Bayesian quantile regression models using the asymmetric Laplace or asymmetric Huberised-type (AH) distribution, which lacks changeable mode, diminishing influence of outliers, and asymmetry under median regression, we propose a new generalized AH distribution which is achieved through a hierarchical mixture representation, thus leading to a flexible Bayesian Huberised quantile regression framework. With many parameters in the model, we develop an efficient Markov chain Monte Carlo procedure based on the Metropolis-within-Gibbs sampling algorithm. The robustness and flexibility of the new distribution are examined through intensive simulation studies and application to two real data sets.},
  archive      = {J_SAC},
  author       = {Hu, Weitao and Zhang, Weiping},
  doi          = {10.1007/s11222-024-10453-1},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Stat. Comput.},
  title        = {Flexible bayesian quantile regression based on the generalized asymmetric huberised-type distribution},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structured prior distributions for the covariance matrix in
latent factor models. <em>SAC</em>, <em>34</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10454-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor models are widely used for dimension reduction in the analysis of multivariate data. This is achieved through decomposition of a $$p \times p$$ covariance matrix into the sum of two components. Through a latent factor representation, they can be interpreted as a diagonal matrix of idiosyncratic variances and a shared variation matrix, that is, the product of a $$p \times k$$ factor loadings matrix and its transpose. If $$k \ll p$$ , this defines a parsimonious factorisation of the covariance matrix. Historically, little attention has been paid to incorporating prior information in Bayesian analyses using factor models where, at best, the prior for the factor loadings is order invariant. In this work, a class of structured priors is developed that can encode ideas of dependence structure about the shared variation matrix. The construction allows data-informed shrinkage towards sensible parametric structures while also facilitating inference over the number of factors. Using an unconstrained reparameterisation of stationary vector autoregressions, the methodology is extended to stationary dynamic factor models. For computational inference, parameter-expanded Markov chain Monte Carlo samplers are proposed, including an efficient adaptive Gibbs sampler. Two substantive applications showcase the scope of the methodology and its inferential benefits.},
  archive      = {J_SAC},
  author       = {Heaps, Sarah Elizabeth and Jermyn, Ian Hyla},
  doi          = {10.1007/s11222-024-10454-0},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Structured prior distributions for the covariance matrix in latent factor models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cure rate analysis through integration of machine
learning models: A comparative study. <em>SAC</em>, <em>34</em>(4),
1–13. (<a href="https://doi.org/10.1007/s11222-024-10456-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cure rate models have been thoroughly investigated across various domains, encompassing medicine, reliability, and finance. The merging of machine learning (ML) with cure models is emerging as a promising strategy to improve predictive accuracy and gain profound insights into the underlying mechanisms influencing the probability of cure. The current body of literature has explored the benefits of incorporating a single ML algorithm with cure models. However, there is a notable absence of a comprehensive study that compares the performances of various ML algorithms in this context. This paper seeks to address and bridge this gap. Specifically, we focus on the well-known mixture cure model and examine the incorporation of five distinct ML algorithms: extreme gradient boosting, neural networks, support vector machines, random forests, and decision trees. To bolster the robustness of our comparison, we also include cure models with logistic and spline-based regression. For parameter estimation, we formulate an expectation maximization algorithm. A comprehensive simulation study is conducted across diverse scenarios to compare various models based on the accuracy and precision of estimates for different quantities of interest, along with the predictive accuracy of cure. The results derived from both the simulation study, as well as the analysis of real cutaneous melanoma data, indicate that the incorporation of ML models into cure model provides a beneficial contribution to the ongoing endeavors aimed at improving the accuracy of cure rate estimation.},
  archive      = {J_SAC},
  author       = {Aselisewine, Wisdom and Pal, Suvra},
  doi          = {10.1007/s11222-024-10456-y},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing cure rate analysis through integration of machine learning models: A comparative study},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Birnbaum–saunders frailty regression models for clustered
survival data. <em>SAC</em>, <em>34</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10458-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel frailty model for modeling clustered survival data. In particular, we consider the Birnbaum–Saunders (BS) distribution for the frailty terms with a new directly parameterized on the variance of the frailty distribution. This allows, among other things, compare the estimated frailty terms among traditional models, such as the gamma frailty model. Some mathematical properties of the new model are studied including the conditional distribution of frailties among the survivors, the frailty of individuals dying at time t, and the Kendall’s $$\tau $$ measure. Furthermore, an explicit form to the derivatives of the Laplace transform for the BS distribution using the di Bruno’s formula is found. Parametric, non-parametric and semiparametric versions of the BS frailty model are studied. We use a simple Expectation-Maximization (EM) algorithm to estimate the model parameters and evaluate its performance under different censoring proportion by a Monte Carlo simulation study. We also show that the BS frailty model is competitive over the gamma and weighted Lindley frailty models under misspecification. We illustrate our methodology by using a real data sets.},
  archive      = {J_SAC},
  author       = {Gallardo, Diego I. and Bourguignon, Marcelo and Romeo, José S.},
  doi          = {10.1007/s11222-024-10458-w},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Birnbaum–Saunders frailty regression models for clustered survival data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parsimonious consensus hierarchies, partitions and fuzzy
partitioning of a set of hierarchies. <em>SAC</em>, <em>34</em>(3),
1–19. (<a href="https://doi.org/10.1007/s11222-024-10415-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methodology is described for fitting a fuzzy partition and a parsimonious consensus hierarchy (ultrametric matrix) to a set of hierarchies of the same set of objects. A model defining a fuzzy partition of a set of hierarchical classifications, with every class of the partition synthesized by a parsimonious consensus hierarchy is described. Each consensus includes an optimal consensus hard partition of objects and all the hierarchical agglomerative aggregations among the clusters of the consensus partition. The performances of the methodology are illustrated by an extended simulation study and applications to real data. A discussion is provided on the new methodology and some interesting future developments are described.},
  archive      = {J_SAC},
  author       = {Bombelli, Ilaria and Vichi, Maurizio},
  doi          = {10.1007/s11222-024-10415-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Parsimonious consensus hierarchies, partitions and fuzzy partitioning of a set of hierarchies},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parsimonious ultrametric gaussian mixture models.
<em>SAC</em>, <em>34</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11222-024-10405-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian mixture models represent a conceptually and mathematically elegant class of models for casting the density of a heterogeneous population where the observed data is collected from a population composed of a finite set of G homogeneous subpopulations with a Gaussian distribution. A limitation of these models is that they suffer from the curse of dimensionality, and the number of parameters becomes easily extremely large in the presence of high-dimensional data. In this paper, we propose a class of parsimonious Gaussian mixture models with constrained extended ultrametric covariance structures that are capable of exploring hierarchical relations among variables. The proposal shows to require a reduced number of parameters to be fit and includes constrained covariance structures across and within components that further reduce the number of parameters of the model.},
  archive      = {J_SAC},
  author       = {Cavicchia, Carlo and Vichi, Maurizio and Zaccaria, Giorgia},
  doi          = {10.1007/s11222-024-10405-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Parsimonious ultrametric gaussian mixture models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional mixtures-of-experts. <em>SAC</em>,
<em>34</em>(3), 1–31. (<a
href="https://doi.org/10.1007/s11222-023-10379-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the statistical analysis of heterogeneous data for prediction, in situations where the observations include functions, typically time series. We extend the modeling with mixtures-of-experts (ME), as a framework of choice in modeling heterogeneity in data for prediction with vectorial observations, to this functional data analysis context. We first present a new family of ME models, named functional ME (FME), in which the predictors are potentially noisy observations, from entire functions. Furthermore, the data generating process of the predictor and the real response, is governed by a hidden discrete variable representing an unknown partition. Second, by imposing sparsity on derivatives of the underlying functional parameters via Lasso-like regularizations, we provide sparse and interpretable functional representations of the FME models called iFME. We develop dedicated expectation–maximization algorithms for Lasso-like regularized maximum-likelihood parameter estimation strategies to fit the models. The proposed models and algorithms are studied in simulated scenarios and in applications to two real data sets, and the obtained results demonstrate their performance in accurately capturing complex nonlinear relationships and in clustering the heterogeneous regression data.},
  archive      = {J_SAC},
  author       = {Chamroukhi, Faïcel and Pham, Nhat Thien and Hoang, Van Hà and McLachlan, Geoffrey J.},
  doi          = {10.1007/s11222-023-10379-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Functional mixtures-of-experts},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Screen then select: A strategy for correlated predictors in
high-dimensional quantile regression. <em>SAC</em>, <em>34</em>(3),
1–31. (<a href="https://doi.org/10.1007/s11222-024-10424-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong correlation among predictors and heavy-tailed noises pose a great challenge in the analysis of ultra-high dimensional data. Such challenge leads to an increase in the computation time for discovering active variables and a decrease in selection accuracy. To address this issue, we propose an innovative two-stage screen-then-select approach and its derivative procedure based on a robust quantile regression with sparsity assumption. This approach initially screens important features by ranking quantile ridge estimation and subsequently employs a likelihood-based post-screening selection strategy to refine variable selection. Additionally, we conduct an internal competition mechanism along the greedy search path to enhance the robustness of algorithm against the design dependence. Our methods are simple to implement and possess numerous desirable properties from theoretical and computational standpoints. Theoretically, we establish the strong consistency of feature selection for the proposed methods under some regularity conditions. In empirical studies, we assess the finite sample performance of our methods by comparing them with utility screening approaches and existing penalized quantile regression methods. Furthermore, we apply our methods to identify genes associated with anticancer drug sensitivities for practical guidance.},
  archive      = {J_SAC},
  author       = {Jiang, Xuejun and Liang, Yakun and Wang, Haofeng},
  doi          = {10.1007/s11222-024-10424-6},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Screen then select: A strategy for correlated predictors in high-dimensional quantile regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general model-checking procedure for semiparametric
accelerated failure time models. <em>SAC</em>, <em>34</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10431-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a set of goodness-of-fit tests for the semiparametric accelerated failure time (AFT) model, including an omnibus test, a link function test, and a functional form test. This set of tests is derived from a multi-parameter cumulative sum process shown to follow asymptotically a zero-mean Gaussian process. Its evaluation is based on the asymptotically equivalent perturbed version, which enables both graphical and numerical evaluations of the assumed AFT model. Empirical p-values are obtained using the Kolmogorov-type supremum test, which provides a reliable approach for estimating the significance of both proposed un-standardized and standardized test statistics. The proposed procedure is illustrated using the rank-based estimator but is general in the sense that it is directly applicable to some other popular estimators such as induced smoothed rank-based estimator or least-squares estimator that satisfies certain properties. Our proposed methods are rigorously evaluated using extensive simulation experiments that demonstrate their effectiveness in maintaining a Type I error rate and detecting departures from the assumed AFT model in practical sample sizes and censoring rates. Furthermore, the proposed approach is applied to the analysis of the Primary Biliary Cirrhosis data, a widely studied dataset in survival analysis, providing further evidence of the practical usefulness of the proposed methods in real-world scenarios. To make the proposed methods more accessible to researchers, we have implemented them in the R package afttest, which is publicly available on the Comprehensive R Archive Network.},
  archive      = {J_SAC},
  author       = {Choi, Dongrak and Bae, Woojung and Yan, Jun and Kang, Sangwook},
  doi          = {10.1007/s11222-024-10431-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {A general model-checking procedure for semiparametric accelerated failure time models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized spherical principal component analysis.
<em>SAC</em>, <em>34</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10413-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers contaminating data sets are a challenge to statistical estimators. Even a small fraction of outlying observations can heavily influence most classical statistical methods. In this paper we propose generalized spherical principal component analysis, a new robust version of principal component analysis that is based on the generalized spatial sign covariance matrix. Theoretical properties of the proposed method including influence functions, breakdown values and asymptotic efficiencies are derived. These theoretical results are complemented with an extensive simulation study and two real-data examples. We illustrate that generalized spherical principal component analysis can combine great robustness with solid efficiency properties, in addition to a low computational cost.},
  archive      = {J_SAC},
  author       = {Leyder, Sarah and Raymaekers, Jakob and Verdonck, Tim},
  doi          = {10.1007/s11222-024-10413-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Generalized spherical principal component analysis},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expectile and m-quantile regression for panel data.
<em>SAC</em>, <em>34</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11222-024-10396-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear fixed effect models are a general way to fit panel or longitudinal data with a distinct intercept for each unit. Based on expectile and M-quantile approaches, we propose alternative regression estimation methods to estimate the parameters of linear fixed effect models. The estimation functions are penalized by the least absolute shrinkage and selection operator to reduce the dimensionality of the data. Some asymptotic properties of the estimators are established, and finite sample size investigations are conducted to verify the empirical performances of the estimation methods. The computational implementations of the procedures are discussed, and real economic panel data from the Organisation for Economic Cooperation and Development are analyzed to show the usefulness of the methods in a practical problem.},
  archive      = {J_SAC},
  author       = {Danilevicz, Ian Meneghel and Reisen, Valdério Anselmo and Bondon, Pascal},
  doi          = {10.1007/s11222-024-10396-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Expectile and M-quantile regression for panel data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection using axis-aligned random projections for
partial least-squares regression. <em>SAC</em>, <em>34</em>(3), 1–11.
(<a href="https://doi.org/10.1007/s11222-024-10417-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional data modeling, variable selection plays a crucial role in improving predictive accuracy and enhancing model interpretability through sparse representation. Unfortunately, certain variable selection methods encounter challenges such as insufficient model sparsity, high computational overhead, and difficulties in handling large-scale data. Recently, axis-aligned random projection techniques have been applied to address these issues by selecting variables. However, these techniques have seen limited application in handling complex data within the regression framework. In this study, we propose a novel method, sparse partial least squares via axis-aligned random projection, designed for the analysis of high-dimensional data. Initially, axis-aligned random projection is utilized to obtain a sparse loading vector, significantly reducing computational complexity. Subsequently, partial least squares regression is conducted within the subspace of the top-ranked significant variables. The submatrices are iteratively updated until an optimal sparse partial least squares model is achieved. Comparative analysis with some state-of-the-art high-dimensional regression methods demonstrates that the proposed method exhibits superior predictive performance. To illustrate its effectiveness, we apply the method to four cases, including one simulated dataset and three real-world datasets. The results show the proposed method’s ability to identify important variables in all four cases.},
  archive      = {J_SAC},
  author       = {Lin, Youwu and Zeng, Xin and Wang, Pei and Huang, Shuai and Teo, Kok Lay},
  doi          = {10.1007/s11222-024-10417-5},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Variable selection using axis-aligned random projections for partial least-squares regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A constant-per-iteration likelihood ratio test for online
changepoint detection for exponential family models. <em>SAC</em>,
<em>34</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11222-024-10416-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online changepoint detection algorithms that are based on (generalised) likelihood-ratio tests have been shown to have excellent statistical properties. However, a simple online implementation is computationally infeasible as, at time T, it involves considering O(T) possible locations for the change. Recently, the FOCuS algorithm has been introduced for detecting changes in mean in Gaussian data that decreases the per-iteration cost to $$O(\log T)$$ . This is possible by using pruning ideas, which reduce the set of changepoint locations that need to be considered at time T to approximately $$\log T$$ . We show that if one wishes to perform the likelihood ratio test for a different one-parameter exponential family model, then exactly the same pruning rule can be used, and again one need only consider approximately $$\log T$$ locations at iteration T. Furthermore, we show how we can adaptively perform the maximisation step of the algorithm so that we need only maximise the test statistic over a small subset of these possible locations. Empirical results show that the resulting online algorithm, which can detect changes under a wide range of models, has a constant-per-iteration cost on average.},
  archive      = {J_SAC},
  author       = {Ward, Kes and Romano, Gaetano and Eckley, Idris and Fearnhead, Paul},
  doi          = {10.1007/s11222-024-10416-6},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {A constant-per-iteration likelihood ratio test for online changepoint detection for exponential family models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An expectile computation cookbook. <em>SAC</em>,
<em>34</em>(3), 1–37. (<a
href="https://doi.org/10.1007/s11222-024-10403-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A substantial body of work in the last 15 years has shown that expectiles constitute an excellent candidate for becoming a standard tool in probabilistic and statistical modeling. Surprisingly, the question of how expectiles may be efficiently calculated has been left largely untouched. We fill this gap by, first, providing a general outlook on the computation of expectiles that relies on the knowledge of analytic expressions of the underlying distribution function and mean residual life function. We distinguish between discrete distributions, for which an exact calculation is always feasible, and continuous distributions, where a Newton–Raphson approximation algorithm can be implemented and a list of exceptional distributions whose expectiles are in analytic form can be given. When the distribution function and/or the mean residual life is difficult to compute, Monte-Carlo algorithms are introduced, based on an exact calculation of sample expectiles and on the use of control variates to improve computational efficiency. We discuss the relevance of our findings to statistical practice and provide numerical evidence of the performance of the considered methods.},
  archive      = {J_SAC},
  author       = {Daouia, Abdelaati and Stupfler, Gilles and Usseglio-Carleve, Antoine},
  doi          = {10.1007/s11222-024-10403-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Stat. Comput.},
  title        = {An expectile computation cookbook},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated generation of initial points for adaptive
rejection sampling of log-concave distributions. <em>SAC</em>,
<em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10425-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive rejection sampling requires that users provide points that span the distribution’s mode. If these points are far from the mode, it significantly increases computational costs. This paper introduces a simple, automated approach for selecting initial points that uses numerical optimization to quickly bracket the mode. When an initial point is given that resides in a high-density area, the method often requires just four function evaluations to draw a sample—just one more than the sampler’s minimum. This feature makes it well-suited for Gibbs sampling, where the previous round’s draw can serve as the starting point.},
  archive      = {J_SAC},
  author       = {James, Jonathan},
  doi          = {10.1007/s11222-024-10425-5},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Automated generation of initial points for adaptive rejection sampling of log-concave distributions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic three-term conjugate gradient method with
variance technique for non-convex learning. <em>SAC</em>,
<em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10409-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training process of machine learning, the minimization of the empirical risk loss function is often used to measure the difference between the model’s predicted value and the real value. Stochastic gradient descent is very popular for this type of optimization problem, but converges slowly in theoretical analysis. To solve this problem, there are already many algorithms with variance reduction techniques, such as SVRG, SAG, SAGA, etc. Some scholars apply the conjugate gradient method in traditional optimization to these algorithms, such as CGVR, SCGA, SCGN, etc., which can basically achieve linear convergence speed, but these conclusions often need to be established under some relatively strong assumptions. In traditional optimization, the conjugate gradient method often requires the use of line search techniques to achieve good experimental results. In a sense, line search embodies some properties of the conjugate methods. Taking inspiration from this, we apply the modified three-term conjugate gradient method and line search technique to machine learning. In our theoretical analysis, we obtain the same convergence rate as SCGA under weaker conditional assumptions. We also test the convergence of our algorithm using two non-convex machine learning models.},
  archive      = {J_SAC},
  author       = {Ouyang, Chen and Lu, Chenkaixiang and Zhao, Xiong and Huang, Ruping and Yuan, Gonglin and Jiang, Yiyan},
  doi          = {10.1007/s11222-024-10409-5},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic three-term conjugate gradient method with variance technique for non-convex learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resampling-based confidence intervals and bands for the
average treatment effect in observational studies with competing risks.
<em>SAC</em>, <em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10420-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The g-formula can be used to estimate the treatment effect while accounting for confounding bias in observational studies. With regard to time-to-event endpoints, possibly subject to competing risks, the construction of valid pointwise confidence intervals and time-simultaneous confidence bands for the causal risk difference is complicated, however. A convenient solution is to approximate the asymptotic distribution of the corresponding stochastic process by means of resampling approaches. In this paper, we consider three different resampling methods, namely the classical nonparametric bootstrap, the influence function equipped with a resampling approach as well as a martingale-based bootstrap version, the so-called wild bootstrap. For the latter, three sub-versions based on differing distributions of the underlying random multipliers are examined. We set up a simulation study to compare the accuracy of the different techniques, which reveals that the wild bootstrap should in general be preferred if the sample size is moderate and sufficient data on the event of interest have been accrued. For illustration, the resampling methods are further applied to data on the long-term survival in patients with early-stage Hodgkin’s disease.},
  archive      = {J_SAC},
  author       = {Rühl, Jasmin and Friedrich, Sarah},
  doi          = {10.1007/s11222-024-10420-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Resampling-based confidence intervals and bands for the average treatment effect in observational studies with competing risks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust estimation of optimal treatment regimes for
survival data using an instrumental variable. <em>SAC</em>,
<em>34</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-024-10407-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival contexts, substantial literature exists on estimating optimal treatment regimes, where treatments are assigned based on personal characteristics to maximize the survival probability. These methods assume that a set of covariates is sufficient to deconfound the treatment-outcome relationship. However, this assumption can be limited in observational studies or randomized trials in which non-adherence occurs. Therefore, we propose a novel approach to estimating optimal treatment regimes when certain confounders are unobservable and a binary instrumental variable is available. Specifically, via a binary instrumental variable, we propose a semiparametric estimator for optimal treatment regimes by maximizing a Kaplan–Meier-like estimator of the survival function. Furthermore, to increase resistance to model misspecification, we construct novel doubly robust estimators. Since the estimators of the survival function are jagged, we incorporate kernel smoothing methods to improve performance. Under appropriate regularity conditions, the asymptotic properties are rigorously established. Moreover, the finite sample performance is evaluated through simulation studies. Finally, we illustrate our method using data from the National Cancer Institute’s prostate, lung, colorectal, and ovarian cancer screening trial.},
  archive      = {J_SAC},
  author       = {Junwen, Xia and Zishu, Zhan and Jingxiao, Zhang},
  doi          = {10.1007/s11222-024-10407-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Doubly robust estimation of optimal treatment regimes for survival data using an instrumental variable},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel sampling method for the von mises–fisher distribution.
<em>SAC</em>, <em>34</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10419-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The von Mises–Fisher distribution is a widely used probability model in directional statistics. An algorithm for generating pseudo-random vectors from this distribution was suggested by Wood (Commun Stat Simul Comput 23(1):157–164, 1994), which is based on a rejection sampling scheme. This paper proposes an alternative to this rejection sampling approach for drawing pseudo-random vectors from arbitrary von Mises–Fisher distributions. A useful mixture representation is derived, which is a mixture of beta distributions with mixing weights that follow a confluent hypergeometric distribution. A condensed table-lookup method is adopted for sampling from the confluent hypergeometric distribution. A theoretical analysis investigates the amount of computation required to construct the condensed lookup table. Through numerical experiments, we demonstrate that the proposed algorithm outperforms the rejection-based method when generating a large number of pseudo-random vectors from the same distribution.},
  archive      = {J_SAC},
  author       = {Kang, Seungwoo and Oh, Hee-Seok},
  doi          = {10.1007/s11222-024-10419-3},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Novel sampling method for the von Mises–Fisher distribution},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible bayesian tool for CoDa mixed models:
Logistic-normal distribution with dirichlet covariance. <em>SAC</em>,
<em>34</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10427-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Data Analysis (CoDa) has gained popularity in recent years. This type of data consists of values from disjoint categories that sum up to a constant. Both Dirichlet regression and logistic-normal regression have become popular as CoDa analysis methods. However, fitting this kind of multivariate models presents challenges, especially when structured random effects are included in the model, such as temporal or spatial effects. To overcome these challenges, we propose the logistic-normal Dirichlet Model (LNDM). We seamlessly incorporate this approach into the R-INLA package, facilitating model fitting and model prediction within the framework of Latent Gaussian Models. Moreover, we explore metrics like Deviance Information Criteria, Watanabe Akaike information criterion, and cross-validation measure conditional predictive ordinate for model selection in R-INLA for CoDa. Illustrating LNDM through two simulated examples and with an ecological case study on Arabidopsis thaliana in the Iberian Peninsula, we underscore its potential as an effective tool for managing CoDa and large CoDa databases.},
  archive      = {J_SAC},
  author       = {Martínez-Minaya, Joaquín and Rue, Haavard},
  doi          = {10.1007/s11222-024-10427-3},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {A flexible bayesian tool for CoDa mixed models: Logistic-normal distribution with dirichlet covariance},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Bayesian high-dimensional covariate selection
in non-linear mixed-effects models using the SAEM algorithm.
<em>SAC</em>, <em>34</em>(3), 1. (<a
href="https://doi.org/10.1007/s11222-024-10421-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Naveau, Marion and King, Guillaume Kon Kam and Rincent, Renaud and Sansonnet, Laure and Delattre, Maud},
  doi          = {10.1007/s11222-024-10421-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: Bayesian high-dimensional covariate selection in non-linear mixed-effects models using the SAEM algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spike and slab bayesian sparse principal component analysis.
<em>SAC</em>, <em>34</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10430-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse principal component analysis (SPCA) is a popular tool for dimensionality reduction in high-dimensional data. However, there is still a lack of theoretically justified Bayesian SPCA methods that can scale well computationally. One of the major challenges in Bayesian SPCA is selecting an appropriate prior for the loadings matrix, considering that principal components are mutually orthogonal. We propose a novel parameter-expanded coordinate ascent variational inference (PX-CAVI) algorithm. This algorithm utilizes a spike and slab prior, which incorporates parameter expansion to cope with the orthogonality constraint. Besides comparing to two popular SPCA approaches, we introduce the PX-EM algorithm as an EM analogue to the PX-CAVI algorithm for comparison. Through extensive numerical simulations, we demonstrate that the PX-CAVI algorithm outperforms these SPCA approaches, showcasing its superiority in terms of performance. We study the posterior contraction rate of the variational posterior, providing a novel contribution to the existing literature. The PX-CAVI algorithm is then applied to study a lung cancer gene expression dataset. The $$\textsf{R}$$ package $$\textsf{VBsparsePCA}$$ with an implementation of the algorithm is available on the Comprehensive R Archive Network (CRAN).},
  archive      = {J_SAC},
  author       = {Ning, Yu-Chien Bo and Ning, Ning},
  doi          = {10.1007/s11222-024-10430-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Spike and slab bayesian sparse principal component analysis},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous estimation and variable selection for a
non-crossing multiple quantile regression using deep neural networks.
<em>SAC</em>, <em>34</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10418-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the DNN-NMQR estimator, an approach that utilizes a deep neural network structure to solve multiple quantile regression problems. When estimating multiple quantiles, our approach leverages the structural characteristics of DNN to enhance estimation results by encouraging shared learning across different quantiles through DNN-NMQR. Also, this method effectively addresses quantile crossing issues through the penalization method. To refine our methodology, we introduce a convolution-type quadratic smoothing function, ensuring that the objective function remains differentiable throughout. Furthermore, we provide a brief discussion on the convergence analysis of DNN-NMQR, drawing on the concept of the neural tangent kernel. For a high-dimensional case, we propose the (A)GDNN-NMQR estimator, which applies group-wise $$L_1$$ -type regularization methods and enjoys the advantages of quantile estimation and variable selection simultaneously. We extensively validate all of our proposed methods through numerical experiments and real data analysis.},
  archive      = {J_SAC},
  author       = {Shin, Jungmin and Gwak, Seunghyun and Shin, Seung Jun and Bang, Sungwan},
  doi          = {10.1007/s11222-024-10418-4},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Simultaneous estimation and variable selection for a non-crossing multiple quantile regression using deep neural networks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A communication-efficient, online changepoint detection
method for monitoring distributed sensor networks. <em>SAC</em>,
<em>34</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10428-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud. We propose an online, communication-efficient method to detect such changes. The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively. We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes. Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs.},
  archive      = {J_SAC},
  author       = {Yang, Ziyang and Eckley, Idris A. and Fearnhead, Paul},
  doi          = {10.1007/s11222-024-10428-2},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A communication-efficient, online changepoint detection method for monitoring distributed sensor networks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving model choice in classification: An approach based
on clustering of covariance matrices. <em>SAC</em>, <em>34</em>(3),
1–21. (<a href="https://doi.org/10.1007/s11222-024-10410-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a refinement of the Parsimonious Model for fitting a Gaussian Mixture. The improvement is based on the consideration of clusters of the involved covariance matrices according to a criterion, such as sharing Principal Directions. This and other similarity criteria that arise from the spectral decomposition of a matrix are the bases of the Parsimonious Model. We show that such groupings of covariance matrices can be achieved through simple modifications of the CEM (Classification Expectation Maximization) algorithm. Our approach leads to propose Gaussian Mixture Models for model-based clustering and discriminant analysis, in which covariance matrices are clustered according to a parsimonious criterion, creating intermediate steps between the fourteen widely known parsimonious models. The added versatility not only allows us to obtain models with fewer parameters for fitting the data, but also provides greater interpretability. We show its usefulness for model-based clustering and discriminant analysis, providing algorithms to find approximate solutions verifying suitable size, shape and orientation constraints, and applying them to both simulation and real data examples.},
  archive      = {J_SAC},
  author       = {Rodríguez-Vítores, David and Matrán, Carlos},
  doi          = {10.1007/s11222-024-10410-y},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Improving model choice in classification: An approach based on clustering of covariance matrices},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reversed particle filtering for hidden markov models.
<em>SAC</em>, <em>34</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10426-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to selecting the distributions in sampling-resampling which improves the efficiency of the weighted bootstrap. To complement the standard scheme of sampling from the prior and reweighting with the likelihood, we introduce a reversed scheme, which samples from the (normalized) likelihood and reweights with the prior. We begin with some motivating examples, before developing the relevant theory. We then apply the approach to the particle filtering of time series, including nonlinear and non-Gaussian Bayesian state-space models, a task that demands efficiency, given the repeated application of the weighted bootstrap. Through simulation studies on a normal dynamic linear model, Poisson hidden Markov model, and stochastic volatility model, we demonstrate the gains in efficiency obtained by the approach, involving the choice of the standard or reversed filter. In addition, for the stochastic volatility model, we provide three real-data examples, including a comparison with importance sampling methods that attempt to incorporate information about the data indirectly into the standard filtering scheme and an extension to multivariate models. We determine that the reversed filtering scheme offers an advantage over such auxiliary methods owing to its ability to incorporate information about the data directly into the sampling, an ability that further facilitates its performance in higher-dimensional settings.},
  archive      = {J_SAC},
  author       = {Rotiroti, Frank and Walker, Stephen G.},
  doi          = {10.1007/s11222-024-10426-4},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Reversed particle filtering for hidden markov models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix regression heterogeneity analysis. <em>SAC</em>,
<em>34</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10401-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of modern science and technology has facilitated the collection of a large amount of matrix data in fields such as biomedicine. Matrix data modeling has been extensively studied, which advances from the naive approach of flattening the matrix into a vector. However, existing matrix modeling methods mainly focus on homogeneous data, failing to handle the data heterogeneity frequently encountered in the biomedical field, where samples from the same study belong to several underlying subgroups, and different subgroups follow different models. In this paper, we focus on regression-based heterogeneity analysis. We propose a matrix data heterogeneity analysis framework, by combining matrix bilinear sparse decomposition and penalized fusion techniques, which enables data-driven subgroup detection, including determining the number of subgroups and subgrouping membership. A rigorous theoretical analysis is conducted, including asymptotic consistency in terms of subgroup detection, the number of subgroups, and regression coefficients. Numerous numerical studies based on simulated and real data have been constructed, showcasing the superior performance of the proposed method in analyzing matrix heterogeneous data.},
  archive      = {J_SAC},
  author       = {Zhang, Fengchuan and Zhang, Sanguo and Li, Shi-Ming and Ren, Mingyang},
  doi          = {10.1007/s11222-024-10401-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Matrix regression heterogeneity analysis},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). R-VGAL: A sequential variational bayes algorithm for
generalised linear mixed models. <em>SAC</em>, <em>34</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10422-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models with random effects, such as generalised linear mixed models (GLMMs), are often used for analysing clustered data. Parameter inference with these models is difficult because of the presence of cluster-specific random effects, which must be integrated out when evaluating the likelihood function. Here, we propose a sequential variational Bayes algorithm, called Recursive Variational Gaussian Approximation for Latent variable models (R-VGAL), for estimating parameters in GLMMs. The R-VGAL algorithm operates on the data sequentially, requires only a single pass through the data, and can provide parameter updates as new data are collected without the need of re-processing the previous data. At each update, the R-VGAL algorithm requires the gradient and Hessian of a “partial” log-likelihood function evaluated at the new observation, which are generally not available in closed form for GLMMs. To circumvent this issue, we propose using an importance-sampling-based approach for estimating the gradient and Hessian via Fisher’s and Louis’ identities. We find that R-VGAL can be unstable when traversing the first few data points, but that this issue can be mitigated by introducing a damping factor in the initial steps of the algorithm. Through illustrations on both simulated and real datasets, we show that R-VGAL provides good approximations to posterior distributions, that it can be made robust through damping, and that it is computationally efficient.},
  archive      = {J_SAC},
  author       = {Vu, Bao Anh and Gunawan, David and Zammit-Mangion, Andrew},
  doi          = {10.1007/s11222-024-10422-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {R-VGAL: A sequential variational bayes algorithm for generalised linear mixed models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering longitudinal ordinal data via finite mixture of
matrix-variate distributions. <em>SAC</em>, <em>34</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10390-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social sciences, studies are often based on questionnaires asking participants to express ordered responses several times over a study period. We present a model-based clustering algorithm for such longitudinal ordinal data. Assuming that an ordinal variable is the discretization of an underlying latent continuous variable, the model relies on a mixture of matrix-variate normal distributions, accounting simultaneously for within- and between-time dependence structures. The model is thus able to concurrently model the heterogeneity, the association among the responses and the temporal dependence structure. An EM algorithm is developed and presented for parameters estimation, and approaches to deal with some arising computational challenges are outlined. An evaluation of the model through synthetic data shows its estimation abilities and its advantages when compared to competitors. A real-world application concerning changes in eating behaviors during the Covid-19 pandemic period in France will be presented.},
  archive      = {J_SAC},
  author       = {Amato, Francesco and Jacques, Julien and Prim-Allaz, Isabelle},
  doi          = {10.1007/s11222-024-10390-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Clustering longitudinal ordinal data via finite mixture of matrix-variate distributions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum likelihood estimation of log-concave densities on
tree space. <em>SAC</em>, <em>34</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-024-10400-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic trees are key data objects in biology, and the method of phylogenetic reconstruction has been highly developed. The space of phylogenetic trees is a nonpositively curved metric space. Recently, statistical methods to analyze samples of trees on this space are being developed utilizing this property. Meanwhile, in Euclidean space, the log-concave maximum likelihood method has emerged as a new nonparametric method for probability density estimation. In this paper, we derive a sufficient condition for the existence and uniqueness of the log-concave maximum likelihood estimator on tree space. We also propose an estimation algorithm for one and two dimensions. Since various factors affect the inferred trees, it is difficult to specify the distribution of a sample of trees. The class of log-concave densities is nonparametric, and yet the estimation can be conducted by the maximum likelihood method without selecting hyperparameters. We compare the estimation performance with a previously developed kernel density estimator numerically. In our examples where the true density is log-concave, we demonstrate that our estimator has a smaller integrated squared error when the sample size is large. We also conduct numerical experiments of clustering using the Expectation-Maximization algorithm and compare the results with k-means++ clustering using Fréchet mean.},
  archive      = {J_SAC},
  author       = {Takazawa, Yuki and Sei, Tomonari},
  doi          = {10.1007/s11222-024-10400-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Maximum likelihood estimation of log-concave densities on tree space},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reliable data-based smoothing parameter selection method
for circular kernel estimation. <em>SAC</em>, <em>34</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11222-024-10384-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new data-based smoothing parameter for circular kernel density (and its derivatives) estimation is proposed. Following the plug-in ideas, unknown quantities on an optimal smoothing parameter are replaced by suitable estimates. This paper provides a circular version of the well-known Sheather and Jones bandwidths (J R Stat Soc Ser B Stat Methodol 53(3):683–690, 1991. https://doi.org/10.1111/j.2517-6161.1991.tb01857.x ), with direct and solve-the-equation plug-in rules. Theoretical support for our developments, related to the asymptotic mean squared error of the estimator of the density, its derivatives, and its functionals, for circular data, are provided. The proposed selectors are compared with previous data-based smoothing parameters for circular kernel density estimation. This paper also contributes to the study of the optimal kernel for circular data. An illustration of the proposed plug-in rules is also shown using real data on the time of car accidents.},
  archive      = {J_SAC},
  author       = {Ameijeiras-Alonso, Jose},
  doi          = {10.1007/s11222-024-10384-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A reliable data-based smoothing parameter selection method for circular kernel estimation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional sparse single–index regression via
hilbert–schmidt independence criterion. <em>SAC</em>, <em>34</em>(2),
1–13. (<a href="https://doi.org/10.1007/s11222-024-10399-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hilbert-Schmidt Independence Criterion (HSIC) has recently been introduced to the field of single-index models to estimate the directions. Compared with other well-established methods, the HSIC based method requires relatively weak conditions. However, its performance has not yet been studied in the prevalent high-dimensional scenarios, where the number of covariates can be much larger than the sample size. In this article, based on HSIC, we propose to estimate the possibly sparse directions in the high-dimensional single-index models through a parameter reformulation. Our approach estimates the subspace of the direction directly and performs variable selection simultaneously. Due to the non-convexity of the objective function and the complexity of the constraints, a majorize-minimize algorithm together with the linearized alternating direction method of multipliers is developed to solve the optimization problem. Since it does not involve the inverse of the covariance matrix, the algorithm can naturally handle large p small n scenarios. Through extensive simulation studies and a real data analysis, we show that our proposal is efficient and effective in the high-dimensional settings. The $$\texttt {Matlab}$$ codes for this method are available online.},
  archive      = {J_SAC},
  author       = {Chen, Xin and Deng, Chang and He, Shuaida and Wu, Runxiong and Zhang, Jia},
  doi          = {10.1007/s11222-024-10399-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional sparse single–index regression via Hilbert–Schmidt independence criterion},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Total effects with constrained features. <em>SAC</em>,
<em>34</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11222-024-10398-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have emphasized the connection between machine learning feature importance measures and total order sensitivity indices (total effects, henceforth). Feature correlations and the need to avoid unrestricted permutations make the estimation of these indices challenging. Additionally, there is no established theory or approach for non-Cartesian domains. We propose four alternative strategies for computing total effects that account for both dependent and constrained features. Our first approach involves a generalized winding stairs design combined with the Knothe-Rosenblatt transformation. This approach, while applicable to a wide family of input dependencies, becomes impractical when inputs are physically constrained. Our second approach is a U-statistic that combines the Jansen estimator with a weighting factor. The U-statistic framework allows the derivation of a central limit theorem for this estimator. However, this design is computationally intensive. Then, our third approach uses derangements to significantly reduce computational burden. We prove consistency and central limit theorems for these estimators as well. Our fourth approach is based on a nearest-neighbour intuition and it further reduces computational burden. We test these estimators through a series of increasingly complex computational experiments with features constrained on compact and connected domains (circle, simplex), non-compact and non-connected domains (Sierpinski gaskets), we provide comparisons with machine learning approaches and conclude with an application to a realistic simulator.},
  archive      = {J_SAC},
  author       = {Borgonovo, Emanuele and Plischke, Elmar and Prieur, Clémentine},
  doi          = {10.1007/s11222-024-10398-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Total effects with constrained features},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum likelihood estimation for discrete latent variable
models via evolutionary algorithms. <em>SAC</em>, <em>34</em>(2), 1–15.
(<a href="https://doi.org/10.1007/s11222-023-10358-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an evolutionary optimization method for maximum likelihood and approximate maximum likelihood estimation of discrete latent variable models. The proposal is based on modified versions of the expectation–maximization (EM) and variational EM (VEM) algorithms, which are based on the genetic approach and allow us to accurately explore the parameter space, reducing the chance to be trapped into one of the multiple local maxima of the log-likelihood function. Their performance is examined through an extensive Monte Carlo simulation study where they are employed to estimate latent class, hidden Markov, and stochastic block models and compared with the standard EM and VEM algorithms. We observe a significant increase in the chance to reach global maximum of the target function and a high accuracy of the estimated parameters for each model. Applications focused on the analysis of cross-sectional, longitudinal, and network data are proposed to illustrate and compare the algorithms.},
  archive      = {J_SAC},
  author       = {Brusa, Luca and Pennoni, Fulvia and Bartolucci, Francesco},
  doi          = {10.1007/s11222-023-10358-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Maximum likelihood estimation for discrete latent variable models via evolutionary algorithms},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantile ratio regression. <em>SAC</em>, <em>34</em>(2),
1–15. (<a href="https://doi.org/10.1007/s11222-024-10406-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce quantile ratio regression. Our proposed model assumes that the ratio of two arbitrary quantiles of a continuous response distribution is a function of a linear predictor. Thanks to basic quantile properties, estimation can be carried out on the scale of either the response or the link function. The advantage of using the latter becomes tangible when implementing fast optimizers for linear regression in the presence of large datasets. We show the theoretical properties of the estimator and derive an efficient method to obtain standard errors. The good performance and merit of our methods are illustrated by means of a simulation study and a real data analysis; where we investigate income inequality in the European Union (EU) using data from a sample of about two million households. We find a significant association between inequality, as measured by quantile ratios, and certain macroeconomic indicators; and we identify countries with outlying income inequality relative to the rest of the EU. An R implementation of the proposed methods is freely available.},
  archive      = {J_SAC},
  author       = {Farcomeni, Alessio and Geraci, Marco},
  doi          = {10.1007/s11222-024-10406-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Quantile ratio regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale correlation screening under dependence for brain
functional connectivity network inference. <em>SAC</em>, <em>34</em>(2),
1–15. (<a href="https://doi.org/10.1007/s11222-024-10411-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data produced by resting-state functional Magnetic Resonance Imaging are widely used to infer brain functional connectivity networks. Such networks correlate neural signals to connect brain regions, which consist in groups of dependent voxels. Previous work has focused on aggregating data across voxels within predefined regions. However, the presence of within-region correlations has noticeable impacts on inter-regional correlation detection, and thus edge identification. To alleviate them, we propose to leverage techniques from the large-scale correlation screening literature, and derive simple and practical characterizations of the mean number of correlation discoveries that flexibly incorporate intra-regional dependence structures. A connectivity network inference framework is then presented. First, inter-regional correlation distributions are estimated. Then, correlation thresholds that can be tailored to one’s application are constructed for each edge. Finally, the proposed framework is implemented on synthetic and real-world datasets. This novel approach for handling arbitrary intra-regional correlation is shown to limit false positives while improving true positive rates.},
  archive      = {J_SAC},
  author       = {Lbath, Hanâ and Petersen, Alexander and Achard, Sophie},
  doi          = {10.1007/s11222-024-10411-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Large-scale correlation screening under dependence for brain functional connectivity network inference},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New mixed portmanteau tests for time series models.
<em>SAC</em>, <em>34</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10393-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes omnibus portmanteau tests for contrasting adequacy of time series models. The test statistics are based on combining the autocorrelation function of the conditional residuals, the autocorrelation function of the conditional squared residuals, and the cross-correlation function between these residuals and their squares. The maximum likelihood estimator is used to derive the asymptotic distribution of the proposed test statistics under a general class of time series models, including ARMA, GARCH, and other nonlinear structures. An extensive Monte Carlo simulation study shows that the proposed tests successfully control the type I error probability and tend to have more power than other competitor tests in many scenarios. Two applications to a set of weekly stock returns for 92 companies from the S &amp;P 500 demonstrate the practical use of the proposed tests.},
  archive      = {J_SAC},
  author       = {Mahdi, Esam},
  doi          = {10.1007/s11222-024-10393-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {New mixed portmanteau tests for time series models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantile generalized measures of correlation. <em>SAC</em>,
<em>34</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10414-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a quantile Generalized Measure of Correlation (GMC) to describe nonlinear quantile relationship between response variable and predictors. The introduced correlation takes values between zero and one. It is zero if and only if the conditional quantile function is equal to the unconditional quantile. We also introduce a quantile partial Generalized Measure of Correlation. Estimators of these correlations are developed. Notably by adopting machine learning methods, our estimation procedures allow the dimension of predictors very large. Under mild conditions, we establish the estimators’ consistency. For construction of confidence interval, we adopt sample splitting and show that the corresponding estimators are asymptotic normal. We also consider composite quantile GMC by integrating information from different quantile levels. Numerical studies are conducted to illustrate our methods. Moreover, we apply our methods to analyze genome-wide association study data from Carworth Farms White mice.},
  archive      = {J_SAC},
  author       = {Zhang, Xinyu and Shi, Hongwei and Zhou, Niwen and Tan, Falong and Guo, Xu},
  doi          = {10.1007/s11222-024-10414-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Quantile generalized measures of correlation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust quantile regression for bounded variables based on
the kumaraswamy rectangular distribution. <em>SAC</em>, <em>34</em>(2),
1–18. (<a href="https://doi.org/10.1007/s11222-024-10381-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression (QR) models offer an interesting alternative compared with ordinary regression models for the response mean. Besides allowing a more appropriate characterization of the response distribution, the former is less sensitive to outlying observations than the latter. Indeed, the QR models allow modeling other characteristics of the response distribution, such as the lower and/or upper tails. However, in the presence of outlying observations, the estimates can still be affected. In this context, a robust quantile parametric regression model for bounded responses is developed, considering a new distribution, the Kumaraswamy Rectangular (KR) distribution. The KR model corresponds to a finite mixture structure similar to the Beta Rectangular distribution. That is, the KR distribution has heavier tails compared to the Kumaraswamy model. Indeed, we show that the correspondent KR quantile regression model is more robust and flexible than the usual Kumaraswamy one. Bayesian inference, which includes parameter estimation, model fit assessment, model comparison, and influence analysis, is developed through a hybrid-based MCMC approach. Since the quantile of the KR distribution is not analytically tractable, we consider the modeling of the conditional quantile based on a suitable data augmentation scheme. To link both quantiles in terms of a regression structure, a two-step estimation algorithm under a Bayesian approach is proposed to obtain the numerical approximation of the respective posterior distributions of the parameters of the regression structure for the KR quantile. Such an algorithm combines a Markov Chain Monte Carlo algorithm with the Ordinary Least Squares approach. Our proposal showed to be robust against outlying observations related to the response while keeping the estimation process simple without adding too much to the computational complexity. We showed the effectiveness of our estimation method with a simulation study, whereas two other studies showed some benefits of the proposed model in terms of robustness and flexibility. To exemplify the adequacy of our approach, under the presence of outlying observations, we analyzed two data sets regarding socio-economic indicators from Brazil and compared them with alternatives.},
  archive      = {J_SAC},
  author       = {Castro, Matheus and Azevedo, Caio and Nobre, Juvêncio},
  doi          = {10.1007/s11222-024-10381-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A robust quantile regression for bounded variables based on the kumaraswamy rectangular distribution},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational bayesian analysis of survival data using a
log-logistic accelerated failure time model. <em>SAC</em>,
<em>34</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10365-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The log-logistic regression model is one of the most commonly used accelerated failure time (AFT) models in survival analysis, for which statistical inference methods are mainly established under the frequentist framework. Recently, Bayesian inference for log-logistic AFT models using Markov chain Monte Carlo (MCMC) techniques has also been widely developed. In this work, we develop an alternative approach to MCMC methods and infer the parameters of the log-logistic AFT model via a mean-field variational Bayes (VB) algorithm. A piecewise approximation technique is embedded in deriving the VB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and compared with frequentist and MCMC techniques using simulated data under various scenarios. A publicly available dataset is employed for illustration. We have demonstrated that our proposed Variational Bayes (VB) algorithm consistently produces satisfactory estimation results and, in most scenarios, outperforms the likelihood-based method in terms of empirical mean squared error (MSE). When compared to MCMC, similar performance was achieved by our proposed VB, and, in certain scenarios, VB yielded the lowest MSE. Furthermore, the proposed VB algorithm offers a significantly reduced computational cost compared to MCMC, with an average speedup of 300 times.},
  archive      = {J_SAC},
  author       = {Xian, Chengqian and de Souza, Camila P. E. and He, Wenqing and Rodrigues, Felipe F. and Tian, Renfang},
  doi          = {10.1007/s11222-023-10365-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayesian analysis of survival data using a log-logistic accelerated failure time model},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian variable selection for matrix autoregressive
models. <em>SAC</em>, <em>34</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10402-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian method is proposed for variable selection in high-dimensional matrix autoregressive models which reflects and exploits the original matrix structure of data to (a) reduce dimensionality and (b) foster interpretability of multidimensional relationship structures. A compact form of the model is derived which facilitates the estimation procedure and two computational methods for the estimation are proposed: a Markov chain Monte Carlo algorithm and a scalable Bayesian EM algorithm. Being based on the spike-and-slab framework for fast posterior mode identification, the latter enables Bayesian data analysis of matrix-valued time series at large scales. The theoretical properties, comparative performance, and computational efficiency of the proposed model is investigated through simulated examples and an application to a panel of country economic indicators.},
  archive      = {J_SAC},
  author       = {Celani, Alessandro and Pagnottoni, Paolo and Jones, Galin},
  doi          = {10.1007/s11222-024-10402-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian variable selection for matrix autoregressive models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improvements on scalable stochastic bayesian inference
methods for multivariate hawkes process. <em>SAC</em>, <em>34</em>(2),
1–24. (<a href="https://doi.org/10.1007/s11222-024-10392-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Hawkes Processes (MHPs) are a class of point processes that can account for complex temporal dynamics among event sequences. In this work, we study the accuracy and computational efficiency of three classes of algorithms which, while widely used in the context of Bayesian inference, have rarely been applied in the context of MHPs: stochastic gradient expectation-maximization, stochastic gradient variational inference and stochastic gradient Langevin Monte Carlo. An important contribution of this paper is a novel approximation to the likelihood function that allows us to retain the computational advantages associated with conjugate settings while reducing approximation errors associated with the boundary effects. The comparisons are based on various simulated scenarios as well as an application to the study of risk dynamics in the Standard &amp; Poor’s 500 intraday index prices among its 11 sectors.},
  archive      = {J_SAC},
  author       = {Jiang, Alex Ziyu and Rodriguez, Abel},
  doi          = {10.1007/s11222-024-10392-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Improvements on scalable stochastic bayesian inference methods for multivariate hawkes process},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The minimum covariance determinant estimator for
interval-valued data. <em>SAC</em>, <em>34</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s11222-024-10386-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective estimation of covariance matrices is crucial for statistical analyses and applications. In this paper, we focus on the robust estimation of covariance matrix for interval-valued data in low and moderately high dimensions. In the low-dimensional scenario, we extend the Minimum Covariance Determinant (MCD) estimator to interval-valued data. We derive an iterative algorithm for computing this estimator, demonstrate its convergence, and theoretically establish that it retains the high breakdown-point property of the MCD estimator. Further, we propose a projection-based estimator and a regularization-based estimator to extend the MCD estimator to moderately high-dimensional settings, respectively. We propose efficient iterative algorithms for solving these two estimators and demonstrate their convergence properties. We conduct extensive simulation studies and real data analysis to validate the finite sample properties of these proposed estimators.},
  archive      = {J_SAC},
  author       = {Tian, Wan and Qin, Zhongfeng},
  doi          = {10.1007/s11222-024-10386-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {The minimum covariance determinant estimator for interval-valued data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian contiguity constrained clustering. <em>SAC</em>,
<em>34</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10376-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a well-known and studied problem, one of its variants, called contiguity-constrained clustering, accepts as a second input a graph used to encode prior information about cluster structure by means of contiguity constraints i.e. clusters must form connected subgraphs of this graph. This paper discusses the interest of such a setting and proposes a new way to formalise it in a Bayesian setting, using results on spanning trees to compute exactly a posteriori probabilities of candidate partitions. An algorithmic solution is then investigated to find a maximum a posteriori partition and extract a Bayesian dendrogram from it. The interest of this last tool, which is reminiscent of the classical output of a simple hierarchical clustering algorithm, is analysed. Finally, the proposed approach is demonstrated with experiments on simulated data and real applications. A reference implementation of this work is available in the R package gtclust that accompanies the paper.},
  archive      = {J_SAC},
  author       = {Côme, Etienne},
  doi          = {10.1007/s11222-023-10376-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian contiguity constrained clustering},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-output quantile regression neural network.
<em>SAC</em>, <em>34</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10408-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression neural network (QRNN) model has received increasing attention in various fields to provide conditional quantiles of responses. However, almost all the available literature about QRNN is devoted to handling the case with one-dimensional responses, which presents a great limitation when we focus on the quantiles of multivariate responses. To deal with this issue, we propose a novel multiple-output quantile regression neural network (MOQRNN) model in this paper to estimate the conditional quantiles of multivariate data. The MOQRNN model is constructed by the following steps. Step 1 acquires the conditional distribution of multivariate responses by a nonparametric method. Step 2 obtains the optimal transport map that pushes the spherical uniform distribution forward to the conditional distribution through the input convex neural network (ICNN). Step 3 provides the conditional quantile contours and regions by the ICNN-based optimal transport map. In both simulation studies and real data application, comparative analyses with the existing method demonstrate that the proposed MOQRNN model is more appealing to yield excellent quantile contours, which are not only smoother but also closer to their theoretical counterparts.},
  archive      = {J_SAC},
  author       = {Hao, Ruiting and Yang, Xiaorong},
  doi          = {10.1007/s11222-024-10408-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Multiple-output quantile regression neural network},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale unsupervised spatio-temporal semantic analysis
of vast regions from satellite images sequences. <em>SAC</em>,
<em>34</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10383-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal sequences of satellite images constitute a highly valuable and abundant resource for analyzing regions of interest. However, the automatic acquisition of knowledge on a large scale is a challenging task due to different factors such as the lack of precise labeled data, the definition and variability of the terrain entities, or the inherent complexity of the images and their fusion. In this context, we present a fully unsupervised and general methodology to conduct spatio-temporal taxonomies of large regions from sequences of satellite images. Our approach relies on a combination of deep embeddings and time series clustering to capture the semantic properties of the ground and its evolution over time, providing a comprehensive understanding of the region of interest. The proposed method is enhanced by a novel procedure specifically devised to refine the embedding and exploit the underlying spatio-temporal patterns. We use this methodology to conduct an in-depth analysis of a 220 km $$^2$$ region in northern Spain in different settings. The results provide a broad and intuitive perspective of the land where large areas are connected in a compact and well-structured manner, mainly based on climatic, phytological, and hydrological factors.},
  archive      = {J_SAC},
  author       = {Echegoyen, Carlos and Pérez, Aritz and Santafé, Guzmán and Pérez-Goya, Unai and Ugarte, María Dolores},
  doi          = {10.1007/s11222-024-10383-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Large-scale unsupervised spatio-temporal semantic analysis of vast regions from satellite images sequences},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kent feature embedding for classification of compositional
data with zeros. <em>SAC</em>, <em>34</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10382-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional data have posed challenges to current classification methods owing to the non-negative and unit-sum constraints, especially when a certain of the components are zeros. In this paper, we develop an effective classification method for multivariate compositional data with certain of the components equal to zero. Specifically, a Kent feature embedding technique is first proposed to transform compositional data and improve data quality. We then use support vector machine as the state-of-the-art machine learning model to build the classifier. The proposed method is proved to be effective through numerical simulations. Results on multiple real datasets, including species classification, day-night image classification and household’s consumption pattern recognition, further verify that the proposed method can achieve good classification performance and outperform the other competitors. This method would help to broaden the practical usage of compositional data with zeros in the task of classification.},
  archive      = {J_SAC},
  author       = {Lu, Shan and Wang, Wenjing and Guan, Rong},
  doi          = {10.1007/s11222-024-10382-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Kent feature embedding for classification of compositional data with zeros},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expectile hidden markov regression models for analyzing
cryptocurrency returns. <em>SAC</em>, <em>34</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11222-023-10377-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop a linear expectile hidden Markov model for the analysis of cryptocurrency time series in a risk management framework. The methodology proposed allows to focus on extreme returns and describe their temporal evolution by introducing in the model time-dependent coefficients evolving according to a latent discrete homogeneous Markov chain. As it is often used in the expectile literature, estimation of the model parameters is based on the asymmetric normal distribution. Maximum likelihood estimates are obtained via an Expectation–Maximization algorithm using efficient M-step update formulas for all parameters. We evaluate the introduced method with both artificial data under several experimental settings and real data investigating the relationship between daily Bitcoin returns and major world market indices.},
  archive      = {J_SAC},
  author       = {Foroni, Beatrice and Merlo, Luca and Petrella, Lea},
  doi          = {10.1007/s11222-023-10377-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Expectile hidden markov regression models for analyzing cryptocurrency returns},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian parameter inference for partially observed
stochastic volterra equations. <em>SAC</em>, <em>34</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11222-024-10389-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider Bayesian parameter inference for a type of partially observed stochastic Volterra equation (SVE). SVEs are found in many areas such as physics and mathematical finance. In the latter field they can be used to represent long memory in unobserved volatility processes. In many cases of practical interest, SVEs must be time-discretized and then parameter inference is based upon the posterior associated to this time-discretized process. Based upon recent studies on time-discretization of SVEs (e.g. Richard et al. in Stoch Proc Appl 141:109–138, 2021) we use Euler–Maruyama methods for the afore-mentioned discretization. We then show how multilevel Markov chain Monte Carlo (MCMC) methods (Jasra et al. in SIAM J Sci Comp 40:A887–A902, 2018) can be applied in this context. In the examples we study, we give a proof that shows that the cost to achieve a mean square error (MSE) of $$\mathcal {O}(\epsilon ^2)$$ , $$\epsilon &gt;0$$ , is $$\mathcal {O}(\epsilon ^{-\tfrac{4}{2H+1}})$$ , where H is the Hurst parameter. If one uses a single level MCMC method then the cost is $$\mathcal {O}(\epsilon ^{-\tfrac{2(2H+3)}{2H+1}})$$ to achieve the same MSE. We illustrate these results in the context of state-space and stochastic volatility models, with the latter applied to real data.},
  archive      = {J_SAC},
  author       = {Jasra, Ajay and Ruzayqat, Hamza and Wu, Amin},
  doi          = {10.1007/s11222-024-10389-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian parameter inference for partially observed stochastic volterra equations},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient exponential tilting with applications.
<em>SAC</em>, <em>34</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s11222-023-10374-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To minimize the variance of Monte Carlo estimators, we develop a novel exponential embedding technique that extends the classical concept of sufficient statistics in importance sampling. Our method demonstrates bounded relative error and logarithmic efficiency when applied to normal and gamma distributions, especially in rare event scenarios. To illustrate this innovative technique, we address the problem of credit risk measurement in portfolios and present an efficient simulation algorithm to estimate the likelihood of significant portfolio losses, leveraging multi-factor models with a normal mixture copula. Finally, supported by comprehensive simulation studies, our approach offers a more effective and efficient way to simulate moderately rare events.},
  archive      = {J_SAC},
  author       = {Fuh, Cheng-Der and Wang, Chuan-Ju},
  doi          = {10.1007/s11222-023-10374-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Efficient exponential tilting with applications},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forward stability and model path selection. <em>SAC</em>,
<em>34</em>(2), 1–28. (<a
href="https://doi.org/10.1007/s11222-024-10395-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most scientific publications follow the familiar recipe of (i) obtain data, (ii) fit a model, and (iii) comment on the scientific relevance of the effects of particular covariates in that model. This approach, however, ignores the fact that there may exist a multitude of similarly-accurate models in which the implied effects of individual covariates may be vastly different. This problem of finding an entire collection of plausible models has also received relatively little attention in the statistics community, with nearly all of the proposed methodologies being narrowly tailored to a particular model class and/or requiring an exhaustive search over all possible models, making them largely infeasible in the current big data era. This work develops the idea of forward stability and proposes a novel, computationally-efficient approach to finding collections of accurate models we refer to as model path selection (MPS). MPS builds up a plausible model collection via a forward selection approach and is entirely agnostic to the model class and loss function employed. The resulting model collection can be displayed in a simple and intuitive graphical fashion, easily allowing practitioners to visualize whether some covariates can be swapped for others with minimal loss.},
  archive      = {J_SAC},
  author       = {Kissel, Nicholas and Mentch, Lucas},
  doi          = {10.1007/s11222-024-10395-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Forward stability and model path selection},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast generation of exchangeable sequences of clusters data.
<em>SAC</em>, <em>34</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-024-10385-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Bayesian models for random partitions have led to the formulation and exploration of Exchangeable Sequences of Clusters (ESC) models. Under ESC models, it is the cluster sizes that are exchangeable, rather than the observations themselves. This property is particularly useful for obtaining microclustering behavior, whereby cluster sizes grow sublinearly in the number of observations, as is common in applications such as record linkage, sparse networks and genomics. Unfortunately, the exchangeable clusters property comes at the cost of projectivity. As a consequence, in contrast to more traditional Dirichlet Process or Pitman–Yor process mixture models, samples a priori from ESC models cannot be easily obtained in a sequential fashion and instead require the use of rejection or importance sampling. In this work, drawing on connections between ESC models and discrete renewal theory, we obtain closed-form expressions for certain ESC models and develop faster methods for generating samples a priori from these models compared with the existing state of the art. In the process, we establish analytical expressions for the distribution of the number of clusters under ESC models, which was unknown prior to this work.},
  archive      = {J_SAC},
  author       = {Levin, Keith and Betancourt, Brenda},
  doi          = {10.1007/s11222-024-10385-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Fast generation of exchangeable sequences of clusters data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subsampling approach for least squares fitting of
semi-parametric accelerated failure time models to massive survival
data. <em>SAC</em>, <em>34</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-024-10391-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive survival data are increasingly common in many research fields, and subsampling is a practical strategy for analyzing such data. Although optimal subsampling strategies have been developed for Cox models, little has been done for semiparametric accelerated failure time (AFT) models due to the challenges posed by non-smooth estimating functions for the regression coefficients. We develop optimal subsampling algorithms for fitting semi-parametric AFT models using the least-squares approach. By efficiently estimating the slope matrix of the non-smooth estimating functions using a resampling approach, we construct optimal subsampling probabilities for the observations. For feasible point and interval estimation of the unknown coefficients, we propose a two-step method, drawing multiple subsamples in the second stage to correct for overestimation of the variance in higher censoring scenarios. We validate the performance of our estimators through a simulation study that compares single and multiple subsampling methods and apply the methods to analyze the survival time of lymphoma patients in the Surveillance, Epidemiology, and End Results program.},
  archive      = {J_SAC},
  author       = {Yang, Zehan and Wang, HaiYing and Yan, Jun},
  doi          = {10.1007/s11222-024-10391-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Subsampling approach for least squares fitting of semi-parametric accelerated failure time models to massive survival data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of regime-switching diffusions via fourier
transforms. <em>SAC</em>, <em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10397-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an algorithm for maximum-likelihood estimation of regime-switching diffusions is proposed. The proposed approach uses a Fourier transform to numerically solve the system of Fokker–Planck or forward Kolmogorow equations for the temporal evolution of the state densities. Monte Carlo simulations confirm the theoretically expected consistency of this approach for moderate sample sizes and its practical feasibility for certain regime-switching diffusions used in economics and biology with moderate numbers of states and parameters. An application to animal movement data serves as an illustration of the proposed algorithm.},
  archive      = {J_SAC},
  author       = {Lux, Thomas},
  doi          = {10.1007/s11222-024-10397-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of regime-switching diffusions via fourier transforms},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust score matching for compositional data. <em>SAC</em>,
<em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10412-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted polynomially-tilted pairwise interaction (RPPI) distribution gives a flexible model for compositional data. It is particularly well-suited to situations where some of the marginal distributions of the components of a composition are concentrated near zero, possibly with right skewness. This article develops a method of tractable robust estimation for the model by combining two ideas. The first idea is to use score matching estimation after an additive log-ratio transformation. The resulting estimator is automatically insensitive to zeros in the data compositions. The second idea is to incorporate suitable weights in the estimating equations. The resulting estimator is additionally resistant to outliers. These properties are confirmed in simulation studies where we further also demonstrate that our new outlier-robust estimator is efficient in high concentration settings, even in the case when there is no model contamination. An example is given using microbiome data. A user-friendly R package accompanies the article.},
  archive      = {J_SAC},
  author       = {Scealy, Janice L. and Hingee, Kassel L. and Kent, John T. and Wood, Andrew T. A.},
  doi          = {10.1007/s11222-024-10412-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Robust score matching for compositional data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enmsp: An elastic-net multi-step screening procedure for
high-dimensional regression. <em>SAC</em>, <em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10394-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the estimation efficiency of high-dimensional regression problems, penalized regularization is routinely used. However, accurately estimating the model remains challenging, particularly in the presence of correlated effects, wherein irrelevant covariates exhibit strong correlation with relevant ones. This situation, referred to as correlated data, poses additional complexities for model estimation. In this paper, we propose the elastic-net multi-step screening procedure (EnMSP), an iterative algorithm designed to recover sparse linear models in the context of correlated data. EnMSP uses a small repeated penalty strategy to identify truly relevant covariates in a few iterations. Specifically, in each iteration, EnMSP enhances the adaptive lasso method by adding a weighted $$l_2$$ penalty, which improves the selection of relevant covariates. The method is shown to select the true model and achieve the $$l_2$$ -norm error bound under certain conditions. The effectiveness of EnMSP is demonstrated through numerical comparisons and applications in financial data.},
  archive      = {J_SAC},
  author       = {Xue, Yushan and Ren, Jie and Yang, Bin},
  doi          = {10.1007/s11222-024-10394-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Enmsp: An elastic-net multi-step screening procedure for high-dimensional regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COMBSS: Best subset selection via continuous optimization.
<em>SAC</em>, <em>34</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10387-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of best subset selection in linear regression is considered with the aim to find a fixed size subset of features that best fits the response. This is particularly challenging when the total available number of features is very large compared to the number of data samples. Existing optimal methods for solving this problem tend to be slow while fast methods tend to have low accuracy. Ideally, new methods perform best subset selection faster than existing optimal methods but with comparable accuracy, or, being more accurate than methods of comparable computational speed. Here, we propose a novel continuous optimization method that identifies a subset solution path, a small set of models of varying size, that consists of candidates for the single best subset of features, that is optimal in a specific sense in linear regression. Our method turns out to be fast, making the best subset selection possible when the number of features is well in excess of thousands. Because of the outstanding overall performance, framing the best subset selection challenge as a continuous optimization problem opens new research directions for feature extraction for a large variety of regression models.},
  archive      = {J_SAC},
  author       = {Moka, Sarat and Liquet, Benoit and Zhu, Houying and Muller, Samuel},
  doi          = {10.1007/s11222-024-10387-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {COMBSS: Best subset selection via continuous optimization},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The sparse dynamic factor model: A regularised quasi-maximum
likelihood approach. <em>SAC</em>, <em>34</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10378-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concepts of sparsity, and regularised estimation, have proven useful in many high-dimensional statistical applications. Dynamic factor models (DFMs) provide a parsimonious approach to modelling high-dimensional time series, however, it is often hard to interpret the meaning of the latent factors. This paper formally introduces a class of sparse DFMs whereby the loading matrices are constrained to have few non-zero entries, thus increasing interpretability of factors. We present a regularised M-estimator for the model parameters, and construct an efficient expectation maximisation algorithm to enable estimation. Synthetic experiments demonstrate consistency in terms of estimating the loading structure, and superior predictive performance where a low-rank factor structure may be appropriate. The utility of the method is further illustrated in an application forecasting electricity consumption across a large set of smart meters.},
  archive      = {J_SAC},
  author       = {Mosley, Luke and Chan, Tak-Shing T. and Gibberd, Alex},
  doi          = {10.1007/s11222-023-10378-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {The sparse dynamic factor model: A regularised quasi-maximum likelihood approach},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric bayesian online change point detection using
kernel density estimation with nonparametric hazard function.
<em>SAC</em>, <em>34</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10375-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop Bayesian online change point detection (BOCD), a parametric change point detection method, into a nonparametric method to be able to detect change points in a free-distribution time series. Instead of using predefined exponential family distribution for predictive probability, we use kernel density estimation in which two possible options have been proposed. The first is manual constant bandwidth selection. This option provides a fast computation of KDE as it can follow dynamic programming. Another option for the best performance is a nonparametric bandwidth estimator. For the goal of fully nonparametric change point detection, the predefined hazard function in the BOCD method is changed to be a nonparametric estimator. The performance of the proposed method is intensively evaluated with simulated data and compared with other traditional methods. It is found that nonparametric BOCD gives a better solution in all cases as a consequence of the adaptive property of KDE. Furthermore, the real-life application of the method with real data demonstrated its outstanding performance in detecting change points across diverse datasets. This success signifies a promising solution for expanding the potential of change point detection algorithms across a broader range of fields through the use of a nonparametric approach. Nevertheless, it requires a sufficient amount of data to form a precise predictive distribution curve to accurately detect change points.},
  archive      = {J_SAC},
  author       = {Prabpon, Naruesorn and Homsud, Kitakorn and Vatiwutipong, Pat},
  doi          = {10.1007/s11222-023-10375-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Nonparametric bayesian online change point detection using kernel density estimation with nonparametric hazard function},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global–local shrinkage multivariate logit-beta priors for
multiple response-type data. <em>SAC</em>, <em>34</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-024-10380-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-type outcomes are often encountered in many statistical applications, one may want to study the association between multiple responses and determine the covariates useful for prediction. However, literature on variable selection methods for multiple-type data is arguably underdeveloped. In this article, we develop a novel global–local shrinkage prior in multiple response-types settings, where the observed dataset consists of multiple response-types (e.g., continuous, count-valued, Bernoulli trials, etc.), by combining the perspectives of global–local shrinkage and the conjugate multivaraite distribution. One benefit of our model is that a transformation or a Gaussian approximation on the data is not needed to perform variable selection for multiple response-type data, and thus one can avoid computational difficulties and restrictions on the joint distribution of the responses. Another benefit is that it allows one to parsimoniously model cross-variable dependence. Specifically, our method uses basis functions with random effects, which can be presented as known covariates or pre-defined basis functions, to model dependence between responses and dependence can be detected by our proposed global–local shrinkage model with a sparsity-inducing model. We provide connections to the original horseshoe model and existing basis function models. An efficient block Gibbs sampler is developed, which is found to be effective in obtaining accurate estimates and variable selection results. We also provide a motivating analysis of public health and financial costs from natural disasters in the U.S. using data provided by the National Centers for Environmental Information.},
  archive      = {J_SAC},
  author       = {Wu, Hongyu and Bradley, Jonathan R.},
  doi          = {10.1007/s11222-024-10380-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Global–local shrinkage multivariate logit-beta priors for multiple response-type data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do applied statisticians prefer more randomness or less?
Bootstrap or jackknife? <em>SAC</em>, <em>34</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s11222-024-10388-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrap and Jackknife estimates, $$T_{n,B}^*$$ and $$T_{n,J},$$ respectively, of a population parameter $$\theta $$ are both used in statistical computations; n is the sample size, B is the number of Bootstrap samples. For any $$n_0$$ and $$B_0,$$ Bootstrap samples do not add new information about $$\theta $$ being observations from the original sample and when $$B_0&lt;\infty ,$$ $$T_{n_0,B_0}^*$$ includes also resampling variability, an additional source of uncertainty not affecting $$T_{n_0, J}.$$ These are neglected in theoretical papers with results for the utopian $$T_{n, \infty }^*, $$ that do not hold for $$B&lt;\infty .$$ The consequence is that $$T^*_{n_0, B_0}$$ is expected to have larger mean squared error (MSE) than $$T_{n_0,J},$$ namely $$T_{n_0,B_0}^*$$ is inadmissible. The amount of inadmissibility may be very large when populations’ parameters, e.g. the variance, are unbounded and/or with big data. A palliating remedy is increasing B,  the larger the better, but the MSEs ordering remains unchanged for $$B&lt;\infty .$$ This is confirmed theoretically when $$\theta $$ is the mean of a population, and is observed in the estimated total MSE for linear regression coefficients. In the latter, the chance the estimated total MSE with $$T_{n,B}^*$$ improves that with $$T_{n,J}$$ decreases to 0 as B increases.},
  archive      = {J_SAC},
  author       = {Yatracos, Yannis G.},
  doi          = {10.1007/s11222-024-10388-7},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Do applied statisticians prefer more randomness or less? bootstrap or jackknife?},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Finite mixtures of quantile and m-quantile
regression models. <em>SAC</em>, <em>34</em>(1), 1. (<a
href="https://doi.org/10.1007/s11222-023-10316-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Alfò, Marco and Salvati, Nicola and Giovanna, Ranalli M.},
  doi          = {10.1007/s11222-023-10316-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: Finite mixtures of quantile and M-quantile regression models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biclustering multivariate discrete longitudinal data.
<em>SAC</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10292-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-based biclustering method for multivariate discrete longitudinal data is proposed. We consider a finite mixture of generalized linear models to cluster units and, within each mixture component, we adopt a flexible and parsimonious parameterization of the component-specific canonical parameter to define subsets of variables (segments) sharing common dynamics over time. We develop an Expectation-Maximization-type algorithm for maximum likelihood estimation of model parameters. The performance of the proposed model is evaluated on a large scale simulation study, where we consider different choices for the sample the size, the number of measurement occasions, the number of components and segments. The proposal is applied to Italian crime data (font ISTAT) with the aim to detect areas sharing common longitudinal trajectories for specific subsets of crime types. The identification of such biclusters may potentially be helpful for policymakers to make decisions on safety.},
  archive      = {J_SAC},
  author       = {Alfó, M. and Marino, M. F. and Martella, F.},
  doi          = {10.1007/s11222-023-10292-6},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Biclustering multivariate discrete longitudinal data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent causal inference from time series with PC
algorithm and its time-aware extension. <em>SAC</em>, <em>34</em>(1),
1–21. (<a href="https://doi.org/10.1007/s11222-023-10330-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimator of a causal directed acyclic graph (DAG) with the PC algorithm is known to be consistent based on independent and identically distributed samples. In this paper, we consider the scenario when the multivariate samples are identically distributed but not independent. A common example is a stationary multivariate time series. We show that under a standard set of assumptions on the underlying time series involving $$\rho $$ -mixing, the PC algorithm is consistent under temporal dependence. Further, we show that for the popular time series models such as vector auto-regressive moving average and linear processes, consistency of the PC algorithm holds. We also prove the consistency for the Time-Aware PC algorithm, a recent adaptation of the PC algorithm for the time series scenario. Our findings are supported by simulations and benchmark real data analyses provided towards the end of the paper.},
  archive      = {J_SAC},
  author       = {Biswas, Rahul and Mukherjee, Somabha},
  doi          = {10.1007/s11222-023-10330-3},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Consistent causal inference from time series with PC algorithm and its time-aware extension},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust model-based clustering based on the geometric
median and the median covariation matrix. <em>SAC</em>, <em>34</em>(1),
1–21. (<a href="https://doi.org/10.1007/s11222-023-10362-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grouping observations into homogeneous groups is a recurrent task in statistical data analysis. We consider Gaussian Mixture Models, which are the most famous parametric model-based clustering method. We propose a new robust approach for model-based clustering, which consists in a modification of the EM algorithm (more specifically, the M-step) by replacing the estimates of the mean and the variance by robust versions based on the median and the median covariation matrix. All the proposed methods are available in the R package RGMM accessible on CRAN.},
  archive      = {J_SAC},
  author       = {Godichon-Baggioni, Antoine and Robin, Stéphane},
  doi          = {10.1007/s11222-023-10362-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {A robust model-based clustering based on the geometric median and the median covariation matrix},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multivariate heavy-tailed integer-valued GARCH process
with EM algorithm-based inference. <em>SAC</em>, <em>34</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s11222-023-10372-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new multivariate integer-valued Generalized AutoRegressive Conditional Heteroscedastic (GARCH) process based on a multivariate Poisson generalized inverse Gaussian distribution is proposed. The estimation of parameters of the proposed multivariate heavy-tailed count time series model via maximum likelihood method is challenging since the likelihood function involves a Bessel function that depends on the multivariate counts and its dimension. As a consequence, numerical instability is often experienced in optimization procedures. To overcome this computational problem, two feasible variants of the expectation-maximization (EM) algorithm are proposed for estimating the parameters of our model under low and high-dimensional settings. These EM algorithm variants provide computational benefits and help avoid the difficult direct optimization of the likelihood function from the proposed process. Our model and proposed estimation procedures can handle multiple features such as modeling of multivariate counts, heavy-tailedness, overdispersion, accommodation of outliers, allowances for both positive and negative autocorrelations, estimation of cross/contemporaneous-correlation, and the efficient estimation of parameters from both statistical and computational points of view. Extensive Monte Carlo simulation studies are presented to assess the performance of the proposed EM algorithms. Two empirical applications of our approach are provided. The first application concerns modeling bivariate count time series data on cannabis possession-related offenses in Australia, while the second one involves modeling intraday high-frequency financial transactions data from multiple holdings in the U.S. financial market.},
  archive      = {J_SAC},
  author       = {Jang, Yuhyeong and Sundararajan, Raanju R. and Barreto-Souza, Wagner},
  doi          = {10.1007/s11222-023-10372-7},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {A multivariate heavy-tailed integer-valued GARCH process with EM algorithm-based inference},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized time riemannian manifold hamiltonian monte carlo.
<em>SAC</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10303-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamiltonian Monte Carlo (HMC) algorithms, which combine numerical approximation of Hamiltonian dynamics on finite intervals with stochastic refreshment and Metropolis correction, are popular sampling schemes, but it is known that they may suffer from slow convergence in the continuous time limit. A recent paper of Bou-Rabee and Sanz-Serna (Ann Appl Prob, 27:2159-2194, 2017) demonstrated that this issue can be addressed by simply randomizing the duration parameter of the Hamiltonian paths. In this article, we use the same idea to enhance the sampling efficiency of a constrained version of HMC, with potential benefits in a variety of application settings. We demonstrate both the conservation of the stationary distribution and the ergodicity of the method. We also compare the performance of various schemes in numerical studies of model problems, including an application to high-dimensional covariance estimation.},
  archive      = {J_SAC},
  author       = {Whalley, Peter A. and Paulin, Daniel and Leimkuhler, Benedict},
  doi          = {10.1007/s11222-023-10303-6},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Randomized time riemannian manifold hamiltonian monte carlo},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational inference for bayesian bridge regression.
<em>SAC</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10317-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bridge approach for regularization of coefficients in regression models uses $$\ell _{\alpha }$$ norm, with $$\alpha \in (0, +\infty )$$ , to define a penalization on large values of the regression coefficients. Particular cases include the Lasso and Ridge penalizations. In Bayesian models, the penalization is enforced by a prior distribution on the coefficients. Although MCMC approaches are available for Bayesian bridge regression, they can be very slow for large datasets, specially in high dimensions. This paper develops an implementation of Automatic Differentiation Variational Inference for Bayesian inference on semi-parametric regression models with bridge penalization. The non-parametric effects of covariates are modeled by B-splines. The proposed inference procedure allows the use of small batches of data at each iteration (due to stochastic gradient based updates), therefore drastically reducing computational time in comparison with MCMC. Full Bayesian inference is preserved so joint uncertainty estimates for all model parameters are available. A simulation study shows the main properties of the proposed method and an application to a large real dataset is presented.},
  archive      = {J_SAC},
  author       = {Zanini, Carlos Tadeu Pagani and Migon, Helio S. and Dias, Ronaldo},
  doi          = {10.1007/s11222-023-10317-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Variational inference for bayesian bridge regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of extreme quantiles from heavy-tailed
distributions with neural networks. <em>SAC</em>, <em>34</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s11222-023-10331-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new parametrizations for neural networks in order to estimate extreme quantiles in both non-conditional and conditional heavy-tailed settings. All proposed neural network estimators feature a bias correction based on an extension of the usual second-order condition to an arbitrary order. The convergence rate of the uniform error between extreme log-quantiles and their neural network approximation is established. The finite sample performances of the non-conditional neural network estimator are compared to other bias-reduced extreme-value competitors on simulated data. It is shown that our method outperforms them in difficult heavy-tailed situations where other estimators almost all fail. Finally, the conditional neural network estimators are implemented to investigate the behavior of extreme rainfalls as functions of their geographical location in the southern part of France. The source code is available at https://github.com/michael-allouche/nn-quantile-extrapolation.git .},
  archive      = {J_SAC},
  author       = {Allouche, Michaël and Girard, Stéphane and Gobet, Emmanuel},
  doi          = {10.1007/s11222-023-10331-2},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of extreme quantiles from heavy-tailed distributions with neural networks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SURE-tuned bridge regression. <em>SAC</em>, <em>34</em>(1),
1–17. (<a href="https://doi.org/10.1007/s11222-023-10350-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the $$\ell _{\alpha }$$ regularized linear regression, also termed Bridge regression. For $$\alpha \in (0,1)$$ , Bridge regression enjoys several statistical properties of interest such as sparsity and near-unbiasedness of the estimates (Fan and Li in J Am Stat Assoc 96(456): 1348–1360, 2001). However, the main difficulty lies in the non-convex nature of the penalty for these values of $$\alpha $$ , which makes an optimization procedure challenging and usually it is only possible to find a local optimum. To address this issue, Polson et al. (J R Stat Soc B 76(4):713–733, 2013) took a sampling based fully Bayesian approach to this problem, using the correspondence between the Bridge penalty and a power exponential prior on the regression coefficients. However, their sampling procedure relies on Markov chain Monte Carlo (MCMC) techniques, which are inherently sequential and not scalable to large problem dimensions. Cross validation approaches are similarly computation-intensive. To this end, our contribution is a novel non-iterative method to fit a Bridge regression model. The main contribution lies in an explicit formula for Stein’s unbiased risk estimate for the out of sample prediction risk of Bridge regression, which can then be optimized to select the desired tuning parameters, allowing us to completely bypass MCMC as well as computation-intensive cross validation approaches. Our procedure yields results in a fraction of computational times compared to iterative schemes, without any appreciable loss in statistical performance. An R implementation is publicly available online at: https://github.com/loriaJ/Sure-tuned_BridgeRegression .},
  archive      = {J_SAC},
  author       = {Loría, Jorge and Bhadra, Anindya},
  doi          = {10.1007/s11222-023-10350-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {SURE-tuned bridge regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Off-policy evaluation for tabular reinforcement learning
with synthetic trajectories. <em>SAC</em>, <em>34</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10351-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of offline evaluation in tabular reinforcement learning (RL). We propose a novel method that leverages synthetic trajectories constructed from the available data using a “sampling with replacement” basis, combining the advantages of model-based and Monte Carlo policy evaluation. The method is accompanied by theoretically derived finite sample upper error bounds, offering performance guarantees and allowing for a trade-off between statistical efficiency and computational cost. The results from computational experiments demonstrate that our method consistently achieves lower upper error bounds and relative mean square errors compared to Importance Sampling, Doubly Robust methods, and other existing approaches. Furthermore, this method achieves these superior results in significantly shorter running times compared to traditional model-based approaches. These findings highlight the effectiveness and efficiency of this synthetic trajectory method for accurate offline policy evaluation in RL.},
  archive      = {J_SAC},
  author       = {Wang, Weiwei and Li, Yuqiang and Wu, Xianyi},
  doi          = {10.1007/s11222-023-10351-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Off-policy evaluation for tabular reinforcement learning with synthetic trajectories},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust estimation and robust empirical likelihood in
generalized linear models with missing responses. <em>SAC</em>,
<em>34</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10347-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study doubly robust estimation and robust empirical likelihood of regression parameter for generalized linear models with missing responses. A doubly robust estimating equation is proposed to estimate the regression parameter, and the resulting estimator has consistency and asymptotic normality, regardless of whether the assumed model contains the true model. A robust empirical log-likelihood ratio statistic for the regression parameter is constructed, showing that the statistic weakly converges to the standard $$\chi ^2$$ distribution. The result can be directly used to construct the confidence region of the regression parameter. A method for selecting the tuning parameters of $$\psi $$ -function is also given. Simulation studies show the robustness of the estimator of the regression parameter and evaluate the performance of the robust empirical likelihood method. A real data example shows that the proposed method is feasible.},
  archive      = {J_SAC},
  author       = {Xue, Liugen},
  doi          = {10.1007/s11222-023-10347-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Doubly robust estimation and robust empirical likelihood in generalized linear models with missing responses},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood-free inference in state-space models with unknown
dynamics. <em>SAC</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10339-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-free inference (LFI) has been successfully applied to state-space models, where the likelihood of observations is not available but synthetic observations generated by a black-box simulator can be used for inference instead. However, much of the research up to now has been restricted to cases in which a model of state transition dynamics can be formulated in advance and the simulation budget is unrestricted. These methods fail to address the problem of state inference when simulations are computationally expensive and the Markovian state transition dynamics are undefined. The approach proposed in this manuscript enables LFI of states with a limited number of simulations by estimating the transition dynamics and using state predictions as proposals for simulations. In the experiments with non-stationary user models, the proposed method demonstrates significant improvement in accuracy for both state inference and prediction, where a multi-output Gaussian process is used for LFI of states and a Bayesian neural network as a surrogate model of transition dynamics.},
  archive      = {J_SAC},
  author       = {Aushev, Alexander and Tran, Thong and Pesonen, Henri and Howes, Andrew and Kaski, Samuel},
  doi          = {10.1007/s11222-023-10339-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Likelihood-free inference in state-space models with unknown dynamics},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling volatility for high-frequency data with rounding
error: A nonparametric bayesian approach. <em>SAC</em>, <em>34</em>(1),
1–15. (<a href="https://doi.org/10.1007/s11222-023-10341-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rounding is a pivotal source of market microstructure noise which should be carefully addressed in high-frequency data analysis. This study incorporates available market information in modeling rounding errors and proposes a Rounding Error-Trading Information model from the Bayesian perspective. We assign a thresholded Gaussian process prior to the instantaneous volatility of the log-price process and adopt a fully Bayesian approach with an efficient Markov chain Monte Carlo algorithm for model inference, based on which a novel Trading Information-based estimator for the integrated volatility is provided. Simulation studies show that the proposed method can effectively recover the true latent log-prices, instantaneous volatility, and integrated volatility with multiple volatility shapes and rounding mechanisms, even under model misspecification. Extensive empirical studies show that the rounding mechanism in the Shanghai A-share market is likely to be random and the trading directions have a greater impact on the rounding results of the asset prices than the true latent log-prices, which is a consistent finding with that in the literature.},
  archive      = {J_SAC},
  author       = {Liang, Wanwan and Wu, Ben and Zhang, Bo},
  doi          = {10.1007/s11222-023-10341-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Modeling volatility for high-frequency data with rounding error: A nonparametric bayesian approach},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood-based surrogate dimension reduction.
<em>SAC</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10357-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of surrogate sufficient dimension reduction, that is, estimating the central subspace of a regression model, when the covariates are contaminated by measurement error. When no measurement error is present, a likelihood-based dimension reduction method that relies on maximizing the likelihood of a Gaussian inverse regression model on the Grassmann manifold is well-known to have superior performance to traditional inverse moment methods. We propose two likelihood-based estimators for the central subspace in measurement error settings, which make different adjustments to the observed surrogates. Both estimators are computed based on maximizing objective functions on the Grassmann manifold and are shown to consistently recover the true central subspace. When the central subspace is assumed to depend on only a few covariates, we further propose to augment the likelihood function with a penalty term that induces sparsity on the Grassmann manifold to obtain sparse estimators. The resulting objective function has a closed-form Riemann gradient which facilitates efficient computation of the penalized estimator. We leverage the state-of-the-art trust region algorithm on the Grassmann manifold to compute the proposed estimators efficiently. Simulation studies and a data application demonstrate the proposed likelihood-based estimators perform better than inverse moment-based estimators in terms of both estimation and variable selection accuracy.},
  archive      = {J_SAC},
  author       = {Nghiem, Linh H. and Hui, Francis K. C. and Muller, Samuel and Welsh, A. H.},
  doi          = {10.1007/s11222-023-10357-6},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Likelihood-based surrogate dimension reduction},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate and regression models for directional data
based on projected pólya trees. <em>SAC</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10337-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projected distributions have proved to be useful in the study of circular and directional data. Although any multivariate distribution can be used to produce a projected model, these distributions are typically parametric. In this article we consider a multivariate Pólya tree on $$\mathbb {R}^k$$ and project it to the unit hypersphere $$\mathbb {S}^k$$ to define a new Bayesian nonparametric model for directional data. We study the properties of the proposed model and in particular, concentrate on the implied conditional distributions of some directions given the others to define a directional–directional regression model. We also define a multivariate linear regression model with Pólya tree errors and project it to define a linear-directional regression model. We obtain the posterior characterisation of all models via their full conditional distributions. Metropolis-Hastings steps are required, where random walk proposal distributions are optimised with a novel adaptation scheme. We show the performance of our models with simulated and real datasets.},
  archive      = {J_SAC},
  author       = {Nieto-Barajas, Luis E.},
  doi          = {10.1007/s11222-023-10337-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Multivariate and regression models for directional data based on projected pólya trees},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lévy langevin monte carlo. <em>SAC</em>, <em>34</em>(1),
1–15. (<a href="https://doi.org/10.1007/s11222-023-10345-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analogously to the well-known Langevin Monte Carlo method, in this article we provide a method to sample from a target distribution $$\varvec{\pi }$$ by simulating a solution of a stochastic differential equation. Hereby, the stochastic differential equation is driven by a general Lévy process which—unlike the case of Langevin Monte Carlo—allows for non-smooth targets. Our method will be fully explored in the particular setting of target distributions supported on the half-line $$(0,\infty )$$ and a compound Poisson driving noise. Several illustrative examples conclude the article.},
  archive      = {J_SAC},
  author       = {Oechsler, David},
  doi          = {10.1007/s11222-023-10345-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Lévy langevin monte carlo},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power-law distribution in pieces: A semi-parametric approach
with change point detection. <em>SAC</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10336-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Piecewise models play a crucial role in statistical analysis as they allow the same pattern to be adjusted over different regions of the data, achieving a higher quality of fit than would be obtained by fitting them all at once. The standard piecewise linear distribution assumes that the hazard rate is constant between each change point. However, this assumption may be unrealistic in many applications. To address this issue, we introduce a piecewise distribution based on the power-law model. The proposed semi-parametric distribution boasts excellent properties and features a non-constant hazard function between change points. We discuss parameter estimates using the maximum likelihood estimators (MLEs), which yield closed-form expressions for the estimators and the Fisher information matrix for both complete and randomly censored data. Since MLEs can be biased for small samples, we derived bias-corrected MLEs that are unbiased up to the second order and also have closed-form expressions. We consider a profiled MLE approach to estimate change points and construct a hypothesis test to determine the number of change points. We apply our proposed model to analyze the survival pattern of monarchs in the Pharaoh dynasties. Our results indicate that the piecewise power-law distribution fits the data well, suggesting that the lifespans of pharaonic monarchs exhibit varied survival patterns.},
  archive      = {J_SAC},
  author       = {Ramos, Pedro L. and Jerez-Lillo, Nixon and Segovia, Francisco A. and Egbon, Osafu A. and Louzada, Francisco},
  doi          = {10.1007/s11222-023-10336-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Power-law distribution in pieces: A semi-parametric approach with change point detection},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized expectation model selection algorithm for
latent variable selection in multidimensional item response theory
models. <em>SAC</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10360-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generalized expectation model selection (GEMS) algorithm for latent variable selection in multidimensional item response theory models which are commonly used for identifying the relationships between the latent traits and test items. Under some mild assumptions, we prove the numerical convergence of GEMS for model selection by minimizing the generalized information criteria of observed data in the presence of missing data. For latent variable selection in the multidimensional two-parameter logistic (M2PL) models, we present an efficient implementation of GEMS to minimize the Bayesian information criterion. To ensure parameter identifiability, the variances of all latent traits are assumed to be unity and each latent trait is required to have an item exclusively associated with it. The convergence of GEMS for the M2PL models is verified. Simulation studies show that GEMS is computationally more efficient than the expectation model selection (EMS) algorithm and the expectation maximization based $$L_{1}$$ -penalized method (EML1), and it yields better correct rate of latent variable selection and mean squared error of parameter estimates than the EMS and EML1. The GEMS algorithm is illustrated by analyzing a real dataset related to the Eysenck Personality Questionnaire.},
  archive      = {J_SAC},
  author       = {Shang, Laixu and Zheng, Qian-Zhen and Xu, Ping-Feng and Shan, Na and Tang, Man-Lai},
  doi          = {10.1007/s11222-023-10360-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A generalized expectation model selection algorithm for latent variable selection in multidimensional item response theory models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient probabilistic reconciliation of forecasts for
real-valued and count time series. <em>SAC</em>, <em>34</em>(1), 1–15.
(<a href="https://doi.org/10.1007/s11222-023-10343-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical time series are common in several applied fields. The forecasts for these time series are required to be coherent, that is, to satisfy the constraints given by the hierarchy. The most popular technique to enforce coherence is called reconciliation, which adjusts the base forecasts computed for each time series. However, recent works on probabilistic reconciliation present several limitations. In this paper, we propose a new approach based on conditioning to reconcile any type of forecast distribution. We then introduce a new algorithm, called Bottom-Up Importance Sampling, to efficiently sample from the reconciled distribution. It can be used for any base forecast distribution: discrete, continuous, or in the form of samples, providing a major speedup compared to the current methods. Experiments on several temporal hierarchies show a significant improvement over base probabilistic forecasts.},
  archive      = {J_SAC},
  author       = {Zambon, Lorenzo and Azzimonti, Dario and Corani, Giorgio},
  doi          = {10.1007/s11222-023-10343-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Efficient probabilistic reconciliation of forecasts for real-valued and count time series},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional diffusion maps. <em>SAC</em>, <em>34</em>(1),
1–16. (<a href="https://doi.org/10.1007/s11222-023-10332-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays many real-world datasets can be considered as functional, in the sense that the processes which generate them are continuous. A fundamental property of this type of data is that in theory they belong to an infinite-dimensional space. Although in practice we usually receive finite observations, they are still high-dimensional and hence dimensionality reduction methods are crucial. In this vein, the main state-of-the-art method for functional data analysis is Functional PCA. Nevertheless, this classic technique assumes that the data lie in a linear manifold, and hence it could have problems when this hypothesis is not fulfilled. In this research, attention has been placed on a non-linear manifold learning method: Diffusion Maps. The article explains how to extend this multivariate method to functional data and compares its behavior against Functional PCA over different simulated and real examples.},
  archive      = {J_SAC},
  author       = {Barroso, M. and Alaíz, C. M. and Torrecilla, J. L. and Fernández, A.},
  doi          = {10.1007/s11222-023-10332-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Functional diffusion maps},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of a likelihood ratio ordered family of
distributions. <em>SAC</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10370-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider bivariate observations $$(X_1,Y_1), \ldots , (X_n,Y_n) \in {\mathbb {R}}\times {\mathbb {R}}$$ with unknown conditional distributions $$Q_x$$ of Y, given that $$X = x$$ . The goal is to estimate these distributions under the sole assumption that $$Q_x$$ is isotonic in x with respect to likelihood ratio order. If the observations are identically distributed, a related goal is to estimate the joint distribution $$\mathcal {L}(X,Y)$$ under the sole assumption that it is totally positive of order two. An algorithm is developed which estimates the unknown family of distributions $$(Q_x)_x$$ via empirical likelihood. The benefit of the stronger regularization imposed by likelihood ratio order over the usual stochastic order is evaluated in terms of estimation and predictive performances on simulated as well as real data.},
  archive      = {J_SAC},
  author       = {Mösching, Alexandre and Dümbgen, Lutz},
  doi          = {10.1007/s11222-023-10370-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of a likelihood ratio ordered family of distributions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and locally adaptive bayesian quantile smoothing using
calibrated variational approximations. <em>SAC</em>, <em>34</em>(1),
1–16. (<a href="https://doi.org/10.1007/s11222-023-10327-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantiles are useful characteristics of random variables that can provide substantial information on distributions compared with commonly used summary statistics such as means. In this study, we propose a Bayesian quantile trend filtering method to estimate the non-stationary trend of quantiles. We introduce general shrinkage priors to induce locally adaptive Bayesian inference on trends and mixture representation of the asymmetric Laplace likelihood. To quickly compute the posterior distribution, we develop calibrated mean-field variational approximations to guarantee that the frequentist coverage of credible intervals obtained from the approximated posterior is a specified nominal level. Simulation and empirical studies show that the proposed algorithm is computationally much more efficient than the Gibbs sampler and tends to provide stable inference results, especially for high/low quantiles.},
  archive      = {J_SAC},
  author       = {Onizuka, Takahiro and Hashimoto, Shintaro and Sugasawa, Shonosuke},
  doi          = {10.1007/s11222-023-10327-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Fast and locally adaptive bayesian quantile smoothing using calibrated variational approximations},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based clustering of multiple networks with a
hierarchical algorithm. <em>SAC</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10329-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper tackles the problem of clustering multiple networks, directed or not, that do not share the same set of vertices, into groups of networks with similar topology. A statistical model-based approach based on a finite mixture of stochastic block models is proposed. A clustering is obtained by maximizing the integrated classification likelihood criterion. This is done by a hierarchical agglomerative algorithm, that starts from singleton clusters and successively merges clusters of networks. As such, a sequence of nested clusterings is computed that can be represented by a dendrogram providing valuable insights on the collection of networks. Using a Bayesian framework, model selection is performed in an automated way since the algorithm stops when the best number of clusters is attained. The algorithm is computationally efficient, when carefully implemented. The aggregation of clusters requires a means to overcome the label-switching problem of the stochastic block model and to match the block labels of the networks. To address this problem, a new tool is proposed based on a comparison of the graphons of the associated stochastic block models. The clustering approach is assessed on synthetic data. An application to a set of ecological networks illustrates the interpretability of the obtained results.},
  archive      = {J_SAC},
  author       = {Rebafka, Tabea},
  doi          = {10.1007/s11222-023-10329-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Model-based clustering of multiple networks with a hierarchical algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive study of variational bayes classification for
dense deep neural networks. <em>SAC</em>, <em>34</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s11222-023-10338-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Bayesian deep neural network models are ubiquitous in classification problems; their Markov Chain Monte Carlo based implementation suffers from high computational cost, limiting the use of this powerful technique in large-scale studies. Variational Bayes (VB) has emerged as a competitive alternative to overcome some of these computational issues. This paper focuses on the variational Bayesian deep neural network estimation methodology and discusses the related statistical theory and algorithmic implementations in the context of classification. For a dense deep neural network-based classification, the paper compares and contrasts the true posterior’s consistency and contraction rates and the corresponding variational posterior. Based on the complexity of the deep neural network (DNN), this paper provides an assessment of the loss in classification accuracy due to VB’s use and guidelines on the characterization of the prior distributions and the variational family. The difficulty of the numerical optimization for obtaining the variational Bayes solution has also been quantified as a function of the complexity of the DNN. The development is motivated by an important biomedical engineering application, namely building predictive tools for the transition from mild cognitive impairment to Alzheimer’s disease. The predictors are multi-modal and may involve complex interactive relations.},
  archive      = {J_SAC},
  author       = {Bhattacharya, Shrijita and Liu, Zihuan and Maiti, Tapabrata},
  doi          = {10.1007/s11222-023-10338-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Stat. Comput.},
  title        = {Comprehensive study of variational bayes classification for dense deep neural networks},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Renewable composite quantile method and algorithm for
nonparametric models with streaming data. <em>SAC</em>, <em>34</em>(1),
1–24. (<a href="https://doi.org/10.1007/s11222-023-10352-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in renewable estimations and algorithms for nonparametric models with streaming data. In our method, the nonparametric function of interest is expressed through a functional depending on a weight function and a conditional distribution function (CDF). The CDF is estimated by renewable kernel estimations together with function interpolations, based on which we propose the method of renewable weighted composite quantile regression (WCQR). Then, by fully utilizing the model structure, we obtain new selectors for the weight function, such that the WCQR can achieve asymptotic unbiasness when estimating specific functions in the model. We also propose practical bandwidth selectors for streaming data and find the optimal weight function by minimizing the asymptotic variance. The asymptotical results show that our estimator is almost equivalent to the oracle estimator obtained from the entire data together. Besides, our method also enjoys adaptiveness to error distributions, robustness to outliers, and efficiency in both estimation and computation. Simulation studies and real data analyses further confirm our theoretical findings.},
  archive      = {J_SAC},
  author       = {Chen, Yan and Fang, Shuixin and Lin, Lu},
  doi          = {10.1007/s11222-023-10352-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Renewable composite quantile method and algorithm for nonparametric models with streaming data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian tree-based heterogeneous mediation analysis with a
time-to-event outcome. <em>SAC</em>, <em>34</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11222-023-10340-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis aims at quantifying and explaining the underlying causal mechanism between an exposure and an outcome of interest. In the context of survival analysis, mediation models have been widely used to achieve causal interpretation for the direct and indirect effects on the survival of interest. Although heterogeneity in treatment effect is drawing increasing attention in biomedical studies, none of the existing methods have accommodated the presence of heterogeneous causal pathways pointing to a time-to-event outcome. In this study, we consider a heterogeneous mediation analysis for survival data based on a Bayesian tree-based Cox proportional hazards model with shared topologies. Under the potential outcomes framework, individual-specific conditional direct and indirect effects are derived on the scale of the logarithm of hazards, survival probability, and restricted mean survival time. A Bayesian approach with efficient sampling strategies is developed to estimate the conditional causal effects through the Monte Carlo implementation of the mediation formula. Simulation studies show the satisfactory performance of the proposed method. The proposed model is then applied to an HIV dataset extracted from the ACTG175 study to demonstrate its usage in detecting heterogeneous causal pathways.},
  archive      = {J_SAC},
  author       = {Sun, Rongqian and Song, Xinyuan},
  doi          = {10.1007/s11222-023-10340-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian tree-based heterogeneous mediation analysis with a time-to-event outcome},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian projection pursuit regression. <em>SAC</em>,
<em>34</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10334-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In projection pursuit regression (PPR), a univariate response variable is approximated by the sum of M “ridge functions,” which are flexible functions of one-dimensional projections of a multivariate input variable. Traditionally, optimization routines are used to choose the projection directions and ridge functions via a sequential algorithm, and M is typically chosen via cross-validation. We introduce a novel Bayesian version of PPR, which has the benefit of accurate uncertainty quantification. To infer appropriate projection directions and ridge functions, we apply novel adaptations of methods used for the single ridge function case ( $$M=1$$ ), called the Bayesian Single Index Model; and use a Reversible Jump Markov chain Monte Carlo algorithm to infer the number of ridge functions M. We evaluate the predictive ability of our model in 20 simulated scenarios and for 23 real datasets, in a bake-off against an array of state-of-the-art regression methods. Finally, we generalize this methodology and demonstrate the ability to accurately model multivariate response variables. Its effective performance indicates that Bayesian Projection Pursuit Regression is a valuable addition to the existing regression toolbox.},
  archive      = {J_SAC},
  author       = {Collins, Gavin and Francom, Devin and Rumsey, Kellin},
  doi          = {10.1007/s11222-023-10334-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian projection pursuit regression},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A finite mixture model for multiple dependent competing
risks with applications of automotive warranty claims data.
<em>SAC</em>, <em>34</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10326-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a parametric finite mixture model (FMM) approach to analyze the dependent competing risks data subjected to progressive first-failure censoring and multiple causes of failure. The cause-specific failure times are assumed to be flexibly modeled by the Lehmann family of distributions (also known as the exponentiated distributions) with variation in both distribution parameters. Application of the expectation maximization (EM) algorithm facilitates the maximum likelihood estimation of the model parameters and illuminates the contribution of the censored data. For interval estimation purposes, we resort to using the asymptotic confidence intervals based on the observed Fisher information matrix. Practitioners often prefer employing simpler lifetime distribution in order to facilitate the data modeling process while knowing the true distribution. In this context, the effects of model misspecification are studied based on the p-th quantile when the true distribution is misspecified. An extensive simulation study is performed to validate our proposed model. Finally, an automotive warranty claims data set is used as an illustration to study the effectiveness of our proposed model, assuming some important members of the Lehmann family, like generalized exponential and exponentiated Pareto distributions.},
  archive      = {J_SAC},
  author       = {Prajapati, Deepak and Pal, Ayan and Kundu, Debasis},
  doi          = {10.1007/s11222-023-10326-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {A finite mixture model for multiple dependent competing risks with applications of automotive warranty claims data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving and lossless distributed estimation of
high-dimensional generalized additive mixed models. <em>SAC</em>,
<em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11222-023-10323-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various privacy-preserving frameworks that respect the individual’s privacy in the analysis of data have been developed in recent years. However, available model classes such as simple statistics or generalized linear models lack the flexibility required for a good approximation of the underlying data-generating process in practice. In this paper, we propose an algorithm for a distributed, privacy-preserving, and lossless estimation of generalized additive mixed models (GAMM) using component-wise gradient boosting (CWB). Making use of CWB allows us to reframe the GAMM estimation as a distributed fitting of base learners using the $$L_2$$ -loss. In order to account for the heterogeneity of different data location sites, we propose a distributed version of a row-wise tensor product that allows the computation of site-specific (smooth) effects. Our adaption of CWB preserves all the important properties of the original algorithm, such as an unbiased feature selection and the feasibility to fit models in high-dimensional feature spaces, and yields equivalent model estimates as CWB on pooled data. Next to a derivation of the equivalence of both algorithms, we also showcase the efficacy of our algorithm on a distributed heart disease data set and compare it with state-of-the-art methods.},
  archive      = {J_SAC},
  author       = {Daniel, Schalk and Bernd, Bischl and David, Rügamer},
  doi          = {10.1007/s11222-023-10323-2},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Privacy-preserving and lossless distributed estimation of high-dimensional generalized additive mixed models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized self-updating process for clustering large-scale
data. <em>SAC</em>, <em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11222-023-10355-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the randomized self-updating process (rSUP) algorithm for clustering large-scale data. rSUP is an extension of the self-updating process (SUP) algorithm, which has shown effectiveness in clustering data with characteristics such as noise, varying cluster shapes and sizes, and numerous clusters. However, SUP’s reliance on pairwise dissimilarities between data points makes it computationally inefficient for large-scale data. To address this challenge, rSUP performs location updates within randomly generated data subsets at each iteration. The Law of Large Numbers guarantees that the clustering results of rSUP converge to those of the original SUP as the partition size grows. This paper demonstrates the effectiveness and computational efficiency of rSUP in large-scale data clustering through simulations and real datasets.},
  archive      = {J_SAC},
  author       = {Shiu, Shang-Ying and Chin, Yen-Shiu and Lin, Szu-Han and Chen, Ting-Li},
  doi          = {10.1007/s11222-023-10355-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Randomized self-updating process for clustering large-scale data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifiability of discrete input–output hidden markov
models with external signals. <em>SAC</em>, <em>34</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10364-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a bivariate process $$(X_t,Y_t)_{t\in {\mathbb {Z}}}$$ which, conditionally on a signal $$(W_t)_{t\in {{\mathbb {Z}}}}$$ , is a hidden Markov model whose transition and emission kernels depend on $$(W_t)_{t\in {{\mathbb {Z}}}}$$ . The resulting process $$(X_t,Y_t,W_t)_{t\in {{\mathbb {Z}}}}$$ is referred to as an input–output hidden Markov model or hidden Markov model with external signals. We prove that this model is identifiable and that the associated maximum likelihood estimator is consistent. Introducing an Expectation Maximization-based algorithm, we train and evaluate the performance of this model in several frameworks. In addition to learning dependencies between $$(X_t,Y_t)_{t\in {\mathbb {Z}}}$$ and $$(W_t)_{t\in {{\mathbb {Z}}}}$$ , our approach based on hidden Markov models with external signals also outperforms state-of-the-art algorithms on real-world fashion sequences.},
  archive      = {J_SAC},
  author       = {David, Étienne and Bellot, Jean and Le Corff, Sylvain and Lehéricy, Luc},
  doi          = {10.1007/s11222-023-10364-7},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Identifiability of discrete input–output hidden markov models with external signals},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clusterwise multivariate regression of mixed-type panel
data. <em>SAC</em>, <em>34</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10304-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate panel data of mixed type are routinely collected in many different areas of application, often jointly with additional covariates which complicate the statistical analysis. Moreover, it is often of interest to identify unknown groups of subjects in a study population using such data structure, i.e., to perform clustering. In the Bayesian framework, we propose a finite mixture of multivariate generalised linear mixed effects regression models to cluster numeric, binary, ordinal and categorical panel outcomes jointly. The specification of suitable priors on the model parameters allows for convenient posterior inference based on Markov chain Monte Carlo (MCMC) sampling with data augmentation. This approach allows to classify subjects in the data and new subjects as well as to characterise the cluster-specific models. Model estimation and selection of the number of data clusters are simultaneously performed when approximating the posterior for a single model using MCMC sampling without resorting to multiple model estimations. The performance of the proposed methodology is evaluated in a simulation study. Its application is illustrated on two data sets, one from a longitudinal patient study to infer prognosis groups, and a second one from the Czech part of the EU-SILC survey where households are annually interviewed to obtain insights into changes in their financial capability.},
  archive      = {J_SAC},
  author       = {Vávra, Jan and Komárek, Arnošt and Grün, Bettina and Malsiner-Walli, Gertraud},
  doi          = {10.1007/s11222-023-10304-5},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Clusterwise multivariate regression of mixed-type panel data},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerated gradient methods for sparse statistical learning
with nonconvex penalties. <em>SAC</em>, <em>34</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10371-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nesterov’s accelerated gradient (AG) is a popular technique to optimize objective functions comprising two components: a convex loss and a penalty function. While AG methods perform well for convex penalties, such as the LASSO, convergence issues may arise when it is applied to nonconvex penalties, such as SCAD. A recent proposal generalizes Nesterov’s AG method to the nonconvex setting. The proposed algorithm requires specification of several hyperparameters for its practical application. Aside from some general conditions, there is no explicit rule for selecting the hyperparameters, and how different selection can affect convergence of the algorithm. In this article, we propose a hyperparameter setting based on the complexity upper bound to accelerate convergence, and consider the application of this nonconvex AG algorithm to high-dimensional linear and logistic sparse learning problems. We further establish the rate of convergence and present a simple and useful bound to characterize our proposed optimal damping sequence. Simulation studies show that convergence can be made, on average, considerably faster than that of the conventional proximal gradient algorithm. Our experiments also show that the proposed method generally outperforms the current state-of-the-art methods in terms of signal recovery.},
  archive      = {J_SAC},
  author       = {Yang, Kai and Asgharian, Masoud and Bhatnagar, Sahir},
  doi          = {10.1007/s11222-023-10371-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Accelerated gradient methods for sparse statistical learning with nonconvex penalties},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology-driven goodness-of-fit tests in arbitrary
dimensions. <em>SAC</em>, <em>34</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10333-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper adopts a tool from computational topology, the Euler characteristic curve (ECC) of a sample, to perform one- and two-sample goodness of fit tests. We call our procedure TopoTests. The presented tests work for samples of arbitrary dimension, having comparable power to the state-of-the-art tests in the one-dimensional case. It is demonstrated that the type I error of TopoTests can be controlled and their type II error vanishes exponentially with increasing sample size. Extensive numerical simulations of TopoTests are conducted to demonstrate their power for samples of various sizes.},
  archive      = {J_SAC},
  author       = {Dłotko, Paweł and Hellmer, Niklas and Stettner, Łukasz and Topolnicki, Rafał},
  doi          = {10.1007/s11222-023-10333-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Topology-driven goodness-of-fit tests in arbitrary dimensions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous analysis for clustered data using grouped
finite mixture models. <em>SAC</em>, <em>34</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10353-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to observe significant heterogeneity in clustered data across scientific fields. Cluster-wise conditional distributions are widely used to explore variations and relationships within and among clusters. This paper aims to capture such heterogeneity by employing cluster-wise finite mixture models. To address the heterogeneity among clusters, we introduce latent group structure and incorporate heterogeneous mixing proportions across different groups, accommodating the diverse characteristics observed in the data. The specific number of groups and their membership are unknown. To identify the latent group structure, we employ concave penalty functions to the pairwise differences of the preliminary consistent estimators for the mixing proportions. This approach enables the automatic division of clusters into finite subgroups. Theoretical results demonstrate that as the number of clusters and cluster sizes tend to infinity, the true latent group structure can be recovered with probability close to one, and the post-classification estimators exhibit oracle efficiency. We support our proposed approach’s performance and applicability through extensive simulations and analysis of basic consumption expenditure among urban households in China.},
  archive      = {J_SAC},
  author       = {Liang, Chunhui and Ma, Wenqing},
  doi          = {10.1007/s11222-023-10353-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneous analysis for clustered data using grouped finite mixture models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ECOPICA: Empirical copula-based independent component
analysis. <em>SAC</em>, <em>34</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10368-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a non-parametric ICA method, called ECOPICA, which describes the joint distribution of data by empirical copulas and measures the dependence between recovery signals by an independent test statistic. We employ the grasshopper algorithm to optimize the proposed objective function. Several acceleration tricks are further designed to enhance the computational efficiency of the proposed algorithm under the parallel computing framework. Our simulation and empirical analysis show that ECOPICA produces better and more robust recovery performances than other well-known ICA approaches for various source distribution shapes, especially when the source distribution is skewed or near-Gaussian.},
  archive      = {J_SAC},
  author       = {Pi, Hung-Kai and Guo, Mei-Hui and Chen, Ray-Bing and Huang, Shih-Feng},
  doi          = {10.1007/s11222-023-10368-3},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {ECOPICA: Empirical copula-based independent component analysis},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online estimation and community detection of network point
processes for event streams. <em>SAC</em>, <em>34</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11222-023-10342-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common goal in network modeling is to uncover the latent community structure present among nodes. For many real-world networks, the true connections consist of events arriving as streams, which are then aggregated to form edges, ignoring the dynamic temporal component. A natural way to take account of these temporal dynamics of interactions is to use point processes as the foundation of network models for community detection. Computational complexity hampers the scalability of such approaches to large sparse networks. To circumvent this challenge, we propose a fast online variational inference algorithm for estimating the latent structure underlying dynamic event arrivals on a network, using continuous-time point process latent network models. We describe this procedure for network models capturing community structure. This structure can be learned as new events are observed on the network, updating the inferred community assignments. We investigate the theoretical properties of such an inference scheme, and provide regret bounds on the loss function of this procedure. The proposed inference procedure is then thoroughly compared, using both simulation studies and real data, to non-online variants. We demonstrate that online inference can obtain comparable performance, in terms of community recovery, to non-online variants, while realising computational gains. Our proposed inference framework can also be readily modified to incorporate other popular network structures.},
  archive      = {J_SAC},
  author       = {Fang, Guanhua and Ward, Owen G. and Zheng, Tian},
  doi          = {10.1007/s11222-023-10342-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Online estimation and community detection of network point processes for event streams},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point process simulation of generalised hyperbolic lévy
processes. <em>SAC</em>, <em>34</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11222-023-10344-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalised hyperbolic (GH) processes are a class of stochastic processes that are used to model the dynamics of a wide range of complex systems that exhibit heavy-tailed behavior, including systems in finance, economics, biology, and physics. In this paper, we present novel simulation methods based on subordination with a generalised inverse Gaussian (GIG) process and using a generalised shot-noise representation that involves random thinning of infinite series of decreasing jump sizes. Compared with our previous work on GIG processes, we provide tighter bounds for the construction of rejection sampling ratios, leading to improved acceptance probabilities in simulation. Furthermore, we derive methods for the adaptive determination of the number of points required in the associated random series using concentration inequalities. Residual small jumps are then approximated using an appropriately scaled Brownian motion term with drift. Finally the rejection sampling steps are made significantly more computationally efficient through the use of squeezing functions based on lower and upper bounds on the Lévy density. Experimental results are presented illustrating the strong performance under various parameter settings and comparing the marginal distribution of the GH paths with exact simulations of GH random variates. The new simulation methodology is made available to researchers through the publication of a Python code repository.},
  archive      = {J_SAC},
  author       = {Kındap, Yaman and Godsill, Simon},
  doi          = {10.1007/s11222-023-10344-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Point process simulation of generalised hyperbolic lévy processes},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cauchy robust principal component analysis with applications
to high-dimensional data sets. <em>SAC</em>, <em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10328-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a standard dimensionality reduction technique used in various research and applied fields. From an algorithmic point of view, classical PCA can be formulated in terms of operations on a multivariate Gaussian likelihood. As a consequence of the implied Gaussian formulation, the principal components are not robust to outliers. In this paper, we propose a modified formulation, based on the use of a multivariate Cauchy likelihood instead of the Gaussian likelihood, which has the effect of robustifying the principal components. We present an algorithm to compute these robustified principal components. We additionally derive the relevant influence function of the first component and examine its theoretical properties. Simulation experiments on high-dimensional datasets demonstrate that the estimated principal components based on the Cauchy likelihood typically outperform, or are on a par with, existing robust PCA techniques. Moreover, the Cauchy PCA algorithm we have used has much lower computational cost in very high dimensional settings than the other public domain robust PCA methods we consider.},
  archive      = {J_SAC},
  author       = {Fayomi, Aisha and Pantazis, Yannis and Tsagris, Michail and Wood, Andrew T. A.},
  doi          = {10.1007/s11222-023-10328-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Cauchy robust principal component analysis with applications to high-dimensional data sets},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dimension-independent spectral gap of polar slice sampling.
<em>SAC</em>, <em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10335-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polar slice sampling, a Markov chain construction for approximate sampling, performs, under suitable assumptions on the target and initial distribution, provably independent of the state space dimension. We extend the aforementioned result of Roberts and Rosenthal (Stoch Model 18(2):257–280, 2002) by developing a theory which identifies conditions, in terms of a generalized level set function, that imply an explicit lower bound on the spectral gap even in a general slice sampling context. Verifying the identified conditions for polar slice sampling yields a lower bound of 1/2 on the spectral gap for arbitrary dimension if the target density is rotationally invariant, log-concave along rays emanating from the origin and sufficiently smooth. The general theoretical result is potentially applicable beyond the polar slice sampling framework.},
  archive      = {J_SAC},
  author       = {Rudolf, Daniel and Schär, Philip},
  doi          = {10.1007/s11222-023-10335-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Dimension-independent spectral gap of polar slice sampling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalised likelihood profiles for models with intractable
likelihoods. <em>SAC</em>, <em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10361-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood profiling is an efficient and powerful frequentist approach for parameter estimation, uncertainty quantification and practical identifiablity analysis. Unfortunately, these methods cannot be easily applied for stochastic models without a tractable likelihood function. Such models are typical in many fields of science, rendering these classical approaches impractical in these settings. To address this limitation, we develop a new approach to generalising the methods of likelihood profiling for situations when the likelihood cannot be evaluated but stochastic simulations of the assumed data generating process are possible. Our approach is based upon recasting developments from generalised Bayesian inference into a frequentist setting. We derive a method for constructing generalised likelihood profiles and calibrating these profiles to achieve desired frequentist coverage for a given coverage level. We demonstrate the performance of our method on realistic examples from the literature and highlight the capability of our approach for the purpose of practical identifability analysis for models with intractable likelihoods.},
  archive      = {J_SAC},
  author       = {Warne, David J. and Maclaren, Oliver J. and Carr, Elliot J. and Simpson, Matthew J. and Drovandi, Christopher},
  doi          = {10.1007/s11222-023-10361-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Generalised likelihood profiles for models with intractable likelihoods},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation and simulation of multivariate dickman
distributions and vervaat perpetuities. <em>SAC</em>, <em>34</em>(1),
1–12. (<a href="https://doi.org/10.1007/s11222-023-10349-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multivariate extension of the Dickman distribution was recently introduced, but very few properties have been studied. We discuss several properties with an emphasis on simulation. Further, we introduce and study a multivariate extension of the more general class of Vervaat perpetuities and derive a number of properties and representations. Most of our results are presented in the even more general context of so-called $$\alpha $$ -times self-decomposable distributions.},
  archive      = {J_SAC},
  author       = {Grabchak, Michael and Zhang, Xingnan},
  doi          = {10.1007/s11222-023-10349-6},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Representation and simulation of multivariate dickman distributions and vervaat perpetuities},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pareto-efficient designs for multi- and mixed-level
supersaturated designs. <em>SAC</em>, <em>34</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11222-023-10354-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supersaturated designs are used in science and engineering to efficiently explore a large number of factors with a limited number of runs. It is not uncommon in engineering to consider a few, if not all, factors at more than two levels. Multi- and mixed-level supersaturated designs may, therefore, be handy. While the two-level supersaturated designs are widely studied, the literature on multi- and mixed-level designs is still scarce. A recent paper establishes that the group LASSO should be preferred as an analysis method because it can retain the natural group structure of multi- and mixed-level designs. A few optimality criteria for such designs also exist in the literature. These criteria typically aim to find designs that maximize average pairwise orthogonality. However, the literature lacks guidance on the better or ‘right’ optimality criteria from a screening perspective. In addition, the existing optimal designs are often balanced and are rarely available. We propose two new optimality criteria based on the large-sample properties of group LASSO. Our criteria fill the gap in the literature by providing design selection criteria that are directly related to the preferred analysis method. We then construct Pareto-efficient designs on the two new criteria and demonstrate that (a) our optimality criteria can be used to order existing optimal designs on their screening performance, (b) the Pareto-efficient designs are often better than or as good as the existing optimal designs, and (c) the Pareto-efficient designs can be constructed using a coordinate exchange algorithm and are, therefore, available for any choice of the number of runs, factors, and levels. A repository of three- and four-level designs with the number of runs between 8 and 16 is also provided.},
  archive      = {J_SAC},
  author       = {Singh, Rakhi},
  doi          = {10.1007/s11222-023-10354-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Pareto-efficient designs for multi- and mixed-level supersaturated designs},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting and diagnosing prior and likelihood sensitivity
with power-scaling. <em>SAC</em>, <em>34</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s11222-023-10366-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach using importance sampling to estimate properties of posteriors resulting from power-scaling the prior or likelihood. On this basis, we suggest a diagnostic that can indicate the presence of prior-data conflict or likelihood noninformativity and discuss limitations to this power-scaling approach. The approach can be easily included in Bayesian workflows with minimal effort by the model builder and we present an implementation in our new R package priorsense. We further demonstrate the workflow on case studies of real data using models varying in complexity from simple linear models to Gaussian process models.},
  archive      = {J_SAC},
  author       = {Kallioinen, Noa and Paananen, Topi and Bürkner, Paul-Christian and Vehtari, Aki},
  doi          = {10.1007/s11222-023-10366-5},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Detecting and diagnosing prior and likelihood sensitivity with power-scaling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian high-dimensional covariate selection in non-linear
mixed-effects models using the SAEM algorithm. <em>SAC</em>,
<em>34</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11222-023-10367-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional variable selection, with many more covariates than observations, is widely documented in standard regression models, but there are still few tools to address it in non-linear mixed-effects models where data are collected repeatedly on several individuals. In this work, variable selection is approached from a Bayesian perspective and a selection procedure is proposed, combining the use of a spike-and-slab prior and the Stochastic Approximation version of the Expectation Maximisation (SAEM) algorithm. Similarly to Lasso regression, the set of relevant covariates is selected by exploring a grid of values for the penalisation parameter. The SAEM approach is much faster than a classical Markov chain Monte Carlo algorithm and our method shows very good selection performances on simulated data. Its flexibility is demonstrated by implementing it for a variety of nonlinear mixed effects models. The usefulness of the proposed method is illustrated on a problem of genetic markers identification, relevant for genomic-assisted selection in plant breeding.},
  archive      = {J_SAC},
  author       = {Naveau, Marion and Kon Kam King, Guillaume and Rincent, Renaud and Sansonnet, Laure and Delattre, Maud},
  doi          = {10.1007/s11222-023-10367-4},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian high-dimensional covariate selection in non-linear mixed-effects models using the SAEM algorithm},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the f-divergences between densities of a multivariate
location or scale family. <em>SAC</em>, <em>34</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11222-023-10373-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first extend the result of Ali and Silvey [J R Stat Soc Ser B, 28:131–142, 1966] who proved that any f-divergence between two isotropic multivariate Gaussian distributions amounts to a corresponding strictly increasing scalar function of their corresponding Mahalanobis distance. We give sufficient conditions on the standard probability density function generating a multivariate location family and the function generator f in order to generalize this result. This property is useful in practice as it allows to compare exactly f-divergences between densities of these location families via their corresponding Mahalanobis distances, even when the f-divergences are not available in closed-form as it is the case, for example, for the Jensen–Shannon divergence or the total variation distance between densities of a normal location family. Second, we consider f-divergences between densities of multivariate scale families: We recall Ali and Silvey ’s result that for normal scale families we get matrix spectral divergences, and we extend this result to densities of a scale family.},
  archive      = {J_SAC},
  author       = {Nielsen, Frank and Okamura, Kazuki},
  doi          = {10.1007/s11222-023-10373-6},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {On the f-divergences between densities of a multivariate location or scale family},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NuZZ: Numerical zig-zag for general models. <em>SAC</em>,
<em>34</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10363-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) is a key algorithm in computational statistics, and as datasets grow larger and models grow more complex, many popular MCMC algorithms become too computationally expensive to be practical. Recent progress has been made on this problem through development of MCMC algorithms based on Piecewise Deterministic Markov Processes (PDMPs), irreversible processes which can be engineered to converge at a rate which is independent of the size of the dataset. While there has understandably been a surge of theoretical studies following these results, PDMPs have so far only been implemented for models where certain gradients can be bounded in closed form, which is not possible in many relevant statistical problems. Furthermore, there has been substantionally less focus on practical implementation, or the efficiency of PDMP dynamics in exploring challenging densities. Focusing on the Zig-Zag process, we present the Numerical Zig-Zag (NuZZ) algorithm, which is applicable to general statistical models without the need for bounds on the gradient of the log posterior. This allows us to perform numerical experiments on: (i) how the Zig-Zag dynamics behaves on some test problems with common challenging features; and (ii) how the error between the target and sampled distributions evolves as a function of computational effort for different MCMC algorithms including NuZZ. Moreover, due to the specifics of the NuZZ algorithms, we are able to give an explicit bound on the Wasserstein distance between the exact posterior and its numerically perturbed counterpart in terms of the user-specified numerical tolerances of NuZZ.},
  archive      = {J_SAC},
  author       = {Pagani, Filippo and Chevallier, Augustin and Power, Sam and House, Thomas and Cotter, Simon},
  doi          = {10.1007/s11222-023-10363-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {NuZZ: Numerical zig-zag for general models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic variational inference for GARCH models.
<em>SAC</em>, <em>34</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10356-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic variational inference algorithms are derived for fitting various heteroskedastic time series models. We examine Gaussian, t, and skewed t response GARCH models and fit these using Gaussian variational approximating densities. We implement efficient stochastic gradient ascent procedures based on the use of control variates or the reparameterization trick and demonstrate that the proposed implementations provide a fast and accurate alternative to Markov chain Monte Carlo sampling. Additionally, we present sequential updating versions of our variational algorithms, which are suitable for efficient portfolio construction and dynamic asset allocation.},
  archive      = {J_SAC},
  author       = {Xuan, Hanwen and Maestrini, Luca and Chen, Feng and Grazian, Clara},
  doi          = {10.1007/s11222-023-10356-7},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic variational inference for GARCH models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-adaptive dimension reduction for functional data via
penalized low-rank approximation. <em>SAC</em>, <em>34</em>(1), 1–19.
(<a href="https://doi.org/10.1007/s11222-023-10348-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a data-adaptive nonparametric dimension reduction tool to obtain a low-dimensional approximation of functional data contaminated by erratic measurement errors following symmetric or asymmetric distributions. We propose to apply robust submatrix completion techniques to matrices consisting of coefficients of basis functions calculated by projecting the observed trajectories onto a given orthogonal basis set. In this process, we use a composite asymmetric Huber loss function to accommodate domain-specific erratic behaviors in a data-adaptive manner. We further incorporate the $$L_1$$ penalty to regularize the smoothness of latent factor curves. The proposed method can also be applied to partially observed functional data, where each trajectory contains individual-specific missing segments. Moreover, since our method does not require estimating the covariance operator, the extension to any dimensional functional data observed over a continuum is straightforward. We demonstrate the empirical performance in estimating lower-dimensional space and reconstruction of trajectories of the proposed method through simulation studies. We then apply the proposed method to two real datasets, one-dimensional Advanced Metering Infrastructure (AMI) data in South Korea and two-dimensional max precipitation spatial data collected in North America and South America.},
  archive      = {J_SAC},
  author       = {Park, Yeonjoo and Oh, Hee-Seok and Lim, Yaeji},
  doi          = {10.1007/s11222-023-10348-7},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A data-adaptive dimension reduction for functional data via penalized low-rank approximation},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapping multiple systems estimates to account for
model selection. <em>SAC</em>, <em>34</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s11222-023-10346-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple systems estimation using a Poisson loglinear model is a standard approach to quantifying hidden populations where data sources are based on lists of known cases. Information criteria are often used for selecting between the large number of possible models. Confidence intervals are often reported conditional on the model selected, providing an over-optimistic impression of estimation accuracy. A bootstrap approach is a natural way to account for the model selection. However, because the model selection step has to be carried out for every bootstrap replication, there may be a high or even prohibitive computational burden. We explore the merit of modifying the model selection procedure in the bootstrap to look only among a subset of models, chosen on the basis of their information criterion score on the original data. This provides large computational gains with little apparent effect on inference. We also incorporate rigorous and economical ways of approaching issues of the existence of estimators when applying the method to sparse data tables.},
  archive      = {J_SAC},
  author       = {Silverman, Bernard W. and Chan, Lax and Vincent, Kyle},
  doi          = {10.1007/s11222-023-10346-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Bootstrapping multiple systems estimates to account for model selection},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
