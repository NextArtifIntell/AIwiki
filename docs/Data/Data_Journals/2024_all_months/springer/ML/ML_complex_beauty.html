<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---294">ML - 294</h2>
<ul>
<li><details>
<summary>
(2024a). Correction to: Extracting automata from recurrent neural
networks using queries and counterexamples (extended version).
<em>ML</em>, <em>113</em>(11), 8769. (<a
href="https://doi.org/10.1007/s10994-024-06628-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  doi          = {10.1007/s10994-024-06628-6},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8769},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Extracting automata from recurrent neural networks using queries and counterexamples (extended version)},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for laryngoscopy in vocal fold
diseases: A review of dataset, technology, and ethics. <em>ML</em>,
<em>113</em>(11), 8749–8767. (<a
href="https://doi.org/10.1007/s10994-024-06602-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laryngoscopy plays a crucial role in providing essential visual access to the larynx, especially vocal folds, for diagnosis and treatment interventions. The field of laryngoscopy is witnessing remarkable advancements driven by artificial intelligence (AI) and deep learning, particularly in diagnosing vocal fold disorders. This paper delves into a comprehensive analysis of diverse publicly available laryngoscopy image datasets and cutting-edge deep learning techniques, demonstrating their immense potential to revolutionize diagnostic accuracy and efficiency. However, the ethical and legal challenges surrounding AI in healthcare cannot be overlooked. We meticulously examine critical considerations such as dataset collection, algorithm bias, and responsible clinical application. By addressing these concerns, we emphasize the pivotal role AI can play while ensuring fairness, trust, and adherence to medical ethics. Our aim is to foster a comprehensive understanding of both the potential and the ethical considerations for implementing AI in laryngoscopy. This responsible approach will ultimately lead to improved patient outcomes and a stronger foundation for medical ethics in the age of AI.},
  archive      = {J_ML},
  author       = {Dao, Thao Thi Phuong and Nguyen, Tan-Cong and Huynh, Viet-Tham and Bui, Xuan-Hai and Le, Trung-Nghia and Tran, Minh-Triet},
  doi          = {10.1007/s10994-024-06602-2},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8749-8767},
  shortjournal = {Mach. Learn.},
  title        = {Artificial intelligence for laryngoscopy in vocal fold diseases: A review of dataset, technology, and ethics},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and provable online reduced rank regression via
online gradient descent. <em>ML</em>, <em>113</em>(11), 8711–8748. (<a
href="https://doi.org/10.1007/s10994-024-06622-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Reduced Rank Regression (RRR) model is frequently employed in machine learning. It increases efficiency and interpretability by adding a low-rank restriction to the coefficient matrix, which can be used to cut down on the number of parameters. In this paper, we study the RRR issue in an online setting. Only a small batch of data can be utilized each time, arriving in a stream. Previous analogous methods have relied on conventional least squares estimation, which is inefficient and does not theoretically guarantee convergence rate or build connections with offline strategies. We proposed an efficient online RRR algorithm based on non-convex online gradient descent. More importantly, based on a constant order batch size and appropriate initialization, we theoretically prove the convergence result of the mean estimation error generated by our algorithm. Our result achieves an optimal rate of up to a logarithmic factor. We also propose an accelerated version of our algorithm. Our methods compete with the existing method in terms of accuracy and calculation speed in numerical simulations and real applications.},
  archive      = {J_ML},
  author       = {Liu, Xiao and Liu, Weidong and Mao, Xiaojun},
  doi          = {10.1007/s10994-024-06622-y},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8711-8748},
  shortjournal = {Mach. Learn.},
  title        = {Efficient and provable online reduced rank regression via online gradient descent},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a foundation large events model for soccer.
<em>ML</em>, <em>113</em>(11), 8687–8709. (<a
href="https://doi.org/10.1007/s10994-024-06606-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Large Events Model (LEM) for soccer, a novel deep learning framework for generating and analyzing soccer matches. The framework can simulate games from a given game state, with its primary output being the ensuing probabilities and events from multiple simulations. These can provide insights into match dynamics and underlying mechanisms. We discuss the framework’s design, features, and methodologies, including model optimization, data processing, and evaluation techniques. The models within this framework are developed to predict specific aspects of soccer events, such as event type, success likelihood, and further details. In an applied context, we showcase the estimation of xP+, a metric estimating a player’s contribution to the team’s points earned. This work ultimately enhances the field of sports event prediction and practical applications and emphasizes the potential for this kind of method.},
  archive      = {J_ML},
  author       = {Mendes-Neves, Tiago and Meireles, Luís and Mendes-Moreira, João},
  doi          = {10.1007/s10994-024-06606-y},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8687-8709},
  shortjournal = {Mach. Learn.},
  title        = {Towards a foundation large events model for soccer},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attacks on neural networks through canonical
riemannian foliations. <em>ML</em>, <em>113</em>(11), 8655–8686. (<a
href="https://doi.org/10.1007/s10994-024-06624-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are known to be vulnerable to adversarial attacks. Adversarial learning is therefore becoming a crucial task. We propose a new vision on neural network robustness using Riemannian geometry and foliation theory. The idea is illustrated by creating a new adversarial attack that takes into account the curvature of the data space. This new adversarial attack, called the two-step spectral attack, is a piece-wise linear approximation of a geodesic in the data space. The data space is treated as a (degenerate) Riemannian manifold equipped with the pullback of the Fisher Information Metric (FIM) of the neural network. In most cases, this metric is only semi-definite and its kernel becomes a central object to study. A canonical foliation is derived from this kernel. The curvature of transverse leaves gives the appropriate correction to get a two-step approximation of the geodesic and hence a new efficient adversarial attack. The method is first illustrated on a 2D toy example in order to visualize the neural network foliation and the corresponding attacks. Next, we report numerical results on the MNIST and CIFAR10 datasets with the proposed technique and state of the art attacks presented by Zhao et al. (in: Proceedings of the AAAI conference on artificial intelligence, vol 33. pp 5869–5876, 2019) (OSSA) and Croce and Hein (in: III HD, Singh A (eds) Proceedings of machine learning research, vol 119, PMLR, Cambridge, pp 2206–2216, https://proceedings.mlr.press/v119/croce20b.html, 2020) (AutoAttack). The results show that the proposed attack is more efficient at all levels of available budget for the attack (norm of the attack), confirming that the curvature of the transverse neural network FIM foliation plays an important role in the robustness of neural networks. The main objective and interest of this study is to provide a mathematical understanding of the geometrical issues at play in the data space when constructing efficient attacks on neural networks.},
  archive      = {J_ML},
  author       = {Tron, Eliot and Couëllan, Nicolas and Puechmorel, Stéphane},
  doi          = {10.1007/s10994-024-06624-w},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8655-8686},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial attacks on neural networks through canonical riemannian foliations},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to rank anomalies: Scalar performance criteria and
maximization of rank statistics. <em>ML</em>, <em>113</em>(11),
8623–8653. (<a
href="https://doi.org/10.1007/s10994-024-06609-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to collect and store ever more massive data, unlabeled in many cases, has been accompanied by the need to process them efficiently in order to extract relevant information and possibly design solutions based on the latter. In various situations, the vast majority of the observations exhibit the same behavior, while a small proportion deviates from it. Detecting these outlier observations (or equivalently defined as anomalies) is now one of the major challenges for machine learning applications (e.g. fraud detection or predictive maintenance). We propose here a novel methodology for outlier/anomaly detection, by learning a scoring function defined on the feature space allowing for ranking the observations by degree of abnormality. The scoring function is built through maximization of an empirical performance criterion taking the form of a (two-sample) linear rank statistic. We show that bipartite ranking algorithms can thus be used to learn nearly optimal scoring function with provable theoretical guarantees. We illustrate our methodology with numerical experiments based on open access online code.},
  archive      = {J_ML},
  author       = {Limnios, Myrto and Noiry, Nathan and Clémençon, Stephan},
  doi          = {10.1007/s10994-024-06609-9},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8623-8653},
  shortjournal = {Mach. Learn.},
  title        = {Learning to rank anomalies: Scalar performance criteria and maximization of rank statistics},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change detection and adaptation in multi-target regression
on data streams. <em>ML</em>, <em>113</em>(11), 8585–8622. (<a
href="https://doi.org/10.1007/s10994-024-06621-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential characteristic of data streams is the possibility of occurrence of concept drift, i.e., change in the distribution of the data in the stream over time. The capability to detect and adapt to changes in data stream mining methods is thus a necessity. While methods for multi-target prediction on data streams have recently appeared, they have largely remained without such capability. In this paper, we propose novel methods for change detection and adaptation in the context of incremental online learning of decision trees for multi-target regression. One of the approaches we propose is ensemble based, while the other uses the Page–Hinckley test. We perform an extensive evaluation of the proposed methods on real-world and artificial data streams and show their effectiveness. We also demonstrate their utility on a case study from spacecraft operations, where cosmic events can cause change and demand an appropriate and timely positioning of the space craft.},
  archive      = {J_ML},
  author       = {Stevanoski, Bozhidar and Kostovska, Ana and Panov, Panče and Džeroski, Sašo},
  doi          = {10.1007/s10994-024-06621-z},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8585-8622},
  shortjournal = {Mach. Learn.},
  title        = {Change detection and adaptation in multi-target regression on data streams},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning differentiable logic programs for abstract visual
reasoning. <em>ML</em>, <em>113</em>(11), 8533–8584. (<a
href="https://doi.org/10.1007/s10994-024-06610-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reasoning is essential for building intelligent agents that understand the world and perform problem-solving beyond perception. Differentiable forward reasoning has been developed to integrate reasoning with gradient-based machine learning paradigms. However, due to the memory intensity, most existing approaches do not bring the best of the expressivity of first-order logic, excluding a crucial ability to solve abstract visual reasoning, where agents need to perform reasoning by using analogies on abstract concepts in different scenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN), which is a graph-based differentiable forward reasoner, passing messages in a memory-efficient manner and handling structured programs with functors. Moreover, we propose a computationally-efficient structure learning algorithm to perform explanatory program induction on complex visual scenes. To evaluate, in addition to conventional visual reasoning tasks, we propose a new task, visual reasoning behind-the-scenes, where agents need to learn abstract programs and then answer queries by imagining scenes that are not observed. We empirically demonstrate that NEUMANN solves visual reasoning tasks efficiently, outperforming neural, symbolic, and neuro-symbolic baselines.},
  archive      = {J_ML},
  author       = {Shindo, Hikaru and Pfanschilling, Viktor and Dhami, Devendra Singh and Kersting, Kristian},
  doi          = {10.1007/s10994-024-06610-2},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8533-8584},
  shortjournal = {Mach. Learn.},
  title        = {Learning differentiable logic programs for abstract visual reasoning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient private SCO for heavy-tailed data via averaged
clipping. <em>ML</em>, <em>113</em>(11), 8487–8532. (<a
href="https://doi.org/10.1007/s10994-024-06617-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic convex optimization for heavy-tailed data with the guarantee of being differentially private (DP). Most prior works on differentially private stochastic convex optimization for heavy-tailed data are either restricted to gradient descent (GD) or performed multi-times clipping on stochastic gradient descent (SGD), which is inefficient for large-scale problems. In this paper, we consider a one-time clipping strategy and provide principled analyses of its bias and private mean estimation. We establish new convergence results and improved complexity bounds for the proposed algorithm called AClipped-dpSGD for constrained and unconstrained convex problems. We also extend our convergent analysis to the strongly convex case and non-smooth case (which works for generalized smooth objectives with H $$\ddot{\text {o}}$$ lder-continuous gradients). All the above results are guaranteed with a high probability for heavy-tailed data. Numerical experiments are conducted to justify the theoretical improvement.},
  archive      = {J_ML},
  author       = {Jin, Chenhan and Zhou, Kaiwen and Han, Bo and Cheng, James and Zeng, Tieyong},
  doi          = {10.1007/s10994-024-06617-9},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8487-8532},
  shortjournal = {Mach. Learn.},
  title        = {Efficient private SCO for heavy-tailed data via averaged clipping},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random fourier features for asymmetric kernels. <em>ML</em>,
<em>113</em>(11), 8459–8485. (<a
href="https://doi.org/10.1007/s10994-024-06626-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random Fourier features (RFFs) method is a powerful and popular technique in kernel approximation for scalability of kernel methods. The theoretical foundation of RFFs is based on the Bochner theorem (Bochner in Harmonic Analysis and the Theory of Probability, University of California Press, 1995) that relates symmetric, positive definite (PD) functions to probability measures. This condition naturally excludes asymmetric functions with a wide range applications in practice, e.g., directed graphs, conditional probability, and asymmetric kernels. Nevertheless, understanding asymmetric functions (kernels) and its scalability via RFFs is unclear both theoretically and empirically. In this paper, we introduce a complex measure with the real and imaginary parts corresponding to four finite positive measures, which expands the application scope of the Bochner theorem. By doing so, this framework allows for handling classical symmetric, PD kernels via one positive measure; symmetric, non-positive definite kernels via signed measures; and asymmetric kernels via complex measures, thereby unifying them into a general framework by RFFs, named AsK-RFFs. Such approximation scheme via complex measures enjoys theoretical guarantees in the perspective of uniform convergence. In algorithmic implementation, to speed up the kernel approximation process, which is expensive due to the calculation of total masses, we propose a subset-based fast estimation method. This method focuses on optimizing total masses within a sub-training set, effectively transforming the numerical integration for total masses into quadratic programming in three-dimension with low time complexity. AsK-RFFs provides two explicit feature mappings to approximate an asymmetric kernel. These mappings can be utilized in classifiers within the framework of AsK-LS (He et al. in IEEE Trans Pattern Anal Machine Intell 45(8):10044–10054), fulfilling the purpose of using asymmetric kernels in machine learning applications. Our AsK-RFFs method is empirically validated on several typical large-scale datasets and achieves promising kernel approximation performance, which demonstrates the effectiveness of AsK-RFFs.},
  archive      = {J_ML},
  author       = {He, Mingzhen and He, Fan and Liu, Fanghui and Huang, Xiaolin},
  doi          = {10.1007/s10994-024-06626-8},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8459-8485},
  shortjournal = {Mach. Learn.},
  title        = {Random fourier features for asymmetric kernels},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fedpower: Privacy-preserving distributed eigenspace
estimation. <em>ML</em>, <em>113</em>(11), 8427–8458. (<a
href="https://doi.org/10.1007/s10994-024-06620-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eigenspace estimation is a fundamental tool in data analytics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually involves data that come from and belong to different organizations. The low communication power and possible data privacy breaches make the eigenspace estimation challenging. To address these issues, we propose a class of algorithms called FedPower within the federated learning (FL) framework. FedPower leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with Orthogonal Procrustes Transformation (OPT) for better alignment. We add Gaussian noise in each iteration to ensure strong privacy protection by adopting the notion of differential privacy (DP). We provide convergence bounds for FedPower composed of different interpretable terms corresponding to the effects of Gaussian noise, parallelization, and random sampling of local machines. Additionally, we conduct experiments to demonstrate the effectiveness of our proposed algorithms.},
  archive      = {J_ML},
  author       = {Guo, Xiao and Li, Xiang and Chang, Xiangyu and Wang, Shusen and Zhang, Zhihua},
  doi          = {10.1007/s10994-024-06620-0},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8427-8458},
  shortjournal = {Mach. Learn.},
  title        = {Fedpower: Privacy-preserving distributed eigenspace estimation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning curves for decision making in supervised machine
learning: A survey. <em>ML</em>, <em>113</em>(11), 8371–8425. (<a
href="https://doi.org/10.1007/s10994-024-06619-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations. Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection. For instance, learning curves can be used to model the performance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process. Various learning curve models have been proposed to use learning curves for decision making. Some of these models answer the binary decision question of whether a given algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorises learning curve approaches using three criteria: the decision-making situation they address, the intrinsic learning curve question they answer and the type of resources they use. We survey papers from the literature and classify them into this framework.},
  archive      = {J_ML},
  author       = {Mohr, Felix and van Rijn, Jan N.},
  doi          = {10.1007/s10994-024-06619-7},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8371-8425},
  shortjournal = {Mach. Learn.},
  title        = {Learning curves for decision making in supervised machine learning: A survey},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adjusting regression models for conditional uncertainty
calibration. <em>ML</em>, <em>113</em>(11), 8347–8370. (<a
href="https://doi.org/10.1007/s10994-024-06627-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal Prediction methods have finite-sample distributional-free marginal coverage guarantees. However, they generally do not offer conditional coverage guarantees, which can be important for high-stakes decisions. In this paper, we propose a novel algorithm to train a regression function to improve the conditional coverage after applying the split conformal prediction procedure. We establish an upper bound for the miscoverage gap between the conditional coverage and the nominal coverage rate and propose an end-to-end algorithm to control this upper bound. We demonstrate the efficacy of our method empirically on synthetic and real-world datasets.},
  archive      = {J_ML},
  author       = {Gao, Ruijiang and Yin, Mingzhang and Mcinerney, James and Kallus, Nathan},
  doi          = {10.1007/s10994-024-06627-7},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8347-8370},
  shortjournal = {Mach. Learn.},
  title        = {Adjusting regression models for conditional uncertainty calibration},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active model selection: A variance minimization approach.
<em>ML</em>, <em>113</em>(11), 8327–8345. (<a
href="https://doi.org/10.1007/s10994-024-06603-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of labeling is a significant challenge in practical machine learning. This issue arises not only during the learning phase but also at the model evaluation phase, as there is a need for a substantial amount of labeled test data in addition to the training data. In this study, we address the challenge of active model selection with the goal of minimizing labeling costs for choosing the best-performing model from a set of model candidates. Based on an appropriate test loss estimator, we propose an adaptive labeling strategy that can estimate the difference of test losses with small variance, thereby enabling the estimation of the best model using fewer labeling cost. Experimental results on real-world datasets confirm that our method efficiently selects the best model.},
  archive      = {J_ML},
  author       = {Hara, Satoshi and Matsuura, Mitsuru and Honda, Junya and Ito, Shinji},
  doi          = {10.1007/s10994-024-06603-1},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8327-8345},
  shortjournal = {Mach. Learn.},
  title        = {Active model selection: A variance minimization approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalizing flow sampling with langevin dynamics in the
latent space. <em>ML</em>, <em>113</em>(11), 8301–8326. (<a
href="https://doi.org/10.1007/s10994-024-06623-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalizing flows (NF) use a continuous generator to map a simple latent (e.g. Gaussian) distribution, towards an empirical target distribution associated with a training data set. Once trained by minimizing a variational objective, the learnt map provides an approximate generative model of the target distribution. Since standard NF implement differentiable maps, they may suffer from pathological behaviors when targeting complex distributions. For instance, such problems may appear for distributions on multi-component topologies or characterized by multiple modes with high probability regions separated by very unlikely areas. A typical symptom is the explosion of the Jacobian norm of the transformation in very low probability areas. This paper proposes to overcome this issue thanks to a new Markov chain Monte Carlo algorithm to sample from the target distribution in the latent domain before transporting it back to the target domain. The approach relies on a Metropolis adjusted Langevin algorithm whose dynamics explicitly exploits the Jacobian of the transformation. Contrary to alternative approaches, the proposed strategy preserves the tractability of the likelihood and it does not require a specific training. Notably, it can be straightforwardly used with any pre-trained NF network, regardless of the architecture. Experiments conducted on synthetic and high-dimensional real data sets illustrate the efficiency of the method.},
  archive      = {J_ML},
  author       = {Coeurdoux, Florentin and Dobigeon, Nicolas and Chainais, Pierre},
  doi          = {10.1007/s10994-024-06623-x},
  journal      = {Machine Learning},
  month        = {12},
  number       = {11},
  pages        = {8301-8326},
  shortjournal = {Mach. Learn.},
  title        = {Normalizing flow sampling with langevin dynamics in the latent space},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic approach for learning imbalanced data:
Enhancing zero-inflated models through boosting. <em>ML</em>,
<em>113</em>(10), 8233–8299. (<a
href="https://doi.org/10.1007/s10994-024-06558-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose systematic approaches for learning imbalanced data based on a two-regime process: regime 0, which generates excess zeros (majority class), and regime 1, which contributes to generating an outcome of one (minority class). The proposed model contains two latent equations: a split probit (logit) equation in the first stage and an ordinary probit (logit) equation in the second stage. Because boosting improves the accuracy of prediction versus using a single classifier, we combined a boosting strategy with the two-regime process. Thus, we developed the zero-inflated probit boost (ZIPBoost) and zero-inflated logit boost (ZILBoost) methods. We show that the weight functions of ZIPBoost have the desired properties for good predictive performance. Like AdaBoost, the weight functions upweight misclassified examples and downweight correctly classified examples. We show that the weight functions of ZILBoost have similar properties to those of LogitBoost. The algorithm will focus more on examples that are hard to classify in the next iteration, resulting in improved prediction accuracy. We provide the relative performance of ZIPBoost and ZILBoost, which rely on the excess kurtosis of the data distribution. Furthermore, we show the convergence and time complexity of our proposed methods. We demonstrate the performance of our proposed methods using a Monte Carlo simulation, mergers and acquisitions (M&amp;A) data application, and imbalanced datasets from the Keel repository. The results of the experiments show that our proposed methods yield better prediction accuracy compared to other learning algorithms.},
  archive      = {J_ML},
  author       = {Jeong, Yeasung and Lee, Kangbok and Park, Young Woong and Han, Sumin},
  doi          = {10.1007/s10994-024-06558-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8233-8299},
  shortjournal = {Mach. Learn.},
  title        = {A systematic approach for learning imbalanced data: Enhancing zero-inflated models through boosting},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extrapolation is not the same as interpolation. <em>ML</em>,
<em>113</em>(10), 8205–8232. (<a
href="https://doi.org/10.1007/s10994-024-06591-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new machine learning formulation designed specifically for extrapolation. The textbook way to apply machine learning to drug design is to learn a univariate function that when a drug (structure) is input, the function outputs a real number (the activity): f(drug) $$\rightarrow$$ activity. However, experience in real-world drug design suggests that this formulation of the drug design problem is not quite correct. Specifically, what one is really interested in is extrapolation: predicting the activity of new drugs with higher activity than any existing ones. Our new formulation for extrapolation is based on learning a bivariate function that predicts the difference in activities of two drugs F(drug1, drug2) $$\rightarrow$$ difference in activity, followed by the use of ranking algorithms. This formulation is general and agnostic, suitable for finding samples with target values beyond the target value range of the training set. We applied the formulation to work with support vector machines , random forests , and Gradient Boosting Machines . We compared the formulation with standard regression on thousands of drug design datasets, gene expression datasets and material property datasets. The test set extrapolation metric was the identification of examples with greater values than the training set, and top-performing examples (within the top 10% of the whole dataset). On this metric our pairwise formulation vastly outperformed standard regression. Its proposed variations also showed a consistent outperformance. Its application in the stock selection problem further confirmed the advantage of this pairwise formulation.},
  archive      = {J_ML},
  author       = {Wang, Yuxuan and King, Ross D.},
  doi          = {10.1007/s10994-024-06591-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8205-8232},
  shortjournal = {Mach. Learn.},
  title        = {Extrapolation is not the same as interpolation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data- and knowledge-driven framework for developing
machine learning models to predict soccer match outcomes. <em>ML</em>,
<em>113</em>(10), 8165–8204. (<a
href="https://doi.org/10.1007/s10994-024-06625-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2023 Soccer Prediction Challenge invited the machine learning community to develop innovative methods to predict the outcomes of 736 future soccer matches. The Challenge included two tasks. Task 1 was to forecast the exact match score, i.e., the number of goals scored by each team. Task 2 was to predict the match outcome as probability vector over the three possible result categories: victory of the home team, draw, and victory of the away team. Here, we present a new data- and knowledge-driven framework for building machine learning models from readily available data to predict soccer match outcomes. A key component of this framework is an innovative approach to modeling interdependent time series data of competing entities. Using this framework, we developed various predictive models based on k-nearest neighbors, artificial neural networks, naive Bayes, and ordinal forests, which we applied to the two tasks of the 2023 Soccer Prediction Challenge. Among all submissions to the Challenge, our machine learning models based on k-nearest neighbors and neural networks achieved top performances. Our main insights from the Challenge are that relatively simple learning algorithms perform remarkably well compared to more complex algorithms, and that the key to successful predictions lies in how well soccer domain knowledge can be incorporated in the modeling process.},
  archive      = {J_ML},
  author       = {Berrar, Daniel and Lopes, Philippe and Dubitzky, Werner},
  doi          = {10.1007/s10994-024-06625-9},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8165-8204},
  shortjournal = {Mach. Learn.},
  title        = {A data- and knowledge-driven framework for developing machine learning models to predict soccer match outcomes},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From MNIST to ImageNet and back: Benchmarking continual
curriculum learning. <em>ML</em>, <em>113</em>(10), 8137–8164. (<a
href="https://doi.org/10.1007/s10994-024-06524-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) is one of the most promising trends in recent machine learning research. Its goal is to go beyond classical assumptions in machine learning and develop models and learning strategies that present high robustness in dynamic environments. This goal is realized by designing strategies that simultaneously foster the incorporation of new knowledge while avoiding forgetting past knowledge. The landscape of CL research is fragmented into several learning evaluation protocols, comprising different learning tasks, datasets, and evaluation metrics. Additionally, the benchmarks adopted so far are still distant from the complexity of real-world scenarios, and are usually tailored to highlight capabilities specific to certain strategies. In such a landscape, it is hard to clearly and objectively assess models and strategies. In this work, we fill this gap for CL on image data by introducing two novel CL benchmarks that involve multiple heterogeneous tasks from six image datasets, with varying levels of complexity and quality. Our aim is to fairly evaluate current state-of-the-art CL strategies on a common ground that is closer to complex real-world scenarios. We additionally structure our benchmarks so that tasks are presented in increasing and decreasing order of complexity—according to a curriculum—in order to evaluate if current CL models are able to exploit structure across tasks. We devote particular emphasis to providing the CL community with a rigorous and reproducible evaluation protocol for measuring the ability of a model to generalize and not to forget while learning. Furthermore, we provide an extensive experimental evaluation showing that popular CL strategies, when challenged with our proposed benchmarks, yield sub-par performance, high levels of forgetting, and present a limited ability to effectively leverage curriculum task ordering. We believe that these results highlight the need for rigorous comparisons in future CL works as well as pave the way to design new CL strategies that are able to deal with more complex scenarios.},
  archive      = {J_ML},
  author       = {Faber, Kamil and Zurek, Dominik and Pietron, Marcin and Japkowicz, Nathalie and Vergari, Antonio and Corizzo, Roberto},
  doi          = {10.1007/s10994-024-06524-z},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8137-8164},
  shortjournal = {Mach. Learn.},
  title        = {From MNIST to ImageNet and back: Benchmarking continual curriculum learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoMadOut—a robust outlier detection algorithm based on
CoMAD. <em>ML</em>, <em>113</em>(10), 8061–8135. (<a
href="https://doi.org/10.1007/s10994-024-06521-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised learning methods are well established in the area of anomaly detection and achieve state of the art performances on outlier datasets. Outliers play a significant role, since they bear the potential to distort the predictions of a machine learning algorithm on a given dataset. Especially among PCA-based methods, outliers have an additional destructive potential regarding the result: they may not only distort the orientation and translation of the principal components, they also make it more complicated to detect outliers. To address this problem, we propose the robust outlier detection algorithm CoMadOut, which satisfies two required properties: (1) being robust towards outliers and (2) detecting them. Our CoMadOut outlier detection variants using comedian PCA define, dependent on its variant, an inlier region with a robust noise margin by measures of in-distribution (variant CMO) and optimized scores by measures of out-of-distribution (variants CMO*), e.g. kurtosis-weighting by CMO+k. These measures allow distribution based outlier scoring for each principal component, and thus, an appropriate alignment of the degree of outlierness between normal and abnormal instances. Experiments comparing CoMadOut with traditional, deep and other comparable robust outlier detection methods showed that the performance of the introduced CoMadOut approach is competitive to well established methods related to average precision (AP), area under the precision recall curve (AUPRC) and area under the receiver operating characteristic (AUROC) curve. In summary our approach can be seen as a robust alternative for outlier detection tasks.},
  archive      = {J_ML},
  author       = {Lohrer, Andreas and Kazempour, Daniyal and Hünemörder, Maximilian and Kröger, Peer},
  doi          = {10.1007/s10994-024-06521-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8061-8135},
  shortjournal = {Mach. Learn.},
  title        = {CoMadOut—a robust outlier detection algorithm based on CoMAD},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalanced COVID-19 vaccine sentiment classification with
synthetic resampling coupled deep adversarial active learning.
<em>ML</em>, <em>113</em>(10), 8027–8059. (<a
href="https://doi.org/10.1007/s10994-024-06562-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite an abundance of scientific evidence supporting the effectiveness of COVID-19 vaccines, there has been a recent global surge in vaccine hesitancy, primarily driven by the spread of misinformation on social media platforms. It is crucial to address this issue and raise awareness about the importance of vaccination in combating the deadly COVID-19 virus. Predicting community sentiment through social media platforms can provide valuable insights into vaccine hesitancy, aiding health workers and medical professionals in taking necessary precautionary measures. However, the lack of high-quality labeled data presents a challenge for building an effective COVID-19 sentiment classifier. Additionally, the available labeled datasets suffer from severe class imbalance. To address these challenges, this article presents an effective COVID-19 sentiment prediction framework. Firstly, a deep adversarial active learning framework leverages abundant unlabeled data by training autoencoder and discriminator components adversarially to select the most informative unlabeled samples. Secondly, to mitigate the effects of imbalanced labeled datasets, a resampling phase is incorporated into the adversarial training loop. The proposed framework, named Resampling Supported Deep Adversarial Active Learning (RS-DAAL), is rigorously evaluated using two different datasets comprising social media posts from Twitter and Reddit. Various resampling techniques, including undersampling, oversampling, and hybrid methods, are assessed, with oversampling techniques further tested at different levels of resampling. Comparative studies are conducted against a baseline model without any resampling layer and with current state-of-the-art methods as well. Experimental results and statistical analysis demonstrate the superiority of the proposed RS-DAAL method in identifying COVID-19 sentiments on social media platforms.},
  archive      = {J_ML},
  author       = {Chatterjee, Sankhadeep and Bhattacharjee, Saranya and Das, Asit Kumar and Banerjee, Soumen},
  doi          = {10.1007/s10994-024-06562-7},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {8027-8059},
  shortjournal = {Mach. Learn.},
  title        = {Imbalanced COVID-19 vaccine sentiment classification with synthetic resampling coupled deep adversarial active learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding prediction discrepancies in classification.
<em>ML</em>, <em>113</em>(10), 7997–8026. (<a
href="https://doi.org/10.1007/s10994-024-06557-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multitude of classifiers can be trained on the same data to achieve similar performances during test time while having learned significantly different classification patterns. When selecting a classifier, the machine learning practitioner has no understanding on the differences between models, their limits, where they agree and where they don’t. But this choice will result in concrete consequences for instances to be classified in the discrepancy zone, since the final decision will be based on the selected classification pattern. Besides the arbitrary nature of the result, a bad choice could have further negative consequences such as loss of opportunity or lack of fairness. This paper proposes to address this question by analyzing the prediction discrepancies in a pool of best-performing models trained on the same data. A model-agnostic algorithm, DIG, is proposed to capture and explain discrepancies locally in tabular datasets, to enable the practitioner to make the best educated decision when selecting a model by anticipating its potential undesired consequences.},
  archive      = {J_ML},
  author       = {Renard, Xavier and Laugel, Thibault and Detyniecki, Marcin},
  doi          = {10.1007/s10994-024-06557-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7997-8026},
  shortjournal = {Mach. Learn.},
  title        = {Understanding prediction discrepancies in classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POMDP inference and robust solution via deep reinforcement
learning: An application to railway optimal maintenance. <em>ML</em>,
<em>113</em>(10), 7967–7995. (<a
href="https://doi.org/10.1007/s10994-024-06559-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially Observable Markov Decision Processes (POMDPs) can model complex sequential decision-making problems under stochastic and uncertain environments. A main reason hindering their broad adoption in real-world applications is the unavailability of a suitable POMDP model or a simulator thereof. Available solution algorithms, such as Reinforcement Learning (RL), typically benefit from the knowledge of the transition dynamics and the observation generating process, which are often unknown and non-trivial to infer. In this work, we propose a combined framework for inference and robust solution of POMDPs via deep RL. First, all transition and observation model parameters are jointly inferred via Markov Chain Monte Carlo sampling of a hidden Markov model, which is conditioned on actions, in order to recover full posterior distributions from the available data. The POMDP with uncertain parameters is then solved via deep RL techniques with the parameter distributions incorporated into the solution via domain randomization, in order to develop solutions that are robust to model uncertainty. As a further contribution, we compare the use of Transformers and long short-term memory networks, which constitute model-free RL solutions and work directly on the observation space, with an approach termed the belief-input method, which works on the belief space by exploiting the learned POMDP model for belief inference. We apply these methods to the real-world problem of optimal maintenance planning for railway assets and compare the results with the current real-life policy. We show that the RL policy learned by the belief-input method is able to outperform the real-life policy by yielding significantly reduced life-cycle costs.},
  archive      = {J_ML},
  author       = {Arcieri, Giacomo and Hoelzl, Cyprien and Schwery, Oliver and Straub, Daniel and Papakonstantinou, Konstantinos G. and Chatzi, Eleni},
  doi          = {10.1007/s10994-024-06559-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7967-7995},
  shortjournal = {Mach. Learn.},
  title        = {POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On metafeatures’ ability of implicit concept identification.
<em>ML</em>, <em>113</em>(10), 7931–7966. (<a
href="https://doi.org/10.1007/s10994-024-06612-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift in data stream processing remains an intriguing challenge and states a popular research topic. Methods that actively process data streams usually employ drift detectors, whose performance is often based on monitoring the variability of different stream properties. This publication provides an overview and analysis of metafeatures variability describing data streams with concept drifts. Five experiments conducted on synthetic, semi-synthetic, and real-world data streams examine the ability of over 160 metafeatures from 9 categories to recognize concepts in non-stationary data streams. The work reveals the distinctions in the considered sources of streams and specifies 17 metafeatures with a high ability of concept identification.},
  archive      = {J_ML},
  author       = {Komorniczak, Joanna and Ksieniewicz, Paweł},
  doi          = {10.1007/s10994-024-06612-0},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7931-7966},
  shortjournal = {Mach. Learn.},
  title        = {On metafeatures’ ability of implicit concept identification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-domain user association scheme based on graph
attention networks with trajectory embedding. <em>ML</em>,
<em>113</em>(10), 7905–7930. (<a
href="https://doi.org/10.1007/s10994-024-06613-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of mobile internet, users generate vast amounts of location-based data across multiple social networking platforms. This data is valuable for applications such as personalized recommendations and targeted advertising. Accurately identifying users across different platforms enhances understanding of user behavior and preferences. To address the complexity of cross-domain user identification caused by varying check-in frequencies and data precision differences, we propose HTEGAT, a hierarchical trajectory embedding-based graph attention network model. HTEGAT addresses these issues by combining an Encoder and a Trajectory Identification module. The Encoder module, by integrating self-attention mechanisms with LSTM, can effectively extract location point-level features and accurately capture trajectory transition features, thereby accurately characterizing hierarchical temporal trajectories. Trajectory Identification module introduces trajectory distance-neighbor relationships and constructs an adjacency matrix based on these relationships. By utilizing attention weight coefficients in a graph attention network to capture similarities between trajectories, this approach reduces identification complexity while addressing the issue of dataset sparsity. Experiments on two cross-domain Location-Based Social Network (LBSN) datasets demonstrate that HTEGAT achieves higher hit rates with lower time complexity. On the Foursquare-Twitter dataset, HTEGAT significantly improved hit rates, surpassing state-of-the-art methods. On the Instagram-Twitter dataset, HTEGAT consistently outperformed contemporary models, showcasing its effectiveness and superiority.},
  archive      = {J_ML},
  author       = {Cen, Keqing and Yang, Zhenghao and Wang, Ze and Dong, Minhong},
  doi          = {10.1007/s10994-024-06613-z},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7905-7930},
  shortjournal = {Mach. Learn.},
  title        = {A cross-domain user association scheme based on graph attention networks with trajectory embedding},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A class sensitivity feature guided t-type generative model
for noisy label classification. <em>ML</em>, <em>113</em>(10),
7867–7904. (<a
href="https://doi.org/10.1007/s10994-024-06598-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets inevitably contain noisy labels, which induces weak performance of deep neural networks (DNNs). Many existing methods focus on loss and regularization tricks, as well as characterizing and modelling differences between noisy and clean samples. However, taking advantage of information from different extents of distortion in latent feature space, is less explored and remains challenging. To solve this problem, we analyze characteristic distortion extents of different high-dimensional features, achieving the conclusion that features vary in their degree of deformation in their correlations with respect to categorical variables. Aforementioned disturbances on features not only reduce sensitivity and contribution of latent features to classification, but also bring obstacles into generating decision boundaries. To mitigate these issues, we propose class sensitivity feature extractor (CSFE) and T-type generative classifier (TGC). Based on the weighted Mahalanobis distance between conditional and unconditional cumulative distribution function after variance-stabilizing transformation, CSFE realizes high quality feature extraction through evaluating class-wise discrimination ability and sensitivity to classification. TGC introduces student-t estimator to clustering analysis in latent space, which is more robust in generating decision boundaries while maintaining equivalent efficiency. To alleviate the cost of retraining a whole DNN, we propose an ensemble model to simultaneously generate robust decision boundaries and train the DNN with the improved CSFE named SoftCSFE. Extensive experiments on three datasets, which are the RML2016.10a dataset, UCR Time Series Classification Archive dataset and a real-world dataset Clothing1M, show advantages of our methods.},
  archive      = {J_ML},
  author       = {Bai, Yidi and Cui, Hengjian},
  doi          = {10.1007/s10994-024-06598-9},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7867-7904},
  shortjournal = {Mach. Learn.},
  title        = {A class sensitivity feature guided T-type generative model for noisy label classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal prediction for regression models with
asymmetrically distributed errors: Application to aircraft navigation
during landing maneuver. <em>ML</em>, <em>113</em>(10), 7841–7866. (<a
href="https://doi.org/10.1007/s10994-024-06615-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-autonomous aircraft navigation is a high-risk domain where confidence on the prediction is required. For that, this paper introduces the use of conformal predictions strategies for regression problems. While standard approaches use an absolute nonconformity scores, we aim at introducing a signed version of the nonconformity scores. Experimental results on synthetic data have shown their interest for non-centered errors. Moreover, in order to reduce the width of the prediction interval, we introduce an optimization procedure which learn the optimal alpha risks for the lower and upper bounds of the interval. In practice, we show that a line search algorithm can be employed to solve it. Practically, this novel adaptive conformal prediction strategy has revealed to be well adapted for skew distributed errors. In addition, an extension of these conformal prediction strategies is introduced to incorporate numeric and categorical auxiliary variables describing the acquisition context. Based on a quantile regression model, they allow to maintain the coverage for each metadata value. All these strategies have then been applied on a real use case of runway localization from data acquired by an aircraft during landing maneuver. Extensive experiments on multiple airports have shown the interest of the proposed conformal prediction strategies, in particular for runways equipped with a very long ramp approach where asymmetric angular deviation error are observed.},
  archive      = {J_ML},
  author       = {Vilfroy, Solène and Bombrun, Lionel and Urruty, Thierry and De Grancey, Florence and Lebrat, Jean-Philippe and Carré, Philippe},
  doi          = {10.1007/s10994-024-06615-x},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7841-7866},
  shortjournal = {Mach. Learn.},
  title        = {Conformal prediction for regression models with asymmetrically distributed errors: Application to aircraft navigation during landing maneuver},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nested barycentric coordinate system as an explicit feature
map for polyhedra approximation and learning tasks. <em>ML</em>,
<em>113</em>(10), 7807–7840. (<a
href="https://doi.org/10.1007/s10994-024-06596-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new embedding technique based on a nested barycentric coordinate system. We show that our embedding can be used to transform the problems of polyhedron approximation, piecewise linear classification and convex regression into one of finding a linear classifier or regressor in a higher dimensional (but nevertheless quite sparse) representation. Our embedding maps a piecewise linear function into an everywhere-linear function, and allows us to invoke well-known algorithms for the latter problem to solve the former. We explain the applications of our embedding to the problems of approximating separating polyhedra—in fact, it can approximate any convex body and unions of convex bodies—as well as to classification by separating polyhedra, and to piecewise linear regression.},
  archive      = {J_ML},
  author       = {Gottlieb, Lee-Ad and Kaufman, Eran and Kontorovich, Aryeh and Nivasch, Gabriel and Pele, Ofir},
  doi          = {10.1007/s10994-024-06596-x},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7807-7840},
  shortjournal = {Mach. Learn.},
  title        = {Nested barycentric coordinate system as an explicit feature map for polyhedra approximation and learning tasks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-organizing maps with adaptive distances for multiple
dissimilarity matrices. <em>ML</em>, <em>113</em>(10), 7783–7806. (<a
href="https://doi.org/10.1007/s10994-024-06607-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increasing interest in multi-view approaches based on their ability to manage data from several sources. However, regarding unsupervised learning, most multi-view approaches are clustering algorithms suitable for analyzing vector data. Currently, only a relatively few SOM algorithms can manage multi-view dissimilarity data, despite their usefulness. This paper proposes two new families of batch SOM algorithms for multi-view dissimilarity data: multi-medoids SOM and relational SOM, both designed to give a crisp partition and learn the relevance weight for each dissimilarity matrix by optimizing an objective function, aiming to preserve the topological properties of the map data. In both families, the weight represents the relevance of each dissimilarity matrix for the learning task being computed, either locally, for each cluster, or globally, for the whole partition. The proposed algorithms were compared with already in the literature single-view SOM and set-medoids SOM for multi-view dissimilarity data. According to the experiments using 14 datasets for F-measure, NMI, Topographic Error, and Silhouette, the relevance weights of the dissimilarity matrices must be considered. In addition, the multi-medoids and relational SOM performed better than the set-medoids SOM. An application study was also carried out on a dermatology dataset, where the proposed methods have the best performance.},
  archive      = {J_ML},
  author       = {Palomino Mariño, Laura Maria and Tenorio de Carvalho, Francisco de Assis},
  doi          = {10.1007/s10994-024-06607-x},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7783-7806},
  shortjournal = {Mach. Learn.},
  title        = {Self-organizing maps with adaptive distances for multiple dissimilarity matrices},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-enhanced graph neural networks with global context
representation. <em>ML</em>, <em>113</em>(10), 7761–7781. (<a
href="https://doi.org/10.1007/s10994-024-06523-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification is a crucial task for efficiently analyzing graph-structured data. Related semi-supervised methods have been extensively studied to address the scarcity of labeled data in emerging classes. However, two fundamental weaknesses hinder the performance: lacking the ability to mine latent semantic information between nodes, or ignoring to simultaneously capture local and global coupling dependencies between different nodes. To solve these limitations, we propose a novel semantic-enhanced graph neural networks with global context representation for semi-supervised node classification. Specifically, we first use graph convolution network to learn short-range local dependencies, which not only considers the spatial topological structure relationship between nodes, but also takes into account the semantic correlation between nodes to enhance the representation ability of nodes. Second, an improved Transformer model is introduced to reasoning the long-range global pairwise relationships, which has linear computational complexity and is particularly important for large datasets. Finally, the proposed model shows strong performance on various open datasets, demonstrating the superiority of our solutions.},
  archive      = {J_ML},
  author       = {Qian, Youcheng and Yin, Xueyan},
  doi          = {10.1007/s10994-024-06523-0},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7761-7781},
  shortjournal = {Mach. Learn.},
  title        = {Semantic-enhanced graph neural networks with global context representation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining siamese networks in few-shot learning.
<em>ML</em>, <em>113</em>(10), 7723–7760. (<a
href="https://doi.org/10.1007/s10994-024-06529-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models often struggle to generalize accurately when tested on new class distributions that were not present in their training data. This is a significant challenge for real-world applications that require quick adaptation without the need for retraining. To address this issue, few-shot learning frameworks, which includes models such as Siamese Networks, have been proposed. Siamese Networks learn similarity between pairs of records through a metric that can be easily extended to new, unseen classes. However, these systems lack interpretability, which can hinder their use in certain applications. To address this, we propose a data-agnostic method to explain the outcomes of Siamese Networks in the context of few-shot learning. Our explanation method is based on a post-hoc perturbation-based procedure that evaluates the contribution of individual input features to the final outcome. As such, it falls under the category of post-hoc explanation methods. We present two variants, one that considers each input feature independently, and another that evaluates the interplay between features. Additionally, we propose two perturbation procedures to evaluate feature contributions. Qualitative and quantitative results demonstrate that our method is able to identify highly discriminant intra-class and inter-class characteristics, as well as predictive behaviors that lead to misclassification by relying on incorrect features.},
  archive      = {J_ML},
  author       = {Fedele, Andrea and Guidotti, Riccardo and Pedreschi, Dino},
  doi          = {10.1007/s10994-024-06529-8},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7723-7760},
  shortjournal = {Mach. Learn.},
  title        = {Explaining siamese networks in few-shot learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic grammars for modeling dynamical systems from
coarse, noisy, and partial data. <em>ML</em>, <em>113</em>(10),
7689–7721. (<a
href="https://doi.org/10.1007/s10994-024-06522-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinary differential equations (ODEs) are a widely used formalism for the mathematical modeling of dynamical systems, a task omnipresent in scientific domains. The paper introduces a novel method for inferring ODEs from data, which extends ProGED, a method for equation discovery that allows users to formalize domain-specific knowledge as probabilistic context-free grammars and use it for constraining the space of candidate equations. The extended method can discover ODEs from partial observations of dynamical systems, where only a subset of state variables can be observed. To evaluate the performance of the newly proposed method, we perform a systematic empirical comparison with alternative state-of-the-art methods for equation discovery and system identification from complete and partial observations. The comparison uses Dynobench, a set of ten dynamical systems that extends the standard Strogatz benchmark. We compare the ability of the considered methods to reconstruct the known ODEs from synthetic data simulated at different temporal resolutions. We also consider data with different levels of noise, i.e., signal-to-noise ratios. The improved ProGED compares favourably to state-of-the-art methods for inferring ODEs from data regarding reconstruction abilities and robustness to data coarseness, noise, and completeness.},
  archive      = {J_ML},
  author       = {Omejc, Nina and Gec, Boštjan and Brence, Jure and Todorovski, Ljupčo and Džeroski, Sašo},
  doi          = {10.1007/s10994-024-06522-1},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7689-7721},
  shortjournal = {Mach. Learn.},
  title        = {Probabilistic grammars for modeling dynamical systems from coarse, noisy, and partial data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic approach for reproducible model distillation.
<em>ML</em>, <em>113</em>(10), 7645–7688. (<a
href="https://doi.org/10.1007/s10994-024-06597-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable “student” model to mimic the predictions made by the black box “teacher” model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough sample of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed separately for each specific class of student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the estimated fidelity of the student to the teacher. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a sample size such that the consistent student model would be selected under different pseudo samples. We demonstrate the application of our proposed approach on three commonly used intelligible models: decision trees, falling rule lists and symbolic regression. Finally, we conduct simulation experiments on Mammographic Mass and Breast Cancer datasets and illustrate the testing procedure throughout a theoretical analysis with Markov process. The code is publicly available at https://github.com/yunzhe-zhou/GenericDistillation .},
  archive      = {J_ML},
  author       = {Zhou, Yunzhe and Xu, Peiru and Hooker, Giles},
  doi          = {10.1007/s10994-024-06597-w},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7645-7688},
  shortjournal = {Mach. Learn.},
  title        = {A generic approach for reproducible model distillation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autoreplicative random forests with applications to missing
value imputation. <em>ML</em>, <em>113</em>(10), 7617–7643. (<a
href="https://doi.org/10.1007/s10994-024-06584-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values are a common problem in data science and machine learning. Removing instances with missing values is a straightforward workaround, but this can significantly hinder subsequent data analysis, particularly when features outnumber instances. There are a variety of methodologies proposed in the literature for imputing missing values. Denoising Autoencoders, for example, have been leveraged efficiently for imputation. However, neural network approaches have been relatively less effective on smaller datasets. In this work, we propose Autoreplicative Random Forests (ARF) as a multi-output learning approach, which we introduce in the context of a framework that may impute via either an iterative or procedural process. Experiments on several low- and high-dimensional datasets show that ARF is computationally efficient and exhibits better imputation performance than its competitors, including neural network approaches. In order to provide statistical analysis and mathematical background to the proposed missing value imputation framework, we also propose probabilistic ARFs, where the confidence values are provided over different imputation hypotheses, therefore maximizing the utility of such a framework in a machine-learning pipeline targeting predictive performance.},
  archive      = {J_ML},
  author       = {Antonenko, Ekaterina and Carreño, Ander and Read, Jesse},
  doi          = {10.1007/s10994-024-06584-1},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7617-7643},
  shortjournal = {Mach. Learn.},
  title        = {Autoreplicative random forests with applications to missing value imputation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Describing group evolution in temporal data using
multi-faceted events. <em>ML</em>, <em>113</em>(10), 7591–7615. (<a
href="https://doi.org/10.1007/s10994-024-06600-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Groups—such as clusters of points or communities of nodes—are fundamental when addressing various data mining tasks. In temporal data, the predominant approach for characterizing group evolution has been through the identification of “events”. However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations. Moving beyond existing taxonomies, we think of events as “archetypes” characterized by a unique combination of quantitative dimensions that we call “facets”. Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities. Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes. We apply our framework to evolving groups from several face-to-face interaction datasets, showing it enables richer, more reliable characterization of group dynamics with respect to state-of-the-art methods, especially when the groups are subject to complex relationships. Our approach also offers intuitive solutions to common tasks related to dynamic group analysis, such as choosing an appropriate aggregation scale, quantifying partition stability, and evaluating event quality.},
  archive      = {J_ML},
  author       = {Failla, Andrea and Cazabet, Rémy and Rossetti, Giulio and Citraro, Salvatore},
  doi          = {10.1007/s10994-024-06600-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7591-7615},
  shortjournal = {Mach. Learn.},
  title        = {Describing group evolution in temporal data using multi-faceted events},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining outliers and anomalous groups via subspace
density contrastive loss. <em>ML</em>, <em>113</em>(10), 7565–7589. (<a
href="https://doi.org/10.1007/s10994-024-06618-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable AI refers to techniques by which the reasons underlying decisions taken by intelligent artifacts are single out and provided to users. Outlier detection is the task of individuating anomalous objects within a given data population they belong to. In this paper we propose a new technique to explain why a given data object has been singled out as anomalous. The explanation our technique returns also includes counterfactuals, each of which denotes a possible way to “repair” the outlier to make it an inlier. Thus, given in input a reference data population and an object deemed to be anomalous, the aim is to provide possible explanations for the anomaly of the input object, where an explanation consists of a subset of the features, called choice, and an associated set of changes to be applied, called mask, in order to make the object “behave normally”. The paper presents a deep learning architecture exploiting a features choice module and mask generation module in order to learn both components of explanations. The learning procedure is guided by an ad-hoc loss function that simultaneously maximizes (minimizes, resp.) the isolation of the input outlier before applying the mask (resp., after the application of the mask returned by the mask generation module) within the subspace singled out by the features choice module, all that while also minimizing the number of features involved in the selected choice. We consider also the case in which a common explanation is required for a group of outliers provided together in input. We present experiments on both artificial and real data sets and a comparison with competitors validating the effectiveness of the proposed approach.},
  archive      = {J_ML},
  author       = {Angiulli, Fabrizio and Fassetti, Fabio and Nisticò, Simona and Palopoli, Luigi},
  doi          = {10.1007/s10994-024-06618-8},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7565-7589},
  shortjournal = {Mach. Learn.},
  title        = {Explaining outliers and anomalous groups via subspace density contrastive loss},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating soccer match prediction models: A deep learning
approach and feature optimization for gradient-boosted trees.
<em>ML</em>, <em>113</em>(10), 7541–7564. (<a
href="https://doi.org/10.1007/s10994-024-06608-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models have become increasingly popular for predicting the results of soccer matches, however, the lack of publicly-available benchmark datasets has made model evaluation challenging. The 2023 Soccer Prediction Challenge required the prediction of match results first in terms of the exact goals scored by each team, and second, in terms of the probabilities for a win, draw, and loss. The original training set of matches and features, which was provided for the competition, was augmented with additional matches that were played between 4 April and 13 April 2023, representing the period after which the training set ended, but prior to the first matches that were to be predicted (upon which the performance was evaluated). A CatBoost model was employed using pi-ratings as the features, which were initially identified as the optimal choice for calculating the win/draw/loss probabilities. Notably, deep learning models have frequently been disregarded in this particular task. Therefore, in this study, we aimed to assess the performance of a deep learning model and determine the optimal feature set for a gradient-boosted tree model. The model was trained using the most recent 5 years of data, and three training and validation sets were used in a hyperparameter grid search. The results from the validation sets show that our model had strong performance and stability compared to previously published models from the 2017 Soccer Prediction Challenge for win/draw/loss prediction. Our model ranked 16th in the 2023 Soccer Prediction Challenge with RPS 0.2195.},
  archive      = {J_ML},
  author       = {Yeung, Calvin and Bunker, Rory and Umemoto, Rikuhei and Fujii, Keisuke},
  doi          = {10.1007/s10994-024-06608-w},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7541-7564},
  shortjournal = {Mach. Learn.},
  title        = {Evaluating soccer match prediction models: A deep learning approach and feature optimization for gradient-boosted trees},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalization of temporal logic tasks via future dependent
options. <em>ML</em>, <em>113</em>(10), 7509–7540. (<a
href="https://doi.org/10.1007/s10994-024-06614-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal logic (TL) tasks consist of complex and temporally extended subgoals and they are common for many real-world applications, such as service and navigation robots. However, it is often inefficient or even infeasible to train reinforcement learning (RL) agents to solve multiple TL tasks, since rewards are sparse and non-Markovian in these tasks. A promising solution to this problem is to learn task-conditioned policies which can zero-shot generalize to new TL tasks without further training. However, influenced by some practical issues, such as issues of lossy symbolic observation and long time-horizon of completing TL task, previous works suffer from sample inefficiency in training and sub-optimality (or even infeasibility) in task execution. In order to tackle these issues, this paper proposes an option-based framework to generalize TL tasks, consisting of option training and task execution parts. We have innovations in both parts. In option training, we propose to learn options dependent on the future subgoals via a novel approach. Additionally, we propose to train a multi-step value function which can propagate the rewards of satisfying future subgoals more efficiently in long-horizon tasks. In task execution, in order to ensure the optimality and safety, we propose a model-free MPC planner for option selection, circumventing the learning of a transition model which is required by previous MPC planners. In experiments on three different domains, we evaluate the generalization capability of the agent trained by the proposed method, showing its significant advantage over previous methods.},
  archive      = {J_ML},
  author       = {Xu, Duo and Fekri, Faramarz},
  doi          = {10.1007/s10994-024-06614-y},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7509-7540},
  shortjournal = {Mach. Learn.},
  title        = {Generalization of temporal logic tasks via future dependent options},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule learning by modularity. <em>ML</em>, <em>113</em>(10),
7479–7508. (<a
href="https://doi.org/10.1007/s10994-024-06556-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a modular methodology that combines state-of-the-art methods in (stochastic) machine learning with well-established methods in inductive logic programming (ILP) and rule induction to provide efficient and scalable algorithms for the classification of vast data sets. By construction, these classifications are based on the synthesis of simple rules, thus providing direct explanations of the obtained classifications. Apart from evaluating our approach on the common large scale data sets MNIST, Fashion-MNIST and IMDB, we present novel results on explainable classifications of dental bills. The latter case study stems from an industrial collaboration with Allianz Private Krankenversicherung which is an insurance company offering diverse services in Germany.},
  archive      = {J_ML},
  author       = {Nössig, Albert and Hell, Tobias and Moser, Georg},
  doi          = {10.1007/s10994-024-06556-5},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7479-7508},
  shortjournal = {Mach. Learn.},
  title        = {Rule learning by modularity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical bayes linked matrix decomposition. <em>ML</em>,
<em>113</em>(10), 7451–7477. (<a
href="https://doi.org/10.1007/s10994-024-06599-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data for several applications in diverse fields can be represented as multiple matrices that are linked across rows or columns. This is particularly common in molecular biomedical research, in which multiple molecular “omics” technologies may capture different feature sets (e.g., corresponding to rows in a matrix) and/or different sample populations (corresponding to columns). This has motivated a large body of work on integrative matrix factorization approaches that identify and decompose low-dimensional signal that is shared across multiple matrices or specific to a given matrix. We propose an empirical variational Bayesian approach to this problem that has several advantages over existing techniques, including the flexibility to accommodate shared signal over any number of row or column sets (i.e., bidimensional integration), an intuitive model-based objective function that yields appropriate shrinkage for the inferred signals, and a relatively efficient estimation algorithm with no tuning parameters. A general result establishes conditions for the uniqueness of the underlying decomposition for a broad family of methods that includes the proposed approach. For scenarios with missing data, we describe an associated iterative imputation approach that is novel for the single-matrix context and a powerful approach for “blockwise” imputation (in which an entire row or column is missing) in various linked matrix contexts. Extensive simulations show that the method performs very well under different scenarios with respect to recovering underlying low-rank signal, accurately decomposing shared and specific signals, and accurately imputing missing data. The approach is applied to gene expression and miRNA data from breast cancer tissue and normal breast tissue, for which it gives an informative decomposition of variation and outperforms alternative strategies for missing data imputation.},
  archive      = {J_ML},
  author       = {Lock, Eric F.},
  doi          = {10.1007/s10994-024-06599-8},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7451-7477},
  shortjournal = {Mach. Learn.},
  title        = {Empirical bayes linked matrix decomposition},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local neighborhood encodings for imbalanced data
classification. <em>ML</em>, <em>113</em>(10), 7421–7449. (<a
href="https://doi.org/10.1007/s10994-024-06563-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to propose Local Neighborhood Encodings (LNE)-a hybrid data preprocessing method dedicated to skewed class distribution balancing. The proposed LNE algorithm uses both over- and undersampling methods. The intensity of the methods is chosen separately for each fraction of minority and majority class objects. It is selected depending on the type of neighborhoods of objects of a given class, understood as the number of neighbors from the same class closest to a given object. The process of selecting the over- and undersampling intensities is treated as an optimization problem for which an evolutionary algorithm is used. The quality of the proposed method was evaluated through computer experiments. Compared with SOTA resampling strategies, LNE shows very good results. In addition, an experimental analysis of the algorithms behavior was performed, i.e., the determination of data preprocessing parameters depending on the selected characteristics of the decision problem, as well as the type of classifier used. An ablation study was also performed to evaluate the influence of components on the quality of the obtained classifiers. The evaluation of how the quality of classification is influenced by the evaluation of the objective function in an evolutionary algorithm is presented. In the considered task, the objective function is not de facto deterministic and its value is subject to estimation. Hence, it was important from the point of view of computational efficiency to investigate the possibility of using for quality assessment the so-called proxy classifier, i.e., a classifier of low computational complexity, although the final model was learned using a different model. The proposed data preprocessing method has high quality compared to SOTA, however, it should be noted that it requires significantly more computational effort. Nevertheless, it can be successfully applied to the case as no very restrictive model building time constraints are imposed.},
  archive      = {J_ML},
  author       = {Koziarski, Michał and Woźniak, Michał},
  doi          = {10.1007/s10994-024-06563-6},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7421-7449},
  shortjournal = {Mach. Learn.},
  title        = {Local neighborhood encodings for imbalanced data classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-game soccer outcome prediction with offline reinforcement
learning. <em>ML</em>, <em>113</em>(10), 7393–7419. (<a
href="https://doi.org/10.1007/s10994-024-06611-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting outcomes in soccer is crucial for various stakeholders, including teams, leagues, bettors, the betting industry, media, and fans. With advancements in computer vision, player tracking data has become abundant, leading to the development of sophisticated soccer analytics models. However, existing models often rely solely on spatiotemporal features derived from player tracking data, which may not fully capture the complexities of in-game dynamics. In this paper, we present an end-to-end system that leverages raw event and tracking data to predict both offensive and defensive actions, along with the optimal decision for each game scenario, based solely on historical game data. Our model incorporates the effectiveness of these actions to accurately predict win probabilities at every minute of the game. Experimental results demonstrate the effectiveness of our approach, achieving an accuracy of 87% in predicting offensive and defensive actions. Furthermore, our in-game outcome prediction model exhibits an error rate of 0.1, outperforming counterpart models and bookmakers’ odds.},
  archive      = {J_ML},
  author       = {Rahimian, Pegah and Mihalyi, Balazs Mark and Toka, Laszlo},
  doi          = {10.1007/s10994-024-06611-1},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7393-7419},
  shortjournal = {Mach. Learn.},
  title        = {In-game soccer outcome prediction with offline reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighting non-IID batches for out-of-distribution detection.
<em>ML</em>, <em>113</em>(10), 7371–7391. (<a
href="https://doi.org/10.1007/s10994-024-06605-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard network pretrained on in-distribution (ID) samples could make high-confidence predictions on out-of-distribution (OOD) samples, leaving the possibility of failing to distinguish ID and OOD samples in the test phase. To address this over-confidence issue, the existing methods improve the OOD sensitivity from modeling perspectives, i.e., retraining it by modifying training processes or objective functions. In contrast, this paper proposes a simple but effective method, namely Weighted Non-IID Batching (WNB), by adjusting batch weights. WNB builds on a key observation: increasing the batch size can improve the OOD detection performance. This is because a smaller batch size may make its batch samples more likely to be treated as non-IID from the assumed ID, i.e., associated with an OOD. This causes a network to provide high-confidence predictions for all samples from the OOD. Accordingly, WNB applies a weight function to weight each batch according to the discrepancy between batch samples and the entire training ID dataset. Specifically, the weight function is derived by minimizing the generalization error bound. It ensures that the weight function assigns larger weights to batches with smaller discrepancies and makes a trade-off between ID classification and OOD detection performance. Experimental results show that incorporating WNB into state-of-the-art OOD detection methods can further improve their performance.},
  archive      = {J_ML},
  author       = {Zhao, Zhilin and Cao, Longbing},
  doi          = {10.1007/s10994-024-06605-z},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7371-7391},
  shortjournal = {Mach. Learn.},
  title        = {Weighting non-IID batches for out-of-distribution detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection for both outcomes and predictors: Sparse
multivariate principal covariates regression. <em>ML</em>,
<em>113</em>(10), 7319–7370. (<a
href="https://doi.org/10.1007/s10994-024-06520-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets comprised of large sets of both predictor and outcome variables are becoming more widely used in research. In addition to the well-known problems of model complexity and predictor variable selection, predictive modelling with such large data also presents a relatively novel and under-studied challenge of outcome variable selection. Certain outcome variables in the data may not be adequately predicted by the given sets of predictors. In this paper, we propose the method of Sparse Multivariate Principal Covariates Regression that addresses these issues altogether by expanding the Principal Covariates Regression model to incorporate sparsity penalties on both of predictor and outcome variables. Our method is one of the first methods that perform variable selection for both predictors and outcomes simultaneously. Moreover, by relying on summary variables that explain the variance in both predictor and outcome variables, the method offers a sparse and succinct model representation of the data. In a simulation study, the method performed better than methods with similar aims such as sparse Partial Least Squares at prediction of the outcome variables and recovery of the population parameters. Lastly, we administered the method on an empirical dataset to illustrate its application in practice.},
  archive      = {J_ML},
  author       = {Park, Soogeun and Ceulemans, Eva and Van Deun, Katrijn},
  doi          = {10.1007/s10994-024-06520-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7319-7370},
  shortjournal = {Mach. Learn.},
  title        = {Variable selection for both outcomes and predictors: Sparse multivariate principal covariates regression},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integration of multi-modal datasets to estimate human aging.
<em>ML</em>, <em>113</em>(10), 7293–7317. (<a
href="https://doi.org/10.1007/s10994-024-06588-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aging involves complex biological processes leading to the decline of living organisms. As population lifespan increases worldwide, the importance of identifying factors underlying healthy aging has become critical. Integration of multi-modal datasets is a powerful approach for the analysis of complex biological systems, with the potential to uncover novel aging biomarkers. In this study, we leveraged publicly available epigenomic, transcriptomic and telomere length data along with histological images from the Genotype-Tissue Expression project to build tissue-specific regression models for age prediction. Using data from two tissues, lung and ovary, we aimed to compare model performance across data modalities, as well as to assess the improvement resulting from integrating multiple data types. Our results demostrate that methylation outperformed the other data modalities, with a mean absolute error of 3.36 and 4.36 in the test sets for lung and ovary, respectively. These models achieved lower error rates when compared with established state-of-the-art tissue-agnostic methylation models, emphasizing the importance of a tissue-specific approach. Additionally, this work has shown how the application of Hierarchical Image Pyramid Transformers for feature extraction significantly enhances age modeling using histological images. Finally, we evaluated the benefits of integrating multiple data modalities into a single model. Combining methylation data with other data modalities only marginally improved performance likely due to the limited number of available samples. Combining gene expression with histological features yielded more accurate age predictions compared with the individual performance of these data types. Given these results, this study shows how machine learning applications can be extended to/in multi-modal aging research. Code used is available at https://github.com/zroger49/multi_modal_age_prediction .},
  archive      = {J_ML},
  author       = {Ribeiro, Rogério and Moraes, Athos and Moreno, Marta and Ferreira, Pedro G.},
  doi          = {10.1007/s10994-024-06588-x},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7293-7317},
  shortjournal = {Mach. Learn.},
  title        = {Integration of multi-modal datasets to estimate human aging},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persistent laplacian-enhanced algorithm for scarcely labeled
data classification. <em>ML</em>, <em>113</em>(10), 7267–7292. (<a
href="https://doi.org/10.1007/s10994-024-06616-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of many machine learning (ML) methods depends crucially on having large amounts of labeled data. However, obtaining enough labeled data can be expensive, time-consuming, and subject to ethical constraints for many applications. One approach that has shown tremendous value in addressing this challenge is semi-supervised learning (SSL); this technique utilizes both labeled and unlabeled data during training, often with much less labeled data than unlabeled data, which is often relatively easy and inexpensive to obtain. In fact, SSL methods are particularly useful in applications where the cost of labeling data is especially expensive, such as medical analysis, natural language processing, or speech recognition. A subset of SSL methods that have achieved great success in various domains involves algorithms that integrate graph-based techniques. These procedures are popular due to the vast amount of information provided by the graphical framework. In this work, we propose an algebraic topology-based semi-supervised method called persistent Laplacian-enhanced graph MBO by integrating persistent spectral graph theory with the classical Merriman–Bence–Osher (MBO) scheme. Specifically, we use a filtration procedure to generate a sequence of chain complexes and associated families of simplicial complexes, from which we construct a family of persistent Laplacians. Overall, it is a very efficient procedure that requires much less labeled data to perform well compared to many ML techniques, and it can be adapted for both small and large datasets. We evaluate the performance of our method on classification, and the results indicate that the technique outperforms other existing semi-supervised algorithms.},
  archive      = {J_ML},
  author       = {Bhusal, Gokul and Merkurjev, Ekaterina and Wei, Guo-Wei},
  doi          = {10.1007/s10994-024-06616-w},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7267-7292},
  shortjournal = {Mach. Learn.},
  title        = {Persistent laplacian-enhanced algorithm for scarcely labeled data classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating large language models for user stance detection
on x (twitter). <em>ML</em>, <em>113</em>(10), 7243–7266. (<a
href="https://doi.org/10.1007/s10994-024-06587-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current stance detection methods employ topic-aligned data, resulting in many unexplored topics due to insufficient training samples. Large Language Models (LLMs) pre-trained on a vast amount of web data offer a viable solution when training data is unavailable. This work introduces Tweets2Stance - T2S, an unsupervised stance detection framework based on zero-shot classification, i.e. leveraging an LLM pre-trained on Natural Language Inference tasks. T2S detects a five-valued user’s stance on social-political statements by analyzing their X (Twitter) timeline. The Ground Truth of a user’s stance is obtained from Voting Advice Applications (VAAs). Through comprehensive experiments, a T2S’s optimal setting was identified for each election. Linguistic limitations related to the language model are further addressed by integrating state-of-the-art LLMs like GPT-4 and Mixtral into the T2S framework. The T2S framework’s generalization potential is demonstrated by measuring its performance (F1 and MAE scores) across nine datasets. These datasets were built by collecting tweets from competing parties’ Twitter accounts in nine political elections held in different countries from 2019 to 2021. The results, in terms of F1 and MAE scores, outperformed all baselines and approached the best scores for each election. This showcases the ability of T2S, particularly when combined with state-of-the-art LLMs, to generalize across different cultural-political contexts.},
  archive      = {J_ML},
  author       = {Gambini, Margherita and Senette, Caterina and Fagni, Tiziano and Tesconi, Maurizio},
  doi          = {10.1007/s10994-024-06587-y},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7243-7266},
  shortjournal = {Mach. Learn.},
  title        = {Evaluating large language models for user stance detection on x (Twitter)},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep negative correlation classification. <em>ML</em>,
<em>113</em>(10), 7223–7241. (<a
href="https://doi.org/10.1007/s10994-024-06604-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning serves as a straightforward way to improve the performance of almost any machine learning algorithm. Existing deep ensemble methods usually naïvely train many different models and then aggregate their predictions. This is not optimal in our view from two aspects: (1) Naïvely training multiple models adds much more computational burden, especially in the deep learning era; (2) Purely optimizing each base model without considering their interactions limits the diversity of ensemble and performance gains. We tackle these issues by proposing deep negative correlation classification (DNCC), in which the accuracy and diversity trade-off is systematically controlled by decomposing the loss function seamlessly into individual accuracy and the “correlation” between individual models and the ensemble. DNCC yields a deep classification ensemble where the individual estimator is both accurate and “negatively correlated”. Thanks to the optimized diversities, DNCC works well even when utilizing a shared network backbone, which significantly improves its efficiency when compared with most existing ensemble systems, as illustrated in Fig. 2. Extensive experiments on multiple benchmark datasets and network structures demonstrate the superiority of the proposed method.},
  archive      = {J_ML},
  author       = {Zhang, Le and Hou, Qibin and Liu, Yun and Bian, Jia-Wang and Xu, Xun and Zhou, Joey Tianyi and Zhu, Ce},
  doi          = {10.1007/s10994-024-06604-0},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7223-7241},
  shortjournal = {Mach. Learn.},
  title        = {Deep negative correlation classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Permutation-invariant linear classifiers. <em>ML</em>,
<em>113</em>(10), 7195–7221. (<a
href="https://doi.org/10.1007/s10994-024-06561-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invariant concept classes form the backbone of classification algorithms immune to specific data transformations, ensuring consistent predictions regardless of these alterations. However, this robustness can come at the cost of limited access to the original sample information, potentially impacting generalization performance. This study introduces an addition to these classes—the permutation-invariant linear classifiers. Distinguished by their structural characteristics, permutation-invariant linear classifiers are unaffected by permutations on feature vectors, a property not guaranteed by other non-constant linear classifiers. The study characterizes this new concept class, highlighting its constant capacity, independent of input dimensionality. In practical assessments using linear support vector machines, the permutation-invariant classifiers exhibit superior performance in permutation experiments on artificial datasets and real mutation profiles. Interestingly, they outperform general linear classifiers not only in permutation experiments but also in permutation-free settings, surpassing unconstrained counterparts. Additionally, findings from real mutation profiles support the significance of tumor mutational burden as a biomarker.},
  archive      = {J_ML},
  author       = {Lausser, Ludwig and Szekely, Robin and Kestler, Hans A.},
  doi          = {10.1007/s10994-024-06561-8},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7195-7221},
  shortjournal = {Mach. Learn.},
  title        = {Permutation-invariant linear classifiers},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning an adaptive forwarding strategy for mobile wireless
networks: Resource usage vs. latency. <em>ML</em>, <em>113</em>(10),
7157–7193. (<a
href="https://doi.org/10.1007/s10994-024-06601-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile wireless networks present several challenges for any learning system, due to uncertain and variable device movement, a decentralized network architecture, and constraints on network resources. In this work, we use deep reinforcement learning (DRL) to learn a scalable and generalizable forwarding strategy for such networks. We make the following contributions: (i) we use hierarchical RL to design DRL packet agents rather than device agents to capture the packet forwarding decisions that are made over time and improve training efficiency; (ii) we use relational features to ensure generalizability of the learned forwarding strategy to a wide range of network dynamics and enable offline training; and (iii) we incorporate both forwarding goals and network resource considerations into packet decision-making by designing a weighted reward function. Our results show that the forwarding strategy used by our DRL packet agent often achieves a similar delay per packet delivered as the oracle forwarding strategy and almost always outperforms all other strategies (including state-of-the-art strategies) in terms of delay, even on scenarios on which the DRL agent was not trained.},
  archive      = {J_ML},
  author       = {Manfredi, Victoria and Wolfe, Alicia P. and Zhang, Xiaolan and Wang, Bing},
  doi          = {10.1007/s10994-024-06601-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7157-7193},
  shortjournal = {Mach. Learn.},
  title        = {Learning an adaptive forwarding strategy for mobile wireless networks: Resource usage vs. latency},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural calibration of hidden inhomogeneous markov chains:
Information decompression in life insurance. <em>ML</em>,
<em>113</em>(10), 7129–7156. (<a
href="https://doi.org/10.1007/s10994-024-06551-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chains play a key role in a vast number of areas, including life insurance mathematics. Standard actuarial quantities as the premium value can be interpreted as compressed, lossy information about the underlying Markov process. We introduce a method to reconstruct the underlying Markov chain given collective information of a portfolio of contracts. Our neural architecture characterizes the process in a highly explainable way by explicitly providing one-step transition probabilities. Further, we provide an intrinsic, economic model validation to inspect the quality of the information decompression. Lastly, our methodology is successfully tested for a realistic data set of German term life insurance contracts.},
  archive      = {J_ML},
  author       = {Kiermayer, Mark and Weiß, Christian},
  doi          = {10.1007/s10994-024-06551-w},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {7129-7156},
  shortjournal = {Mach. Learn.},
  title        = {Neural calibration of hidden inhomogeneous markov chains: Information decompression in life insurance},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Exploiting sparsity and statistical
dependence in multivariate data fusion: An application to misinformation
detection for high-impact events. <em>ML</em>, <em>113</em>(9),
7127–7128. (<a
href="https://doi.org/10.1007/s10994-024-06580-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Damasceno, Lucas P. and Rexhepi, Egzona and Shafer, Allison and Whitehouse, Ian and Japkowicz, Nathalie and Cavalcante, Charles C. and Corizzo, Roberto and Boukouvalas, Zois},
  doi          = {10.1007/s10994-024-06580-5},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {7127-7128},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: exploiting sparsity and statistical dependence in multivariate data fusion: an application to misinformation detection for high-impact events},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Jaccard-constrained dense subgraph discovery. <em>ML</em>,
<em>113</em>(9), 7103–7125. (<a
href="https://doi.org/10.1007/s10994-024-06595-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding dense subgraphs is a core problem in graph mining with many applications in diverse domains. At the same time many real-world networks vary over time, that is, the dataset can be represented as a sequence of graph snapshots. Hence, it is natural to consider the question of finding dense subgraphs in a temporal network that are allowed to vary over time to a certain degree. In this paper, we search for dense subgraphs that have large pairwise Jaccard similarity coefficients. More formally, given a set of graph snapshots and input parameter $$\alpha$$ , we find a collection of dense subgraphs, with pairwise Jaccard index at least $$\alpha$$ , such that the sum of densities of the induced subgraphs is maximized. We prove that this problem is NP-hard and we present a greedy, iterative algorithm which runs in $${\mathcal {O}} \mathopen {} \left( nk^2 + m\right)$$ time per single iteration, where k is the length of the graph sequence and n and m denote number of vertices and total number of edges respectively. We also consider an alternative problem where subgraphs with large pairwise Jaccard indices are rewarded. We do this by incorporating the indices directly into the objective function. More formally, given a set of graph snapshots and a weight $$\lambda$$ , we find a collection of dense subgraphs such that the sum of densities of the induced subgraphs plus the sum of Jaccard indices, weighted by $$\lambda$$ , is maximized. We prove that this problem is NP-hard. To discover dense subgraphs with good objective value, we present an iterative algorithm which runs in $${\mathcal {O}} \mathopen {}\left( n^2k^2 + m \log n + k^3 n\right)$$ time per single iteration, and a greedy algorithm which runs in $${\mathcal {O}} \mathopen {}\left( n^2k^2 + m \log n + k^3 n\right)$$ time. We show experimentally that our algorithms are efficient, they can find ground truth in synthetic datasets and provide good results from real-world datasets. Finally, we present two case studies that show the usefulness of our problem.},
  archive      = {J_ML},
  author       = {Wickrama Arachchi, Chamalee and Tatti, Nikolaj},
  doi          = {10.1007/s10994-024-06595-y},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {7103-7125},
  shortjournal = {Mach. Learn.},
  title        = {Jaccard-constrained dense subgraph discovery},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution-free conformal joint prediction regions for
neural marked temporal point processes. <em>ML</em>, <em>113</em>(9),
7055–7102. (<a
href="https://doi.org/10.1007/s10994-024-06594-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for an event’s arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions. We first consider a simple but overly conservative approach that combines individual prediction regions for the event’s arrival time and mark. Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of arrival times and marks. By leveraging the dependencies between these two variables, this method excludes unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level. We also explore the generation of individual univariate prediction regions for events’ arrival times and marks through conformal regression and classification techniques. Moreover, we evaluate the stronger notion of conditional coverage. Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods.},
  archive      = {J_ML},
  author       = {Dheur, Victor and Bosser, Tanguy and Izbicki, Rafael and Ben Taieb, Souhaib},
  doi          = {10.1007/s10994-024-06594-z},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {7055-7102},
  shortjournal = {Mach. Learn.},
  title        = {Distribution-free conformal joint prediction regions for neural marked temporal point processes},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards efficient AutoML: A pipeline synthesis approach
leveraging pre-trained transformers for multimodal data. <em>ML</em>,
<em>113</em>(9), 7011–7053. (<a
href="https://doi.org/10.1007/s10994-024-06568-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an Automated Machine Learning (AutoML) framework specifically designed to efficiently synthesize end-to-end multimodal machine learning pipelines. Traditional reliance on the computationally demanding Neural Architecture Search is minimized through the strategic integration of pre-trained transformer models. This innovative approach enables the effective unification of diverse data modalities into high-dimensional embeddings, streamlining the pipeline development process. We leverage an advanced Bayesian Optimization strategy, informed by meta-learning, to facilitate the warm-starting of the pipeline synthesis, thereby enhancing computational efficiency. Our methodology demonstrates its potential to create advanced and custom multimodal pipelines within limited computational resources. Extensive testing across 23 varied multimodal datasets indicates the promise and utility of our framework in diverse scenarios. The results contribute to the ongoing efforts in the AutoML field, suggesting new possibilities for efficiently handling complex multimodal data. This research represents a step towards developing more efficient and versatile tools in multimodal machine learning pipeline development, acknowledging the collaborative and ever-evolving nature of this field.},
  archive      = {J_ML},
  author       = {Moharil, Ambarish and Vanschoren, Joaquin and Singh, Prabhant and Tamburri, Damian},
  doi          = {10.1007/s10994-024-06568-1},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {7011-7053},
  shortjournal = {Mach. Learn.},
  title        = {Towards efficient AutoML: A pipeline synthesis approach leveraging pre-trained transformers for multimodal data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Methodology and evaluation in sports analytics: Challenges,
approaches, and lessons learned. <em>ML</em>, <em>113</em>(9),
6977–7010. (<a
href="https://doi.org/10.1007/s10994-024-06585-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an explosion of data collected about sports. Because such data is extremely rich and complex, machine learning is increasingly being used to extract actionable insights from it. Typically, machine learning is used to build models and indicators that capture the skills, capabilities, and tendencies of athletes and teams. Such indicators and models are in turn used to inform decision-making at professional clubs. Designing these indicators requires paying careful attention to a number of subtle issues from a methodological and evaluation perspective. In this paper, we highlight these challenges in sports and discuss a variety of approaches for handling them. Methodologically, we highlight that dependencies affect how to perform data partitioning for evaluation as well as the need to consider contextual factors. From an evaluation perspective, we draw a distinction between evaluating the developed indicators themselves versus the underlying models that power them. We argue that both aspects must be considered, but that they require different approaches. We hope that this article helps bridge the gap between traditional sports expertise and modern data analytics by providing a structured framework with practical examples.},
  archive      = {J_ML},
  author       = {Davis, Jesse and Bransen, Lotte and Devos, Laurens and Jaspers, Arne and Meert, Wannes and Robberechts, Pieter and Van Haaren, Jan and Van Roy, Maaike},
  doi          = {10.1007/s10994-024-06585-0},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6977-7010},
  shortjournal = {Mach. Learn.},
  title        = {Methodology and evaluation in sports analytics: Challenges, approaches, and lessons learned},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial entropy as an inductive bias for vision
transformers. <em>ML</em>, <em>113</em>(9), 6945–6975. (<a
href="https://doi.org/10.1007/s10994-024-06570-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work on Vision Transformers (VTs) showed that introducing a local inductive bias in the VT architecture helps reducing the number of samples necessary for training. However, the architecture modifications lead to a loss of generality of the Transformer backbone, partially contradicting the push towards the development of uniform architectures, shared, e.g., by both the Computer Vision and the Natural Language Processing areas. In this work, we propose a different and complementary direction, in which a local bias is introduced using an auxiliary self-supervised task, performed jointly with standard supervised training. Specifically, we exploit the observation that the attention maps of VTs, when trained with self-supervision, can contain a semantic segmentation structure which does not spontaneously emerge when training is supervised. Thus, we explicitly encourage the emergence of this spatial clustering as a form of training regularization. In more detail, we exploit the assumption that, in a given image, objects usually correspond to few connected regions, and we propose a spatial formulation of the information entropy to quantify this object-based inductive bias. By minimizing the proposed spatial entropy, we include an additional self-supervised signal during training. Using extensive experiments, we show that the proposed regularization leads to equivalent or better results than other VT proposals which include a local bias by changing the basic Transformer architecture, and it can drastically boost the VT final accuracy when using small-medium training sets. The code is available at https://github.com/helia95/SAR .},
  archive      = {J_ML},
  author       = {Peruzzo, Elia and Sangineto, Enver and Liu, Yahui and De Nadai, Marco and Bi, Wei and Lepri, Bruno and Sebe, Nicu},
  doi          = {10.1007/s10994-024-06570-7},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6945-6975},
  shortjournal = {Mach. Learn.},
  title        = {Spatial entropy as an inductive bias for vision transformers},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICM ensemble with novel betting functions for concept drift.
<em>ML</em>, <em>113</em>(9), 6911–6944. (<a
href="https://doi.org/10.1007/s10994-024-06593-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study builds upon our previous work by introducing a refined Inductive Conformal Martingale (ICM) approach for addressing Concept Drift. Specifically, we enhance our previously proposed CAUTIOUS betting function to incorporate multiple density estimators for improving detection ability. We also combine this betting function with two base estimators that have not been previously utilized within the ICM framework: the Interpolated Histogram and Nearest Neighbor Density Estimators. We assess these extensions using both a single ICM and an ensemble of ICMs. For the latter, we conduct a comprehensive experimental investigation into the influence of the ensemble size on prediction accuracy and the number of available predictions. Our experimental results on four benchmark datasets demonstrate that the proposed approach surpasses our previous methodology in terms of performance while matching or in many cases exceeding that of three contemporary state-of-the-art techniques.},
  archive      = {J_ML},
  author       = {Eliades, Charalambos and Papadopoulos, Harris},
  doi          = {10.1007/s10994-024-06593-0},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6911-6944},
  shortjournal = {Mach. Learn.},
  title        = {ICM ensemble with novel betting functions for concept drift},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XAI-TRIS: Non-linear image benchmarks to quantify false
positive post-hoc attribution of feature importance. <em>ML</em>,
<em>113</em>(9), 6871–6910. (<a
href="https://doi.org/10.1007/s10994-024-06574-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of ‘explainable’ artificial intelligence (XAI) has produced highly acclaimed methods that seek to make the decisions of complex machine learning (ML) methods ‘understandable’ to humans, for example by attributing ‘importance’ scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for one linear and three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI methods are often unable to significantly outperform random performance baselines and edge detection methods, attributing false-positive importance to features with no statistical relationship to the prediction target rather than truly important features. Moreover, we demonstrate that explanations derived from different model architectures can be vastly different; thus, prone to misinterpretation even under controlled conditions.},
  archive      = {J_ML},
  author       = {Clark, Benedict and Wilming, Rick and Haufe, Stefan},
  doi          = {10.1007/s10994-024-06574-3},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6871-6910},
  shortjournal = {Mach. Learn.},
  title        = {XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc attribution of feature importance},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partitioned least squares. <em>ML</em>, <em>113</em>(9),
6839–6869. (<a
href="https://doi.org/10.1007/s10994-024-06582-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear least squares is one of the most widely used regression methods in many fields. The simplicity of the model allows this method to be used when data is scarce and allows practitioners to gather some insight into the problem by inspecting the values of the learnt parameters. In this paper we propose a variant of the linear least squares model allowing practitioners to partition the input features into groups of variables that they require to contribute similarly to the final result. We show that the new formulation is not convex and provide two alternative methods to deal with the problem: one non-exact method based on an alternating least squares approach; and one exact method based on a reformulation of the problem. We show the correctness of the exact method and compare the two solutions showing that the exact solution provides better results in a fraction of the time required by the alternating least squares solution (when the number of partitions is small). We also provide a branch and bound algorithm that can be used in place of the exact method when the number of partitions is too large as well as a proof of NP-completeness of the optimization problem.},
  archive      = {J_ML},
  author       = {Esposito, Roberto and Cerrato, Mattia and Locatelli, Marco},
  doi          = {10.1007/s10994-024-06582-3},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6839-6869},
  shortjournal = {Mach. Learn.},
  title        = {Partitioned least squares},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LaMMOn: Language model combined graph neural network for
multi-target multi-camera tracking in online scenarios. <em>ML</em>,
<em>113</em>(9), 6811–6837. (<a
href="https://doi.org/10.1007/s10994-024-06592-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target multi-camera tracking is crucial to intelligent transportation systems. Numerous recent studies have been undertaken to address this issue. Nevertheless, using the approaches in real-world situations is challenging due to the scarcity of publicly available data and the laborious process of manually annotating the new dataset and creating a tailored rule-based matching system for each camera scenario. To address this issue, we present a novel solution termed LaMMOn, an end-to-end transformer and graph neural network-based multi-camera tracking model. LaMMOn consists of three main modules: (1) Language Model Detection (LMD) for object detection; (2) Language and Graph Model Association module (LGMA) for object tracking and trajectory clustering; (3) Text-to-embedding module (T2E) that overcome the problem of data limitation by synthesizing the object embedding from defined texts. LaMMOn can be run online in real-time scenarios and achieve a competitive result on many datasets, e.g., CityFlow (HOTA 76.46%), I24 (HOTA 25.7%), and TrackCUIP (HOTA 80.94%) with an acceptable FPS (from 12.20 to 13.37) for an online application.},
  archive      = {J_ML},
  author       = {Nguyen, Tuan T. and Nguyen, Hoang H. and Sartipi, Mina and Fisichella, Marco},
  doi          = {10.1007/s10994-024-06592-1},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6811-6837},
  shortjournal = {Mach. Learn.},
  title        = {LaMMOn: Language model combined graph neural network for multi-target multi-camera tracking in online scenarios},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L2XGNN: Learning to explain graph neural networks.
<em>ML</em>, <em>113</em>(9), 6787–6809. (<a
href="https://doi.org/10.1007/s10994-024-06576-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2xGnn, a framework for explainable GNNs which provides faithful explanations by design. L2xGnn learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2xGnn is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2xGnn achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2xGnn is able to identify motifs responsible for the graph’s properties it is intended to predict.},
  archive      = {J_ML},
  author       = {Serra, Giuseppe and Niepert, Mathias},
  doi          = {10.1007/s10994-024-06576-1},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6787-6809},
  shortjournal = {Mach. Learn.},
  title        = {L2XGNN: Learning to explain graph neural networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable dating of greek papyri images. <em>ML</em>,
<em>113</em>(9), 6765–6786. (<a
href="https://doi.org/10.1007/s10994-024-06589-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greek literary papyri, which are unique witnesses of antique literature, do not usually bear a date. They are thus currently dated based on palaeographical methods, with broad approximations which often span more than a century. We created a dataset of 242 images of papyri written in “bookhand” scripts whose date can be securely assigned, and we used it to train algorithms for the task of dating, showing its challenging nature. To address data scarcity, we extended our dataset by segmenting each image into its respective text lines. By using the line-based version of our dataset, we trained a Convolutional Neural Network, equipped with a fragmentation-based augmentation strategy, and we achieved a mean absolute error of 54 years. The results improve further when the task is cast as a multi-class classification problem, predicting the century. Using our network, we computed precise date estimations for papyri whose date is disputed or vaguely defined, employing explainability to understand dating-driving features.},
  archive      = {J_ML},
  author       = {Pavlopoulos, John and Konstantinidou, Maria and Perdiki, Elpida and Marthot-Santaniello, Isabelle and Essler, Holger and Vardakas, Georgios and Likas, Aristidis},
  doi          = {10.1007/s10994-024-06589-w},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6765-6786},
  shortjournal = {Mach. Learn.},
  title        = {Explainable dating of greek papyri images},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressed sensing: A discrete optimization approach.
<em>ML</em>, <em>113</em>(9), 6725–6764. (<a
href="https://doi.org/10.1007/s10994-024-06577-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression, image reconstruction, and multi-label learning. We introduce an $$\ell _2$$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve small-scale instances of CS to certifiable optimality. When compared against solutions produced by three state of the art benchmark methods on synthetic data, our numerical results show that our approach produces solutions that are on average $$6.22\%$$ more sparse. When compared only against the experiment-wise best performing benchmark method on synthetic data, our approach produces solutions that are on average $$3.10\%$$ more sparse. On real world ECG data, for a given $$\ell _2$$ reconstruction error our approach produces solutions that are on average $$9.95\%$$ more sparse than benchmark methods ( $$3.88\%$$ more sparse if only compared against the best performing benchmark), while for a given sparsity level our approach produces solutions that have on average $$10.77\%$$ lower reconstruction error than benchmark methods ( $$1.42\%$$ lower error if only compared against the best performing benchmark). When used as a component of a multi-label classification algorithm, our approach achieves greater classification accuracy than benchmark compressed sensing methods. This improved accuracy comes at the cost of an increase in computation time by several orders of magnitude. Thus, for applications where runtime is not of critical importance, leveraging integer optimization can yield sparser and lower error solutions to CS than existing benchmarks.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Johnson, Nicholas A. G.},
  doi          = {10.1007/s10994-024-06577-0},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6725-6764},
  shortjournal = {Mach. Learn.},
  title        = {Compressed sensing: A discrete optimization approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Moreau-yoshida variational transport: A general framework
for solving regularized distributional optimization problems.
<em>ML</em>, <em>113</em>(9), 6697–6724. (<a
href="https://doi.org/10.1007/s10994-024-06586-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a general optimization problem involving the minimization of a composite objective functional defined over a class of probability distributions. The objective function consists of two components: one assumed to have a variational representation, and the other expressed in terms of the expectation operator of a possibly nonsmooth convex regularizer function. Such a regularized distributional optimization problem widely appears in machine learning and statistics, including proximal Monte-Carlo sampling, Bayesian inference, and generative modeling for regularized estimation and generation. Our proposed method, named Moreau-Yoshida Variational Transport (MYVT), introduces a novel approach to tackle this regularized distributional optimization problem. First, as the name suggests, our method utilizes the Moreau-Yoshida envelope to provide a smooth approximation of the nonsmooth function in the objective. Second, we reformulate the approximate problem as a concave-convex saddle point problem by leveraging the variational representation. Subsequently, we develop an efficient primal–dual algorithm to approximate the saddle point. Furthermore, we provide theoretical analyses and present experimental results to showcase the effectiveness of the proposed method.},
  archive      = {J_ML},
  author       = {Nguyen, Dai Hai and Sakurai, Tetsuya},
  doi          = {10.1007/s10994-024-06586-z},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6697-6724},
  shortjournal = {Mach. Learn.},
  title        = {Moreau-yoshida variational transport: A general framework for solving regularized distributional optimization problems},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional bias in monolingual english language models.
<em>ML</em>, <em>113</em>(9), 6663–6696. (<a
href="https://doi.org/10.1007/s10994-024-06555-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), pre-trained language models (LLMs) are widely employed and refined for various tasks. These models have shown considerable social and geographic biases creating skewed or even unfair representations of certain groups. Research focuses on biases toward L2 (English as a second language) regions but neglects bias within L1 (first language) regions. In this work, we ask if there is regional bias within L1 regions already inherent in pre-trained LLMs and, if so, what the consequences are in terms of downstream model performance. We contribute an investigation framework specifically tailored for low-resource regions, offering a method to identify bias without imposing strict requirements for labeled datasets. Our research reveals subtle geographic variations in the word embeddings of BERT, even in cultures traditionally perceived as similar. These nuanced features, once captured, have the potential to significantly impact downstream tasks. Generally, models exhibit comparable performance on datasets that share similarities, and conversely, performance may diverge when datasets differ in their nuanced features embedded within the language. It is crucial to note that estimating model performance solely based on standard benchmark datasets may not necessarily apply to the datasets with distinct features from the benchmark datasets. Our proposed framework plays a pivotal role in identifying and addressing biases detected in word embeddings, particularly evident in low-resource regions such as New Zealand.},
  archive      = {J_ML},
  author       = {Lyu, Jiachen and Dost, Katharina and Koh, Yun Sing and Wicker, Jörg},
  doi          = {10.1007/s10994-024-06555-6},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6663-6696},
  shortjournal = {Mach. Learn.},
  title        = {Regional bias in monolingual english language models},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal predictions for probabilistically robust scalable
machine learning classification. <em>ML</em>, <em>113</em>(9),
6645–6661. (<a
href="https://doi.org/10.1007/s10994-024-06571-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal predictions make it possible to define reliable and robust learning algorithms. But they are essentially a method for evaluating whether an algorithm is good enough to be used in practice. To define a reliable learning framework for classification from the very beginning of its design, the concept of scalable classifier was introduced to generalize the concept of classical classifier by linking it to statistical order theory and probabilistic learning theory. In this paper, we analyze the similarities between scalable classifiers and conformal predictions by introducing a new definition of a score function and defining a special set of input variables, the conformal safety set, which can identify patterns in the input space that satisfy the error coverage guarantee, i.e., that the probability of observing the wrong (possibly unsafe) label for points belonging to this set is bounded by a predefined $$\varepsilon$$ error level. We demonstrate the practical implications of this framework through an application in cybersecurity for identifying DNS tunneling attacks. Our work contributes to the development of probabilistically robust and reliable machine learning models.},
  archive      = {J_ML},
  author       = {Carlevaro, Alberto and Alamo, Teodoro and Dabbene, Fabrizio and Mongelli, Maurizio},
  doi          = {10.1007/s10994-024-06571-6},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6645-6661},
  shortjournal = {Mach. Learn.},
  title        = {Conformal predictions for probabilistically robust scalable machine learning classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural discovery of balance-aware polarized communities.
<em>ML</em>, <em>113</em>(9), 6611–6644. (<a
href="https://doi.org/10.1007/s10994-024-06581-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed graphs are a model to depict friendly (positive) or antagonistic (negative) interactions (edges) among users (nodes). 2-Polarized-Communities (2pc) is a well-established combinatorial-optimization problem whose goal is to find two polarized communities from a signed graph, i.e., two subsets of nodes (disjoint, but not necessarily covering the entire node set) which exhibit a high number of both intra-community positive edges and negative inter-community edges. The state of the art in 2pc suffers from the limitations that (i) existing methods rely on a single (optimal) solution to a continuous relaxation of the problem in order to produce the ultimate discrete solution via rounding, and (ii) 2pc objective function comes with no control on size balance among communities. In this paper, we provide advances to the 2pc problem by addressing both these limitations, with a twofold contribution. First, we devise a novel neural approach that allows for soundly and elegantly explore a variety of suboptimal solutions to the relaxed 2pc problem, so as to pick the one that leads to the best discrete solution after rounding. Second, we introduce a generalization of 2pc objective function – termed $$\gamma $$ -polarity – which fosters size balance among communities, and we incorporate it into the proposed machine-learning framework. Extensive experiments attest high accuracy of our approach, its superiority over the state of the art, and capability of function $$\gamma $$ -polarity to discover high-quality size-balanced communities.},
  archive      = {J_ML},
  author       = {Gullo, Francesco and Mandaglio, Domenico and Tagarelli, Andrea},
  doi          = {10.1007/s10994-024-06581-4},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6611-6644},
  shortjournal = {Mach. Learn.},
  title        = {Neural discovery of balance-aware polarized communities},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast linear model trees by PILOT. <em>ML</em>,
<em>113</em>(9), 6561–6610. (<a
href="https://doi.org/10.1007/s10994-024-06590-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear model trees are regression trees that incorporate linear models in the leaf nodes. This preserves the intuitive interpretation of decision trees and at the same time enables them to better capture linear relationships, which is hard for standard decision trees. But most existing methods for fitting linear model trees are time consuming and therefore not scalable to large data sets. In addition, they are more prone to overfitting and extrapolation issues than standard regression trees. In this paper we introduce PILOT, a new algorithm for linear model trees that is fast, regularized, stable and interpretable. PILOT trains in a greedy fashion like classic regression trees, but incorporates an L2 boosting approach and a model selection rule for fitting linear models in the nodes. The abbreviation PILOT stands for PIecewise Linear Organic Tree, where ‘organic’ refers to the fact that no pruning is carried out. PILOT has the same low time and space complexity as CART without its pruning. An empirical study indicates that PILOT tends to outperform standard decision trees and other linear model trees on a variety of data sets. Moreover, we prove its consistency in an additive model setting under weak assumptions. When the data is generated by a linear model, the convergence rate is polynomial.},
  archive      = {J_ML},
  author       = {Raymaekers, Jakob and Rousseeuw, Peter J. and Verdonck, Tim and Yao, Ruicong},
  doi          = {10.1007/s10994-024-06590-3},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6561-6610},
  shortjournal = {Mach. Learn.},
  title        = {Fast linear model trees by PILOT},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FairMOE: Counterfactually-fair mixture of experts with
levels of interpretability. <em>ML</em>, <em>113</em>(9), 6539–6559. (<a
href="https://doi.org/10.1007/s10994-024-06583-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of artificial intelligence in our everyday lives, the need for human interpretation of machine learning models’ predictions emerges as a critical issue. Generally, interpretability is viewed as a binary notion with a performance trade-off. Either a model is fully-interpretable but lacks the ability to capture more complex patterns in the data, or it is a black box. In this paper, we argue that this view is severely limiting and that instead interpretability should be viewed as a continuous domain-informed concept. We leverage the well-known Mixture of Experts architecture with user-defined limits on non-interpretability. We extend this idea with a counterfactual fairness module to ensure the selection of consistently fair experts: FairMOE. We perform an extensive experimental evaluation with fairness-related data sets and compare our proposal against state-of-the-art methods. Our results demonstrate that FairMOE is competitive with the leading fairness-aware algorithms in both fairness and predictive measures while providing more consistent performance, competitive scalability, and, most importantly, greater interpretability.},
  archive      = {J_ML},
  author       = {Germino, Joe and Moniz, Nuno and Chawla, Nitesh V.},
  doi          = {10.1007/s10994-024-06583-2},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6539-6559},
  shortjournal = {Mach. Learn.},
  title        = {FairMOE: Counterfactually-fair mixture of experts with levels of interpretability},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). PROUD: PaRetO-gUided diffusion model for multi-objective
generation. <em>ML</em>, <em>113</em>(9), 6511–6538. (<a
href="https://doi.org/10.1007/s10994-024-06575-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in the realm of deep generative models focus on generating samples that satisfy multiple desired properties. However, prevalent approaches optimize these property functions independently, thus omitting the trade-offs among them. In addition, the property optimization is often improperly integrated into the generative models, resulting in an unnecessary compromise on generation quality (i.e., the quality of generated samples). To address these issues, we formulate a constrained optimization problem. It seeks to optimize generation quality while ensuring that generated samples reside at the Pareto front of multiple property objectives. Such a formulation enables the generation of samples that cannot be further improved simultaneously on the conflicting property functions and preserves good quality of generated samples.Building upon this formulation, we introduce the ParetO-gUided Diffusion model (PROUD), wherein the gradients in the denoising process are dynamically adjusted to enhance generation quality while the generated samples adhere to Pareto optimality. Experimental evaluations on image generation and protein generation tasks demonstrate that our PROUD consistently maintains superior generation quality while approaching Pareto optimality across multiple property functions compared to various baselines},
  archive      = {J_ML},
  author       = {Yao, Yinghua and Pan, Yuangang and Li, Jing and Tsang, Ivor and Yao, Xin},
  doi          = {10.1007/s10994-024-06575-2},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6511-6538},
  shortjournal = {Mach. Learn.},
  title        = {PROUD: PaRetO-gUided diffusion model for multi-objective generation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample complexity of variance-reduced policy gradient:
Weaker assumptions and lower bounds. <em>ML</em>, <em>113</em>(9),
6475–6510. (<a
href="https://doi.org/10.1007/s10994-024-06573-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several variance-reduced versions of REINFORCE based on importance sampling achieve an improved $$O(\epsilon ^{-3})$$ sample complexity to find an $$\epsilon$$ -stationary point, under an unrealistic assumption on the variance of the importance weights. In this paper, we propose the Defensive Policy Gradient (DEF-PG) algorithm, based on defensive importance sampling, achieving the same result without any assumption on the variance of the importance weights. We also show that this is not improvable by establishing a matching $$\Omega (\epsilon ^{-3})$$ lower bound, and that REINFORCE with its $$O(\epsilon ^{-4})$$ sample complexity is actually optimal under weaker assumptions on the policy class. Numerical simulations show promising results for the proposed technique compared to similar algorithms based on vanilla importance sampling.},
  archive      = {J_ML},
  author       = {Paczolay, Gabor and Papini, Matteo and Metelli, Alberto Maria and Harmati, Istvan and Restelli, Marcello},
  doi          = {10.1007/s10994-024-06573-4},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6475-6510},
  shortjournal = {Mach. Learn.},
  title        = {Sample complexity of variance-reduced policy gradient: Weaker assumptions and lower bounds},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidential uncertainty sampling strategies for active
learning. <em>ML</em>, <em>113</em>(9), 6453–6474. (<a
href="https://doi.org/10.1007/s10994-024-06567-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies in active learning, particularly in uncertainty sampling, have focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, the aim is to simplify the computational process while eliminating the dependence on observations. Crucially, the inherent uncertainty in the labels is considered, i.e. the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which tackles the exploration–exploitation dilemma, and sampling by evidential epistemic uncertainty, which extends the concept of reducible uncertainty within the evidential framework, both using the theory of belief functions. Experimental results in active learning demonstrate that our proposed method can outperform uncertainty sampling.},
  archive      = {J_ML},
  author       = {Hoarau, Arthur and Lemaire, Vincent and Le Gall, Yolande and Dubois, Jean-Christophe and Martin, Arnaud},
  doi          = {10.1007/s10994-024-06567-2},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6453-6474},
  shortjournal = {Mach. Learn.},
  title        = {Evidential uncertainty sampling strategies for active learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure and fast asynchronous vertical federated learning via
cascaded hybrid optimization. <em>ML</em>, <em>113</em>(9), 6413–6451.
(<a href="https://doi.org/10.1007/s10994-024-06541-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) is gaining increasing attention due to its ability to enable multiple parties to collaboratively train a privacy-preserving model using vertically partitioned data. Recent research has highlighted the advantages of using zeroth-order optimization (ZOO) in developing practical VFL algorithms. However, a significant drawback of ZOO-based VFL is its slow convergence rate, which limits its applicability in handling large modern models. To address this issue, we propose a cascaded hybrid optimization method for VFL. In this method, the downstream models (clients) are trained using ZOO to ensure privacy and prevent the sharing of internal information. Simultaneously, the upstream model (server) is updated locally using first-order optimization, which significantly improves the convergence rate. This approach allows for the training of large models without compromising privacy and security. We theoretically prove that our VFL method achieves faster convergence compared to ZOO-based VFL because the convergence rate of our framework is not limited by the size of the server model, making it effective for training large models. Extensive experiments demonstrate that our method achieves faster convergence than ZOO-based VFL while maintaining an equivalent level of privacy protection. Additionally, we demonstrate the feasibility of training large models using our method.},
  archive      = {J_ML},
  author       = {Wang, Ganyu and Zhang, Qingsong and Li, Xiang and Wang, Boyu and Gu, Bin and Ling, Charles X.},
  doi          = {10.1007/s10994-024-06541-y},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6413-6451},
  shortjournal = {Mach. Learn.},
  title        = {Secure and fast asynchronous vertical federated learning via cascaded hybrid optimization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete-time graph neural networks for transaction
prediction in web3 social platforms. <em>ML</em>, <em>113</em>(9),
6395–6412. (<a
href="https://doi.org/10.1007/s10994-024-06579-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Web3 social platforms, i.e. social web applications that rely on blockchain technology to support their functionalities, interactions among users are usually multimodal, from common social interactions such as following, liking, or posting, to specific relations given by crypto-token transfers facilitated by the blockchain. In this dynamic and intertwined networked context, modeled as a financial network, our main goals are (i) to predict whether a pair of users will be involved in a financial transaction, i.e. the transaction prediction task, even using textual information produced by users, and (ii) to verify whether performances may be enhanced by textual content. To address the above issues, we compared current snapshot-based temporal graph learning methods and developed T3GNN, a solution based on state-of-the-art temporal graph neural networks’ design, which integrates fine-tuned sentence embeddings and a simple yet effective graph-augmentation strategy for representing content, and historical negative sampling. We evaluated models in a Web3 context by leveraging a novel high-resolution temporal dataset, collected from one of the most used Web3 social platforms, which spans more than one year of financial interactions as well as published textual content. The experimental evaluation has shown that T3GNN consistently achieved the best performance over time and for most of the snapshots. Furthermore, through an extensive analysis of the performance of our model, we show that, despite the graph structure being crucial for making predictions, textual content contains useful information for forecasting transactions, highlighting an interplay between users’ interests and economic relationships in Web3 platforms. Finally, the evaluation has also highlighted the importance of adopting sampling methods alternative to random negative sampling when dealing with prediction tasks on temporal networks.},
  archive      = {J_ML},
  author       = {Dileo, Manuel and Zignani, Matteo},
  doi          = {10.1007/s10994-024-06579-y},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6395-6412},
  shortjournal = {Mach. Learn.},
  title        = {Discrete-time graph neural networks for transaction prediction in web3 social platforms},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantitative gaussian approximation of randomly initialized
deep neural networks. <em>ML</em>, <em>113</em>(9), 6373–6393. (<a
href="https://doi.org/10.1007/s10994-024-06578-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given any deep fully connected neural network, initialized with random Gaussian parameters, we bound from above the quadratic Wasserstein distance between its output distribution and a suitable Gaussian process. Our explicit inequalities indicate how the hidden and output layers sizes affect the Gaussian behaviour of the network and quantitatively recover the distributional convergence results in the wide limit, i.e., if all the hidden layers sizes become large.},
  archive      = {J_ML},
  author       = {Basteri, Andrea and Trevisan, Dario},
  doi          = {10.1007/s10994-024-06578-z},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6373-6393},
  shortjournal = {Mach. Learn.},
  title        = {Quantitative gaussian approximation of randomly initialized deep neural networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kalt: Generating adversarial explainable chinese legal
texts. <em>ML</em>, <em>113</em>(9), 6341–6371. (<a
href="https://doi.org/10.1007/s10994-024-06572-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial examples (AEs), which are well-designed input samples with imperceptible perturbations. Existing methods generate AEs to evaluate the robustness of DNN-based natural language processing models. However, the AE attack performance significantly degrades in some verticals, such as law, due to overlooking essential domain knowledge. To generate explainable Chinese legal adversarial texts, we introduce legal knowledge and propose a novel black-box approach, knowledge-aware law tricker (KALT), in the framework of adversarial text generation based on word importance. Firstly, we invent a legal knowledge extraction method based on KeyBERT. The knowledge contains unique features from each category and shared features among different categories. Additionally, we design two perturbation strategies, Strengthen Similar Label and Weaken Original Label, to selectively perturb the two types of features, which can significantly reduce the classification accuracy of the target model. These two perturbation strategies can be regarded as components, which can be conveniently integrated into any perturbation method to enhance attack performance. Furthermore, we propose a strong hybrid perturbation method to introduce perturbation into the original texts. The perturbation method combines seven representative perturbation methods for Chinese. Finally, we design a formula to calculate interpretability scores, quantifying the interpretability of adversarial text generation methods. Experimental results demonstrate that KALT can effectively generate explainable Chinese legal adversarial texts that can be misclassified with high confidence and achieve excellent attack performance against the powerful Chinese BERT.},
  archive      = {J_ML},
  author       = {Zhang, Yunting and Li, Shang and Ye, Lin and Zhang, Hongli and Chen, Zhe and Fang, Binxing},
  doi          = {10.1007/s10994-024-06572-5},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6341-6371},
  shortjournal = {Mach. Learn.},
  title        = {Kalt: Generating adversarial explainable chinese legal texts},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighborhood relation-based incremental label propagation
algorithm for partially labeled hybrid data. <em>ML</em>,
<em>113</em>(9), 6293–6339. (<a
href="https://doi.org/10.1007/s10994-024-06560-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label propagation can rapidly predict the labels of unlabeled objects as the correct answers from a small amount of given label information, which can enhance the performance of subsequent machine learning tasks. Most existing label propagation methods are proposed for static data. However, in many applications, real datasets including multiple feature value types and massive unlabeled objects vary dynamically over time, whereas applying these label propagation methods for dynamic partially labeled hybrid data will be a huge drain due to recalculating from scratch when the data changes every time. To improve efficiency, a novel incremental label propagation algorithm based on neighborhood relation (ILPN) is developed in this paper. Specifically, we first construct graph structures by utilizing neighborhood relations to eliminate unnecessary label information. Then, a new label propagation strategy is designed in consideration of the weights assigned to each class so that it does not rely on a probabilistic transition matrix to fix the structure for propagation. On this basis, a new label propagation algorithm called neighborhood relation-based label propagation (LPN) is developed. For the dynamic partially labeled hybrid data, we integrate incremental learning into LPN and develop an updating mechanism that allows incremental label propagation over previous label propagation results and graph structures, rather than recalculating from scratch. Finally, extensive experiments on UCI datasets validate that our proposed algorithm LPN can outperform other label propagation algorithms in speed on the premise of ensuring accuracy. Especially for simulated dynamic data, the incremental algorithm ILPN is more efficient than other non-incremental methods with the variation of the partially labeled hybrid data.},
  archive      = {J_ML},
  author       = {Shu, Wenhao and Cao, Dongtao and Qian, Wenbin and Li, Shipeng},
  doi          = {10.1007/s10994-024-06560-9},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6293-6339},
  shortjournal = {Mach. Learn.},
  title        = {Neighborhood relation-based incremental label propagation algorithm for partially labeled hybrid data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-detect: Explainable adversarial patch detection for object
detectors in retail. <em>ML</em>, <em>113</em>(9), 6273–6292. (<a
href="https://doi.org/10.1007/s10994-024-06548-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection models, which are widely used in various domains (such as retail), have been shown to be vulnerable to adversarial attacks. Existing methods for detecting adversarial attacks on object detectors have had difficulty detecting new real-life attacks. We present X-Detect, a novel adversarial patch detector that can: (1) detect adversarial samples in real time, allowing the defender to take preventive action; (2) provide explanations for the alerts raised to support the defender’s decision-making process, and (3) handle unfamiliar threats in the form of new attacks. Given a new scene, X-Detect uses an ensemble of explainable-by-design detectors that utilize object extraction, scene manipulation, and feature transformation techniques to determine whether an alert needs to be raised. X-Detect was evaluated in both the physical and digital space using five different attack scenarios (including adaptive attacks) and the benchmark COCO dataset and our new Superstore dataset. The physical evaluation was performed using a smart shopping cart setup in real-world settings and included 17 adversarial patch attacks recorded in 1700 adversarial videos. The results showed that X-Detect outperforms the state-of-the-art methods in distinguishing between benign and adversarial scenes for all attack scenarios while maintaining a 0% FPR (no false alarms) and providing actionable explanations for the alerts raised. A demo is available.},
  archive      = {J_ML},
  author       = {Hofman, Omer and Giloni, Amit and Hayun, Yarin and Morikawa, Ikuya and Shimizu, Toshiya and Elovici, Yuval and Shabtai, Asaf},
  doi          = {10.1007/s10994-024-06548-5},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6273-6292},
  shortjournal = {Mach. Learn.},
  title        = {X-detect: Explainable adversarial patch detection for object detectors in retail},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-order proximity and relation analysis for cross-network
heterogeneous node classification. <em>ML</em>, <em>113</em>(9),
6247–6272. (<a
href="https://doi.org/10.1007/s10994-024-06566-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-network node classification aims to leverage the labeled nodes from a source network to assist the learning in a target network. Existing approaches work mainly in homogeneous settings, i.e., the nodes of the source and target networks are characterized by the same features. However, in many practical applications, nodes from different networks usually have heterogeneous features. To handle this issue, in this paper, we study the cross-network node classification under heterogeneous settings, i.e., cross-network heterogeneous node classification. Specifically, we propose a new model called High-order Proximity and Relation Analysis, which studies the high-order proximity in each network and the high-order relation between nodes across the networks to obtain two kinds of features. Subsequently, these features are exploited to learn the final effective representations by introducing a feature matching mechanism and an adversarial domain adaptation. We perform extensive experiments on several real-world datasets and make comparisons with existing baseline methods. Experimental results demonstrate the effectiveness of the proposed model.},
  archive      = {J_ML},
  author       = {Wu, Hanrui and Wu, Yanxin and Li, Nuosi and Yang, Min and Zhang, Jia and Ng, Michael K. and Long, Jinyi},
  doi          = {10.1007/s10994-024-06566-3},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6247-6272},
  shortjournal = {Mach. Learn.},
  title        = {High-order proximity and relation analysis for cross-network heterogeneous node classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REFUEL: Rule extraction for imbalanced neural node
classification. <em>ML</em>, <em>113</em>(9), 6227–6246. (<a
href="https://doi.org/10.1007/s10994-024-06569-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced graph node classification is a highly relevant and challenging problem in many real-world applications. The inherent data scarcity, a central characteristic of this task, substantially limits the performance of neural classification models driven solely by data. Given the limited instances of relevant nodes and complex graph structures, current methods fail to capture the distinct characteristics of node attributes and graph patterns within the underrepresented classes. In this article, we propose REFUEL—a novel approach for highly imbalanced node classification problems in graphs. Whereas symbolic and neural methods have complementary strengths and weaknesses when applied to such problems, REFUEL combines the power of symbolic and neural learning in a novel neural rule-extraction architecture. REFUEL captures the class semantics in the automatically extracted rule vectors. Then, REFUEL augments the graph nodes with the extracted rules vectors and adopts a Graph Attention Network-based neural node embedding, enhancing the downstream neural node representation. Our evaluation confirms the effectiveness of the proposed REFUEL approach for three real-world datasets with different minority class sizes. REFUEL achieves at least a 4% point improvement in precision on the minority classes of 1.5–2% compared to the baselines.},
  archive      = {J_ML},
  author       = {Markwald, Marco and Demidova, Elena},
  doi          = {10.1007/s10994-024-06569-0},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6227-6246},
  shortjournal = {Mach. Learn.},
  title        = {REFUEL: Rule extraction for imbalanced neural node classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised maximum variance unfolding. <em>ML</em>,
<em>113</em>(9), 6197–6226. (<a
href="https://doi.org/10.1007/s10994-024-06553-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum Variance Unfolding (MVU) is among the first methods in nonlinear dimensionality reduction for data visualization and classification. It aims to preserve local data structure and in the meantime push the variance among data as big as possible. However, MVU in general remains a computationally challenging problem and this may explain why it is less popular than other leading methods such as Isomap and t-SNE. In this paper, based on a key observation that the structure-preserving term in MVU is actually the squared stress in Multi-Dimensional Scaling (MDS), we replace the term with the stress function from MDS, resulting in a model that is usable. The property of the usability guarantees the “crowding phenomenon” will not happen in the dimension reduced results. The new model also allows us to combine label information and hence we call it the supervised MVU (SMVU). We then develop a fast algorithm that is based on Euclidean distance matrix optimization. By making use of the majorization-mininmization technique, the algorithm at each iteration solves a number of one-dimensional optimization problems, each having a closed-form solution. This strategy significantly speeds up the computation. We demonstrate the advantage of SMVU on some standard data sets against a few leading algorithms including Isomap and t-SNE.},
  archive      = {J_ML},
  author       = {Yang, Deliang and Qi, Hou-Duo},
  doi          = {10.1007/s10994-024-06553-8},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6197-6226},
  shortjournal = {Mach. Learn.},
  title        = {Supervised maximum variance unfolding},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving interpretability via regularization of neural
activation sensitivity. <em>ML</em>, <em>113</em>(9), 6165–6196. (<a
href="https://doi.org/10.1007/s10994-024-06549-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art deep neural networks (DNNs) are highly effective at tackling many real-world tasks. However, their widespread adoption in mission-critical contexts is limited due to two major weaknesses - their susceptibility to adversarial attacks and their opaqueness. The former raises concerns about DNNs’ security and generalization in real-world conditions, while the latter, opaqueness, directly impacts interpretability. The lack of interpretability diminishes user trust as it is challenging to have confidence in a model’s decision when its reasoning is not aligned with human perspectives. In this research, we (1) examine the effect of adversarial robustness on interpretability, and (2) present a novel approach for improving DNNs’ interpretability that is based on the regularization of neural activation sensitivity. We evaluate the interpretability of models trained using our method to that of standard models and models trained using state-of-the-art adversarial robustness techniques. Our results show that adversarially robust models are superior to standard models, and that models trained using our proposed method are even better than adversarially robust models in terms of interpretability.(Code provided in supplementary material.)},
  archive      = {J_ML},
  author       = {Moshe, Ofir and Fidel, Gil and Bitton, Ron and Shabtai, Asaf},
  doi          = {10.1007/s10994-024-06549-4},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6165-6196},
  shortjournal = {Mach. Learn.},
  title        = {Improving interpretability via regularization of neural activation sensitivity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of data distribution on q-learning with function
approximation. <em>ML</em>, <em>113</em>(9), 6141–6163. (<a
href="https://doi.org/10.1007/s10994-024-06564-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the interplay between the data distribution and Q-learning-based algorithms with function approximation. We provide a unified theoretical and empirical analysis as to how different properties of the data distribution influence the performance of Q-learning-based algorithms. We connect different lines of research, as well as validate and extend previous results, being primarily focused on offline settings. First, we analyze the impact of the data distribution by using optimization as a tool to better understand which data distributions yield low concentrability coefficients. We motivate high-entropy distributions from a game-theoretical point of view and propose an algorithm to find the optimal data distribution from the point of view of concentrability. Second, from an empirical perspective, we introduce a novel four-state MDP specifically tailored to highlight the impact of the data distribution in the performance of Q-learning-based algorithms with function approximation. Finally, we experimentally assess the impact of the data distribution properties on the performance of two offline Q-learning-based algorithms under different environments. Our results attest to the importance of different properties of the data distribution such as entropy, coverage, and data quality (closeness to optimal policy).},
  archive      = {J_ML},
  author       = {Santos, Pedro P. and Carvalho, Diogo S. and Sardinha, Alberto and Melo, Francisco S.},
  doi          = {10.1007/s10994-024-06564-5},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6141-6163},
  shortjournal = {Mach. Learn.},
  title        = {The impact of data distribution on Q-learning with function approximation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for training larger networks for deep
reinforcement learning. <em>ML</em>, <em>113</em>(9), 6115–6139. (<a
href="https://doi.org/10.1007/s10994-024-06547-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning in computer vision and natural language processing communities can be attributed to the training of very deep neural networks with millions or billions of parameters, which can then be trained with massive amounts of data. However, a similar trend has largely eluded the training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during the training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address the training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of (1) wider networks with DenseNet connection, (2) decoupling representation learning from the training of RL, and (3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.},
  archive      = {J_ML},
  author       = {Ota, Kei and Jha, Devesh K. and Kanezaki, Asako},
  doi          = {10.1007/s10994-024-06547-6},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6115-6139},
  shortjournal = {Mach. Learn.},
  title        = {A framework for training larger networks for deep reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learning for heterogeneous treatment effect estimation
with closed-form solvers. <em>ML</em>, <em>113</em>(9), 6093–6114. (<a
href="https://doi.org/10.1007/s10994-024-06546-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method outperforms the existing meta-learning approaches and CATE estimation methods.},
  archive      = {J_ML},
  author       = {Iwata, Tomoharu and Chikahara, Yoichi},
  doi          = {10.1007/s10994-024-06546-7},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6093-6114},
  shortjournal = {Mach. Learn.},
  title        = {Meta-learning for heterogeneous treatment effect estimation with closed-form solvers},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting residual errors in nonlinear online prediction.
<em>ML</em>, <em>113</em>(9), 6065–6091. (<a
href="https://doi.org/10.1007/s10994-024-06554-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel online (or sequential) nonlinear prediction approach that incorporates the residuals, i.e., prediction errors in the past observations, as additional features for the current data. Including the past error terms in an online prediction algorithm naturally improves prediction performance significantly since this information is essential for an algorithm to adjust itself based on its past errors. These terms are well exploited in many linear statistical models such as ARMA, SES, and Holts-Winters models. However, the past error terms are rarely or in a certain sense not optimally exploited in nonlinear prediction models since training them requires complex nonlinear state-space modeling. To this end, for the first time in the literature, we introduce a nonlinear prediction framework that utilizes not only the current features but also the past error terms as additional features, thereby exploiting the residual state information in the error terms, i.e., the model’s performance on the past samples. Since the new feature vectors contain error terms that change with every update, our algorithm jointly optimizes the model parameters and the feature vectors simultaneously. We achieve this by introducing new update equations that handle the effects resulting from the changes in the feature vectors in an online manner. We use soft decision trees and neural networks as the nonlinear prediction algorithms since these are the most widely used methods in highly publicized competitions. However, as we show, our methods are generic and any algorithm supporting gradient calculations can be straightforwardly used. We show through our experiments on the well-known real-life competition datasets that our method significantly outperforms the state-of-the-art. We also provide the implementation of our approach including the source code to facilitate reproducibility ( https://github.com/ahmetberkerkoc/SDT-ARMA ).},
  archive      = {J_ML},
  author       = {Ilhan, Emirhan and Koc, Ahmet B. and Kozat, Suleyman S.},
  doi          = {10.1007/s10994-024-06554-7},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6065-6091},
  shortjournal = {Mach. Learn.},
  title        = {Exploiting residual errors in nonlinear online prediction},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating feature attribution methods in the image domain.
<em>ML</em>, <em>113</em>(9), 6019–6064. (<a
href="https://doi.org/10.1007/s10994-024-06550-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature attribution maps are a popular approach to highlight the most important pixels in an image for a given prediction of a model. Despite a recent growth in popularity and available methods, the objective evaluation of such attribution maps remains an open problem. Building on previous work in this domain, we investigate existing quality metrics and propose new variants of metrics for the evaluation of attribution maps. We confirm a recent finding that different quality metrics seem to measure different underlying properties of attribution maps, and extend this finding to a larger selection of attribution methods, quality metrics, and datasets. We also find that metric results on one dataset do not necessarily generalize to other datasets, and methods with desirable theoretical properties do not necessarily outperform computationally cheaper alternatives in practice. Based on these findings, we propose a general benchmarking approach to help guide the selection of attribution methods for a given use case. Implementations of attribution metrics and our experiments are available online ( https://github.com/arnegevaert/benchmark-general-imaging ).},
  archive      = {J_ML},
  author       = {Gevaert, Arne and Rousseau, Axel-Jan and Becker, Thijs and Valkenborg, Dirk and De Bie, Tijl and Saeys, Yvan},
  doi          = {10.1007/s10994-024-06550-x},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {6019-6064},
  shortjournal = {Mach. Learn.},
  title        = {Evaluating feature attribution methods in the image domain},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time error bounds for greedy-GQ. <em>ML</em>,
<em>113</em>(9), 5981–6018. (<a
href="https://doi.org/10.1007/s10994-024-06542-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greedy-GQ with linear function approximation, originally proposed in Maei et al. (in: Proceedings of the international conference on machine learning (ICML), 2010), is a value-based off-policy algorithm for optimal control in reinforcement learning, and it has a non-linear two timescale structure with non-convex objective function. This paper develops its tightest finite-time error bounds. We show that the Greedy-GQ algorithm converges as fast as $$\mathcal {O}({1}/{\sqrt{T}})$$ under the i.i.d. setting and $$\mathcal {O}({\log T}/{\sqrt{T}})$$ under the Markovian setting. We further design variant of the vanilla Greedy-GQ algorithm using the nested-loop approach, and show that its sample complexity is $$\mathcal {O}({\log (1/\epsilon )\epsilon ^{-2}})$$ , which matches with the one of the vanilla Greedy-GQ. Our finite-time error bounds match with the one of the stochastic gradient descent algorithm for general smooth non-convex optimization problems, despite of its additonal challenge in the two time-scale updates. Our finite-sample analysis provides theoretical guidance on choosing step-sizes for faster convergence in practice, and suggests the trade-off between the convergence rate and the quality of the obtained policy. Our techniques provide a general approach for finite-sample analysis of non-convex two timescale value-based reinforcement learning algorithms.},
  archive      = {J_ML},
  author       = {Wang, Yue and Zhou, Yi and Zou, Shaofeng},
  doi          = {10.1007/s10994-024-06542-x},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {5981-6018},
  shortjournal = {Mach. Learn.},
  title        = {Finite-time error bounds for greedy-GQ},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SWoTTeD: An extension of tensor decomposition to temporal
phenotyping. <em>ML</em>, <em>113</em>(9), 5939–5980. (<a
href="https://doi.org/10.1007/s10994-024-06545-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records. However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.},
  archive      = {J_ML},
  author       = {Sebia, Hana and Guyet, Thomas and Audureau, Etienne},
  doi          = {10.1007/s10994-024-06545-8},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {5939-5980},
  shortjournal = {Mach. Learn.},
  title        = {SWoTTeD: An extension of tensor decomposition to temporal phenotyping},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reversible jump attack to textual classifiers with
modification reduction. <em>ML</em>, <em>113</em>(9), 5907–5937. (<a
href="https://doi.org/10.1007/s10994-024-06539-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on adversarial examples expose vulnerabilities of natural language processing models. Existing techniques for generating adversarial examples are typically driven by deterministic hierarchical rules that are agnostic to the optimal adversarial examples, a strategy that often results in adversarial samples with a suboptimal balance between magnitudes of changes and attack successes. To this end, in this research we propose two algorithms, Reversible Jump Attack (RJA) and Metropolis–Hasting Modification Reduction (MMR), to generate highly effective adversarial examples and to improve the imperceptibility of the examples, respectively. RJA utilizes a novel randomization mechanism to enlarge the search space and efficiently adapts to a number of perturbed words for adversarial examples. With these generated adversarial examples, MMR applies the Metropolis–Hasting sampler to enhance the imperceptibility of adversarial examples. Extensive experiments demonstrate that RJA-MMR outperforms current state-of-the-art methods in attack performance, imperceptibility, fluency and grammar correctness.},
  archive      = {J_ML},
  author       = {Ni, Mingze and Sun, Zhensu and Liu, Wei},
  doi          = {10.1007/s10994-024-06539-6},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {5907-5937},
  shortjournal = {Mach. Learn.},
  title        = {Reversible jump attack to textual classifiers with modification reduction},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coresets for kernel clustering. <em>ML</em>,
<em>113</em>(8), 5891–5906. (<a
href="https://doi.org/10.1007/s10994-024-06540-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise coresets for kernel $$k$$ -Means with a general kernel, and use them to obtain new, more efficient, algorithms. Kernel $$k$$ -Means has superior clustering capability compared to classical $$k$$ -Means, particularly when clusters are non-linearly separable, but it also introduces significant computational challenges. We address this computational issue by constructing a coreset, which is a reduced dataset that accurately preserves the clustering costs. Our main result is a coreset for kernel $$k$$ -Means that works for a general kernel and has size $${{\,\textrm{poly}\,}}(k\epsilon ^{-1})$$ . Our new coreset both generalizes and greatly improves all previous results; moreover, it can be constructed in time near-linear in n. This result immediately implies new algorithms for kernel $$k$$ -Means, such as a $$(1+\epsilon )$$ -approximation in time near-linear in n, and a streaming algorithm using space and update time $${{\,\textrm{poly}\,}}(k \epsilon ^{-1} \log n)$$ . We validate our coreset on various datasets with different kernels. Our coreset performs consistently well, achieving small errors while using very few points. We show that our coresets can speed up kernel $$\textsc {k-Means++}$$ (the kernelized version of the widely used $$\textsc {k-Means++}$$ algorithm), and we further use this faster kernel $$\textsc {k-Means++}$$ for spectral clustering. In both applications, we achieve significant speedup and a better asymptotic growth while the error is comparable to baselines that do not use coresets.},
  archive      = {J_ML},
  author       = {Jiang, Shaofeng H. -C. and Krauthgamer, Robert and Lou, Jianing and Zhang, Yubo},
  doi          = {10.1007/s10994-024-06540-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5891-5906},
  shortjournal = {Mach. Learn.},
  title        = {Coresets for kernel clustering},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on interpretable reinforcement learning.
<em>ML</em>, <em>113</em>(8), 5847–5890. (<a
href="https://doi.org/10.1007/s10994-024-06543-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as an intrinsic property of a model) and explainability (as a post-hoc operation) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions, notably related to the recent development of foundation models (e.g., large language models, RL from human feedback).},
  archive      = {J_ML},
  author       = {Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  doi          = {10.1007/s10994-024-06543-w},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5847-5890},
  shortjournal = {Mach. Learn.},
  title        = {A survey on interpretable reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PolieDRO: A novel classification and regression framework
with non-parametric data-driven regularization. <em>ML</em>,
<em>113</em>(8), 5807–5846. (<a
href="https://doi.org/10.1007/s10994-024-06544-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PolieDRO is a novel analytics framework for classification and regression that harnesses the power and flexibility of data-driven distributionally robust optimization (DRO) to circumvent the need for regularization hyperparameters. Recent literature shows that traditional machine learning methods such as SVM and (square-root) LASSO can be written as Wasserstein-based DRO problems. Inspired by those results we propose a hyperparameter-free ambiguity set that explores the polyhedral structure of data-driven convex hulls, generating computationally tractable regression and classification methods for any convex loss function. Numerical results based on 100 real-world databases and an extensive experiment with synthetically generated data show that our methods consistently outperform their traditional counterparts.},
  archive      = {J_ML},
  author       = {Gutierrez, Tomás and Valladão, Davi and Pagnoncelli, Bernardo K.},
  doi          = {10.1007/s10994-024-06544-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5807-5846},
  shortjournal = {Mach. Learn.},
  title        = {PolieDRO: A novel classification and regression framework with non-parametric data-driven regularization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective keyword search co-occurrence multi-layer graph
mining approach. <em>ML</em>, <em>113</em>(8), 5773–5806. (<a
href="https://doi.org/10.1007/s10994-024-06528-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A combination of tools and methods known as &quot;graph mining&quot; is used to evaluate real-world graphs, forecast the potential effects of a given graph’s structure and properties for various applications, and build models that can yield actual graphs that closely resemble the structure seen in real-world graphs of interest. However, some graph mining approaches possess scalability and dynamic graph challenges, limiting practical applications. In machine learning and data mining, among the unique methods is graph embedding, known as network representation learning where representative methods suggest encoding the complicated graph structures into embedding by utilizing specific pre-defined metrics. Co-occurrence graphs and keyword searches are the foundation of search engine optimizations for diverse real-world applications. Current work on keyword searches on graphs is based on pre-established information retrieval search criteria and does not provide semantic linkages. Recent works on co-occurrence and keyword search methods function effectively on graphs with only one layer instead of many layers. However, the graph neural network has been utilized in recent years as a branch of graph model due to its excellent performance. This paper proposes an Effective Keyword Search Co-occurrence Multi-Layer Graph mining method by employing two core approaches: Multi-layer Graph Embedding and Graph Neural Networks. We conducted extensive tests using benchmarks on real-world data sets. Considering the experimental findings, the proposed method enhanced with the regularization approach is substantially excellent, with a 10% increment in precision, recall, and f1-score.},
  archive      = {J_ML},
  author       = {Bolorunduro, Janet Oluwasola and Zou, Zhaonian and Bah, Mohamed Jaward},
  doi          = {10.1007/s10994-024-06528-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5773-5806},
  shortjournal = {Mach. Learn.},
  title        = {An effective keyword search co-occurrence multi-layer graph mining approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ijuice: Integer JUstIfied counterfactual explanations.
<em>ML</em>, <em>113</em>(8), 5731–5771. (<a
href="https://doi.org/10.1007/s10994-024-06530-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations modify the feature values of an instance in order to alter its prediction from an undesired to a desired label. As such, they are highly useful for providing trustworthy interpretations of decision-making in domains where complex and opaque machine learning algorithms are utilized. To guarantee their quality and promote user trust, they need to satisfy the faithfulness desideratum, when supported by the data distribution. We hereby propose a counterfactual generation algorithm for mixed-feature spaces that prioritizes faithfulness through k-justification, a novel counterfactual property introduced in this paper. The proposed algorithm employs a graph representation of the search space and provides counterfactuals by solving an integer program. In addition, the algorithm is classifier-agnostic and is not dependent on the order in which the feature space is explored. In our empirical evaluation, we demonstrate that it guarantees k-justification while showing comparable performance to state-of-the-art methods in feasibility, sparsity, and proximity.},
  archive      = {J_ML},
  author       = {Kuratomi, Alejandro and Miliou, Ioanna and Lee, Zed and Lindgren, Tony and Papapetrou, Panagiotis},
  doi          = {10.1007/s10994-024-06530-1},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5731-5771},
  shortjournal = {Mach. Learn.},
  title        = {Ijuice: Integer JUstIfied counterfactual explanations},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure discovery in PAC-learning by random projections.
<em>ML</em>, <em>113</em>(8), 5685–5730. (<a
href="https://doi.org/10.1007/s10994-024-06531-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional learning is data-hungry in general; however, many natural data sources and real-world learning problems posses some hidden low-complexity structure that permit effective learning from relatively small sample sizes. We are interested in the general question of how to discover and exploit such hidden benign traits when problem-specific prior knowledge is insufficient. In this work, we address this question through random projection’s ability to expose structure. We study both compressive learning and high dimensional learning from this angle by introducing the notions of compressive distortion and compressive complexity. We give user-friendly PAC bounds in the agnostic setting that are formulated in terms of these quantities, and we show that our bounds can be tight when these quantities are small. We then instantiate these quantities in several examples of particular learning problems, demonstrating their ability to discover interpretable structural characteristics that make high dimensional instances of these problems solvable to good approximation in a random linear subspace. In the examples considered, these turn out to resemble some familiar benign traits such as the margin, the margin distribution, the intrinsic dimension, the spectral decay of the data covariance, or the norms of parameters—while our general notions of compressive distortion and compressive complexity serve to unify these, and may be used to discover benign structural traits for other PAC-learnable problems.},
  archive      = {J_ML},
  author       = {Kabán, Ata and Reeve, Henry},
  doi          = {10.1007/s10994-024-06531-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5685-5730},
  shortjournal = {Mach. Learn.},
  title        = {Structure discovery in PAC-learning by random projections},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stress detection with encoding physiological signals and
convolutional neural network. <em>ML</em>, <em>113</em>(8), 5655–5683.
(<a href="https://doi.org/10.1007/s10994-023-06509-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress is a significant and growing phenomenon in the modern world that leads to numerous health problems. Robust and non-invasive method developments for early and accurate stress detection are crucial in enhancing people’s quality of life. Previous researches show that using machine learning approaches on physiological signals is a reliable stress predictor by achieving significant results. However, it requires determining features by hand. Such a selection is a challenge in this context since stress determines nonspecific human responses. This work overcomes such limitations by considering STREDWES, an approach for Stress Detection from Wearable Sensors Data. STREDWES encodes signal fragments of physiological signals into images and classifies them by a Convolutional Neural Network (CNN). This study aims to study several encoding methods, including the Gramian Angular Summation/Difference Field method and Markov Transition Field, to evaluate the best way to encode signals into images in this domain. Such a study is performed on the NEURO dataset. Moreover, we investigate the usefulness of STREDWES in real scenarios by considering the SWELL dataset and a personalized approach. Finally, we compare the proposed approach with its competitors by considering the WESAD dataset. It outperforms the others.},
  archive      = {J_ML},
  author       = {Quadrini, Michela and Capuccio, Antonino and Falcone, Denise and Daberdaku, Sebastian and Blanda, Alessandro and Bellanova, Luca and Gerard, Gianluca},
  doi          = {10.1007/s10994-023-06509-4},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5655-5683},
  shortjournal = {Mach. Learn.},
  title        = {Stress detection with encoding physiological signals and convolutional neural network},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Recurrent segmentation meets block models in temporal
networks. <em>ML</em>, <em>113</em>(8), 5623–5653. (<a
href="https://doi.org/10.1007/s10994-023-06507-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular approach to model interactions is to represent them as a network with nodes being the agents and the interactions being the edges. Interactions are often timestamped, which leads to having timestamped edges. Many real-world temporal networks have a recurrent or possibly cyclic behaviour. In this paper, our main interest is to model recurrent activity in such temporal networks. As a starting point we use stochastic block model, a popular choice for modelling static networks, where nodes are split into R groups. We extend the block model to temporal networks by modelling the edges with a Poisson process. We make the parameters of the process dependent on time by segmenting the time line into K segments. We require that only $$H \le K$$ different set of parameters can be used. If $$H &lt; K$$ , then several, not necessarily consecutive, segments must share their parameters, modelling repeating behaviour. We propose two variants where a group membership of a node is fixed over the course of entire time line and group memberships are allowed to vary from segment to segment. We prove that searching for optimal groups and segmentation in both variants is NP-hard. Consequently, we split the problem into 3 subproblems where we optimize groups, model parameters, and segmentation in turn while keeping the remaining structures fixed. We propose an iterative algorithm that requires $$\mathcal {O} \left( KHm + Rn + R^2\,H\right)$$ time per iteration, where n and m are the number of nodes and edges in the network. We demonstrate experimentally that the number of required iterations is typically low, the algorithm is able to discover the ground truth from synthetic datasets, and show that certain real-world networks exhibit recurrent behaviour as the likelihood does not deteriorate when H is lowered.},
  archive      = {J_ML},
  author       = {Wickrama Arachchi, Chamalee and Tatti, Nikolaj},
  doi          = {10.1007/s10994-023-06507-6},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5623-5653},
  shortjournal = {Mach. Learn.},
  title        = {Recurrent segmentation meets block models in temporal networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPG: A model to build feature subspace against adversarial
patch attack. <em>ML</em>, <em>113</em>(8), 5601–5622. (<a
href="https://doi.org/10.1007/s10994-023-06417-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial patch attacks in the physical world are a major threat to the application of deep learning. However, current research on adversarial patch defense algorithms focuses on image pre-processing defenses, it has been demonstrated that this defense reduces the classification accuracy of clean images and is unable to defend against physically realizable attacks. In this paper, we propose a defense patch GNN (DPG), using a new perspective for defending against adversarial patch attacks. First, we extract the input image features with the feature extraction to obtain a feature set. Then downsampling the feature set by applying the global average pooling layer to reduce the perturbation of the features by the adversarial patch. Finally, this paper proposes a graph-structured feature subspace to robust the feature performance. In addition, we design an optimization algorithm based on stochastic gradient descent (SGD), which can significantly increase the mode’s generalization ability. We demonstrate empirically the superior robustness of the DPG model on existing adversarial patch attacks. DPG shows without any accuracy loss in the prediction of clean images.},
  archive      = {J_ML},
  author       = {Xue, Yunsheng and Wen, Mi and He, Wei and Li, Weiwei},
  doi          = {10.1007/s10994-023-06417-7},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5601-5622},
  shortjournal = {Mach. Learn.},
  title        = {DPG: A model to build feature subspace against adversarial patch attack},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paf-tracker: A novel pre-frame auxiliary and fusion visual
tracker. <em>ML</em>, <em>113</em>(8), 5577–5600. (<a
href="https://doi.org/10.1007/s10994-023-06466-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese-like trackers expose considerable shortcomings in the case of brief occlusion due mainly to the inadequate consideration of the correlation information between adjacent frames. The precision of predicted bounding boxes still has much room for further improvement because the traditional regression loss cannot effectively handle the case where one box contains the other. To address these shortages, the paper proposes a novel pre-frame auxiliary and fusion tracking framework. Within this framework, a retained variable is first introduced to avoid some additional twin branches while retaining the previously obtained deep features of the search frames. Based on such a variable, a pre-frame auxiliary module is constructed to establish the relationship between encoding features and the retained pre-frame information. Furthermore, a decoding fusion module is designed to fuse the generated similarity relationship between the template patch and the search patch and the one between the search frame and previous frames. Moreover, the Efficient IoU (EIoU) loss is employed to increase the precision of predicted bounding boxes by adding three penalty terms for the differences in the center point, length, and width of the two bounding boxes. Finally, the superiority over state-of-the-art methods is verified by numerous tests on visual tracking benchmarks.},
  archive      = {J_ML},
  author       = {Liang, Wei and Ding, Derui and Yu, Hui},
  doi          = {10.1007/s10994-023-06466-y},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5577-5600},
  shortjournal = {Mach. Learn.},
  title        = {Paf-tracker: A novel pre-frame auxiliary and fusion visual tracker},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical analysis of performance assessment for imbalanced
classification. <em>ML</em>, <em>113</em>(8), 5533–5575. (<a
href="https://doi.org/10.1007/s10994-023-06497-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are multiple scenarios in machine learning where the data used presents a heavy bias towards one of the classes. Evaluating the performance of machine learning models in such imbalanced scenarios proves to be difficult and challenging, as one of the classes is poorly represented in the data, and this class is often more relevant to the end-user. An abundance of performance metrics have been devised and commonly used in order to solve these specific problems, however, there is often a lack of common agreement on which metric is best and which to use in specific imbalanced scenarios. In this study, we experimentally study the impact of choosing one metric over another in the evaluation of a classifier for binary classification, as well as the effect of data characteristics such as class imbalance and noise on those metrics. Based on our extensive empirical analysis, we provide a set of easy-to-follow guidelines for which performance metric is best to use depending on the context of the problem. Specifically, we highlight the importance of using multiple different metrics which are fundamentally different in imbalanced domains, we also display results on why the usage of Davis’ interpolation of the area under the precision-recall curve and the Matthews Correlation Coefficient metrics should be preferred over other similar metrics, as well as why the usage of geometric mean and $$F_1$$ score should be avoided in scenarios likely to present noise on the labels.},
  archive      = {J_ML},
  author       = {Gaudreault, Jean-Gabriel and Branco, Paula},
  doi          = {10.1007/s10994-023-06497-5},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5533-5575},
  shortjournal = {Mach. Learn.},
  title        = {Empirical analysis of performance assessment for imbalanced classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey on extreme learning machines for outlier detection.
<em>ML</em>, <em>113</em>(8), 5495–5531. (<a
href="https://doi.org/10.1007/s10994-023-06375-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a two-class classification task, if the number of examples of one class (majority) is much greater than that of another class (minority), then the classification is said to be class imbalanced. It can occur among many real-world applications, such as intrusion detection, medical diagnosis, etc. The class imbalance issue can make learning difficult since learning opts to bias towards the majority class. Outliers are cases with anomalous behaviors and are extreme cases of class imbalance. Despite late advances in extreme learning machines (ELMs), there are not many experimental investigations in the field of ELM with outlier detection. In this survey, we provide a comprehensive overview of existing ELMs to address the problem of outlier detection under a unified perspective. Firstly, we describe the background of our work, which includes a brief overview of previous surveys and a detailed description of the enhanced ELMs. Next, available studies regarding why ELMs are used to tackle the class imbalance problem are reviewed. Furthermore, cutting-edge algorithms are surveyed for improved ELMs to detect outliers. We classify these methods under three different machine learning perspectives (i.e., supervised, unsupervised, and semi-supervised approaches). In addition, we explore the developments of existing solutions based on three standardized quality metrics (i.e., accuracy, robustness, and speed) and other performance metrics (e.g., mean absolute percentage error and mean absolute error). After that, related datasets are detailed to facilitate future studies in this field. Last but the most important, this study concludes with discussions, challenges, and suggestions to guide future research.},
  archive      = {J_ML},
  author       = {Kiani, Rasoul and Jin, Wei and Sheng, Victor S.},
  doi          = {10.1007/s10994-023-06375-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5495-5531},
  shortjournal = {Mach. Learn.},
  title        = {Survey on extreme learning machines for outlier detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mental stress detection from ultra-short heart rate
variability using explainable graph convolutional network with network
pruning and quantisation. <em>ML</em>, <em>113</em>(8), 5467–5494. (<a
href="https://doi.org/10.1007/s10994-023-06504-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel pruning approach based on explainable graph convolutional networks, strategically amalgamating pruning and quantisation, aimed to tackle the complexities associated with existing machine learning and deep learning models for stress detection using ultra-short heart rate variability analysis. These complexities often impede the implementation ability of such models on resource-limited devices. The proposed method exhibits exceptional performance, demonstrating high accuracy (97.75%) and efficiency (97.66%) on the WESAD dataset, along with an impressive accuracy (94.48%) and efficiency (94.39%) on the SWELL dataset. Importantly, the runtime complexity saw a significant reduction, down by 63.4% and 69.34% compared to the original model. The proposed method&#39;s notable advantage lies in its ability to retain nearly all of the initial model&#39;s performance with negligible loss, even when the pruning levels are below 60%. This innovative approach, thus, offers a promising solution for effective stress detection, specifically designed to operate smoothly on devices with limited resources.},
  archive      = {J_ML},
  author       = {Adarsh, V. and Gangadharan, G. R.},
  doi          = {10.1007/s10994-023-06504-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5467-5494},
  shortjournal = {Mach. Learn.},
  title        = {Mental stress detection from ultra-short heart rate variability using explainable graph convolutional network with network pruning and quantisation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning sample-aware threshold for semi-supervised
learning. <em>ML</em>, <em>113</em>(8), 5423–5445. (<a
href="https://doi.org/10.1007/s10994-023-06425-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-labeling methods are popular in semi-supervised learning (SSL). Their performance heavily relies on a proper threshold to generate hard labels for unlabeled data. To this end, most existing studies resort to a manually pre-specified function to adjust the threshold, which, however, requires prior knowledge and suffers from the scalability issue. In this paper, we propose a novel method named Meta-Threshold, which learns a dynamic confidence threshold for each unlabeled instance and does not require extra hyperparameters except a learning rate. Specifically, the instance-level confidence threshold is automatically learned by an extra network in a meta-learning manner. Considering limited labeled data as meta-data, the overall training objective of the classifier network and the meta-net can be formulated as a nested optimization problem that can be solved by a bi-level optimization scheme. Furthermore, by replacing the indicator function existed in the pseudo-labeling with a surrogate function, we theoretically provide the convergence of our training procedure, while discussing the training complexity and proposing a strategy to reduce its time cost. Extensive experiments and analyses demonstrate the effectiveness of our method on both typical and imbalanced SSL tasks.},
  archive      = {J_ML},
  author       = {Wei, Qi and Feng, Lei and Sun, Haoliang and Wang, Ren and He, Rundong and Yin, Yilong},
  doi          = {10.1007/s10994-023-06425-7},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5423-5445},
  shortjournal = {Mach. Learn.},
  title        = {Learning sample-aware threshold for semi-supervised learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PANACEA: A neural model ensemble for cyber-threat detection.
<em>ML</em>, <em>113</em>(8), 5379–5422. (<a
href="https://doi.org/10.1007/s10994-023-06470-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning is a strategy commonly used to fuse different base models by creating a model ensemble that is expected more accurate on unseen data than the base models. This study describes a new cyber-threat detection method, called PANACEA, that uses ensemble learning coupled with adversarial training in deep learning, in order to gain accuracy with neural models trained in cybersecurity problems. The selection of the base models is one of the main challenges to handle, in order to train accurate ensembles. This study describes a model ensemble pruning approach based on eXplainable AI (XAI) to increase the ensemble diversity and gain accuracy in ensemble classification. We base on the idea that being able to identify base models that give relevance to different input feature sub-spaces may help in improving the accuracy of an ensemble trained to recognise different signatures of different cyber-attack patterns. To this purpose, we use a global XAI technique to measure the ensemble model diversity with respect to the effect of the input features on the accuracy of the base neural models combined in the ensemble. Experiments carried out on four benchmark cybersecurity datasets (three network intrusion detection datasets and one malware detection dataset) show the beneficial effects of the proposed combination of adversarial training, ensemble learning and XAI on the accuracy of multi-class classifications of cyber-data achieved by the neural model ensemble.},
  archive      = {J_ML},
  author       = {AL-Essa, Malik and Andresini, Giuseppina and Appice, Annalisa and Malerba, Donato},
  doi          = {10.1007/s10994-023-06470-2},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5379-5422},
  shortjournal = {Mach. Learn.},
  title        = {PANACEA: A neural model ensemble for cyber-threat detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entity recognition based on heterogeneous graph reasoning of
visual region and text candidate. <em>ML</em>, <em>113</em>(8),
5351–5378. (<a
href="https://doi.org/10.1007/s10994-023-06456-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity recognition plays a crucial role in various domains, such as natural language processing, information retrieval, and question-answering systems. While significant progress has been made in recognizing entities from plain text, the exploration of entity recognition from multimodal data remains limited due to disparities in semantic representation. In light of this challenge, given the supportive nature of visual and text data, we propose a novel entity recognition model called Heterogeneous Graph Reasoning(HGR), leveraging the synergistic nature of visual and textual data. HGR utilizes image objects to facilitate text entity extraction by mining the potential pair projection between text entity and image object. This is achieved through the utilization of the Vision Refine and Graph Cross Inference modules. In the Vision Refine module, semantically relevant objects hidden in the image are selected to aid in the text entity extraction. In the Graph Cross Inference module, cross-association inference between visual regions and textual entities is constructed through graph construction, heterogeneous graph fusion, visual region refinement and cross inference. To validate the effectiveness of our model, extensive experiments on four multimodal datasets are conducted. Among these datasets, two originate from Chinese unmanned surface vehicles and journalism(USV and NEWS), while the remaining two are public English multimodal datasets(Twitter-2015 and Twitter-2017). The experimental results demonstrate the superiority of our model, with F1-sore improvements of 1.55%, 0.12%, 0.22%, and 0.99% on the four datasets, respectively, when compared to the second-best state-of-the-art model.},
  archive      = {J_ML},
  author       = {Wang, Xinzhi and Zhu, Nengjun and Li, Jiahao and Chang, Yudong and Li, Zhennan},
  doi          = {10.1007/s10994-023-06456-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5351-5378},
  shortjournal = {Mach. Learn.},
  title        = {Entity recognition based on heterogeneous graph reasoning of visual region and text candidate},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GS2P: A generative pre-trained learning to rank model with
over-parameterization for web-scale search. <em>ML</em>,
<em>113</em>(8), 5331–5349. (<a
href="https://doi.org/10.1007/s10994-023-06469-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While learning to rank (LTR) is widely employed in web searches to prioritize pertinent webpages from the retrieved contents based on input queries, traditional LTR models stumble over two principal stumbling blocks leading to subpar performance: (1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, debilitating their coverage of search queries across the popularity spectrum, and (2) ill-trained models that are incapable of inducing generalized representations for LTR, culminating in overfitting. To tackle above challenges, we proposed a $$\underline{G}enerative\ \underline{S}emi$$ - $$\underline{S}upervised\ \underline{P}re$$ -trained (GS $$^2$$ P) Learning to Rank model. Specifically, GS $$^2$$ P first generates pseudo-labels for the unlabeled samples using tree-based LTR models after a series of co-training procedures, then learns the representations of query-webpage pairs with self-attentive transformers via both discriminative (LTR) and generative (denoising autoencoding for reconstruction) losses. Finally, GS $$^2$$ P boosts the performance of LTR through incorporating Random Fourier Features to over-parameterize the models into “interpolating regime”, so as to enjoy the further descent of generalization errors with learned representations. We conduct extensive offline experiments on a publicly available dataset and a real-world dataset collected from a large-scale search engine. The results show that GS $$^2$$ P can achieve the best performance on both datasets, compared to baselines. We also deploy GS $$^2$$ P at a large-scale web search engine with realistic traffic, where we can still observe significant improvement in real-world applications. GS $$^2$$ P performs consistently in both online and offline experiments.},
  archive      = {J_ML},
  author       = {Li, Yuchen and Xiong, Haoyi and Kong, Linghe and Bian, Jiang and Wang, Shuaiqiang and Chen, Guihai and Yin, Dawei},
  doi          = {10.1007/s10994-023-06469-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5331-5349},
  shortjournal = {Mach. Learn.},
  title        = {GS2P: A generative pre-trained learning to rank model with over-parameterization for web-scale search},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving imbalanced learning with outlier detection and
features reduction. <em>ML</em>, <em>113</em>(8), 5273–5330. (<a
href="https://doi.org/10.1007/s10994-023-06448-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical problem for several real world applications is class imbalance. Indeed, in contexts like fraud detection or medical diagnostics, standard machine learning models fail because they are designed to handle balanced class distributions. Existing solutions typically increase the rare class instances by generating synthetic records to achieve a balanced class distribution. However, these procedures generate not plausible data and tend to create unnecessary noise. We propose a change of perspective where instead of relying on resampling techniques, we depend on unsupervised features engineering approaches to represent records with a combination of features that will help the classifier capturing the differences among classes, even in presence of imbalanced data. Thus, we combine a large array of outlier detection, features projection, and features selection approaches to augment the expressiveness of the dataset population. We show the effectiveness of our proposal in a deep and wide set of benchmarking experiments as well as in real case studies.},
  archive      = {J_ML},
  author       = {Lusito, Salvatore and Pugnana, Andrea and Guidotti, Riccardo},
  doi          = {10.1007/s10994-023-06448-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5273-5330},
  shortjournal = {Mach. Learn.},
  title        = {Solving imbalanced learning with outlier detection and features reduction},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Libsignal: An open library for traffic signal control.
<em>ML</em>, <em>113</em>(8), 5235–5271. (<a
href="https://doi.org/10.1007/s10994-023-06412-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a library for cross-simulator comparison of reinforcement learning models in traffic signal control tasks. This library is developed to implement recent state-of-the-art reinforcement learning models with extensible interfaces and unified cross-simulator evaluation metrics. It supports commonly-used simulators in traffic signal control tasks, including Simulation of Urban MObility(SUMO) and CityFlow, and multiple benchmark datasets for fair comparisons. We conducted experiments to validate our implementation of the models and to calibrate the simulators so that the experiments from one simulator could be referential to the other. Based on the validated models and calibrated environments, this paper compares and reports the performance of current state-of-the-art RL algorithms across different datasets and simulators. This is the first time that these methods have been compared fairly under the same datasets with different simulators.},
  archive      = {J_ML},
  author       = {Mei, Hao and Lei, Xiaoliang and Da, Longchao and Shi, Bin and Wei, Hua},
  doi          = {10.1007/s10994-023-06412-y},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5235-5271},
  shortjournal = {Mach. Learn.},
  title        = {Libsignal: An open library for traffic signal control},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-agent reinforcement learning for fast-timescale
demand response of residential loads. <em>ML</em>, <em>113</em>(8),
5203–5234. (<a
href="https://doi.org/10.1007/s10994-023-06460-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To integrate high amounts of renewable energy resources, electrical power grids must be able to cope with high amplitude, fast timescale variations in power generation. Frequency regulation through demand response has the potential to coordinate temporally flexible loads, such as air conditioners, to counteract these variations. Existing approaches for discrete control with dynamic constraints struggle to provide satisfactory performance for fast timescale action selection with hundreds of agents. We propose a decentralized agent trained with multi-agent proximal policy optimization with localized communication. We explore two communication frameworks: hand-engineered, or learned through targeted multi-agent communication. The resulting policies perform well and robustly for frequency regulation, and scale seamlessly to arbitrary numbers of houses for constant processing times.},
  archive      = {J_ML},
  author       = {Mai, Vincent and Maisonneuve, Philippe and Zhang, Tianyu and Nekoei, Hadi and Paull, Liam and Lesage-Landry, Antoine},
  doi          = {10.1007/s10994-023-06460-4},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5203-5234},
  shortjournal = {Mach. Learn.},
  title        = {Multi-agent reinforcement learning for fast-timescale demand response of residential loads},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMMSampling: A new model-based, data difficulty-driven
resampling method for multi-class imbalanced data. <em>ML</em>,
<em>113</em>(8), 5183–5202. (<a
href="https://doi.org/10.1007/s10994-023-06416-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from multi-class imbalanced data has still received limited research attention. Most of the proposed methods focus on the global class imbalance ratio only. In contrast, experimental studies demonstrated that the imbalance ratio itself is not the main difficulty in the imbalanced learning. It is the combination of the imbalance ratio with other data difficulty factors, such as class overlapping or minority class decomposition into various subconcepts, that significantly affects the classification performance. This paper presents GMMSampling—a new resampling method that exploits information about data difficulty factors to clear class overlapping regions from majority class instances and to simultaneously oversample each subconcept of the minority class. The experimental evaluation demonstrated that the proposed method achieves better results in terms of G-mean, balanced accuracy, macro-AP, MCC and F-score than other related methods.},
  archive      = {J_ML},
  author       = {Naglik, Iwo and Lango, Mateusz},
  doi          = {10.1007/s10994-023-06416-8},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5183-5202},
  shortjournal = {Mach. Learn.},
  title        = {GMMSampling: A new model-based, data difficulty-driven resampling method for multi-class imbalanced data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GVFs in the real world: Making predictions online for water
treatment. <em>ML</em>, <em>113</em>(8), 5151–5181. (<a
href="https://doi.org/10.1007/s10994-023-06413-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate the use of reinforcement-learning based prediction approaches for a real drinking-water treatment plant. Developing such a prediction system is a critical step on the path to optimizing and automating water treatment. Before that, there are many questions to answer about the predictability of the data, suitable neural network architectures, how to overcome partial observability and more. We first describe this dataset, and highlight challenges with seasonality, nonstationarity, partial observability, and heterogeneity across sensors and operation modes of the plant. We then describe General Value Function (GVF) predictions—discounted cumulative sums of observations–and highlight why they might be preferable to classical n-step predictions common in time series prediction. We discuss how to use offline data to appropriately pre-train our temporal difference learning (TD) agents that learn these GVF predictions, including how to select hyperparameters for online fine-tuning in deployment. We find that the TD-prediction agent obtains an overall lower normalized mean-squared error than the n-step prediction agent. Finally, we show the importance of learning in deployment, by comparing a TD agent trained purely offline with no online updating to a TD agent that learns online. This final result is one of the first to motivate the importance of adapting predictions in real-time, for non-stationary high-volume systems in the real world.},
  archive      = {J_ML},
  author       = {Janjua, Muhammad Kamran and Shah, Haseeb and White, Martha and Miahi, Erfan and Machado, Marlos C. and White, Adam},
  doi          = {10.1007/s10994-023-06413-x},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5151-5181},
  shortjournal = {Mach. Learn.},
  title        = {GVFs in the real world: Making predictions online for water treatment},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DOC <span class="math display"><sup>3</sup></span>: Deep one
class classification using contradictions. <em>ML</em>, <em>113</em>(8),
5109–5150. (<a
href="https://doi.org/10.1007/s10994-023-06362-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the notion of learning from contradictions (a.k.a Universum learning) for deep one class classification problems. We formalize this notion for the widely adopted one class large-margin loss (Schölkopf et al. in Neural Comput 13(7):1443–1471), and propose the deep one class classification using contradictions (DOC $$^3$$ ) algorithm. We show that learning from contradictions incurs lower generalization error by comparing the empirical Rademacher complexity of DOC $$^3$$ against its traditional inductive learning counterpart. Further, our proposed ‘learning from contradiction’ is a generic learning setting and can compliment other advanced learning settings. To illustrate this, we extend the adversarial learning based DROCC-LF (Goyal et al. in International conference on machine learning, PMLR, 2020) algorithm under this new setting. Our empirical results demonstrate the efficacy of DOC $$^3$$ and it’s extensions compared to popular baseline algorithms on several benchmark and real-life data sets.},
  archive      = {J_ML},
  author       = {Dhar, Sauptik and Gonzalez-Torres, Bernardo},
  doi          = {10.1007/s10994-023-06362-5},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5109-5150},
  shortjournal = {Mach. Learn.},
  title        = {DOC $$^3$$: Deep one class classification using contradictions},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fraud detection with natural language processing.
<em>ML</em>, <em>113</em>(8), 5087–5108. (<a
href="https://doi.org/10.1007/s10994-023-06354-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated fraud detection can assist organisations to safeguard user accounts, a task that is very challenging due to the great sparsity of known fraud transactions. Many approaches in the literature focus on credit card fraud and ignore the growing field of online banking. However, there is a lack of publicly available data for both. The lack of publicly available data hinders the progress of the field and limits the investigation of potential solutions. With this work, we: (a) introduce FraudNLP, the first anonymised, publicly available dataset for online fraud detection, (b) benchmark machine and deep learning methods with multiple evaluation measures, (c) argue that online actions do follow rules similar to natural language and hence can be approached successfully by natural language processing methods.},
  archive      = {J_ML},
  author       = {Boulieris, Petros and Pavlopoulos, John and Xenos, Alexandros and Vassalos, Vasilis},
  doi          = {10.1007/s10994-023-06354-5},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5087-5108},
  shortjournal = {Mach. Learn.},
  title        = {Fraud detection with natural language processing},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of class-imbalanced semi-supervised learning.
<em>ML</em>, <em>113</em>(8), 5057–5086. (<a
href="https://doi.org/10.1007/s10994-023-06344-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning(SSL) can substantially improve the performance of deep neural networks by utilizing unlabeled data when labeled data is scarce. The state-of-the-art(SOTA) semi-supervised algorithms implicitly assume that the class distribution of labeled datasets and unlabeled datasets are balanced, which means the different classes have the same numbers of training samples. However, they can hardly perform well on minority classes when the class distribution of training data is imbalanced. Recent work has found several ways to decrease the degeneration of semi-supervised learning models in class-imbalanced learning. In this article, we comprehensively review class-imbalanced semi-supervised learning (CISSL), starting with an introduction to this field, followed by a realistic evaluation of existing class-imbalanced semi-supervised learning algorithms and a brief summary of them.},
  archive      = {J_ML},
  author       = {Gui, Qian and Zhou, Hong and Guo, Na and Niu, Baoning},
  doi          = {10.1007/s10994-023-06344-7},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5057-5086},
  shortjournal = {Mach. Learn.},
  title        = {A survey of class-imbalanced semi-supervised learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSFuse: Automated feature construction for multiple time
series data. <em>ML</em>, <em>113</em>(8), 5001–5056. (<a
href="https://doi.org/10.1007/s10994-021-06096-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central paradigm for building predictive models from time series data is to convert the data into a feature vector representation and then apply standard inductive learners. Typically, the conversion is done by manually defining features, which is an extremely time-consuming and error-prone process. This has motivated the development of algorithms that automatically construct features from time series. However, these systems are typically designed for univariate time series data. In contrast, many real-world applications require analyzing time series consisting of data collected by multiple sensors. In this context, it is often useful to derive new series by fusing the collected data both within a sensor and across multiple different sensors. Unfortunately, this poses additional challenges for automated construction as exponentially more operations are possible than in the univariate case. This paper proposes an automated feature construction system called TSFuse, which supports fusion and explores the search space in a computationally efficient way. We perform an empirical evaluation on real-world time series classification datasets and show that our system is able to find a better feature representation compared to existing feature construction systems for univariate time series data.},
  archive      = {J_ML},
  author       = {De Brabandere, Arne and Op De Beéck, Tim and Hendrickx, Kilian and Meert, Wannes and Davis, Jesse},
  doi          = {10.1007/s10994-021-06096-2},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {5001-5056},
  shortjournal = {Mach. Learn.},
  title        = {TSFuse: Automated feature construction for multiple time series data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved evolutionary wrapper-filter feature selection
approach with a new initialisation scheme. <em>ML</em>, <em>113</em>(8),
4977–5000. (<a
href="https://doi.org/10.1007/s10994-021-05990-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treated as one of the popular measures in information theory, fuzzy mutual information quantifies the amount of information that one random variable has about another one. Different from standard mutual information, fuzzy mutual information can deal with not only discrete-valued but also real-valued variables. Therefore, fuzzy mutual information has been recently used in evolutionary filter feature selection approaches to measure the correlation between the classes and the features, and the dependencies within a feature set. Typically, this way can be considered as computationally efficient but sometimes it may not contribute to the performance of a classification algorithm. To address this issue, an improved evolutionary wrapper-filter approach which integrates an initialisation scheme and a local search module based on fuzzy mutual information in differential evolution is proposed. According to a number of experiments conducted on several real-world benchmark datasets, the proposed approach does not only significantly improve the computational efficiency of an evolutionary computation technique but also the performance of a classification algorithm.},
  archive      = {J_ML},
  author       = {Hancer, Emrah},
  doi          = {10.1007/s10994-021-05990-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {4977-5000},
  shortjournal = {Mach. Learn.},
  title        = {An improved evolutionary wrapper-filter feature selection approach with a new initialisation scheme},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transforming variables to central normality. <em>ML</em>,
<em>113</em>(8), 4953–4975. (<a
href="https://doi.org/10.1007/s10994-021-05960-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real data sets contain numerical features (variables) whose distribution is far from normal (Gaussian). Instead, their distribution is often skewed. In order to handle such data it is customary to preprocess the variables to make them more normal. The Box–Cox and Yeo–Johnson transformations are well-known tools for this. However, the standard maximum likelihood estimator of their transformation parameter is highly sensitive to outliers, and will often try to move outliers inward at the expense of the normality of the central part of the data. We propose a modification of these transformations as well as an estimator of the transformation parameter that is robust to outliers, so the transformed data can be approximately normal in the center and a few outliers may deviate from it. It compares favorably to existing techniques in an extensive simulation study and on real data.},
  archive      = {J_ML},
  author       = {Raymaekers, Jakob and Rousseeuw, Peter J.},
  doi          = {10.1007/s10994-021-05960-5},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {4953-4975},
  shortjournal = {Mach. Learn.},
  title        = {Transforming variables to central normality},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event causality extraction through external event knowledge
learning and polyhedral word embedding. <em>ML</em>, <em>113</em>(8),
1–20. (<a href="https://doi.org/10.1007/s10994-023-06477-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting causal relations between events from text is vital in natural language processing. Existing methods, which explore the text shallowly, usually aim at casual connection words but neglect implicit causal cues. Furthermore, most of them represent words based solely on contextual semantics, without explicitly considering information related to causality. All of these factors contribute to the inaccuracy of causal relation extraction. To address these issues, in this paper, we propose an event causality extraction method based on external event Knowledge Learning and Polyhedral Word Embedding to alleviate these issues. Specifically, the related background knowledge in knowledge bases is embedded into a vector initially. This infusion of information beyond the sentence allows for the discovery of latent causal relationships. Additionally, we enhance the causal semantic features of words by utilizing their part-of-speech and character features, which helps distinguish causal-related words in sentences. The experimental results on an extended SemEval dataset indicate that our method achieves the best results compared to other existing methods.},
  archive      = {J_ML},
  author       = {Wei, Xiao and Huang, Chenyang and Zhu, Nengjun},
  doi          = {10.1007/s10994-023-06477-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Event causality extraction through external event knowledge learning and polyhedral word embedding},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: Learning sample-aware threshold for
semi-supervised learning. <em>ML</em>, <em>113</em>(7), 4951. (<a
href="https://doi.org/10.1007/s10994-024-06552-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {WeiLei, Qi and Feng, Lei and Sun, Haoliang and Wang, Ren and He, Rundong and Yin, Yilong},
  doi          = {10.1007/s10994-024-06552-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4951},
  shortjournal = {Mach. Learn.},
  title        = {Correction: Learning sample-aware threshold for semi-supervised learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consolidated learning: A domain-specific model-free
optimization strategy with validation on metaMIMIC benchmarks.
<em>ML</em>, <em>113</em>(7), 4925–4949. (<a
href="https://doi.org/10.1007/s10994-023-06359-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many machine learning models, a choice of hyperparameters is a crucial step towards achieving high performance. Prevalent meta-learning approaches focus on obtaining good hyperparameter configurations with a limited computational budget for a completely new task based on the results obtained from the prior tasks. This paper proposes a new formulation of the tuning problem, called consolidated learning, more suited to practical challenges faced by model developers, in which a large number of predictive models are created on similar datasets. In such settings, we are interested in the total optimization time rather than tuning for a single task. We show that a carefully selected static portfolio of hyperparameter configurations yields good results for anytime optimization, while maintaining the ease of use and implementation. Moreover, we point out how to construct such a portfolio for specific domains. The improvement in the optimization is possible due to the more efficient transfer of hyperparameter configurations between similar tasks. We demonstrate the effectiveness of this approach through an empirical study for the XGBoost algorithm and the newly created metaMIMIC benchmarks of predictive tasks extracted from the MIMIC-IV medical database. In the paper, we show that the potential of consolidated learning is considerably greater due to its compatibility with many machine learning application scenarios.},
  archive      = {J_ML},
  author       = {Woźnica, Katarzyna and Grzyb, Mateusz and Trafas, Zuzanna and Biecek, Przemysław},
  doi          = {10.1007/s10994-023-06359-0},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4925-4949},
  shortjournal = {Mach. Learn.},
  title        = {Consolidated learning: A domain-specific model-free optimization strategy with validation on metaMIMIC benchmarks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A theoretical distribution analysis of synthetic minority
oversampling technique (SMOTE) for imbalanced learning. <em>ML</em>,
<em>113</em>(7), 4903–4923. (<a
href="https://doi.org/10.1007/s10994-022-06296-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance occurs when the class distribution is not equal. Namely, one class is under-represented (minority class), and the other class has significantly more samples in the data (majority class). The class imbalance problem is prevalent in many real world applications. Generally, the under-represented minority class is the class of interest. The synthetic minority over-sampling technique (SMOTE) method is considered the most prominent method for handling unbalanced data. The SMOTE method generates new synthetic data patterns by performing linear interpolation between minority class samples and their K nearest neighbors. However, the SMOTE generated patterns do not necessarily conform to the original minority class distribution. This paper develops a novel theoretical analysis of the SMOTE method by deriving the probability distribution of the SMOTE generated samples. To the best of our knowledge, this is the first work deriving a mathematical formulation for the SMOTE patterns’ probability distribution. This allows us to compare the density of the generated samples with the true underlying class-conditional density, in order to assess how representative the generated samples are. The derived formula is verified by computing it on a number of densities versus densities computed and estimated empirically.},
  archive      = {J_ML},
  author       = {Elreedy, Dina and Atiya, Amir F. and Kamalov, Firuz},
  doi          = {10.1007/s10994-022-06296-4},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4903-4923},
  shortjournal = {Mach. Learn.},
  title        = {A theoretical distribution analysis of synthetic minority oversampling technique (SMOTE) for imbalanced learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The class imbalance problem in deep learning. <em>ML</em>,
<em>113</em>(7), 4845–4901. (<a
href="https://doi.org/10.1007/s10994-022-06268-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently unleashed the ability for Machine learning (ML) to make unparalleled strides. It did so by confronting and successfully addressing, at least to a certain extent, the knowledge bottleneck that paralyzed ML and artificial intelligence for decades. The community is currently basking in deep learning’s success, but a question that comes to mind is: have all of the issues previously affecting machine learning systems been solved by deep learning or do some issues remain for which deep learning is not a bulletproof solution? This question in the context of the class imbalance becomes a motivation for this paper. Imbalance problem was first recognized almost three decades ago and has remained a critical challenge at least for traditional learning approaches. Our goal is to investigate whether the tight dependency between class imbalances, concept complexities, dataset size and classifier performance, known to exist in traditional learning systems, is alleviated in any way in deep learning approaches and to what extent, if any, network depth and regularization can help. To answer these questions we conduct a survey of the recent literature focused on deep learning and the class imbalance problem as well as a series of controlled experiments on both artificial and real-world domains. This allows us to formulate lessons learned about the impact of class imbalance on deep learning models, as well as pose open challenges that should be tackled by researchers in this field.},
  archive      = {J_ML},
  author       = {Ghosh, Kushankur and Bellinger, Colin and Corizzo, Roberto and Branco, Paula and Krawczyk, Bartosz and Japkowicz, Nathalie},
  doi          = {10.1007/s10994-022-06268-8},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4845-4901},
  shortjournal = {Mach. Learn.},
  title        = {The class imbalance problem in deep learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing reinforcement learning for de novo drug design.
<em>ML</em>, <em>113</em>(7), 4811–4843. (<a
href="https://doi.org/10.1007/s10994-024-06519-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based approaches for generating novel drug molecules with specific properties have gained a lot of interest in the last few years. Recent studies have demonstrated promising performance for string-based generation of novel molecules utilizing reinforcement learning. In this paper, we develop a unified framework for using reinforcement learning for de novo drug design, wherein we systematically study various on- and off-policy reinforcement learning algorithms and replay buffers to learn an RNN-based policy to generate novel molecules predicted to be active against the dopamine receptor DRD2. Our findings suggest that it is advantageous to use at least both top-scoring and low-scoring molecules for updating the policy when structural diversity is essential. Using all generated molecules at an iteration seems to enhance performance stability for on-policy algorithms. In addition, when replaying high, intermediate, and low-scoring molecules, off-policy algorithms display the potential of improving the structural diversity and number of active molecules generated, but possibly at the cost of a longer exploration phase. Our work provides an open-source framework enabling researchers to investigate various reinforcement learning methods for de novo drug design.},
  archive      = {J_ML},
  author       = {Gummesson Svensson, Hampus and Tyrchan, Christian and Engkvist, Ola and Haghir Chehreghani, Morteza},
  doi          = {10.1007/s10994-024-06519-w},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4811-4843},
  shortjournal = {Mach. Learn.},
  title        = {Utilizing reinforcement learning for de novo drug design},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding CNN fragility when learning with imbalanced
data. <em>ML</em>, <em>113</em>(7), 4785–4810. (<a
href="https://doi.org/10.1007/s10994-023-06326-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved impressive results on imbalanced image data, but they still have difficulty generalizing to minority classes and their decisions are difficult to interpret. These problems are related because the method by which CNNs generalize to minority classes, which requires improvement, is wrapped in a black-box. To demystify CNN decisions on imbalanced data, we focus on their latent features. Although CNNs embed the pattern knowledge learned from a training set in model parameters, the effect of this knowledge is contained in feature and classification embeddings (FE and CE). These embeddings can be extracted from a trained model and their global, class properties (e.g., frequency, magnitude and identity) can be analyzed. We find that important information regarding the ability of a neural network to generalize to minority classes resides in the class top-K CE and FE. We show that a CNN learns a limited number of class top-K CE per category, and that their magnitudes vary based on whether the same class is balanced or imbalanced. We hypothesize that latent class diversity is as important as the number of class examples, which has important implications for re-sampling and cost-sensitive methods. These methods generally focus on rebalancing model weights, class numbers and margins; instead of diversifying class latent features. We also demonstrate that a CNN has difficulty generalizing to test data if the magnitude of its top-K latent features do not match the training set. We use three popular image datasets and two cost-sensitive algorithms commonly employed in imbalanced learning for our experiments.},
  archive      = {J_ML},
  author       = {Dablain, Damien and Jacobson, Kristen N. and Bellinger, Colin and Roberts, Mark and Chawla, Nitesh V.},
  doi          = {10.1007/s10994-023-06326-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4785-4810},
  shortjournal = {Mach. Learn.},
  title        = {Understanding CNN fragility when learning with imbalanced data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting directional bitcoin price returns using
aspect-based sentiment analysis on online text data. <em>ML</em>,
<em>113</em>(7), 4761–4784. (<a
href="https://doi.org/10.1007/s10994-021-06095-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of cryptocurrency markets has drastically changed how online transactions are conducted and provide a new investment opportunity. This study contributes to the literature on directional cryptocurrency price returns prediction by expanding the set of meaningful features extracted from textual data with sentiment analysis and comparing their usefulness across multiple data sources. In contrast to previous studies, we use fine-grained topic-sentiment features. More specifically, aspect-based sentiment analysis models, JST and TS-LDA, are implemented to incorporate joint topical-sentiment features and the degree of text subjectivity. We collected, and make available, a dataset, which consists of data scraped from Reddit, Bitcointalk and CryptoCompare sources, to demonstrate that proposed features lead to interpretable topics and an improvement in predictive performance.},
  archive      = {J_ML},
  author       = {Loginova, Ekaterina and Tsang, Wai Kit and van Heijningen, Guus and Kerkhove, Louis-Philippe and Benoit, Dries F.},
  doi          = {10.1007/s10994-021-06095-3},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4761-4784},
  shortjournal = {Mach. Learn.},
  title        = {Forecasting directional bitcoin price returns using aspect-based sentiment analysis on online text data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coefficient tree regression: Fast, accurate and
interpretable predictive modeling. <em>ML</em>, <em>113</em>(7),
4723–4759. (<a
href="https://doi.org/10.1007/s10994-021-06091-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of data collection technologies often results in large data sets with many observations and many variables. In practice, highly relevant engineered features are often groups of predictors that share a common regression coefficient (i.e., the predictors in the group affect the response only via their collective sum), where the groups are unknown in advance and must be discovered from the data. We propose an algorithm called coefficient tree regression (CTR) to discover the group structure and fit the resulting regression model. In this regard CTR is an automated way of engineering new features, each of which is the collective sum of the predictors within each group. The algorithm can be used when the number of variables is larger than, or smaller than, the number of observations. Creating new features that affect the response in a similar manner improves predictive modeling, especially in domains where the relationships between predictors are not known a priori. CTR borrows computational strategies from both linear regression (fast model updating when adding/modifying a feature in the model) and regression trees (fast partitioning to form and split groups) to achieve outstanding computational and predictive performance. Finding features that represent hidden groups of predictors (i.e., a hidden ontology) that impact the response only via their sum also has major interpretability advantages, which we demonstrate with a real data example of predicting political affiliations with television viewing habits. In numerical comparisons over a variety of examples, we demonstrate that both computational expense and predictive performance are far superior to existing methods that create features as groups of predictors. Moreover, CTR has overall predictive performance that is comparable to or slightly better than the regular lasso method, which we include as a reference benchmark for comparison even though it is non-group-based, in addition to having substantial computational and interpretive advantages over lasso.},
  archive      = {J_ML},
  author       = {Sürer, Özge and Apley, Daniel W. and Malthouse, Edward C.},
  doi          = {10.1007/s10994-021-06091-7},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4723-4759},
  shortjournal = {Mach. Learn.},
  title        = {Coefficient tree regression: Fast, accurate and interpretable predictive modeling},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoclust: Synthetic minority oversampling based on stream
clustering for evolving data streams. <em>ML</em>, <em>113</em>(7),
4671–4721. (<a
href="https://doi.org/10.1007/s10994-023-06420-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world data stream applications not only suffer from concept drift but also class imbalance. Yet, very few existing studies investigated this joint challenge. Data difficulty factors, which have been shown to be key challenges in class imbalanced data streams, are not taken into account by existing approaches when learning class imbalanced data streams. In this work, we propose a drift adaptable oversampling strategy to synthesise minority class examples based on stream clustering. The motivation is that stream clustering methods continuously update themselves to reflect the characteristics of the current underlying concept, including data difficulty factors. This nature can potentially be used to compress past information without caching data in the memory explicitly. Based on the compressed information, synthetic examples can be created within the region that recently generated new minority class examples. Experiments with artificial and real-world data streams show that the proposed approach can handle concept drift involving different minority class decomposition better than existing approaches, especially when the data stream is severely class imbalanced and presenting high proportions of safe and borderline minority class examples.},
  archive      = {J_ML},
  author       = {Chiu, Chun Wai and Minku, Leandro L.},
  doi          = {10.1007/s10994-023-06420-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4671-4721},
  shortjournal = {Mach. Learn.},
  title        = {Smoclust: Synthetic minority oversampling based on stream clustering for evolving data streams},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Glacier: Guided locally constrained counterfactual
explanations for time series classification. <em>ML</em>,
<em>113</em>(7), 4639–4669. (<a
href="https://doi.org/10.1007/s10994-023-06502-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning applications, there is a need to obtain predictive models of high performance and, most importantly, to allow end-users and practitioners to understand and act on their predictions. One way to obtain such understanding is via counterfactuals, that provide sample-based explanations in the form of recommendations on which features need to be modified from a test example so that the classification outcome of a given classifier changes from an undesired outcome to a desired one. This paper focuses on the domain of time series classification, more specifically, on defining counterfactual explanations for univariate time series. We propose Glacier, a model-agnostic method for generating locally-constrained counterfactual explanations for time series classification using gradient search either on the original space or on a latent space that is learned through an auto-encoder. An additional flexibility of our method is the inclusion of constraints on the counterfactual generation process that favour applying changes to particular time series points or segments while discouraging changing others. The main purpose of these constraints is to ensure more reliable counterfactuals, while increasing the efficiency of the counterfactual generation process. Two particular types of constraints are considered, i.e., example-specific constraints and global constraints. We conduct extensive experiments on 40 datasets from the UCR archive, comparing different instantiations of Glacier against three competitors. Our findings suggest that Glacier outperforms the three competitors in terms of two common metrics for counterfactuals, i.e., proximity and compactness. Moreover, Glacier obtains comparable counterfactual validity compared to the best of the three competitors. Finally, when comparing the unconstrained variant of Glacier to the constraint-based variants, we conclude that the inclusion of example-specific and global constraints yields a good performance while demonstrating the trade-off between the different metrics.},
  archive      = {J_ML},
  author       = {Wang, Zhendong and Samsten, Isak and Miliou, Ioanna and Mochaourab, Rami and Papapetrou, Panagiotis},
  doi          = {10.1007/s10994-023-06502-x},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4639-4669},
  shortjournal = {Mach. Learn.},
  title        = {Glacier: Guided locally constrained counterfactual explanations for time series classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exposing and explaining fake news on-the-fly. <em>ML</em>,
<em>113</em>(7), 4615–4637. (<a
href="https://doi.org/10.1007/s10994-024-06527-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms enable the rapid dissemination and consumption of information. However, users instantly consume such content regardless of the reliability of the shared data. Consequently, the latter crowdsourcing model is exposed to manipulation. This work contributes with an explainable and online classification method to recognize fake news in real-time. The proposed method combines both unsupervised and supervised Machine Learning approaches with online created lexica. The profiling is built using creator-, content- and context-based features using Natural Language Processing techniques. The explainable classification mechanism displays in a dashboard the features selected for classification and the prediction confidence. The performance of the proposed solution has been validated with real data sets from Twitter and the results attain 80% accuracy and macro F-measure. This proposal is the first to jointly provide data stream processing, profiling, classification and explainability. Ultimately, the proposed early detection, isolation and explanation of fake news contribute to increase the quality and trustworthiness of social media contents.},
  archive      = {J_ML},
  author       = {de Arriba-Pérez, Francisco and García-Méndez, Silvia and Leal, Fátima and Malheiro, Benedita and Burguillo, Juan Carlos},
  doi          = {10.1007/s10994-024-06527-w},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4615-4637},
  shortjournal = {Mach. Learn.},
  title        = {Exposing and explaining fake news on-the-fly},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning explanatory logical rules in non-linear domains: A
neuro-symbolic approach. <em>ML</em>, <em>113</em>(7), 4579–4614. (<a
href="https://doi.org/10.1007/s10994-024-06538-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks, despite their capabilities, are constrained by the need for large-scale training data, and often fall short in generalisation and interpretability. Inductive logic programming (ILP) presents an intriguing solution with its data-efficient learning of first-order logic rules. However, ILP grapples with challenges, notably the handling of non-linearity in continuous domains. With the ascent of neuro-symbolic ILP, there’s a drive to mitigate these challenges, synergising deep learning with relational ILP models to enhance interpretability and create logical decision boundaries. In this research, we introduce a neuro-symbolic ILP framework, grounded on differentiable Neural Logic networks, tailored for non-linear rule extraction in mixed discrete-continuous spaces. Our methodology consists of a neuro-symbolic approach, emphasising the extraction of non-linear functions from mixed domain data. Our preliminary findings showcase our architecture’s capability to identify non-linear functions from continuous data, offering a new perspective in neural-symbolic research and underlining the adaptability of ILP-based frameworks for regression challenges in continuous scenarios.},
  archive      = {J_ML},
  author       = {Bueff, Andreas and Belle, Vaishak},
  doi          = {10.1007/s10994-024-06538-7},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4579-4614},
  shortjournal = {Mach. Learn.},
  title        = {Learning explanatory logical rules in non-linear domains: A neuro-symbolic approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hellinger distance decision trees for PU learning in
imbalanced data sets. <em>ML</em>, <em>113</em>(7), 4547–4578. (<a
href="https://doi.org/10.1007/s10994-023-06323-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from positive and unlabeled data, or PU learning, is the setting in which a binary classifier can only train from positive and unlabeled instances, the latter containing both positive as well as negative instances. Many PU applications, e.g., fraud detection, are also characterized by class imbalance, which creates a challenging setting. Not only are fewer minority class examples compared to the case where all labels are known, there is also only a small fraction of unlabeled observations that would actually be positive. Despite the relevance of the topic, only a few studies have considered a class imbalance setting in PU learning. In this paper, we propose a novel technique that can directly handle imbalanced PU data, named the PU Hellinger Decision Tree (PU-HDT). Our technique exploits the class prior to estimate the counts of positives and negatives in every node in the tree. Moreover, the Hellinger distance is used instead of more conventional splitting criteria because it has been shown to be class-imbalance insensitive. This simple yet effective adaptation allows PU-HDT to perform well in highly imbalanced PU data sets. We also introduce PU Stratified Hellinger Random Forest (PU-SHRF), which uses PU-HDT as its base learner and integrates a stratified bootstrap sampling. Our empirical analysis shows that PU-SHRF substantially outperforms state-of-the-art PU learning methods for imbalanced data sets in most experimental settings.},
  archive      = {J_ML},
  author       = {Ortega Vázquez, Carlos and vanden Broucke, Seppe and De Weerdt, Jochen},
  doi          = {10.1007/s10994-023-06323-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4547-4578},
  shortjournal = {Mach. Learn.},
  title        = {Hellinger distance decision trees for PU learning in imbalanced data sets},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VEST: Automatic feature engineering for forecasting.
<em>ML</em>, <em>113</em>(7), 4523–4545. (<a
href="https://doi.org/10.1007/s10994-021-05959-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is a challenging task with applications in a wide range of domains. Auto-regression is one of the most common approaches to address these problems. Accordingly, observations are modelled by multiple regression using their past lags as predictor variables. We investigate the extension of auto-regressive processes using statistics which summarise the recent past dynamics of time series. The result of our research is a novel framework called VEST, designed to perform feature engineering using univariate and numeric time series automatically. The proposed approach works in three main steps. First, recent observations are mapped onto different representations. Second, each representation is summarised by statistical functions. Finally, a filter is applied for feature selection. We discovered that combining the features generated by VEST with auto-regression significantly improves forecasting performance in a database composed by 90 time series with high sampling frequency. However, we also found that there are no improvements when the framework is applied for multi-step forecasting or in time series with low sample size. VEST is publicly available online.},
  archive      = {J_ML},
  author       = {Cerqueira, Vitor and Moniz, Nuno and Soares, Carlos},
  doi          = {10.1007/s10994-021-05959-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4523-4545},
  shortjournal = {Mach. Learn.},
  title        = {VEST: Automatic feature engineering for forecasting},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification with costly features in hierarchical deep
sets. <em>ML</em>, <em>113</em>(7), 4487–4522. (<a
href="https://doi.org/10.1007/s10994-024-06565-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification with costly features (CwCF) is a classification problem that includes the cost of features in the optimization criteria. Individually for each sample, its features are sequentially acquired to maximize accuracy while minimizing the acquired features’ cost. However, existing approaches can only process data that can be expressed as vectors of fixed length. In real life, the data often possesses rich and complex structure, which can be more precisely described with formats such as XML or JSON. The data is hierarchical and often contains nested lists of objects. In this work, we extend an existing deep reinforcement learning-based algorithm with hierarchical deep sets and hierarchical softmax, so that it can directly process this data. The extended method has greater control over which features it can acquire and, in experiments with seven datasets, we show that this leads to superior performance. To showcase the real usage of the new method, we apply it to a real-life problem of classifying malicious web domains, using an online service.},
  archive      = {J_ML},
  author       = {Janisch, Jaromír and Pevný, Tomáš and Lisý, Viliam},
  doi          = {10.1007/s10994-024-06565-4},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4487-4522},
  shortjournal = {Mach. Learn.},
  title        = {Classification with costly features in hierarchical deep sets},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed and explainable GHSOM for anomaly detection in
sensor networks. <em>ML</em>, <em>113</em>(7), 4445–4486. (<a
href="https://doi.org/10.1007/s10994-023-06501-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of anomalous activities is a challenging and crucially important task in sensor networks. This task is becoming increasingly complex with the increasing volume of data generated in real-world domains, and greatly benefits from the use of predictive models to identify anomalies in real time. A key use case for this task is the identification of misbehavior that may be caused by involuntary faults or deliberate actions. However, currently adopted anomaly detection methods are often affected by limitations such as the inability to analyze large-scale data, a reduced effectiveness when data presents multiple densities, a strong dependence on user-defined threshold configurations, and a lack of explainability in the extracted predictions. In this paper, we propose a distributed deep learning method that extends growing hierarchical self-organizing maps, originally designed for clustering tasks, to address anomaly detection tasks. The SOM-based modeling capabilities of the method enable the analysis of data with multiple densities, by exploiting multiple SOMs organized as a hierarchy. Our map-reduce implementation under Apache Spark allows the method to process and analyze large-scale sensor network data. An automatic threshold-tuning strategy reduces user efforts and increases the robustness of the method with respect to noisy instances. Moreover, an explainability component resorting to instance-based feature ranking emphasizes the most salient features influencing the decisions of the anomaly detection model, supporting users in their understanding of raised alerts. Experiments are conducted on five real-world sensor network datasets, including wind and photovoltaic energy production, vehicular traffic, and pedestrian flows. Our results show that the proposed method outperforms state-of-the-art anomaly detection competitors. Furthermore, a scalability analysis reveals that the method is able to scale linearly as the data volume presented increases, leveraging multiple worker nodes in a distributed computing setting. Qualitative analyses on the level of anomalous pollen in the air further emphasize the effectiveness of our proposed method, and its potential in determining the level of danger in raised alerts.},
  archive      = {J_ML},
  author       = {Mignone, Paolo and Corizzo, Roberto and Ceci, Michelangelo},
  doi          = {10.1007/s10994-023-06501-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4445-4486},
  shortjournal = {Mach. Learn.},
  title        = {Distributed and explainable GHSOM for anomaly detection in sensor networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-sensitive sparse group online learning for imbalanced
data streams. <em>ML</em>, <em>113</em>(7), 4407–4444. (<a
href="https://doi.org/10.1007/s10994-023-06403-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective streaming feature selection in dynamic online environments is essential in numerous applications. However, most existing methods evaluate high-dimensional features individually and ignore the potentially pertainable group structures of features. Moreover, the class imbalance underlying streaming data may further decrease the discriminative efficacy of the selected features, resulting in deteriorated classification performance. Motivated by this observation, we propose a cost-sensitive sparse group online learning (CSGOL) framework and its proximal version (PCSGOL) to handle imbalanced and high-dimensional streaming data. We formulate this issue as a new cost-sensitive online optimization problem by leveraging the $$\ell _2$$ -norm, $$\ell _1$$ -norm, and groupwise sparsity constraints in the dual averaging regularization. Inspired by the proximal optimization, we further introduce the average weighted distance in CSGOL and develop the PCSGOL method to achieve stable prediction results. We mathematically derive closed-form solutions to the optimization problems with four modified hinge loss functions, leading to four variants of CSGOL and PCSGOL. Extensive empirical studies on real-world streaming datasets and online anomaly detection tasks demonstrate the effectiveness of our proposed methods.},
  archive      = {J_ML},
  author       = {Chen, Zhong and Sheng, Victor and Edwards, Andrea and Zhang, Kun},
  doi          = {10.1007/s10994-023-06403-z},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4407-4444},
  shortjournal = {Mach. Learn.},
  title        = {Cost-sensitive sparse group online learning for imbalanced data streams},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite score for anomaly detection in imbalanced
real-world industrial dataset. <em>ML</em>, <em>113</em>(7), 4381–4406.
(<a href="https://doi.org/10.1007/s10994-023-06415-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the industrial sector has evolved towards its fourth revolution. The quality control domain is particularly interested in advanced machine learning for computer vision anomaly detection. Nevertheless, several challenges have to be faced, including imbalanced datasets, the image complexity, and the zero-false-negative (ZFN) constraint to guarantee the high-quality requirement. This paper illustrates a use case for an industrial partner, where Printed Circuit Board Assembly (PCBA) images are first reconstructed with a Vector Quantized Generative Adversarial Network (VQGAN) trained on normal products. Then, several multi-level metrics are extracted on a few normal and abnormal images, highlighting anomalies through reconstruction differences. Finally, a classifier is trained to build a composite anomaly score thanks to the metrics extracted. This three-step approach is performed on the public MVTec-AD datasets and on the partner PCBA dataset, where it achieves a regular accuracy of 94.65% and 87.93% under the ZFN constraint.},
  archive      = {J_ML},
  author       = {Bougaham, Arnaud and El Adoui, Mohammed and Linden, Isabelle and Frénay, Benoît},
  doi          = {10.1007/s10994-023-06415-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4381-4406},
  shortjournal = {Mach. Learn.},
  title        = {Composite score for anomaly detection in imbalanced real-world industrial dataset},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persian offensive language detection. <em>ML</em>,
<em>113</em>(7), 4359–4379. (<a
href="https://doi.org/10.1007/s10994-023-06370-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of social networks and their impact on human life, one of the rising problems in this environment is the rise in verbal and written insults and hatred. As one of the significant platforms for distributing text-based content, Twitter frequently publishes its users’ abusive remarks. Creating a model that requires a complete collection of offensive sentences is the initial stage in recognizing objectionable phrases. In addition, despite the abundance of resources in English and other languages, there are limited resources and studies on identifying hateful and offensive statements in Persian. In this study, we compiled a 38K-tweet dataset of Persian Hate and Offensive language using keyword-based data selection strategies. A Persian offensive lexicon and nine hatred target group lexicons were gathered through crowdsourcing for this purpose. The dataset was annotated manually so that at least two annotators investigated tweets. In addition, for the purpose of analyzing the effect of used lexicons on language model functionality, we employed two assessment criteria (FPED and pAUCED) to measure the dataset’s potential bias. Then, by configuring the dataset based on the results of the bias measurement, we mitigated the effect of words’ bias in tweets on language model performance. The results indicate that bias is significantly diminished, while less than a hundredth reduced the F1 score.},
  archive      = {J_ML},
  author       = {Kebriaei, Emad and Homayouni, Ali and Faraji, Roghayeh and Razavi, Armita and Shakery, Azadeh and Faili, Heshaam and Yaghoobzadeh, Yadollah},
  doi          = {10.1007/s10994-023-06370-5},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4359-4379},
  shortjournal = {Mach. Learn.},
  title        = {Persian offensive language detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-consensus decentralized primal-dual fixed point
algorithm for distributed learning. <em>ML</em>, <em>113</em>(7),
4315–4357. (<a
href="https://doi.org/10.1007/s10994-024-06537-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized distributed learning has recently attracted significant attention in many applications in machine learning and signal processing. To solve a decentralized optimization with regularization, we propose a Multi-consensus Decentralized Primal-Dual Fixed Point (MD-PDFP) algorithm. We apply multiple consensus steps with the gradient tracking technique to extend the primal-dual fixed point method over a network. The communication complexities of our procedure are given under certain conditions. Moreover, we show that our algorithm is consistent under general conditions and enjoys global linear convergence under strong convexity. With some particular choices of regularizations, our algorithm can be applied to decentralized machine learning applications. Finally, several numerical experiments and real data analyses are conducted to demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_ML},
  author       = {Tang, Kejie and Liu, Weidong and Mao, Xiaojun},
  doi          = {10.1007/s10994-024-06537-8},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4315-4357},
  shortjournal = {Mach. Learn.},
  title        = {Multi-consensus decentralized primal-dual fixed point algorithm for distributed learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can cross-domain term extraction benefit from cross-lingual
transfer and nested term labeling? <em>ML</em>, <em>113</em>(7),
4285–4314. (<a
href="https://doi.org/10.1007/s10994-023-06506-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic term extraction (ATE) is a natural language processing task that eases the effort of manually identifying terms from domain-specific corpora by providing a list of candidate terms. In this paper, we treat ATE as a sequence-labeling task and explore the efficacy of XLMR in evaluating cross-lingual and multilingual learning against monolingual learning in the cross-domain ATE context. Additionally, we introduce NOBI, a novel annotation mechanism enabling the labeling of single-word nested terms. Our experiments are conducted on the ACTER corpus, encompassing four domains and three languages (English, French, and Dutch), as well as the RSDO5 Slovenian corpus, encompassing four additional domains. Results indicate that cross-lingual and multilingual models outperform monolingual settings, showcasing improved F1-scores for all languages within the ACTER dataset. When incorporating an additional Slovenian corpus into the training set, the multilingual model exhibits superior performance compared to state-of-the-art approaches in specific scenarios. Moreover, the newly introduced NOBI labeling mechanism enhances the classifier’s capacity to extract short nested terms significantly, leading to substantial improvements in Recall for the ACTER dataset and consequentially boosting the overall F1-score performance.},
  archive      = {J_ML},
  author       = {Tran, Hanh Thi Hong and Martinc, Matej and Repar, Andraz and Ljubešić, Nikola and Doucet, Antoine and Pollak, Senja},
  doi          = {10.1007/s10994-023-06506-7},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4285-4314},
  shortjournal = {Mach. Learn.},
  title        = {Can cross-domain term extraction benefit from cross-lingual transfer and nested term labeling?},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can metafeatures help improve explanations of prediction
models when using behavioral and textual data? <em>ML</em>,
<em>113</em>(7), 4245–4284. (<a
href="https://doi.org/10.1007/s10994-021-05981-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models built on behavioral and textual data can result in highly accurate prediction models, but are often very difficult to interpret. Linear models require investigating thousands of coefficients, while the opaqueness of nonlinear models makes things worse. Rule-extraction techniques have been proposed to combine the desired predictive accuracy of complex “black-box” models with global explainability. However, rule-extraction in the context of high-dimensional, sparse data, where many features are relevant to the predictions, can be challenging, as replacing the black-box model by many rules leaves the user again with an incomprehensible explanation. To address this problem, we develop and test a rule-extraction methodology based on higher-level, less-sparse “metafeatures”. We empirically validate the quality of the explanation rules in terms of fidelity, stability, and accuracy over a collection of data sets, and benchmark their performance against rules extracted using the fine-grained behavioral and textual features. A key finding of our analysis is that metafeatures-based explanations are better at mimicking the behavior of the black-box prediction model, as measured by the fidelity of explanations.},
  archive      = {J_ML},
  author       = {Ramon, Yanou and Martens, David and Evgeniou, Theodoros and Praet, Stiene},
  doi          = {10.1007/s10994-021-05981-0},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4245-4284},
  shortjournal = {Mach. Learn.},
  title        = {Can metafeatures help improve explanations of prediction models when using behavioral and textual data?},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on learning from imbalanced data streams: Taxonomy,
challenges, empirical study, and reproducible experimental framework.
<em>ML</em>, <em>113</em>(7), 4165–4243. (<a
href="https://doi.org/10.1007/s10994-023-06353-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance poses new challenges when it comes to classifying data streams. Many algorithms recently proposed in the literature tackle this problem using a variety of data-level, algorithm-level, and ensemble approaches. However, there is a lack of standardized and agreed-upon procedures and benchmarks on how to evaluate these algorithms. This work proposes a standardized, exhaustive, and comprehensive experimental framework to evaluate algorithms in a collection of diverse and challenging imbalanced data stream scenarios. The experimental study evaluates 24 state-of-the-art data streams algorithms on 515 imbalanced data streams that combine static and dynamic class imbalance ratios, instance-level difficulties, concept drift, real-world and semi-synthetic datasets in binary and multi-class scenarios. This leads to a large-scale experimental study comparing state-of-the-art classifiers in the data stream mining domain. We discuss the advantages and disadvantages of state-of-the-art classifiers in each of these scenarios and we provide general recommendations to end-users for selecting the best algorithms for imbalanced data streams. Additionally, we formulate open challenges and future directions for this domain. Our experimental framework is fully reproducible and easy to extend with new methods. This way, we propose a standardized approach to conducting experiments in imbalanced data streams that can be used by other researchers to create complete, trustworthy, and fair evaluation of newly proposed methods. Our experimental framework can be downloaded from https://github.com/canoalberto/imbalanced-streams .},
  archive      = {J_ML},
  author       = {Aguiar, Gabriel and Krawczyk, Bartosz and Cano, Alberto},
  doi          = {10.1007/s10994-023-06353-6},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4165-4243},
  shortjournal = {Mach. Learn.},
  title        = {A survey on learning from imbalanced data streams: Taxonomy, challenges, empirical study, and reproducible experimental framework},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An encoding approach for stable change point detection.
<em>ML</em>, <em>113</em>(7), 4133–4163. (<a
href="https://doi.org/10.1007/s10994-023-06510-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Without imposing prior distributional knowledge underlying multivariate time series of interest, we propose a nonparametric change-point detection approach to estimate the number of change points and their locations along the temporal axis. We develop a structural subsampling procedure such that the observations are encoded into multiple sequences of Bernoulli variables. A maximum likelihood approach in conjunction with a newly developed searching algorithm is implemented to detect change points on each Bernoulli process separately. Then, aggregation statistics are proposed to collectively synthesize change-point results from all individual univariate time series into consistent and stable location estimations. We also study a weighting strategy to measure the degree of relevance for different subsampled groups. Simulation studies are conducted and shown that the proposed change-point methodology for multivariate time series has favorable performance comparing with currently available state-of-the-art nonparametric methods under various settings with different degrees of complexity. Real data analyses are finally performed on categorical, ordinal, and continuous time series taken from fields of genetics, climate, and finance.},
  archive      = {J_ML},
  author       = {Wang, Xiaodong and Hsieh, Fushing},
  doi          = {10.1007/s10994-023-06510-x},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4133-4163},
  shortjournal = {Mach. Learn.},
  title        = {An encoding approach for stable change point detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Understanding transfer learning and gradient-based
meta-learning techniques. <em>ML</em>, <em>113</em>(7), 4113–4132. (<a
href="https://doi.org/10.1007/s10994-023-06387-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data regimes of similar data distribution as the one used for training. Our findings show that both the output layer and the noisy training conditions induced by data scarcity play important roles in facilitating this specialization for MAML. Lastly, we show that the pre-trained features as obtained by the finetuning baseline are more diverse and discriminative than those learned by MAML and Reptile. Due to this lack of diversity and distribution specialization, MAML and Reptile may fail to generalize to out-of-distribution tasks whereas finetuning can fall back on the diversity of the learned features.},
  archive      = {J_ML},
  author       = {Huisman, Mike and Plaat, Aske and van Rijn, Jan N.},
  doi          = {10.1007/s10994-023-06387-w},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4113-4132},
  shortjournal = {Mach. Learn.},
  title        = {Understanding transfer learning and gradient-based meta-learning techniques},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPQ: Dynamic pseudo-mean mixed-precision quantization for
pruned neural network. <em>ML</em>, <em>113</em>(7), 4099–4112. (<a
href="https://doi.org/10.1007/s10994-023-06453-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing layers and hyper-parameters of deep neural network are continuously growing to generate large-scale network by training huge masses of data. However, it is difficult to deploy deep neural network on resource-constrained edge devices. Network mixed-precision quantization is a challenging way to prune and compress deep neural network models while discovering the optimal bit width for each layer. To solve the big challenge, we therefore propose the dynamic pseudo-mean mixed-precision quantization (DPQ) by introducing two-bit scaling factors to compensate errors of quantization. Furthermore, the activation quantization named random parameters clipping (RPC) is proposed. RPC adopts partial activation quantization to reduce loss of accuracy. Therefore, DPQ can dynamically adjust the bit precision of weight quantization according to the distribution of weights. It results in a quantification scheme with strong robustness compared to previous methods. Extensive experiments demonstrate that DPQ achieves 15.43 $$\times$$ compression rate of ResNet20 on CIFAR-10 dataset with 0.22% increase in accuracy, and 35.25 $$\times$$ compression rate of Resnet56 on SVHN dataset with 0.12% increase in accuracy.},
  archive      = {J_ML},
  author       = {Pei, Songwen and Wang, Jiyao and Zhang, Bingxue and Qin, Wei and Xue, Hai and Ye, Xiaochun and Chen, Mingsong},
  doi          = {10.1007/s10994-023-06453-3},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4099-4112},
  shortjournal = {Mach. Learn.},
  title        = {DPQ: Dynamic pseudo-mean mixed-precision quantization for pruned neural network},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid approaches to optimization and machine learning
methods: A systematic literature review. <em>ML</em>, <em>113</em>(7),
4055–4097. (<a
href="https://doi.org/10.1007/s10994-023-06467-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notably, real problems are increasingly complex and require sophisticated models and algorithms capable of quickly dealing with large data sets and finding optimal solutions. However, there is no perfect method or algorithm; all of them have some limitations that can be mitigated or eliminated by combining the skills of different methodologies. In this way, it is expected to develop hybrid algorithms that can take advantage of the potential and particularities of each method (optimization and machine learning) to integrate methodologies and make them more efficient. This paper presents an extensive systematic and bibliometric literature review on hybrid methods involving optimization and machine learning techniques for clustering and classification. It aims to identify the potential of methods and algorithms to overcome the difficulties of one or both methodologies when combined. After the description of optimization and machine learning methods, a numerical overview of the works published since 1970 is presented. Moreover, an in-depth state-of-art review over the last three years is presented. Furthermore, a SWOT analysis of the ten most cited algorithms of the collected database is performed, investigating the strengths and weaknesses of the pure algorithms and detaching the opportunities and threats that have been explored with hybrid methods. Thus, with this investigation, it was possible to highlight the most notable works and discoveries involving hybrid methods in terms of clustering and classification and also point out the difficulties of the pure methods and algorithms that can be strengthened through the inspirations of other methodologies; they are hybrid methods.},
  archive      = {J_ML},
  author       = {Azevedo, Beatriz Flamia and Rocha, Ana Maria A. C. and Pereira, Ana I.},
  doi          = {10.1007/s10994-023-06467-x},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4055-4097},
  shortjournal = {Mach. Learn.},
  title        = {Hybrid approaches to optimization and machine learning methods: A systematic literature review},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DynamiSE: Dynamic signed network embedding for link
prediction. <em>ML</em>, <em>113</em>(7), 4037–4053. (<a
href="https://doi.org/10.1007/s10994-023-06473-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, dynamic signed networks are ubiquitous where edges have positive and negative types and evolve over time. Graph neural networks have achieved impressive performance in node representation learning, facilitating various downstream tasks, i.e., link prediction. However, employing existing methods to learn dynamic signed network embedding faces the following challenges. First, it is hard to encode the dynamics and sign semantics of network simultaneously. Positive and negative edges have different sign semantics and the graph structure is dynamically changing with time. Second, it is non-trivial to alleviate over-smoothing and construct the deeper dynamic signed network due to the learning of network dynamics. In this paper, we propose Dynamic Signed Network Embedding (DynamiSE) to tackle the above problems. DynamiSE effectively integrates the balance theory and ordinary differential equation into node representation learning to encode the dynamics of network and capture sign semantics between neighbors. Specifically, we design the dynamic sign collaboration unit to construct a deeper dynamic signed graph neural network, which models the evolution patterns and simulates the propagation and aggregation of sign semantics. The complex sign influence between nodes formed by different semantics of positive and negative edges is captured by the sign semantics aggregation unit. Extensive experiments on real-world dynamic signed networks show that DynamiSE outperforms most state-of-the-art methods in link prediction task.},
  archive      = {J_ML},
  author       = {Sun, Haiting and Tian, Peng and Xiong, Yun and Zhang, Yao and Xiang, Yali and Jia, Xing and Wang, Haofen},
  doi          = {10.1007/s10994-023-06473-z},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4037-4053},
  shortjournal = {Mach. Learn.},
  title        = {DynamiSE: Dynamic signed network embedding for link prediction},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On-the-fly image-level oversampling for imbalanced datasets
of manufacturing defects. <em>ML</em>, <em>113</em>(7), 4013–4035. (<a
href="https://doi.org/10.1007/s10994-023-06498-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual defect recognition and its manufacturing applications have been an upcoming topic in recent AI research. Defect datasets are often severely imbalanced and can be additionally burdened with separating classes of high visual similarity. Although various methods of data augmentation have been proposed to mitigate the class imbalance, they often fail to cope with tinier minority classes or have fidelity issues with smaller defects while, at the same time, needing significant computational resources to train. Also, augmentation based on vector-based oversampling struggles to produce high-fidelity inputs and is hard to apply on custom CNN architectures, which often perform better for this type of problem. Our work presents an image-level oversampling method based on an instance-based image generator that can be applied to any CNN directly during the training process without increasing the order of training time required. It is based on identifying a small number of the most uncertain base samples close to the estimated class boundaries and using them as seeds for augmentation. The resulting images are of high visual quality preserving small class differences, and they also improve the classifier boundary leading to higher recall scores than other state-of-the-art approaches.},
  archive      = {J_ML},
  author       = {Theodoropoulos, Spyros and Zajec, Patrik and Rožanec, Jože M. and Kyriazis, Dimosthenis and Tsanakas, Panayiotis},
  doi          = {10.1007/s10994-023-06498-4},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {4013-4035},
  shortjournal = {Mach. Learn.},
  title        = {On-the-fly image-level oversampling for imbalanced datasets of manufacturing defects},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TOCOL: Improving contextual representation of pre-trained
language models via token-level contrastive learning. <em>ML</em>,
<em>113</em>(7), 3999–4012. (<a
href="https://doi.org/10.1007/s10994-023-06512-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention, which allows transformers to capture deep bidirectional contexts, plays a vital role in BERT-like pre-trained language models. However, the maximum likelihood pre-training objective of BERT may produce an anisotropic word embedding space, which leads to biased attention scores for high-frequency tokens, as they are very close to each other in representation space and thus have higher similarities. This bias may ultimately affect the encoding of global contextual information. To address this issue, we propose TOCOL, a TOken-Level COntrastive Learning framework for improving the contextual representation of pre-trained language models, which integrates a novel self-supervised objective to the attention mechanism to reshape the word representation space and encourages PLM to capture the global semantics of sentences. Results on the GLUE Benchmark show that TOCOL brings considerable improvement over the original BERT. Furthermore, we conduct a detailed analysis and demonstrate the robustness of our approach for low-resource scenarios.},
  archive      = {J_ML},
  author       = {Wang, Keheng and Yin, Chuantao and Li, Rumei and Wang, Sirui and Xian, Yunsen and Rong, Wenge and Xiong, Zhang},
  doi          = {10.1007/s10994-023-06512-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {3999-4012},
  shortjournal = {Mach. Learn.},
  title        = {TOCOL: Improving contextual representation of pre-trained language models via token-level contrastive learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Did we personalize? Assessing personalization by an online
reinforcement learning algorithm using resampling. <em>ML</em>,
<em>113</em>(7), 3961–3997. (<a
href="https://doi.org/10.1007/s10994-024-06526-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user’s context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user’s historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an “optimized” intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resampling-based methodology for investigating whether the personalization exhibited by the RL algorithm is an artifact of the RL algorithm stochasticity. We illustrate our methodology with a case study by analyzing the data from a physical activity clinical trial called HeartSteps, which included the use of an online RL algorithm. We demonstrate how our approach enhances data-driven truth-in-advertising of algorithm personalization both across all users as well as within specific users in the study.},
  archive      = {J_ML},
  author       = {Ghosh, Susobhan and Kim, Raphael and Chhabria, Prasidh and Dwivedi, Raaz and Klasnja, Predrag and Liao, Peng and Zhang, Kelly and Murphy, Susan},
  doi          = {10.1007/s10994-024-06526-x},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {3961-3997},
  shortjournal = {Mach. Learn.},
  title        = {Did we personalize? assessing personalization by an online reinforcement learning algorithm using resampling},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalization bounds for learning under graph-dependence: A
survey. <em>ML</em>, <em>113</em>(7), 3929–3959. (<a
href="https://doi.org/10.1007/s10994-024-06536-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional statistical learning theory relies on the assumption that data are identically and independently distributed (i.i.d.). However, this assumption often does not hold in many real-life applications. In this survey, we explore learning scenarios where examples are dependent and their dependence relationship is described by a dependency graph, a commonly utilized model in probability and combinatorics. We collect various graph-dependent concentration bounds, which are then used to derive Rademacher complexity and stability generalization bounds for learning from graph-dependent data. We illustrate this paradigm through practical learning tasks and provide some research directions for future work. To our knowledge, this survey is the first of this kind on this subject.},
  archive      = {J_ML},
  author       = {Zhang, Rui-Ray and Amini, Massih-Reza},
  doi          = {10.1007/s10994-024-06536-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {3929-3959},
  shortjournal = {Mach. Learn.},
  title        = {Generalization bounds for learning under graph-dependence: A survey},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on feature engineering editorial. <em>ML</em>,
<em>113</em>(7), 3917–3928. (<a
href="https://doi.org/10.1007/s10994-021-06042-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the performance of any machine learning model, it is important to focus more on the data itself instead of continuously developing new algorithms. This is exactly the aim of feature engineering. It can be defined as the clever engineering of data hereby exploiting the intrinsic bias of the machine learning technique to our benefit, ideally both in terms of accuracy and interpretability at the same time. Often times it will be applied in combination with simple machine learning techniques such as regression models or decision trees to boost their performance (whilst maintaining the interpretability property which is so often needed in analytical modeling) but it may also improve complex techniques such as XGBoost and neural networks. Feature engineering aims at designing smart features in one of two possible ways: either by adjusting existing features using various transformations or by extracting or creating new meaningful features (a process often called “featurization”) from different sources (e.g., transactional data, network data, time series data, text data, etc.).},
  archive      = {J_ML},
  author       = {Verdonck, Tim and Baesens, Bart and Óskarsdóttir, María and vanden Broucke, Seppe},
  doi          = {10.1007/s10994-021-06042-2},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {3917-3928},
  shortjournal = {Mach. Learn.},
  title        = {Special issue on feature engineering editorial},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New algorithms for trace-ratio problem with application to
high-dimension and large-sample data dimensionality reduction.
<em>ML</em>, <em>113</em>(7), 3889–3916. (<a
href="https://doi.org/10.1007/s10994-020-05937-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning large-scale data sets with high dimensionality is a main concern in research areas including machine learning, visual recognition, information retrieval, to name a few. In many practical uses such as images, video, audio, and text processing, we have to face with high-dimension and large-sample data problems. The trace-ratio problem is a key problem for feature extraction and dimensionality reduction to circumvent the high dimensional space. However, it has been long believed that this problem has no closed-form solution, and one has to solve it by using some inner-outer iterative algorithms that are very time consuming. Therefore, efficient algorithms for high-dimension and large-sample trace-ratio problems are still lacking, especially for dense data problems. In this work, we present a closed-form solution for the trace-ratio problem, and propose two algorithms to solve it. Based on the formula and the randomized singular value decomposition, we first propose a randomized algorithm for solving high-dimension and large-sample dense trace-ratio problems. For high-dimension and large-sample sparse trace-ratio problems, we then propose an algorithm based on the closed-form solution and solving some consistent under-determined linear systems. Theoretical results are established to show the rationality and efficiency of the proposed methods. Numerical experiments are performed on some real-world data sets, which illustrate the superiority of the proposed algorithms over many state-of-the-art algorithms for high-dimension and large-sample dimensionality reduction problems.},
  archive      = {J_ML},
  author       = {Shi, Wenya and Wu, Gang},
  doi          = {10.1007/s10994-020-05937-w},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {3889-3916},
  shortjournal = {Mach. Learn.},
  title        = {New algorithms for trace-ratio problem with application to high-dimension and large-sample data dimensionality reduction},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient clustered federated learning via
model distance. <em>ML</em>, <em>113</em>(6), 3869–3888. (<a
href="https://doi.org/10.1007/s10994-023-06443-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered Federated Learning (CFL) leverages the differences among data distributions on clients to partition all clients into several clusters for personalized federated training. Compared with the conventional federated algorithms such as FedAvg, existing methods for CFL require either more communication costs or multi-stage computation overheads. In this paper, we propose an iterative CFL framework with almost the same communication cost as FedAvg in each round based on a novel model distance. Specifically, the model distance measures the discrepancy between the client model and the cluster model so that we can estimate the cluster identities for clients on the server side. The proposed model distance considers class-wise model dissimilarity, which enables us to apply it to multi-class classification even when the labels are non-iid across clients. To calculate the proposed model distance, we introduce two sampling methods which generate samples from feature distributions approximately without accessing the raw dataset. Experimental results show that our method can achieve superior and comparable performance on non-iid and iid data respectively with less communication cost compared with the baselines.},
  archive      = {J_ML},
  author       = {Zhang, Mao and Zhang, Tie and Cheng, Yifei and Bao, Changcun and Cao, Haoyu and Jiang, Deqiang and Xu, Linli},
  doi          = {10.1007/s10994-023-06443-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3869-3888},
  shortjournal = {Mach. Learn.},
  title        = {Communication-efficient clustered federated learning via model distance},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting counter-examples for active learning with partial
labels. <em>ML</em>, <em>113</em>(6), 3849–3868. (<a
href="https://doi.org/10.1007/s10994-023-06485-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a new problem, active learning with partial labels (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the overfitting, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from counter-examples (CEs), our objective is to leverage this human-like learning pattern to tackle the overfitting while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this complementary pattern. By leveraging the distribution gap between WorseNet and the predictor, this adversarial evaluation manner could enhance both the performance of the predictor itself and the sample selection process, allowing the predictor to capture more accurate patterns in the data. Experimental results on five real-world datasets and four benchmark datasets show that our proposed method achieves comprehensive improvements over ten representative AL frameworks, highlighting the superiority of WorseNet.},
  archive      = {J_ML},
  author       = {Zhang, Fei and Ye, Yunjie and Feng, Lei and Rao, Zhongwen and Zhu, Jieming and Kalander, Marcus and Gong, Chen and Hao, Jianye and Han, Bo},
  doi          = {10.1007/s10994-023-06485-9},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3849-3868},
  shortjournal = {Mach. Learn.},
  title        = {Exploiting counter-examples for active learning with partial labels},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-decoupled interactive embedding network for object
detection. <em>ML</em>, <em>113</em>(6), 3829–3848. (<a
href="https://doi.org/10.1007/s10994-023-06430-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional object detection methods rely on manually annotated data, which can be costly and time-consuming, particularly for objects with low occurrence frequency or those that are neglected in existing datasets. When we need to generalize the model from the training datasets to the target datasets, false positive detection will appear with limited annotations in some categories and the model performance will decrease for unseen categories. In this paper, we found that the problems are related to the model’s overfitting to foreground objects during the training stage and the inadequate robustness of feature representations. In order to effectively improve generalization of deep learning network, we propose a task-decoupled interactive embedding network. We decouple the sub-tasks in the detection pipeline with parallel convolution branches, with gradient propagation independently and anchor boxes generation from coarse to fine. And we introduce an embedding-interactive self-supervised decoder into the detector, so that the weaker object representations can be enhanced, and the representations of the same object can be closely aggregated, providing multi-scale semantic information for detection. Our method achieves great results on two visual tasks: few-shot object detection and open world object detection. It can effectively improve generalization on novel classes without hurting the detection of base classes and have good generalization ability for unknown categories detection. Our code is available at: https://github.com/hommelibrelm/DINet .},
  archive      = {J_ML},
  author       = {Liu, Mai and Jiao, Jichao and Li, Ning and Pang, Min},
  doi          = {10.1007/s10994-023-06430-w},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3829-3848},
  shortjournal = {Mach. Learn.},
  title        = {Task-decoupled interactive embedding network for object detection},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chinese character recognition with radical-structured stroke
trees. <em>ML</em>, <em>113</em>(6), 3807–3827. (<a
href="https://doi.org/10.1007/s10994-023-06450-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flourishing blossom of deep learning has witnessed the rapid development of Chinese character recognition. However, it remains a great challenge that the characters for testing may have different distributions from those of the training dataset. Existing methods based on a single-level representation (character-level, radical-level, or stroke-level) may be either too sensitive to distribution changes (e.g., induced by blurring, occlusion, and zero-shot problems) or too tolerant to one-to-many ambiguities. In this paper, we represent each Chinese character as a stroke tree, which is organized according to its radical structures, to fully exploit the merits of both radical and stroke levels in a decent way. We propose a two-stage decomposition framework, where a Feature-to-Radical Decoder decomposes each character into a radical sequence and a Radical-to-Stroke Decoder further decomposes each radical into the corresponding stroke sequence. The generated radical and stroke sequences are encoded as a radical-structured stroke tree (RSST), which is fed into a Tree-to-Character Translator based on the proposed Weighted Edit Distance to match the closest candidate character in the RSST lexicon. We have conducted extensive experiments on various datasets, such as handwritten, printed artistic, scene character datasets. The experimental results demonstrate that the proposed method outperforms the state-of-the-art single-level methods by increasing margins as the distribution difference becomes more severe in the blurring, occlusion, and zero-shot scenarios. For example, compared with the previous SOTA method, our method improve performance by 1.74–7.58% in the handwritten character zero-shot settings.},
  archive      = {J_ML},
  author       = {Yu, Haiyang and Chen, Jingye and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-023-06450-6},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3807-3827},
  shortjournal = {Mach. Learn.},
  title        = {Chinese character recognition with radical-structured stroke trees},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-aware image understanding with multi-level visual
representation enhancement for visual question answering. <em>ML</em>,
<em>113</em>(6), 3789–3805. (<a
href="https://doi.org/10.1007/s10994-023-06426-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing visual question answering (VQA) methods tend to focus excessively on visual objects in images, neglecting the understanding of implicit knowledge within the images, thus limiting the comprehension of image content. Furthermore, current mainstream VQA methods employ a bottom-up attention mechanism, which was initially proposed in 2017 and has become a bottleneck in visual question answering. In order to address the aforementioned issues and improve the ability to understand images, we have made the following improvements and innovations: (1) We utilize an OCR model to detect and extract scene text in the images, further enriching the understanding of image content. And we introduce the descriptive information from the images to enhance the model’s comprehension of the images. (2) We have made improvements to the bottom-up attention model by obtaining two region features from the images, we concatenate the two region features to form the final visual feature, which better represents the image. (3) We design an extensible deep co-attention model, which includes self-attention units and co-attention units. This model can incorporate both image description information and scene text into the model, and it can be extended with other knowledge to further enhance the model’s reasoning ability. (4) Experimental results demonstrate that our best single model achieves an overall accuracy of 74.38% on the VQA 2.0 test set. To the best of our knowledge, without using external datasets for pretraining, our model has reached a state-of-the-art level.},
  archive      = {J_ML},
  author       = {Yan, Feng and Li, Zhe and Silamu, Wushour and Li, Yanbing},
  doi          = {10.1007/s10994-023-06426-6},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3789-3805},
  shortjournal = {Mach. Learn.},
  title        = {Knowledge-aware image understanding with multi-level visual representation enhancement for visual question answering},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A neural meta model for predicting winter wheat crop yield.
<em>ML</em>, <em>113</em>(6), 3771–3788. (<a
href="https://doi.org/10.1007/s10994-023-06455-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents the development and evaluation of machine learning models to predict winter wheat crop yield using heterogeneous soil and weather data sets. A concept of an error stabilisation stopping mechanism is introduced in an LSTM model specifically designed for heterogeneous datasets. The comparative analysis of this model against an LSTM model highlighted its superior predictive performance. Furthermore, weighted regression models were developed to capture environmental factors using agroclimatic indices. Finally, a neural meta model was built by combining the predictions of several individual models. The experimental results indicated that a neural meta model with an MAE of 0.82 and RMSE of 0.983 tons/hectare demonstrated a notable performance, highlighting the importance of incorporating weighted regression models based on agroclimatic indices. This study shows the potential for improved yield prediction through the proposed model and the subsequent development of a meta model.},
  archive      = {J_ML},
  author       = {Bansal, Yogesh and Lillis, David and Kechadi, M.-Tahar},
  doi          = {10.1007/s10994-023-06455-1},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3771-3788},
  shortjournal = {Mach. Learn.},
  title        = {A neural meta model for predicting winter wheat crop yield},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding imbalanced data: XAI &amp; interpretable ML
framework. <em>ML</em>, <em>113</em>(6), 3751–3769. (<a
href="https://doi.org/10.1007/s10994-023-06414-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a gap between current methods that explain deep learning models that work on imbalanced image data and the needs of the imbalanced learning community. Existing methods that explain imbalanced data are geared toward binary classification, single layer machine learning models and low dimensional data. Current eXplainable Artificial Intelligence (XAI) techniques for vision data mainly focus on mapping predictions of specific instances to inputs, instead of examining global data properties and complexities of entire classes. Therefore, there is a need for a framework that is tailored to modern deep networks, that incorporates large, high dimensional, multi-class datasets, and uncovers data complexities commonly found in imbalanced data. We propose a set of techniques that can be used by both deep learning model users to identify, visualize and understand class prototypes, sub-concepts and outlier instances; and by imbalanced learning algorithm developers to detect features and class exemplars that are key to model performance. The components of our framework can be applied sequentially in their entirety or individually, making it fully flexible to the user’s specific needs ( https://github.com/dd1github/XAI_for_Imbalanced_Learning ).},
  archive      = {J_ML},
  author       = {Dablain, Damien and Bellinger, Colin and Krawczyk, Bartosz and Aha, David W. and Chawla, Nitesh},
  doi          = {10.1007/s10994-023-06414-w},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3751-3769},
  shortjournal = {Mach. Learn.},
  title        = {Understanding imbalanced data: XAI &amp; interpretable ML framework},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian tensor factorisations for time series of counts.
<em>ML</em>, <em>113</em>(6), 3731–3750. (<a
href="https://doi.org/10.1007/s10994-023-06441-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a flexible nonparametric Bayesian modelling framework for multivariate time series of count data based on tensor factorisations. Our models can be viewed as infinite state space Markov chains of known maximal order with non-linear serial dependence through the introduction of appropriate latent variables. Alternatively, our models can be viewed as Bayesian hierarchical models with conditionally independent Poisson distributed observations. Inference about the important lags and their complex interactions is achieved via MCMC. When the observed counts are large, we deal with the resulting computational complexity of Bayesian inference via a two-step inferential strategy based on an initial analysis of a training set of the data. Our methodology is illustrated using simulation experiments and analysis of real-world data.},
  archive      = {J_ML},
  author       = {Wang, Zhongzhen and Dellaportas, Petros and Kosmidis, Ioannis},
  doi          = {10.1007/s10994-023-06441-7},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3731-3750},
  shortjournal = {Mach. Learn.},
  title        = {Bayesian tensor factorisations for time series of counts},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Sanitized clustering against confounding bias. <em>ML</em>,
<em>113</em>(6), 3711–3730. (<a
href="https://doi.org/10.1007/s10994-023-06451-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets inevitably contain biases that arise from different sources or conditions during data collection. Consequently, such inconsistency itself acts as a confounding factor that disturbs the cluster analysis. Existing methods eliminate the biases by projecting data onto the orthogonal complement of the subspace expanded by the confounding factor before clustering. Therein, the interested clustering factor and the confounding factor are coarsely considered in the raw feature space, where the correlation between the data and the confounding factor is ideally assumed to be linear for convenient solutions. These approaches are thus limited in scope as the data in real applications is usually complex and non-linearly correlated with the confounding factor. This paper presents a new clustering framework named Sanitized Clustering Against confounding Bias, which removes the confounding factor in the semantic latent space of complex data through a non-linear dependence measure. To be specific, we eliminate the bias information in the latent space by minimizing the mutual information between the confounding factor and the latent representation delivered by variational auto-encoder. Meanwhile, a clustering module is introduced to cluster over the purified latent representations. Extensive experiments on complex datasets demonstrate that our SCAB achieves a significant gain in clustering performance by removing the confounding bias.},
  archive      = {J_ML},
  author       = {Yao, Yinghua and Pan, Yuangang and Li, Jing and Tsang, Ivor W. and Yao, Xin},
  doi          = {10.1007/s10994-023-06451-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3711-3730},
  shortjournal = {Mach. Learn.},
  title        = {Sanitized clustering against confounding bias},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network structure simplification by assessing
evolution in node weight magnitude. <em>ML</em>, <em>113</em>(6),
3693–3710. (<a
href="https://doi.org/10.1007/s10994-023-06438-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of artificial intelligence models has given rise to extensive work toward understanding the inner workings of neural networks. Much of that work, however, has focused on manipulating input data feeding the network to assess their effects on network output or pruning model components after the often-extensive time-consuming training. It is shown in this study that model simplification can benefit from investigating the network node, the most fundamental unit of neural networks, during training. Whereas studies on simplification of model structure have mostly required repeated model training, assessing evolving trends in node weights toward model stabilization may circumvent that requirement. Node magnitude stability, defined as the number of epochs where node weights retained their magnitude within a tolerance value, was the central construct in this study. To test evolving trends, a manipulated, a contrived, and two life science data sets were used. Data sets were run on convolutional and deep neural network models. Findings indicated that neural network progress toward stability differed by model, where CNNs tended to add influential nodes early during training. The magnitude stability approach of this study showed superior time efficiencies, which may assist in XAI research toward producing more transparent models and clear outcomes to technical and non-technical audiences.},
  archive      = {J_ML},
  author       = {Riedel, Ralf and Segev, Aviv},
  doi          = {10.1007/s10994-023-06438-2},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3693-3710},
  shortjournal = {Mach. Learn.},
  title        = {Neural network structure simplification by assessing evolution in node weight magnitude},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid acceleration techniques for the physics-informed
neural networks: A comparative analysis. <em>ML</em>, <em>113</em>(6),
3675–3692. (<a
href="https://doi.org/10.1007/s10994-023-06442-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-informed neural networks (PINN) has emerged as a promising approach for solving partial differential equations (PDEs). However, the training process for PINN can be computationally expensive, limiting its practical applications. To address this issue, we investigate several acceleration techniques for PINN that combine Fourier neural operators, separable PINN, and first-order PINN. We also propose novel acceleration techniques based on second-order PINN and Koopman neural operators. We evaluate the efficiency of these techniques on various PDEs, and our results show that the hybrid models can provide much more accurate results than classical PINN under time constraints for the training, making PINN a more viable option for practical applications. The proposed methodology in the manuscript is generic and can be extended on a larger set of problems including inverse problems.},
  archive      = {J_ML},
  author       = {Buzaev, Fedor and Gao, Jiexing and Chuprov, Ivan and Kazakov, Evgeniy},
  doi          = {10.1007/s10994-023-06442-6},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3675-3692},
  shortjournal = {Mach. Learn.},
  title        = {Hybrid acceleration techniques for the physics-informed neural networks: A comparative analysis},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tracking treatment effect heterogeneity in evolving
environments. <em>ML</em>, <em>113</em>(6), 3653–3673. (<a
href="https://doi.org/10.1007/s10994-023-06421-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous treatment effect (HTE) estimation plays a crucial role in developing personalized treatment plans across various applications. Conventional approaches assume that the observed data are independent and identically distributed (i.i.d.). In some real applications, however, the assumption does not hold: the environment may evolve, which leads to variations in HTE over time. To enable HTE estimation in evolving environments, we introduce and formulate the online HTE estimation problem. We propose an online ensemble-based HTE estimation method called ETHOS, which is capable of adapting to unknown evolving environments by ensembling the outputs of multiple base estimators that track environmental changes at different scales. Theoretical analysis reveals that ETHOS achieves an optimal expected dynamic regret $$O(\sqrt{T(1+P_T)})$$ , where T denotes the number of observed examples and $$P_T$$ characterizes the intensity of environment changes. The achieved dynamic regret ensures that our method consistently approaches the optimal online estimators as long as the evolution of the environment is moderate. We conducted extensive experiments on three common benchmark datasets with various environment evolving mechanisms. The results validate the theoretical analysis and the effectiveness of our proposed method.},
  archive      = {J_ML},
  author       = {Qin, Tian and Li, Long-Fei and Wang, Tian-Zuo and Zhou, Zhi-Hua},
  doi          = {10.1007/s10994-023-06421-x},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3653-3673},
  shortjournal = {Mach. Learn.},
  title        = {Tracking treatment effect heterogeneity in evolving environments},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining neural networks without access to training data.
<em>ML</em>, <em>113</em>(6), 3633–3652. (<a
href="https://doi.org/10.1007/s10994-023-06428-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider generating explanations for neural networks in cases where the network’s training data is not accessible, for instance due to privacy or safety issues. Recently, Interpretation Nets ( $$\mathcal {I}$$ -Nets) have been proposed as a sample-free approach to post-hoc, global model interpretability that does not require access to training data. They formulate interpretation as a machine learning task that maps network representations (parameters) to a representation of an interpretable function. In this paper, we extend the $$\mathcal {I}$$ -Net framework to the cases of standard and soft decision trees as surrogate models. We propose a suitable decision tree representation and design of the corresponding $$\mathcal {I}$$ -Net output layers. Furthermore, we make $$\mathcal {I}$$ -Nets applicable to real-world tasks by considering more realistic distributions when generating the $$\mathcal {I}$$ -Net’s training data. We empirically evaluate our approach against traditional global, post-hoc interpretability approaches and show that it achieves superior results when the training data is not accessible.},
  archive      = {J_ML},
  author       = {Marton, Sascha and Lüdtke, Stefan and Bartelt, Christian and Tschalzev, Andrej and Stuckenschmidt, Heiner},
  doi          = {10.1007/s10994-023-06428-4},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3633-3652},
  shortjournal = {Mach. Learn.},
  title        = {Explaining neural networks without access to training data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation, augmentation, and alignment: A pseudo-source
domain based method for source-free domain adaptation. <em>ML</em>,
<em>113</em>(6), 3611–3631. (<a
href="https://doi.org/10.1007/s10994-023-06432-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to train a well-performed model in the target domain given both a trained source model and unlabeled target samples. Although achieving remarkable progress, existing SFDA methods do not explicitly reduce the distribution shift across domains, which is the key to a good adaptation. However, the absence of source samples makes it difficult to estimate and reduce domain discrepancy. Although there are no source samples available, fortunately, we find that some target samples can be used to approximate the source domain, which is denoted as the pseudo-source domain for approximatively estimating domain discrepancy. In this paper, inspired by this observation, we propose a novel method based on the pseudo-source domain to explicitly reduce the domain discrepancy even without source samples. The proposed method generates and augments the pseudo-source domain, and then employs distribution alignment with four novel losses based on pseudo-label based strategy. Thus, the domain shift can be reduced. The extensive results on three real-world datasets verify the effectiveness of the proposed method. The source code is available at https://github.com/yuntaodu/PS_code .},
  archive      = {J_ML},
  author       = {Du, Yuntao and Yang, Haiyang and Chen, Mingcai and Luo, Hongtao and Jiang, Juan and Xin, Yi and Wang, Chongjun},
  doi          = {10.1007/s10994-023-06432-8},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3611-3631},
  shortjournal = {Mach. Learn.},
  title        = {Generation, augmentation, and alignment: A pseudo-source domain based method for source-free domain adaptation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nrat: Towards adversarial training with inherent label
noise. <em>ML</em>, <em>113</em>(6), 3589–3610. (<a
href="https://doi.org/10.1007/s10994-023-06437-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) has been widely recognized as the most effective defense approach against adversarial attacks on deep neural networks and it is formulated as a min-max optimization. Most AT algorithms are geared towards research-oriented datasets such as MNIST, CIFAR10, etc., where the labels are generally correct. However, noisy labels, e.g., mislabelling, are inevitable in real-world datasets. In this paper, we investigate AT with inherent label noise, where the training dataset itself contains mislabeled samples. We first empirically show that the performance of AT typically degrades as the label noise rate increases. Then, we propose a Noisy-Robust Adversarial Training (NRAT) algorithm, which leverages the recent advancements in learning with noisy labels to enhance the performance of AT in the presence of label noise. For experimental comparison, we consider two essential metrics in AT: (i) trade-off between natural and robust accuracy; (ii) robust overfitting. Our experiments show that NRAT’s performance is on par with, or better than, the state-of-the-art AT methods on both evaluation metrics. Our code is publicly available at: https://github.com/TrustAI/NRAT .},
  archive      = {J_ML},
  author       = {Chen, Zhen and Wang, Fu and Mu, Ronghui and Xu, Peipei and Huang, Xiaowei and Ruan, Wenjie},
  doi          = {10.1007/s10994-023-06437-3},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3589-3610},
  shortjournal = {Mach. Learn.},
  title        = {Nrat: Towards adversarial training with inherent label noise},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Better schedules for low precision training of deep neural
networks. <em>ML</em>, <em>113</em>(6), 3569–3587. (<a
href="https://doi.org/10.1007/s10994-023-06480-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout training according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine schedules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency and model performance, as well as derive a set of best practices for choosing CPT schedules. Going further, we find that a correlation exists between model performance and training cost, and that changing the underlying CPT schedule can control the tradeoff between these two variables. To explain the direct correlation between model performance and training cost, we draw a connection between quantized training and critical learning periods, suggesting that aggressive quantization is a form of learning impairment that can permanently damage model performance.},
  archive      = {J_ML},
  author       = {Wolfe, Cameron R. and Kyrillidis, Anastasios},
  doi          = {10.1007/s10994-023-06480-0},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3569-3587},
  shortjournal = {Mach. Learn.},
  title        = {Better schedules for low precision training of deep neural networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Margin distribution and structural diversity guided ensemble
pruning. <em>ML</em>, <em>113</em>(6), 3545–3567. (<a
href="https://doi.org/10.1007/s10994-023-06429-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble methods that train and combine multiple learners have always been among the state-of-the-art learning methods, and ensemble pruning aims at generating a smaller-sized ensemble with even better generalization performance. Abundant ensemble pruning methods that use evaluation criteria such as diversity or margin together with validation error have been proposed. However, as these evaluation criteria are used together with the validation error, their effect on generalization performance is less clear. In this paper, we propose a margin distribution and structural diversity guided ensemble pruning framework, called Decoupled Ensemble Pruning (DEP). It decouples the optimization of margin distribution and structural diversity and the optimization of validation error into two stages. Our information-theoretic analysis reveals that the expected generalization gap is related to the combination distribution, i.e., validation error distribution of all the combinations of base learners. And show that optimizing margin mean and structural diversity benefits combination distribution. Concretely, we provide an instantiation of DEP framework in the classic tree-based ensemble pruning setting. Experimental results not only verify the effectiveness in optimizing the distribution, but also show that DEP enjoys better test accuracy than existing ensemble pruning methods.},
  archive      = {J_ML},
  author       = {He, Yi-Xiao and Wu, Yu-Chang and Qian, Chao and Zhou, Zhi-Hua},
  doi          = {10.1007/s10994-023-06429-3},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3545-3567},
  shortjournal = {Mach. Learn.},
  title        = {Margin distribution and structural diversity guided ensemble pruning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable variable selection for two-view learning tasks with
projection operators. <em>ML</em>, <em>113</em>(6), 3525–3544. (<a
href="https://doi.org/10.1007/s10994-023-06433-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features.},
  archive      = {J_ML},
  author       = {Szedmak, Sandor and Huusari, Riikka and Duong Le, Tat Hong and Rousu, Juho},
  doi          = {10.1007/s10994-023-06433-7},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3525-3544},
  shortjournal = {Mach. Learn.},
  title        = {Scalable variable selection for two-view learning tasks with projection operators},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional scene modeling with global object-centric
representations. <em>ML</em>, <em>113</em>(6), 3505–3524. (<a
href="https://doi.org/10.1007/s10994-023-06419-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of the same object may vary in different scene images due to occlusions between objects. Humans can quickly identify the same object, even if occlusions exist, by completing the occluded parts based on its complete canonical image in the memory. Achieving this ability is still challenging for existing models, especially in the unsupervised learning setting. Inspired by such an ability of humans, we propose a novel object-centric representation learning method to identify the same object in different scenes that may be occluded by learning global object-centric representations of complete canonical objects without supervision. The representation of each object is divided into an extrinsic part, which characterizes scene-dependent information (i.e., position and size), and an intrinsic part, which characterizes globally invariant information (i.e., appearance and shape). The former can be inferred with an improved IC-SBP module. The latter is extracted by combining rectangular and arbitrary-shaped attention and is used to infer the identity representation via a proposed patch-matching strategy with a set of learnable global object-centric representations of complete canonical objects. In the experiment, three 2D scene datasets are used to verify the proposed method’s ability to recognize the identity of the same object in different scenes. A complex 3D scene dataset and a real-world dataset are used to evaluate the performance of scene decomposition. Our experimental results demonstrate that the proposed method outperforms the comparison methods in terms of same object recognition and scene decomposition.},
  archive      = {J_ML},
  author       = {Chen, Tonglin and Shen, Zhimeng and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-023-06419-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3505-3524},
  shortjournal = {Mach. Learn.},
  title        = {Compositional scene modeling with global object-centric representations},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Style spectroscope: Improve interpretability and
controllability through fourier analysis. <em>ML</em>, <em>113</em>(6),
3485–3503. (<a
href="https://doi.org/10.1007/s10994-023-06435-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal style transfer (UST) infuses styles from arbitrary reference images into content images. Existing methods, while enjoying many practical successes, are unable of explaining experimental observations, including different performances of UST algorithms in preserving the spatial structure of content images. In addition, methods are limited to cumbersome global controls on stylization, so that they require additional spatial masks for desired stylization. In this work, we first provide a systematic Fourier analysis on a general framework for UST. We present an equivalent form of the framework in the frequency domain. The form implies that existing algorithms treat all frequency components and pixels of feature maps equally, except for the zero-frequency component. We connect Fourier amplitude and phase with a widely used style loss and a well-known content reconstruction loss in style transfer, respectively. Based on such equivalence and connections, we can thus interpret different structure preservation behaviors between algorithms with Fourier phase. Given the interpretations, we propose two plug-and-play manipulations upon style transfer methods for better structure preservation and desired stylization. Both qualitative and quantitative experiments demonstrate the improved performance of our manipulations upon mainstreaming methods without any additional training. Specifically, the metrics are improved by 6% in average on the content images from MS-COCO dataset and the style images from WikiArt dataset. We also conduct experiments to demonstrate (1) the abovementioned equivalence, (2) the interpretability based on Fourier amplitude and phase and (3) the controllability associated with frequency components.},
  archive      = {J_ML},
  author       = {Jin, Zhiyu and Shen, Xuli and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-023-06435-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3485-3503},
  shortjournal = {Mach. Learn.},
  title        = {Style spectroscope: Improve interpretability and controllability through fourier analysis},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online binary classification from similar and dissimilar
data. <em>ML</em>, <em>113</em>(6), 3463–3484. (<a
href="https://doi.org/10.1007/s10994-023-06434-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar-dissimilar (SD) classification aims to train a binary classifier from only similar and dissimilar data pairs, which indicate whether two instances belong to the same class (similar) or not (dissimilar). Although effective learning methods have been proposed for SD classification, they cannot deal with online learning scenarios with sequential data that can be frequently encountered in real-world applications. In this paper, we provide the first attempt to investigate the online SD classification problem. Specifically, we first adapt the unbiased risk estimator of SD classification to online learning scenarios with a conservative regularization term, which could serve as a naive method to solve the online SD classification problem. Then, by further introducing a margin criterion for whether to update the classifier or not with the received cost, we propose two improvements (one with linearly scaled cost and the other with quadratically scaled cost) that result in two online SD classification methods. Theoretically, we derive the regret, mistake, and relative loss bounds for our proposed methods, which guarantee the performance on sequential data. Extensive experiments on various datasets validate the effectiveness of our proposed methods.},
  archive      = {J_ML},
  author       = {Shu, Senlin and Wang, Haobo and Wang, Zhuowei and Han, Bo and Xiang, Tao and An, Bo and Feng, Lei},
  doi          = {10.1007/s10994-023-06434-6},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3463-3484},
  shortjournal = {Mach. Learn.},
  title        = {Online binary classification from similar and dissimilar data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label image classification with multi-layered
multi-perspective dynamic semantic representation. <em>ML</em>,
<em>113</em>(6), 3443–3461. (<a
href="https://doi.org/10.1007/s10994-023-06440-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning techniques, multi-label image classification tasks have achieved good performance. Recently, graph convolutional network has been proved to be an effective way to explore the labels dependencies. However, due to the complexity of label semantic relations, the static dependencies obtained by existing methods cannot consider the overall characteristics of an image and accurately locate the target region. Therefore, we propose the Multi-layered Multi-perspective Dynamic Semantic Representation (MMDSR) for multi-label image classification, which mainly includes three important modules: (1) multi-scale feature reconstruction, which aggregates complementary information at different levels in convolutional neural network through cross-layer attention, it can effectively identify target categories of different sizes; (2) channel dual-branch cross-attention module, we propose to explore the correlation between global information and local features in multi-scale features by the way of adaptive cross-fusion to locate the target area more accurately; (3) dynamic semantic representation module, we design the multi-perspective weighted cosine measure to construct content-based label dependencies for each image to dynamically construct a semantic relationship graph. Extensive experiments on the two datasets MS-COCO and VOC2007 have verified that the classification performance of our proposed MMDSR is better than many state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Kuang, Wenlan and Li, Zhixin},
  doi          = {10.1007/s10994-023-06440-8},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3443-3461},
  shortjournal = {Mach. Learn.},
  title        = {Multi-label image classification with multi-layered multi-perspective dynamic semantic representation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applied machine learning to the determination of biochar
hydrogen sulfide adsorption capacity. <em>ML</em>, <em>113</em>(6),
3419–3441. (<a
href="https://doi.org/10.1007/s10994-023-06446-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biogas desulfurization using biochar is complex and highly nonlinear, affected by various variables and their interactions. Moreover, achieving maximum adsorption capacity and investigating the simultaneous effects of different variables on the efficiency of the adsorption process is challenging. In this study, machine learning algorithms were successfully applied to predict the biochar hydrogen sulfide adsorption capacity in biogas purification. Three supervised machine learning models were devised and evaluated in three-step model development to determine biochars&#39; hydrogen sulfide adsorption capacity. In each model, a feature selection procedure was used in combination with feature important analysis to extract the most influential parameters on the hydrogen sulfide adsorption capacity and improve the total accuracy of models. The exhaustive feature selection method was used to find the best subset of features in each machine learning algorithm. The models used twenty features as input variables and were trained to learn complex relationships between these variables and the target variable. Based on features important and Shapley Additive Explanation analysis, the biochar surface&#39;s pH and the feedstock H/C molar ratio were among the most influential parameters in the adsorption process. The gradient boosting regression model was the most accurate prediction model reaching R2 scores of 0.998, 0.91, and 0.81 in the training, testing, and fivefold cross-validation sets, respectively. Overall, the study demonstrates the significance of machine learning in predicting and optimizing the biochar Hydrogen Sulfide adsorption process, which can be an asset in selecting appropriate biochar for removing hydrogen sulfide from biogas streams.},
  archive      = {J_ML},
  author       = {Banisheikholeslami, Abolhassan and Qaderi, Farhad},
  doi          = {10.1007/s10994-023-06446-2},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3419-3441},
  shortjournal = {Mach. Learn.},
  title        = {Applied machine learning to the determination of biochar hydrogen sulfide adsorption capacity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical u-net with re-parameterization technique for
spatio-temporal weather forecasting. <em>ML</em>, <em>113</em>(6),
3399–3417. (<a
href="https://doi.org/10.1007/s10994-023-06445-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the considerable computational demands of physics-based numerical weather prediction, especially when modeling fine-grained spatio-temporal atmospheric phenomena, deep learning methods offer an advantageous approach by leveraging specialized computing devices to accelerate training and significantly reduce computational costs. Consequently, the application of deep learning methods has presented a novel solution in the field of weather forecasting. In this context, we introduce a groundbreaking deep learning-based weather prediction architecture known as Hierarchical U-Net (HU-Net) with re-parameterization techniques. The HU-Net comprises two essential components: a feature extraction module and a U-Net module with re-parameterization techniques. The feature extraction module consists of two branches. First, the global pattern extraction employs adaptive Fourier neural operators and self-attention, well-known for capturing long-term dependencies in the data. Second, the local pattern extraction utilizes convolution operations as fundamental building blocks, highly proficient in modeling local correlations. Moreover, a feature fusion block dynamically combines dual-scale information. The U-Net module adopts RepBlock with re-parameterization techniques as the fundamental building block, enabling efficient and rapid inference. In extensive experiments carried out on the large-scale weather benchmark dataset WeatherBench at a resolution of 1.40625 $$^\circ $$ , the results demonstrate that our proposed HU-Net outperforms other baseline models in both prediction accuracy and inference time.},
  archive      = {J_ML},
  author       = {Xu, Baowen and Wang, Xuelei and Li, Jingwei and Liu, Chengbao},
  doi          = {10.1007/s10994-023-06445-3},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3399-3417},
  shortjournal = {Mach. Learn.},
  title        = {Hierarchical U-net with re-parameterization technique for spatio-temporal weather forecasting},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning de-biased regression trees and forests from complex
samples. <em>ML</em>, <em>113</em>(6), 3379–3398. (<a
href="https://doi.org/10.1007/s10994-023-06439-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression trees and forests are widely used due to their flexibility and predictive accuracy. Whereas typical tree induction assumes independently identically distributed (i.i.d.) data, in many applications the training sample follows a complex sampling structure. This includes unequal probability sampling, which is often found in survey data. Then, a ‘naive estimation’ that simply ignores the sampling weights may be substantially biased. This article analyzes the bias arising from a naive estimation of regression trees or forests under complex sample designs and proposes ways of de-biasing. This is achieved by bridging tree learning to survey statistics, due to the correspondence of the mean-squared-error criterion in regression trees and variance estimation. Transferring population variance estimation approaches from survey statistics to tree induction, indeed considerably reduces the bias in the resulting trees, both in predictions and the tree structure. The latter is particularly crucial if the trees are to be interpreted. Our methodology is extended to random forests, where we show on simulated data and a housing dataset that correcting for complex sample designs leads to overall much better predictive accuracy and more trustworthy interpretation. Interestingly, corrected forests can surpass forests learned on i.i.d. samples in terms of accuracy, which also has important implications for adaptive data collection approaches.},
  archive      = {J_ML},
  author       = {Nalenz, Malte and Rodemann, Julian and Augustin, Thomas},
  doi          = {10.1007/s10994-023-06439-1},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3379-3398},
  shortjournal = {Mach. Learn.},
  title        = {Learning de-biased regression trees and forests from complex samples},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reduced implication-bias logic loss for neuro-symbolic
learning. <em>ML</em>, <em>113</em>(6), 3357–3377. (<a
href="https://doi.org/10.1007/s10994-023-06436-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in the field of Neuro-Symbolic Learning. However, some differentiable operators could introduce significant biases during backpropagation, which can degrade the performance of Neuro-Symbolic systems. In this paper, we demonstrate that the loss functions derived from fuzzy logic operators commonly exhibit a bias, referred to as Implication Bias. To mitigate this bias, we propose a simple yet efficient method to transform the biased loss functions into Reduced Implication-bias Logic Loss (RILL). Empirical studies demonstrate that RILL outperforms the biased logic loss functions, especially when the knowledge base is incomplete or the supervised training data is insufficient.},
  archive      = {J_ML},
  author       = {He, Hao-Yuan and Dai, Wang-Zhou and Li, Ming},
  doi          = {10.1007/s10994-023-06436-4},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {3357-3377},
  shortjournal = {Mach. Learn.},
  title        = {Reduced implication-bias logic loss for neuro-symbolic learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Multi-agent reinforcement learning for
fast-timescale demand response of residential loads. <em>ML</em>,
<em>113</em>(5), 3355. (<a
href="https://doi.org/10.1007/s10994-024-06514-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Mai, Vincent and Maisonneuve, Philippe and Zhang, Tianyu and Nekoei, Hadi and Paull, Liam and Lesage-Landry, Antoine},
  doi          = {10.1007/s10994-024-06514-1},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3355},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Multi-agent reinforcement learning for fast-timescale demand response of residential loads},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction to: A neural meta-model for predicting winter
wheat crop yield. <em>ML</em>, <em>113</em>(5), 3353. (<a
href="https://doi.org/10.1007/s10994-024-06535-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Bansal, Yogesh and Lillis, David and Kechadi, M.-Tahar},
  doi          = {10.1007/s10994-024-06535-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3353},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: A neural meta-model for predicting winter wheat crop yield},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient boosted trees for evolving data streams.
<em>ML</em>, <em>113</em>(5), 3325–3352. (<a
href="https://doi.org/10.1007/s10994-024-06517-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient Boosting is a widely-used machine learning technique that has proven highly effective in batch learning. However, its effectiveness in stream learning contexts lags behind bagging-based ensemble methods, which currently dominate the field. One reason for this discrepancy is the challenge of adapting the booster to new concept following a concept drift. Resetting the entire booster can lead to significant performance degradation as it struggles to learn the new concept. Resetting only some parts of the booster can be more effective, but identifying which parts to reset is difficult, given that each boosting step builds on the previous prediction. To overcome these difficulties, we propose Streaming Gradient Boosted Trees (Sgbt), which is trained using weighted squared loss elicited in XGBoost. Sgbt exploits trees with a replacement strategy to detect and recover from drifts, thus enabling the ensemble to adapt without sacrificing the predictive performance. Our empirical evaluation of Sgbt on a range of streaming datasets with challenging drift scenarios demonstrates that it outperforms current state-of-the-art methods for evolving data streams.},
  archive      = {J_ML},
  author       = {Gunasekara, Nuwan and Pfahringer, Bernhard and Gomes, Heitor and Bifet, Albert},
  doi          = {10.1007/s10994-024-06517-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3325-3352},
  shortjournal = {Mach. Learn.},
  title        = {Gradient boosted trees for evolving data streams},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair tree classifier using strong demographic parity.
<em>ML</em>, <em>113</em>(5), 3305–3324. (<a
href="https://doi.org/10.1007/s10994-023-06376-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dealing with sensitive data in automated data-driven decision-making, an important concern is to learn predictors with high performance towards a class label, whilst minimising for the discrimination towards any sensitive attribute, like gender or race, induced from biased data. Hybrid tree optimisation criteria have been proposed which combine classification performance and fairness. Although the threshold-free ROC-AUC is the standard for measuring classification model performance, current fair tree classification methods mainly optimise for a fixed threshold on the fairness metric. In this paper, we propose SCAFF—splitting criterion AUC for Fairness—a compound decision tree splitting criterion which combines the threshold-free strong demographic parity with ROC-AUC termed, easily applicable as an ensemble. Our method simultaneously leverages multiple sensitive attributes of which the values may be multicategorical, and is tunable with respect to the unavoidable performance-fairness trade-off. In our experiments, we demonstrate how SCAFF generates effective models with competitive performance and fairness with respect to binary, multicategorical, and multiple sensitive attributes.},
  archive      = {J_ML},
  author       = {Pereira Barata, António and Takes, Frank W. and van den Herik, H. Jaap and Veenman, Cor J.},
  doi          = {10.1007/s10994-023-06376-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3305-3324},
  shortjournal = {Mach. Learn.},
  title        = {Fair tree classifier using strong demographic parity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: Adversarial concept drift detection under
poisoning attacks for robust data stream mining. <em>ML</em>,
<em>113</em>(5), 3303–3304. (<a
href="https://doi.org/10.1007/s10994-023-06459-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Korycki, Łukasz and Krawczyk, Bartosz},
  doi          = {10.1007/s10994-023-06459-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3303-3304},
  shortjournal = {Mach. Learn.},
  title        = {Correction: Adversarial concept drift detection under poisoning attacks for robust data stream mining},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning (k,l)-context-sensitive probabilistic grammars with
nonparametric bayesian approach. <em>ML</em>, <em>113</em>(5),
3267–3301. (<a
href="https://doi.org/10.1007/s10994-021-06034-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring formal grammars with nonparametric Bayesian approach is one of the most powerful approach for achieving high accuracy from unsupervised data. In this paper, mildly-context-sensitive probabilities, called (k, l)-context-sensitive probabilities, are defined on context-free grammars (CFGs). Inferring CFGs where the probabilities of rules are identified from contexts can be seen as a kind of dual approaches for distributional learning, in which the contexts characterize the substrings. We can handle the data sparsity for the context-sensitive probabilities by the smoothing effect of the hierarchical nonparametric Bayesian models such as Pitman–Yor processes (PYPs). We define the hierarchy of PYPs naturally by augmenting the infinite PCFGs. The blocked Gibbs sampling is known to be effective for inferring PCFGs. We show that, by modifying the inside probabilities, the blocked Gibbs sampling is able to be applied to the (k, l)-context-sensitive probabilistic grammars. At the same time, we show that the time complexity for (k, l)-context-sensitive probabilities of a CFG is $$O(|V|^{l+3}|w|^3)$$ for each sentence w, where V is a set of nonterminals. Since it is computationally too expensive to iterate sufficient times especially when |V| is not small, some alternative sampling algorithms are required. Therefore, we propose a new sampling method called composite sampling, with which the sampling procedure is separated into sub-procedures for nonterminals and for derivation trees. Finally, we demonstrate that the inferred (k, 0)-context-sensitive probabilistic grammars can achieve lower perplexities than other probabilistic language models such as PCFGs, n-grams, and HMMs.},
  archive      = {J_ML},
  author       = {Shibata, Chihiro},
  doi          = {10.1007/s10994-021-06034-2},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3267-3301},
  shortjournal = {Mach. Learn.},
  title        = {Learning (k,l)-context-sensitive probabilistic grammars with nonparametric bayesian approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distillation of weighted automata from recurrent neural
networks using a spectral approach. <em>ML</em>, <em>113</em>(5),
3233–3266. (<a
href="https://doi.org/10.1007/s10994-021-05948-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is an attempt to bridge the gap between deep learning and grammatical inference. Indeed, it provides an algorithm to extract a (stochastic) formal language from any recurrent neural network trained for language modelling. In detail, the algorithm uses the already trained network as an oracle—and thus does not require the access to the inner representation of the black-box—and applies a spectral approach to infer a weighted automaton. As weighted automata compute linear functions, they are computationally more efficient than neural networks and thus the nature of the approach is the one of knowledge distillation. We detail experiments on 62 data sets (both synthetic and from real-world applications) that allow an in-depth study of the abilities of the proposed algorithm. The results show the WA we extract are good approximations of the RNN, validating the approach. Moreover, we show how the process provides interesting insights toward the behavior of RNN learned on data, enlarging the scope of this work to the one of explainability of deep learning models.},
  archive      = {J_ML},
  author       = {Eyraud, Rémi and Ayache, Stéphane},
  doi          = {10.1007/s10994-021-05948-1},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3233-3266},
  shortjournal = {Mach. Learn.},
  title        = {Distillation of weighted automata from recurrent neural networks using a spectral approach},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness seen as global sensitivity analysis. <em>ML</em>,
<em>113</em>(5), 3205–3232. (<a
href="https://doi.org/10.1007/s10994-022-06202-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that a predictor is not biased against a sensitive feature is the goal of fair learning. Meanwhile, Global Sensitivity Analysis (GSA) is used in numerous contexts to monitor the influence of any feature on an output variable. We merge these two domains, Global Sensitivity Analysis and Fairness, by showing how fairness can be defined using a special framework based on Global Sensitivity Analysis and how various usual indicators are common between these two fields. We also present new Global Sensitivity Analysis indices, as well as rates of convergence, that are useful as fairness proxies.},
  archive      = {J_ML},
  author       = {Bénesse, Clément and Gamboa, Fabrice and Loubes, Jean-Michel and Boissin, Thibaut},
  doi          = {10.1007/s10994-022-06202-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3205-3232},
  shortjournal = {Mach. Learn.},
  title        = {Fairness seen as global sensitivity analysis},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein dropout. <em>ML</em>, <em>113</em>(5),
3161–3204. (<a
href="https://doi.org/10.1007/s10994-022-06230-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite of its importance for safe machine learning, uncertainty quantification for neural networks is far from being solved. State-of-the-art approaches to estimate neural uncertainties are often hybrid, combining parametric models with explicit or implicit (dropout-based) ensembling. We take another pathway and propose a novel approach to uncertainty quantification for regression tasks, Wasserstein dropout, that is purely non-parametric. Technically, it captures aleatoric uncertainty by means of dropout-based sub-network distributions. This is accomplished by a new objective which minimizes the Wasserstein distance between the label distribution and the model distribution. An extensive empirical analysis shows that Wasserstein dropout outperforms state-of-the-art methods, on vanilla test data as well as under distributional shift in terms of producing more accurate and stable uncertainty estimates.},
  archive      = {J_ML},
  author       = {Sicking, Joachim and Akila, Maram and Pintz, Maximilian and Wirtz, Tim and Wrobel, Stefan and Fischer, Asja},
  doi          = {10.1007/s10994-022-06230-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3161-3204},
  shortjournal = {Mach. Learn.},
  title        = {Wasserstein dropout},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LIMEcraft: Handcrafted superpixel selection and inspection
for visual eXplanations. <em>ML</em>, <em>113</em>(5), 3143–3160. (<a
href="https://doi.org/10.1007/s10994-022-06204-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased interest in deep learning applications, and their hard-to-detect biases result in the need to validate and explain complex models. However, current explanation methods are limited as far as both the explanation of the reasoning process and prediction results are concerned. They usually only show the location in the image that was important for model prediction. The lack of possibility to interact with explanations makes it difficult to verify and understand exactly how the model works. This creates a significant risk when using the model. The risk is compounded by the fact that explanations do not take into account the semantic meaning of the explained objects. To escape from the trap of static and meaningless explanations, we propose a tool and a process called LIMEcraft. LIMEcraft enhances the process of explanation by allowing a user to interactively select semantically consistent areas and thoroughly examine the prediction for the image instance in case of many image features. Experiments on several models show that our tool improves model safety by inspecting model fairness for image pieces that may indicate model bias. The code is available at: http://github.com/MI2DataLab/LIMEcraft .},
  archive      = {J_ML},
  author       = {Hryniewska, Weronika and Grudzień, Adrianna and Biecek, Przemysław},
  doi          = {10.1007/s10994-022-06204-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3143-3160},
  shortjournal = {Mach. Learn.},
  title        = {LIMEcraft: Handcrafted superpixel selection and inspection for visual eXplanations},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PreCoF: Counterfactual explanations for fairness.
<em>ML</em>, <em>113</em>(5), 3111–3142. (<a
href="https://doi.org/10.1007/s10994-023-06319-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies how counterfactual explanations can be used to assess the fairness of a model. Using machine learning for high-stakes decisions is a threat to fairness as these models can amplify bias present in the dataset, and there is no consensus on a universal metric to detect this. The appropriate metric and method to tackle the bias in a dataset will be case-dependent, and it requires insight into the nature of the bias first. We aim to provide this insight by integrating explainable AI (XAI) research with the fairness domain. More specifically, apart from being able to use (Predictive) Counterfactual Explanations to detect explicit bias when the model is directly using the sensitive attribute, we show that it can also be used to detect implicit bias when the model does not use the sensitive attribute directly but does use other correlated attributes leading to a substantial disadvantage for a protected group. We call this metric PreCoF, or Predictive Counterfactual Fairness. Our experimental results show that our metric succeeds in detecting occurrences of implicit bias in the model by assessing which attributes are more present in the explanations of the protected group compared to the unprotected group. These results could help policymakers decide on whether this discrimination is justified or not.},
  archive      = {J_ML},
  author       = {Goethals, Sofie and Martens, David and Calders, Toon},
  doi          = {10.1007/s10994-023-06319-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3111-3142},
  shortjournal = {Mach. Learn.},
  title        = {PreCoF: Counterfactual explanations for fairness},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning with a reject option: A survey.
<em>ML</em>, <em>113</em>(5), 3073–3110. (<a
href="https://doi.org/10.1007/s10994-024-06534-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model’s predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.},
  archive      = {J_ML},
  author       = {Hendrickx, Kilian and Perini, Lorenzo and Van der Plas, Dries and Meert, Wannes and Davis, Jesse},
  doi          = {10.1007/s10994-024-06534-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3073-3110},
  shortjournal = {Mach. Learn.},
  title        = {Machine learning with a reject option: A survey},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalization for web-based services using offline
reinforcement learning. <em>ML</em>, <em>113</em>(5), 3049–3071. (<a
href="https://doi.org/10.1007/s10994-024-06525-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Web-based services present opportunities for improving UI policies based on observed user interactions. We address challenges of learning such policies through offline reinforcement learning (RL). Deployed in a production system for user authentication in a major social network, it significantly improves long-term objectives. We articulate practical challenges, provide insights on training and evaluation of offline RL, and discuss generalizations toward offline RL’s deployment in industry-scale applications.},
  archive      = {J_ML},
  author       = {Apostolopoulos, Pavlos Athanasios and Wang, Zehui and Wang, Hanson and Xu, Tenghyu and Zhou, Chad and Virochsiri, Kittipate and Zhou, Norm and Markov, Igor L.},
  doi          = {10.1007/s10994-024-06525-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3049-3071},
  shortjournal = {Mach. Learn.},
  title        = {Personalization for web-based services using offline reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning tutor better supported lower
performers in a math task. <em>ML</em>, <em>113</em>(5), 3023–3048. (<a
href="https://doi.org/10.1007/s10994-023-06423-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource limitations make it challenging to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a pivotal tool to decrease the development costs and enhance the effectiveness of intelligent tutoring software, that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we extracted interpretable insights about the pedagogical policy learned and demonstrated that the resulting policy had similar performance in a different student population. Most importantly, in both studies, the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for those most in need.},
  archive      = {J_ML},
  author       = {Ruan, Sherry and Nie, Allen and Steenbergen, William and He, Jiayu and Zhang, J. Q. and Guo, Meng and Liu, Yao and Dang Nguyen, Kyle and Wang, Catherine Y. and Ying, Rui and Landay, James A. and Brunskill, Emma},
  doi          = {10.1007/s10994-023-06423-9},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {3023-3048},
  shortjournal = {Mach. Learn.},
  title        = {Reinforcement learning tutor better supported lower performers in a math task},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning with superquantile aggregation for
heterogeneous data. <em>ML</em>, <em>113</em>(5), 2955–3022. (<a
href="https://doi.org/10.1007/s10994-023-06332-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a federated learning framework that is designed to robustly deliver good predictive performance across individual clients with heterogeneous data. The proposed approach hinges upon a superquantile-based learning objective that captures the tail statistics of the error distribution over heterogeneous clients. We present a stochastic training algorithm that interleaves differentially private client filtering with federated averaging steps. We prove finite time convergence guarantees for the algorithm: $$O(1/\sqrt{T})$$ in the nonconvex case in T communication rounds and $$O(\exp (-T/\kappa ^{3/2}) + \kappa /T)$$ in the strongly convex case with local condition number $$\kappa$$ . Experimental results on benchmark datasets for federated learning demonstrate that our approach is competitive with classical ones in terms of average error and outperforms them in terms of tail statistics of the error.},
  archive      = {J_ML},
  author       = {Pillutla, Krishna and Laguel, Yassine and Malick, Jérôme and Harchaoui, Zaid},
  doi          = {10.1007/s10994-023-06332-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2955-3022},
  shortjournal = {Mach. Learn.},
  title        = {Federated learning with superquantile aggregation for heterogeneous data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dealing with the unevenness: Deeper insights in graph-based
attack and defense. <em>ML</em>, <em>113</em>(5), 2921–2953. (<a
href="https://doi.org/10.1007/s10994-022-06234-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved state-of-the-art performance on various graph-related learning tasks. Due to the importance of safety in real-life applications, adversarial attacks and defenses on GNNs have attracted significant research attention. While the adversarial attacks successfully degrade GNNs’ performance significantly, the internal mechanisms and theoretical properties of graph-based attacks remain largely unexplored. In this paper, we develop deeper insights into graph structure attacks. Firstly, investigating the perturbations of representative attacking methods such as Metattack, we reveal that the perturbations are unevenly distributed on the graph. By analyzing empirically, we show that such perturbations shift the distribution of the training set to break the i.i.d. assumption. Although degrading GNNs’ performance successfully, such attacks lack robustness. Simply training the network on the validation set could severely degrade the attacking performance. To overcome the drawbacks, we propose a novel k-fold training strategy, leading to the Black-Box Gradient Attack algorithm. Extensive experiments are conducted to demonstrate that our proposed algorithm is able to achieve stable attacking performance without accessing the training sets. Finally, we introduce the first study to analyze the theoretical properties of graph structure attacks by verifying the existence of trade-offs when conducting graph structure attacks.},
  archive      = {J_ML},
  author       = {Zhan, Haoxi and Pei, Xiaobing},
  doi          = {10.1007/s10994-022-06234-4},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2921-2953},
  shortjournal = {Mach. Learn.},
  title        = {Dealing with the unevenness: Deeper insights in graph-based attack and defense},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Extracting automata from recurrent neural networks using
queries and counterexamples (extended version). <em>ML</em>,
<em>113</em>(5), 2877–2919. (<a
href="https://doi.org/10.1007/s10994-022-06163-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of extracting a deterministic finite automaton (DFA) from a trained recurrent neural network (RNN). We present a novel algorithm that uses exact learning and abstract interpretation to perform efficient extraction of a minimal DFA describing the state dynamics of a given RNN. We use Angluin’s $$\hbox {L}^{*}$$ algorithm as a learner and the given RNN as an oracle, refining the abstraction of the RNN only as much as necessary for answering equivalence queries. Our technique allows DFA-extraction from the RNN while avoiding state explosion, even when the state vectors are large and fine differentiation is required between RNN states. We experiment on multi-layer GRUs and LSTMs with state-vector dimensions, alphabet sizes, and underlying DFA which are significantly larger than in previous DFA-extraction work. Aditionally, we discuss when it may be relevant to apply the technique to RNNs trained as language models rather than binary classifiers, and present experiments on some such examples. In some of our experiments, the underlying target language can be described with a succinct DFA, yet we find that the extracted DFA is large and complex. These are cases in which the RNN has failed to learn the intended generalisation, and our extraction procedure highlights words which are misclassified by the seemingly “perfect” RNN.},
  archive      = {J_ML},
  author       = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  doi          = {10.1007/s10994-022-06163-2},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2877-2919},
  shortjournal = {Mach. Learn.},
  title        = {Extracting automata from recurrent neural networks using queries and counterexamples (extended version)},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental learning of iterated dependencies. <em>ML</em>,
<em>113</em>(5), 2841–2875. (<a
href="https://doi.org/10.1007/s10994-021-05947-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study some learnability problems in the family of Categorial Dependency Grammars (CDG), a class of categorial grammars defining dependency structures. CDG is a formal system, where types are attached to words, combining the classical categorial grammars’ elimination rules with valency pairing rules defining non-projective (discontinuous) dependencies; very importantly, the elimination rules are naturally extended to the so called “iterated dependencies” expressed by a specific type constructor and related elimination rules. This paper first reviews key points on negative results: even the rigid (one type per word) CDG cannot be learned neither from function/argument structures, nor even from dependency structures themselves. Such negative results prove the impossibility to define a learning algorithm for these grammar classes. Nevertheless, we show that the CDG satisfying reasonable and linguistically valid conditions on the iterated dependencies are incrementally learnable in the limit from dependency structures. We provide algorithms and also discuss these aspects for recent variants of the formalism that allow the inference of CDG from linguistic treebanks.},
  archive      = {J_ML},
  author       = {Béchet, Denis and Foret, Annie},
  doi          = {10.1007/s10994-021-05947-2},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2841-2875},
  shortjournal = {Mach. Learn.},
  title        = {Incremental learning of iterated dependencies},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic datasets and market environments for financial
reinforcement learning. <em>ML</em>, <em>113</em>(5), 2795–2839. (<a
href="https://doi.org/10.1007/s10994-023-06511-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The financial market is a particularly challenging playground for deep reinforcement learning due to its unique feature of dynamic datasets. Building high-quality market environments for training financial reinforcement learning (FinRL) agents is difficult due to major factors such as the low signal-to-noise ratio of financial data, survivorship bias of historical data, and model overfitting. In this paper, we present an updated version of FinRL-Meta, a data-centric and openly accessible library that processes dynamic datasets from real-world markets into gym-style market environments and has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we provide hundreds of market environments through an automatic data curation pipeline. Second, we provide homegrown examples and reproduce popular research papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, we provide dozens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. The codes are available at https://github.com/AI4Finance-Foundation/FinRL-Meta},
  archive      = {J_ML},
  author       = {Liu, Xiao-Yang and Xia, Ziyi and Yang, Hongyang and Gao, Jiechao and Zha, Daochen and Zhu, Ming and Wang, Christina Dan and Wang, Zhaoran and Guo, Jian},
  doi          = {10.1007/s10994-023-06511-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2795-2839},
  shortjournal = {Mach. Learn.},
  title        = {Dynamic datasets and market environments for financial reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When are they coming? Understanding and forecasting the
timeline of arrivals at the FC barcelona stadium on match days.
<em>ML</em>, <em>113</em>(5), 2765–2794. (<a
href="https://doi.org/10.1007/s10994-023-06499-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Futbol Club Barcelona operates the largest stadium in Europe (with a seating capacity of almost one hundred thousand people) and manages recurring sports events. These are influenced by multiple conditions (time and day of the week, weather, adversary) and affect city dynamics—e.g., peak demand for related services like public transport and stores. We study fine grain audience entrances at the stadium segregated by visitor type and gate to gain insights and predict the arrival behavior of future games, with a direct impact on the organizational performance and productivity of the business. We can forecast the timeline of arrivals at gate level 72 h prior to kickoff, facilitating operational and organizational decision-making by anticipating potential agglomerations and audience behavior. Furthermore, we can identify patterns for different types of visitors and understand how relevant factors affect them. These findings directly impact commercial and business interests and can alter operational logistics, venue management, and safety.},
  archive      = {J_ML},
  author       = {Serra-Burriel, Feliu and Delicado, Pedro and Cucchietti, Fernando M. and Graells-Garrido, Eduardo and Gil, Alex and Eguskiza, Imanol},
  doi          = {10.1007/s10994-023-06499-3},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2765-2794},
  shortjournal = {Mach. Learn.},
  title        = {When are they coming? understanding and forecasting the timeline of arrivals at the FC barcelona stadium on match days},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal clustering from noisy binary feedback. <em>ML</em>,
<em>113</em>(5), 2733–2764. (<a
href="https://doi.org/10.1007/s10994-024-06532-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of clustering a set of items from binary user feedback. Such a problem arises in crowdsourcing platforms solving large-scale labeling tasks with minimal effort put on the users. For example, in some of the recent reCAPTCHA systems, users clicks (binary answers) can be used to efficiently label images. In our inference problem, items are grouped into initially unknown non-overlapping clusters. To recover these clusters, the learner sequentially presents to users a finite list of items together with a question with a binary answer selected from a fixed finite set. For each of these items, the user provides a noisy answer whose expectation is determined by the item cluster and the question and by an item-specific parameter characterizing the hardness of classifying the item. The objective is to devise an algorithm with a minimal cluster recovery error rate. We derive problem-specific information-theoretical lower bounds on the error rate satisfied by any algorithm, for both uniform and adaptive (list, question) selection strategies. For uniform selection, we present a simple algorithm built upon the K-means algorithm and whose performance almost matches the fundamental limits. For adaptive selection, we develop an adaptive algorithm that is inspired by the derivation of the information-theoretical error lower bounds, and in turn allocates the budget in an efficient way. The algorithm learns to select items hard to cluster and relevant questions more often. We compare the performance of our algorithms with or without the adaptive selection strategy numerically and illustrate the gain achieved by being adaptive.},
  archive      = {J_ML},
  author       = {Ariu, Kaito and Ok, Jungseul and Proutiere, Alexandre and Yun, Seyoung},
  doi          = {10.1007/s10994-024-06532-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2733-2764},
  shortjournal = {Mach. Learn.},
  title        = {Optimal clustering from noisy binary feedback},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair and green hyperparameter optimization via
multi-objective and multiple information source bayesian optimization.
<em>ML</em>, <em>113</em>(5), 2701–2731. (<a
href="https://doi.org/10.1007/s10994-024-06515-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been recently remarked that focusing only on accuracy in searching for optimal Machine Learning models amplifies biases contained in the data, leading to unfair predictions and decision supports. Recently, multi-objective hyperparameter optimization has been proposed to search for Machine Learning models which offer equally Pareto-efficient trade-offs between accuracy and fairness. Although these approaches proved to be more versatile than fairness-aware Machine Learning algorithms—which instead optimize accuracy constrained to some threshold on fairness—their carbon footprint could be dramatic, due to the large amount of energy required in the case of large datasets. We propose an approach named FanG-HPO: fair and green hyperparameter optimization (HPO), based on both multi-objective and multiple information source Bayesian optimization. FanG-HPO uses subsets of the large dataset to obtain cheap approximations (aka information sources) of both accuracy and fairness, and multi-objective Bayesian optimization to efficiently identify Pareto-efficient (accurate and fair) Machine Learning models. Experiments consider four benchmark (fairness) datasets and four Machine Learning algorithms, and provide an assessment of FanG-HPO against both fairness-aware Machine Learning approaches and two state-of-the-art Bayesian optimization tools addressing multi-objective and energy-aware optimization.},
  archive      = {J_ML},
  author       = {Candelieri, Antonio and Ponti, Andrea and Archetti, Francesco},
  doi          = {10.1007/s10994-024-06515-0},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2701-2731},
  shortjournal = {Mach. Learn.},
  title        = {Fair and green hyperparameter optimization via multi-objective and multiple information source bayesian optimization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient learning of power grid voltage control strategies
via model-based deep reinforcement learning. <em>ML</em>,
<em>113</em>(5), 2675–2700. (<a
href="https://doi.org/10.1007/s10994-023-06422-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a model-based deep reinforcement learning (DRL) method to design emergency control strategies for short-term voltage stability problems in power systems. Recent advances show promising results for model-free DRL-based methods in power systems control problems. But in power systems applications, these model-free methods have certain issues related to training time (clock time) and sample efficiency; both are critical for making state-of-the-art DRL algorithms practically applicable. DRL-agent learns an optimal policy via a trial-and-error method while interacting with the real-world environment. It is also desirable to minimize the direct interaction of the DRL agent with the real-world power grid due to its safety-critical nature. Additionally, the state-of-the-art DRL-based policies are mostly trained using a physics-based grid simulator where dynamic simulation is computationally intensive, lowering the training efficiency. We propose a novel model-based DRL framework where a deep neural network (DNN)-based dynamic surrogate model (SM), instead of a real-world power grid or physics-based simulation, is utilized within the policy learning framework, making the process faster and more sample efficient. However, having stable training in model-based DRL is challenging because of the complex system dynamics of large-scale power systems. We addressed these issues by incorporating imitation learning to have a warm start in policy learning, reward-shaping, and multi-step loss in surrogate model training. Finally, we achieved 97.5% reduction in samples and 87.7% reduction in training time for an application to the IEEE 300-bus test system.},
  archive      = {J_ML},
  author       = {Hossain, Ramij Raja and Yin, Tianzhixi and Du, Yan and Huang, Renke and Tan, Jie and Yu, Wenhao and Liu, Yuan and Huang, Qiuhua},
  doi          = {10.1007/s10994-023-06422-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2675-2700},
  shortjournal = {Mach. Learn.},
  title        = {Efficient learning of power grid voltage control strategies via model-based deep reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning for multi-class imbalanced
training: Applications in healthcare. <em>ML</em>, <em>113</em>(5),
2655–2674. (<a
href="https://doi.org/10.1007/s10994-023-06481-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of memory and computing power, datasets are becoming increasingly complex and imbalanced. This is especially severe in the context of clinical data, where there may be one rare event for many cases in the majority class. We introduce an imbalanced classification framework, based on reinforcement learning, for training extremely imbalanced data sets, and extend it for use in multi-class settings. We combine dueling and double deep Q-learning architectures, and formulate a custom reward function and episode-training procedure, specifically with the capability of handling multi-class imbalanced training. Using real-world clinical case studies, we demonstrate that our proposed framework outperforms current state-of-the-art imbalanced learning methods, achieving more fair and balanced classification, while also significantly improving the prediction of minority classes.},
  archive      = {J_ML},
  author       = {Yang, Jenny and El-Bouri, Rasheed and O’Donoghue, Odhran and Lachapelle, Alexander S. and Soltan, Andrew A. S. and Eyre, David W. and Lu, Lei and Clifton, David A.},
  doi          = {10.1007/s10994-023-06481-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2655-2674},
  shortjournal = {Mach. Learn.},
  title        = {Deep reinforcement learning for multi-class imbalanced training: Applications in healthcare},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Connecting weighted automata, tensor networks and recurrent
neural networks through spectral learning. <em>ML</em>, <em>113</em>(5),
2619–2653. (<a
href="https://doi.org/10.1007/s10994-022-06164-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present connections between three models used in different research fields: weighted finite automata (WFA) from formal languages and linguistics, recurrent neural networks used in machine learning, and tensor networks which encompasses a set of optimization techniques for high-order tensors used in quantum physics and numerical analysis. We first present an intrinsic relation between WFA and the tensor train decomposition, a particular form of tensor network. This relation allows us to exhibit a novel low rank structure of the Hankel matrix of a function computed by a WFA and to design an efficient spectral learning algorithm leveraging this structure to scale the algorithm up to very large Hankel matrices. We then unravel a fundamental connection between WFA and second-order recurrent neural networks (2-RNN): in the case of sequences of discrete symbols, WFA and 2-RNN with linear activation functions are expressively equivalent. Leveraging this equivalence result combined with the classical spectral learning algorithm for weighted automata, we introduce the first provable learning algorithm for linear 2-RNN defined over sequences of continuous input vectors. This algorithm relies on estimating low rank sub-blocks of the Hankel tensor, from which the parameters of a linear 2-RNN can be provably recovered. The performances of the proposed learning algorithm are assessed in a simulation study on both synthetic and real-world data.},
  archive      = {J_ML},
  author       = {Li, Tianyu and Precup, Doina and Rabusseau, Guillaume},
  doi          = {10.1007/s10994-022-06164-1},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2619-2653},
  shortjournal = {Mach. Learn.},
  title        = {Connecting weighted automata, tensor networks and recurrent neural networks through spectral learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network relief: A pruning algorithm based on neural
activity. <em>ML</em>, <em>113</em>(5), 2597–2618. (<a
href="https://doi.org/10.1007/s10994-024-06516-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered—Adam and SGD. The algorithm is not designed to minimize FLOPs when considering current hardware and software implementations, although it performs reasonably when compared to the state of the art.},
  archive      = {J_ML},
  author       = {Dekhovich, Aleksandr and Tax, David M. J. and Sluiter, Marcel H. F. and Bessa, Miguel A.},
  doi          = {10.1007/s10994-024-06516-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2597-2618},
  shortjournal = {Mach. Learn.},
  title        = {Neural network relief: A pruning algorithm based on neural activity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackle balancing constraints in semi-supervised ordinal
regression. <em>ML</em>, <em>113</em>(5), 2575–2595. (<a
href="https://doi.org/10.1007/s10994-024-06518-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised ordinal regression (S2OR) has been recognized as a valuable technique to improve the performance of the ordinal regression (OR) model by leveraging available unlabeled samples. The balancing constraint is a useful approach for semi-supervised algorithms, as it can prevent the trivial solution of classifying a large number of unlabeled examples into a few classes. However, rapid training of the S2OR model with balancing constraints is still an open problem due to the difficulty in formulating and solving the corresponding optimization objective. To tackle this issue, we propose a novel form of balancing constraints and extend the traditional convex–concave procedure (CCCP) approach to solve our objective function. Additionally, we transform the convex inner loop (CIL) problem generated by the CCCP approach into a quadratic problem that resembles support vector machine, where multiple equality constraints are treated as virtual samples. As a result, we can utilize the existing fast solver to efficiently solve the CIL problem. Experimental results conducted on several benchmark and real-world datasets not only validate the effectiveness of our proposed algorithm but also demonstrate its superior performance compared to other supervised and semi-supervised algorithms},
  archive      = {J_ML},
  author       = {Zhang, Chenkang and Huang, Heng and Gu, Bin},
  doi          = {10.1007/s10994-024-06518-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2575-2595},
  shortjournal = {Mach. Learn.},
  title        = {Tackle balancing constraints in semi-supervised ordinal regression},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning to bid and rank together in recommendation
systems. <em>ML</em>, <em>113</em>(5), 2559–2573. (<a
href="https://doi.org/10.1007/s10994-023-06444-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Internet applications adopt real-time bidding mechanisms to ensure different services (types of content) are shown to the users through fair competitions. The service offering the highest bid price gets the content slot to present a list of items in its candidate pool. Through user interactions with the recommended items, the service obtains the desired engagement activities. We propose a contextual-bandit framework to jointly optimize the price to bid for the slot and the order to rank its candidates for a given service in this type of recommendation systems. Our method can take as input any feature that describes the user and the candidates, including the outputs of other machine learning models. We train  reinforcement learning policies using deep neural networks, and compute top-K Gaussian propensity scores to exclude the variance in the gradients caused by randomness unrelated to the reward. This setup further facilitates us to automatically find accurate reward functions that trade off between budget spending and user engagements. In online A/B experiments on two major services of Facebook Home Feed, Groups You Should Join and Friend Requests, our method statistically significantly boosted the number of groups joined by 14.7%, the number of friend requests accepted by 7.0%, and the number of daily active Facebook users by about 1 million, against strong hand-tuned baselines that have been iterated in production over years.},
  archive      = {J_ML},
  author       = {Ji, Geng and Jiang, Wentao and Li, Jiang and Fahid, Fahmid Morshed and Chen, Zhengxing and Li, Yinghua and Xiao, Jun and Bao, Chongxi and Zhu, Zheqing},
  doi          = {10.1007/s10994-023-06444-4},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2559-2573},
  shortjournal = {Mach. Learn.},
  title        = {Learning to bid and rank together in recommendation systems},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goal exploration augmentation via pre-trained skills for
sparse-reward long-horizon goal-conditioned reinforcement learning.
<em>ML</em>, <em>113</em>(5), 2527–2557. (<a
href="https://doi.org/10.1007/s10994-023-06503-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning often struggles to accomplish a sparse-reward long-horizon task in a complex environment. Goal-conditioned reinforcement learning (GCRL) has been employed to tackle this difficult problem via a curriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is essential for the agent to ultimately find the pathway to the desired goal. How to explore novel sub-goals efficiently is one of the most challenging issues in GCRL. Several goal exploration methods have been proposed to address this issue but still struggle to find the desired goals efficiently. In this paper, we propose a novel learning objective by optimizing the entropy of both achieved and new goals to be explored for more efficient goal exploration in sub-goal selection based GCRL. To optimize this objective, we first explore and exploit the frequently occurring goal-transition patterns mined in the environments similar to the current task to compose skills via skill learning. Then, the pre-trained skills are applied in goal exploration with theoretical justification. Evaluation on a variety of spare-reward long-horizon benchmark tasks suggests that incorporating our method into several state-of-the-art GCRL baselines significantly boosts their exploration efficiency while improving or maintaining their performance.},
  archive      = {J_ML},
  author       = {Wu, Lisheng and Chen, Ke},
  doi          = {10.1007/s10994-023-06503-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2527-2557},
  shortjournal = {Mach. Learn.},
  title        = {Goal exploration augmentation via pre-trained skills for sparse-reward long-horizon goal-conditioned reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UnbiasedNets: A dataset diversification framework for
robustness bias alleviation in neural networks. <em>ML</em>,
<em>113</em>(5), 2499–2526. (<a
href="https://doi.org/10.1007/s10994-023-06314-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance of trained neural network (NN) models, in terms of testing accuracy, has improved remarkably over the past several years, especially with the advent of deep learning. However, even the most accurate NNs can be biased toward a specific output classification due to the inherent bias in the available training datasets, which may propagate to the real-world implementations. This paper deals with the robustness bias, i.e., the bias exhibited by the trained NN by having a significantly large robustness to noise for a certain output class, as compared to the remaining output classes. The bias is shown to result from imbalanced datasets, i.e., the datasets where all output classes are not equally represented. Towards this, we propose the UnbiasedNets framework, which leverages K-means clustering and the NN’s noise tolerance to diversify the given training dataset, even from relatively smaller datasets. This generates balanced datasets and reduces the bias within the datasets themselves. To the best of our knowledge, this is the first framework catering to the robustness bias problem in NNs. We use real-world datasets to demonstrate the efficacy of the UnbiasedNets for data diversification, in case of both binary and multi-label classifiers. The results are compared to well-known tools aimed at generating balanced datasets, and illustrate how existing works have limited success while addressing the robustness bias. In contrast, UnbiasedNets provides a notable improvement over existing works, while even reducing the robustness bias significantly in some cases, as observed by comparing the NNs trained on the diversified and original datasets.},
  archive      = {J_ML},
  author       = {Naseer, Mahum and Prabakaran, Bharath Srinivas and Hasan, Osman and Shafique, Muhammad},
  doi          = {10.1007/s10994-023-06314-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2499-2526},
  shortjournal = {Mach. Learn.},
  title        = {UnbiasedNets: A dataset diversification framework for robustness bias alleviation in neural networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounding the rademacher complexity of fourier neural
operators. <em>ML</em>, <em>113</em>(5), 2467–2498. (<a
href="https://doi.org/10.1007/s10994-024-06533-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, several types of neural operators have been developed, including deep operator networks, graph neural operators, and Multiwavelet-based operators. Compared with these models, the Fourier neural operator (FNO), a physics-inspired machine learning method, is computationally efficient and can learn nonlinear operators between function spaces independent of a certain finite basis. This study investigated the bounding of the Rademacher complexity of the FNO based on specific group norms. Using capacity based on these norms, we bound the generalization error of the model. In addition, we investigate the correlation between the empirical generalization error and the proposed capacity of FNO. We infer that the type of group norm determines the information about the weights and architecture of the FNO model stored in capacity. The experimental results offer insight into the impact of the number of modes used in the FNO model on the generalization error. The results confirm that our capacity is an effective index for estimating generalization errors.},
  archive      = {J_ML},
  author       = {Kim, Taeyoung and Kang, Myungjoo},
  doi          = {10.1007/s10994-024-06533-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2467-2498},
  shortjournal = {Mach. Learn.},
  title        = {Bounding the rademacher complexity of fourier neural operators},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goal-conditioned offline reinforcement learning through
state space partitioning. <em>ML</em>, <em>113</em>(5), 2435–2465. (<a
href="https://doi.org/10.1007/s10994-023-06500-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) aims to create policies for sequential decision-making using exclusively offline datasets. This presents a significant challenge, especially when attempting to accomplish multiple distinct goals or outcomes within a given scenario while receiving sparse rewards. Prior methods using advantage weighting for offline goal-conditioned learning improve policies monotonically. However, they still face challenges from distribution shift and multi-modality that arise due to conflicting ways to reach a goal. This issue is especially challenging in long-horizon tasks, where the presence of multiple, often conflicting, solutions makes it hard to identify a single optimal policy for transitioning from a state to a desired goal. To address these challenges, we introduce a complementary advantage-based weighting scheme that incorporates an additional source of inductive bias. Given a value-based partitioning of the state space, the contribution of actions expected to lead to target regions that are easier to reach, compared to the final goal, is further increased. Our proposed approach, Dual-Advantage Weighted Offline Goal-conditioned RL, outperforms several competing offline algorithms in widely used benchmarks. Furthermore, we provide a theoretical guarantee that the learned policy will not be inferior to the underlying behavior policy.},
  archive      = {J_ML},
  author       = {Wang, Mianchu and Jin, Yue and Montana, Giovanni},
  doi          = {10.1007/s10994-023-06500-z},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2435-2465},
  shortjournal = {Mach. Learn.},
  title        = {Goal-conditioned offline reinforcement learning through state space partitioning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the incompatibility of accuracy and equal opportunity.
<em>ML</em>, <em>113</em>(5), 2405–2434. (<a
href="https://doi.org/10.1007/s10994-023-06331-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main concerns about fairness in machine learning (ML) is that, in order to achieve it, one may have to trade off some accuracy. To overcome this issue, Hardt et al. (Adv Neural Inf Process Syst 29, 2016) proposed the notion of equality of opportunity (EO), which is compatible with maximal accuracy when the target label is deterministic with respect to the input features. In the probabilistic case, however, the issue is more complicated: It has been shown that under differential privacy constraints, there are data sources for which EO can only be achieved at the total detriment of accuracy, in the sense that a classifier that satisfies EO cannot be more accurate than a trivial (i.e., constant) classifier. In this paper, we strengthen this result by removing the privacy constraint. Namely, we show that for certain data sources, the most accurate classifier that satisfies EO is a trivial classifier. Furthermore, we study the admissible trade-offs between accuracy and EO loss (opportunity difference) and characterize the conditions on the data source under which EO and non-trivial accuracy are compatible.},
  archive      = {J_ML},
  author       = {Pinzón, Carlos and Palamidessi, Catuscia and Piantanida, Pablo and Valencia, Frank},
  doi          = {10.1007/s10994-023-06331-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2405-2434},
  shortjournal = {Mach. Learn.},
  title        = {On the incompatibility of accuracy and equal opportunity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training data influence analysis and estimation: A survey.
<em>ML</em>, <em>113</em>(5), 2351–2403. (<a
href="https://doi.org/10.1007/s10994-023-06495-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training’s underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data’s influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future research directions to make influence analysis more useful in practice as well as more theoretically and empirically sound.},
  archive      = {J_ML},
  author       = {Hammoudeh, Zayd and Lowd, Daniel},
  doi          = {10.1007/s10994-023-06495-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2351-2403},
  shortjournal = {Mach. Learn.},
  title        = {Training data influence analysis and estimation: A survey},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-attentive experience replay in off-policy reinforcement
learning. <em>ML</em>, <em>113</em>(5), 2327–2349. (<a
href="https://doi.org/10.1007/s10994-023-06505-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experience replay, which stores past samples for reuse, has become a fundamental component of off-policy reinforcement learning. Some pioneering works have indicated that prioritization or reweighting of samples with on-policiness can yield significant performance improvements. However, this method doesn’t pay enough attention to sample diversity, which may result in instability or even long-term performance slumps. In this work, we introduce a novel Re-attention criterion to reevaluate recent experiences, thus benefiting the agent from learning about them. We call this overall algorithm, Re-attentive Experience Replay (RAER). RAER employs a parameter-insensitive dynamic testing technique to enhance the attention of samples generated by policies with promising trends in overall performance. By wisely leveraging diverse samples, RAER fulfills the positive effects of on-policiness while avoiding its potential negative influences. Extensive experiments demonstrate the effectiveness of RAER in improving both performance and stability. Moreover, replacing the on-policiness component of the state-of-the-art approach with RAER can yield significant benefits.},
  archive      = {J_ML},
  author       = {Wei, Wei and Wang, Da and Li, Lin and Liang, Jiye},
  doi          = {10.1007/s10994-023-06505-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2327-2349},
  shortjournal = {Mach. Learn.},
  title        = {Re-attentive experience replay in off-policy reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalanced gradients: A subtle cause of overestimated
adversarial robustness. <em>ML</em>, <em>113</em>(5), 2301–2326. (<a
href="https://doi.org/10.1007/s10994-023-06328-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called Imbalanced Gradients that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a margin decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We also propose a multi-targeted and ensemble version of our MD attack. By investigating 24 defense models proposed since 2018, we find that 11 models are susceptible to a certain degree of imbalanced gradients and our MD attack can decrease their robustness evaluated by the best standalone baseline attack by more than 1%. We also provide an in-depth investigation on the likely causes of imbalanced gradients and effective countermeasures.},
  archive      = {J_ML},
  author       = {Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
  doi          = {10.1007/s10994-023-06328-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {2301-2326},
  shortjournal = {Mach. Learn.},
  title        = {Imbalanced gradients: A subtle cause of overestimated adversarial robustness},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting of mobile network traffic and spatio–temporal
analysis using modLSTM. <em>ML</em>, <em>113</em>(4), 2277–2300. (<a
href="https://doi.org/10.1007/s10994-023-06471-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative system and prediction model for forecasting network traffic in specific geographical locations using historical data. As Internet service providers increasingly rely on data analytics for decision-making, optimized network forecasting faces challenges such as data cleaning and preprocessing. Our approach utilizes an Artificial Recurrent Neural Network-based Modified Long Short-Term Memory model to provide continuous and precise predictions of network traffic. Notably, the proposed model outperforms conventional LSTM models, achieving a 61.9% reduction in Mean Absolute Percent Error. Our approach also integrates an interpolation technique to address the zero-component error. This further enhances the effectiveness and reliability of the model. The model promises to enhance resource utilization and lighten the load on traffic resource provisioning entities, promoting more efficient mobile network traffic management. The low training time of 3.26 min and prediction time of 0.14 s pave the way for real-time implementation of the model for network traffic forecasting and management. The comparative analysis with state-of-the-art models proves the supremacy of the proposed model.},
  archive      = {J_ML},
  author       = {Aski, Vidyadhar J. and Chavan, Rugved Sanjay and Dhaka, Vijaypal Singh and Rani, Geeta and Zumpano, Ester and Vocaturo, Eugenio},
  doi          = {10.1007/s10994-023-06471-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2277-2300},
  shortjournal = {Mach. Learn.},
  title        = {Forecasting of mobile network traffic and spatio–temporal analysis using modLSTM},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural language inference model for customer advocacy
detection in online customer engagement. <em>ML</em>, <em>113</em>(4),
2249–2275. (<a
href="https://doi.org/10.1007/s10994-023-06476-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online customer advocacy has developed as a distinctive strategic way to improve organisational performance by fostering favourable reciprocal affinitive customer behaviours between the business and its customers. Intelligent systems that can identify online social advocates based on their social interaction and long-standing conversations with the brads are still lacking. This study adds to the burgeoning body of literature in this research area by developing a novel model to identify brand advocates using natural language inference (NLI) and artificial intelligence (AI) approaches. In particular, a hybridised deep learning model (BERT-BiLSTM-TextCNN) is proposed and adept at extracting the amount of entailment, contradiction, and neutrality obtained from the advocates&#39; replies to the brands. This offers a new dimension to identify advocates based on the semantic similarities between the brands’ tweets and customers’ replies. The experimental results demonstrate the applicability of integrating the advantages of fine-tuned BERT, TextCNN, and BiLSTM using various evaluation metrics. Further, the proposed model is incorporated in a downstream task to verify and validate its effectiveness in capturing the correlation between brands and their advocates. Our findings contribute to the burgeoning body of literature in this research area and have important implications for identifying and engaging with brand advocates in online customer engagement.},
  archive      = {J_ML},
  author       = {Abu-Salih, Bilal and Alweshah, Mohammed and Alazab, Moutaz and Al-Okaily, Manaf and Alahmari, Muteeb and Al-Habashneh, Mohammad and Al-Sharaeh, Saleh},
  doi          = {10.1007/s10994-023-06476-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2249-2275},
  shortjournal = {Mach. Learn.},
  title        = {Natural language inference model for customer advocacy detection in online customer engagement},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Word embeddings for retrieving tabular data from research
publications. <em>ML</em>, <em>113</em>(4), 2227–2248. (<a
href="https://doi.org/10.1007/s10994-023-06472-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists face challenges when finding datasets related to their research problems due to the limitations of current dataset search engines. Existing tools for searching research datasets rely on publication content or metadata, do not considering the data contained in the publication in the form of tables. Moreover, scientists require more elaborate inputs and functionalities to retrieve different parts of an article, such as data presented in tables, based on their search purposes. Therefore, this paper proposes a novel approach to retrieve relevant tabular datasets from publications. The input of our system is a research problem stated as an abstract from a scientific paper, and the output is a set of relevant tables from publications that are related to the research problem. This approach aims to provide a better solution for scientists to find useful datasets that support them in addressing their research problems. To validate this approach, experiments were conducted using word embedding from different language models to calculate the semantic similarity between abstracts and tables. The results showed that contextual models significantly outperformed non-contextual models, especially when pre-trained with scientific data. Furthermore, the importance of context was found to be crucial for improving the results.},
  archive      = {J_ML},
  author       = {Berenguer, Alberto and Mazón, Jose-Norberto and Tomás, David},
  doi          = {10.1007/s10994-023-06472-0},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2227-2248},
  shortjournal = {Mach. Learn.},
  title        = {Word embeddings for retrieving tabular data from research publications},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal graph learning for dynamic link prediction with
text in online social networks. <em>ML</em>, <em>113</em>(4), 2207–2226.
(<a href="https://doi.org/10.1007/s10994-023-06475-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction in Online Social Networks—OSNs—has been the focus of numerous studies in the machine learning community. A successful machine learning-based solution for this task needs to (i) leverage global and local properties of the graph structure surrounding links; (ii) leverage the content produced by OSN users; and (iii) allow their representations to change over time, as thousands of new links between users and new content like textual posts, comments, images and videos are created/uploaded every month. Current works have successfully leveraged the structural information but only a few have also taken into account the textual content and/or the dynamicity of network structure and node attributes. In this paper, we propose a methodology based on temporal graph neural networks to handle the challenges described above. To understand the impact of textual content on this task, we provide a novel pipeline to include textual information alongside the structural one with the usage of BERT language models, dense preprocessing layers, and an effective post-processing decoder. We conducted the evaluation on a novel dataset gathered from an emerging blockchain-based online social network, using a live-update setting that takes into account the evolving nature of data and models. The dataset serves as a useful testing ground for link prediction evaluation because it provides high-resolution temporal information on link creation and textual content, characteristics hard to find in current benchmark datasets. Our results show that temporal graph learning is a promising solution for dynamic link prediction with text. Indeed, combining textual features and dynamic Graph Neural Networks—GNNs—leads to the best performances over time. On average, the textual content can enhance the performance of a dynamic GNN by 3.1% and, as the collection of documents increases in size over time, help even models that do not consider the structural information of the network.},
  archive      = {J_ML},
  author       = {Dileo, Manuel and Zignani, Matteo and Gaito, Sabrina},
  doi          = {10.1007/s10994-023-06475-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2207-2226},
  shortjournal = {Mach. Learn.},
  title        = {Temporal graph learning for dynamic link prediction with text in online social networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Exploiting sparsity and statistical dependence in
multivariate data fusion: An application to misinformation detection for
high-impact events. <em>ML</em>, <em>113</em>(4), 2183–2205. (<a
href="https://doi.org/10.1007/s10994-023-06424-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the evolution of social media, cyberspace has become the de-facto medium for users to communicate during high-impact events such as natural disasters, terrorist attacks, and periods of political unrest. However, during such high-impact events, misinformation can spread rapidly on social media, affecting decision-making and creating social unrest. Identifying the spread of misinformation during high-impact events is a significant data challenge, given the multi-modal data associated with social media posts. Advances in multi-modal learning have shown promise for detecting misinformation; however, key limitations still make this a significant challenge. These limitations include the explicit and efficient modeling of the underlying non-linear associations of multi-modal data geared at misinformation detection. This paper presents a novel avenue of work that demonstrates how to frame the problem of misinformation detection in social media using multi-modal latent variable modeling and presents two novel algorithms capable of modeling the underlying associations of multi-modal data. We demonstrate the effectiveness of the proposed algorithms using simulated data and study their performance in the context of misinformation detection using a popular multi-modal dataset that consists of tweets published during several high-impact events.},
  archive      = {J_ML},
  author       = {Damasceno, Lucas P. and Rexhepi, Egzona and Shafer, Allison and Whitehouse, Ian and Japkowicz, Nathalie and Cavalcante, Charles C. and Corizzo, Roberto and Boukouvalas, Zois},
  doi          = {10.1007/s10994-023-06424-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2183-2205},
  shortjournal = {Mach. Learn.},
  title        = {Exploiting sparsity and statistical dependence in multivariate data fusion: An application to misinformation detection for high-impact events},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast parameterless prototype-based co-clustering.
<em>ML</em>, <em>113</em>(4), 2153–2181. (<a
href="https://doi.org/10.1007/s10994-023-06474-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor co-clustering algorithms have been proven useful in many application scenarios, such as recommender systems, biological data analysis and the analysis of complex and evolving networks. However, they are significantly affected by wrong parameter configurations, since, at the very least, they require the cluster number to be set for each mode of the matrix/tensor, although they typically have other algorithm-specific hyper-parameters that need to be fine-tuned. Among the few known objective functions that can be optimized without setting these parameters, the Goodman–Kruskal $$\tau $$ —a statistical association measure that estimates the strength of the link between two or more discrete random variables—has proven its effectiveness in complex matrix and tensor co-clustering applications. However, its optimization in a co-clustering setting is tricky and, so far, has leaded to very slow and, at least in some specific but not unfrequent cases, inaccurate algorithms, due to its normalization term. In this paper, we investigate some interesting mathematical properties of $$\tau $$ , and propose a new simplified objective function with the ability of discovering an arbitrary and a priori unspecified number of good-quality co-clusters. Additionally, the new objective function definition allows for a novel prototype-based optimization strategy that enables the fast execution of matrix and higher-order tensor co-clustering. We show experimentally that the new algorithm preserves or even improves the quality of the discovered co-clusters by outperforming state-of-the-art competing approaches, while reducing the execution time by at least two orders of magnitude.},
  archive      = {J_ML},
  author       = {Battaglia, Elena and Peiretti, Federico and Pensa, Ruggero G.},
  doi          = {10.1007/s10994-023-06474-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2153-2181},
  shortjournal = {Mach. Learn.},
  title        = {Fast parameterless prototype-based co-clustering},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic weighted ensemble for diarrhoea incidence
predictions. <em>ML</em>, <em>113</em>(4), 2129–2152. (<a
href="https://doi.org/10.1007/s10994-023-06465-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diarrhoea (DH) disease pose significant threats to national morbidity and mortality in Vietnam, especially on children. Being a climate sensitive disease, it has strong links to various meteorological factors like rainfalls or temperatures. Hence, together with global climate changes, the risk of diarrhoea has been increasing gradually while Vietnam is already a hotspot of diarrhoea worldwide. Thus, having an effective early warning system is becoming an urgent need. However, it has not been paid enough attention with very few research works, mainly focusing on quantilizing the relationships among various climate factors and diarrhoea incidences. Exploring more sophisticated machine learning techniques is therefore an interesting work towards more efficient and effective warning systems. This paper consists of two main contributions. First, many different state-of-the-art prediction models from traditional to most recent advantaged methods, e.g., SARIMA, SARIMAX, LSTM, CNN, Xgboost, SVM, LightGBM, Catboost, LightGBM, N-HiST, BlockRNN, TCN, TFT, or Transformer, are studied for predicting DH rates for a large number of locations (55 provinces) with different climates, geographics and socio-economy factors. It provides a useful view on the overall performances of different ML models on the prediction task, which is extremely useful for other researchers when developing early-warning systems for DH in other places. Second, we introduce a novel ensemble prediction model, called dynamic weighted ensemble (DWE), for further improving the DH prediction performance. DWE is a two layer ensemble approach. The first generates different meta models based on four base component models. The second layer employs a novel approach to predict the performances of all selected meta models and uses these predicted results to dynamically combine these models in a weighted scheme to produce final results. This is totally different to traditional ensemble approaches which only rely on fixed combinations of their components. To the best of our knowledge, DWE is also the first ensemble approach for diarrhoea prediction. Extensive experiments are conducted over all 55 provinces of Vietnam to demonstrate the performance of DWE and to reveal its important characteristics.},
  archive      = {J_ML},
  author       = {Do, Thanh Duy and Nguyen, Thuan Dinh and Ta, Viet Cuong and Anh, Duong Tran and Tran Thi, Tuyet-Hanh and Phan, Diep and Mai, Son T.},
  doi          = {10.1007/s10994-023-06465-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2129-2152},
  shortjournal = {Mach. Learn.},
  title        = {Dynamic weighted ensemble for diarrhoea incidence predictions},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting potential real-time donations in YouTube live
streaming services via continuous-time dynamic graphs. <em>ML</em>,
<em>113</em>(4), 2093–2127. (<a
href="https://doi.org/10.1007/s10994-023-06449-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online live streaming platforms, such as YouTube Live and Twitch, have seen a surge in popularity in recent years. These platforms allow viewers to send real-time gifts to streamers, which can bring significant profits and fame. However, there has been little research on the donation system used on live streaming platforms. This paper aims to fill this gap by building a continuous-time dynamic graph to model the interactions among viewers based on real-time chat messages and predict the real-time donations on live streaming platforms. To achieve this, we propose a novel model called the Temporal Difference Graph Neural Network (TDGNN) that incorporates imbalanced learning strategies to identify potential donors during live streaming. Our model can predict the exact time when donations will appear. We conduct extensive experiments on three live streaming video datasets and demonstrate that our proposed model is more effective and robust than other baseline methods from other fields.},
  archive      = {J_ML},
  author       = {Jin, Ruidong and Liu, Xin and Murata, Tsuyoshi},
  doi          = {10.1007/s10994-023-06449-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2093-2127},
  shortjournal = {Mach. Learn.},
  title        = {Predicting potential real-time donations in YouTube live streaming services via continuous-time dynamic graphs},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal silhouette: Validation of stream clustering robust
to concept drift. <em>ML</em>, <em>113</em>(4), 2067–2091. (<a
href="https://doi.org/10.1007/s10994-023-06462-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream clustering is required in applications where data is generated continuously or periodically and must be processed considering its temporal nature. In the absence of a ground truth, internal validation is the only option to evaluate the quality of performances. Traditional internal validation is commonly used also in stream clustering, even in spite of the fact that it becomes inconsistent in the event of data evolution. Recent trends opt for incremental approaches, but these are closer to change detection rather than validation methods and limit themselves by imposing online validation on online analysis. In this work we study the impact of concept drift in the validation of stream clustering and propose the Temporal Silhouette index, therefore making internal validation conform to streaming data. We conduct tests with more than 200 datasets and contrast performances of four popular stream clustering algorithms with seven validation methods (three static internal, three incremental internal, one external) and the proposed index. Results show the suitability of the Temporal Silhouette index for stream clustering validation in the event of concept drift and different types of outliers. The demand for reliable unsupervised learning in applications that process data in streams is ever-increasing, and such reliability inevitably requires the use of validation. This fact highlights the significance of the novel approach proposed in this work.},
  archive      = {J_ML},
  author       = {Iglesias Vázquez, Félix and Zseby, Tanja},
  doi          = {10.1007/s10994-023-06462-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2067-2091},
  shortjournal = {Mach. Learn.},
  title        = {Temporal silhouette: Validation of stream clustering robust to concept drift},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Textmatcher: Cross-attentional neural network to compare
image and text. <em>ML</em>, <em>113</em>(4), 2045–2066. (<a
href="https://doi.org/10.1007/s10994-023-06418-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multimodal-learning problem where, given an image containing a single-line (printed or handwritten) text and a candidate text transcription, the goal is to assess whether the text represented in the image corresponds to the candidate text. This problem, which we dub text matching, is primarily motivated by a real industrial application scenario of automated cheque processing, whose goal is to automatically assess whether the information in a bank cheque (e.g., issue date) match the data that have been entered by the customer while depositing the cheque to an automated teller machine (ATM). The problem finds more general application in several other scenarios too, e.g., personal-identity-document processing in user-registration procedures. We devise a machine-learning model specifically designed for the text-matching problem. The proposed model, termed TextMatcher, compares the two inputs by applying a novel cross-attention mechanism over the embedding representations of image and text, and it is trained in an end-to-end fashion on the desired distribution of errors to be detected. We demonstrate the effectiveness of TextMatcher on the automated-cheque-processing use case, where TextMatcher is shown to generalize well to future unseen dates, unlike existing models designed for related problems. We further assess the performance of TextMatcher on different distributions of errors on the public IAM dataset. Results attest that, compared to a naïve model, a variant with fully-connected layers instead of the cross-attention module and existing models for related problems, TextMatcher achieves higher performance on a variety of configurations.},
  archive      = {J_ML},
  author       = {Arrigoni, Valentina and Repele, Luisa and Saccavino, Dario Marino},
  doi          = {10.1007/s10994-023-06418-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2045-2066},
  shortjournal = {Mach. Learn.},
  title        = {Textmatcher: Cross-attentional neural network to compare image and text},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable AI in drug discovery: Self-interpretable graph
neural network for molecular property prediction using concept
whitening. <em>ML</em>, <em>113</em>(4), 2013–2044. (<a
href="https://doi.org/10.1007/s10994-023-06369-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction is a fundamental task in the field of drug discovery. Several works use graph neural networks to leverage molecular graph representations. Although they have been successfully applied in a variety of applications, their decision process is not transparent. In this work, we adapt concept whitening to graph neural networks. This approach is an explainability method used to build an inherently interpretable model, which allows identifying the concepts and consequently the structural parts of the molecules that are relevant for the output predictions. We test popular models on several benchmark datasets from MoleculeNet. Starting from previous work, we identify the most significant molecular properties to be used as concepts to perform classification. We show that the addition of concept whitening layers brings an improvement in both classification performance and interpretability. Finally, we provide several structural and conceptual explanations for the predictions.},
  archive      = {J_ML},
  author       = {Proietti, Michela and Ragno, Alessio and Rosa, Biagio La and Ragno, Rino and Capobianco, Roberto},
  doi          = {10.1007/s10994-023-06369-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {2013-2044},
  shortjournal = {Mach. Learn.},
  title        = {Explainable AI in drug discovery: Self-interpretable graph neural network for molecular property prediction using concept whitening},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining recommendation system using counterfactual
textual explanations. <em>ML</em>, <em>113</em>(4), 1989–2012. (<a
href="https://doi.org/10.1007/s10994-023-06390-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a significant amount of research being conducted in the field of artificial intelligence to improve the explainability and interpretability of deep learning models. It is found that if end-users understand the reason for the production of some output, it is easier to trust the system. Recommender systems are one example of systems that great efforts have been conducted to make their output more explainable. One method for producing a more explainable output is using counterfactual reasoning, which involves altering minimal features to generate a counterfactual item that results in changing the output of the system. This process allows the identification of input features that have a significant impact on the desired output, leading to effective explanations. In this paper, we present a method for generating counterfactual explanations for both tabular and textual features. We evaluated the performance of our proposed method on three real-world datasets and demonstrated a +5% improvement on finding effective features (based on model-based measures) compared to the baseline method.},
  archive      = {J_ML},
  author       = {Ranjbar, Niloofar and Momtazi, Saeedeh and Homayoonpour, MohammadMehdi},
  doi          = {10.1007/s10994-023-06390-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1989-2012},
  shortjournal = {Mach. Learn.},
  title        = {Explaining recommendation system using counterfactual textual explanations},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dense subgraphs induced by edge labels. <em>ML</em>,
<em>113</em>(4), 1967–1987. (<a
href="https://doi.org/10.1007/s10994-023-06377-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding densely connected groups of nodes in networks is a widely-used tool for analysis in graph mining. A popular choice for finding such groups is to find subgraphs with a high average degree. While useful, interpreting such subgraphs may be difficult. On the other hand, many real-world networks have additional information, and we are specifically interested in networks with labels on edges. In this paper, we study finding sets of labels that induce dense subgraphs. We consider two notions of density: average degree and the number of edges minus the number of nodes weighted by a parameter $$\alpha$$ . There are many ways to induce a subgraph from a set of labels, and we study two cases: First, we study conjunctive-induced dense subgraphs, where the subgraph edges need to have all labels. Secondly, we study disjunctive-induced dense subgraphs, where the subgraph edges need to have at least one label. We show that both problems are NP-hard. Because of the hardness, we resort to greedy heuristics. We show that we can implement the greedy search efficiently: the respective running times for finding conjunctive-induced and disjunctive-induced dense subgraphs are in $$\mathcal {O} \mathopen {}\left( p \log k\right)$$ and $$\mathcal {O} \mathopen {}\left( p \log ^2 k\right)$$ , where p is the number of edge-label pairs and k is the number of labels. Our experimental evaluation demonstrates that we can find the ground truth in synthetic graphs and that we can find interpretable subgraphs from real-world networks.},
  archive      = {J_ML},
  author       = {Kumpulainen, Iiro and Tatti, Nikolaj},
  doi          = {10.1007/s10994-023-06377-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1967-1987},
  shortjournal = {Mach. Learn.},
  title        = {Dense subgraphs induced by edge labels},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperparameter importance and optimization of quantum neural
networks across small datasets. <em>ML</em>, <em>113</em>(4), 1941–1966.
(<a href="https://doi.org/10.1007/s10994-023-06389-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As restricted quantum computers become available, research focuses on finding meaningful applications. For example, in quantum machine learning, a special type of quantum circuit called a quantum neural network is one of the most investigated approaches. However, we know little about suitable circuit architectures or important model hyperparameters for a given task. In this work, we apply the functional ANOVA framework to the quantum neural network architectures to analyze which of the quantum machine learning hyperparameters are most influential for their predictive performance. We restrict our study to 7 open-source datasets from the OpenML-CC18 classification benchmark, which are small enough for simulations on quantum hardware with fewer than 20 qubits. Using this framework, three main levels of importance were identified, confirming expected patterns and revealing new insights. For instance, the learning rate is identified as the most important hyperparameter on all datasets, whereas the particular choice of entangling gates used is found to be the least important on all except for one dataset. In addition to identifying the relevant hyperparameters, for each of them, we also learned data-driven priors based on values that perform well on previously seen datasets, which can then be used to steer hyperparameter optimization processes. We utilize these priors in the hyperparameter optimization method hyperband and show that these improve performance against uniform sampling across all datasets by, on average, $$0.53 \%$$ , up to $$6.11 \%$$ , in cross-validation accuracy. We also demonstrate that such improvements hold on average regardless of the configuration hyperband is run with. Our work introduces new methodologies for studying quantum machine learning models toward quantum model selection in practice. All research code is made publicly available.},
  archive      = {J_ML},
  author       = {Moussa, Charles and Patel, Yash J. and Dunjko, Vedran and Bäck, Thomas and van Rijn, Jan N.},
  doi          = {10.1007/s10994-023-06389-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1941-1966},
  shortjournal = {Mach. Learn.},
  title        = {Hyperparameter importance and optimization of quantum neural networks across small datasets},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multimodal representation learning for generalizable
person re-identification. <em>ML</em>, <em>113</em>(4), 1921–1939. (<a
href="https://doi.org/10.1007/s10994-023-06352-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. Recently, leveraging the supervised or semi-unsupervised learning paradigms, which benefits from the large-scale datasets and strong computing performance, has achieved a competitive performance on a specific target domain. However, when Re-ID models are directly deployed in a new domain without target samples, they always suffer from considerable performance degradation and poor domain generalization. To address this challenge, we propose a Deep Multimodal Representation Learning network to elaborate rich semantic knowledge for assisting in representation learning during the pre-training. Importantly, a multimodal representation learning strategy is introduced to translate the features of different modalities into the common space, which can significantly boost generalization capability of Re-ID model. As for the fine-tuning stage, a realistic dataset is adopted to fine-tune the pre-trained model for better distribution alignment with real-world data. Comprehensive experiments on benchmarks demonstrate that our method can significantly outperform previous domain generalization or meta-learning methods with a clear margin. Our source code will also be publicly available at https://github.com/JeremyXSC/DMRL .},
  archive      = {J_ML},
  author       = {Xiang, Suncheng and Chen, Hao and Ran, Wei and Yu, Zefang and Liu, Ting and Qian, Dahong and Fu, Yuzhuo},
  doi          = {10.1007/s10994-023-06352-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1921-1939},
  shortjournal = {Mach. Learn.},
  title        = {Deep multimodal representation learning for generalizable person re-identification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProtoSimi: Label correction for fine-grained visual
categorization. <em>ML</em>, <em>113</em>(4), 1903–1920. (<a
href="https://doi.org/10.1007/s10994-023-06313-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models trained by using clean data have achieved tremendous success in fine-grained image classification. Yet, they generally suffer from significant performance degradation when encountering noisy labels. Existing approaches to handle label noise, though proved to be effective for generic object recognition, usually fail on fine-grained data. The reason is that, on fine-grained data, the category difference is subtle and the training sample size is small. Then deep models could easily overfit the noisy labels. To improve the robustness of deep models on noisy data for fine-grained visual categorization, in this paper, we propose a novel learning framework named ProtoSimi. Our method employs an adaptive label correction strategy, ensuring effective learning on limited data. Specifically, our approach considers the criteria of exploring the effectiveness of both global class-prototype and part class-prototype similarities in identifying and correcting labels of samples. We evaluate our method on three standard benchmarks of fine-grained recognition. Experimental results show that our method outperforms the existing label noisy methods by a large margin. In ablation studies, we also verify that our method is non-sensitive to hyper-parameters selection and can be integrated with other FGVC methods to increase the generalization performance.},
  archive      = {J_ML},
  author       = {Shen, Jialiang and Yao, Yu and Huang, Shaoli and Wang, Zhiyong and Zhang, Jing and Wang, Ruxing and Yu, Jun and Liu, Tongliang},
  doi          = {10.1007/s10994-023-06313-0},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1903-1920},
  shortjournal = {Mach. Learn.},
  title        = {ProtoSimi: Label correction for fine-grained visual categorization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning state importance for preference-based reinforcement
learning. <em>ML</em>, <em>113</em>(4), 1885–1901. (<a
href="https://doi.org/10.1007/s10994-022-06295-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preference-based reinforcement learning (PbRL) develops agents using human preferences. Due to its empirical success, it has prospect of benefiting human-centered applications. Meanwhile, previous work on PbRL overlooks interpretability, which is an indispensable element of ethical artificial intelligence (AI). While prior art for explainable AI offers some machinery, there lacks an approach to select samples to construct explanations. This becomes an issue for PbRL, as transitions relevant to task solving are often outnumbered by irrelevant ones. Thus, ad-hoc sample selection undermines the credibility of explanations. The present study proposes a framework for learning reward functions and state importance from preferences simultaneously. It offers a systematic approach for selecting samples when constructing explanations. Moreover, the present study proposes a perturbation analysis to evaluate the learned state importance quantitatively. Through experiments on discrete and continuous control tasks, the present study demonstrates the proposed framework’s efficacy for providing interpretability without sacrificing task performance.},
  archive      = {J_ML},
  author       = {Zhang, Guoxi and Kashima, Hisashi},
  doi          = {10.1007/s10994-022-06295-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1885-1901},
  shortjournal = {Mach. Learn.},
  title        = {Learning state importance for preference-based reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning model outputs for class imbalanced non-IID
federated learning. <em>ML</em>, <em>113</em>(4), 1861–1884. (<a
href="https://doi.org/10.1007/s10994-022-06241-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) aims to generate a global shared model via collaborating with decentralized edge computing devices with privacy considerations. A significant challenge in FL is the non-IID data partitioning across heterogeneous devices, making the local update diverge a lot and difficult to aggregate. This diversity in local models is caused by the different posterior probability of samples when class distribution skews. Meanwhile, FL often faces imbalanced global data in practical scenarios. By analyzing the relationship between the samples’ posterior probability in different data distributions, we propose a statistically principled probability-corrected loss to align the posterior probability when models are trained on heterogeneous clients. Additionally, we share fixed prototypes on each client to constrain the distribution of heterogeneous clients’ features. Our approach can well handle non-IID FL with balanced and imbalanced global data. We combine our approach with existing FL algorithms and investigate it on common FL benchmarks. Abundant experimental results verify the superiorities of our methods.},
  archive      = {J_ML},
  author       = {Li, Lan and Zhan, De-chuan and Li, Xin-chun},
  doi          = {10.1007/s10994-022-06241-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1861-1884},
  shortjournal = {Mach. Learn.},
  title        = {Aligning model outputs for class imbalanced non-IID federated learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards enabling learnware to handle heterogeneous feature
spaces. <em>ML</em>, <em>113</em>(4), 1839–1860. (<a
href="https://doi.org/10.1007/s10994-022-06245-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learnware paradigm was recently proposed by Zhou (2016) with the wish of developing the learnware market to help users build models more efficiently by reusing existing well-performed models rather than starting from scratch. Specifically, a learnware in the learnware market is a well-performed pre-trained model with a specification describing its specialty and utility, and the market identifies helpful learnware(s) for the user’s task based on the specification. Recent studies have attempted to realize a homogeneous prototype learnware market initially through Reduced Kernel Mean Embedding (RKME) specification, which requires all models in the market to share the same feature space. However, this limits the application scope of the learnware paradigm because various pre-trained models are often obtained from different feature spaces in real-world scenarios. In this paper, we make the first attempt to enable the learnware to handle heterogeneous feature spaces. We propose a more powerful specification to manage heterogeneous learnwares by integrating subspace learning in the specification design, along with a practical approach for identifying and reusing helpful learnwares for the user’s task. Empirical studies on both synthetic data and real-world tasks validate the efficacy of our approach.},
  archive      = {J_ML},
  author       = {Tan, Peng and Tan, Zhi-Hao and Jiang, Yuan and Zhou, Zhi-Hua},
  doi          = {10.1007/s10994-022-06245-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1839-1860},
  shortjournal = {Mach. Learn.},
  title        = {Towards enabling learnware to handle heterogeneous feature spaces},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local AdaGrad-type algorithm for stochastic convex-concave
optimization. <em>ML</em>, <em>113</em>(4), 1819–1838. (<a
href="https://doi.org/10.1007/s10994-022-06239-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale convex-concave minimax problems arise in numerous applications, including game theory, robust training, and training of generative adversarial networks. Despite their wide applicability, solving such problems efficiently and effectively is challenging in the presence of large amounts of data using existing stochastic minimax methods. We study a class of stochastic minimax methods and develop a communication-efficient distributed stochastic extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable for solving convex-concave minimax problems in the Parameter-Server model. LocalAdaSEG has three main features: (1) a periodic communication strategy that reduces the communication cost between workers and the server; (2) an adaptive learning rate that is computed locally and allows for tuning-free implementation; and (3) theoretically, a nearly linear speed-up with respect to the dominant variance term, arising from the estimation of the stochastic gradient, is proven in both the smooth and nonsmooth convex-concave settings. LocalAdaSEG is used to solve a stochastic bilinear game, and train a generative adversarial network. We compare LocalAdaSEG against several existing optimizers for minimax problems and demonstrate its efficacy through several experiments in both homogeneous and heterogeneous settings.},
  archive      = {J_ML},
  author       = {Liao, Luofeng and Shen, Li and Duan, Jia and Kolar, Mladen and Tao, Dacheng},
  doi          = {10.1007/s10994-022-06239-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1819-1838},
  shortjournal = {Mach. Learn.},
  title        = {Local AdaGrad-type algorithm for stochastic convex-concave optimization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InfoCEVAE: Treatment effect estimation with hidden
confounding variables matching. <em>ML</em>, <em>113</em>(4), 1799–1817.
(<a href="https://doi.org/10.1007/s10994-022-06246-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment effect estimation is a fundamental problem in various domains for effective decision making. While many studies assume that observational data include all the confounding variables, we cannot practically guarantee that observational data include such confounding variables, and there might be confounding variables that are not included in observational data, referred to as hidden confounding variables. Recently, variational autencoder (VAE) based methods have been successfully applied to treatment effect estimation problem. However, although they can recover a large class of latent variable models, they do not give the correct treatment effect, even when they achieve an optimal solution due to the nature of VAE loss function. We propose an efficient VAE-based method that employs information theory to estimate treatment effect and combines it with a matching technique. To the best of our knowledge, this is the first work that gives the correct treatment effect given an optimal solution using VAE-based methods. Experiments on a semi-real dataset and synthetic dataset demonstrate that the proposed method mitigates VAE problems and observational bias effectively, even under hidden confounding variables, and outperforms strong baseline methods.},
  archive      = {J_ML},
  author       = {Harada, Shonosuke and Kashima, Hisashi},
  doi          = {10.1007/s10994-022-06246-0},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1799-1817},
  shortjournal = {Mach. Learn.},
  title        = {InfoCEVAE: Treatment effect estimation with hidden confounding variables matching},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DVerifier: Efficient robustness verification for 3D point
cloud models. <em>ML</em>, <em>113</em>(4), 1771–1798. (<a
href="https://doi.org/10.1007/s10994-022-06235-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud models are widely applied in safety-critical scenes, which delivers an urgent need to obtain more solid proofs to verify the robustness of models. Existing verification method for point cloud model is time-expensive and computationally unattainable on large networks. Additionally, they cannot handle the complete PointNet model with joint alignment network that contains multiplication layers, which effectively boosts the performance of 3D models. This motivates us to design a more efficient and general framework to verify various architectures of point cloud models. The key challenges in verifying the large-scale complete PointNet models are addressed as dealing with the cross-non-linearity operations in the multiplication layers and the high computational complexity of high-dimensional point cloud inputs and added layers. Thus, we propose an efficient verification framework, 3DVerifier, to tackle both challenges by adopting a linear relaxation function to bound the multiplication layer and combining forward and backward propagation to compute the certified bounds of the outputs of the point cloud models. Our comprehensive experiments demonstrate that 3DVerifier outperforms existing verification algorithms for 3D models in terms of both efficiency and accuracy. Notably, our approach achieves an orders-of-magnitude improvement in verification efficiency for the large network, and the obtained certified bounds are also significantly tighter than the state-of-the-art verifiers. We release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use by the community.},
  archive      = {J_ML},
  author       = {Mu, Ronghui and Ruan, Wenjie and Marcolino, Leandro S. and Ni, Qiang},
  doi          = {10.1007/s10994-022-06235-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1771-1798},
  shortjournal = {Mach. Learn.},
  title        = {3DVerifier: Efficient robustness verification for 3D point cloud models},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IA-NGM: A bidirectional learning method for neural graph
matching with feature fusion. <em>ML</em>, <em>113</em>(4), 1743–1769.
(<a href="https://doi.org/10.1007/s10994-022-06255-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep learning methods for graph matching tasks tend to focus on affinity learning in a feedforward fashion to assist the neural network solver. However, the potential benefits of a direct feedback from the neural network solver to the affinity learning are usually underestimated and overlooked. In this paper, we propose a bidirectional learning method to tackle the above issues. Our method leverages the output of a neural network solver to perform feature fusion on the input of affinity learning. Such direct feedback helps augment the input feature maps of the raw images according to the current solution. A feature fusion procedure is proposed to enhance the raw features with pseudo features that contain deviation information of the current solution from the ground-truth one. As a result, the bidrectional alternation enables the learning component to benefit from the feedback, while keeping the strengths of learning affinity models. According to the results of experiments conducted on five benchmark datasets, our methods outperform the corresponding state-of-the-art feedforward methods.},
  archive      = {J_ML},
  author       = {Qin, Tianxiang and Tu, Shikui and Xu, Lei},
  doi          = {10.1007/s10994-022-06255-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1743-1769},
  shortjournal = {Mach. Learn.},
  title        = {IA-NGM: A bidirectional learning method for neural graph matching with feature fusion},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer and share: Semi-supervised learning from
long-tailed data. <em>ML</em>, <em>113</em>(4), 1725–1742. (<a
href="https://doi.org/10.1007/s10994-022-06247-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Tailed Semi-Supervised Learning (LTSSL) aims to learn from class-imbalanced data where only a few samples are annotated. Existing solutions typically require substantial cost to solve complex optimization problems, or class-balanced undersampling which can result in information loss. In this paper, we present the TRAS (TRAnsfer and Share) to effectively utilize long-tailed semi-supervised data. TRAS transforms the imbalanced pseudo-label distribution of a traditional SSL model via a delicate function to enhance the supervisory signals for minority classes. It then transfers the distribution to a target model such that the minority class will receive significant attention. Interestingly, TRAS shows that more balanced pseudo-label distribution can substantially benefit minority-class training, instead of seeking to generate accurate pseudo-labels as in previous works. To simplify the approach, TRAS merges the training of the traditional SSL model and the target model into a single procedure by sharing the feature extractor, where both classifiers help improve the representation learning. According to extensive experiments, TRAS delivers much higher accuracy than state-of-the-art methods in the entire set of classes as well as minority classes. Code for TRAS is available at https://github.com/Stomach-ache/TRAS .},
  archive      = {J_ML},
  author       = {Wei, Tong and Liu, Qian-Yu and Shi, Jiang-Xin and Tu, Wei-Wei and Guo, Lan-Zhe},
  doi          = {10.1007/s10994-022-06247-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1725-1742},
  shortjournal = {Mach. Learn.},
  title        = {Transfer and share: Semi-supervised learning from long-tailed data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIMBA: Discretely masked black-box attack in single object
tracking. <em>ML</em>, <em>113</em>(4), 1705–1723. (<a
href="https://doi.org/10.1007/s10994-022-06252-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial attack can force a CNN-based model to produce an incorrect output by craftily manipulating human-imperceptible input. Exploring such perturbations can help us gain a deeper understanding of the vulnerability of neural networks, and provide robustness to deep learning against miscellaneous adversaries. Despite extensive studies focusing on the robustness of image, audio, and NLP, works on adversarial examples of visual object tracking—especially in a black-box manner—are quite lacking. In this paper, we propose a novel adversarial attack method to generate noises for single object tracking under black-box settings, where perturbations are merely added on initialized frames of tracking sequences, which is difficult to be noticed from the perspective of a whole video clip. Specifically, we divide our algorithm into three components and exploit reinforcement learning for localizing important frame patches precisely while reducing unnecessary computational queries overhead. Compared to existing techniques, our method requires less time to perturb videos, but to manipulate competitive or even better adversarial performance. We test our algorithm in both long-term and short-term datasets, including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate the effectiveness of our method on three mainstream types of trackers: discrimination, Siamese-based, and reinforcement learning-based trackers. We release our attack tool, DIMBA, via GitHub https://github.com/TrustAI/DIMBA for use by the community.},
  archive      = {J_ML},
  author       = {Yin, Xiangyu and Ruan, Wenjie and Fieldsend, Jonathan},
  doi          = {10.1007/s10994-022-06252-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1705-1723},
  shortjournal = {Mach. Learn.},
  title        = {DIMBA: Discretely masked black-box attack in single object tracking},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous sets in dimensionality reduction and ensemble
learning. <em>ML</em>, <em>113</em>(4), 1683–1704. (<a
href="https://doi.org/10.1007/s10994-022-06254-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general framework for dealing with set heterogeneity in data and learning problems, which is able to exploit low complexity components. The main ingredients are (i) A definition of complexity for elements of a convex union that takes into account the complexities of their individual composition – this is used to cover the heterogeneous convex union; and (ii) Upper bounds on the complexities of restricted subsets. We demonstrate this approach in two different application areas, highlighting their conceptual connection. (1) In random projection based dimensionality reduction, we obtain improved bounds on the uniform preservation of Euclidean norms and distances when low complexity components are present in the union. (2) In statistical learning, our generalisation bounds justify heterogeneous ensemble learning methods that were incompletely understood before. We exemplify empirical results with boosting type random subspace and random projection ensembles that implement our bounds.},
  archive      = {J_ML},
  author       = {Reeve, Henry W. J. and Kabán, Ata and Bootkrajang, Jakramate},
  doi          = {10.1007/s10994-022-06254-0},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1683-1704},
  shortjournal = {Mach. Learn.},
  title        = {Heterogeneous sets in dimensionality reduction and ensemble learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learning the invariant representation for domain
generalization. <em>ML</em>, <em>113</em>(4), 1661–1681. (<a
href="https://doi.org/10.1007/s10994-022-06256-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization studies how to generalize a machine learning model to unseen distributions. Learning invariant representation across different source distributions has been shown high effectiveness for domain generalization. However, the intrinsic possibility of overfitting in source domains can limit the generalization of invariance when faced with a target domain with large discrepancy to the source domains. To address this problem, we propose a meta-learning algorithm via bilevel optimization for domain generalization, where the inner-loop objective aims to minimize the discrepancy across different source domains while the outer-loop objective aims to minimize the discrepancy between source domains and a potential target domain. We show from a geometric perspective that the proposed algorithm can improve out-of-domain robustness for invariance learning. Empirically, we evaluate on five datasets and achieve the best results among a range of strong domain generalization baselines.},
  archive      = {J_ML},
  author       = {Jia, Chen and Zhang, Yue},
  doi          = {10.1007/s10994-022-06256-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1661-1681},
  shortjournal = {Mach. Learn.},
  title        = {Meta-learning the invariant representation for domain generalization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-set learning under covariate shift. <em>ML</em>,
<em>113</em>(4), 1643–1659. (<a
href="https://doi.org/10.1007/s10994-022-06237-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set learning deals with the testing distribution where there exist samples from the classes that are unseen during training. They aim to classify the seen classes and recognize the unseen classes. Previous studies typically assume that the marginal distribution of the seen classes is fixed across the training and testing distributions. In many real-world applications, however, there may exist covariate shift between them, i.e., the marginal distribution of seen classes may shift. We call this kind of problem as open-set learning under covariate shift, aim to robustly classify the seen classes under covariate shift and be aware of the unseen classes.We present a new open-set learning framework with covariate generalization based on supervised contrastive learning, called SC–OSG, inspired by the latent connection between contrastive learning and representation invariance. Specifically, we theoretically justify supervised contrastive learning that could promote the conditional invariance of representations, a critical condition for covariate generalization. SC–OSG generates multi-source samples to promote the representation invariance and improve the covariate generalization. Based on this, we propose a detection score that is specific to the proposed training scheme. We evaluate the effectiveness of our method on several real-world datasets, on all of which we achieve competitive results with state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Shao, Jie-Jing and Yang, Xiao-Wen and Guo, Lan-Zhe},
  doi          = {10.1007/s10994-022-06237-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1643-1659},
  shortjournal = {Mach. Learn.},
  title        = {Open-set learning under covariate shift},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards adaptive unknown authentication for universal domain
adaptation by classifier paradox. <em>ML</em>, <em>113</em>(4),
1623–1641. (<a
href="https://doi.org/10.1007/s10994-022-06236-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal domain adaptation (UniDA) is a general unsupervised domain adaptation setting, which addresses both domain and label shifts in adaptation. Its main challenge lies in how to identify target samples in unshared or unknown classes. Previous methods commonly strive to depict sample “confidence” along with a threshold for rejecting unknowns, and align feature distributions of shared classes across domains. However, it is still hard to pre-specify a “confidence” criterion and threshold which are adaptive to different tasks, and a mis-prediction of unknowns further incurs mis-alignment of features in shared classes. In this paper, we propose a new UniDA method with adaptive Unknown Authentication by Classifier Paradox (UACP), considering that samples with paradoxical predictions are probably unknowns belonging to none of the source classes. In UACP, a composite classifier is jointly designed with two types of predictors. That is, a multi-class (MC) predictor classifies samples to one of the multiple source classes, while a binary one-vs-all predictor further verifies the prediction by MC predictor. Samples with verification failure or paradox are identified as unknowns. Further, instead of feature alignment for shared classes, implicit domain alignment is conducted in output space such that samples across domains share the same decision boundary, though with feature discrepancy. Empirical results validate UACP under both open-set and universal UDA settings.},
  archive      = {J_ML},
  author       = {Wang, Yunyun and Liu, Yao and Chen, Songcan},
  doi          = {10.1007/s10994-022-06236-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1623-1641},
  shortjournal = {Mach. Learn.},
  title        = {Towards adaptive unknown authentication for universal domain adaptation by classifier paradox},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Riemannian block SPD coupling manifold and its application
to optimal transport. <em>ML</em>, <em>113</em>(4), 1595–1622. (<a
href="https://doi.org/10.1007/s10994-022-06258-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the optimal transport (OT) problem between symmetric positive definite (SPD) matrix-valued measures. We formulate the above as a generalized optimal transport problem where the cost, the marginals, and the coupling are represented as block matrices and each component block is a SPD matrix. The summation of row blocks and column blocks in the coupling matrix are constrained by the given block-SPD marginals. We endow the set of such block-coupling matrices with a novel Riemannian manifold structure. This allows to exploit the versatile Riemannian optimization framework to solve generic SPD matrix-valued OT problems. We illustrate the usefulness of the proposed approach in several applications.},
  archive      = {J_ML},
  author       = {Han, Andi and Mishra, Bamdev and Jawanpuria, Pratik and Gao, Junbin},
  doi          = {10.1007/s10994-022-06258-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1595-1622},
  shortjournal = {Mach. Learn.},
  title        = {Riemannian block SPD coupling manifold and its application to optimal transport},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parametric non-parallel support vector machines for pattern
classification. <em>ML</em>, <em>113</em>(4), 1567–1594. (<a
href="https://doi.org/10.1007/s10994-022-06238-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes Parametric non-parallel support vector machines for binary pattern classification. Through an intelligent redesigning of the Support vector machine optimisation, not only do we bring noise resilience into the model, but also retain its sparsity. Our model exhibits properties similar to Support vector machines, hence many SVM related learning algorithms can be extended to make it scalable for large scale problems. Experimental results on several benchmark UCI datasets validate our claims.},
  archive      = {J_ML},
  author       = {Jain, Sambhav and Rastogi, Reshma},
  doi          = {10.1007/s10994-022-06238-0},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1567-1594},
  shortjournal = {Mach. Learn.},
  title        = {Parametric non-parallel support vector machines for pattern classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial label learning with emerging new labels.
<em>ML</em>, <em>113</em>(4), 1549–1565. (<a
href="https://doi.org/10.1007/s10994-022-06244-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning deals with the problem where each training instance is associated with a set of candidate labels, among which only one is valid. Existing approaches on partial label learning assume that the scale of label space is fixed, however, this assumption may not be satisfied in open and dynamic environment. In this paper, the first attempt towards the problem of partial label learning with emerging new labels is presented. There are mainly three challenges in this task, namely new label detection, effective classification, and efficient model updating. Specifically, a new method is proposed to address these challenges which consists of three parts: (1) An ensemble-based detector that identifies instances from new labels while also assigns candidate labels to instances which may belong to known labels. (2) An effective classification mechanism that involves data pool construction and label disambiguation process. (3) An efficient updating procedure that adapts both the detector and classifier to new labels without training from scratch. Our experiments on artificial and real-world partial label data sets validate the effectiveness of the proposed method in dealing with emerging labels for partial label learning.},
  archive      = {J_ML},
  author       = {Yu, Xiang-Ru and Wang, Deng-Bao and Zhang, Min-Ling},
  doi          = {10.1007/s10994-022-06244-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1549-1565},
  shortjournal = {Mach. Learn.},
  title        = {Partial label learning with emerging new labels},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hitting the target: Stopping active learning at the
cost-based optimum. <em>ML</em>, <em>113</em>(4), 1529–1547. (<a
href="https://doi.org/10.1007/s10994-022-06253-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning allows machine learning models to be trained using fewer labels while retaining similar performance to traditional supervised learning. An active learner selects the most informative data points, requests their labels, and retrains itself. While this approach is promising, it raises the question of how to determine when the model is ‘good enough’ without the additional labels required for traditional evaluation. Previously, different stopping criteria have been proposed aiming to identify the optimal stopping point. Yet, optimality can only be expressed as a domain-dependent trade-off between accuracy and the number of labels, and no criterion is superior in all applications. As a further complication, a comparison of criteria for a particular real-world application would require practitioners to collect additional labelled data they are aiming to avoid by using active learning in the first place. This work enables practitioners to employ active learning by providing actionable recommendations for which stopping criteria are best for a given real-world scenario. We contribute the first large-scale comparison of stopping criteria for pool-based active learning, using a cost measure to quantify the accuracy/label trade-off, public implementations of all stopping criteria we evaluate, and an open-source framework for evaluating stopping criteria. Our research enables practitioners to substantially reduce labelling costs by utilizing the stopping criterion which best suits their domain.},
  archive      = {J_ML},
  author       = {Pullar-Strecker, Zac and Dost, Katharina and Frank, Eibe and Wicker, Jörg},
  doi          = {10.1007/s10994-022-06253-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1529-1547},
  shortjournal = {Mach. Learn.},
  title        = {Hitting the target: Stopping active learning at the cost-based optimum},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box bayesian adversarial attack with transferable
priors. <em>ML</em>, <em>113</em>(4), 1511–1528. (<a
href="https://doi.org/10.1007/s10994-022-06251-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to adversarial attacks, even in the black-box setting, where the attacker only has query access to the model. The most popular black-box adversarial attacks usually rely on substitute models or gradient estimation to generate imperceptible adversarial examples, which either suffer from low attack success rates or low query efficiency. In real-world scenarios, it is extremely improbable for an attacker to have unlimited bandwidth to query a target classifier. In this paper, we proposed a query efficient gradient-free score-based attack, named BO-ATP, which combines Bayesian optimization strategy with transfer-based attacks and searches for perturbation in low-dimensional latent space. Different from the gradient-based method, in the search process, our attack makes full use of the prior information obtained from the previous query to sample the next optimal point instead of local gradient approximation. Results on MNIST, CIFAR10, and ImageNet show that even at a low 1000 query budget, we still achieve high attack success rates in both targeted and untargeted attacks, and the query efficiency is dozens of times higher than the previous state-of-the-art attack methods. Furthermore, we show that BO-ATP can successfully attack some state-of-the-art defenses, such as adversarial training.},
  archive      = {J_ML},
  author       = {Zhang, Shudong and Gao, Haichang and Shu, Chao and Cao, Xiwen and Zhou, Yunyi and He, Jianping},
  doi          = {10.1007/s10994-022-06251-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1511-1528},
  shortjournal = {Mach. Learn.},
  title        = {Black-box bayesian adversarial attack with transferable priors},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Learning to bid and rank together in
recommendation systems. <em>ML</em>, <em>113</em>(3), 1509. (<a
href="https://doi.org/10.1007/s10994-023-06496-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Ji, Geng and Jiang, Wentao and Li, Jiang and Fahid, Fahmid Morshed and Chen, Zhengxing and Li, Yinghua and Xiao, Jun and Bao, Chongxi and Zhu, Zheqing},
  doi          = {10.1007/s10994-023-06496-6},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1509},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Learning to bid and rank together in recommendation systems},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast deep mixtures of gaussian process experts. <em>ML</em>,
<em>113</em>(3), 1483–1508. (<a
href="https://doi.org/10.1007/s10994-023-06491-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of experts have become an indispensable tool for flexible modelling in a supervised learning context, allowing not only the mean function but the entire density of the output to change with the inputs. Sparse Gaussian processes (GP) have shown promise as a leading candidate for the experts in such models, and in this article, we propose to design the gating network for selecting the experts from such mixtures of sparse GPs using a deep neural network (DNN). Furthermore, a fast one pass algorithm called Cluster–Classify–Regress (CCR) is leveraged to approximate the maximum a posteriori (MAP) estimator extremely quickly. This powerful combination of model and algorithm together delivers a novel method which is flexible, robust, and extremely efficient. In particular, the method is able to outperform competing methods in terms of accuracy and uncertainty quantification. The cost is competitive on low-dimensional and small data sets, but is significantly lower for higher-dimensional and big data sets. Iteratively maximizing the distribution of experts given allocations and allocations given experts does not provide significant improvement, which indicates that the algorithm achieves a good approximation to the local MAP estimator very fast. This insight can be useful also in the context of other mixture of experts models.},
  archive      = {J_ML},
  author       = {Etienam, Clement and Law, Kody J. H. and Wade, Sara and Zankin, Vitaly},
  doi          = {10.1007/s10994-023-06491-x},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1483-1508},
  shortjournal = {Mach. Learn.},
  title        = {Fast deep mixtures of gaussian process experts},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online semi-supervised learning of composite event rules by
combining structure and mass-based predicate similarity. <em>ML</em>,
<em>113</em>(3), 1445–1481. (<a
href="https://doi.org/10.1007/s10994-023-06447-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic event recognition systems detect event occurrences using first-order logic rules. Although existing online structure learning approaches ease the discovery of such rules in noisy data streams, they assume the existence of fully labelled training data. Splice is a recent online graph-based approach that estimates the labels of unlabelled data and makes it possible to learn such rules from semi-supervised training sequences of logical interpretations. However, Splice labelling depends significantly on the metric used to compute the distances of unlabelled examples to their labelled counterparts. Moreover, there is no guarantee about the quality of the labelling found in the local graphs that are built as the data stream in. In this paper, we propose a new online learning method, which includes an enhanced hybrid measure that combines an optimised structural distance, and a data-driven one. The former is guided by feature selection targeted to kNN classification, while the latter is a mass-based dissimilarity. Additionally, the enhanced Splice method, improves the graph construction process, by storing a synopsis of the past, in order to achieve more informed labelling on the local graphs. We evaluate our approach by learning Event Calculus theories for the tasks of human activity recognition, maritime monitoring, and fleet management. The evaluation suggests that our approach outperforms its predecessor, in terms of inferring the missing labels and improving the predictive accuracy of the underlying structure learning system.},
  archive      = {J_ML},
  author       = {Michelioudakis, Evangelos and Artikis, Alexander and Paliouras, Georgios},
  doi          = {10.1007/s10994-023-06447-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1445-1481},
  shortjournal = {Mach. Learn.},
  title        = {Online semi-supervised learning of composite event rules by combining structure and mass-based predicate similarity},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principled diverse counterfactuals in multilinear models.
<em>ML</em>, <em>113</em>(3), 1421–1443. (<a
href="https://doi.org/10.1007/s10994-023-06411-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) applications have automated numerous real-life tasks, improving both private and public life. However, the black-box nature of many state-of-the-art models poses the challenge of model verification; how can one be sure that the algorithm bases its decisions on the proper criteria, or that it does not discriminate against certain minority groups? In this paper we propose a way to generate diverse counterfactual explanations from multilinear models, a broad class which includes Random Forests, as well as Bayesian Networks.},
  archive      = {J_ML},
  author       = {Papantonis, Ioannis and Belle, Vaishak},
  doi          = {10.1007/s10994-023-06411-z},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1421-1443},
  shortjournal = {Mach. Learn.},
  title        = {Principled diverse counterfactuals in multilinear models},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persistence b-spline grids: Stable vector representation of
persistence diagrams based on data fitting. <em>ML</em>,
<em>113</em>(3), 1373–1420. (<a
href="https://doi.org/10.1007/s10994-023-06492-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many attempts have been made in recent decades to integrate machine learning (ML) and topological data analysis. A prominent problem in applying persistent homology to ML tasks is finding a vector representation of a persistence diagram (PD), which is a summary diagram for representing topological features. From the perspective of data fitting, a stable vector representation, namely, persistence B-spline grid (PBSG), is developed based on the efficient technique of progressive-iterative approximation for least-squares B-spline function fitting. We theoretically prove that the PBSG method is stable with respect to the metric of 1-Wasserstein distance defined on the PD space. The developed method was tested on a synthetic data set, data sets of randomly generated PDs, data of a dynamical system, and 3D CAD models, showing its effectiveness and efficiency.},
  archive      = {J_ML},
  author       = {Dong, Zhetong and Lin, Hongwei and Zhou, Chi and Zhang, Ben and Li, Gengchen},
  doi          = {10.1007/s10994-023-06492-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1373-1420},
  shortjournal = {Mach. Learn.},
  title        = {Persistence B-spline grids: Stable vector representation of persistence diagrams based on data fitting},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling PU learning using probabilistic logic programming.
<em>ML</em>, <em>113</em>(3), 1351–1372. (<a
href="https://doi.org/10.1007/s10994-023-06461-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of learning from positive and unlabeled (PU) examples is to learn a classifier that predicts the posterior class probability. The challenge is that the available labels in the data are determined by (1) the true class, and (2) the labeling mechanism that selects which positive examples get labeled, where often certain examples have a higher probability to be selected than others. Incorrectly assuming an unbiased labeling mechanism leads to learning a biased classifier. Yet, this is what most existing methods do. A handful of methods makes more realistic assumptions, but they are either so general that it is impossible to distinguish between the effects of the true classification and of the labeling mechanism, or too restrictive to correctly model the real situation, or require knowledge that is typically unavailable. This paper studies how to formulate and integrate more realistic assumptions for learning better classifiers, by exploiting the strengths of probabilistic logic programming (PLP). Concretely, (1) we propose PU ProbLog: a PLP-based general method that allows to (partially) model the labeling mechanism. (2) We show that our method generalizes existing methods, in the sense that it can model the same assumptions. (3) Thanks to the use of PLP, our method supports also PU learning in relational domains. (4) Our empirical analysis shows that partially modeling the labeling bias, improves the learned classifiers.},
  archive      = {J_ML},
  author       = {Verreet, Victor and De Raedt, Luc and Bekker, Jessa},
  doi          = {10.1007/s10994-023-06461-3},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1351-1372},
  shortjournal = {Mach. Learn.},
  title        = {Modeling PU learning using probabilistic logic programming},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural causal models reveal confounder bias in linear
program modelling. <em>ML</em>, <em>113</em>(3), 1329–1349. (<a
href="https://doi.org/10.1007/s10994-023-06431-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent years have been marked by extended research on adversarial attacks, especially on deep neural networks. With this work we intend on posing and investigating the question of whether the phenomenon might be more general in nature, that is, adversarial-style attacks outside classical classification tasks. Specifically, we investigate optimization problems as they constitute a fundamental part of modern AI research. To this end, we consider the base class of optimizers namely Linear Programs (LPs). On our initial attempt of a naïve mapping between the formalism of adversarial examples and LPs, we quickly identify the key ingredients missing for making sense of a reasonable notion of adversarial examples for LPs. Intriguingly, the formalism of Pearl’s notion to causality allows for the right description of adversarial like examples for LPs. Characteristically, we show the direct influence of the Structural Causal Model (SCM) onto the subsequent LP optimization, which ultimately exposes a notion of confounding in LPs (inherited by said SCM) that allows for adversarial-style attacks. We provide both the general proof formally alongside existential proofs of such intriguing LP-parameterizations based on SCM for three combinatorial problems, namely Linear Assignment, Shortest Path and a real world problem of energy systems.},
  archive      = {J_ML},
  author       = {Zečević, Matej and Dhami, Devendra Singh and Kersting, Kristian},
  doi          = {10.1007/s10994-023-06431-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1329-1349},
  shortjournal = {Mach. Learn.},
  title        = {Structural causal models reveal confounder bias in linear program modelling},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable models via compression of tree ensembles.
<em>ML</em>, <em>113</em>(3), 1303–1328. (<a
href="https://doi.org/10.1007/s10994-023-06463-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble models (bagging and gradient-boosting) of relational decision trees have proved to be some of the most effective learning methods in the area of probabilistic logic models (PLMs). While effective, they lose one of the most important benefits of PLMs—interpretability. In this paper we consider the problem of compressing a large set of learned trees into a single explainable model. To this effect, we propose CoTE—Compression of Tree Ensembles—that produces a single small decision list as a compressed representation. CoTE first converts the trees to decision lists and then performs the combination and compression with the aid of the original training set. An experimental evaluation demonstrates the effectiveness of CoTE in several benchmark relational data sets.},
  archive      = {J_ML},
  author       = {Yan, Siwen and Natarajan, Sriraam and Joshi, Saket and Khardon, Roni and Tadepalli, Prasad},
  doi          = {10.1007/s10994-023-06463-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1303-1328},
  shortjournal = {Mach. Learn.},
  title        = {Explainable models via compression of tree ensembles},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Word embeddings-based transfer learning for boosted
relational dependency networks. <em>ML</em>, <em>113</em>(3), 1269–1302.
(<a href="https://doi.org/10.1007/s10994-023-06404-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional machine learning methods assume data to be independent and identically distributed (i.i.d.) and ignore the relational structure of the data, which contains crucial information about how objects participate in relationships and events. Statistical Relational Learning (SRL) combines elements from statistical and probabilistic modeling to relational learning to represent and learn in domains with complex relational and rich probabilistic structures. SRL models do not suppose data to be i.i.d., but, as conventional machine learning models, they also assume training and testing data are sampled from the same distribution. Transfer learning has emerged as an essential technique to handle scenarios where such an assumption does not hold. It aims to provide methods with the ability to recognize knowledge previously learned in a source domain and apply it to a new model in a target domain to start solving a new task. For SRL models, the primary challenge is to transfer the learned structure, mapping the vocabulary across different domains. In this work, we propose TransBoostler, an algorithm that uses pre-trained word embeddings to guide the mapping. We follow the assumption that the name of a predicate has a semantic connotation that can be mapped to a vector space model. Next, TransBoostler employs theory revision to adapt the mapped model to the target data. Experimental results showed that TransBoostler successfully transferred trees across different domains. It performs equally well as, or better than, previous systems and requires less training time for some investigated scenarios.},
  archive      = {J_ML},
  author       = {Luca, Thais and Paes, Aline and Zaverucha, Gerson},
  doi          = {10.1007/s10994-023-06404-y},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1269-1302},
  shortjournal = {Mach. Learn.},
  title        = {Word embeddings-based transfer learning for boosted relational dependency networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OT-net: A reusable neural optimal transport solver.
<em>ML</em>, <em>113</em>(3), 1243–1268. (<a
href="https://doi.org/10.1007/s10994-023-06493-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of optimal transport (OT), its calculation becomes essential, and various algorithms have emerged. However, the existing methods either have low efficiency or cannot represent discontinuous maps. A novel reusable neural OT solver OT-Net is thus presented, which first learns Brenier’s height representation via the neural network to get its potential, and then obtains the OT map by the gradient of the potential. The algorithm has two merits: (1) When new target samples are added, the OT map can be calculated straightly, which greatly improves the efficiency and reusability of the map. (2) It can easily represent discontinuous maps, which allows it to match any target distribution with discontinuous supports and achieve sharp boundaries, and thus eliminate mode collapse. Moreover, we conducted error analyses on the proposed algorithm and demonstrated the empirical success of our approach in image generation, color transfer, and domain adaptation.},
  archive      = {J_ML},
  author       = {Li, Zezeng and Li, Shenghao and Jin, Lianbao and Lei, Na and Luo, Zhongxuan},
  doi          = {10.1007/s10994-023-06493-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1243-1268},
  shortjournal = {Mach. Learn.},
  title        = {OT-net: A reusable neural optimal transport solver},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilising energy function and variational inference training
for learning a graph neural network architecture. <em>ML</em>,
<em>113</em>(3), 1219–1241. (<a
href="https://doi.org/10.1007/s10994-024-06513-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, the field of deep learning has demonstrated significant advancements, resulting in the enhancement of all machine learning tasks, ranging from image and video processing to natural language understanding and speech recognition. However, conventional deep learning models like Convolutional NEURAL NETWORKS (CNNs) face limitations in processing real-world data that possess arbitrary shapes. Graphs, which are instrumental data structures, offer a solution for modeling such complex non-Euclidean data. Several methods like Statistical relational learning (SRL) and Graph neural networks (GNNs) have made groundbreaking contributions to graph analysis. While GNNs build graphical representations via feature aggregation, SRLs tend to learn inter-dependencies using a combination of probabilistic modelling and logical reasoning. However, these methods offer critical limitations in computational efficiency and stability. This paper presents a novel approach, where the SRL and GNN aspects of graph learning are integrated to create a variational distribution called the Potts-Coulomb variational model (PCVM). By utilizing energy functions, the method effectively captures and leverages the intricate relationships among labels and features within the graphs. This innovative model demonstrates significantly better results than other baseline models and can serve as a benchmark for further innovative research. The model can be extensively used for multiple applications like node classification, link prediction etc. It also offers high flexibility for training as the basic framework can be effortlessly modified according to user requirements.},
  archive      = {J_ML},
  author       = {Girish, Gayathri and Mishra, Deepak and Moosath, Subrahamanian K. S.},
  doi          = {10.1007/s10994-024-06513-2},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1219-1241},
  shortjournal = {Mach. Learn.},
  title        = {Utilising energy function and variational inference training for learning a graph neural network architecture},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the effects of biased quantum random numbers on the
initialization of artificial neural networks. <em>ML</em>,
<em>113</em>(3), 1189–1217. (<a
href="https://doi.org/10.1007/s10994-023-06490-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in practical quantum computing have led to a variety of cloud-based quantum computing platforms that allow researchers to evaluate their algorithms on noisy intermediate-scale quantum devices. A common property of quantum computers is that they can exhibit instances of true randomness as opposed to pseudo-randomness obtained from classical systems. Investigating the effects of such true quantum randomness in the context of machine learning is appealing, and recent results vaguely suggest that benefits can indeed be achieved from the use of quantum random numbers. To shed some more light on this topic, we empirically study the effects of hardware-biased quantum random numbers on the initialization of artificial neural network weights in numerical experiments. We find no statistically significant difference in comparison with unbiased quantum random numbers as well as biased and unbiased random numbers from a classical pseudo-random number generator. The quantum random numbers for our experiments are obtained from real quantum hardware.},
  archive      = {J_ML},
  author       = {Heese, Raoul and Wolter, Moritz and Mücke, Sascha and Franken, Lukas and Piatkowski, Nico},
  doi          = {10.1007/s10994-023-06490-y},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1189-1217},
  shortjournal = {Mach. Learn.},
  title        = {On the effects of biased quantum random numbers on the initialization of artificial neural networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). No regret sample selection with noisy labels. <em>ML</em>,
<em>113</em>(3), 1163–1188. (<a
href="https://doi.org/10.1007/s10994-023-06478-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) suffer from noisy-labeled data because of the risk of overfitting. To avoid the risk, in this paper, we propose a novel DNN training method with sample selection based on adaptive k-set selection, which selects k (&lt; n, where n is the number of training samples) samples with a small noise-risk from the whole n noisy training samples at each epoch. It has the strong advantage of guaranteeing the performance of the selection theoretically. Roughly speaking, a regret, which is defined by the difference between the actual selection and the best selection, of the proposed method is theoretically bounded, even though the best selection is unknown until the end of all epochs. The experimental results on multiple noisy-labeled datasets demonstrate that our sample selection strategy works effectively in the DNN training; in fact, the proposed method achieved the best or the second-best performance among state-of-the-art methods, while requiring a significantly lower computational cost.},
  archive      = {J_ML},
  author       = {Song, Heon and Mitsuo, Nariaki and Uchida, Seiichi and Suehiro, Daiki},
  doi          = {10.1007/s10994-023-06478-8},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1163-1188},
  shortjournal = {Mach. Learn.},
  title        = {No regret sample selection with noisy labels},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Differentially private riemannian optimization.
<em>ML</em>, <em>113</em>(3), 1133–1161. (<a
href="https://doi.org/10.1007/s10994-023-06508-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the differentially private empirical risk minimization problem where the parameter is constrained to a Riemannian manifold. We introduce a framework for performing differentially private Riemannian optimization by adding noise to the Riemannian gradient on the tangent space. The noise follows a Gaussian distribution intrinsically defined with respect to the Riemannian metric on the tangent space. We adapt the Gaussian mechanism from the Euclidean space to the tangent space compatible to such generalized Gaussian distribution. This approach presents a novel analysis as compared to directly adding noise on the manifold. We further prove privacy guarantees of the proposed differentially private Riemannian (stochastic) gradient descent using an extension of the moments accountant technique. Overall, we provide utility guarantees under geodesic (strongly) convex, general nonconvex objectives as well as under the Riemannian Polyak-Łojasiewicz condition. Empirical results illustrate the versatility and efficacy of the proposed framework in several applications.},
  archive      = {J_ML},
  author       = {Han, Andi and Mishra, Bamdev and Jawanpuria, Pratik and Gao, Junbin},
  doi          = {10.1007/s10994-023-06508-5},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1133-1161},
  shortjournal = {Mach. Learn.},
  title        = {Differentially private riemannian optimization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composition of relational features with an application to
explaining black-box predictors. <em>ML</em>, <em>113</em>(3),
1091–1132. (<a
href="https://doi.org/10.1007/s10994-023-06399-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three key strengths of relational machine learning programs like those developed in Inductive Logic Programming (ILP) are: (1) The use of an expressive subset of first-order logic that allows models that capture complex relationships amongst data instances; (2) The use of domain-specific relations to guide the construction of models; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. The price for these advantages is that ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. Motivated by the work of McCreath and Sharma, we formulate the notion of a set of $$\textrm{M}$$ -simple features in a mode language $$\textrm{M}$$ and identify two composition operators ( $$\rho _1$$ and $$\rho _2$$ ) from which all possible complex features can be derived. We use these results to implement a form of “explainable neural network” called Compositional Relational Machines, or CRMs. CRMs are labelled directed-acyclic graphs. The vertex-label for any vertex j in the CRM contains a feature-function $$f_j$$ and an continuous activation function $$g_j$$ . If j is a “non-input” vertex, then $$f_j$$ is the composition of features associated with vertices in the direct predecessors of j. Our focus is on CRMs in which input vertices (those without any direct predecessors) all have $$\textrm{M}$$ -simple features in their vertex-labels. We provide a randomised procedure for constructing the structure of such CRMs, and a procedure for estimating the parameters (the $$w_{ij}$$ ’s) using back-propagation and stochastic gradient descent. Using a notion of explanations based on the compositional structure of features in a CRM, we provide empirical evidence on synthetic data of the ability to identify appropriate explanations; and demonstrate the use of CRMs as ‘explanation machines’ for black-box models that do not provide explanations for their predictions.},
  archive      = {J_ML},
  author       = {Srinivasan, Ashwin and Baskar, A. and Dash, Tirtharaj and Shah, Devanshu},
  doi          = {10.1007/s10994-023-06399-6},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1091-1132},
  shortjournal = {Mach. Learn.},
  title        = {Composition of relational features with an application to explaining black-box predictors},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving fraud detection via imbalanced graph structure
learning. <em>ML</em>, <em>113</em>(3), 1069–1090. (<a
href="https://doi.org/10.1007/s10994-023-06464-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based fraud detection methods have recently attracted much attention due to the rich relational information of graph-structured data, which may facilitate the detection of fraudsters. However, the GNN-based algorithms may exhibit unsatisfactory performance faced with graph heterophily as the fraudsters usually disguise themselves by deliberately making extensive connections to normal users. In addition to this, the class imbalance problem also causes GNNs to overfit normal users and perform poorly for fraudsters. To address these problems, we propose an Imbalanced Graph Structure Learning framework for fraud detection (IGSL for short). Specifically, nodes are picked with a devised multi-relational class-balanced sampler for mini-batch training. Then, an iterative graph structure learning module is proposed to iteratively construct a global homophilic adjacency matrix in the embedding domain. Further, an anchor node message passing mechanism is proposed to reduce the computational complexity of the constructing homophily adjacency matrix. Extensive experiments on benchmark datasets show that IGSL achieves significantly better performance even when the graph is heavily heterophilic and imbalanced.},
  archive      = {J_ML},
  author       = {Ren, Lingfei and Hu, Ruimin and Liu, Yang and Li, Dengshi and Wu, Junhang and Zang, Yilong and Hu, Wenyi},
  doi          = {10.1007/s10994-023-06464-0},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1069-1090},
  shortjournal = {Mach. Learn.},
  title        = {Improving fraud detection via imbalanced graph structure learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial counterfactual identification and uplift modeling:
Theoretical results and real-world assessment. <em>ML</em>,
<em>113</em>(3), 1043–1067. (<a
href="https://doi.org/10.1007/s10994-023-06317-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactuals are central in causal human reasoning and the scientific discovery process. The uplift, also called conditional average treatment effect, measures the causal effect of some action, or treatment, on the outcome of an individual. This paper discusses how it is possible to derive bounds on the probability of counterfactual statements based on uplift terms. First, we derive some original bounds on the probability of counterfactuals and we show that tightness of such bounds depends on the information of the feature set on the uplift term. Then, we propose a point estimator based on the assumption of conditional independence between the counterfactual outcomes. The quality of the bounds and the point estimators are assessed on synthetic data and a large real-world customer data set provided by a telecom company, showing significant improvement over the state of the art.},
  archive      = {J_ML},
  author       = {Verhelst, Théo and Mercier, Denis and Shrestha, Jeevan and Bontempi, Gianluca},
  doi          = {10.1007/s10994-023-06317-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1043-1067},
  shortjournal = {Mach. Learn.},
  title        = {Partial counterfactual identification and uplift modeling: Theoretical results and real-world assessment},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing model-agnostic random subspace ensembles.
<em>ML</em>, <em>113</em>(2), 993–1042. (<a
href="https://doi.org/10.1007/s10994-023-06427-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a model-agnostic ensemble approach for supervised learning. The proposed approach is based on a parametric version of Random Subspace, in which each base model is learned from a feature subset sampled according to a Bernoulli distribution. Parameter optimization is performed using gradient descent and is rendered tractable by using an importance sampling approach that circumvents frequent re-training of the base models after each gradient descent step. The degree of randomization in our parametric Random Subspace is thus automatically tuned through the optimization of the feature selection probabilities. This is an advantage over the standard Random Subspace approach, where the degree of randomization is controlled by a hyper-parameter. Furthermore, the optimized feature selection probabilities can be interpreted as feature importance scores. Our algorithm can also easily incorporate any differentiable regularization term to impose constraints on these importance scores. We show the good performance of the proposed approach, both in terms of prediction and feature ranking, on simulated and real-world datasets. We also show that PRS can be successfully used for the reconstruction of gene regulatory networks.},
  archive      = {J_ML},
  author       = {Huynh-Thu, Vân Anh and Geurts, Pierre},
  doi          = {10.1007/s10994-023-06427-5},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {993-1042},
  shortjournal = {Mach. Learn.},
  title        = {Optimizing model-agnostic random subspace ensembles},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LogicDT: A procedure for identifying response-associated
interactions between binary predictors. <em>ML</em>, <em>113</em>(2),
933–992. (<a href="https://doi.org/10.1007/s10994-023-06488-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions between predictors play an important role in many applications. Popular and successful tree-based supervised learning methods such as random forests or logic regression can incorporate interactions associated with the considered outcome without specifying which variables might interact. Nonetheless, these algorithms suffer from certain drawbacks such as limited interpretability of model predictions and difficulties with negligible marginal effects in the case of random forests or not being able to incorporate interactions with continuous variables, being restricted to additive structures between Boolean terms, and not directly considering conjunctions that reveal the interactions in the case of logic regression. We, therefore, propose a novel method called logic decision trees (logicDT) that is specifically tailored to binary input data and helps to overcome the drawbacks of existing methods. The main idea consists of considering sets of Boolean conjunctions, using these terms as input variables for decision trees, and searching for the best performing model. logicDT is also accompanied by a framework for estimating the importance of identified terms, i.e., input variables and interactions between input variables. This new method is compared to other popular statistical learning algorithms in simulations and real data applications. As these evaluations show, logicDT is able to yield high prediction performances while maintaining interpretability.},
  archive      = {J_ML},
  author       = {Lau, Michael and Schikowski, Tamara and Schwender, Holger},
  doi          = {10.1007/s10994-023-06488-6},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {933-992},
  shortjournal = {Mach. Learn.},
  title        = {LogicDT: A procedure for identifying response-associated interactions between binary predictors},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous multi-task feature learning with mixed <span
class="math display"><em>ℓ</em><sub>2, 1</sub></span> regularization.
<em>ML</em>, <em>113</em>(2), 891–932. (<a
href="https://doi.org/10.1007/s10994-023-06410-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration is the process of extracting information from multiple sources and jointly analyzing different data sets. In this paper, we propose to use the mixed $$\ell _{2,1}$$ regularized composite quasi-likelihood function to perform multi-task feature learning with different types of responses, including continuous and discrete responses. For high dimensional settings, our result establishes the sign recovery consistency and estimation error bounds of the penalized estimates under regularity conditions. Simulation studies and real data analysis examples are provided to illustrate the utility of the proposed method to combine correlated platforms with heterogeneous tasks and perform joint sparse estimation.},
  archive      = {J_ML},
  author       = {Zhong, Yuan and Xu, Wei and Gao, Xin},
  doi          = {10.1007/s10994-023-06410-0},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {891-932},
  shortjournal = {Mach. Learn.},
  title        = {Heterogeneous multi-task feature learning with mixed $$\ell _{2,1}$$ regularization},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted adversarial attacks on wind power forecasts.
<em>ML</em>, <em>113</em>(2), 863–889. (<a
href="https://doi.org/10.1007/s10994-023-06396-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semi-targeted, and untargeted adversarial attacks. We consider a long short-term memory (LSTM) network for predicting the power generation of individual wind farms and a convolutional neural network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to targeted and semi-targeted adversarial attacks. It assesses the impact of attacks on the model’s performance, as well as the extent to which the attacker’s goal was achieved, by assigning a score between 0 (very vulnerable) and 1 (very robust). In our experiments, the LSTM forecasting model was fairly robust and achieved a TARS value of over 0.78 for all adversarial attacks investigated. The CNN forecasting model only achieved TARS values below 0.10 when trained ordinarily, and was thus very vulnerable. Yet, its robustness could be significantly improved by adversarial training, which always resulted in a TARS above 0.46.},
  archive      = {J_ML},
  author       = {Heinrich, René and Scholz, Christoph and Vogt, Stephan and Lehna, Malte},
  doi          = {10.1007/s10994-023-06396-9},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {863-889},
  shortjournal = {Mach. Learn.},
  title        = {Targeted adversarial attacks on wind power forecasts},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automotive fault nowcasting with machine learning and
natural language processing. <em>ML</em>, <em>113</em>(2), 843–861. (<a
href="https://doi.org/10.1007/s10994-023-06398-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated fault diagnosis can facilitate diagnostics assistance, speedier troubleshooting, and better-organised logistics. Currently, most AI-based prognostics and health management in the automotive industry ignore textual descriptions of the experienced problems or symptoms. With this study, however, we propose an ML-assisted workflow for automotive fault nowcasting that improves on current industry standards. We show that a multilingual pre-trained Transformer model can effectively classify the textual symptom claims from a large company with vehicle fleets, despite the task’s challenging nature due to the 38 languages and 1357 classes involved. Overall, we report an accuracy of more than 80% for high-frequency classes and above 60% for classes with reasonable minimum support, bringing novel evidence that automotive troubleshooting management can benefit from multilingual symptom text classification.},
  archive      = {J_ML},
  author       = {Pavlopoulos, John and Romell, Alv and Curman, Jacob and Steinert, Olof and Lindgren, Tony and Borg, Markus and Randl, Korbinian},
  doi          = {10.1007/s10994-023-06398-7},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {843-861},
  shortjournal = {Mach. Learn.},
  title        = {Automotive fault nowcasting with machine learning and natural language processing},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep doubly robust outcome weighted learning. <em>ML</em>,
<em>113</em>(2), 815–842. (<a
href="https://doi.org/10.1007/s10994-023-06484-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is a framework that adapts treatment strategies to a patient’s individual characteristics and provides helpful clinical decision support. Existing research has been extended to various situations but high-dimensional data have not yet been fully incorporated into the paradigm. We propose a new precision medicine approach called deep doubly robust outcome weighted learning (DDROWL) that can handle big and complex data. This is a machine learning tool that directly estimates the optimal decision rule and achieves the best of three worlds: deep learning, double robustness, and residual weighted learning. Two architectures have been implemented in the proposed method, a fully-connected feedforward neural network and the Deep Kernel Learning model, a Gaussian process with deep learning-filtered inputs. We compare and discuss the performance and limitation of different methods through a range of simulations. Using longitudinal and brain imaging data from patients with Alzheimer’s disease, we demonstrate the application of the proposed method in real-world clinical practice. With the implementation of deep learning, the proposed method can expand the influence of precision medicine to high-dimensional abundant data with greater flexibility and computational power.},
  archive      = {J_ML},
  author       = {Jiang, Xiaotong and Zhou, Xin and Kosorok, Michael R.},
  doi          = {10.1007/s10994-023-06484-w},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {815-842},
  shortjournal = {Mach. Learn.},
  title        = {Deep doubly robust outcome weighted learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markov chain importance sampling for minibatches.
<em>ML</em>, <em>113</em>(2), 789–814. (<a
href="https://doi.org/10.1007/s10994-023-06489-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates importance sampling under the scheme of minibatch stochastic gradient descent, under which the contributions are twofold. First, theoretically, we develop a neat tilting formula, which can be regarded as a general device for asymptotically optimal importance sampling. Second, practically, guided by the formula, we present an effective algorithm for importance sampling which accounts for the effects of minibatches and leverages the Markovian property of the gradients between iterations. Experiments conducted on artificial data confirm that our algorithm consistently delivers superior performance in terms of variance reduction. Furthermore, experiments carried out on real-world data demonstrate that our method, when paired with relatively straightforward models like multilayer perceptron and convolutional neural networks, outperforms in terms of training loss and testing error.},
  archive      = {J_ML},
  author       = {Fuh, Cheng-Der and Wang, Chuan-Ju and Pai, Chen-Hung},
  doi          = {10.1007/s10994-023-06489-5},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {789-814},
  shortjournal = {Mach. Learn.},
  title        = {Markov chain importance sampling for minibatches},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning algorithm through the lens of rejection
arguments. <em>ML</em>, <em>113</em>(2), 753–788. (<a
href="https://doi.org/10.1007/s10994-023-06494-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning is a paradigm of machine learning which aims at reducing the amount of labeled data needed to train a classifier. Its overall principle is to sequentially select the most informative data points, which amounts to determining the uncertainty of regions of the input space. The main challenge lies in building a procedure that is computationally efficient and that offers appealing theoretical properties; most of the current methods satisfy only one or the other. In this paper, we use the classification with rejection in a novel way to estimate the uncertain regions. We provide an active learning algorithm and prove its theoretical benefits under classical assumptions. In addition to the theoretical results, numerical experiments are carried out on synthetic and non-synthetic datasets. These experiments provide empirical evidence that the use of rejection arguments in our active learning algorithm is beneficial and allows good performance in various statistical situations.},
  archive      = {J_ML},
  author       = {Denis, Christophe and Hebiri, Mohamed and Ndjia Njike, Boris and Siebert, Xavier},
  doi          = {10.1007/s10994-023-06494-8},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {753-788},
  shortjournal = {Mach. Learn.},
  title        = {Active learning algorithm through the lens of rejection arguments},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Subspace adaptation prior for few-shot learning.
<em>ML</em>, <em>113</em>(2), 725–752. (<a
href="https://doi.org/10.1007/s10994-023-06393-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based meta-learning techniques aim to distill useful prior knowledge from a set of training tasks such that new tasks can be learned more efficiently with gradient descent. While these methods have achieved successes in various scenarios, they commonly adapt all parameters of trainable layers when learning new tasks. This neglects potentially more efficient learning strategies for a given task distribution and may be susceptible to overfitting, especially in few-shot learning where tasks must be learned from a limited number of examples. To address these issues, we propose Subspace Adaptation Prior (SAP), a novel gradient-based meta-learning algorithm that jointly learns good initialization parameters (prior knowledge) and layer-wise parameter subspaces in the form of operation subsets that should be adaptable. In this way, SAP can learn which operation subsets to adjust with gradient descent based on the underlying task distribution, simultaneously decreasing the risk of overfitting when learning new tasks. We demonstrate that this ability is helpful as SAP yields superior or competitive performance in few-shot image classification settings (gains between 0.1% and 3.9% in accuracy). Analysis of the learned subspaces demonstrates that low-dimensional operations often yield high activation strengths, indicating that they may be important for achieving good few-shot learning performance. For reproducibility purposes, we publish all our research code publicly.},
  archive      = {J_ML},
  author       = {Huisman, Mike and Plaat, Aske and van Rijn, Jan N.},
  doi          = {10.1007/s10994-023-06393-y},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {725-752},
  shortjournal = {Mach. Learn.},
  title        = {Subspace adaptation prior for few-shot learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards accurate knowledge transfer via target-awareness
representation disentanglement. <em>ML</em>, <em>113</em>(2), 699–723.
(<a href="https://doi.org/10.1007/s10994-023-06381-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-tuning deep neural networks pre-trained on large scale datasets is one of the most practical transfer learning paradigm given limited quantity of training samples. To obtain better generalization, using the starting point as the reference (SPAR), either through weights or features, has been successfully applied to transfer learning as a regularizer. However, due to the domain discrepancy between the source and target task, there exists obvious risk of negative transfer in a straightforward manner of knowledge preserving. In this paper, we propose a novel transfer learning algorithm, introducing the idea of Target-awareness REpresentation Disentanglement ( $$\textrm{TRED}$$ ), where the relevant knowledge with respect to the target task is disentangled from the original source model and used as a regularizer during fine-tuning the target model. Two alternative approaches, maximizing Maximum Mean Discrepancy (Max-MMD) and minimizing mutual information (Min-MI) are introduced to achieve the desired disentanglement. Experiments on various real world datasets show that our method stably improves the standard fine-tuning by more than 2% in average. $$\textrm{TRED}$$ also outperforms related state-of-the-art transfer learning regularizers such as $$\mathrm {L^2\text {-}SP}$$ , $$\textrm{AT}$$ , $$\textrm{DELTA}$$ , and $$\textrm{BSS}$$ . Moreover, our solution is compatible with different choices of disentangling strategies. While the combination of Max-MMD and Min-MI typically achieves higher accuracy, only using Max-MMD can be a preferred choice in applications with low resource budgets.},
  archive      = {J_ML},
  author       = {Li, Xingjian and Hu, Di and Li, Xuhong and Xiong, Haoyi and Xu, Chengzhong and Dou, Dejing},
  doi          = {10.1007/s10994-023-06381-2},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {699-723},
  shortjournal = {Mach. Learn.},
  title        = {Towards accurate knowledge transfer via target-awareness representation disentanglement},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-classifier free negative sampling for extreme
multilabel classification. <em>ML</em>, <em>113</em>(2), 675–697. (<a
href="https://doi.org/10.1007/s10994-023-06468-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative sampling is a common approach for making the training of deep models in classification problems with very large output spaces, known as extreme multilabel classification (XMC) problems, tractable. Negative sampling methods aim to find per instance negative labels with higher scores, known as hard negatives, and limit the computations of the negative part of the loss to these labels. Two well-known methods for negative sampling in XMC models are meta-classifier-based and Maximum Inner product Search (MIPS)-based adaptive methods. Owing to their good prediction performance, methods which employ a meta classifier are more common in contemporary XMC research. On the flip side, they need to train and store the meta classifier (apart from the extreme classifier), which can involve millions of additional parameters. In this paper, we focus on the MIPS-based methods for negative sampling. We highlight two issues which may prevent deep models trained by these methods to undergo stable training. First, we argue that using hard negatives excessively from the beginning of training leads to unstable gradient. Second, we show that when all the negative labels in a MIPS-based method are restricted to only those determined by MIPS, training is sensitive to the length of intervals for pre-processing the weights in the MIPS method. To mitigate the aforementioned issues, we propose to limit the labels selected by MIPS to only a few and sample the rest of the needed labels from a uniform distribution. We show that our proposed MIPS-based negative sampling can reach the performance of LightXML, a transformer-based model trained by a meta classifier, while there is no need to train and store any additional classifier. The code for our experiments is available at https://github.com/xmc-aalto/mips-negative-sampling .},
  archive      = {J_ML},
  author       = {Qaraei, Mohammadreza and Babbar, Rohit},
  doi          = {10.1007/s10994-023-06468-w},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {675-697},
  shortjournal = {Mach. Learn.},
  title        = {Meta-classifier free negative sampling for extreme multilabel classification},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based trajectory stitching for improved behavioural
cloning and its applications. <em>ML</em>, <em>113</em>(2), 647–674. (<a
href="https://doi.org/10.1007/s10994-023-06392-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioural cloning (BC) is a commonly used imitation learning method to infer a sequential decision-making policy from expert demonstrations. However, when the quality of the data is not optimal, the resulting behavioural policy also performs sub-optimally once deployed. Recently, there has been a surge in offline reinforcement learning methods that hold the promise to extract high-quality policies from sub-optimal historical data. A common approach is to perform regularisation during training, encouraging updates during policy evaluation and/or policy improvement to stay close to the underlying data. In this work, we investigate whether an offline approach to improving the quality of the existing data can lead to improved behavioural policies without any changes in the BC algorithm. The proposed data improvement approach - Model-Based Trajectory Stitching (MBTS) - generates new trajectories (sequences of states and actions) by ‘stitching’ pairs of states that were disconnected in the original data and generating their connecting new action. By construction, these new transitions are guaranteed to be highly plausible according to probabilistic models of the environment, and to improve a state-value function. We demonstrate that the iterative process of replacing old trajectories with new ones incrementally improves the underlying behavioural policy. Extensive experimental results show that significant performance gains can be achieved using MBTS over BC policies extracted from the original data. Furthermore, using the D4RL benchmarking suite, we demonstrate that state-of-the-art results are obtained by combining MBTS with two existing offline learning methodologies reliant on BC, model-based offline planning (MBOP) and policy constraint (TD3+BC).},
  archive      = {J_ML},
  author       = {Hepburn, Charles A. and Montana, Giovanni},
  doi          = {10.1007/s10994-023-06392-z},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {647-674},
  shortjournal = {Mach. Learn.},
  title        = {Model-based trajectory stitching for improved behavioural cloning and its applications},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Softmin discrete minimax classifier for imbalanced classes
and prior probability shifts. <em>ML</em>, <em>113</em>(2), 605–645. (<a
href="https://doi.org/10.1007/s10994-023-06397-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new approach for dealing with imbalanced classes and prior probability shifts in supervised classification tasks. Coupled with any feature space partitioning method, our criterion aims to compute an almost-Bayesian randomized equalizer classifier for which the maxima of the class-conditional risks are minimized. Our approach belongs to the historically well-studied field of randomized minimax criteria. Our new criterion can be considered as a self-sufficient classifier, or can be easily coupled with any pretrained Convolutional Neural Networks and Decision Trees to address the issues of imbalanced classes and prior probability shifts. Numerical experiments compare our criterion to several state-of-the-art algorithms and show the relevance of our approach when it is necessary to well classify the minority classes and to equalize the risks per class. Experiments on the CIFAR-100 database show that our criterion scales well when the number of classes is large.},
  archive      = {J_ML},
  author       = {Gilet, Cyprien and Guyomard, Marie and Destercke, Sébastien and Fillatre, Lionel},
  doi          = {10.1007/s10994-023-06397-8},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {605-645},
  shortjournal = {Mach. Learn.},
  title        = {Softmin discrete minimax classifier for imbalanced classes and prior probability shifts},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for the practical disintegration of
PAC-bayesian bounds. <em>ML</em>, <em>113</em>(2), 519–604. (<a
href="https://doi.org/10.1007/s10994-023-06391-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PAC-Bayesian bounds are known to be tight and informative when studying the generalization ability of randomized classifiers. However, they require a loose and costly derandomization step when applied to some families of deterministic models such as neural networks. As an alternative to this step, we introduce new PAC-Bayesian generalization bounds that have the originality to provide disintegrated bounds, i.e., they give guarantees over one single hypothesis instead of the usual averaged analysis. Our bounds are easily optimizable and can be used to design learning algorithms. We illustrate this behavior on neural networks, and we show a significant practical improvement over the state-of-the-art framework.},
  archive      = {J_ML},
  author       = {Viallard, Paul and Germain, Pascal and Habrard, Amaury and Morvant, Emilie},
  doi          = {10.1007/s10994-023-06391-0},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {519-604},
  shortjournal = {Mach. Learn.},
  title        = {A general framework for the practical disintegration of PAC-bayesian bounds},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Upper and lower bounds for complete linkage in general
metric spaces. <em>ML</em>, <em>113</em>(1), 489–518. (<a
href="https://doi.org/10.1007/s10994-023-06486-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a hierarchical clustering problem the task is to compute a series of mutually compatible clusterings of a finite metric space $$(P,{{\,\textrm{dist}\,}})$$ . Starting with the clustering where every point forms its own cluster, one iteratively merges two clusters until only one cluster remains. Complete linkage is a well-known and popular algorithm to compute such clusterings: in every step it merges the two clusters whose union has the smallest radius (or diameter) among all currently possible merges. We prove that the radius (or diameter) of every k-clustering computed by complete linkage is at most by factor O(k) (or $$O(k^{\ln (3)/\ln (2)})=O(k^{1{.}59})$$ ) worse than an optimal k-clustering minimizing the radius (or diameter). Furthermore we give a negative answer to the question proposed by Dasgupta and Long (J Comput Syst Sci 70(4):555–569, 2005. https://doi.org/10.1016/j.jcss.2004.10.006 ), who show a lower bound of $$\Omega (\log (k))$$ and ask if the approximation guarantee is in fact $$\Theta (\log (k))$$ . We present instances where complete linkage performs poorly in the sense that the k-clustering computed by complete linkage is off by a factor of $$\Omega (k)$$ from an optimal solution for radius and diameter. We conclude that in general metric spaces complete linkage does not perform asymptotically better than single linkage, merging the two clusters with smallest inter-cluster distance, for which we prove an approximation guarantee of O(k).},
  archive      = {J_ML},
  author       = {Arutyunova, Anna and Großwendt, Anna and Röglin, Heiko and Schmidt, Melanie and Wargalla, Julian},
  doi          = {10.1007/s10994-023-06486-8},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {489-518},
  shortjournal = {Mach. Learn.},
  title        = {Upper and lower bounds for complete linkage in general metric spaces},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing policy constraint and ensemble size in
uncertainty-based offline reinforcement learning. <em>ML</em>,
<em>113</em>(1), 443–488. (<a
href="https://doi.org/10.1007/s10994-023-06458-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning agents seek optimal policies from fixed data sets. With environmental interaction prohibited, agents face significant challenges in preventing errors in value estimates from compounding and subsequently causing the learning process to collapse. Uncertainty estimation using ensembles compensates for this by penalising high-variance value estimates, allowing agents to learn robust policies based on data-driven actions. However, the requirement for large ensembles to facilitate sufficient penalisation results in significant computational overhead. In this work, we examine the role of policy constraints as a mechanism for regulating uncertainty, and the corresponding balance between level of constraint and ensemble size. By incorporating behavioural cloning into policy updates, we show empirically that sufficient penalisation can be achieved with a much smaller ensemble size, substantially reducing computational demand while retaining state-of-the-art performance on benchmarking tasks. Furthermore, we show how such an approach can facilitate stable online fine tuning, allowing for continued policy improvement while avoiding severe performance drops.},
  archive      = {J_ML},
  author       = {Beeson, Alex and Montana, Giovanni},
  doi          = {10.1007/s10994-023-06458-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {443-488},
  shortjournal = {Mach. Learn.},
  title        = {Balancing policy constraint and ensemble size in uncertainty-based offline reinforcement learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable reinforcement learning (XRL): A systematic
literature review and taxonomy. <em>ML</em>, <em>113</em>(1), 355–441.
(<a href="https://doi.org/10.1007/s10994-023-06479-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reinforcement learning (RL) systems have shown impressive performance and remarkable achievements. Many achievements can be attributed to combining RL with deep learning. However, those systems lack explainability, which refers to our understanding of the system’s decision-making process. In response to this challenge, the new explainable RL (XRL) field has emerged and grown rapidly to help us understand RL systems. This systematic literature review aims to give a unified view of the field by reviewing ten existing XRL literature reviews and 189 XRL studies from the past five years. Furthermore, we seek to organize these studies into a new taxonomy, discuss each area in detail, and draw connections between methods and stakeholder questions (e.g., “how can I get the agent to do _?”). Finally, we look at the research trends in XRL, recommend XRL methods, and present some exciting research directions for future research. We hope stakeholders, such as RL researchers and practitioners, will utilize this literature review as a comprehensive resource to overview existing state-of-the-art XRL methods. Additionally, we strive to help find research gaps and quickly identify methods that answer stakeholder questions.},
  archive      = {J_ML},
  author       = {Bekkemoen, Yanzhe},
  doi          = {10.1007/s10994-023-06479-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {355-441},
  shortjournal = {Mach. Learn.},
  title        = {Explainable reinforcement learning (XRL): A systematic literature review and taxonomy},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus–relevance kNN and covariate shift mitigation.
<em>ML</em>, <em>113</em>(1), 325–353. (<a
href="https://doi.org/10.1007/s10994-023-06378-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification and regression algorithms based on k-nearest neighbors (kNN) are often ranked among the top-10 Machine learning algorithms, due to their performance, flexibility, interpretability, non-parametric nature, and computational efficiency. Nevertheless, in existing kNN algorithms, the kNN radius, which plays a major role in the quality of kNN estimates, is independent of any weights associated with the training samples in a kNN-neighborhood. This omission, besides limiting the performance and flexibility of kNN, causes difficulties in correcting for covariate shift (e.g., selection bias) in the training data, taking advantage of unlabeled data, domain adaptation and transfer learning. We propose a new weighted kNN algorithm that, given training samples, each associated with two weights, called consensus and relevance (which may depend on the query on hand as well), and a request for an estimate of the posterior at a query, works as follows. First, it determines the kNN neighborhood as the training samples within the kth relevance-weighted order statistic of the distances of the training samples from the query. Second, it uses the training samples in this neighborhood to produce the desired estimate of the posterior (output label or value) via consensus-weighted aggregation as in existing kNN rules. Furthermore, we show that kNN algorithms are affected by covariate shift, and that the commonly used sample reweighing technique does not correct covariate shift in existing kNN algorithms. We then show how to mitigate covariate shift in kNN decision rules by using instead our proposed consensus-relevance kNN algorithm with relevance weights determined by the amount of covariate shift (e.g., the ratio of sample probability densities before and after the shift). Finally, we provide experimental results, using 197 real datasets, demonstrating that the proposed approach is slightly better (in terms of $$F_1$$ score) on average than competing benchmark approaches for mitigating selection bias, and that there are quite a few datasets for which it is significantly better.},
  archive      = {J_ML},
  author       = {Kalpakis, Konstantinos},
  doi          = {10.1007/s10994-023-06378-x},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {325-353},
  shortjournal = {Mach. Learn.},
  title        = {Consensus–relevance kNN and covariate shift mitigation},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual variational dropout: A view of auxiliary local
variables in continual learning. <em>ML</em>, <em>113</em>(1), 281–323.
(<a href="https://doi.org/10.1007/s10994-023-06487-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the learned knowledge. Without any retraining on previous data or extending the network architecture, the mechanism works by setting a constraint on the important weights of previous tasks when learning the current task. Regularization/prior approach, on the other hand, suffers the challenge of weights being moved intensely to the parameter region, in which the model achieves good performance for the latest task but poor ones for earlier tasks. To that end, we suggest a novel solution to this problem by continually applying variational dropout (CVD), thereby generating task-specific local variables that work as modifying factors for the global variables to fit the task. In particular, as we impose a variational distribution on the auxiliary local variables employed as multiplicative noise to the layers’ input, the model enables the global variables to be retained in a good region for all tasks and reduces the forgetting phenomenon. Furthermore, we obtained theoretical properties that are currently unavailable in existing methods: (1) uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes; (2) correlated pre-activation improves the representation ability for each task; and (3) data-dependent regularization assures the global variables to be preserved in a good region for all tasks. Throughout our extensive results, adding the local variables shows its significant advantage in enhancing the performance of regularization/prior-based methods by considerable magnitudes on numerous datasets. Specifically, it brings several standard baselines closer to state-of-the-art results.},
  archive      = {J_ML},
  author       = {Hai, Nam Le and Nguyen, Trang and Van, Linh Ngo and Nguyen, Thien Huu and Than, Khoat},
  doi          = {10.1007/s10994-023-06487-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {281-323},
  shortjournal = {Mach. Learn.},
  title        = {Continual variational dropout: A view of auxiliary local variables in continual learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discordance minimization-based imputation algorithms for
missing values in rating data. <em>ML</em>, <em>113</em>(1), 241–279.
(<a href="https://doi.org/10.1007/s10994-023-06452-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ratings are frequently used to evaluate and compare subjects in various applications, from education to healthcare, because ratings provide succinct yet credible measures for comparing subjects. However, when multiple rating lists are combined or considered together, subjects often have missing ratings, because most rating lists do not rate every subject in the combined list. In this study, we propose analyses on missing value patterns using six real-world data sets in various applications, as well as the conditions for applicability of imputation algorithms. Based on the special structures and properties derived from the analyses, we propose optimization models and algorithms that minimize the total rating discordance across rating providers to impute missing ratings in the combined rating lists, using only the known rating information. The total rating discordance is defined as the sum of the pairwise discordance metric, which can be written as a quadratic function. Computational experiments based on real-world and synthetic rating data sets show that the proposed methods outperform the state-of-the-art general imputation methods in the literature in terms of imputation accuracy.},
  archive      = {J_ML},
  author       = {Park, Young Woong and Kim, Jinhak and Zhu, Dan},
  doi          = {10.1007/s10994-023-06452-4},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {241-279},
  shortjournal = {Mach. Learn.},
  title        = {Discordance minimization-based imputation algorithms for missing values in rating data},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning for data streams: A survey. <em>ML</em>,
<em>113</em>(1), 185–239. (<a
href="https://doi.org/10.1007/s10994-023-06454-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research.},
  archive      = {J_ML},
  author       = {Cacciarelli, Davide and Kulahci, Murat},
  doi          = {10.1007/s10994-023-06454-2},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {185-239},
  shortjournal = {Mach. Learn.},
  title        = {Active learning for data streams: A survey},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Holistic deep learning. <em>ML</em>, <em>113</em>(1),
159–183. (<a href="https://doi.org/10.1007/s10994-023-06482-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL .},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Villalobos Carballo, Kimberly and Boussioux, Léonard and Li, Michael Lingzhi and Paskov, Alex and Paskov, Ivan},
  doi          = {10.1007/s10994-023-06482-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {159-183},
  shortjournal = {Mach. Learn.},
  title        = {Holistic deep learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature extractor stacking for cross-domain few-shot
learning. <em>ML</em>, <em>113</em>(1), 121–158. (<a
href="https://doi.org/10.1007/s10994-023-06483-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning (CDFSL) addresses learning problems where knowledge needs to be transferred from one or more source domains into an instance-scarce target domain with an explicitly different distribution. Recently published CDFSL methods generally construct a universal model that combines knowledge of multiple source domains into one feature extractor. This enables efficient inference but necessitates re-computation of the extractor whenever a new source domain is added. Some of these methods are also incompatible with heterogeneous source domain extractor architectures. We propose feature extractor stacking (FES), a new CDFSL method for combining information from a collection of extractors, that can utilise heterogeneous pretrained extractors out of the box and does not maintain a universal model that needs to be re-computed when its extractor collection is updated. We present the basic FES algorithm, which is inspired by the classic stacked generalisation approach, and also introduce two variants: convolutional FES (ConFES) and regularised FES (ReFES). Given a target-domain task, these algorithms fine-tune each extractor independently, use cross-validation to extract training data for stacked generalisation from the support set, and learn a simple linear stacking classifier from this data. We evaluate our FES methods on the well-known Meta-Dataset benchmark, targeting image classification with convolutional neural networks, and show that they can achieve state-of-the-art performance.},
  archive      = {J_ML},
  author       = {Wang, Hongyu and Frank, Eibe and Pfahringer, Bernhard and Mayo, Michael and Holmes, Geoffrey},
  doi          = {10.1007/s10994-023-06483-x},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {121-158},
  shortjournal = {Mach. Learn.},
  title        = {Feature extractor stacking for cross-domain few-shot learning},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient zeroth-order proximal stochastic method for
nonconvex nonsmooth black-box problems. <em>ML</em>, <em>113</em>(1),
97–120. (<a href="https://doi.org/10.1007/s10994-023-06409-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximal gradient method has a major role in solving nonsmooth composite optimization problems. However, in some machine learning problems related to black-box optimization models, the proximal gradient method could not be leveraged as the derivation of explicit gradients are difficult or entirely infeasible. Several variants of zeroth-order (ZO) stochastic variance reduced such as ZO-SVRG and ZO-SPIDER algorithms have recently been studied for nonconvex optimization problems. However, almost all the existing ZO-type algorithms suffer from a slowdown and increase in function query complexities up to a small-degree polynomial of the problem size. In order to fill this void, we propose a new analysis for the stochastic gradient algorithm for optimizing nonconvex, nonsmooth finite-sum problems, called ZO-PSVRG+ and ZO-PSPIDER+. The main goal of this work is to present an analysis that brings the convergence analysis for ZO-PSVRG+ and ZO-PSPIDER+ into uniformity, recovering several existing convergence results for arbitrary minibatch sizes while improving the complexity of their ZO oracle and proximal oracle calls. We prove that the studied ZO algorithms under Polyak-Łojasiewicz condition in contrast to the existent ZO-type methods obtain a global linear convergence for a wide range of minibatch sizes when the iterate enters into a local PL region without restart and algorithmic modification. The current analysis in the literature is mainly limited to large minibatch sizes, rendering the existing methods unpractical for real-world problems due to limited computational capacity. In the empirical experiments for black-box models, we show that the new analysis provides superior performance and faster convergence to a solution of nonconvex nonsmooth problems compared to the existing ZO-type methods as they suffer from small-level stepsizes. As a byproduct, the proposed analysis is generic and can be exploited to the other variants of gradient-free variance reduction methods aiming to make them more efficient.},
  archive      = {J_ML},
  author       = {Kazemi, Ehsan and Wang, Liqiang},
  doi          = {10.1007/s10994-023-06409-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {97-120},
  shortjournal = {Mach. Learn.},
  title        = {Efficient zeroth-order proximal stochastic method for nonconvex nonsmooth black-box problems},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manas: Multi-agent neural architecture search. <em>ML</em>,
<em>113</em>(1), 73–96. (<a
href="https://doi.org/10.1007/s10994-023-06379-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximize a graph-level global objective. Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use. In this work, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures. We provide two distinct lightweight implementations, with reduced memory requirements (1/8th of state-of-the-art), and performances above those of much more computationally expensive methods. Theoretically, we demonstrate vanishing regrets of the form $${\mathcal {O}}(\sqrt{T})$$ , with T being the total number of rounds. Finally, we perform experiments on CIFAR-10 and ImageNet, and aware that random search and random sampling are (often ignored) effective baselines, we conducted additional experiments on 3 alternative datasets, with complexity constraints, and 2 network configurations, and achieve competitive results in comparison with the baselines and other methods.},
  archive      = {J_ML},
  author       = {Lopes, Vasco and Carlucci, Fabio Maria and Esperança, Pedro M. and Singh, Marco and Yang, Antoine and Gabillon, Victor and Xu, Hang and Chen, Zewei and Wang, Jun},
  doi          = {10.1007/s10994-023-06379-w},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {73-96},
  shortjournal = {Mach. Learn.},
  title        = {Manas: Multi-agent neural architecture search},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-armed bandits with dependent arms. <em>ML</em>,
<em>113</em>(1), 45–71. (<a
href="https://doi.org/10.1007/s10994-023-06457-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a variant of the multi-armed bandit problem (MABP) which we call as MABs with dependent arms. Multiple arms are grouped together to form a cluster, and the reward distributions of arms in the same cluster are known functions of an unknown parameter that is a characteristic of the cluster. Thus, pulling an arm i not only reveals information about its own reward distribution, but also about all arms belonging to the same cluster. This “correlation” among the arms complicates the exploration–exploitation trade-off that is encountered in the MABP because the observation dependencies allow us to test simultaneously multiple hypotheses regarding the optimality of an arm. We develop learning algorithms based on the principle of optimism in the face of uncertainty (Lattimore and Szepesvári in Bandit algorithms, Cambridge University Press, 2020), which know the clusters, and hence utilize these additional side observations appropriately while performing exploration–exploitation trade-off. We show that the regret of our algorithms grows as $$O(K\log T)$$ , where K is the number of clusters. In contrast, for an algorithm such as the vanilla UCB that does not utilize these dependencies, the regret scales as $$O(M\log T)$$ , where M is the number of arms. When $$K\ll M$$ , i.e. there is a lot of dependencies among arms, our proposed algorithm drastically reduces the dependence of regret on the number of arms.},
  archive      = {J_ML},
  author       = {Singh, Rahul and Liu, Fang and Sun, Yin and Shroff, Ness},
  doi          = {10.1007/s10994-023-06457-z},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {45-71},
  shortjournal = {Mach. Learn.},
  title        = {Multi-armed bandits with dependent arms},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph similarity learning for change-point detection in
dynamic networks. <em>ML</em>, <em>113</em>(1), 1–44. (<a
href="https://doi.org/10.1007/s10994-023-06405-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks are ubiquitous for modelling sequential graph-structured data, e.g., brain connectivity, population migrations, and social networks. In this work, we consider the discrete-time framework of dynamic networks and aim at detecting change-points, i.e., abrupt changes in the structure or attributes of the graph snapshots. This task is often termed network change-point detection and has numerous applications, such as market phase discovery, fraud detection, and activity monitoring. In this work, we propose a data-driven method that can adapt to the specific network domain, and be used to detect distribution changes with no delay and in an online setting. Our algorithm is based on a siamese graph neural network, designed to learn a graph similarity function on the graph snapshots from the temporal network sequence. Without any prior knowledge on the network generative distribution and the type of change-points, our learnt similarity function allows to more effectively compare the current graph and its recent history, compared to standard graph distances or kernels. Moreover, our method can be applied to a large variety of network data, e.g., networks with edge weights or node attributes. We test our method on synthetic and real-world dynamic network data, and demonstrate that it is able to perform online network change-point detection in diverse settings. Besides, we show that it requires a shorter data history to detect changes than most existing state-of-the-art baselines.},
  archive      = {J_ML},
  author       = {Sulem, Déborah and Kenlay, Henry and Cucuringu, Mihai and Dong, Xiaowen},
  doi          = {10.1007/s10994-023-06405-x},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Graph similarity learning for change-point detection in dynamic networks},
  volume       = {113},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
