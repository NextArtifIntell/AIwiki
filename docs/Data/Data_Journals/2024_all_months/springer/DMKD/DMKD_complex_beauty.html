<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---135">DMKD - 135</h2>
<ul>
<li><details>
<summary>
(2024a). Correction: FRAPPE: Fast rank approximation with
explainable features for tensors. <em>DMKD</em>, <em>38</em>(6),
4238–4239. (<a
href="https://doi.org/10.1007/s10618-024-01072-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Shiao, William and Papalexakis, Evangelos E.},
  doi          = {10.1007/s10618-024-01072-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4238-4239},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction: FRAPPE: fast rank approximation with explainable features for tensors},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: Bake off redux: A review and experimental
evaluation of recent time series classification algorithms.
<em>DMKD</em>, <em>38</em>(6), 4236–4237. (<a
href="https://doi.org/10.1007/s10618-024-01040-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Middlehurst, Matthew and Schäfer, Patrick and Bagnall, Anthony},
  doi          = {10.1007/s10618-024-01040-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4236-4237},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction: bake off redux: a review and experimental evaluation of recent time series classification algorithms},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: Marginal effects for non-linear prediction
functions. <em>DMKD</em>, <em>38</em>(6), 4234–4235. (<a
href="https://doi.org/10.1007/s10618-024-01034-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Scholbeck, Christian A. and Casalicchio, Giuseppe and Molnar, Christoph and Bischl, Bernd and Heumann, Christian},
  doi          = {10.1007/s10618-024-01034-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4234-4235},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction: Marginal effects for non-linear prediction functions},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Bias characterization, assessment, and
mitigation in location-based recommender systems. <em>DMKD</em>,
<em>38</em>(6), 4233. (<a
href="https://doi.org/10.1007/s10618-024-01013-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Sánchez, Pablo and Bellogín, Alejandro and Boratto, Ludovico},
  doi          = {10.1007/s10618-024-01013-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4233},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: Bias characterization, assessment, and mitigation in location-based recommender systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FRAPPE: Fast rank approximation with explainable features
for tensors. <em>DMKD</em>, <em>38</em>(6), 4217–4232. (<a
href="https://doi.org/10.1007/s10618-024-01071-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decompositions have proven to be effective in analyzing the structure of multidimensional data. However, most of these methods require a key parameter: the number of desired components. In the case of the CANDECOMP/PARAFAC decomposition (CPD), the ideal value for the number of components is known as the canonical rank and greatly affects the quality of the decomposition results. Existing methods use heuristics or Bayesian methods to estimate this value by repeatedly calculating the CPD, making them extremely computationally expensive. In this work, we propose FRAPPE, the first method to estimate the canonical rank of a tensor without having to compute the CPD. This method is the result of two key ideas. First, it is much cheaper to generate synthetic data with known rank compared to computing the CPD. Second, we can greatly improve the generalization ability and speed of our model by generating synthetic data that matches a given input tensor in terms of size and sparsity. We can then train a specialized single-use regression model on a synthetic set of tensors engineered to match a given input tensor and use that to estimate the canonical rank of the tensor—all without computing the expensive CPD. FRAPPE is over $$24\times $$ faster than the best-performing baseline, and exhibits a $$10\%$$ improvement in MAPE on a synthetic dataset. It also performs as well as or better than the baselines on real-world datasets.},
  archive      = {J_DMKD},
  author       = {Shiao, William and Papalexakis, Evangelos E.},
  doi          = {10.1007/s10618-024-01071-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4217-4232},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {FRAPPE: Fast rank approximation with explainable features for tensors},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic variable importance for predictive
uncertainty: An entropy-based approach. <em>DMKD</em>, <em>38</em>(6),
4184–4216. (<a
href="https://doi.org/10.1007/s10618-024-01070-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the reasons for the model’s level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model’s predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both synthetic and real-world data, we demonstrate the utility of these approaches to understand both the sources of uncertainty and their impact on model performance.},
  archive      = {J_DMKD},
  author       = {Wood, Danny and Papamarkou, Theodore and Benatan, Matt and Allmendinger, Richard},
  doi          = {10.1007/s10618-024-01070-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4184-4216},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Model-agnostic variable importance for predictive uncertainty: An entropy-based approach},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounding the family-wise error rate in local causal
discovery using rademacher averages. <em>DMKD</em>, <em>38</em>(6),
4157–4183. (<a
href="https://doi.org/10.1007/s10618-024-01069-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many algorithms have been proposed to learn local graphical structures around target variables of interest from observational data, focusing on two sets of variables. The first one, called Parent–Children (PC) set, contains all the variables that are direct causes or consequences of the target while the second one, known as Markov boundary (MB), is the minimal set of variables with optimal prediction performances of the target. In this paper we introduce two novel algorithms for the PC and MB discovery tasks with rigorous guarantees on the Family-Wise Error Rate (FWER), that is, the probability of reporting any false positive in output. Our algorithms use Rademacher averages, a key concept from statistical learning theory, to properly account for the multiple-hypothesis testing problem arising in such tasks. Our evaluation on simulated data shows that our algorithms properly control for the FWER, while widely used algorithms do not provide guarantees on false discoveries even when correcting for multiple-hypothesis testing. Our experiments also show that our algorithms identify meaningful relations in real-world data.},
  archive      = {J_DMKD},
  author       = {Simionato, Dario and Vandin, Fabio},
  doi          = {10.1007/s10618-024-01069-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4157-4183},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bounding the family-wise error rate in local causal discovery using rademacher averages},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FRUITS: Feature extraction using iterated sums for time
series classification. <em>DMKD</em>, <em>38</em>(6), 4122–4156. (<a
href="https://doi.org/10.1007/s10618-024-01068-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a pipeline for time series classification that extracts features based on the iterated-sums signature (ISS) and then applies a linear classifier. These features are intrinsically nonlinear, capture chronological information, and, under certain settings, are invariant to a form of time-warping. We achieve competitive results, both in accuracy and speed, on the UCR archive. We make our code available at https://github.com/irkri/fruits .},
  archive      = {J_DMKD},
  author       = {Diehl, Joscha and Krieg, Richard},
  doi          = {10.1007/s10618-024-01068-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4122-4156},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {FRUITS: Feature extraction using iterated sums for time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularization-based methods for ordinal quantification.
<em>DMKD</em>, <em>38</em>(6), 4076–4121. (<a
href="https://doi.org/10.1007/s10618-024-01067-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification, i.e., the task of predicting the class prevalence values in bags of unlabeled data items, has received increased attention in recent years. However, most quantification research has concentrated on developing algorithms for binary and multi-class problems in which the classes are not ordered. Here, we study the ordinal case, i.e., the case in which a total order is defined on the set of $$n&gt;2$$ classes. We give three main contributions to this field. First, we create and make available two datasets for ordinal quantification (OQ) research that overcome the inadequacies of the previously available ones. Second, we experimentally compare the most important OQ algorithms proposed in the literature so far. To this end, we bring together algorithms proposed by authors from very different research fields, such as data mining and astrophysics, who were unaware of each others’ developments. Third, we propose a novel class of regularized OQ algorithms, which outperforms existing algorithms in our experiments. The key to this gain in performance is that our regularization prevents ordinally implausible estimates, assuming that ordinal distributions tend to be smooth in practice. We informally verify this assumption for several real-world applications.},
  archive      = {J_DMKD},
  author       = {Bunse, Mirko and Moreo, Alejandro and Sebastiani, Fabrizio and Senz, Martin},
  doi          = {10.1007/s10618-024-01067-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4076-4121},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Regularization-based methods for ordinal quantification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the disclosure risk of anonymized documents via a
machine learning-based re-identification attack. <em>DMKD</em>,
<em>38</em>(6), 4040–4075. (<a
href="https://doi.org/10.1007/s10618-024-01066-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of textual data depicting human-centered features and behaviors is crucial for many data mining and machine learning tasks. However, data containing personal information should be anonymized prior making them available for secondary use. A variety of text anonymization methods have been proposed in the last years, which are standardly evaluated by comparing their outputs with human-based anonymizations. The residual disclosure risk is estimated with the recall metric, which quantifies the proportion of manually annotated re-identifying terms successfully detected by the anonymization algorithm. Nevertheless, recall is not a risk metric, which leads to several drawbacks. First, it requires a unique ground truth, and this does not hold for text anonymization, where several masking choices could be equally valid to prevent re-identification. Second, it relies on human judgements, which are inherently subjective and prone to errors. Finally, the recall metric weights terms uniformly, thereby ignoring the fact that the influence on the disclosure risk of some missed terms may be much larger than of others. To overcome these drawbacks, in this paper we propose a novel method to evaluate the disclosure risk of anonymized texts by means of an automated re-identification attack. We formalize the attack as a multi-class classification task and leverage state-of-the-art neural language models to aggregate the data sources that attackers may use to build the classifier. We illustrate the effectiveness of our method by assessing the disclosure risk of several methods for text anonymization under different attack configurations. Empirical results show substantial privacy risks for most existing anonymization methods.},
  archive      = {J_DMKD},
  author       = {Manzanares-Salor, Benet and Sánchez, David and Lison, Pierre},
  doi          = {10.1007/s10618-024-01066-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4040-4075},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Evaluating the disclosure risk of anonymized documents via a machine learning-based re-identification attack},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random walks with variable restarts for
negative-example-informed label propagation. <em>DMKD</em>,
<em>38</em>(6), 4024–4039. (<a
href="https://doi.org/10.1007/s10618-024-01065-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label propagation is frequently encountered in machine learning and data mining applications on graphs, either as a standalone problem or as part of node classification. Many label propagation algorithms utilize random walks (or network propagation), which provide limited ability to take into account negatively-labeled nodes (i.e., nodes that are known to be not associated with the label of interest). Specialized algorithms to incorporate negatively-labeled nodes generally focus on learning or readjusting the edge weights to drive walks away from negatively-labeled nodes and toward positively-labeled nodes. This approach has several disadvantages, as it increases the number of parameters to be learned, and does not necessarily drive the walk away from regions of the network that are rich in negatively-labeled nodes. We reformulate random walk with restarts and network propagation to enable “variable restarts&quot;, that is the increased likelihood of restarting at a positively-labeled node when a negatively-labeled node is encountered. Based on this reformulation, we develop CusTaRd, an algorithm that effectively combines variable restart probabilities and edge re-weighting to avoid negatively-labeled nodes. To assess the performance of CusTaRd, we perform comprehensive experiments on network datasets commonly used in benchmarking label propagation and node classification algorithms. Our results show that CusTaRd consistently outperforms competing algorithms that learn edge weights or restart profiles, and that negatives close to positive examples are generally more informative than more distant negatives.},
  archive      = {J_DMKD},
  author       = {Maxwell, Sean and Koyutürk, Mehmet},
  doi          = {10.1007/s10618-024-01065-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4024-4039},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Random walks with variable restarts for negative-example-informed label propagation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opinion dynamics in social networks incorporating
higher-order interactions. <em>DMKD</em>, <em>38</em>(6), 4001–4023. (<a
href="https://doi.org/10.1007/s10618-024-01064-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of opinion sharing and formation has received considerable attention in the academic literature, and a few models have been proposed to study this problem. However, existing models are limited to the interactions among nearest neighbors, with those second, third, and higher-order neighbors only considered indirectly, despite the fact that higher-order interactions occur frequently in real social networks. In this paper, we develop a new model for opinion dynamics by incorporating long-range interactions based on higher-order random walks that can explicitly tune the degree of influence of higher-order neighbor interactions. We prove that the model converges to a fixed opinion vector, which may differ greatly from those models without higher-order interactions. Since direct computation of the equilibrium opinion is computationally expensive, which involves the operations of huge-scale matrix multiplication and inversion, we design a theoretically convergence-guaranteed estimation algorithm that approximates the equilibrium opinion vector nearly linearly in both space and time with respect to the number of edges in the graph. We conduct extensive experiments on various social networks, demonstrating that the new algorithm is both highly efficient and effective.},
  archive      = {J_DMKD},
  author       = {Zhang, Zuobai and Xu, Wanyue and Zhang, Zhongzhi and Chen, Guanrong},
  doi          = {10.1007/s10618-024-01064-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {4001-4023},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Opinion dynamics in social networks incorporating higher-order interactions},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient learning with projected histograms. <em>DMKD</em>,
<em>38</em>(6), 3948–4000. (<a
href="https://doi.org/10.1007/s10618-024-01063-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional learning is a perennial problem due to challenges posed by the “curse of dimensionality”; learning typically demands more computing resources as well as more training data. In differentially private (DP) settings, this is further exacerbated by noise that needs adding to each dimension to achieve the required privacy. In this paper, we present a surprisingly simple approach to address all of these concerns at once, based on histograms constructed on a low-dimensional random projection (RP) of the data. Our approach exploits RP to take advantage of hidden low-dimensional structures in the data, yielding both computational efficiency, and improved error convergence with respect to the sample size—whereby less training data suffice for learning. We also propose a variant for efficient differentially private (DP) classification that further exploits the data-oblivious nature of both the histogram construction and the RP based dimensionality reduction, resulting in an efficient management of the privacy budget. We present a detailed and rigorous theoretical analysis of generalisation of our algorithms in several settings, showing that our approach is able to exploit low-dimensional structure of the data, ameliorates the ill-effects of noise required for privacy, and has good generalisation under minimal conditions. We also corroborate our findings experimentally, and demonstrate that our algorithms achieve competitive classification accuracy in both non-private and private settings.},
  archive      = {J_DMKD},
  author       = {Huang, Zhanliang and Kabán, Ata and Reeve, Henry},
  doi          = {10.1007/s10618-024-01063-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3948-4000},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient learning with projected histograms},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detach-ROCKET: Sequential feature selection for time series
classification with random convolutional kernels. <em>DMKD</em>,
<em>38</em>(6), 3922–3947. (<a
href="https://doi.org/10.1007/s10618-024-01062-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) is essential in fields like medicine, environmental science, and finance, enabling tasks such as disease diagnosis, anomaly detection, and stock price analysis. While machine learning models like Recurrent Neural Networks and InceptionTime are successful in numerous applications, they can face scalability issues due to computational requirements. Recently, ROCKET has emerged as an efficient alternative, achieving state-of-the-art performance and simplifying training by utilizing a large number of randomly generated features from the time series data. However, many of these features are redundant or non-informative, increasing computational load and compromising generalization. Here we introduce Sequential Feature Detachment (SFD) to identify and prune non-essential features in ROCKET-based models, such as ROCKET, MiniRocket, and MultiRocket. SFD estimates feature importance using model coefficients and can handle large feature sets without complex hyperparameter tuning. Testing on the UCR archive shows that SFD can produce models with better test accuracy using only 10% of the original features. We named these pruned models Detach-ROCKET. We also present an end-to-end procedure for determining an optimal balance between the number of features and model accuracy. On the largest binary UCR dataset, Detach-ROCKET improves test accuracy by 0.6% while reducing features by 98.9%. By enabling a significant reduction in model size without sacrificing accuracy, our methodology improves computational efficiency and contributes to model interpretability. We believe that Detach-ROCKET will be a valuable tool for researchers and practitioners working with time series data, who can find a user-friendly implementation of the model at https://github.com/gon-uri/detach_rocket .},
  archive      = {J_DMKD},
  author       = {Uribarri, Gonzalo and Barone, Federico and Ansuini, Alessio and Fransén, Erik},
  doi          = {10.1007/s10618-024-01062-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3922-3947},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ArcMatch: High-performance subgraph matching for labeled
graphs by exploiting edge domains. <em>DMKD</em>, <em>38</em>(6),
3868–3921. (<a
href="https://doi.org/10.1007/s10618-024-01061-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a large labeled graph (network), denoted the target. Subgraph matching is the problem of finding all instances of a small subgraph, denoted the query, in the target graph. Unlike the majority of existing methods that are restricted to graphs with labels solely on vertices, our proposed approach, named can effectively handle graphs with labels on both vertices and edges. ntroduces an efficient new vertex/edge domain data structure filtering procedure to speed up subgraph queries. The procedure, called path-based reduction, filters initial domains by scanning them for paths up to a specified length that appear in the query graph. Additionally, ncorporates existing techniques like variable ordering and parent selection, as well as adapting the core search process, to take advantage of the information within edge domains. Experiments in real scenarios such as protein–protein interaction graphs, co-authorship networks, and email networks, show that s faster than state-of-the-art systems varying the number of distinct vertex labels over the whole target graph and query sizes.},
  archive      = {J_DMKD},
  author       = {Bonnici, Vincenzo and Grasso, Roberto and Micale, Giovanni and Maria, Antonio di and Shasha, Dennis and Pulvirenti, Alfredo and Giugno, Rosalba},
  doi          = {10.1007/s10618-024-01061-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3868-3921},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ArcMatch: High-performance subgraph matching for labeled graphs by exploiting edge domains},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical methods utilizing structural properties of
time-evolving networks for event detection. <em>DMKD</em>,
<em>38</em>(6), 3831–3867. (<a
href="https://doi.org/10.1007/s10618-024-01060-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of technology, real-world networks have become vulnerable to many attacks such as cyber-crimes, terrorist attacks, and financial frauds. Accuracy and scalability are the two principal but contrary characteristics for algorithms detecting such attacks (or events) in these time-varying networks. However, existing approaches confirm to either of these two prerequisites. Hence, we propose two algorithms designated as GraphAnomaly and GraphAnomaly-CS, both satisfying these two requirements together. Given a stream of time-evolving real-world network edges, the proposed algorithms first extract the local structure of network graphs by identifying the relationship between egonets and their properties, and then use this information in Chi-square statistics to discover (1) anomalous time-points at which many network nodes deviate from their normal behavior and (2) those nodes and features that majorly contribute to the change. The proposed algorithms are (a) accurate: upto 7 to 12% more accurate than state-of-the-art methods; (b) speedy: process millions of edges within a few minutes; (c) scalable: scale linearly with the number of edges and nodes in the network graph; (d) theoretically sound: providing theoretical guarantees on the false positive probability of algorithms; We show theoretically and experimentally that the proposed algorithms successfully detect anomalies in time-evolving edge streams. We have selected six baselines, five evaluation metrics, and six real-world network datasets from three different network classes for empirical analysis. The experimental results show that both algorithms are efficient at detecting anomalies in networks that reduce false positives and false negatives in the results, especially in successive time-points. Furthermore, algorithms discover the maximum number of critical events from real-world networks, demonstrating their effectiveness over baselines.},
  archive      = {J_DMKD},
  author       = {Bansal, Monika and Sharma, Dolly},
  doi          = {10.1007/s10618-024-01060-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3831-3867},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Statistical methods utilizing structural properties of time-evolving networks for event detection},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing racism classification: An automatic multilingual
data annotation system using self-training and CNN. <em>DMKD</em>,
<em>38</em>(6), 3805–3830. (<a
href="https://doi.org/10.1007/s10618-024-01059-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate racism classification is crucial on social media, where racist and discriminatory content can harm individuals and society. Automated racism detection requires gathering and annotating a wide range of diverse and representative data as an essential source of information for the system. However, this task proves to be highly demanding in both time and resources, resulting in a significantly costly process. Moreover, racism can appear differently across languages because of the distinct cultural subtleties and vocabularies linked to each language. This necessitates having information resources in native languages to effectively detect racism, which further complicates constructing a database explicitly designed for identifying racism on social media platforms. In this study, an automated data annotation system for racism classification is presented, utilizing self-training and a combination of the Sentence-BERT (SBERT) transformers-based model for data representation and a Convolutional Neural Network (CNN) model. The system aids in the creation of a multilingual racism dataset consisting of 26,866 instances gathered from Facebook and Twitter. This is achieved through a self-training process that utilizes a labeled subset of the dataset to annotate the remaining unlabeled data. The study examines the impact of self-training on the system’s performance, revealing significant enhancements in model effectiveness. Especially for the English dataset, the system achieves a noteworthy accuracy rate of 92.53% and an F-score of 88.26%. The French dataset reaches an accuracy of 93.64% and an F-score of 92.68%. Similarly, for the Arabic dataset, the accuracy reaches 91.03%, accompanied by an F-score value of 92.15%. The implementation of self-training results in a remarkable 8–12% improvement in accuracy and F-score, as demonstrated in this study.},
  archive      = {J_DMKD},
  author       = {El Miqdadi, Ikram and Hourri, Soufiane and El Idrysy, Fatima Zahra and Hayati, Assia and Namir, Yassine and Nikolov, Nikola S. and Kharroubi, Jamal},
  doi          = {10.1007/s10618-024-01059-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3805-3830},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Enhancing racism classification: An automatic multilingual data annotation system using self-training and CNN},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). De-confounding representation learning for counterfactual
inference on continuous treatment via generative adversarial network.
<em>DMKD</em>, <em>38</em>(6), 3783–3804. (<a
href="https://doi.org/10.1007/s10618-024-01058-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment’s linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates decorrelated with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounding representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic and semi-synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC III and demonstrate a detailed causal relationship between red cell width distribution and mortality.},
  archive      = {J_DMKD},
  author       = {Zhao, Yonghe and Huang, Qiang and Zeng, Haolong and Peng, Yun and Sun, Huiyan},
  doi          = {10.1007/s10618-024-01058-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3783-3804},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {De-confounding representation learning for counterfactual inference on continuous treatment via generative adversarial network},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential query prediction based on multi-armed bandits
with ensemble of transformer experts and immediate feedback.
<em>DMKD</em>, <em>38</em>(6), 3758–3782. (<a
href="https://doi.org/10.1007/s10618-024-01057-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of predicting the next query to be recommended in interactive data exploratory analysis to guide users to correct content. Current query prediction approaches are based on sequence-to-sequence learning, exploiting past interaction data. However, due to the resource-hungry training process, such approaches fail to adapt to immediate user feedback. Immediate feedback is essential and considered as a signal of the user’s intent. We contribute with a novel query prediction ensemble mechanism, which adapts to immediate feedback relying on multi-armed bandits framework. Our mechanism, an extension to the popular Exp3 algorithm, augments Transformer-based language models for query predictions by combining predictions from experts, thus dynamically building a candidate set during exploration. Immediate feedback is leveraged to choose the appropriate prediction in a probabilistic fashion. We provide comprehensive large-scale experimental and comparative assessment using a popular online literature discovery service, which showcases that our mechanism (i) improves the per-round regret substantially against state-of-the-art Transformer-based models and (ii) shows the superiority of causal language modelling over masked language modelling for query recommendations.},
  archive      = {J_DMKD},
  author       = {Puthiya Parambath, Shameem A. and Anagnostopoulos, Christos and Murray-Smith, Roderick},
  doi          = {10.1007/s10618-024-01057-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3758-3782},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sequential query prediction based on multi-armed bandits with ensemble of transformer experts and immediate feedback},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating outlier probabilities: Assessing sharpness,
refinement, and calibration using stratified and weighted measures.
<em>DMKD</em>, <em>38</em>(6), 3719–3757. (<a
href="https://doi.org/10.1007/s10618-024-01056-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier probability is the probability that an observation is an outlier. Typically, outlier detection algorithms calculate real-valued outlier scores to identify outliers. Converting outlier scores into outlier probabilities increases the interpretability of outlier scores for domain experts and makes outlier scores from different outlier detection algorithms comparable. Although several transformations to convert outlier scores to outlier probabilities have been proposed in the literature, there is no common understanding of good outlier probabilities and no standard approach to evaluate outlier probabilities. We require that good outlier probabilities be sharp, refined, and calibrated. To evaluate these properties, we adapt and propose novel measures that use ground-truth labels indicating which observation is an outlier or an inlier. The refinement and calibration measures partition the outlier probabilities into bins or use kernel smoothing. Compared to the evaluation of probability in supervised learning, several aspects are relevant when evaluating outlier probabilities, mainly due to the imbalanced and often unsupervised nature of outlier detection. First, stratified and weighted measures are necessary to evaluate the probabilities of outliers well. Second, the joint use of the sharpness, refinement, and calibration errors makes it possible to independently measure the corresponding characteristics of outlier probabilities. Third, equiareal bins, where the product of observations per bin times bin length is constant, balance the number of observations per bin and bin length, allowing accurate evaluation of different outlier probability ranges. Finally, we show that good outlier probabilities, according to the proposed measures, improve the performance of the follow-up task of converting outlier probabilities into labels for outliers and inliers.},
  archive      = {J_DMKD},
  author       = {Röchner, Philipp and Marques, Henrique O. and Campello, Ricardo J. G. B. and Zimek, Arthur},
  doi          = {10.1007/s10618-024-01056-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3719-3757},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Evaluating outlier probabilities: Assessing sharpness, refinement, and calibration using stratified and weighted measures},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient-based explanation for non-linear non-parametric
dimensionality reduction. <em>DMKD</em>, <em>38</em>(6), 3690–3718. (<a
href="https://doi.org/10.1007/s10618-024-01055-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) is a popular technique that shows great results to analyze high-dimensional data. Generally, DR is used to produce visualizations in 2 or 3 dimensions. While it can help understanding correlations between data, embeddings generated by DR are hard to grasp. The position of instances in low-dimension may be difficult to interpret, especially for non-linear, non-parametric DR techniques. Because most of the techniques are said to be neighborhood preserving (which means that explaining long distances is not relevant), some approaches try explaining them locally. These methods use simpler interpretable models to approximate the decision frontier locally. This can lead to misleading explanations. In this paper a novel approach to locally explain non-linear, non-parametric DR embeddings like t-SNE is introduced. It is the first gradient-based method for explaining these DR algorithms. The technique presented in this paper is applied on t-SNE, but is theoretically suitable for any DR method that is a minimization or maximization problem. The approach uses the analytical derivative of a t-SNE embedding to explain the position of an instance in the visualization.},
  archive      = {J_DMKD},
  author       = {Corbugy, Sacha and Marion, Rebecca and Frénay, Benoît},
  doi          = {10.1007/s10618-024-01055-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3690-3718},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Gradient-based explanation for non-linear non-parametric dimensionality reduction},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian network motifs for reasoning over heterogeneous
unlinked datasets. <em>DMKD</em>, <em>38</em>(6), 3643–3689. (<a
href="https://doi.org/10.1007/s10618-024-01054-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data-oriented applications often require integrating data from multiple heterogeneous sources. When these datasets share attributes, but are otherwise unlinked, there is no way to join them and reason at the individual level explicitly. However, as we show in this work, this does not prevent probabilistic reasoning over these heterogeneous datasets even when the data and shared attributes exhibit significant mismatches that are common in real-world data. Different datasets have different sample biases, disagree on category definitions and spatial representations, collect data at different temporal intervals, and mix aggregate-level with individual data. In this work, we demonstrate how a set of Bayesian network motifs allows all of these mismatches to be resolved in a composable framework that permits joint probabilistic reasoning over all datasets without manipulating, modifying, or imputing the original data, thus avoiding potentially harmful assumptions. We provide an open source Python tool that encapsulates our methodology and demonstrate this tool on a number of real-world use cases.},
  archive      = {J_DMKD},
  author       = {Sui, Yi and Kwan, Alex and Olson, Alexander W. and Sanner, Scott and Silver, Daniel A.},
  doi          = {10.1007/s10618-024-01054-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3643-3689},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bayesian network motifs for reasoning over heterogeneous unlinked datasets},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable decomposition of nested dense subgraphs.
<em>DMKD</em>, <em>38</em>(6), 3621–3642. (<a
href="https://doi.org/10.1007/s10618-024-01053-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering dense regions in a graph is a popular tool for analyzing graphs. While useful, analyzing such decompositions may be difficult without additional information. Fortunately, many real-world networks have additional information, namely node labels. In this paper we focus on finding decompositions that have dense inner subgraphs and that can be explained using labels. More formally, we construct a binary tree T with labels on non-leaves that we use to partition the nodes in the input graph. To measure the quality of the tree, we model the edges in the shell and the cross edges to the inner shells as a Bernoulli variable. We reward the decompositions with the dense regions by requiring that the model parameters are non-increasing. We show that our problem is NP-hard, even inapproximable if we constrain the size of the tree. Consequently, we propose a greedy algorithm that iteratively finds the best split and applies it to the current tree. We demonstrate how we can efficiently compute the best split by maintaining certain counters. Our experiments show that our algorithm can process networks with over million edges in few minutes. Moreover, we show that the algorithm can find the ground truth in synthetic data and produces interpretable decompositions when applied to real world networks.},
  archive      = {J_DMKD},
  author       = {Tatti, Nikolaj},
  doi          = {10.1007/s10618-024-01053-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3621-3642},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Explainable decomposition of nested dense subgraphs},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Negative-sample-free knowledge graph embedding.
<em>DMKD</em>, <em>38</em>(6), 3590–3620. (<a
href="https://doi.org/10.1007/s10618-024-01052-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, knowledge graphs (KGs) have been shown to benefit many machine learning applications in multiple domains (e.g. self-driving, agriculture, bio-medicine, recommender systems, etc.). However, KGs suffer from incompleteness, which motivates the task of KG completion which consists of inferring new (unobserved) links between existing entities based on observed links. This task is achieved using either a probabilistic, rule-based, or embedding-based approach. The latter has been shown to consistently outperform the former approaches. It however relies on negative sampling, which supposes that every observed link is “true” and that every unobserved link is “false”. Negative sampling increases the computation complexity of the learning process and introduces noise in the learning. We propose NSF-KGE, a framework for KG embedding that does not require negative sampling, yet achieves performance comparable to that of the negative sampling-based approach. NSF-KGE employs objectives from the non-contrastive self-supervised literature to learn representations that are invariant to relation transformations (e.g. translation, scaling, rotation etc) while avoiding representation collapse.},
  archive      = {J_DMKD},
  author       = {Bahaj, Adil and Ghogho, Mounir},
  doi          = {10.1007/s10618-024-01052-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3590-3620},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Negative-sample-free knowledge graph embedding},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On regime changes in text data using hidden markov model of
contaminated vMF distribution. <em>DMKD</em>, <em>38</em>(6), 3563–3589.
(<a href="https://doi.org/10.1007/s10618-024-01051-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel methodology for analyzing temporal directional data with scatter and heavy tails. A hidden Markov model with contaminated von Mises-Fisher emission distribution is developed. The model is implemented using forward and backward selection approach that provides additional flexibility for contaminated as well as non-contaminated data. The utility of the method for finding homogeneous time blocks (regimes) is demonstrated on several experimental settings and two real-life text data sets containing presidential addresses and corporate financial statements respectively.},
  archive      = {J_DMKD},
  author       = {Zhang, Yingying and Sarkar, Shuchismita and Chen, Yuanyuan and Zhu, Xuwen},
  doi          = {10.1007/s10618-024-01051-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3563-3589},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On regime changes in text data using hidden markov model of contaminated vMF distribution},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph embedding closed under composition.
<em>DMKD</em>, <em>38</em>(6), 3531–3562. (<a
href="https://doi.org/10.1007/s10618-024-01050-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Embedding (KGE) has attracted increasing attention. Relation patterns, such as symmetry and inversion, have received considerable focus. Among them, composition patterns are particularly important, as they involve nearly all relations in KGs. However, prior KGE approaches often consider relations to be compositional only if they are well-represented in the training data. Consequently, it can lead to performance degradation, especially for under-represented composition patterns. To this end, we propose HolmE, a general form of KGE with its relation embedding space closed under composition, namely that the composition of any two given relation embeddings remains within the embedding space. This property ensures that every relation embedding can compose, or be composed by other relation embeddings. It enhances HolmE’s capability to model under-represented (also called long-tail) composition patterns with limited learning instances. To our best knowledge, our work is pioneering in discussing KGE with this property of being closed under composition. We provide detailed theoretical proof and extensive experiments to demonstrate the notable advantages of HolmE in modelling composition patterns, particularly for long-tail patterns. Our results also highlight HolmE’s effectiveness in extrapolating to unseen relations through composition and its state-of-the-art performance on benchmark datasets.},
  archive      = {J_DMKD},
  author       = {Zheng, Zhuoxun and Zhou, Baifan and Yang, Hui and Tan, Zhipeng and Sun, Zequn and Li, Chunnong and Waaler, Arild and Kharlamov, Evgeny and Soylu, Ahmet},
  doi          = {10.1007/s10618-024-01050-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3531-3562},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Knowledge graph embedding closed under composition},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards effective urban region-of-interest demand modeling
via graph representation learning. <em>DMKD</em>, <em>38</em>(6),
3503–3530. (<a
href="https://doi.org/10.1007/s10618-024-01049-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the region’s functionalities and what the specific Point-of-Interest (POI) needs is essential for effective urban planning. However, due to the diversified and ambiguity nature of urban regions, there are still some significant challenges to be resolved in urban POI demand analysis. To this end, we propose a novel framework, in which Region-of-Interest Demand Modeling is enhanced through the graph representation learning, namely Variational Multi-graph Auto-encoding Fusion, aiming to effectively predict the ROI demand from both the POI level and category level. Specifically, we first divide the urban area into spatially differentiated neighborhood regions, extract the corresponding multi-dimensional natures, and then generate the Spatial-Attributed Region Graph (SARG). After that, we introduce an unsupervised multi-graph based variational auto-encoder to map regional profiles of SARG into latent space, and further retrieve the dynamic latent representations through probabilistic sampling and global fusing. Additionally, during the training process, a spatio-temporal constrained Bayesian algorithm is adopted to infer the destination POIs. Finally, extensive experiments are conducted on real-world dataset, which demonstrate our model significantly outperforms state-of-the-art baselines.},
  archive      = {J_DMKD},
  author       = {Wang, Pu and Sun, Jingya and Chen, Wei and Zhao, Lei},
  doi          = {10.1007/s10618-024-01049-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3503-3530},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Towards effective urban region-of-interest demand modeling via graph representation learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomnet: Clustering time series using untrained deep
neural networks. <em>DMKD</em>, <em>38</em>(6), 3473–3502. (<a
href="https://doi.org/10.1007/s10618-024-01048-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are widely used in machine learning and data mining. Typically, these networks need to be trained, implying the adjustment of weights (parameters) within the network based on the input data. In this work, we propose a novel approach, RandomNet, that employs untrained deep neural networks to cluster time series. RandomNet uses different sets of random weights to extract diverse representations of time series and then ensembles the clustering relationships derived from these different representations to build the final clustering results. By extracting diverse representations, our model can effectively handle time series with different characteristics. Since all parameters are randomly generated, no training is required during the process. We provide a theoretical analysis of the effectiveness of the method. To validate its performance, we conduct extensive experiments on all of the 128 datasets in the well-known UCR time series archive and perform statistical analysis of the results. These datasets have different sizes, sequence lengths, and they are from diverse fields. The experimental results show that the proposed method is competitive compared with existing state-of-the-art methods.},
  archive      = {J_DMKD},
  author       = {Li, Xiaosheng and Xi, Wenjie and Lin, Jessica},
  doi          = {10.1007/s10618-024-01048-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3473-3502},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Randomnet: Clustering time series using untrained deep neural networks},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling event sequence data by type-wise neural point
process. <em>DMKD</em>, <em>38</em>(6), 3449–3472. (<a
href="https://doi.org/10.1007/s10618-024-01047-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event sequence data widely exists in real life, where each event is typically represented as a tuple, event type and occurrence time. Recently, neural point process (NPP), a probabilistic model that learns the next event distribution with events history given, has gained a lot of attention for event sequence modelling. Existing NPP models use one single vector to encode the whole events history. However, each type of event has its own historical events of concern, which should have led to a different encoding for events history. To this end, we propose Type-wise Neural Point Process (TNPP), with each type of event having a history vector to encode the historical events of its own interest. Type-wise encoding further leads to the realization of type-wise decoding, which together makes a more effective neural point process. Experimental results on six datasets show that TNPP outperforms existing models on the event type prediction task under both extrapolation and interpolation setting. Moreover, the results in terms of scalability and interpretability show that TNPP scales well to datasets with many event types and can provide high-quality event dependencies for interpretation. The code and data can be found at https://github.com/lbq8942/TNPP .},
  archive      = {J_DMKD},
  author       = {Liu, Bingqing},
  doi          = {10.1007/s10618-024-01047-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3449-3472},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Modelling event sequence data by type-wise neural point process},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoRF: A geospatial random forest. <em>DMKD</em>,
<em>38</em>(6), 3414–3448. (<a
href="https://doi.org/10.1007/s10618-024-01046-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geospatial domain increasingly relies on data-driven methodologies to extract actionable insights from the growing volume of available data. Despite the effectiveness of tree-based models in capturing complex relationships between features and targets, they fall short when it comes to considering spatial factors. This limitation arises from their reliance on univariate, axis-parallel splits that result in rectangular areas on a map. To address this issue and enhance both performance and interpretability, we propose a solution that introduces two novel bivariate splits: an oblique and Gaussian split designed specifically for geographic coordinates. Our innovation, called Geospatial Random Forest (geoRF), builds upon Geospatial Regression Trees (GeoTrees) to effectively incorporate geographic features and extract maximum spatial insights. Through an extensive benchmark, we show that our geoRF model outperforms traditional spatial statistical models, other spatial RF variations, machine learning and deep learning methods across a range of geospatial tasks. Furthermore, we contextualize our method’s computational time complexity relative to baseline approaches. Our prediction maps illustrate that geoRF produces more robust and intuitive decision boundaries compared to conventional tree-based models. Utilizing impurity-based feature importance measures, we validate geoRF’s effectiveness in highlighting the significance of geographic coordinates, especially in data sets exhibiting pronounced spatial patterns.},
  archive      = {J_DMKD},
  author       = {Geerts, Margot and vanden Broucke, Seppe and De Weerdt, Jochen},
  doi          = {10.1007/s10618-024-01046-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3414-3448},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {GeoRF: A geospatial random forest},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust explainer recommendation for time series
classification. <em>DMKD</em>, <em>38</em>(6), 3372–3413. (<a
href="https://doi.org/10.1007/s10618-024-01045-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification is a task which deals with temporal sequences, a prevalent data type common in domains such as human activity recognition, sports analytics and general sensing. In this area, interest in explanability has been growing as explanation is key to understand the data and the model better. Recently, a great variety of techniques (e.g., LIME, SHAP, CAM) have been proposed and adapted for time series to provide explanation in the form of saliency maps, where the importance of each data point in the time series is quantified with a numerical value. However, the saliency maps can and often disagree, so it is unclear which one to use. This paper provides a novel framework to quantitatively evaluate and rank explanation methods for time series classification. We show how to robustly evaluate the informativeness of a given explanation method (i.e., relevance for the classification task), and how to compare explanations side-by-side. The goal is to recommend the best explainer for a given time series classification dataset. We propose AMEE, a Model-Agnostic Explanation Evaluation framework, for recommending saliency-based explanations for time series classification. In this approach, data perturbation is added to the input time series guided by each explanation. Our results show that perturbing discriminative parts of the time series leads to significant changes in classification accuracy, which can be used to evaluate each explanation. To be robust to different types of perturbations and different types of classifiers, we aggregate the accuracy loss across perturbations and classifiers. This novel approach allows us to recommend the best explainer among a set of different explainers, including random and oracle explainers. We provide a quantitative and qualitative analysis for synthetic datasets, a variety of time-series datasets, as well as a real-world case study with known expert ground truth.},
  archive      = {J_DMKD},
  author       = {Nguyen, Thu Trang and Le Nguyen, Thach and Ifrim, Georgiana},
  doi          = {10.1007/s10618-024-01045-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3372-3413},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Robust explainer recommendation for time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discord-based counterfactual explanations for time series
classification. <em>DMKD</em>, <em>38</em>(6), 3347–3371. (<a
href="https://doi.org/10.1007/s10618-024-01028-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The opacity inherent in machine learning models presents a significant hindrance to their widespread incorporation into decision-making processes. To address this challenge and foster trust among stakeholders while ensuring decision fairness, the data mining community has been actively advancing the explainable artificial intelligence paradigm. This paper contributes to the evolving field by focusing on counterfactual generation for time series classification models, a domain where research is relatively scarce. We develop, a post-hoc, model agnostic counterfactual explanation algorithm that leverages the Matrix Profile to map time series discords to their nearest neighbors in a target sequence and use this mapping to generate new counterfactual instances. To our knowledge, this is the first effort towards the use of time series discords for counterfactual explanations. We evaluate our algorithm on the University of California Riverside and University of East Anglia archives and compare it to three state-of-the-art univariate and multivariate methods.},
  archive      = {J_DMKD},
  author       = {Bahri, Omar and Li, Peiyu and Filali Boubrahimi, Soukaina and Hamdi, Shah Muhammad},
  doi          = {10.1007/s10618-024-01028-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {3347-3371},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Discord-based counterfactual explanations for time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximation trees: Statistical reproducibility in model
distillation. <em>DMKD</em>, <em>38</em>(5), 3308–3346. (<a
href="https://doi.org/10.1007/s10618-022-00907-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the reproducibility of learned explanations for black-box predictions via model distillation using classification trees. We find that common tree distillation methods fail to reproduce a single stable explanation when applied to the same teacher model due the randomness of the distillation process. We study this issue of reliable interpretation and propose a standardized framework for tree distillation to achieve reproducibility. The proposed framework consists of (1) a statistical test to stabilize tree splits, and (2) a stopping rule for tree building when using a teacher that provides an estimate of the uncertainty of its predictions, e.g. random forests. We demonstrate the empirical performance of the proposed distillation method on a variety of synthetic and real-world datasets.},
  archive      = {J_DMKD},
  author       = {Zhou, Yichen and Zhou, Zhengze and Hooker, Giles},
  doi          = {10.1007/s10618-022-00907-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3308-3346},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Approximation trees: Statistical reproducibility in model distillation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preventing deception with explanation methods using focused
sampling. <em>DMKD</em>, <em>38</em>(5), 3262–3307. (<a
href="https://doi.org/10.1007/s10618-022-00900-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models are used in many sensitive areas where, besides predictive accuracy, their comprehensibility is also essential. Interpretability of prediction models is necessary to determine their biases and causes of errors and is a prerequisite for users’ confidence. For complex state-of-the-art black-box models, post-hoc model-independent explanation techniques are an established solution. Popular and effective techniques, such as IME, LIME, and SHAP, use perturbation of instance features to explain individual predictions. Recently, (Slack et al. in Fooling LIME and SHAP: Adversarial attacks on post-hoc explanation methods, 2020) put their robustness into question by showing that their outcomes can be manipulated due to inadequate perturbation sampling employed. This weakness would allow owners of sensitive models to deceive inspection and hide potentially unethical or illegal biases existing in their predictive models. Such possibility could undermine public trust in machine learning models and give rise to legal restrictions on their use. We show that better sampling in these explanation methods prevents malicious manipulations. The proposed sampling uses data generators that learn the training set distribution and generate new perturbation instances much more similar to the training set. We show that the improved sampling increases the LIME and SHAP’s robustness, while the previously untested method IME is the most robust. Further ablation studies show how the enhanced sampling changes the quality of explanations, reveal differences between data generators, and analyze the effect of different level of conservatism in the employment of biased classifiers.},
  archive      = {J_DMKD},
  author       = {Vreš, Domen and Robnik-Šikonja, Marko},
  doi          = {10.1007/s10618-022-00900-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3262-3307},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Preventing deception with explanation methods using focused sampling},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On GNN explainability with activation rules. <em>DMKD</em>,
<em>38</em>(5), 3227–3261. (<a
href="https://doi.org/10.1007/s10618-022-00870-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GNNs are powerful models based on node representation learning that perform particularly well in many machine learning problems related to graphs. The major obstacle to the deployment of GNNs is mostly a problem of societal acceptability and trustworthiness, properties which require making explicit the internal functioning of such models. Here, we propose to mine activation rules in the hidden layers to understand how the GNNs perceive the world. The problem is not to discover activation rules that are individually highly discriminating for an output of the model. Instead, the challenge is to provide a small set of rules that cover all input graphs. To this end, we introduce the subjective activation pattern domain. We define an effective and principled algorithm to enumerate activations rules in each hidden layer. The proposed approach for quantifying the interest of these rules is rooted in information theory and is able to account for background knowledge on the input graph data. The activation rules can then be redescribed thanks to pattern languages involving interpretable features. We show that the activation rules provide insights on the characteristics used by the GNN to classify the graphs. Especially, this allows to identify the hidden features built by the GNN through its different layers. Also, these rules can subsequently be used for explaining GNN decisions. Experiments on both synthetic and real-life datasets show highly competitive performance, with up to $$200\%$$ improvement in fidelity on explaining graph classification over the SOTA methods.},
  archive      = {J_DMKD},
  author       = {Veyrin-Forrer, Luca and Kamal, Ataollah and Duffner, Stefan and Plantevit, Marc and Robardet, Céline},
  doi          = {10.1007/s10618-022-00870-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3227-3261},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On GNN explainability with activation rules},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining deep convolutional models by measuring the
influence of interpretable features in image classification.
<em>DMKD</em>, <em>38</em>(5), 3169–3226. (<a
href="https://doi.org/10.1007/s10618-023-00915-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy and flexibility of Deep Convolutional Neural Networks (DCNNs) have been highly validated over the past years. However, their intrinsic opaqueness is still affecting their reliability and limiting their application in critical production systems, where the black-box behavior is difficult to be accepted. This work proposes EBAnO, an innovative explanation framework able to analyze the decision-making process of DCNNs in image classification by providing prediction-local and class-based model-wise explanations through the unsupervised mining of knowledge contained in multiple convolutional layers. EBAnO provides detailed visual and numerical explanations thanks to two specific indexes that measure the features’ influence and their influence precision in the decision-making process. The framework has been experimentally evaluated, both quantitatively and qualitatively, by (i) analyzing its explanations with four state-of-the-art DCNN architectures, (ii) comparing its results with three state-of-the-art explanation strategies and (iii) assessing its effectiveness and easiness of understanding through human judgment, by means of an online survey. EBAnO has been released as open-source code and it is freely available online.},
  archive      = {J_DMKD},
  author       = {Ventura, Francesco and Greco, Salvatore and Apiletti, Daniele and Cerquitelli, Tania},
  doi          = {10.1007/s10618-023-00915-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3169-3226},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Explaining deep convolutional models by measuring the influence of interpretable features in image classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explanatory artificial intelligence (YAI): Human-centered
explanations of explainable AI and complex data. <em>DMKD</em>,
<em>38</em>(5), 3141–3168. (<a
href="https://doi.org/10.1007/s10618-022-00872-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a new class of software tools engaged in delivering successful explanations of complex processes on top of basic Explainable AI (XAI) software systems. These tools, that we call cumulatively Explanatory AI (YAI) systems, enhance the quality of the basic output of a XAI by adopting a user-centred approach to explanation that can cater to the individual needs of the explainees with measurable improvements in usability. Our approach is based on Achinstein’s theory of explanations, where explaining is an illocutionary (i.e., broad yet pertinent and deliberate) act of pragmatically answering a question. Accordingly, user-centrality enters in the equation by considering that the overall amount of information generated by answering all questions can rapidly become overwhelming and that individual users may perceive the need to explore just a few of them. In this paper, we give the theoretical foundations of YAI, formally defining a user-centred explanatory tool and the space of all possible explanations, or explanatory space, generated by it. To this end, we frame the explanatory space as an hypergraph of knowledge and we identify a set of heuristics and properties that can help approximating a decomposition of it into a tree-like representation for efficient and user-centred explanation retrieval. Finally, we provide some old and new empirical results to support our theory, showing that explanations are more than textual or visual presentations of the sole information provided by a XAI.},
  archive      = {J_DMKD},
  author       = {Sovrano, Francesco and Vitali, Fabio},
  doi          = {10.1007/s10618-022-00872-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3141-3168},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Explanatory artificial intelligence (YAI): Human-centered explanations of explainable AI and complex data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable representations in explainable AI: From theory
to practice. <em>DMKD</em>, <em>38</em>(5), 3102–3140. (<a
href="https://doi.org/10.1007/s10618-024-01010-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretable representations are the backbone of many explainers that target black-box predictive systems based on artificial intelligence and machine learning algorithms. They translate the low-level data representation necessary for good predictive performance into high-level human-intelligible concepts used to convey the explanatory insights. Notably, the explanation type and its cognitive complexity are directly controlled by the interpretable representation, tweaking which allows to target a particular audience and use case. However, many explainers built upon interpretable representations overlook their merit and fall back on default solutions that often carry implicit assumptions, thereby degrading the explanatory power and reliability of such techniques. To address this problem, we study properties of interpretable representations that encode presence and absence of human-comprehensible concepts. We demonstrate how they are operationalised for tabular, image and text data; discuss their assumptions, strengths and weaknesses; identify their core building blocks; and scrutinise their configuration and parameterisation. In particular, this in-depth analysis allows us to pinpoint their explanatory properties, desiderata and scope for (malicious) manipulation in the context of tabular data where a linear model is used to quantify the influence of interpretable concepts on a black-box prediction. Our findings lead to a range of recommendations for designing trustworthy interpretable representations; specifically, the benefits of class-aware (supervised) discretisation of tabular data, e.g., with decision trees, and sensitivity of image interpretable representations to segmentation granularity and occlusion colour.},
  archive      = {J_DMKD},
  author       = {Sokol, Kacper and Flach, Peter},
  doi          = {10.1007/s10618-024-01010-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3102-3140},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interpretable representations in explainable AI: From theory to practice},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive taxonomy for explainable artificial
intelligence: A systematic survey of surveys on methods and concepts.
<em>DMKD</em>, <em>38</em>(5), 3043–3101. (<a
href="https://doi.org/10.1007/s10618-022-00867-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.},
  archive      = {J_DMKD},
  author       = {Schwalbe, Gesina and Finzel, Bettina},
  doi          = {10.1007/s10618-022-00867-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {3043-3101},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comprehensive taxonomy for explainable artificial intelligence: A systematic survey of surveys on methods and concepts},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Marginal effects for non-linear prediction functions.
<em>DMKD</em>, <em>38</em>(5), 2997–3042. (<a
href="https://doi.org/10.1007/s10618-023-00993-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beta coefficients for linear regression models represent the ideal form of an interpretable feature effect. However, for non-linear models such as generalized linear models, the estimated coefficients cannot be interpreted as a direct feature effect on the predicted outcome. Hence, marginal effects are typically used as approximations for feature effects, either as derivatives of the prediction function or forward differences in prediction due to changes in feature values. While marginal effects are commonly used in many scientific fields, they have not yet been adopted as a general model-agnostic interpretation method for machine learning models. This may stem from the ambiguity surrounding marginal effects and their inability to deal with the non-linearities found in black box models. We introduce a unified definition of forward marginal effects (FMEs) that includes univariate and multivariate, as well as continuous, categorical, and mixed-type features. To account for the non-linearity of prediction functions, we introduce a non-linearity measure for FMEs. Furthermore, we argue against summarizing feature effects of a non-linear prediction function in a single metric such as the average marginal effect. Instead, we propose to average homogeneous FMEs within population subgroups, which serve as conditional feature effect estimates.},
  archive      = {J_DMKD},
  author       = {Scholbeck, Christian A. and Casalicchio, Giuseppe and Molnar, Christoph and Bischl, Bernd and Heumann, Christian},
  doi          = {10.1007/s10618-023-00993-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2997-3042},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Marginal effects for non-linear prediction functions},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reflective-net: Learning from explanations. <em>DMKD</em>,
<em>38</em>(5), 2975–2996. (<a
href="https://doi.org/10.1007/s10618-023-00920-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine whether data generated by explanation techniques, which promote a process of self-reflection, can improve classifier performance. Our work is based on the idea that humans have the ability to make quick, intuitive decisions as well as to reflect on their own thinking and learn from explanations. To the best of our knowledge, this is the first time that the potential of mimicking this process by using explanations generated by explainability methods has been explored. We found that combining explanations with traditional labeled data leads to significant improvements in classification accuracy and training efficiency across multiple image classification datasets and convolutional neural network architectures. It is worth noting that during training, we not only used explanations for the correct or predicted class, but also for other classes. This serves multiple purposes, including allowing for reflection on potential outcomes and enriching the data through augmentation.},
  archive      = {J_DMKD},
  author       = {Schneider, Johannes and Vlachos, Michalis},
  doi          = {10.1007/s10618-023-00920-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2975-2996},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Reflective-net: Learning from explanations},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining pareto-optimal counterfactual antecedents with a
branch-and-bound model-agnostic algorithm. <em>DMKD</em>,
<em>38</em>(5), 2942–2974. (<a
href="https://doi.org/10.1007/s10618-022-00906-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining counterfactual antecedents became a valuable tool to discover knowledge and explain machine learning models. It consists of generating synthetic samples from an original sample to achieve the desired outcome in a machine learning model, thus helping to understand the prediction. An insightful methodology would explore a broader set of counterfactual antecedents to reveal multiple possibilities while operating on any classifier. Therefore, we create a tree-based search that requires monotonicity from the objective functions (a.k.a. cost functions); it allows pruning branches that will not improve the objective functions. Since monotonicity is only required for the objective function, this method can be used for any family of classifiers (e.g., linear models, neural networks, and decision trees). However, additional classifier properties speed up the tree search when it foresees branches that will not result in feasible actions. Moreover, the proposed optimization generates a diverse set of Pareto-optimal counterfactual antecedents by relying on multi-objective concepts. The results show an algorithm with working guarantees that enumerates a wide range of counterfactual antecedents. It helps the decision-maker understand the machine learning decision and finds alternatives to achieve the desired outcome. The user can inspect these multiple counterfactual antecedents to find the most suitable one and better understand the prediction.},
  archive      = {J_DMKD},
  author       = {Raimundo, Marcos M. and Nonato, Luis Gustavo and Poco, Jorge},
  doi          = {10.1007/s10618-022-00906-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2942-2974},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining pareto-optimal counterfactual antecedents with a branch-and-bound model-agnostic algorithm},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic feature importance and effects with dependent
features: A conditional subgroup approach. <em>DMKD</em>,
<em>38</em>(5), 2903–2941. (<a
href="https://doi.org/10.1007/s10618-022-00901-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretation of feature importance in machine learning models is challenging when features are dependent. Permutation feature importance (PFI) ignores such dependencies, which can cause misleading interpretations due to extrapolation. A possible remedy is more advanced conditional PFI approaches that enable the assessment of feature importance conditional on all other features. Due to this shift in perspective and in order to enable correct interpretations, it is beneficial if the conditioning is transparent and comprehensible. In this paper, we propose a new sampling mechanism for the conditional distribution based on permutations in conditional subgroups. As these subgroups are constructed using tree-based methods such as transformation trees, the conditioning becomes inherently interpretable. This not only provides a simple and effective estimator of conditional PFI, but also local PFI estimates within the subgroups. In addition, we apply the conditional subgroups approach to partial dependence plots, a popular method for describing feature effects that can also suffer from extrapolation when features are dependent and interactions are present in the model. In simulations and a real-world application, we demonstrate the advantages of the conditional subgroup approach over existing methods: It allows to compute conditional PFI that is more true to the data than existing proposals and enables a fine-grained interpretation of feature effects and importance within the conditional subgroups.},
  archive      = {J_DMKD},
  author       = {Molnar, Christoph and König, Gunnar and Bischl, Bernd and Casalicchio, Giuseppe},
  doi          = {10.1007/s10618-022-00901-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2903-2941},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Model-agnostic feature importance and effects with dependent features: A conditional subgroup approach},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse oblique decision trees: A tool to understand and
manipulate neural net features. <em>DMKD</em>, <em>38</em>(5),
2863–2902. (<a
href="https://doi.org/10.1007/s10618-022-00892-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread deployment of deep nets in practical applications has lead to a growing desire to understand how and why such black-box methods perform prediction. Much work has focused on understanding what part of the input pattern (an image, say) is responsible for a particular class being predicted, and how the input may be manipulated to predict a different class. We focus instead on understanding which of the internal features computed by the neural net are responsible for a particular class. We achieve this by mimicking part of the neural net with an oblique decision tree having sparse weight vectors at the decision nodes. Using the recently proposed Tree Alternating Optimization (TAO) algorithm, we are able to learn trees that are both highly accurate and interpretable. Such trees can faithfully mimic the part of the neural net they replaced, and hence they can provide insights into the deep net black box. Further, we show we can easily manipulate the neural net features in order to make the net predict, or not predict, a given class, thus showing that it is possible to carry out adversarial attacks at the level of the features. These insights and manipulations apply globally to the entire training and test set, not just at a local (single-instance) level. We demonstrate this robustly in the MNIST and ImageNet datasets with LeNet5 and VGG networks.},
  archive      = {J_DMKD},
  author       = {Hada, Suryabhan Singh and Carreira-Perpiñán, Miguel Á. and Zharmagambetov, Arman},
  doi          = {10.1007/s10618-022-00892-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2863-2902},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sparse oblique decision trees: A tool to understand and manipulate neural net features},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable and actionable explanations of black-box models
through factual and counterfactual rules. <em>DMKD</em>, <em>38</em>(5),
2825–2862. (<a
href="https://doi.org/10.1007/s10618-022-00878-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rise of accurate but obscure classification models that hide the logic of their internal decision processes. Explaining the decision taken by a black-box classifier on a specific input instance is therefore of striking interest. We propose a local rule-based model-agnostic explanation method providing stable and actionable explanations. An explanation consists of a factual logic rule, stating the reasons for the black-box decision, and a set of actionable counterfactual logic rules, proactively suggesting the changes in the instance that lead to a different outcome. Explanations are computed from a decision tree that mimics the behavior of the black-box locally to the instance to explain. The decision tree is obtained through a bagging-like approach that favors stability and fidelity: first, an ensemble of decision trees is learned from neighborhoods of the instance under investigation; then, the ensemble is merged into a single decision tree. Neighbor instances are synthetically generated through a genetic algorithm whose fitness function is driven by the black-box behavior. Experiments show that the proposed method advances the state-of-the-art towards a comprehensive approach that successfully covers stability and actionability of factual and counterfactual explanations.},
  archive      = {J_DMKD},
  author       = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Naretto, Francesca and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
  doi          = {10.1007/s10618-022-00878-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2825-2862},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Stable and actionable explanations of black-box models through factual and counterfactual rules},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactual explanations and how to find them: Literature
review and benchmarking. <em>DMKD</em>, <em>38</em>(5), 2770–2824. (<a
href="https://doi.org/10.1007/s10618-022-00831-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
  archive      = {J_DMKD},
  author       = {Guidotti, Riccardo},
  doi          = {10.1007/s10618-022-00831-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2770-2824},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Counterfactual explanations and how to find them: Literature review and benchmarking},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactual explanations as interventions in latent
space. <em>DMKD</em>, <em>38</em>(5), 2733–2769. (<a
href="https://doi.org/10.1007/s10618-022-00889-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) is a set of techniques that allows the understanding of both technical and non-technical aspects of Artificial Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly important demand of trustworthy Artificial Intelligence, characterized by fundamental aspects such as respect of human autonomy, prevention of harm, transparency, accountability, etc. Within XAI techniques, counterfactual explanations aim to provide to end users a set of features (and their corresponding values) that need to be changed in order to achieve a desired outcome. Current approaches rarely take into account the feasibility of actions needed to achieve the proposed explanations, and in particular, they fall short of considering the causal impact of such actions. In this paper, we present Counterfactual Explanations as Interventions in Latent Space (CEILS), a methodology to generate counterfactual explanations capturing by design the underlying causal relations from the data, and at the same time to provide feasible recommendations to reach the proposed profile. Moreover, our methodology has the advantage that it can be set on top of existing counterfactuals generator algorithms, thus minimising the complexity of imposing additional causal constrains. We demonstrate the effectiveness of our approach with a set of different experiments using synthetic and real datasets (including a proprietary dataset of the financial domain).},
  archive      = {J_DMKD},
  author       = {Crupi, Riccardo and Castelnovo, Alessandro and Regoli, Daniele and San Miguel Gonzalez, Beatriz},
  doi          = {10.1007/s10618-022-00889-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2733-2769},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Counterfactual explanations as interventions in latent space},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A case study of improving a non-technical losses detection
system through explainability. <em>DMKD</em>, <em>38</em>(5), 2704–2732.
(<a href="https://doi.org/10.1007/s10618-023-00927-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and reacting to non-technical losses (NTL) is a fundamental activity that energy providers need to face in their daily routines. This is known to be challenging since the phenomenon of NTL is multi-factored, dynamic and extremely contextual, which makes artificial intelligence (AI) and, in particular, machine learning, natural areas to bring effective and tailored solutions. If the human factor is disregarded in the process of detecting NTL, there is a high risk of performance degradation since typical problems like dataset shift and biases cannot be easily identified by an algorithm. This paper presents a case study on incorporating explainable AI (XAI) in a mature NTL detection system that has been in production in the last years both in electricity and gas. The experience shows that incorporating this capability brings interesting improvements to the initial system and especially serves as a common ground where domain experts, data scientists, and business analysts can meet.},
  archive      = {J_DMKD},
  author       = {Coma-Puig, Bernat and Calvo, Albert and Carmona, Josep and Gavaldà, Ricard},
  doi          = {10.1007/s10618-023-00927-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2704-2732},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A case study of improving a non-technical losses detection system through explainability},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NICE: An algorithm for nearest instance counterfactual
explanations. <em>DMKD</em>, <em>38</em>(5), 2665–2703. (<a
href="https://doi.org/10.1007/s10618-023-00930-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new algorithm, named NICE, to generate counterfactual explanations for tabular data that specifically takes into account algorithmic requirements that often emerge in real-life deployments: (1) the ability to provide an explanation for all predictions, (2) being able to handle any classification model (also non-differentiable ones), (3) being efficient in run time, and (4) providing multiple counterfactual explanations with different characteristics. More specifically, our approach exploits information from a nearest unlike neighbor to speed up the search process, by iteratively introducing feature values from this neighbor in the instance to be explained. We propose four versions of NICE, one without optimization and, three which optimize the explanations for one of the following properties: sparsity, proximity or plausibility. An extensive empirical comparison on 40 datasets shows that our algorithm outperforms the current state-of-the-art in terms of these criteria. Our analyses show a trade-off between on the one hand plausibility and on the other hand proximity or sparsity, with our different optimization methods offering users the choice to select the types of counterfactuals that they prefer. An open-source implementation of NICE can be found at https://github.com/ADMAntwerp/NICE.},
  archive      = {J_DMKD},
  author       = {Brughmans, Dieter and Leyman, Pieter and Martens, David},
  doi          = {10.1007/s10618-023-00930-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2665-2703},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {NICE: An algorithm for nearest instance counterfactual explanations},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shapley values for cluster importance. <em>DMKD</em>,
<em>38</em>(5), 2633–2664. (<a
href="https://doi.org/10.1007/s10618-022-00896-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel approach to explain the predictions made by data-driven methods. Since such predictions rely heavily on the data used for training, explanations that convey information about how the training data affects the predictions are useful. The paper proposes a novel approach to quantify how different data-clusters of the training data affect a prediction. The quantification is based on Shapley values, a concept which originates from coalitional game theory, developed to fairly distribute the payout among a set of cooperating players. A player’s Shapley value is a measure of that player’s contribution. Shapley values are often used to quantify feature importance, ie. how features affect a prediction. This paper extends this to cluster importance, letting clusters of the training data act as players in a game where the predictions are the payouts. The novel methodology proposed in this paper lets us explore and investigate how different clusters of the training data affect the predictions made by any black-box model, allowing new aspects of the reasoning and inner workings of a prediction model to be conveyed to the users. The methodology is fundamentally different from existing explanation methods, providing insight which would not be available otherwise, and should complement existing explanation methods, including explanations based on feature importance.},
  archive      = {J_DMKD},
  author       = {Brandsæter, Andreas and Glad, Ingrid K.},
  doi          = {10.1007/s10618-022-00896-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2633-2664},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Shapley values for cluster importance},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The grammar of interactive explanatory model analysis.
<em>DMKD</em>, <em>38</em>(5), 2596–2632. (<a
href="https://doi.org/10.1007/s10618-023-00924-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing need for in-depth analysis of predictive models leads to a series of new methods for explaining their local and global properties. Which of these methods is the best? It turns out that this is an ill-posed question. One cannot sufficiently explain a black-box machine learning model using a single method that gives only one perspective. Isolated explanations are prone to misunderstanding, leading to wrong or simplistic reasoning. This problem is known as the Rashomon effect and refers to diverse, even contradictory, interpretations of the same phenomenon. Surprisingly, most methods developed for explainable and responsible machine learning focus on a single-aspect of the model behavior. In contrast, we showcase the problem of explainability as an interactive and sequential analysis of a model. This paper proposes how different Explanatory Model Analysis (EMA) methods complement each other and discusses why it is essential to juxtapose them. The introduced process of Interactive EMA (IEMA) derives from the algorithmic side of explainable machine learning and aims to embrace ideas developed in cognitive sciences. We formalize the grammar of IEMA to describe human-model interaction. It is implemented in a widely used human-centered open-source software framework that adopts interactivity, customizability and automation as its main traits. We conduct a user study to evaluate the usefulness of IEMA, which indicates that an interactive sequential analysis of a model may increase the accuracy and confidence of human decision making.},
  archive      = {J_DMKD},
  author       = {Baniecki, Hubert and Parzych, Dariusz and Biecek, Przemyslaw},
  doi          = {10.1007/s10618-023-00924-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2596-2632},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The grammar of interactive explanatory model analysis},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable and interpretable machine learning and data
mining. <em>DMKD</em>, <em>38</em>(5), 2571–2595. (<a
href="https://doi.org/10.1007/s10618-024-01041-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing number of applications of machine learning and data mining in many domains—from agriculture to business, education, industrial manufacturing, and medicine—gave rise to new requirements for how to inspect and control the learned models. The research domain of explainable artificial intelligence (XAI) has been newly established with a strong focus on methods being applied post-hoc on black-box models. As an alternative, the use of interpretable machine learning methods has been considered—where the learned models are white-box ones. Black-box models can be characterized as representing implicit knowledge—typically resulting from statistical and neural approaches of machine learning, while white-box models are explicit representations of knowledge—typically resulting from rule-learning approaches. In this introduction to the special issue on ‘Explainable and Interpretable Machine Learning and Data Mining’ we propose to bring together both perspectives, pointing out commonalities and discussing possibilities to integrate them.},
  archive      = {J_DMKD},
  author       = {Atzmueller, Martin and Fürnkranz, Johannes and Kliegr, Tomáš and Schmid, Ute},
  doi          = {10.1007/s10618-024-01041-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2571-2595},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Explainable and interpretable machine learning and data mining},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of variable ordering on bayesian network
structure learning. <em>DMKD</em>, <em>38</em>(4), 2545–2569. (<a
href="https://doi.org/10.1007/s10618-024-01044-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal Bayesian Networks (CBNs) provide an important tool for reasoning under uncertainty with potential application to many complex causal systems. Structure learning algorithms that can tell us something about the causal structure of these systems are becoming increasingly important. In the literature, the validity of these algorithms is often tested for sensitivity over varying sample sizes, hyper-parameters, and occasionally objective functions, but the effect of the order in which the variables are read from data is rarely quantified. We show that many commonly-used algorithms, both established and state-of-the-art, are more sensitive to variable ordering than these other factors when learning CBNs from discrete variables. This effect is strongest in hill-climbing and its variants where we explain how it arises, but extends to hybrid, and to a lesser-extent, constraint-based algorithms. Because the variable ordering is arbitrary, any significant effect it has on learnt graph accuracy is concerning, and raises questions about the validity of both many older and more recent results produced by these algorithms in practical applications and their rankings in performance evaluations.},
  archive      = {J_DMKD},
  author       = {Kitson, Neville K. and Constantinou, Anthony C.},
  doi          = {10.1007/s10618-024-01044-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2545-2569},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The impact of variable ordering on bayesian network structure learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Series2vec: Similarity-based self-supervised representation
learning for time series classification. <em>DMKD</em>, <em>38</em>(4),
2520–2544. (<a
href="https://doi.org/10.1007/s10618-024-01043-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We argue that time series analysis is fundamentally different in nature to either vision or natural language processing with respect to the forms of meaningful self-supervised learning tasks that can be defined. Motivated by this insight, we introduce a novel approach called Series2Vec for self-supervised representation learning. Unlike the state-of-the-art methods in time series which rely on hand-crafted data augmentation, Series2Vec is trained by predicting the similarity between two series in both temporal and spectral domains through a self-supervised task. By leveraging the similarity prediction task, which has inherent meaning for a wide range of time series analysis tasks, Series2Vec eliminates the need for hand-crafted data augmentation. To further enforce the network to learn similar representations for similar time series, we propose a novel approach that applies order-invariant attention to each representation within the batch during training. Our evaluation of Series2Vec on nine large real-world datasets, along with the UCR/UEA archive, shows enhanced performance compared to current state-of-the-art self-supervised techniques for time series. Additionally, our extensive experiments show that Series2Vec performs comparably with fully supervised training and offers high efficiency in datasets with limited-labeled data. Finally, we show that the fusion of Series2Vec with other representation learning models leads to enhanced performance for time series classification. Code and models are open-source at https://github.com/Navidfoumani/Series2Vec},
  archive      = {J_DMKD},
  author       = {Foumani, Navid Mohammadi and Tan, Chang Wei and Webb, Geoffrey I. and Rezatofighi, Hamid and Salehi, Mahsa},
  doi          = {10.1007/s10618-024-01043-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2520-2544},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Series2vec: Similarity-based self-supervised representation learning for time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uplift modeling with quasi-loss-functions. <em>DMKD</em>,
<em>38</em>(4), 2495–2519. (<a
href="https://doi.org/10.1007/s10618-024-01042-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uplift modeling, also referred to as heterogeneous treatment effect estimation, is a machine learning technique utilized in marketing for estimating the incremental impact of treatment on the response of each customer. Uplift models face a fundamental challenge in causal inference because the variable of interest (i.e., the uplift itself) remains unobservable. As a result, popular uplift models (such as meta-learners and uplift trees) do not incorporate loss functions for uplifts in their algorithms. This article addresses that gap by proposing uplift models with quasi-loss functions (UpliftQL models), which separately use four specially designed quasi-loss functions for uplift estimation in algorithms. Using simulated data, our analysis reveals that, on average, 55% (34%) of the top five models from a set of 14 are UpliftQL models for binary (continuous) outcomes. Further empirical data analysis shows that over 60% of the top-performing models are consistently UpliftQL models.},
  archive      = {J_DMKD},
  author       = {Hu, Jinping and de Haan, Evert and Skiera, Bernd},
  doi          = {10.1007/s10618-024-01042-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2495-2519},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Uplift modeling with quasi-loss-functions},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the impact of out-of-schema questions in
task-oriented dialog systems. <em>DMKD</em>, <em>38</em>(4), 2466–2494.
(<a href="https://doi.org/10.1007/s10618-024-01039-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing work on task-oriented dialog systems generally assumes that the interaction of users with the system is restricted to the information stored in a closed data schema. However, in practice users may ask ‘out-of-schema’ questions, that is, questions that the system cannot answer, because the information does not exist in the schema. Failure to answer these questions may lead the users to drop out of the chat before reaching the success state (e.g. reserving a restaurant). A key challenge is that the number of these questions may be too high for a domain expert to answer them all. We formulate the problem of out-of-schema question detection and selection that identifies the most critical out-of-schema questions to answer, in order to maximize the expected success rate of the system. We propose a two-stage pipeline to solve the problem. In the first stage, we propose a novel in-context learning (ICL) approach to detect out-of-schema questions. In the second stage, we propose two algorithms for out-of-schema question selection (OQS): a naive approach that chooses a question based on its frequency in the dropped-out conversations, and a probabilistic approach that represents each conversation as a Markov chain and a question is picked based on its overall benefit. We propose and publish two new datasets for the problem, as existing datasets do not contain out-of-schema questions or user drop-outs. Our quantitative and simulation-based experimental analyses on these datasets measure how our methods can effectively identify out-of-schema questions and positively impact the success rate of the system.},
  archive      = {J_DMKD},
  author       = {Meem, Jannat Ara and Rashid, Muhammad Shihab and Hristidis, Vagelis},
  doi          = {10.1007/s10618-024-01039-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2466-2494},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Modeling the impact of out-of-schema questions in task-oriented dialog systems},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving graph-based recommendation with unraveled graph
learning. <em>DMKD</em>, <em>38</em>(4), 2440–2465. (<a
href="https://doi.org/10.1007/s10618-024-01038-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Collaborative Filtering (GraphCF) has emerged as a promising approach in recommendation systems, leveraging the inferential power of Graph Neural Networks. Furthermore, the integration of contrastive learning has enhanced the performance of GraphCF methods. Recent research has shifted from graph augmentation to noise perturbation in contrastive learning, leading to significant performance improvements. However, we contend that the primary factor in performance enhancement is not graph augmentation or noise perturbation, but rather the balance of the embedding from each layer in the output embedding. To substantiate our claim, we conducted preliminary experiments with multiple state-of-the-art GraphCF methods. Based on our observations and insights, we propose a novel approach named Unraveled Graph Contrastive Learning (UGCL), which includes a new propagation scheme to further enhance performance. To the best of our knowledge, this is the first approach that specifically addresses the balance factor in the output embedding for performance improvement. We have carried out extensive experiments on multiple large-scale benchmark datasets to evaluate the effectiveness of our proposed approach. The results indicate that UGCL significantly outperforms all other state-of-the-art baseline models, also showing superior performance in terms of fairness and debiasing capabilities compared to other baselines.},
  archive      = {J_DMKD},
  author       = {Chang, Chih-Chieh and Tzeng, Diing-Ruey and Lu, Chia-Hsun and Chang, Ming-Yi and Shen, Chih-Ya},
  doi          = {10.1007/s10618-024-01038-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2440-2465},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving graph-based recommendation with unraveled graph learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WaveLSea: Helping experts interactively explore pattern
mining search spaces. <em>DMKD</em>, <em>38</em>(4), 2403–2439. (<a
href="https://doi.org/10.1007/s10618-024-01037-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the method Wave Top-k Random-d Lineage Search (WaveLSea) which guides an expert through data mining results according to her interest. The method exploits expert feedback, combined with the relation between patterns to spread the expert’s interest. It avoids the typical feature definition step commonly used in interactive data mining which limits the flexibility of the discovery process. We empirically demonstrate that WaveLSea returns the most relevant results for the user’s subjective interest. Even with imperfect feedback, WaveLSea behavior remains robust as it primarily still delivers most interesting results during experiments on graph-structured data. In order to assess the robustness of the method we design novel oracles called soothsayers giving imperfect feedback. Finally, we complete our quantitative study with a qualitative study using a user interface to evaluate WaveLSea.},
  archive      = {J_DMKD},
  author       = {Lehembre, Etienne and Cremilleux, Bruno and Zimmermann, Albrecht and Cuissart, Bertrand and Ouali, Abdelkader},
  doi          = {10.1007/s10618-024-01037-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2403-2439},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {WaveLSea: Helping experts interactively explore pattern mining search spaces},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quant: A minimalist interval method for time series
classification. <em>DMKD</em>, <em>38</em>(4), 2377–2402. (<a
href="https://doi.org/10.1007/s10618-024-01036-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that it is possible to achieve the same accuracy, on average, as the most accurate existing interval methods for time series classification on a standard set of benchmark datasets using a single type of feature (quantiles), fixed intervals, and an ‘off the shelf’ classifier. This distillation of interval-based approaches represents a fast and accurate method for time series classification, achieving state-of-the-art accuracy on the expanded set of 142 datasets in the UCR archive with a total compute time (training and inference) of less than 15 min using a single CPU core.},
  archive      = {J_DMKD},
  author       = {Dempster, Angus and Schmidt, Daniel F. and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-024-01036-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2377-2402},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Quant: A minimalist interval method for time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSGNN: Multi-scale spatio-temporal graph neural network for
epidemic forecasting. <em>DMKD</em>, <em>38</em>(4), 2348–2376. (<a
href="https://doi.org/10.1007/s10618-024-01035-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious disease forecasting has been a key focus and proved to be crucial in controlling epidemic. A recent trend is to develop forecasting models based on graph neural networks (GNNs). However, existing GNN-based methods suffer from two key limitations: (1) current models broaden receptive fields by scaling the depth of GNNs, which is insufficient to preserve the semantics of long-range connectivity between distant but epidemic related areas. (2) Previous approaches model epidemics within single spatial scale, while ignoring the multi-scale epidemic patterns derived from different scales. To address these deficiencies, we devise the Multi-scale Spatio-temporal Graph Neural Network (MSGNN) based on an innovative multi-scale view. To be specific, in the proposed MSGNN model, we first devise a novel graph learning module, which directly captures long-range connectivity from trans-regional epidemic signals and integrates them into a multi-scale graph. Based on the learned multi-scale graph, we utilize a newly designed graph convolution module to exploit multi-scale epidemic patterns. This module allows us to facilitate multi-scale epidemic modeling by mining both scale-shared and scale-specific patterns. Experimental results on forecasting new cases of COVID-19 in United State demonstrate the superiority of our method over state-of-arts. Further analyses and visualization also show that MSGNN offers not only accurate, but also robust and interpretable forecasting result. Code is available at https://github.com/JashinKorone/MSGNN .},
  archive      = {J_DMKD},
  author       = {Qiu, Mingjie and Tan, Zhiyi and Bao, Bing-Kun},
  doi          = {10.1007/s10618-024-01035-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2348-2376},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MSGNN: Multi-scale spatio-temporal graph neural network for epidemic forecasting},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The hadamard decomposition problem. <em>DMKD</em>,
<em>38</em>(4), 2306–2347. (<a
href="https://doi.org/10.1007/s10618-024-01033-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Hadamard decomposition problem in the context of data analysis. The problem is to represent exactly or approximately a given matrix as the Hadamard (or element-wise) product of two or more low-rank matrices. The motivation for this problem comes from situations where the input matrix has a multiplicative structure. The Hadamard decomposition has potential for giving more succint but equally accurate representations of matrices when compared with the gold-standard of singular value decomposition (svd). Namely, the Hadamard product of two rank- $$h$$ matrices can have rank as high as $${h}^2$$ . We study the computational properties of the Hadamard decomposition problem and give gradient-based algorithms for solving it approximately. We also introduce a mixed model that combines svd and Hadamard decomposition. We present extensive empirical results comparing the approximation accuracy of the Hadamard decomposition with that of the svd using the same number of basis vectors. The results demonstrate that the Hadamard decomposition is competitive with the svd and, for some datasets, it yields a clearly higher approximation accuracy, indicating the presence of multiplicative structure in the data.},
  archive      = {J_DMKD},
  author       = {Ciaperoni, Martino and Gionis, Aristides and Mannila, Heikki},
  doi          = {10.1007/s10618-024-01033-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2306-2347},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The hadamard decomposition problem},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LoCoMotif: Discovering time-warped motifs in time series.
<em>DMKD</em>, <em>38</em>(4), 2276–2305. (<a
href="https://doi.org/10.1007/s10618-024-01032-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series motif discovery (TSMD) refers to the task of identifying patterns that occur multiple times (possibly with minor variations) in a time series. All existing methods for TSMD have one or more of the following limitations: they only look for the two most similar occurrences of a pattern; they only look for patterns of a pre-specified, fixed length; they cannot handle variability along the time axis; and they only handle univariate time series. In this paper, we present a new method, LoCoMotif, that has none of these limitations. The method is motivated by a concrete use case from physiotherapy. We demonstrate the value of the proposed method on this use case. We also introduce a new quantitative evaluation metric for motif discovery, and benchmark data for comparing TSMD methods. LoCoMotif substantially outperforms the existing methods, on top of being more broadly applicable.},
  archive      = {J_DMKD},
  author       = {Van Wesenbeeck, Daan and Yurtman, Aras and Meert, Wannes and Blockeel, Hendrik},
  doi          = {10.1007/s10618-024-01032-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2276-2305},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {LoCoMotif: Discovering time-warped motifs in time series},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the impact of multi-dimensional local differential
privacy on fairness. <em>DMKD</em>, <em>38</em>(4), 2252–2275. (<a
href="https://doi.org/10.1007/s10618-024-01031-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated decision systems are increasingly used to make consequential decisions in people’s lives. Due to the sensitivity of the manipulated data and the resulting decisions, several ethical concerns need to be addressed for the appropriate use of such technologies, particularly fairness and privacy. Unlike previous work, which focused on centralized differential privacy (DP) or on local DP (LDP) for a single sensitive attribute, in this paper, we examine the impact of LDP in the presence of several sensitive attributes (i.e., multi-dimensional data) on fairness. Detailed empirical analysis on synthetic and benchmark datasets revealed very relevant observations. In particular, (1) multi-dimensional LDP is an efficient approach to reduce disparity, (2) the variant of the multi-dimensional approach of LDP (we employ two variants) matters only at low privacy guarantees (high $$\epsilon$$ ), and (3) the true decision distribution has an important effect on which group is more sensitive to the obfuscation. Last, we summarize our findings in the form of recommendations to guide practitioners in adopting effective privacy-preserving practices while maintaining fairness and utility in machine learning applications.},
  archive      = {J_DMKD},
  author       = {Makhlouf, Karima and Arcolezi, Héber H. and Zhioua, Sami and Brahim, Ghassen Ben and Palamidessi, Catuscia},
  doi          = {10.1007/s10618-024-01031-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2252-2275},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On the impact of multi-dimensional local differential privacy on fairness},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective interpretable learning for large-scale categorical
data. <em>DMKD</em>, <em>38</em>(4), 2223–2251. (<a
href="https://doi.org/10.1007/s10618-024-01030-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale categorical datasets are ubiquitous in machine learning and the success of most deployed machine learning models rely on how effectively the features are engineered. For large-scale datasets, parametric methods are generally used, among which three strategies for feature engineering are quite common. The first strategy focuses on managing the breadth (or width) of a network, e.g., generalized linear models (aka. wide learning). The second strategy focuses on the depth of a network, e.g., Artificial Neural networks or ANN (aka. deep learning). The third strategy relies on factorizing the interaction terms, e.g., Factorization Machines (aka. factorized learning). Each of these strategies brings its own advantages and disadvantages. Recently, it has been shown that for categorical data, combination of various strategies leads to excellent results. For example,  WD-Learning, xdeepFM, etc., leads to state-of-the-art results. Following the trend, in this work, we have proposed another learning framework—WBDF-Learning, based on the combination of wide, deep, factorization, and a newly introduced component named Broad Interaction network (BIN). BIN is in the form of a Bayesian network classifier whose structure is learned apriori, and parameters are learned by optimizing a joint objective function along with wide, deep and factorized parts. We denote the learning of BIN parameters as broad learning. Additionally, the parameters of BIN are constrained to be actual probabilities—therefore, it is extremely interpretable. Furthermore, one can sample or generate data from BIN, which can facilitate learning and provides a framework for knowledge-guided machine learning. We demonstrate that our proposed framework possesses the resilience to maintain excellent classification performance when confronted with biased datasets. We evaluate the efficacy of our framework in terms of classification performance on various benchmark large-scale categorical datasets and compare against state-of-the-art methods. It is shown that, WBDF framework (a) exhibits superior performance on classification tasks, (b) boasts outstanding interpretability and (c) demonstrates exceptional resilience and effectiveness in scenarios involving skewed distributions.},
  archive      = {J_DMKD},
  author       = {Zhang, Yishuo and Zaidi, Nayyar and Zhou, Jiahui and Wang, Tao and Li, Gang},
  doi          = {10.1007/s10618-024-01030-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2223-2251},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Effective interpretable learning for large-scale categorical data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intersectional fair ranking via subgroup divergence.
<em>DMKD</em>, <em>38</em>(4), 2186–2222. (<a
href="https://doi.org/10.1007/s10618-024-01029-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Societal biases encoded in real-world data can contaminate algorithmic decisions, perpetuating preexisting inequalities in domains such as employment and education. In the fair ranking literature, following the doctrine of affirmative action, fairness is enforced by means of a group-fairness constraint requiring “enough” individuals from protected groups in the top-k positions, for a ranking to be considered valid. However, which are the groups that need to be protected? And how much representation is “enough”? As the biases affecting the process may not always be directly observable nor measurable, these questions might be hard to answer in a principled way, especially when many different potentially discriminated subgroups exist. This paper addresses this issue by automatically identifying the disadvantaged groups in the data and mitigating their disparate representation in the final ranking. Our proposal leverages the notion of divergence to automatically identify which subgroups, defined as combination of sensitive attributes, show a statistically significant deviation, in terms of ranking utility, compared to the overall population. Subgroups with negative divergence experience a disadvantage. We formulate the problem of re-ranking instances to maximize the minimum subgroup divergence, while maintaining the new ranking as close as possible to the original one. We develop a method which is based on identifying the divergent subgroups and applying a re-ranking procedure which is monotonic w.r.t. the goal of maximizing the minimum divergence. Our experimental results show that our method effectively eliminates the existence of disadvantaged subgroups while producing rankings which are very close to the original ones.},
  archive      = {J_DMKD},
  author       = {Pastor, Eliana and Bonchi, Francesco},
  doi          = {10.1007/s10618-024-01029-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2186-2222},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Intersectional fair ranking via subgroup divergence},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised feature based algorithms for time series
extrinsic regression. <em>DMKD</em>, <em>38</em>(4), 2141–2185. (<a
href="https://doi.org/10.1007/s10618-024-01027-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random intervals. Our study demonstrates that both algorithms, along with InceptionTime, exhibit significantly better performance compared to the other 18 regressors tested. More importantly, DrCIF is the only one that significantly outperforms a standard rotation forest regressor.},
  archive      = {J_DMKD},
  author       = {Guijo-Rubio, David and Middlehurst, Matthew and Arcencio, Guilherme and Silva, Diego Furtado and Bagnall, Anthony},
  doi          = {10.1007/s10618-024-01027-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2141-2185},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Unsupervised feature based algorithms for time series extrinsic regression},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning with biased non-response to label requests.
<em>DMKD</em>, <em>38</em>(4), 2117–2140. (<a
href="https://doi.org/10.1007/s10618-024-01026-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning can improve the efficiency of training prediction models by identifying the most informative new labels to acquire. However, non-response to label requests can impact active learning’s effectiveness in real-world contexts. We conceptualise this degradation by considering the type of non-response present in the data, demonstrating that biased non-response is particularly detrimental to model performance. We argue that biased non-response is likely in contexts where the labelling process, by nature, relies on user interactions. To mitigate the impact of biased non-response, we propose a cost-based correction to the sampling strategy–the Upper Confidence Bound of the Expected Utility (UCB-EU)–that can, plausibly, be applied to any active learning algorithm. Through experiments, we demonstrate that our method successfully reduces the harm from labelling non-response in many settings. However, we also characterise settings where the non-response bias in the annotations remains detrimental under UCB-EU for specific sampling methods and data generating processes. Finally, we evaluate our method on a real-world dataset from an e-commerce platform. We show that UCB-EU yields substantial performance improvements to conversion models that are trained on clicked impressions. Most generally, this research serves to both better conceptualise the interplay between types of non-response and model improvements via active learning, and to provide a practical, easy-to-implement correction that mitigates model degradation.},
  archive      = {J_DMKD},
  author       = {Robinson, Thomas S. and Tax, Niek and Mudd, Richard and Guy, Ido},
  doi          = {10.1007/s10618-024-01026-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2117-2140},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Active learning with biased non-response to label requests},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A practical approach to novel class discovery in tabular
data. <em>DMKD</em>, <em>38</em>(4), 2087–2116. (<a
href="https://doi.org/10.1007/s10618-024-01025-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of novel class discovery (NCD) consists in extracting knowledge from a labeled set of known classes to accurately partition an unlabeled set of novel classes. While NCD has recently received a lot of attention from the community, it is often solved on computer vision problems and under unrealistic conditions. In particular, the number of novel classes is usually assumed to be known in advance, and their labels are sometimes used to tune hyperparameters. Methods that rely on these assumptions are not applicable in real-world scenarios. In this work, we focus on solving NCD in tabular data when no prior knowledge of the novel classes is available. To this end, we propose to tune the hyperparameters of NCD methods by adapting the k-fold cross-validation process and hiding some of the known classes in each fold. Since we have found that methods with too many hyperparameters are likely to overfit these hidden classes, we define a simple deep NCD model. This method is composed of only the essential elements necessary for the NCD problem and shows robust performance under realistic conditions. Furthermore, we find that the latent space of this method can be used to reliably estimate the number of novel classes. Additionally, we adapt two unsupervised clustering algorithms (k-means and Spectral Clustering) to leverage the knowledge of the known classes. Extensive experiments are conducted on 7 tabular datasets and demonstrate the effectiveness of the proposed method and hyperparameter tuning process, and show that the NCD problem can be solved without relying on knowledge from the novel classes.},
  archive      = {J_DMKD},
  author       = {Colin, Troisemaine and Alexandre, Reiffers-Masson and Stéphane, Gosselin and Vincent, Lemaire and Sandrine, Vaton},
  doi          = {10.1007/s10618-024-01025-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2087-2116},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A practical approach to novel class discovery in tabular data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bias-aware ranking from pairwise comparisons. <em>DMKD</em>,
<em>38</em>(4), 2062–2086. (<a
href="https://doi.org/10.1007/s10618-024-01024-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human feedback is often used, either directly or indirectly, as input to algorithmic decision making. However, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. In this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. Specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. While the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. By detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. Our proposal is a novel method that extends the classic Bradley-Terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. Thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. Our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.},
  archive      = {J_DMKD},
  author       = {Ferrara, Antonio and Bonchi, Francesco and Fabbri, Francesco and Karimi, Fariba and Wagner, Claudia},
  doi          = {10.1007/s10618-024-01024-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2062-2086},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bias-aware ranking from pairwise comparisons},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intention enhanced mixed attentive model for session-based
recommendation. <em>DMKD</em>, <em>38</em>(4), 2032–2061. (<a
href="https://doi.org/10.1007/s10618-024-01023-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation aims to generate recommendations for the next item of users’ interest based on a given session. In this manuscript, we develop intention enhanced mixed attentive model (IMAM) to generate session-based recommendations using two important factors: temporal patterns and estimates of users’ intentions. Unlike existing methods which primarily leverage complicated gated recurrent units to model the temporal patterns, IMAM models the temporal patterns using a light-weight while effective position-sensitive attention mechanism. In IMAM, we also leverage the estimate of users’ prospective preferences to signify important items, and generate better recommendations. Our experimental results demonstrate that IMAM models significantly outperform the state-of-the-art methods in six benchmark datasets, with an improvement as much as 19.2%. In addition, our run-time performance comparison demonstrates that during testing, IMAM models are much more efficient than the best baseline method, with a significant average speedup of 47.7 folds.},
  archive      = {J_DMKD},
  author       = {Peng, Bo and Parthasarathy, Srinivasan and Ning, Xia},
  doi          = {10.1007/s10618-024-01023-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {2032-2061},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Intention enhanced mixed attentive model for session-based recommendation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Bake off redux: A review and experimental evaluation of
recent time series classification algorithms. <em>DMKD</em>,
<em>38</em>(4), 1958–2031. (<a
href="https://doi.org/10.1007/s10618-024-01022-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017, a research paper (Bagnall et al. Data Mining and Knowledge Discovery 31(3):606-660. 2017) compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a ‘bake off’, identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against the previous best-of-category using an expanded UCR archive. We extend the taxonomy to include three new categories to reflect recent developments. Alongside the originally proposed distance, interval, shapelet, dictionary and hybrid based algorithms, we compare newer convolution and feature based algorithms as well as deep learning approaches. We introduce 30 classification datasets either recently donated to the archive or reformatted to the TSC format, and use these to further evaluate the best performing algorithm from each category. Overall, we find that two recently proposed algorithms, MultiROCKET+Hydra (Dempster et al. 2022) and HIVE-COTEv2 (Middlehurst et al. Mach Learn 110:3211-3243. 2021), perform significantly better than other approaches on both the current and new TSC problems.},
  archive      = {J_DMKD},
  author       = {Middlehurst, Matthew and Schäfer, Patrick and Bagnall, Anthony},
  doi          = {10.1007/s10618-024-01022-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1958-2031},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bake off redux: A review and experimental evaluation of recent time series classification algorithms},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple hypergraph convolutional network social
recommendation using dual contrastive learning. <em>DMKD</em>,
<em>38</em>(4), 1929–1957. (<a
href="https://doi.org/10.1007/s10618-024-01021-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the strong representation capabilities of graph structures in social networks, social relationships are often used to improve recommendation quality. Most existing social recommendation models exploit pairwise relations to mine latent user preferences. However, since user interactions are relatively complex with possibly higher-order relationships, their performance in real-world applications is limited. Furthermore, user behavior data in many practical recommendation scenarios tend to be noisy and sparse, which may lead to suboptimal representation performance. To address this issue, we propose a dual objective contrastive learning multiple hypergraph convolution model for social recommendation (DCMHS). Specifically, our model first constructs hypergraphs with different social relationships. Then, we construct hypergraph encoders to obtain higher-order user representations through hypergraph convolution. Aiming to avoid aggregation loss caused by aggregating user embeddings under different views into one, we construct neighbor identification and semantic identification contrastive learning objectives to iteratively refine the user representation. In addition, we optimize the negative sampling process using the global embedding of items. The results of experiments conducted on real-world datasets demonstrate the effectiveness of the proposed DCMHS, and the ablation study validates the rationality of different components of the model.},
  archive      = {J_DMKD},
  author       = {Wang, Hongyu and Zhou, Wei and Wen, Junhao and Qiao, Shutong},
  doi          = {10.1007/s10618-024-01021-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1929-1957},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multiple hypergraph convolutional network social recommendation using dual contrastive learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards more sustainable and trustworthy reporting in
machine learning. <em>DMKD</em>, <em>38</em>(4), 1909–1928. (<a
href="https://doi.org/10.1007/s10618-024-01020-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With machine learning (ML) becoming a popular tool across all domains, practitioners are in dire need of comprehensive reporting on the state-of-the-art. Benchmarks and open databases provide helpful insights for many tasks, however suffer from several phenomena: Firstly, they overly focus on prediction quality, which is problematic considering the demand for more sustainability in ML. Depending on the use case at hand, interested users might also face tight resource constraints and thus should be allowed to interact with reporting frameworks, in order to prioritize certain reported characteristics. Furthermore, as some practitioners might not yet be well-skilled in ML, it is important to convey information on a more abstract, comprehensible level. Usability and extendability are key for moving with the state-of-the-art and in order to be trustworthy, frameworks should explicitly address reproducibility. In this work, we analyze established reporting systems under consideration of the aforementioned issues. Afterwards, we propose STREP, our novel framework that aims at overcoming these shortcomings and paves the way towards more sustainable and trustworthy reporting. We use STREP’s (publicly available) implementation to investigate various existing report databases. Our experimental results unveil the need for making reporting more resource-aware and demonstrate our framework’s capabilities of overcoming current reporting limitations. With our work, we want to initiate a paradigm shift in reporting and help with making ML advances more considerate of sustainability and trustworthiness.},
  archive      = {J_DMKD},
  author       = {Fischer, Raphael and Liebig, Thomas and Morik, Katharina},
  doi          = {10.1007/s10618-024-01020-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1909-1928},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Towards more sustainable and trustworthy reporting in machine learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lost in the forest: Encoding categorical variables and the
absent levels problem. <em>DMKD</em>, <em>38</em>(4), 1889–1908. (<a
href="https://doi.org/10.1007/s10618-024-01019-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Levels of a predictor variable that are absent when a classification tree is grown can not be subject to an explicit splitting rule. This is an issue if these absent levels are present in a new observation for prediction. To date, there remains no satisfactory solution for absent levels in random forest models. Unlike missing data, absent levels are fully observed and known. Ordinal encoding of predictors allows absent levels to be integrated and used for prediction. Using a case study on source attribution of Campylobacter species using whole genome sequencing (WGS) data as predictors, we examine how target-agnostic versus target-based encoding of predictor variables with absent levels affects the accuracy of random forest models. We show that a target-based encoding approach using class probabilities, with absent levels designated the highest rank, is systematically biased, and that this bias is resolved by encoding absent levels according to the a priori hypothesis of equal class probability. We present a novel method of ordinal encoding predictors via principal coordinates analysis (PCO) which capitalizes on the similarity between pairs of predictor levels. Absent levels are encoded according to their similarity to each of the other levels in the training data. We show that the PCO-encoding method performs at least as well as the target-based approach and is not biased.},
  archive      = {J_DMKD},
  author       = {Smith, Helen L. and Biggs, Patrick J. and French, Nigel P. and Smith, Adam N. H. and Marshall, Jonathan C.},
  doi          = {10.1007/s10618-024-01019-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1889-1908},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Lost in the forest: Encoding categorical variables and the absent levels problem},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time series clustering with random convolutional kernels.
<em>DMKD</em>, <em>38</em>(4), 1862–1888. (<a
href="https://doi.org/10.1007/s10618-024-01018-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data, spanning applications ranging from climatology to finance to healthcare, presents significant challenges in data mining due to its size and complexity. One open issue lies in time series clustering, which is crucial for processing large volumes of unlabeled time series data and unlocking valuable insights. Traditional and modern analysis methods, however, often struggle with these complexities. To address these limitations, we introduce R-Clustering, a novel method that utilizes convolutional architectures with randomly selected parameters. Through extensive evaluations, R-Clustering demonstrates superior performance over existing methods in terms of clustering accuracy, computational efficiency and scalability. Empirical results obtained using the UCR archive demonstrate the effectiveness of our approach across diverse time series datasets. The findings highlight the significance of R-Clustering in various domains and applications, contributing to the advancement of time series data mining.},
  archive      = {J_DMKD},
  author       = {Jorge, Marco-Blanco and Rubén, Cuevas},
  doi          = {10.1007/s10618-024-01018-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1862-1888},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Time series clustering with random convolutional kernels},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCCE: Monte carlo sampling of valid and realistic
counterfactual explanations for tabular data. <em>DMKD</em>,
<em>38</em>(4), 1830–1861. (<a
href="https://doi.org/10.1007/s10618-024-01017-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MCCE: $${{{\underline{\varvec{M}}}}}$$ onte $${{{\underline{\varvec{C}}}}}$$ arlo sampling of valid and realistic $${{{\underline{\varvec{C}}}}}$$ ounterfactual $${{{\underline{\varvec{E}}}}}$$ xplanations for tabular data, a novel counterfactual explanation method that generates on-manifold, actionable and valid counterfactuals by modeling the joint distribution of the mutable features given the immutable features and the decision. Unlike other on-manifold methods that tend to rely on variational autoencoders and have strict prediction model and data requirements, MCCE handles any type of prediction model and categorical features with more than two levels. MCCE first models the joint distribution of the features and the decision with an autoregressive generative model where the conditionals are estimated using decision trees. Then, it samples a large set of observations from this model, and finally, it removes the samples that do not obey certain criteria. We compare MCCE with a range of state-of-the-art on-manifold counterfactual methods using four well-known data sets and show that MCCE outperforms these methods on all common performance metrics and speed. In particular, including the decision in the modeling process improves the efficiency of the method substantially.},
  archive      = {J_DMKD},
  author       = {Redelmeier, Annabelle and Jullum, Martin and Aas, Kjersti and Løland, Anders},
  doi          = {10.1007/s10618-024-01017-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1830-1861},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MCCE: Monte carlo sampling of valid and realistic counterfactual explanations for tabular data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative study of methods for estimating model-agnostic
shapley value explanations. <em>DMKD</em>, <em>38</em>(4), 1782–1829.
(<a href="https://doi.org/10.1007/s10618-024-01016-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapley values originated in cooperative game theory but are extensively used today as a model-agnostic explanation framework to explain predictions made by complex machine learning models in the industry and academia. There are several algorithmic approaches for computing different versions of Shapley value explanations. Here, we consider Shapley values incorporating feature dependencies, referred to as conditional Shapley values, for predictive models fitted to tabular data. Estimating precise conditional Shapley values is difficult as they require the estimation of non-trivial conditional expectations. In this article, we develop new methods, extend earlier proposed approaches, and systematize the new refined and existing methods into different method classes for comparison and evaluation. The method classes use either Monte Carlo integration or regression to model the conditional expectations. We conduct extensive simulation studies to evaluate how precisely the different method classes estimate the conditional expectations, and thereby the conditional Shapley values, for different setups. We also apply the methods to several real-world data experiments and provide recommendations for when to use the different method classes and approaches. Roughly speaking, we recommend using parametric methods when we can specify the data distribution almost correctly, as they generally produce the most accurate Shapley value explanations. When the distribution is unknown, both generative methods and regression models with a similar form as the underlying predictive model are good and stable options. Regression-based methods are often slow to train but quickly produce the Shapley value explanations once trained. The vice versa is true for Monte Carlo-based methods, making the different methods appropriate in different practical situations.},
  archive      = {J_DMKD},
  author       = {Olsen, Lars Henry Berge and Glad, Ingrid Kristine and Jullum, Martin and Aas, Kjersti},
  doi          = {10.1007/s10618-024-01016-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1782-1829},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative study of methods for estimating model-agnostic shapley value explanations},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable linear dimensionality reduction based on
bias-variance analysis. <em>DMKD</em>, <em>38</em>(4), 1713–1781. (<a
href="https://doi.org/10.1007/s10618-024-01015-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the central issues of several machine learning applications on real data is the choice of the input features. Ideally, the designer should select a small number of the relevant, nonredundant features to preserve the complete information contained in the original dataset, with little collinearity among features. This procedure helps mitigate problems like overfitting and the curse of dimensionality, which arise when dealing with high-dimensional problems. On the other hand, it is not desirable to simply discard some features, since they may still contain information that can be exploited to improve results. Instead, dimensionality reduction techniques are designed to limit the number of features in a dataset by projecting them into a lower dimensional space, possibly considering all the original features. However, the projected features resulting from the application of dimensionality reduction techniques are usually difficult to interpret. In this paper, we seek to design a principled dimensionality reduction approach that maintains the interpretability of the resulting features. Specifically, we propose a bias-variance analysis for linear models and we leverage these theoretical results to design an algorithm, Linear Correlated Features Aggregation (LinCFA), which aggregates groups of continuous features with their average if their correlation is “sufficiently large”. In this way, all features are considered, the dimensionality is reduced and the interpretability is preserved. Finally, we provide numerical validations of the proposed algorithm both on synthetic datasets to confirm the theoretical results and on real datasets to show some promising applications.},
  archive      = {J_DMKD},
  author       = {Bonetti, Paolo and Metelli, Alberto Maria and Restelli, Marcello},
  doi          = {10.1007/s10618-024-01015-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1713-1781},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interpretable linear dimensionality reduction based on bias-variance analysis},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binary quantification and dataset shift: An experimental
investigation. <em>DMKD</em>, <em>38</em>(4), 1670–1712. (<a
href="https://doi.org/10.1007/s10618-024-01014-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification is the supervised learning task that consists of training predictors of the class prevalence values of sets of unlabelled data, and is of special interest when the labelled data on which the predictor has been trained and the unlabelled data are not IID, i.e., suffer from dataset shift. To date, quantification methods have mostly been tested only on a special case of dataset shift, i.e., prior probability shift; the relationship between quantification and other types of dataset shift remains, by and large, unexplored. In this work we carry out an experimental analysis of how current quantification algorithms behave under different types of dataset shift, in order to identify limitations of current approaches and hopefully pave the way for the development of more broadly applicable methods. We do this by proposing a fine-grained taxonomy of types of dataset shift, by establishing protocols for the generation of datasets affected by these types of shift, and by testing existing quantification methods on the datasets thus generated. One finding that results from this investigation is that many existing quantification methods that had been found robust to prior probability shift are not necessarily robust to other types of dataset shift. A second finding is that no existing quantification method seems to be robust enough to dealing with all the types of dataset shift we simulate in our experiments. The code needed to reproduce all our experiments is publicly available at https://github.com/pglez82/quant_datasetshift .},
  archive      = {J_DMKD},
  author       = {González, Pablo and Moreo, Alejandro and Sebastiani, Fabrizio},
  doi          = {10.1007/s10618-024-01014-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1670-1712},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Binary quantification and dataset shift: An experimental investigation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a bayesian network with multiple latent variables
for implicit relation representation. <em>DMKD</em>, <em>38</em>(4),
1634–1669. (<a
href="https://doi.org/10.1007/s10618-024-01012-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence applications could be more powerful and comprehensive by incorporating the ability of inference, which could be achieved by probabilistic inference over implicit relations. It is significant yet challenging to represent implicit relations among observed variables and latent ones like disease etiologies and user preferences. In this paper, we propose the BN with multiple latent variables (MLBN) as the framework for representing the dependence relations, where multiple latent variables are incorporated to describe multi-dimensional abstract concepts. However, the efficiency of MLBN learning and effectiveness of MLBN based applications are still nontrivial due to the presence of multiple latent variables. To this end, we first propose the constraint induced and Spark based algorithm for MLBN learning, as well as several optimization strategies. Moreover, we present the concept of variation degree and further design a subgraph based algorithm for incremental learning of MLBN. Experimental results suggest that our proposed MLBN model could represent the dependence relations correctly. Our proposed method outperforms some state-of-the-art competitors for personalized recommendation, and facilitates some typical approaches to achieve better performance.},
  archive      = {J_DMKD},
  author       = {Wu, Xinran and Yue, Kun and Duan, Liang and Fu, Xiaodong},
  doi          = {10.1007/s10618-024-01012-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1634-1669},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Learning a bayesian network with multiple latent variables for implicit relation representation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online concept evolution detection based on active learning.
<em>DMKD</em>, <em>38</em>(4), 1589–1633. (<a
href="https://doi.org/10.1007/s10618-024-01011-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept evolution detection is an important and difficult problem in streaming data mining. When the labeled samples in streaming data insufficient to reflect the training data distribution, it will often further restrict the detection performance. This paper proposed a concept evolution detection method based on active learning (CE_AL). Firstly, the initial classifiers are constructed by a small number of labeled samples. The sample areas are divided into the automatic labeling and the active labeling areas according to the relationship between the classifiers of different categories. Secondly, for online new coming samples, according to their different areas, two strategies based on the automatic learning-based model labeling and active learning-based expert labeling are adopted respectively, which can improve the online learning performance with only a small number of labeled samples. Besides, the strategy of “data enhance” combined with “model enhance” is adopted to accelerate the convergence of the evolution category detection model. The experimental results show that the proposed CE_AL method can enhance the detection performance of concept evolution and realize efficient learning in an unstable environment by labeling a small number of key samples.},
  archive      = {J_DMKD},
  author       = {Guo, Husheng and Li, Hai and Cong, Lu and Wang, Wenjian},
  doi          = {10.1007/s10618-024-01011-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1589-1633},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Online concept evolution detection based on active learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMA: Metadata supported multi-variate attention for onset
detection and prediction. <em>DMKD</em>, <em>38</em>(4), 1545–1588. (<a
href="https://doi.org/10.1007/s10618-024-01008-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been applied successfully in sequence understanding and translation problems, especially in univariate, unimodal contexts, where large number of supervision data are available. The effectiveness of deep learning in more complex (multi-modal, multi-variate) contexts, where supervision data is rare, however, is generally not satisfactory. In this paper, we focus on improving detection and prediction accuracy in precisely such contexts – in particular, we focus on the problem of predicting seizure onsets relying on multi-modal (EEG, ICP, ECG, and ABP) sensory data streams, some of which (such as EEG) are inherently multi-variate due to the placement of multiple sensors to capture spatial distribution of the relevant signals. In particular, we note that multi-variate time series often carry robust, spatio-temporally localized features that could help predict onset events. We further argue that such features can be used to support implementation of metadata supported multivariate attention (or MMA) mechanisms that help significantly improve the effectiveness of neural networks architectures. In this paper, we use the proposed MMA approach to develop a multi-modal LSTM-based neural network architecture to tackle seizure onset detection and prediction tasks relying on EEG, ICP, ECG, and ABP data streams. We experimentally evaluate the proposed architecture under different scenarios – the results illustrate the effectiveness of the proposed attention mechanism, especially compared against other metadata driven competitors.},
  archive      = {J_DMKD},
  author       = {Ravindranath, Manjusha and Candan, K. Selçuk and Sapino, Maria Luisa and Appavu, Brian},
  doi          = {10.1007/s10618-024-01008-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1545-1588},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MMA: Metadata supported multi-variate attention for onset detection and prediction},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural learning of simple staged trees. <em>DMKD</em>,
<em>38</em>(3), 1520–1544. (<a
href="https://doi.org/10.1007/s10618-024-01007-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks faithfully represent the symmetric conditional independences existing between the components of a random vector. Staged trees are an extension of Bayesian networks for categorical random vectors whose graph represents non-symmetric conditional independences via vertex coloring. However, since they are based on a tree representation of the sample space, the underlying graph becomes cluttered and difficult to visualize as the number of variables increases. Here, we introduce the first structural learning algorithms for the class of simple staged trees, entertaining a compact coalescence of the underlying tree from which non-symmetric independences can be easily read. We show that data-learned simple staged trees often outperform Bayesian networks in model fit and illustrate how the coalesced graph is used to identify non-symmetric conditional independences.},
  archive      = {J_DMKD},
  author       = {Leonelli, Manuele and Varando, Gherardo},
  doi          = {10.1007/s10618-024-01007-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1520-1544},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Structural learning of simple staged trees},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal representation learning for multivariate time
series using the instance-level and cluster-level supervised contrastive
learning. <em>DMKD</em>, <em>38</em>(3), 1493–1519. (<a
href="https://doi.org/10.1007/s10618-024-01006-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate time series classification (MTSC) task aims to predict a class label for a given time series. Recently, modern deep learning-based approaches have achieved promising performance over traditional methods for MTSC tasks. The success of these approaches relies on access to the massive amount of labeled data (i.e., annotating or assigning tags to each sample that shows its corresponding category). However, obtaining a massive amount of labeled data is usually very time-consuming and expensive in many real-world applications such as medicine, because it requires domain experts’ knowledge to annotate data. Insufficient labeled data prevents these models from learning discriminative features, resulting in poor margins that reduce generalization performance. To address this challenge, we propose a novel approach: supervised contrastive learning for time series classification (SupCon-TSC). This approach improves the classification performance by learning the discriminative low-dimensional representations of multivariate time series, and its end-to-end structure allows for interpretable outcomes. It is based on supervised contrastive (SupCon) loss to learn the inherent structure of multivariate time series. First, two separate augmentation families, including strong and weak augmentation methods, are utilized to generate augmented data for the source and target networks, respectively. Second, we propose the instance-level, and cluster-level SupCon learning approaches to capture contextual information to learn the discriminative and universal representation for multivariate time series datasets. In the instance-level SupCon learning approach, for each given anchor instance that comes from the source network, the low-variance output encodings from the target network are sampled as positive and negative instances based on their labels. However, the cluster-level approach is performed between each instance and cluster centers among batches, as opposed to the instance-level approach. The cluster-level SupCon loss attempts to maximize the similarities between each instance and cluster centers among batches. We tested this novel approach on two small cardiopulmonary exercise testing (CPET) datasets and the real-world UEA Multivariate time series archive. The results of the SupCon-TSC model on CPET datasets indicate its capability to learn more discriminative features than existing approaches in situations where the size of the dataset is small. Moreover, the results on the UEA archive show that training a classifier on top of the universal representation features learned by our proposed method outperforms the state-of-the-art approaches.},
  archive      = {J_DMKD},
  author       = {Moradinasab, Nazanin and Sharma, Suchetha and Bar-Yoseph, Ronen and Radom-Aizik, Shlomit and C. Bilchick, Kenneth and M. Cooper, Dan and Weltman, Arthur and Brown, Donald E.},
  doi          = {10.1007/s10618-024-01006-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1493-1519},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Universal representation learning for multivariate time series using the instance-level and cluster-level supervised contrastive learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MASS: Distance profile of a query over a time series.
<em>DMKD</em>, <em>38</em>(3), 1466–1492. (<a
href="https://doi.org/10.1007/s10618-024-01005-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a long time series, the distance profile of a query time series computes distances between the query and every possible subsequence of a long time series. MASS (Mueen’s Algorithm for Similarity Search) is an algorithm to efficiently compute distance profile under z-normalized Euclidean distance (Mueen et al. in The fastest similarity search algorithm for time series subsequences under Euclidean distance. http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html , 2017). MASS is recognized as a useful tool in many data mining works. However, complete documentation of the increasingly efficient versions of the algorithm does not exist. In this paper, we formalize the notion of a distance profile, describe four versions of the MASS algorithm, show several extensions of distance profiles under various operating conditions, describe how MASS improves performances of existing data mining algorithms, and finally, show utility of MASS in domains including seismology, robotics and power grids.},
  archive      = {J_DMKD},
  author       = {Zhong, Sheng and Mueen, Abdullah},
  doi          = {10.1007/s10618-024-01005-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1466-1492},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MASS: Distance profile of a query over a time series},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revealing the structural behaviour of brunelleschi’s dome
with machine learning techniques. <em>DMKD</em>, <em>38</em>(3),
1440–1465. (<a
href="https://doi.org/10.1007/s10618-024-01004-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Brunelleschi’s Dome is one of the most iconic symbols of the Renaissance and is among the largest masonry domes ever constructed. Since the late 17th century, first masonry cracks appeared on the Dome, giving the start to a monitoring activity. In modern times, since 1988 a monitoring system comprised of 166 electronic sensors, including deformometers and thermometers, has been in operation, providing a valuable source of real-time data on the monument’s health status. With the deformometers taking measurements at least four times per day, a vast amount of data is now available to explore the potential of the latest Artificial Intelligence and Machine Learning techniques in the field of historical-architectural heritage conservation. The objective of this contribution is twofold. Firstly, for the first time ever, we aim to unveil the overall structural behaviour of the Dome as a whole, as well as that of its specific sections (known as webs). We achieve this by evaluating the effectiveness of certain dimensionality reduction techniques on the extensive daily detections generated by the monitoring system, while also accounting for fluctuations in temperature over time. Secondly, we estimate a number of recurrent and convolutional neural network models to verify their capability for medium- and long-term prediction of the structural evolution of the Dome. We believe this contribution is an important step forward in the protection and preservation of historical buildings, showing the utility of machine learning in a context in which these are still little used.},
  archive      = {J_DMKD},
  author       = {Masini, Stefano and Bacci, Silvia and Cipollini, Fabrizio and Bertaccini, Bruno},
  doi          = {10.1007/s10618-024-01004-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1440-1465},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Revealing the structural behaviour of brunelleschi’s dome with machine learning techniques},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Central node identification via weighted kernel density
estimation. <em>DMKD</em>, <em>38</em>(3), 1417–1439. (<a
href="https://doi.org/10.1007/s10618-024-01003-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of central nodes in a network is a fundamental task in network science and graph data analysis. During the past decades, numerous centrality measures have been presented to characterize what is a central node. However, few studies address this issue from a statistical inference perspective. In this paper, we formulate the central node identification issue as a weighted kernel density estimation problem on graphs. Such a formulation provides a generic framework for recognizing central nodes. On one hand, some existing centrality evaluation metrics can be unified under this framework through the manipulation of kernel functions. On the other hand, more effective methods for node centrality assessment can be developed based on proper weighting coefficient specification. Experimental results on 20 simulated networks and 53 real networks show that our method outperforms both six prior state-of-the-art centrality measures and two recently proposed centrality evaluation methods. To the best of our knowledge, this is the first piece of work that addresses the central node identification issue via weighted kernel density estimation.},
  archive      = {J_DMKD},
  author       = {Liu, Yan and Feng, Xue and Lou, Jun and Hu, Lianyu and He, Zengyou},
  doi          = {10.1007/s10618-024-01003-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1417-1439},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Central node identification via weighted kernel density estimation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Better trees: An empirical study on hyperparameter tuning of
classification decision tree induction algorithms. <em>DMKD</em>,
<em>38</em>(3), 1364–1416. (<a
href="https://doi.org/10.1007/s10618-024-01002-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms often contain many hyperparameters whose values affect the predictive performance of the induced models in intricate ways. Due to the high number of possibilities for these hyperparameter configurations and their complex interactions, it is common to use optimization techniques to find settings that lead to high predictive performance. However, insights into efficiently exploring this vast space of configurations and dealing with the trade-off between predictive and runtime performance remain challenging. Furthermore, there are cases where the default hyperparameters fit the suitable configuration. Additionally, for many reasons, including model validation and attendance to new legislation, there is an increasing interest in interpretable models, such as those created by the decision tree (DT) induction algorithms. This paper provides a comprehensive approach for investigating the effects of hyperparameter tuning for the two DT induction algorithms most often used, CART and C4.5. DT induction algorithms present high predictive performance and interpretable classification models, though many hyperparameters need to be adjusted. Experiments were carried out with different tuning strategies to induce models and to evaluate hyperparameters’ relevance using 94 classification datasets from OpenML. The experimental results point out that different hyperparameter profiles for the tuning of each algorithm provide statistically significant improvements in most of the datasets for CART, but only in one-third for C4.5. Although different algorithms may present different tuning scenarios, the tuning techniques generally required few evaluations to find accurate solutions. Furthermore, the best technique for all the algorithms was the Irace. Finally, we found out that tuning a specific small subset of hyperparameters is a good alternative for achieving optimal predictive performance.},
  archive      = {J_DMKD},
  author       = {Gomes Mantovani, Rafael and Horváth, Tomáš and Rossi, André L. D. and Cerri, Ricardo and Barbon Junior, Sylvio and Vanschoren, Joaquin and Carvalho, André C. P. L. F. de},
  doi          = {10.1007/s10618-024-01002-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1364-1416},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Better trees: An empirical study on hyperparameter tuning of classification decision tree induction algorithms},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive bernstein change detector for high-dimensional data
streams. <em>DMKD</em>, <em>38</em>(3), 1334–1363. (<a
href="https://doi.org/10.1007/s10618-023-00999-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection is of fundamental importance when analyzing data streams. Detecting changes both quickly and accurately enables monitoring and prediction systems to react, e.g., by issuing an alarm or by updating a learning algorithm. However, detecting changes is challenging when observations are high-dimensional. In high-dimensional data, change detectors should not only be able to identify when changes happen, but also in which subspace they occur. Ideally, one should also quantify how severe they are. Our approach, ABCD, has these properties. ABCD learns an encoder-decoder model and monitors its accuracy over a window of adaptive size. ABCD derives a change score based on Bernstein’s inequality to detect deviations in terms of accuracy, which indicate changes. Our experiments demonstrate that ABCD outperforms its best competitor by up to 20% in F1-score on average. It can also accurately estimate changes’ subspace, together with a severity measure that correlates with the ground truth.},
  archive      = {J_DMKD},
  author       = {Heyden, Marco and Fouché, Edouard and Arzamasov, Vadim and Fenn, Tanja and Kalinke, Florian and Böhm, Klemens},
  doi          = {10.1007/s10618-023-00999-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1334-1363},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Adaptive bernstein change detector for high-dimensional data streams},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing structural information with knowledge enhanced text
representation for knowledge graph completion. <em>DMKD</em>,
<em>38</em>(3), 1316–1333. (<a
href="https://doi.org/10.1007/s10618-023-00998-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although knowledge graphs store a large number of facts in the form of triplets, they are still limited by incompleteness. Hence, Knowledge Graph Completion (KGC), defined as inferring missing entities or relations based on observed facts, has long been a fundamental issue for various knowledge driven downstream applications. Prevailing KG embedding methods for KGC like TransE rely solely on mining structural information of existing facts, thus failing to handle generalization issue as they are inapplicable to unseen entities. Recently, a series of researches employ pre-trained encoders to learn textual representation for triples i.e., textual-encoding methods. While exhibiting great generalization for unseen entities, they are still inferior compared with above KG embedding based ones. In this paper, we devise a novel textual-encoding learning framework for KGC. To enrich textual prior knowledge for more informative prediction, it features three hierarchical maskings which can utilize far contexts of input text so that textual prior knowledge can be elicited. Besides, to solve predictive ambiguity caused by improper relational modeling, a relational-aware structure learning scheme is applied based on textual embeddings. Extensive experimental results on several popular datasets suggest the effectiveness of our approach even compared with recent state-of-the-arts in this task.},
  archive      = {J_DMKD},
  author       = {Tang, Kang and Li, Shasha and Tang, Jintao and Li, Dong and Wang, Pancheng and Wang, Ting},
  doi          = {10.1007/s10618-023-00998-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1316-1333},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fusing structural information with knowledge enhanced text representation for knowledge graph completion},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving hyper-parameter self-tuning for data streams by
adapting an evolutionary approach. <em>DMKD</em>, <em>38</em>(3),
1289–1315. (<a
href="https://doi.org/10.1007/s10618-023-00997-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-parameter tuning of machine learning models has become a crucial task in achieving optimal results in terms of performance. Several researchers have explored the optimisation task during the last decades to reach a state-of-the-art method. However, most of them focus on batch or offline learning, where data distributions do not change arbitrarily over time. On the other hand, dealing with data streams and online learning is a challenging problem. In fact, the higher the technology goes, the greater the importance of sophisticated techniques to process these data streams. Thus, improving hyper-parameter self-tuning during online learning of these machine learning models is crucial. To this end, in this paper, we present MESSPT, an evolutionary algorithm for self-hyper-parameter tuning for data streams. We apply Differential Evolution to dynamically-sized samples, requiring a single pass-over of data to train and evaluate models and choose the best configurations. We take care of the number of configurations to be evaluated, which necessarily has to be reduced, thus making this evolutionary approach a micro-evolutionary one. Furthermore, we control how our evolutionary algorithm deals with concept drift. Experiments on different learning tasks and over well-known datasets show that our proposed MESSPT outperforms the state-of-the-art on hyper-parameter tuning for data streams.},
  archive      = {J_DMKD},
  author       = {Moya, Antonio R. and Veloso, Bruno and Gama, João and Ventura, Sebastián},
  doi          = {10.1007/s10618-023-00997-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1289-1315},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving hyper-parameter self-tuning for data streams by adapting an evolutionary approach},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CompTrails: Comparing hypotheses across behavioral networks.
<em>DMKD</em>, <em>38</em>(3), 1258–1288. (<a
href="https://doi.org/10.1007/s10618-023-00996-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term Behavioral Networks describes networks that contain relational information on human behavior. This ranges from social networks that contain friendships or cooperations between individuals, to navigational networks that contain geographical or web navigation, and many more. Understanding the forces driving behavior within these networks can be beneficial to improving the underlying network, for example, by generating new hyperlinks on websites, or by proposing new connections and friends on social networks. Previous approaches considered different hypotheses on a single network and evaluated which hypothesis fits best. These hypotheses can represent human intuition and expert opinions or be based on previous insights. In this work, we extend these approaches to enable the comparison of a single hypothesis between multiple networks. We unveil several issues of naive approaches that potentially impact comparisons and lead to undesired results. Based on these findings, we propose a framework with five flexible components that allow addressing specific analysis goals tailored to the application scenario. We show the benefits and limits of our approach by applying it to synthetic data and several real-world datasets, including web navigation, bibliometric navigation, and geographic navigation. Our work supports practitioners and researchers with the aim of understanding similarities and differences in human behavior between environments.},
  archive      = {J_DMKD},
  author       = {Koopmann, Tobias and Becker, Martin and Lemmerich, Florian and Hotho, Andreas},
  doi          = {10.1007/s10618-023-00996-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1258-1288},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {CompTrails: Comparing hypotheses across behavioral networks},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random walk with restart on hypergraphs: Fast computation
and an application to anomaly detection. <em>DMKD</em>, <em>38</em>(3),
1222–1257. (<a
href="https://doi.org/10.1007/s10618-023-00995-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random walk with restart (RWR) is a widely-used measure of node similarity in graphs, and it has proved useful for ranking, community detection, link prediction, anomaly detection, etc. Since RWR is typically required to be computed separately for a larger number of query nodes or even for all nodes, fast computation of it is indispensable. However, for hypergraphs, the fast computation of RWR has been unexplored, despite its great potential. In this paper, we propose ARCHER, a fast computation framework for RWR on hypergraphs. Specifically, we first formally define RWR on hypergraphs, and then we propose two computation methods that compose ARCHER. Since the two methods are complementary (i.e., offering relative advantages on different hypergraphs), we also develop a method for automatic selection between them, which takes a very short time compared to the total running time. Through our extensive experiments on 18 real-world hypergraphs, we demonstrate (a) the speed and space efficiency of ARCHER, (b) the complementary nature of the two computation methods composing ARCHER, (c) the accuracy of its automatic selection method, and (d) its successful application to anomaly detection on hypergraphs.},
  archive      = {J_DMKD},
  author       = {Chun, Jaewan and Lee, Geon and Shin, Kijung and Jung, Jinhong},
  doi          = {10.1007/s10618-023-00995-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1222-1257},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Random walk with restart on hypergraphs: Fast computation and an application to anomaly detection},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Session-based recommendation by exploiting substitutable and
complementary relationships from multi-behavior data. <em>DMKD</em>,
<em>38</em>(3), 1193–1221. (<a
href="https://doi.org/10.1007/s10618-023-00994-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SR) aims to dynamically recommend items to a user based on a sequence of the most recent user-item interactions. Most existing studies on SR adopt advanced deep learning methods. However, the majority only consider a special behavior type (e.g., click), while those few considering multi-typed behaviors ignore to take full advantage of the relationships between products (items). In this case, the paper proposes a novel approach, called Substitutable and Complementary Relationships from Multi-behavior Data (denoted as SCRM) to better explore the relationships between products for effective recommendation. Specifically, we firstly construct substitutable and complementary graphs based on a user’s sequential behaviors in every session by jointly considering ‘click’ and ‘purchase’ behaviors. We then design a denoising network to remove false relationships, and further consider constraints on the two relationships via a particularly designed loss function. Extensive experiments on two e-commerce datasets demonstrate the superiority of our model over state-of-the-art methods, and the effectiveness of every component in SCRM.},
  archive      = {J_DMKD},
  author       = {Wu, Huizi and Geng, Cong and Fang, Hui},
  doi          = {10.1007/s10618-023-00994-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1193-1221},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Session-based recommendation by exploiting substitutable and complementary relationships from multi-behavior data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When graph convolution meets double attention: Online
privacy disclosure detection with multi-label text classification.
<em>DMKD</em>, <em>38</em>(3), 1171–1192. (<a
href="https://doi.org/10.1007/s10618-023-00992-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Web 2.0 platforms such as online social media, people’s private information, such as their location, occupation and even family information, is often inadvertently disclosed through online discussions. Therefore, it is important to detect such unwanted privacy disclosures to help alert people affected and the online platform. In this paper, privacy disclosure detection is modeled as a multi-label text classification (MLTC) problem, and a new privacy disclosure detection model is proposed to construct an MLTC classifier for detecting online privacy disclosures. This classifier takes an online post as the input and outputs multiple labels, each reflecting a possible privacy disclosure. The proposed presentation method combines three different sources of information, the input text itself, the label-to-text correlation and the label-to-label correlation. A double-attention mechanism is used to combine the first two sources of information, and a graph convolutional network is employed to extract the third source of information that is then used to help fuse features extracted from the first two sources of information. Our extensive experimental results, obtained on a public dataset of privacy-disclosing posts on Twitter, demonstrated that our proposed privacy disclosure detection method significantly and consistently outperformed other state-of-the-art methods in terms of all key performance indicators.},
  archive      = {J_DMKD},
  author       = {Liang, Zhanbo and Guo, Jie and Qiu, Weidong and Huang, Zheng and Li, Shujun},
  doi          = {10.1007/s10618-023-00992-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1171-1192},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {When graph convolution meets double attention: Online privacy disclosure detection with multi-label text classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Effective signal reconstruction from
multiple ranked lists via convex optimization. <em>DMKD</em>,
<em>38</em>(3), 1170. (<a
href="https://doi.org/10.1007/s10618-024-01009-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Schimek, Michael G. and Vitale, Luca and Pfeifer, Bastian and La Rocca, Michele},
  doi          = {10.1007/s10618-024-01009-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1170},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: Effective signal reconstruction from multiple ranked lists via convex optimization},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Effective signal reconstruction from multiple ranked lists
via convex optimization. <em>DMKD</em>, <em>38</em>(3), 1125–1169. (<a
href="https://doi.org/10.1007/s10618-023-00991-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ranking of objects is widely used to rate their relative quality or relevance across multiple assessments. Beyond classical rank aggregation, it is of interest to estimate the usually unobservable latent signals that inform a consensus ranking. Under the only assumption of independent assessments, which can be incomplete, we introduce indirect inference via convex optimization in combination with computationally efficient Poisson Bootstrap. Two different objective functions are suggested, one linear and the other quadratic. The mathematical formulation of the signal estimation problem is based on pairwise comparisons of all objects with respect to their rank positions. Sets of constraints represent the order relations. The transitivity property of rank scales allows us to reduce substantially the number of constraints associated with the full set of object comparisons. The key idea is to globally reduce the errors induced by the rankers until optimal latent signals can be obtained. Its main advantage is low computational costs, even when handling $$n &lt; &lt; p$$ data problems. Exploratory tools can be developed based on the bootstrap signal estimates and standard errors. Simulation evidence, a comparison with the state-of-the-art rank centrality method, and two applications, one in higher education evaluation and the other in molecular cancer research, are presented.},
  archive      = {J_DMKD},
  author       = {Schimek, Michael G. and Vitale, Luca and Pfeifer, Bastian and La Rocca, Michele},
  doi          = {10.1007/s10618-023-00991-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1125-1169},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Effective signal reconstruction from multiple ranked lists via convex optimization},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OEC: An online ensemble classifier for mining data streams
with noisy labels. <em>DMKD</em>, <em>38</em>(3), 1101–1124. (<a
href="https://doi.org/10.1007/s10618-023-00990-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distilling actionable patterns from large-scale streaming data in the presence of concept drift is a challenging problem, especially when data is polluted with noisy labels. To date, various data stream mining algorithms have been proposed and extensively used in many real-world applications. Considering the functional complementation of classical online learning algorithms and with the goal of combining their advantages, we propose an Online Ensemble Classification (OEC) algorithm to integrate the predictions obtained by different base online classification algorithms. The proposed OEC method works by learning weights of different base classifiers dynamically through the classical Normalized Exponentiated Gradient (NEG) algorithm framework. As a result, the proposed OEC inherits the adaptability and flexibility of concept drift-tracking online classifiers, while maintaining the robustness of noise-resistant online classifiers. Theoretically, we show OEC algorithm is a low regret algorithm which makes it a good candidate to learn from noisy streaming data. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed OEC method.},
  archive      = {J_DMKD},
  author       = {Jian, Ling and Shao, Kai and Liu, Ying and Li, Jundong and Liang, Xijun},
  doi          = {10.1007/s10618-023-00990-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1101-1124},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {OEC: An online ensemble classifier for mining data streams with noisy labels},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting consumer choice from raw eye-movement data using
the RETINA deep learning architecture. <em>DMKD</em>, <em>38</em>(3),
1069–1100. (<a
href="https://doi.org/10.1007/s10618-023-00989-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the use of a deep learning architecture, called RETINA, to predict multi-alternative, multi-attribute consumer choice from eye movement data. RETINA directly uses the complete time series of raw eye-tracking data from both eyes as input to state-of-the art Transformer and Metric Learning Deep Learning methods. Using the raw data input eliminates the information loss that may result from first calculating fixations, deriving metrics from the fixations data and analysing those metrics, as has been often done in eye movement research, and allows us to apply Deep Learning to eye tracking data sets of the size commonly encountered in academic and applied research. Using a data set with 112 respondents who made choices among four laptops, we show that the proposed architecture outperforms other state-of-the-art machine learning methods (standard BERT, LSTM, AutoML, logistic regression) calibrated on raw data or fixation data. The analysis of partial time and partial data segments reveals the ability of RETINA to predict choice outcomes well before participants reach a decision. Specifically, we find that using a mere 5 s of data, the RETINA architecture achieves a predictive validation accuracy of over 0.7. We provide an assessment of which features of the eye movement data contribute to RETINA’s prediction accuracy. We make recommendations on how the proposed deep learning architecture can be used as a basis for future academic research, in particular its application to eye movements collected from front-facing video cameras.},
  archive      = {J_DMKD},
  author       = {Unger, Moshe and Wedel, Michel and Tuzhilin, Alexander},
  doi          = {10.1007/s10618-023-00989-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1069-1100},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Predicting consumer choice from raw eye-movement data using the RETINA deep learning architecture},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating the metric maze: A taxonomy of evaluation metrics
for anomaly detection in time series. <em>DMKD</em>, <em>38</em>(3),
1027–1068. (<a
href="https://doi.org/10.1007/s10618-023-00988-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of time series anomaly detection is constantly advancing, with several methods available, making it a challenge to determine the most appropriate method for a specific domain. The evaluation of these methods is facilitated by the use of metrics, which vary widely in their properties. Despite the existence of new evaluation metrics, there is limited agreement on which metrics are best suited for specific scenarios and domains, and the most commonly used metrics have faced criticism in the literature. This paper provides a comprehensive overview of the metrics used for the evaluation of time series anomaly detection methods, and also defines a taxonomy of these based on how they are calculated. By defining a set of properties for evaluation metrics and a set of specific case studies and experiments, twenty metrics are analyzed and discussed in detail, highlighting the unique suitability of each for specific tasks. Through extensive experimentation and analysis, this paper argues that the choice of evaluation metric must be made with care, taking into account the specific requirements of the task at hand.},
  archive      = {J_DMKD},
  author       = {Sørbø, Sondre and Ruocco, Massimiliano},
  doi          = {10.1007/s10618-023-00988-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1027-1068},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Navigating the metric maze: A taxonomy of evaluation metrics for anomaly detection in time series},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware decoupled imputation network for
multivariate time series. <em>DMKD</em>, <em>38</em>(3), 1006–1026. (<a
href="https://doi.org/10.1007/s10618-023-00987-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling incomplete multivariate time series is an important and fundamental concern for a variety of domains. Existing time-series imputation approaches rely on basic assumptions regarding relationship information between sensors, posing significant challenges since inter-sensor interactions in the real world are often complex and unknown beforehand. Specifically, there is a lack of in-depth investigation into (1) the coexistence of relationships between sensors and (2) the incorporation of reciprocal impact between sensor properties and inter-sensor relationships for the time-series imputation problem. To fill this gap, we present the Structure-aware Decoupled imputation network (SaD), which is designed to model sensor characteristics and relationships between sensors in distinct latent spaces. Our approach is equipped with a two-step knowledge integration scheme that incorporates the influence between the sensor attribute information as well as sensor relationship information. The experimental results indicate that when compared to state-of-the-art models for time-series imputation tasks, our proposed method can reduce error by around 15%.},
  archive      = {J_DMKD},
  author       = {Ahmed, Nourhan and Schmidt-Thieme, Lars},
  doi          = {10.1007/s10618-023-00987-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1006-1026},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Structure-aware decoupled imputation network for multivariate time series},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection in sleep: Detecting mouth breathing in
children. <em>DMKD</em>, <em>38</em>(3), 976–1005. (<a
href="https://doi.org/10.1007/s10618-023-00985-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying mouth breathing during sleep in a reliable, non-invasive way is challenging and currently not included in sleep studies. However, it has a high clinical relevance in pediatrics, as it can negatively impact the physical and mental health of children. Since mouth breathing is an anomalous condition in the general population with only 2% prevalence in our data set, we are facing an anomaly detection problem. This type of human medical data is commonly approached with deep learning methods. However, applying multiple supervised and unsupervised machine learning methods to this anomaly detection problem showed that classic machine learning methods should also be taken into account. This paper compared deep learning and classic machine learning methods on respiratory data during sleep using a leave-one-out cross validation. This way we observed the uncertainty of the models and their performance across participants with varying signal quality and prevalence of mouth breathing. The main contribution is identifying the model with the highest clinical relevance to facilitate the diagnosis of chronic mouth breathing, which may allow more affected children to receive appropriate treatment.},
  archive      = {J_DMKD},
  author       = {Biedebach, Luka and Óskarsdóttir, María and Arnardóttir, Erna Sif and Sigurdardóttir, Sigridur and Clausen, Michael Valur and Sigurdardóttir, Sigurveig Þ. and Serwatko, Marta and Islind, Anna Sigridur},
  doi          = {10.1007/s10618-023-00985-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {976-1005},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Anomaly detection in sleep: Detecting mouth breathing in children},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated singular value decomposition for high-dimensional
data. <em>DMKD</em>, <em>38</em>(3), 938–975. (<a
href="https://doi.org/10.1007/s10618-023-00983-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is emerging as a privacy-aware alternative to classical cloud-based machine learning. In FL, the sensitive data remains in data silos and only aggregated parameters are exchanged. Hospitals and research institutions which are not willing to share their data can join a federated study without breaching confidentiality. In addition to the extreme sensitivity of biomedical data, the high dimensionality poses a challenge in the context of federated genome-wide association studies (GWAS). In this article, we present a federated singular value decomposition algorithm, suitable for the privacy-related and computational requirements of GWAS. Notably, the algorithm has a transmission cost independent of the number of samples and is only weakly dependent on the number of features, because the singular vectors corresponding to the samples are never exchanged and the vectors associated with the features are only transmitted to an aggregator for a fixed number of iterations. Although motivated by GWAS, the algorithm is generically applicable for both horizontally and vertically partitioned data.},
  archive      = {J_DMKD},
  author       = {Hartebrodt, Anne and Röttger, Richard and Blumenthal, David B.},
  doi          = {10.1007/s10618-023-00983-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {938-975},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Federated singular value decomposition for high-dimensional data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traffic forecasting on new roads using spatial contrastive
pre-training (SCPT). <em>DMKD</em>, <em>38</em>(3), 913–937. (<a
href="https://doi.org/10.1007/s10618-023-00982-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal split to evaluate the models’ capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during inference time. The SCPT framework also incorporates a new layer, named the spatially gated addition layer, to effectively combine the latent features from the output of the spatial encoder to existing backbones. Additionally, since there is limited data on the unseen roads, we argue that it is better to decouple traffic signals to trivial-to-capture periodic signals and difficult-to-capture Markovian signals, and for the spatial encoder to only learn the Markovian signals. Finally, we empirically evaluated SCPT using the ST split setup on four real-world datasets. The results showed that adding SCPT to a backbone consistently improves forecasting performance on unseen roads. More importantly, the improvements are greater when forecasting further into the future. The codes are available on GitHub: https://github.com/cruiseresearchgroup/forecasting-on-new-roads .},
  archive      = {J_DMKD},
  author       = {Prabowo, Arian and Xue, Hao and Shao, Wei and Koniusz, Piotr and Salim, Flora D.},
  doi          = {10.1007/s10618-023-00982-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {913-937},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Traffic forecasting on new roads using spatial contrastive pre-training (SCPT)},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of highly accurate smart contract code
vulnerability detection framework. <em>DMKD</em>, <em>38</em>(3),
888–912. (<a href="https://doi.org/10.1007/s10618-023-00981-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are self-executing programs stored and executed on a blockchain platform. However, previous studies demonstrated that developing secure smart contracts is not easy. Unfortunately, the use of insecure smart contracts results in a significant financial loss for service providers or customers. Therefore, identifying security vulnerabilities in smart contracts would be essential in blockchain platforms using smart contracts. In this paper, we present SmartConDetect as a tool for detecting security vulnerabilities in Solidity smart contracts. SmartConDetect is a static analysis tool that extracts code fragments from Solidity smart contracts and uses a pre-trained BERT model to find susceptible code patterns. To demonstrate the performance of SmartConDetect, we use two public datasets, and our dataset (SmartConDataset) collected from the real-world Ethereum blockchain network. Our experimental results show that SmartConDetect significantly outperforms all state-of-the-art methods, achieving 90.9% F1-score when using our own dataset. Specifically, SmartConDetect is about 2 times faster than SmartCheck in detection. Furthermore, we conduct a real-world case study to analyze the distribution of detected vulnerabilities.},
  archive      = {J_DMKD},
  author       = {Jeon, Sowon and Lee, Gilhee and Kim, Hyoungshick and Woo, Simon S.},
  doi          = {10.1007/s10618-023-00981-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {888-912},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Design and evaluation of highly accurate smart contract code vulnerability detection framework},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cluster analysis via topological manifold
learning. <em>DMKD</em>, <em>38</em>(3), 840–887. (<a
href="https://doi.org/10.1007/s10618-023-00980-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss topological aspects of cluster analysis and show that inferring the topological structure of a dataset before clustering it can considerably enhance cluster detection: we show that clustering embedding vectors representing the inherent structure of a dataset instead of the observed feature vectors themselves is highly beneficial. To demonstrate, we combine manifold learning method UMAP for inferring the topological structure with density-based clustering method DBSCAN. Synthetic and real data results show that this both simplifies and improves clustering in a diverse set of low- and high-dimensional problems including clusters of varying density and/or entangled shapes. Our approach simplifies clustering because topological pre-processing consistently reduces parameter sensitivity of DBSCAN. Clustering the resulting embeddings with DBSCAN can then even outperform complex methods such as SPECTACL and ClusterGAN. Finally, our investigation suggests that the crucial issue in clustering does not appear to be the nominal dimension of the data or how many irrelevant features it contains, but rather how separable the clusters are in the ambient observation space they are embedded in, which is usually the (high-dimensional) Euclidean space defined by the features of the data. The approach is successful because it performs the cluster analysis after projecting the data into a more suitable space that is optimized for separability, in some sense.},
  archive      = {J_DMKD},
  author       = {Herrmann, Moritz and Kazempour, Daniyal and Scheipl, Fabian and Kröger, Peer},
  doi          = {10.1007/s10618-023-00980-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {840-887},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Enhancing cluster analysis via topological manifold learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Somtimes: Self organizing maps for time series clustering
and its application to serious illness conversations. <em>DMKD</em>,
<em>38</em>(3), 813–839. (<a
href="https://doi.org/10.1007/s10618-023-00979-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is demand for scalable algorithms capable of clustering and analyzing large time series data. The Kohonen self-organizing map (SOM) is an unsupervised artificial neural network for clustering, visualizing, and reducing the dimensionality of complex data. Like all clustering methods, it requires a measure of similarity between input data (in this work time series). Dynamic time warping (DTW) is one such measure, and a top performer that accommodates distortions when aligning time series. Despite its popularity in clustering, DTW is limited in practice because the runtime complexity is quadratic with the length of the time series. To address this, we present a new a self-organizing map for clustering TIME Series, called SOMTimeS, which uses DTW as the distance measure. The method has similar accuracy compared with other DTW-based clustering algorithms, yet scales better and runs faster. The computational performance stems from the pruning of unnecessary DTW computations during the SOM’s training phase. For comparison, we implement a similar pruning strategy for K-means, and call the latter K-TimeS. SOMTimeS and K-TimeS pruned 43% and 50% of the total DTW computations, respectively. Pruning effectiveness, accuracy, execution time and scalability are evaluated using 112 benchmark time series datasets from the UC Riverside classification archive, and show that for similar accuracy, a 1.8 $$\times$$ speed-up on average for SOMTimeS and K-TimeS, respectively with that rates vary between 1 $$\times$$ and 18 $$\times$$ depending on the dataset. We also apply SOMTimeS to a healthcare study of patient-clinician serious illness conversations to demonstrate the algorithm’s utility with complex, temporally sequenced natural language.},
  archive      = {J_DMKD},
  author       = {Javed, Ali and Rizzo, Donna M. and Lee, Byung Suk and Gramling, Robert},
  doi          = {10.1007/s10618-023-00979-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {813-839},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Somtimes: Self organizing maps for time series clustering and its application to serious illness conversations},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: AA-forecast: Anomaly-aware forecast for
extreme events. <em>DMKD</em>, <em>38</em>(2), 812. (<a
href="https://doi.org/10.1007/s10618-023-00968-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Farhangi, Ashkan and Bian, Jiang and Huang, Arthur and Xiong, Haoyi and Wang, Jun and Guo, Zhishan},
  doi          = {10.1007/s10618-023-00968-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {812},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: AA-forecast: anomaly-aware forecast for extreme events},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast, accurate and explainable time series classification
through randomization. <em>DMKD</em>, <em>38</em>(2), 748–811. (<a
href="https://doi.org/10.1007/s10618-023-00978-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) aims to predict the class label of a given time series, which is critical to a rich set of application areas such as economics and medicine. State-of-the-art TSC methods have mostly focused on classification accuracy, without considering classification speed. However, efficiency is important for big data analysis. Datasets with a large training size or long series challenge the use of the current highly accurate methods, because they are usually computationally expensive. Similarly, classification explainability, which is an important property required by modern big data applications such as appliance modeling and legislation such as the European General Data Protection Regulation, has received little attention. To address these gaps, we propose a novel TSC method – the Randomized-Supervised Time Series Forest (r-STSF). r-STSF is extremely fast and achieves state-of-the-art classification accuracy. It is an efficient interval-based approach that classifies time series according to aggregate values of the discriminatory sub-series (intervals). To achieve state-of-the-art accuracy, r-STSF builds an ensemble of randomized trees using the discriminatory sub-series. It uses four time series representations, nine aggregation functions and a supervised binary-inspired search combined with a feature ranking metric to identify highly discriminatory sub-series. The discriminatory sub-series enable explainable classifications. Experiments on extensive datasets show that r-STSF achieves state-of-the-art accuracy while being orders of magnitude faster than most existing TSC methods and enabling for explanations on the classifier decision.},
  archive      = {J_DMKD},
  author       = {Cabello, Nestor and Naghizade, Elham and Qi, Jianzhong and Kulik, Lars},
  doi          = {10.1007/s10618-023-00978-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {748-811},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fast, accurate and explainable time series classification through randomization},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representing ensembles of networks for fuzzy cluster
analysis: A case study. <em>DMKD</em>, <em>38</em>(2), 725–747. (<a
href="https://doi.org/10.1007/s10618-023-00977-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the statistical analysis of networks finds application in an increasing number of disciplines, novel methodologies are needed to handle such complexity. In particular, cluster analysis is among the most successful and ubiquitous data exploration and characterisation techniques. In this work, we focus on how to represent networks ensembles for fuzzy clustering. We explore three different network representations based on probability distribution, autoencoders and joint embedding. We compare de facto standard fuzzy computational procedures for clustering multiple networks on synthetic data. Finally, we apply this approach to a real-world case study.},
  archive      = {J_DMKD},
  author       = {Bombelli, Ilaria and Manipur, Ichcha and Guarracino, Mario Rosario and Ferraro, Maria Brigida},
  doi          = {10.1007/s10618-023-00977-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {725-747},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Representing ensembles of networks for fuzzy cluster analysis: A case study},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The art of centering without centering for robust principal
component analysis. <em>DMKD</em>, <em>38</em>(2), 699–724. (<a
href="https://doi.org/10.1007/s10618-023-00976-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many robust variants of Principal Component Analysis remove outliers from the data and compute the principal components of the remaining data. The robust centered variant requires knowledge of the center of the non-outliers. Unfortunately, the center of non-outliers is unknown until after the outliers are determined, and using an inaccurate center may lead to the detection of wrong outliers. We demonstrate this problem in several known robust PCA algorithms. We describe a method that implicitly centers the non-outliers, implemented by appending a constant value (bias) to each data point. This bias method can be used with “black box” robust PCA algorithms by augmenting their input with minimal change to the algorithm itself.},
  archive      = {J_DMKD},
  author       = {Wan, Guihong and He, Baokun and Schweitzer, Haim},
  doi          = {10.1007/s10618-023-00976-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {699-724},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The art of centering without centering for robust principal component analysis},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Community detection in interval-weighted networks.
<em>DMKD</em>, <em>38</em>(2), 653–698. (<a
href="https://doi.org/10.1007/s10618-023-00975-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce and develop the concept of interval-weighted networks (IWN), a novel approach in Social Network Analysis, where the edge weights are represented by closed intervals composed with precise information, comprehending intrinsic variability. We extend IWN for both Newman’s modularity and modularity gain and the Louvain algorithm, considering a tabular representation of networks by contingency tables. We apply our methodology to two real-world IWN. The first is a commuter network in mainland Portugal, between the twenty three NUTS 3 Regions (IWCN). The second focuses on annual merchandise trade between 28 European countries, from 2003 to 2015 (IWTN). The optimal partition of geographic locations (regions or countries) is developed and compared using two new different approaches, designated as “Classic Louvain” and “Hybrid Louvain” , which allow taking into account the variability observed in the original network, thereby minimizing the loss of information present in the raw data. Our findings suggest the division of the twenty three Portuguese regions in three main communities for the IWCN and between two to three country communities for the IWTN. However, we find different geographical partitions according to the community detection methodology used. This analysis can be useful in many real-world applications, since it takes into account that the weights may vary within the ranges, rather than being constant.},
  archive      = {J_DMKD},
  author       = {Alves, Hélder and Brito, Paula and Campos, Pedro},
  doi          = {10.1007/s10618-023-00975-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {653-698},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Community detection in interval-weighted networks},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: A semi‑supervised interactive algorithm for
change point detection. <em>DMKD</em>, <em>38</em>(2), 652. (<a
href="https://doi.org/10.1007/s10618-023-01000-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Cao, Zhenxiang and Seeuws, Nick and De Vos, Maarten and Bertrand, Alexander},
  doi          = {10.1007/s10618-023-01000-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {652},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction: A semi‑supervised interactive algorithm for change point detection},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-supervised interactive algorithm for change point
detection. <em>DMKD</em>, <em>38</em>(2), 623–651. (<a
href="https://doi.org/10.1007/s10618-023-00974-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of change point detection (CPD) is to identify abrupt changes in the statistics of signals or time series that reflect transitions in the underlying system’s properties or states. While many statistical and learning-based approaches have been proposed to address this task, most state-of-the-art methods still treat this problem in an unsupervised setting. As a result, there is often a large gap between the algorithm-detected results and the expected outcomes of the user. To bridge this gap, we propose an active-learning strategy for the CPD problem that combines with the one-class support vector machine (OCSVM) model, resulting in an interactive CPD algorithm that improves itself by querying the end-user. This approach enables us to focus on detecting the desired change points and ignore false-positives or irrelevant change points. We demonstrate that the interactive OCSVM model can be combined with various unsupervised CPD models to function in a semi-supervised setting, resulting in improved detection accuracy. Our experimental results on various simulated and real-life datasets demonstrate a significant improvement in detection performance on both single- and multi-channel time series, even with a limited number of queries.},
  archive      = {J_DMKD},
  author       = {Cao, Zhenxiang and Seeuws, Nick and Vos, Maarten De and Bertrand, Alexander},
  doi          = {10.1007/s10618-023-00974-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {623-651},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A semi-supervised interactive algorithm for change point detection},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network embedding based on high-degree penalty and adaptive
negative sampling. <em>DMKD</em>, <em>38</em>(2), 597–622. (<a
href="https://doi.org/10.1007/s10618-023-00973-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding can effectively dig out potentially useful information and discover the relationships and rules which exist in the data, that has attracted increasing attention in many real-world applications. The goal of network embedding is to map high-dimensional and sparse networks into low-dimensional and dense vector representations. In this paper, we propose a network embedding method based on high-degree penalty and adaptive negative sampling (NEPS). First, we analyze the problem of imbalanced node training in random walk and propose an indicator base on high-degree penalty, which can control the random walk and avoid over-sampling high-degree neighbor node. Then, we propose a two-stage adaptive negative sampling strategy, which can dynamically obtain negative samples suitable for the current training according to the training stage to improve training effect. By comparing with seven well-known network embedding algorithms on eight real-world data sets, experiments show that the NEPS has good performance in node classification, network reconstruction and link prediction. The code is available at: https://github.com/Andrewsama/NEPS-master .},
  archive      = {J_DMKD},
  author       = {Ma, Gang-Feng and Yang, Xu-Hua and Ye, Wei and Xu, Xin-Li and Ye, Lei},
  doi          = {10.1007/s10618-023-00973-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {597-622},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Network embedding based on high-degree penalty and adaptive negative sampling},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mondrian forest for data stream classification under memory
constraints. <em>DMKD</em>, <em>38</em>(2), 569–596. (<a
href="https://doi.org/10.1007/s10618-023-00970-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning algorithms generally assume the availability of enough memory to store data models during the training and test phases. However, this assumption is unrealistic when data comes in the form of infinite data streams, or when learning algorithms are deployed on devices with reduced amounts of memory. In this paper, we adapt the online Mondrian forest classification algorithm to work with memory constraints on data streams. In particular, we design five out-of-memory strategies to update Mondrian trees with new data points when the memory limit is reached. Moreover, we design node trimming mechanisms to make Mondrian trees more robust to concept drifts under memory constraints. We evaluate our algorithms on a variety of real and simulated datasets, and we conclude with recommendations on their use in different situations: the Extend Node strategy appears as the best out-of-memory strategy in all configurations, whereas different node trimming mechanisms should be adopted depending on whether a concept drift is expected. All our methods are implemented in the OrpailleCC open-source library and are ready to be used on embedded systems and connected objects.},
  archive      = {J_DMKD},
  author       = {Khannouz, Martin and Glatard, Tristan},
  doi          = {10.1007/s10618-023-00970-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {569-596},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mondrian forest for data stream classification under memory constraints},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SALτ: Efficiently stopping TAR by improving priors
estimates. <em>DMKD</em>, <em>38</em>(2), 535–568. (<a
href="https://doi.org/10.1007/s10618-023-00961-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high recall retrieval tasks, human experts review a large pool of documents with the goal of satisfying an information need. Documents are prioritized for review through an active learning policy, and the process is usually referred to as Technology-Assisted Review (TAR). TAR tasks also aim to stop the review process once the target recall is achieved to minimize the annotation cost. In this paper, we introduce a new stopping rule called SAL $$_\tau ^R$$ (SLD for Active Learning), a modified version of the Saerens–Latinne–Decaestecker algorithm (SLD) that has been adapted for use in active learning. Experiments show that our algorithm stops the review well ahead of the current state-of-the-art methods, while providing the same guarantees of achieving the target recall.},
  archive      = {J_DMKD},
  author       = {Molinari, Alessio and Esuli, Andrea},
  doi          = {10.1007/s10618-023-00961-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {535-568},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SALτ: Efficiently stopping TAR by improving priors estimates},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An anomaly aware network embedding framework for
unsupervised anomalous link detection. <em>DMKD</em>, <em>38</em>(2),
501–534. (<a href="https://doi.org/10.1007/s10618-023-00960-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing network embedding based anomalous link detection methods regard network embedding and anomalous link detection as two independent tasks. However, removing anomalous links from the original network can reduce the data noise, thus hopefully improving the performance of network embedding models and anomalous link detection. In this paper, we propose an Anomaly Aware Network Embedding (AANE) framework by simultaneously learning node embedding and detecting anomalous links in a unified way. To instantiate the AANE framework, we propose a heuristic anomalous link selection based model AANE-H and an embedding disentangling based model AANE-D on Graph Auto-Encoder (GAE). In AANE-H, we adopt an anomalous link selector to iteratively select significant anomalous links based on a heuristic rule during model training, while in AANE-D the normal and anomalous links are generated by disentangled normal and anomalous embedding respectively. For the evaluation purpose, we propose a heuristic anomalous link generation algorithm to inject synthetic anomalous links into six real-world network datasets used in our experiments. Experimental results show that AANE outperforms both the state-of-the-art network embedding models and anomalous node detection models in terms of anomalous link detection performance. As a general network embedding model, AANE can also improve other downstream tasks like node classification.},
  archive      = {J_DMKD},
  author       = {Duan, Dongsheng and Zhang, Cheng and Tong, Lingling and Lu, Jie and Lv, Cunchi and Hou, Wei and Li, Yangxi and Zhao, Xiaofang},
  doi          = {10.1007/s10618-023-00960-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {501-534},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An anomaly aware network embedding framework for unsupervised anomalous link detection},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal selection of benchmarking datasets for unbiased
machine learning algorithm evaluation. <em>DMKD</em>, <em>38</em>(2),
461–500. (<a href="https://doi.org/10.1007/s10618-023-00957-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whenever a new supervised machine learning (ML) algorithm or solution is developed, it is imperative to evaluate the predictive performance it attains for diverse datasets. This is done in order to stress test the strengths and weaknesses of the novel algorithms and provide evidence for situations in which they are most useful. A common practice is to gather some datasets from public benchmark repositories for such an evaluation. But little or no specific criteria are used in the selection of these datasets, which is often ad-hoc. In this paper, the importance of gathering a diverse benchmark of datasets in order to properly evaluate ML models and really understand their capabilities is investigated. Leveraging from meta-learning studies evaluating the diversity of public repositories of datasets, this paper introduces an optimization method to choose varied classification and regression datasets from a pool of candidate datasets. The method is based on maximum coverage, circular packing, and the meta-heuristic Lichtenberg Algorithm for ensuring that diverse datasets able to challenge the ML algorithms more broadly are chosen. The selections were compared experimentally with a random selection of datasets and with clustering by k-medoids and proved to be more effective regarding the diversity of the chosen benchmarks and the ability to challenge the ML algorithms at different levels.},
  archive      = {J_DMKD},
  author       = {Pereira, João Luiz Junho and Smith-Miles, Kate and Muñoz, Mario Andrés and Lorena, Ana Carolina},
  doi          = {10.1007/s10618-023-00957-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {461-500},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Optimal selection of benchmarking datasets for unbiased machine learning algorithm evaluation},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression tree-based active learning. <em>DMKD</em>,
<em>38</em>(2), 420–460. (<a
href="https://doi.org/10.1007/s10618-023-00951-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms often require large training sets to perform well, but labeling such large amounts of data is not always feasible, as in many applications, substantial human effort and material cost is needed. Finding effective ways to reduce the size of training sets while maintaining the same performance is then crucial: one wants to choose the best sample of fixed size to be labeled among a given population, aiming at an accurate prediction of the response. This challenge has been studied in detail in classification, but not deeply enough in regression, which is known to be a more difficult task for active learning despite its need in practice. Few model-free active learning methods have been proposed that detect the new samples to be labeled using unlabeled data, but they lack the information of the conditional distribution between the response and the features. In this paper, we propose a standard regression tree-based active learning method for regression that improves significantly upon existing active learning approaches. It provides impressive results for small and large training sets and an appreciably low variance within several runs. We also exploit model-free approaches, and adapt them to our algorithm to utilize maximum information. Through experiments on numerous benchmark datasets, we demonstrate that our framework improves existing methods and is effective in learning a regression model from a very limited labeled dataset, reducing the sample size for a fixed level of performance, even with many features.},
  archive      = {J_DMKD},
  author       = {Jose, Ashna and de Mendonça, João Paulo Almeida and Devijver, Emilie and Jakse, Noël and Monbet, Valérie and Poloni, Roberta},
  doi          = {10.1007/s10618-023-00951-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {420-460},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Regression tree-based active learning},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sky-signatures: Detecting and characterizing recurrent
behavior in sequential data. <em>DMKD</em>, <em>38</em>(2), 372–419. (<a
href="https://doi.org/10.1007/s10618-023-00949-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the sky-signature model, an extension of the signature model Gautrais et al. (in: Proceedings of the Pacific-Asia conference on knowledge discovery and data mining (PAKDD), Springer, 2017b) to multi-objective optimization. The signature approach considers a sequence of itemsets, and given a number k it returns a segmentation of the sequence in k segments such that the number of items occuring in all segments is maximized. The limitation of this approach is that it requires to manually set k, and thus fixes the temporal granularity at which the data is analyzed. The sky-signature model proposed in this paper removes this requirement, and allows to examine the results at multiple levels of granularity, while keeping a compact output. This paper also proposes efficient algorithms to mine sky-signatures, as well as an experimental validation both real data both from the retail domain and from natural language processing (political speeches).},
  archive      = {J_DMKD},
  author       = {Gautrais, Clément and Cellier, Peggy and Guyet, Thomas and Quiniou, René and Termier, Alexandre},
  doi          = {10.1007/s10618-023-00949-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {372-419},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sky-signatures: Detecting and characterizing recurrent behavior in sequential data},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VEM <span class="math display"><sup>2</sup></span> l: An
easy but effective framework for fusing text and structure knowledge on
sparse knowledge graph completion. <em>DMKD</em>, <em>38</em>(2),
343–371. (<a href="https://doi.org/10.1007/s10618-023-01001-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of Knowledge Graph Completion (KGC) is to infer missing links for Knowledge Graphs (KGs) by analyzing graph structures. However, with increasing sparsity in KGs, this task becomes increasingly challenging. In this paper, we propose VEM $$^2$$ L, a joint learning framework that incorporates structure and relevant text information to supplement insufficient features for sparse KGs. We begin by training two pre-existing KGC models: one based on structure and the other based on text. Our ultimate goal is to fuse knowledge acquired by these models. To achieve this, we divide knowledge within the models into two non-overlapping parts: expressive power and generalization ability. We then propose two different joint learning methods that co-distill these two kinds of knowledge respectively. For expressive power, we allow each model to learn from and exchange knowledge mutually on training examples. For the generalization ability, we propose a novel co-distillation strategy using the Variational EM algorithm on unobserved queries. Our proposed joint learning framework is supported by both detailed theoretical evidence and qualitative experiments, demonstrating its effectiveness.},
  archive      = {J_DMKD},
  author       = {He, Tao and Liu, Ming and Cao, Yixin and Qu, Meng and Zheng, Zihao and Qin, Bing},
  doi          = {10.1007/s10618-023-01001-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {343-371},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {VEM $$^2$$ l: An easy but effective framework for fusing text and structure knowledge on sparse knowledge graph completion},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-input neural networks for time series forecasting
incorporating historical and prospective context. <em>DMKD</em>,
<em>38</em>(1), 315–341. (<a
href="https://doi.org/10.1007/s10618-023-00984-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual and societal systems are open systems continuously affected by their situational context. In recent years, context sources have been increasingly considered in different domains to aid short and long-term forecasts of systems’ behavior. Nevertheless, available research generally disregards the role of prospective context, such as calendrical planning or weather forecasts. This work proposes a multiple-input neural architecture consisting of a sequential composition of long short-term memory units or temporal convolutional networks able to incorporate both historical and prospective sources of situational context to aid time series forecasting tasks. Considering urban case studies, we further assess the impact that different sources of external context have on medical emergency and mobility forecasts. Results show that the incorporation of external context variables, including calendrical and weather variables, can significantly reduce forecasting errors against state-of-the-art forecasters. In particular, the incorporation of prospective context, generally neglected in related work, mitigates error increases along the forecasting horizon.},
  archive      = {J_DMKD},
  author       = {Palet, João and Manquinho, Vasco and Henriques, Rui},
  doi          = {10.1007/s10618-023-00984-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {315-341},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multiple-input neural networks for time series forecasting incorporating historical and prospective context},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Studying bias in visual features through the
lens of optimal transport. <em>DMKD</em>, <em>38</em>(1), 313–314. (<a
href="https://doi.org/10.1007/s10618-023-00986-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Fabbrizzi, Simone and Zhao, Xuan and Krasanakis, Emmanouil and Papadopoulos, Symeon and Ntoutsi, Eirini},
  doi          = {10.1007/s10618-023-00986-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {313-314},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: Studying bias in visual features through the lens of optimal transport},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Studying bias in visual features through the lens of
optimal transport. <em>DMKD</em>, <em>38</em>(1), 281–312. (<a
href="https://doi.org/10.1007/s10618-023-00972-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision systems are employed in a variety of high-impact applications. However, making them trustworthy requires methods for the detection of potential biases in their training data, before models learn to harm already disadvantaged groups in downstream applications. Image data are typically represented via extracted features, which can be hand-crafted or pre-trained neural network embeddings. In this work, we introduce a framework for bias discovery given such features that is based on optimal transport theory; it uses the (quadratic) Wasserstein distance to quantify disparity between the feature distributions of two demographic groups (e.g., women vs men). In this context, we show that the Kantorovich potentials of the images, which are a byproduct of computing the Wasserstein distance and act as “transportation prices&quot;, can serve as bias scores by indicating which images might exhibit distinct biased characteristics. We thus introduce a visual dataset exploration pipeline that helps auditors identify common characteristics across high- or low-scored images as potential sources of bias. We conduct a case study to identify prospective gender biases and demonstrate theoretically-derived properties with experiments on the CelebA and Biased MNIST datasets.},
  archive      = {J_DMKD},
  author       = {Fabbrizzi, Simone and Zhao, Xuan and Krasanakis, Emmanouil and Papadopoulos, Symeon and Ntoutsi, Eirini},
  doi          = {10.1007/s10618-023-00972-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {281-312},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Studying bias in visual features through the lens of optimal transport},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can local explanation techniques explain linear additive
models? <em>DMKD</em>, <em>38</em>(1), 237–280. (<a
href="https://doi.org/10.1007/s10618-023-00971-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local model-agnostic additive explanation techniques decompose the predicted output of a black-box model into additive feature importance scores. Questions have been raised about the accuracy of the produced local additive explanations. We investigate this by studying whether some of the most popular explanation techniques can accurately explain the decisions of linear additive models. We show that even though the explanations generated by these techniques are linear additives, they can fail to provide accurate explanations when explaining linear additive models. In the experiments, we measure the accuracy of additive explanations, as produced by, e.g., LIME and SHAP, along with the non-additive explanations of Local Permutation Importance (LPI) when explaining Linear and Logistic Regression and Gaussian naive Bayes models over 40 tabular datasets. We also investigate the degree to which different factors, such as the number of numerical or categorical or correlated features, the predictive performance of the black-box model, explanation sample size, similarity metric, and the pre-processing technique used on the dataset can directly affect the accuracy of local explanations.},
  archive      = {J_DMKD},
  author       = {Rahnama, Amir Hossein Akhavan and Bütepage, Judith and Geurts, Pierre and Boström, Henrik},
  doi          = {10.1007/s10618-023-00971-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {237-280},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Can local explanation techniques explain linear additive models?},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Z-time: Efficient and effective interpretable multivariate
time series classification. <em>DMKD</em>, <em>38</em>(1), 206–236. (<a
href="https://doi.org/10.1007/s10618-023-00969-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series classification has become popular due to its prevalence in many real-world applications. However, most state-of-the-art focuses on improving classification performance, with the best-performing models typically opaque. Interpretable multivariate time series classifiers have been recently introduced, but none can maintain sufficient levels of efficiency and effectiveness together with interpretability. We introduce Z-Time, a novel algorithm for effective and efficient interpretable multivariate time series classification. Z-Time employs temporal abstraction and temporal relations of event intervals to create interpretable features across multiple time series dimensions. In our experimental evaluation on the UEA multivariate time series datasets, Z-Time achieves comparable effectiveness to state-of-the-art non-interpretable multivariate classifiers while being faster than all interpretable multivariate classifiers. We also demonstrate that Z-Time is more robust to missing values and inter-dimensional orders, compared to its interpretable competitors.},
  archive      = {J_DMKD},
  author       = {Lee, Zed and Lindgren, Tony and Papapetrou, Panagiotis},
  doi          = {10.1007/s10618-023-00969-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {206-236},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Z-time: Efficient and effective interpretable multivariate time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving neural network’s robustness on tabular data with
d-layers. <em>DMKD</em>, <em>38</em>(1), 173–205. (<a
href="https://doi.org/10.1007/s10618-023-00965-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks ( $${{{\texttt {ANN}}}}$$ ) are widely used machine learning models. Their widespread use has attracted a lot of interest in their robustness. Many studies show that ANN’s performance can be highly vulnerable to input manipulation such as adversarial attacks and covariate drift. Therefore, various techniques that focus on improving $${{{\texttt {ANN}}}}$$ ’s robustness have been proposed in the last few years. However, most of these works have mostly focused on image data. In this paper, we investigate the role of discretization in improving $${{{\texttt {ANN}}}}$$ ’s robustness on tabular datasets. Two custom $${{{\texttt {ANN}}}}$$ layers– D1-Layer and D2-Layer (collectively called D-Layers) are proposed. The two layers integrate discretization during the training phase to improve $${{{\texttt {ANN}}}}$$ ’s ability to defend against adversarial attacks. Additionally, D2-Layer integrates dynamic discretization during testing phase as well, to provide a unified strategy to handle adversarial attacks and covariate drift. The experimental results on 24 publicly available datasets show that our proposed D-Layers add much-needed robustness to $${{{\texttt {ANN}}}}$$ for tabular datasets.},
  archive      = {J_DMKD},
  author       = {Xia, Haiyang and Zaidi, Nayyar and Zhang, Yishuo and Li, Gang},
  doi          = {10.1007/s10618-023-00965-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {173-205},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving neural network’s robustness on tabular data with D-layers},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MODE-bi-GRU: Orthogonal independent bi-GRU model with
multiscale feature extraction. <em>DMKD</em>, <em>38</em>(1), 154–172.
(<a href="https://doi.org/10.1007/s10618-023-00964-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of sentence classification is to extract sentence semantic features. The existing hybrid methods have huge parameters and complex models. Due to the limited dataset, these methods are prone to feature redundancy and overfitting. To address this issue, this paper proposes an orthogonal independent Bi-GRU sentence classification model with multi-scale feature extraction, called Multi-scale Orthogonal Independent Bi-GRU (MODE-Bi-GRU). First, the hidden state of the Bi-GRU model is split into multiple small hidden states, and the corresponding recursive matrix is constrained orthogonally. Then, multiple sliding windows of different sizes are defined according to the forward and reverse angles of the sentence, and the sliding window is obtained. Finally, different sentence fragments are superimposed and input to the model, and the output results of multiple small Bi-GRU models are spliced and processed by soft pooling. The improved focal loss function is adopted to speed up the convergence of the model. Compared to the existing models, our proposed model achieves better results on four benchmark datasets, and it has better generalization ability with fewer parameters.},
  archive      = {J_DMKD},
  author       = {Wang, Wei and Ruan, Wenhan and Meng, Xiangfu},
  doi          = {10.1007/s10618-023-00964-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {154-172},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MODE-bi-GRU: Orthogonal independent bi-GRU model with multiscale feature extraction},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attention matrix for every decision: Faithfulness-based
arbitration among multiple attention-based interpretations of
transformers in text classification. <em>DMKD</em>, <em>38</em>(1),
128–153. (<a href="https://doi.org/10.1007/s10618-023-00962-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers are widely used in natural language processing, where they consistently achieve state-of-the-art performance. This is mainly due to their attention-based architecture, which allows them to model rich linguistic relations between (sub)words. However, transformers are difficult to interpret. Being able to provide reasoning for its decisions is an important property for a model in domains where human lives are affected. With transformers finding wide use in such fields, the need for interpretability techniques tailored to them arises. We propose a new technique that selects the most faithful attention-based interpretation among the several ones that can be obtained by combining different head, layer and matrix operations. In addition, two variations are introduced towards (i) reducing the computational complexity, thus being faster and friendlier to the environment, and (ii) enhancing the performance in multi-label data. We further propose a new faithfulness metric that is more suitable for transformer models and exhibits high correlation with the area under the precision-recall curve based on ground truth rationales. We validate the utility of our contributions with a series of quantitative and qualitative experiments on seven datasets.},
  archive      = {J_DMKD},
  author       = {Mylonas, Nikolaos and Mollas, Ioannis and Tsoumakas, Grigorios},
  doi          = {10.1007/s10618-023-00962-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {128-153},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An attention matrix for every decision: Faithfulness-based arbitration among multiple attention-based interpretations of transformers in text classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving graph neural networks by combining active learning
with self-training. <em>DMKD</em>, <em>38</em>(1), 110–127. (<a
href="https://doi.org/10.1007/s10618-023-00959-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework, called STAL, which makes use of unlabeled graph data, through a combination of Active Learning and Self-Training, in order to improve node labeling by Graph Neural Networks (GNNs). GNNs have been shown to perform well on many tasks, when sufficient labeled data are available. Such data, however, is often scarce, leading to the need for methods that leverage unlabeled data that are abundant. Active Learning and Self-training are two common approaches towards this goal and we investigate here their combination, in the context of GNN training. Specifically, we propose a new framework that first uses active learning to select highly uncertain unlabeled nodes to be labeled and be included in the training set. In each iteration of active labeling, the proposed method expands also the label set through self-training. In particular, highly certain pseudo-labels are obtained and added automatically to the training set. This process is repeated, leading to good classifiers, with a limited amount of labeled data. Our experimental results on various datasets confirm the efficiency of the proposed approach.},
  archive      = {J_DMKD},
  author       = {Katsimpras, Georgios and Paliouras, Georgios},
  doi          = {10.1007/s10618-023-00959-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {110-127},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving graph neural networks by combining active learning with self-training},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tale of two roles: Exploring topic-specific susceptibility
and influence in cascade prediction. <em>DMKD</em>, <em>38</em>(1),
79–109. (<a href="https://doi.org/10.1007/s10618-023-00953-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new deep learning cascade prediction model CasSIM that can simultaneously achieve two most demanded objectives: popularity prediction and final adopter prediction. Compared to existing methods based on cascade representation, CasSIM simulates information diffusion processes by exploring users’ dual roles in information propagation with three basic factors: users’ susceptibilities, influences and message contents. With effective user profiling, we are the first to capture the topic-specific property of susceptibilities and influences. In addition, the use of graph neural networks allows CasSIM to capture the dynamics of susceptibilities and influences during information diffusion. We evaluate the effectiveness of CasSIM on three real-life datasets and the results show that CasSIM outperforms the state-of-the-art methods in popularity and final adopter prediction.},
  archive      = {J_DMKD},
  author       = {Chen, Ninghan and Chen, Xihui and Zhong, Zhiqiang and Pang, Jun},
  doi          = {10.1007/s10618-023-00953-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {79-109},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A tale of two roles: Exploring topic-specific susceptibility and influence in cascade prediction},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable screening for lasso based on multidimensional
indexing. <em>DMKD</em>, <em>38</em>(1), 49–78. (<a
href="https://doi.org/10.1007/s10618-023-00950-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a correlation based safe screening technique for building the complete Lasso path. Unlike many other Lasso screening approaches we do not consider prespecified values of the regularization parameter, but, instead, prune variables which cannot be the next best feature to be added to the model. Based on those results we present a modified homotopy algorithm for computing the regularization path. We demonstrate that, even though our algorithm provides the complete Lasso path, its performance is competitive with state of the art algorithms which, however, only provide solutions at a prespecified sample of regularization parameters. We also address problems of extremely high dimensionality, where the variables may not fit into main memory and are assumed to be stored on disk. A multidimensional index is used to quickly retrieve potentially relevant variables. We apply the approach to the important case when multiple models are built against a fixed set of variables, frequently encountered in statistical databases. We perform experiments using the complete Eurostat database as predictors and demonstrate that our approach allows for practical and efficient construction of Lasso models, which remain accurate and interpretable even when millions of highly correlated predictors are present.},
  archive      = {J_DMKD},
  author       = {Żogała-Siudem, Barbara and Jaroszewicz, Szymon},
  doi          = {10.1007/s10618-023-00950-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {49-78},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Variable screening for lasso based on multidimensional indexing},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving position encoding of transformers for multivariate
time series classification. <em>DMKD</em>, <em>38</em>(1), 22–48. (<a
href="https://doi.org/10.1007/s10618-023-00948-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose a novel multivariate time series classification model combining tAPE/eRPE and convolution-based input encoding named ConvTran to improve the position and data embedding of time series data. The proposed absolute and relative position encoding methods are simple and efficient. They can be easily integrated into transformer blocks and used for downstream tasks such as forecasting, extrinsic regression, and anomaly detection. Extensive experiments on 32 multivariate time-series datasets show that our model is significantly more accurate than state-of-the-art convolution and transformer-based models. Code and models are open-sourced at https://github.com/Navidfoumani/ConvTran .},
  archive      = {J_DMKD},
  author       = {Foumani, Navid Mohammadi and Tan, Chang Wei and Webb, Geoffrey I. and Salehi, Mahsa},
  doi          = {10.1007/s10618-023-00948-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {22-48},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving position encoding of transformers for multivariate time series classification},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TenGAN: Adversarially generating multiplex tensor graphs.
<em>DMKD</em>, <em>38</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10618-023-00947-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore multiplex graph (networks with different types of edges) generation with deep generative models. We discuss some of the challenges associated with multiplex graph generation that make it a more difficult problem than traditional graph generation. We propose TenGAN, the first neural network for multiplex graph generation, which greatly reduces the number of parameters required for multiplex graph generation. We also propose 3 different criteria for evaluating the quality of generated graphs: a graph-attribute-based, a classifier-based, and a tensor-based method. We evaluate its performance on 4 datasets and show that it generally performs better than other existing statistical multiplex graph generative models. We also adapt HGEN, an existing deep generative model for heterogeneous information networks, to work for multiplex graphs and show that our method generally performs better.},
  archive      = {J_DMKD},
  author       = {Shiao, William and Miller, Benjamin A. and Chan, Kevin and Yu, Paul and Eliassi-Rad, Tina and Papalexakis, Evangelos E.},
  doi          = {10.1007/s10618-023-00947-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TenGAN: Adversarially generating multiplex tensor graphs},
  volume       = {38},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
