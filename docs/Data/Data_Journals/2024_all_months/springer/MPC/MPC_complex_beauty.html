<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MPC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mpc---20">MPC - 20</h2>
<ul>
<li><details>
<summary>
(2024). Solving higher dimensional expensive black box global
optimization problems using sparse directional search on surrogates.
<em>MPC</em>, <em>16</em>(4), 665–693. (<a
href="https://doi.org/10.1007/s12532-024-00267-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm Directional Optimization Search with Surrogate (DOSS), for optimizing problems with box constraints and a computationally expensive black-box objective function. DOSS is a radial basis function (RBF) based method that mainly focuses on higher dimensional and computationally expensive objective functions that can be multimodal. DOSS introduces three new techniques not previously used in earlier RBF algorithms, including using a combination of the coordinates knowledge level, fewer initial sampling points, and the surrogate’s gradient information. Numerical results on a test set including 14 test problems with 36, 48, and 60 dimensions show that DOSS outperforms two recently published algorithms RBFOpt and TuRBO, and earlier RBF algorithms such as DYCORS. TuRBO is a Gaussian process based optimization algorithm, which outperformed earlier state-of-the-art methods. DOSS algorithm also has a good performance on real-world optimization problems, including robot pushing and rover trajectory planning problems. Almost sure convergence for the DOSS algorithm is also proven in this paper. An implementation of DOSS is available at https://doi.org/10.5281/zenodo.13731558 .},
  archive      = {J_MPC},
  author       = {Liu, Limeng and Yang, Muming and Shoemaker, Christine A. and Xie, Tingting},
  doi          = {10.1007/s12532-024-00267-7},
  journal      = {Mathematical Programming Computation},
  month        = {12},
  number       = {4},
  pages        = {665-693},
  shortjournal = {Math. Program. Comput.},
  title        = {Solving higher dimensional expensive black box global optimization problems using sparse directional search on surrogates},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularized step directions in nonlinear conjugate gradient
methods. <em>MPC</em>, <em>16</em>(4), 629–664. (<a
href="https://doi.org/10.1007/s12532-024-00265-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conjugate gradient minimization methods (CGM) and their accelerated variants are widely used. We focus on the use of cubic regularization to improve the CGM direction independent of the step length computation. In this paper, we propose the Hybrid Cubic Regularization of CGM, where regularized steps are used selectively. Using Shanno’s reformulation of CGM as a memoryless BFGS method, we derive new formulas for the regularized step direction. We show that the regularized step direction uses the same order of computational burden per iteration as its non-regularized version. Moreover, the Hybrid Cubic Regularization of CGM exhibits global convergence with fewer assumptions. In numerical experiments, the new step directions are shown to require fewer iteration counts, improve runtime, and reduce the need to reset the step direction. Overall, the Hybrid Cubic Regularization of CGM exhibits the same memoryless and matrix-free properties, while outperforming CGM as a memoryless BFGS method in iterations and runtime.},
  archive      = {J_MPC},
  author       = {Buhler, Cassidy K. and Benson, Hande Y. and Shanno, David F.},
  doi          = {10.1007/s12532-024-00265-9},
  journal      = {Mathematical Programming Computation},
  month        = {12},
  number       = {4},
  pages        = {629-664},
  shortjournal = {Math. Program. Comput.},
  title        = {Regularized step directions in nonlinear conjugate gradient methods},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local elimination in the traveling salesman problem.
<em>MPC</em>, <em>16</em>(4), 599–628. (<a
href="https://doi.org/10.1007/s12532-024-00262-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hougardy and Schroeder (WG 2014) proposed a combinatorial technique for pruning the search space in the traveling salesman problem, establishing that, for a given instance, certain edges cannot be present in any optimal tour. We describe an implementation of their technique, employing an exact TSP solver to locate k-opt moves in the elimination process. In our computational study, we combine LP reduced-cost elimination together with the new combinatorial algorithm. We report results on a set of geometric instances, with the number of points n ranging from 3038 up to 115,475. The test set includes all TSPLIB instances having at least 3000 points, together with 250 randomly generated instances, each with 10,000 points, and three currently unsolved instances having 100,000 or more points. In all but two of the test instances, the complete-graph edge sets were reduced to under 3n edges. For the three large unsolved instances, repeated runs of the elimination process reduced the graphs to under 2.5n edges.},
  archive      = {J_MPC},
  author       = {Cook, William and Helsgaun, Keld and Hougardy, Stefan and Schroeder, Rasmus T.},
  doi          = {10.1007/s12532-024-00262-y},
  journal      = {Mathematical Programming Computation},
  month        = {12},
  number       = {4},
  pages        = {599-628},
  shortjournal = {Math. Program. Comput.},
  title        = {Local elimination in the traveling salesman problem},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A numerical study of transformed mixed-integer optimal
control problems. <em>MPC</em>, <em>16</em>(4), 561–597. (<a
href="https://doi.org/10.1007/s12532-024-00263-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time transformation is a ubiquitous tool in theoretical sciences, especially in physics. It can also be used to transform switched optimal control problems into control problems with a fixed switching order and purely continuous decisions. This approach is known either as enhanced time transformation, time-scaling, or switching time optimization (STO) for mixed-integer optimal control. The approach is well understood and used widely due to its many favorable properties. Recently, several extensions and algorithmic improvements have been proposed. We use an alternative formulation, the partial outer convexification (POC), to study convergence properties of (STO). We introduce the open-source software package ampl_mintoc (Sager et al., czeile/ampl_mintoc: Math programming c release, 2024, https://doi.org/10.5281/zenodo.12520490 ). It is based on AMPL, designed for the formulation of mixed-integer optimal control problems, and allows to use almost identical implementations for (STO) and (POC). We discuss and explain our main numerical result: (STO) is likely to result in more local minima for each discretization grid than (POC), but the number of local minima is asymptotically identical for both approaches.},
  archive      = {J_MPC},
  author       = {Sager, Sebastian and Tetschke, Manuel and Zeile, Clemens},
  doi          = {10.1007/s12532-024-00263-x},
  journal      = {Mathematical Programming Computation},
  month        = {12},
  number       = {4},
  pages        = {561-597},
  shortjournal = {Math. Program. Comput.},
  title        = {A numerical study of transformed mixed-integer optimal control problems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PDFO: A cross-platform package for powell’s derivative-free
optimization solvers. <em>MPC</em>, <em>16</em>(4), 535–559. (<a
href="https://doi.org/10.1007/s12532-024-00257-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The late Professor M. J. D. Powell devised five trust-region methods for derivative-free optimization, namely COBYLA, UOBYQA, NEWUOA, BOBYQA, and LINCOA. He carefully implemented them into publicly available solvers, renowned for their robustness and efficiency. However, the solvers were implemented in Fortran 77 and hence may not be easily accessible to some users. We introduce the PDFO package, which provides user-friendly Python and MATLAB interfaces to Powell’s code. With PDFO, users of such languages can call Powell’s Fortran solvers easily without dealing with the Fortran code. Moreover, PDFO includes bug fixes and improvements, which are particularly important for handling problems that suffer from ill-conditioning or failures of function evaluations. In addition to the PDFO package, we provide an overview of Powell’s methods, sketching them from a uniform perspective, summarizing their main features, and highlighting the similarities and interconnections among them. We also present experiments on PDFO to demonstrate its stability under noise, tolerance of failures in function evaluations, and potential to solve certain hyperparameter optimization problems.},
  archive      = {J_MPC},
  author       = {Ragonneau, Tom M. and Zhang, Zaikun},
  doi          = {10.1007/s12532-024-00257-9},
  journal      = {Mathematical Programming Computation},
  month        = {12},
  number       = {4},
  pages        = {535-559},
  shortjournal = {Math. Program. Comput.},
  title        = {PDFO: A cross-platform package for powell’s derivative-free optimization solvers},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving clustered low-rank semidefinite programs arising
from polynomial optimization. <em>MPC</em>, <em>16</em>(3), 503–534. (<a
href="https://doi.org/10.1007/s12532-024-00264-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a primal-dual interior point method specialized to clustered low-rank semidefinite programs requiring high precision numerics, which arise from certain multivariate polynomial (matrix) programs through sums-of-squares characterizations and sampling. We consider the interplay of sampling and symmetry reduction as well as a greedy method to obtain numerically good bases and sample points. We apply this to the computation of three-point bounds for the kissing number problem, for which we show a significant speedup. This allows for the computation of improved kissing number bounds in dimensions 11 through 23. The approach performs well for problems with bad numerical conditioning, which we show through new computations for the binary sphere packing problem.},
  archive      = {J_MPC},
  author       = {Leijenhorst, Nando and de Laat, David},
  doi          = {10.1007/s12532-024-00264-w},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {503-534},
  shortjournal = {Math. Program. Comput.},
  title        = {Solving clustered low-rank semidefinite programs arising from polynomial optimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective matrix adaptation strategy for noisy
derivative-free optimization. <em>MPC</em>, <em>16</em>(3), 459–501. (<a
href="https://doi.org/10.1007/s12532-024-00261-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new effective matrix adaptation evolution strategy (MADFO) for noisy derivative-free optimization problems. Like every MAES solver, MADFO consists of three phases: mutation, selection and recombination. MADFO improves the mutation phase by generating good step sizes, neither too small not too large, that increase the probability of selecting mutation points with small inexact function values in the selection phase. In the recombination phase, a recombination point with lowest inexact function value found among all evaluated points so far may be found by a new randomized non-monotone line search method and accepted as the best point. If no best point is found, a heuristic point may be accepted as the best point. We compare MADFO with state-of-the-art DFO solvers on noisy test problems obtained by adding various kinds and levels of noise to all unconstrained CUTEst test problems with dimensions $$n\le 20$$ , and find that MADFO has the highest number of solved problems},
  archive      = {J_MPC},
  author       = {Kimiaei, Morteza and Neumaier, Arnold},
  doi          = {10.1007/s12532-024-00261-z},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {459-501},
  shortjournal = {Math. Program. Comput.},
  title        = {Effective matrix adaptation strategy for noisy derivative-free optimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient algorithms for implementing incremental
proximal-point methods. <em>MPC</em>, <em>16</em>(3), 423–458. (<a
href="https://doi.org/10.1007/s12532-024-00258-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model training algorithms which observe a small portion of the training set in each computational step are ubiquitous in practical machine learning, and include both stochastic and online optimization methods. In the vast majority of cases, such algorithms typically observe the training samples via the gradients of the cost functions the samples incur. Thus, these methods exploit are the slope of the cost functions via their first-order approximations. To address limitations of gradient-based methods, such as sensitivity to step-size choice in the stochastic setting, or inability to exploit small function variability in the online setting, several streams of research attempt to exploit more information about the cost functions than just their gradients via the well-known proximal operators. However, implementing such methods in practice poses a challenge, since each iteration step boils down to computing the proximal operator, which may not be as easy as computing a gradient. In this work we devise a novel algorithmic framework, which exploits convex duality theory to achieve both algorithmic efficiency and software modularity of proximal operator implementations, in order to make experimentation with incremental proximal optimization algorithms accessible to a larger audience of researchers and practitioners, by reducing the gap between their theoretical description in research papers and their use in practice. We provide a reference Python implementation for the framework developed in this paper as an open source library at on GitHub ( https://github.com/alexshtf/inc_prox_pt/releases/tag/prox_pt_paper ) Shtoff (Efficient implementation of incremental proximal point methods arXiv:2205.01457 , 2024), along with examples which demonstrate our implementation on a variety of problems, and reproduce the numerical experiments in this paper. The pure Python reference implementation is not necessarily the most efficient, but is a basis for creating efficient implementations by combining Python with a native backend.},
  archive      = {J_MPC},
  author       = {Shtoff, Alex},
  doi          = {10.1007/s12532-024-00258-8},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {423-458},
  shortjournal = {Math. Program. Comput.},
  title        = {Efficient algorithms for implementing incremental proximal-point methods},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A trust-region framework for derivative-free mixed-integer
optimization. <em>MPC</em>, <em>16</em>(3), 369–422. (<a
href="https://doi.org/10.1007/s12532-024-00260-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper overviews the development of a framework for the optimization of black-box mixed-integer functions subject to bound constraints. Our methodology is based on the use of tailored surrogate approximations of the unknown objective function, in combination with a trust-region method. To construct suitable model approximations, we assume that the unknown objective is locally quadratic, and we prove that this leads to fully-linear models in restricted discrete neighborhoods. We show that the proposed algorithm converges to a first-order mixed-integer stationary point according to several natural definitions of mixed-integer stationarity, depending on the structure of the objective function. We present numerical results to illustrate the computational performance of different implementations of this methodology in comparison with the state-of-the-art derivative-free solver NOMAD.},
  archive      = {J_MPC},
  author       = {Torres, Juan J. and Nannicini, Giacomo and Traversi, Emiliano and Wolfler Calvo, Roberto},
  doi          = {10.1007/s12532-024-00260-0},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {369-422},
  shortjournal = {Math. Program. Comput.},
  title        = {A trust-region framework for derivative-free mixed-integer optimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PEPit: Computer-assisted worst-case analyses of first-order
optimization methods in python. <em>MPC</em>, <em>16</em>(3), 337–367.
(<a href="https://doi.org/10.1007/s12532-024-00259-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PEPit is a python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first-order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. To do that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modeling parts, and the worst-case analysis is performed numerically via standard solvers.},
  archive      = {J_MPC},
  author       = {Goujaud, Baptiste and Moucer, Céline and Glineur, François and Hendrickx, Julien M. and Taylor, Adrien B. and Dieuleveut, Aymeric},
  doi          = {10.1007/s12532-024-00259-7},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {337-367},
  shortjournal = {Math. Program. Comput.},
  title        = {PEPit: Computer-assisted worst-case analyses of first-order optimization methods in python},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PyEPO: A PyTorch-based end-to-end predict-then-optimize
library for linear and integer programming. <em>MPC</em>,
<em>16</em>(3), 297–335. (<a
href="https://doi.org/10.1007/s12532-024-00255-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deterministic optimization, it is typically assumed that all problem parameters are fixed and known. In practice, however, some parameters may be a priori unknown but can be estimated from contextual information. A typical predict-then-optimize approach separates predictions and optimization into two distinct stages. Recently, end-to-end predict-then-optimize has emerged as an attractive alternative. This work introduces the PyEPO package, a PyTorch-based end-to-end predict-then-optimize library in Python. To the best of our knowledge, PyEPO (pronounced like pineapple with a silent “n&quot;) is the first such generic tool for linear and integer programming with predicted objective function coefficients. It includes various algorithms such as surrogate decision losses, black-box solvers, and perturbated methods. PyEPO offers a user-friendly interface for defining new optimization problems, applying state-of-the-art algorithms, and using custom neural network architectures. We conducted experiments comparing various methods on problems such as the Shortest Path, the Multiple Knapsack, and the Traveling Salesperson Problem, discussing empirical insights that may guide future research. PyEPO and its documentation are available at https://github.com/khalil-research/PyEPO .},
  archive      = {J_MPC},
  author       = {Tang, Bo and Khalil, Elias B.},
  doi          = {10.1007/s12532-024-00255-x},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {297-335},
  shortjournal = {Math. Program. Comput.},
  title        = {PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressively strengthening and tuning MIP solvers for
reoptimization. <em>MPC</em>, <em>16</em>(2), 267–295. (<a
href="https://doi.org/10.1007/s12532-024-00253-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores reoptimization techniques for solving sequences of similar mixed integer programs (MIPs) more effectively. Traditionally, these MIPs are solved independently, without capitalizing on information from previously solved instances. Our approach focuses on primal bound improvements by reusing the solutions of the previously solved instances, as well as dual bound improvements by reusing the branching history and automating parameter tuning. We also describe ways to improve the solver performance by extending ideas from reliability branching to generate better pseudocosts. Our reoptimization approach, crafted for the MIP 2023 workshop computational competition, was honored with the first prize. In this paper, we thoroughly analyze the performance of each technique and their combined impact on the solver’s performance. Finally, we present ways to extend our techniques in practice for further improvements.},
  archive      = {J_MPC},
  author       = {Patel, Krunal Kishor},
  doi          = {10.1007/s12532-024-00253-z},
  journal      = {Mathematical Programming Computation},
  month        = {6},
  number       = {2},
  pages        = {267-295},
  shortjournal = {Math. Program. Comput.},
  title        = {Progressively strengthening and tuning MIP solvers for reoptimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The MIP workshop 2023 computational competition on
reoptimization. <em>MPC</em>, <em>16</em>(2), 255–266. (<a
href="https://doi.org/10.1007/s12532-024-00256-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the computational challenge developed for a computational competition held in 2023 for the $$20{\text {th}}$$ anniversary of the Mixed Integer Programming Workshop. The topic of this competition was reoptimization, also known as warm starting, of mixed integer linear optimization problems after slight changes to the input data for a common formulation. The challenge was to accelerate the proof of optimality of the modified instances by leveraging the information from the solving processes of previously solved instances, all while creating high-quality primal solutions. Specifically, we discuss the competition’s format, the creation of public and hidden datasets, and the evaluation criteria. Our goal is to establish a methodology for the generation of benchmark instances and an evaluation framework, along with benchmark datasets, to foster future research on reoptimization of mixed integer linear optimization problems.},
  archive      = {J_MPC},
  author       = {Bolusani, Suresh and Besançon, Mathieu and Gleixner, Ambros and Berthold, Timo and D’Ambrosio, Claudia and Muñoz, Gonzalo and Paat, Joseph and Thomopulos, Dimitri},
  doi          = {10.1007/s12532-024-00256-w},
  journal      = {Mathematical Programming Computation},
  month        = {6},
  number       = {2},
  pages        = {255-266},
  shortjournal = {Math. Program. Comput.},
  title        = {The MIP workshop 2023 computational competition on reoptimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear conjugate gradient for smooth convex functions.
<em>MPC</em>, <em>16</em>(2), 229–254. (<a
href="https://doi.org/10.1007/s12532-024-00254-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of nonlinear conjugate gradients (NCG) is widely used in practice for unconstrained optimization, but it satisfies weak complexity bounds at best when applied to smooth convex functions. In contrast, Nesterov’s accelerated gradient (AG) method is optimal up to constant factors for this class. However, when specialized to quadratic function, conjugate gradient is optimal in a strong sense among function-gradient methods. Therefore, there is seemingly a gap in the menu of available algorithms: NCG, the optimal algorithm for quadratic functions that also exhibits good practical performance for general functions, has poor complexity bounds compared to AG. We propose an NCG method called C+AG (“conjugate plus accelerated gradient”) to close this gap, that is, it is optimal for quadratic functions and still satisfies the best possible complexity bound for more general smooth convex functions. It takes conjugate gradient steps until insufficient progress is made, at which time it switches to accelerated gradient steps, and later retries conjugate gradient. The proposed method has the following theoretical properties: (i) It is identical to linear conjugate gradient (and hence terminates finitely) if the objective function is quadratic; (ii) Its running-time bound is $$O(\epsilon ^{-1/2})$$ gradient evaluations for an L-smooth convex function, where $$\epsilon $$ is the desired residual reduction, (iii) Its running-time bound is $$O(\sqrt{L/\ell }\ln (1/\epsilon ))$$ if the function is both L-smooth and $$\ell $$ -strongly convex. We also conjecture and outline a proof that a variant of the method has the property: (iv) It is n-step quadratically convergent for a function whose second derivative is smooth and invertible at the optimizer. Note that the bounds in (ii) and (iii) match AG and are the best possible, i.e., they match lower bounds up to constant factors for the classes of functions under consideration. On the other hand, (i) and (iv) match NCG. In computational tests, the function-gradient evaluation count for the C+AG method typically behaves as whichever is better of AG or classical NCG. In some test cases it outperforms both.},
  archive      = {J_MPC},
  author       = {Karimi, Sahar and Vavasis, Stephen A.},
  doi          = {10.1007/s12532-024-00254-y},
  journal      = {Mathematical Programming Computation},
  month        = {6},
  number       = {2},
  pages        = {229-254},
  shortjournal = {Math. Program. Comput.},
  title        = {Nonlinear conjugate gradient for smooth convex functions},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new computational framework for log-concave density
estimation. <em>MPC</em>, <em>16</em>(2), 185–228. (<a
href="https://doi.org/10.1007/s12532-024-00252-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistics, log-concave density estimation is a central problem within the field of nonparametric inference under shape constraints. Despite great progress in recent years on the statistical theory of the canonical estimator, namely the log-concave maximum likelihood estimator, adoption of this method has been hampered by the complexities of the non-smooth convex optimization problem that underpins its computation. We provide enhanced understanding of the structural properties of this optimization problem, which motivates the proposal of new algorithms, based on both randomized and Nesterov smoothing, combined with an appropriate integral discretization of increasing accuracy. We prove that these methods enjoy, both with high probability and in expectation, a convergence rate of order 1/T up to logarithmic factors on the objective function scale, where T denotes the number of iterations. The benefits of our new computational framework are demonstrated on both synthetic and real data, and our implementation is available in a github repository LogConcComp (Log-Concave Computation).},
  archive      = {J_MPC},
  author       = {Chen, Wenyu and Mazumder, Rahul and Samworth, Richard J.},
  doi          = {10.1007/s12532-024-00252-0},
  journal      = {Mathematical Programming Computation},
  month        = {6},
  number       = {2},
  pages        = {185-228},
  shortjournal = {Math. Program. Comput.},
  title        = {A new computational framework for log-concave density estimation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KidneyExchange.jl: A julia package for solving the kidney
exchange problem with branch-and-price. <em>MPC</em>, <em>16</em>(1),
151–184. (<a href="https://doi.org/10.1007/s12532-023-00251-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kidney exchange problem (KEP) is an increasingly important healthcare management problem in most European and North American countries which consists of matching incompatible patient-donor pairs in a centralized system. Despite the significant progress in the exact solution of KEP instances in recent years, larger instances still pose a challenge especially when non-directed donors are taken into account. In this article, we present a branch-and-price algorithm for the exact solution of KEP in the presence of non-directed donors. This algorithm is based on a disaggregated cycle and chains formulation where subproblems are managed through graph copies. We additionally present a branch-and-price algorithm based on the position-indexed chain-edge formulation as well as two compact formulations. We formalize and analyze the complexity of the resulting pricing problems and identify the conditions under which they can be solved using polynomial-time algorithms. We propose several algorithmic improvements for the branch-and-price algorithms as well as for the solution of pricing problems. We extensively test all of our implementations using a benchmark made up of different types of instances. Our numerical results show that the proposed algorithm can be significantly faster compared to the state-of-the-art. All models and algorithms presented in the paper are gathered in an open-access Julia package, KidneyExchange.jl.},
  archive      = {J_MPC},
  author       = {Arslan, Ayşe N. and Omer, Jérémy and Yan, Fulin},
  doi          = {10.1007/s12532-023-00251-7},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {151-184},
  shortjournal = {Math. Program. Comput.},
  title        = {KidneyExchange.jl: A julia package for solving the kidney exchange problem with branch-and-price},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adaptive ADMM for semi-strongly convex problems.
<em>MPC</em>, <em>16</em>(1), 113–150. (<a
href="https://doi.org/10.1007/s12532-023-00250-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a self-adaptive ADMM that updates the penalty parameter adaptively. When one part of the objective function is strongly convex i.e., the problem is semi-strongly convex, our algorithm can update the penalty parameter adaptively with guaranteed convergence. We establish various types of convergence results including accelerated convergence rate of $$O(1/k^2),$$ linear convergence and convergence of iteration points. This enhances various previous results because we allow the penalty parameter to change adaptively. We also develop a partial proximal point method with the subproblems being solved by our adaptive ADMM. This enables us to solve problems without semi-strongly convex property. Numerical experiments are conducted to demonstrate the high efficiency and robustness of our method.},
  archive      = {J_MPC},
  author       = {Tang, Tianyun and Toh, Kim-Chuan},
  doi          = {10.1007/s12532-023-00250-8},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {113-150},
  shortjournal = {Math. Program. Comput.},
  title        = {Self-adaptive ADMM for semi-strongly convex problems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CP-lib: Benchmark instances of the clique partitioning
problem. <em>MPC</em>, <em>16</em>(1), 93–111. (<a
href="https://doi.org/10.1007/s12532-023-00249-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Clique Partitioning Problem is a fundamental and much-studied NP-hard combinatorial optimisation problem, with many applications. Several families of benchmark instances have been created in the past, but they are scattered across the literature and hard to find. To remedy this situation, we present CP-Lib, an online resource that contains most of the known instances, plus some challenging new ones.},
  archive      = {J_MPC},
  author       = {Sørensen, Michael M. and Letchford, Adam N.},
  doi          = {10.1007/s12532-023-00249-1},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {93-111},
  shortjournal = {Math. Program. Comput.},
  title        = {CP-lib: Benchmark instances of the clique partitioning problem},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-driven solver (DDS) version 2.1: A MATLAB-based
software package for convex optimization problems in domain-driven form.
<em>MPC</em>, <em>16</em>(1), 37–92. (<a
href="https://doi.org/10.1007/s12532-023-00248-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-Driven Solver (DDS) is a MATLAB-based software package for convex optimization. The current version of DDS accepts every combination of the following function/set constraints: (1) symmetric cones (LP, SOCP, and SDP); (2) quadratic constraints that are SOCP representable; (3) direct sums of an arbitrary collection of 2-dimensional convex sets defined as the epigraphs of univariate convex functions (including as special cases geometric programming and entropy programming); (4) generalized Koecher (power) cone; (5) epigraphs of matrix norms (including as a special case minimization of nuclear norm over a linear subspace); (6) vector relative entropy; (7) epigraphs of quantum entropy and quantum relative entropy; and (8) constraints involving hyperbolic polynomials. The infeasible-start primal-dual algorithms used for DDS rely heavily on duality theory and properties of Legendre-Fenchel conjugate functions, and are designed to rigorously determine the status of a given problem. We discuss some important implementation details and techniques we used to improve the robustness and efficiency of the software. The appendix contains many examples.},
  archive      = {J_MPC},
  author       = {Karimi, Mehdi and Tunçel, Levent},
  doi          = {10.1007/s12532-023-00248-2},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {37-92},
  shortjournal = {Math. Program. Comput.},
  title        = {Domain-driven solver (DDS) version 2.1: A MATLAB-based software package for convex optimization problems in domain-driven form},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware methods for expensive derivative-free
nonsmooth composite optimization. <em>MPC</em>, <em>16</em>(1), 1–36.
(<a href="https://doi.org/10.1007/s12532-023-00245-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new methods for solving a broad class of bound-constrained nonsmooth composite minimization problems. These methods are specially designed for objectives that are some known mapping of outputs from a computationally expensive function. We provide accompanying implementations of these methods: in particular, a novel manifold sampling algorithm (MS-P) with subproblems that are in a sense primal versions of the dual problems solved by previous manifold sampling methods and a method (GOOMBAH) that employs more difficult optimization subproblems. For these two methods, we provide rigorous convergence analysis and guarantees. We demonstrate extensive testing of these methods. Open-source implementations of the methods developed in this manuscript can be found at https://github.com/POptUS/IBCDFO/ .},
  archive      = {J_MPC},
  author       = {Larson, Jeffrey and Menickelly, Matt},
  doi          = {10.1007/s12532-023-00245-5},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Math. Program. Comput.},
  title        = {Structure-aware methods for expensive derivative-free nonsmooth composite optimization},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
