<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDAR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdar---44">IJDAR - 44</h2>
<ul>
<li><details>
<summary>
(2024). Automated systems for diagnosis of dysgraphia in children: A
survey and novel framework. <em>IJDAR</em>, <em>27</em>(4), 707–735. (<a
href="https://doi.org/10.1007/s10032-024-00464-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning disabilities, which primarily interfere with basic learning skills such as reading, writing, and math, are known to affect around 10% of children in the world. The poor motor skills and motor coordination as part of the neurodevelopmental disorder can become a causative factor for the difficulty in learning to write (dysgraphia), hindering the academic track of an individual. The signs and symptoms of dysgraphia include but are not limited to irregular handwriting, improper handling of writing medium, slow or labored writing, unusual hand position, etc. The widely accepted assessment criterion for all types of learning disabilities including dysgraphia has traditionally relied on examinations conducted by medical expert. However, in recent years, artificial intelligence has been employed to develop diagnostic systems for learning disabilities, utilizing diverse modalities of data, including handwriting analysis. This work presents a review of the existing automated dysgraphia diagnosis systems for children in the literature. The main focus of the work is to review artificial intelligence-based systems for dysgraphia diagnosis in children. This work discusses the data collection method, important handwriting features, and machine learning algorithms employed in the literature for the diagnosis of dysgraphia. Apart from that, this article discusses some of the non-artificial intelligence-based automated systems. Furthermore, this article discusses the drawbacks of existing systems and proposes a novel framework for dysgraphia diagnosis and assistance evaluation.},
  archive      = {J_IJDAR},
  author       = {Kunhoth, Jayakanth and Al-Maadeed, Somaya and Kunhoth, Suchithra and Akbari, Younes and Saleh, Moutaz},
  doi          = {10.1007/s10032-024-00464-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {707-735},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Automated systems for diagnosis of dysgraphia in children: A survey and novel framework},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Datasets and annotations for layout analysis of scientific
articles. <em>IJDAR</em>, <em>27</em>(4), 683–705. (<a
href="https://doi.org/10.1007/s10032-024-00461-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a long time now, datasets containing scientific articles have been crucial to the analysis and recognition of document images. These document collections have frequently served as a testing ground for cutting-edge methods for optical character recognition, layout analysis, and document understanding in general. We thoroughly analyze and compare many datasets proposed for layout analysis of scientific documents, ranging from small collections of scanned papers to modern large-scale datasets containing digital-born papers, which have been proposed to train deep learning-based methods. Furthermore, we outline a detailed taxonomy of the annotation procedures used considering manual, automatic, and generative approaches, and we analyze their benefits and drawbacks. This survey is meant to provide the reader with a review of the most used benchmarks together with detailed information on data, annotations, and complexity, helping scholars to identify the most suitable dataset for their tasks of interest. We also discuss possible open problems to further enhance datasets to support research in the layout analysis of scientific articles.},
  archive      = {J_IJDAR},
  author       = {Gemelli, Andrea and Marinai, Simone and Pisaneschi, Lorenzo and Santoni, Francesco},
  doi          = {10.1007/s10032-024-00461-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {683-705},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Datasets and annotations for layout analysis of scientific articles},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compactnet: A lightweight convolutional neural network for
one-shot online signature verification. <em>IJDAR</em>, <em>27</em>(4),
671–682. (<a href="https://doi.org/10.1007/s10032-024-00478-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method for the online signature verification task that allows the signature to be verified effectively using a single enrolled signature sample. The method utilizes a neural network with two one-dimensional convolutional neural network (1D-CNN) components to extract the vector representation of an online signature. The first component is a global 1D-CNN with full-length kernels. The second component is the standard 1D-CNN with partial length kernels that have been successfully used in many time-series classification tasks. The network is trained from a set of online signature samples to extract the vector representation of unknown signatures. The experimental results demonstrated that when using a vector representation derived from the proposed network, a single unseen enrolled signature sample achieved an Equal Error Rate (EER) of 4.35% when tested against authentic signatures of other users. This result indicates the effectiveness of the network in accurately distinguishing between genuine signatures and those of different users.},
  archive      = {J_IJDAR},
  author       = {Sae-Bae, Napa and Chatwattanasiri, Nida and Udomhunsakul, Somkait},
  doi          = {10.1007/s10032-024-00478-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {671-682},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Compactnet: A lightweight convolutional neural network for one-shot online signature verification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Hand-drawn cadastral map parsing, stitching and assembly
via jigsaw puzzles. <em>IJDAR</em>, <em>27</em>(4), 655–669. (<a
href="https://doi.org/10.1007/s10032-024-00465-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a robust method for parsing the content of hand-drawn cadastral maps in order to obtain high-resolution, digitized assemblies of larger regions from individual maps. The parsing phase involves solving a challenging background grid detection plem. We exploit the geometry of detected grids for stitching overlapping map images. A novel method for computing geometric compatibilities between non-overlapping map pieces is also introduced. It is shown to be important since existing chromatic compatibility measures are not as useful for hand-drawn maps. Assembly of maps involves solving an arbitrary-boundary jigsaw puzzle problem with non-overlapping pieces of the same rectangular shape. It corresponds to finding a maximum spanning graph within a multigraph whose edge weights are the piece compatibilities. Since the problem is NP-hard, we develop a polynomial time approximation algorithm that involves two distinct greedy decisions at each iteration. In contrast to existing evaluation metrics for fixed-boundary jigsaw puzzles, we present an $$F_1$$ -score based evaluation scheme for the arbitrary-boundary jigsaw problem that evaluates relative placements of pieces instead of absolute locations. On a testing set of 218 images of 109 cadastral maps comprising 15 different map assembly problems, we achieve a high average $$F_1$$ -score of 0.88. Results validate our compatibility measure as well as the two-stage greedy nature of our method. An ablation study isolates the importance of individual modules of the developed pipeline.},
  archive      = {J_IJDAR},
  author       = {Iftikhar, Tauseef and Khan, Nazar},
  doi          = {10.1007/s10032-024-00465-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {655-669},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Hand-drawn cadastral map parsing, stitching and assembly via jigsaw puzzles},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experimental study of rehearsal-based incremental
classification of document streams. <em>IJDAR</em>, <em>27</em>(4),
629–653. (<a href="https://doi.org/10.1007/s10032-024-00467-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work proposes a novel protocol for rehearsal-based incremental learning models for the classification of business document streams using deep learning and, in particular, transformer-based natural language processing techniques. When implementing a rehearsal-based incremental classification model, the questions raised most often for parameterizing the model relate to the number of instances from “old” classes (learned in previous training iterations) which need to be kept in memory and the optimal number of new classes to be learned at each iteration. In this paper, we propose an incremental learning protocol that involves training incremental models using a weight-sharing strategy between transformer model layers across incremental training iterations. We provide a thorough experimental study that enables us to determine optimal ranges for various parameters in the context of incremental classification of business document streams. We also study the effect of the order in which the classes are presented to the model for learning and the effects of class imbalance on the model’s performances. Our results reveal no significant difference in the performances of our incrementally trained model and its statically trained counterpart after all training iterations (especially when, in the presence of class imbalance, the most represented classes are learned first). In addition, our proposed approach shows an improvement of 1.55% and 3.66% over a baseline model on two business documents dataset. Based on this experimental study, we provide a list of recommendations for researchers and developers for training rehearsal-based incremental classification models for business document streams. Our protocol can be further re-used for other final applications.},
  archive      = {J_IJDAR},
  author       = {Malik, Usman and Visani, Muriel and Sidere, Nicolas and Coustaty, Mickael and Joseph, Aurelie},
  doi          = {10.1007/s10032-024-00467-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {629-653},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Experimental study of rehearsal-based incremental classification of document streams},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deformity removal from handwritten text documents using
variable cycle GAN. <em>IJDAR</em>, <em>27</em>(4), 615–627. (<a
href="https://doi.org/10.1007/s10032-024-00466-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text recognition systems typically work well for printed documents but struggle with handwritten documents due to different writing styles, background complexities, added noise of image acquisition methods, and deformed text images such as strike-offs and underlines. These deformities change the structural information, making it difficult to restore the deformed images while maintaining the structural information and preserving the semantic dependencies of the local pixels. Current adversarial networks are unable to preserve the structural and semantic dependencies as they focus on individual pixel-to-pixel variation and encourage non-meaningful aspects of the images. To address this, we propose a Variable Cycle Generative Adversarial Network (VCGAN) that considers the perceptual quality of the images. By using a variable Content Loss (Top-k Variable Loss ( $$TV_{k}$$ ) ), VCGAN preserves the inter-dependence of spatially close pixels while removing the strike-off strokes. The similarity of the images is computed with $$TV_{k}$$ considering the intensity variations that do not interfere with the semantic structures of the image. Our results show that VCGAN can remove most deformities with an elevated F1 score of $$97.40 \%$$ and outperforms current state-of-the-art algorithms with a character error rate of $$7.64 \%$$ and word accuracy of $$81.53 \%$$ when tested on the handwritten text recognition system},
  archive      = {J_IJDAR},
  author       = {Nigam, Shivangi and Behera, Adarsh Prasad and Verma, Shekhar and Nagabhushan, P.},
  doi          = {10.1007/s10032-024-00466-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {615-627},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deformity removal from handwritten text documents using variable cycle GAN},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Children age group detection based on human–computer
interaction and time series analysis. <em>IJDAR</em>, <em>27</em>(4),
603–613. (<a href="https://doi.org/10.1007/s10032-024-00462-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel children–computer interaction (CCI) approach for the task of age group detection. This approach focuses on the automatic analysis of the time series generated from the interaction of the children with mobile devices. In particular, we extract a set of 25 time series related to spatial, pressure, and kinematic information of the children interaction while colouring a tree through a pen stylus tablet, a specific test from the large-scale public ChildCIdb database. A complete analysis of the proposed approach is carried out using different time series selection techniques to choose the most discriminative ones for the age group detection task: (i) a statistical analysis and (ii) an automatic algorithm called sequential forward search (SFS). In addition, different classification algorithms such as dynamic time warping barycenter averaging (DBA) and hidden Markov models (HMM) are studied. Accuracy results over 85% are achieved, outperforming previous approaches in the literature and in more challenging age group conditions. Finally, the approach presented in this study can benefit many children-related applications, for example, towards an age-appropriate environment with the technology.},
  archive      = {J_IJDAR},
  author       = {Ruiz-Garcia, Juan Carlos and Hojas, Carlos and Tolosana, Ruben and Vera-Rodriguez, Ruben and Morales, Aythami and Fierrez, Julian and Ortega-Garcia, Javier and Herreros-Rodriguez, Jaime},
  doi          = {10.1007/s10032-024-00462-1},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {603-613},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Children age group detection based on human–computer interaction and time series analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An unsupervised automatic organization method for professor
shirakawa’s hand-notated documents of oracle bone inscriptions.
<em>IJDAR</em>, <em>27</em>(4), 583–601. (<a
href="https://doi.org/10.1007/s10032-024-00463-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most influential Chinese cultural researchers in the second half of the twentieth-century, Professor Shirakawa is active in the research field of ancient Chinese characters. He has left behind many valuable research documents, especially his hand-notated oracle bone inscriptions (OBIs) documents. OBIs are one of the world’s oldest characters and were used in the Shang Dynasty about 3600 years ago for divination and recording events. The organization of OBIs is not only helpful in better understanding Prof. Shirakawa’s research and further study of OBIs in general and their importance in ancient Chinese history. This paper proposes an unsupervised automatic organization method to organize Prof. Shirakawa’s OBIs and construct a handwritten OBIs data set for neural network learning. First, a suite of noise reduction is proposed to remove strangely shaped noise to reduce the data loss of OBIs. Secondly, a novel segmentation method based on the supervised classification of OBIs regions is proposed to reduce adverse effects between characters for more accurate OBIs segmentation. Thirdly, a unique unsupervised clustering method is proposed to classify the segmented characters. Finally, all the same characters in the hand-notated OBIs documents are organized together. The evaluation results show that noise reduction has been proposed to remove noises with an accuracy of 97.85%, which contains number information and closed-loop-like edges in the dataset. In addition, the accuracy of supervised classification of OBIs regions based on our model achieves 85.50%, which is higher than eight state-of-the-art deep learning models, and a particular preprocessing method we proposed improves the classification accuracy by nearly 11.50%. The accuracy of OBIs clustering based on supervised classification achieves 74.91%. These results demonstrate the effectiveness of our proposed unsupervised automatic organization of Prof. Shirakawa’s hand-notated OBIs documents. The code and datasets are available at http://www.ihpc.se.ritsumei.ac.jp/obidataset.html .},
  archive      = {J_IJDAR},
  author       = {Yue, Xuebin and Wang, Ziming and Ishibashi, Ryuto and Kaneko, Hayata and Meng, Lin},
  doi          = {10.1007/s10032-024-00463-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {583-601},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {An unsupervised automatic organization method for professor shirakawa’s hand-notated documents of oracle bone inscriptions},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the improvement of handwritten text line recognition with
octave convolutional recurrent neural networks. <em>IJDAR</em>,
<em>27</em>(4), 567–581. (<a
href="https://doi.org/10.1007/s10032-024-00460-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-line handwritten text recognition (HTR) poses a significant challenge due to the complexities of variable handwriting styles, background degradation, and unconstrained word sequences. This work tackles the handwritten text line recognition problem using octave convolutional recurrent neural networks (OctCRNN). Our approach requires no word segmentation, preprocessing, or explicit feature extraction and leverages octave convolutions to process multiscale features without increasing the number of learnable parameters. We investigate the OctCRNN under different settings, including an octave design that efficiently balances computational cost and recognition performance. We thoroughly investigate the OctCRNN under different settings by formulating an experimental pipeline with a visualization step to get intuitions about how the model works compared to a counterpart based on traditional convolutions. The system becomes complete by adding a language model to increase linguistic knowledge. Finally, we assess the performance of our solution using character and word error rates against established handwritten text recognition benchmarks: IAM, RIMES, and ICFHR 2016 READ. According to the results, our proposal achieves state-of-the-art performance while reducing the computational requirements. Our findings suggest that the architecture provides a robust framework for building HTR systems.},
  archive      = {J_IJDAR},
  author       = {Castro, Dayvid and Zanchettin, Cleber and Amaral, Luís A. Nunes},
  doi          = {10.1007/s10032-024-00460-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {567-581},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {On the improvement of handwritten text line recognition with octave convolutional recurrent neural networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training transformer architectures on few annotated data: An
application to historical handwritten text recognition. <em>IJDAR</em>,
<em>27</em>(4), 553–566. (<a
href="https://doi.org/10.1007/s10032-023-00459-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based architectures show excellent results on the task of handwritten text recognition, becoming the standard architecture for modern datasets. However, they require a significant amount of annotated data to achieve competitive results. They typically rely on synthetic data to solve this problem. Historical handwritten text recognition represents a challenging task due to degradations, specific handwritings for which few examples are available and ancient languages that vary over time. These limitations also make it difficult to generate realistic synthetic data. Given sufficient and appropriate data, Transformer-based architectures could alleviate these concerns, thanks to their ability to have a global view of textual images and their language modeling capabilities. In this paper, we propose the use of a lightweight Transformer model to tackle the task of historical handwritten text recognition. To train the architecture, we introduce realistic looking synthetic data reproducing the style of historical handwritings. We present a specific strategy, both for training and prediction, to deal with historical documents, where only a limited amount of training data are available. We evaluate our approach on the ICFHR 2018 READ dataset which is dedicated to handwriting recognition in specific historical documents. The results show that our Transformer-based approach is able to outperform existing methods.},
  archive      = {J_IJDAR},
  author       = {Barrere, Killian and Soullard, Yann and Lemaitre, Aurélie and Coüasnon, Bertrand},
  doi          = {10.1007/s10032-023-00459-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {553-566},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Training transformer architectures on few annotated data: An application to historical handwritten text recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open writer identification from handwritten text fragments
using lite convolutional neural network. <em>IJDAR</em>, <em>27</em>(4),
529–551. (<a href="https://doi.org/10.1007/s10032-023-00458-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Usually, a writer identification system based on the convolutional neural network (CNN) is designed as a closed system, which is composed of many convolutional layers trained often on the entire document for achieving a high performance but requiring a high computation cost. This paper proposes an open writer identification system using a lite CNN composed of only four convolutional layers for extracting features from text fragments. The CNN is trained on a small subset of writers, and then, the resulting model is used for feature generation for new writers, without retraining, associated with the distance-based classifier. The proposed system is simple and easy to deploy for adding new writers without retraining. Extensive experiments performed on text fragments produced from the standard IFN/ENIT and IAM datasets show an encouraging performance against the state of the art of closed systems with an identification rate of 97.08% and 91.00%, respectively, despite few fragments used for writer identification.},
  archive      = {J_IJDAR},
  author       = {Briber, Amina and Chibani, Youcef},
  doi          = {10.1007/s10032-023-00458-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {529-551},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Open writer identification from handwritten text fragments using lite convolutional neural network},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving accuracy and explainability of online handwritten
character recognition. <em>IJDAR</em>, <em>27</em>(4), 515–528. (<a
href="https://doi.org/10.1007/s10032-023-00456-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting recognition technology allows recognizing a written text from a given data. The recognition task can target letters, symbols, or words, and the input data can be a digital image or recorded by various sensors. Over the years, there has been an increasing interest in experimenting with different types of technology to collect handwriting data, create datasets, and develop algorithms to recognize characters and symbols. More recently, the OnHW-chars dataset has been published that contains multivariate time series data of the English alphabet collected using a ballpoint pen fitted with sensors. The authors of OnHW-chars also provided some baseline results through their machine learning (ML) and deep learning (DL) classifiers. In this paper, we develop handwriting recognition models on the OnHW-chars dataset and improve the accuracy of previous models. More specifically, our ML models provide 11.3– $$23.56\%$$ improvements over the previous ML models, and our optimized DL models with ensemble learning provide 3.08– $$7.01\%$$ improvements over the previous DL models. In addition to our accuracy improvements over the spectrum, we aim to provide some level of explainability, using a specialized version of LIME, for our models to provide more logic behind chosen methods and why the models make sense for the data type in the dataset, as well as provide some explanation as to why ensemble methods may lead to an advantage in accuracy rates. In order to verify the robustness of our models trained over the OnHW-chars dataset, we trained our DL models using the same model parameters over a more recently published OnHW-equations dataset. Our DL models with ensemble learning provide 0.05– $$4.75\%$$ improvements over the previous DL models. Our results are verifiable and reproducible via the provided public repository.},
  archive      = {J_IJDAR},
  author       = {Azimi, Hilda and Chang, Steven and Gold, Jonathan and Karabina, Koray},
  doi          = {10.1007/s10032-023-00456-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {515-528},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Improving accuracy and explainability of online handwritten character recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Background grid extraction from historical hand-drawn
cadastral maps. <em>IJDAR</em>, <em>27</em>(4), 501–514. (<a
href="https://doi.org/10.1007/s10032-023-00457-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle a novel problem of detecting background grids in hand-drawn cadastral maps. Grid extraction is necessary for accessing and contextualizing the actual map content. The problem is challenging since the background grid is the bottommost map layer that is severely occluded by subsequent map layers. We present a novel automatic method for robust, bottom-up extraction of background grid structures in historical cadastral maps. The proposed algorithm extracts grid structures under significant occlusion, missing information, and noise by iteratively providing an increasingly refined estimate of the grid structure. The key idea is to exploit periodicity of background grid lines to corroborate the existence of each other. We also present an automatic scheme for determining the ‘gridness’ of any detected grid so that the proposed method self-evaluates its result as being good or poor without using ground truth. We present empirical evidence to show that the proposed gridness measure is a good indicator of quality. On a dataset of 268 historical cadastral maps with resolution $$1424\times 2136$$ pixels, the proposed method detects grids in 247 images yielding an average root-mean-square error (RMSE) of 5.0 pixels and average intersection over union (IoU) of 0.990. On grids self-evaluated as being good, we report average RMSE of 4.39 pixels and average IoU of 0.991. To compare with the proposed bottom-up approach, we also develop three increasingly sophisticated top-down algorithms based on RANSAC-based model fitting. Experimental results show that our bottom-up algorithm yields better results than the top-down algorithms. We also demonstrate that using detected background grids for stitching different maps is visually better than both manual and SURF-based stitching.},
  archive      = {J_IJDAR},
  author       = {Iftikhar, Tauseef and Khan, Nazar},
  doi          = {10.1007/s10032-023-00457-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {501-514},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Background grid extraction from historical hand-drawn cadastral maps},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards privacy preserved document image classification: A
comprehensive benchmark. <em>IJDAR</em>, <em>27</em>(3), 475–499. (<a
href="https://doi.org/10.1007/s10032-024-00469-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data-driven AI systems become increasingly integrated into industry, concerns have recently arisen regarding potential privacy breaches and the inadvertent leakage of sensitive user data through the exploitation of these systems. In this paper, we explore the intersection of data privacy and AI-powered document analysis systems, presenting a comprehensive benchmark of well-known privacy-preserving methods for the task of document image classification. In particular, we investigate four different privacy methods—Differential Privacy (DP), Federated Learning (FL), Differentially Private Federated Learning (DP-FL), and Secure Multi-Party Computation (SMPC)—on two well-known document benchmark datasets, namely RVL-CDIP and Tobacco3482. Furthermore, we investigate the performance of each method under a variety of configurations for thorough benchmarking. Finally, the privacy strength of each approach is assessed by subjecting the private models to well-known membership inference attacks. Our results demonstrate that, with sufficient tuning of hyperparameters, Differential Privacy (DP) can achieve reasonable performance on the task of document image classification while also ensuring rigorous privacy constraints, both in standalone and federated learning setups. On the other hand, while FL-based approaches present less implementation complexity and incur little to no loss in performance on the task, they do not offer sufficient protection against privacy attacks. By rigorously benchmarking various privacy approaches, our study paves the way for integrating deep document classification models into industrial pipelines while meeting regulatory and ethical standards, including GDPR and the AI Act 2022.},
  archive      = {J_IJDAR},
  author       = {Saifullah, Saifullah and Mercier, Dominique and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10032-024-00469-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {475-499},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Towards privacy preserved document image classification: A comprehensive benchmark},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DocXclassifier: Towards a robust and interpretable deep
neural network for document image classification. <em>IJDAR</em>,
<em>27</em>(3), 447–473. (<a
href="https://doi.org/10.1007/s10032-024-00483-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model interpretability and robustness are becoming increasingly critical today for the safe and practical deployment of deep learning (DL) models in industrial settings. As DL-backed automated document processing systems become increasingly common in business workflows, there is a pressing need today to enhance interpretability and robustness for the task of document image classification, an integral component of such systems. Surprisingly, while much research has been devoted to improving the performance of deep models for this task, little attention has been given to their interpretability and robustness. In this paper, we aim to improve upon both aspects and introduce two inherently interpretable deep document classifiers, DocXClassifier and DocXClassifierFPN, both of which not only achieve significant performance improvements over existing approaches but also hold the capability to simultaneously generate feature importance maps while making their predictions. Our approach involves integrating a convolutional neural network (ConvNet) backbone with an attention mechanism to perform weighted aggregation of features based on their importance to the class, enabling the generation of interpretable importance maps. Additionally, we propose integrating Feature Pyramid Networks with the attention mechanism to significantly enhance the resolution of the interpretability maps, especially for pyramidal ConvNet architectures. Our approach attains state-of-the-art performance in image-based classification on two popular document datasets, RVL-CDIP and Tobacco3482, with top-1 classification accuracies of 94.19% and 95.71%, respectively. Additionally, it sets a new record for the highest image-based classification accuracy on Tobacco3482 without transfer learning from RVL-CDIP, at 90.29%. In addition, our proposed training strategy demonstrates superior robustness compared to existing approaches, significantly outperforming them on 19 out of 21 different types of novel data distortions, while achieving comparable results on the remaining two. By combining robustness with interpretability, DocXClassifier presents a promising step toward the practical deployment of DL models for document classification tasks.},
  archive      = {J_IJDAR},
  author       = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10032-024-00483-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {447-473},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {DocXclassifier: Towards a robust and interpretable deep neural network for document image classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Am i readable? Transfer learning based document image
rectification. <em>IJDAR</em>, <em>27</em>(3), 433–446. (<a
href="https://doi.org/10.1007/s10032-024-00476-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document image rectification is a commonly explored problem in computer vision. However, in recent works, the improvements made on a distorted document page are mostly confined to a few specific and limited types of distortions in the document images. Apart from projective and a few other distortions, many other types of distortions are largely ignored in the prior published works. However, some developments have parallelly been made in this area for real-world image rectification (of outdoor scenes). The goodness and strength of such existing real-world rectification models are leveraged in our work to solve the problem of unconstrained document image rectification. However, there are subtle distinctions between the two tasks that prevent the direct use of existing real-world image rectification models for document image rectification. Thus, in this work, we focus on narrowing this gap and propose a novel network called DocTLNet, which is based on transfer learning, for rectifying unconstrained document images from a single input image. The proposed network strikes the right balance between the transfer of knowledge from generic rectification models and facilitates the learning of document-specific features. In addition to the rectification task, our proposed work also focuses on issues such as shading, contrast, and illumination corrections on the document images. While maintaining the color information and contrast on the rectified image, the proposed approach incorporates non-uniform sampling and illumination correction. To the best of our knowledge, this is the first method that uses transfer learning for the rectification of document images. The efficacy of our system is demonstrated by extensive experiments on various datasets such as DIR300 and DocUNet.},
  archive      = {J_IJDAR},
  author       = {Kumari, Pooja and Das, Sukhendu},
  doi          = {10.1007/s10032-024-00476-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {433-446},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Am i readable? transfer learning based document image rectification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generate, transform, and clean: The role of GANs and
transformers in palm leaf manuscript generation and enhancement.
<em>IJDAR</em>, <em>27</em>(3), 415–432. (<a
href="https://doi.org/10.1007/s10032-024-00472-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palm leaf manuscripts offer a rich source of data critical for document analysis tasks, including character, word, and text analysis. However, their cleaning and denoising present substantial challenges due to language diversity, unbalanced datasets, and inherent variations. Existing research often struggles to overcome these unique conditions, necessitating a novel approach. In response, we introduce the Generate, Transform, and Clean (GTC) strategy, which uses two generative adversarial networks (GANs) for data augmentation and enhancement. First, the “Generate&quot; module utilizes an enhanced deep generative adversarial network (DCGAN) with preprocessing techniques to create more realistic backgrounds for various scripts. Second, the “Transform&quot; module employs a multi-task algorithm for text and background transformation, integrating filtering techniques based on palm leaf characteristics. Lastly, the “Clean&quot; module employs a customized transformer-based conditional generative adversarial network (PALM-GAN) for binarization tasks, enhancing learning quality and stability for palm leaf datasets. For our experiments, we evaluate our approach using palm leaf manuscript datasets, focusing specifically on mixed collections from the ICFHR 2016 and 2018 contests. These experiments include various manuscripts, like Balinese and Sundanese from Indonesia, and Khmer from Cambodia. The results underscore not only the superiority of our proposed GTC approach over existing state-of-the-art methods but also highlight its potential to further advance the digitization processes for historical palm leaf documents.},
  archive      = {J_IJDAR},
  author       = {Thuon, Nimol and Du, Jun and Zhang, Zhenrong and Ma, Jiefeng and Hu, Pengfei},
  doi          = {10.1007/s10032-024-00472-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {415-432},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Generate, transform, and clean: The role of GANs and transformers in palm leaf manuscript generation and enhancement},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChemScraper: Leveraging PDF graphics instructions for
molecular diagram parsing. <em>IJDAR</em>, <em>27</em>(3), 395–414. (<a
href="https://doi.org/10.1007/s10032-024-00486-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most molecular diagram parsers recover chemical structure from raster images (e.g., PNGs). However, many PDFs include commands giving explicit locations and shapes for characters, lines, and polygons. We present a new parser that uses these born-digital PDF primitives as input. The parsing model is fast and accurate, and does not require GPUs, Optical Character Recognition (OCR), or vectorization. We use the parser to annotate raster images and then train a new multi-task neural network for recognizing molecules in raster images. We evaluate our parsers using SMILES and standard benchmarks, along with a novel evaluation protocol comparing molecular graphs directly that supports automatic error compilation and reveals errors missed by SMILES-based evaluation. On the synthetic USPTO benchmark, our born-digital parser obtains a recognition rate of 98.4% (1% higher than previous models) and our relatively simple neural parser for raster images obtains a rate of 85% using less training data than existing neural approaches (thousands vs. millions of molecules).},
  archive      = {J_IJDAR},
  author       = {Shah, Ayush Kumar and Amador, Bryan and Dey, Abhisek and Creekmore, Ming and Ocampo, Blake and Denmark, Scott and Zanibbi, Richard},
  doi          = {10.1007/s10032-024-00486-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {395-414},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {ChemScraper: Leveraging PDF graphics instructions for molecular diagram parsing},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified representation framework for the evaluation of
optical music recognition systems. <em>IJDAR</em>, <em>27</em>(3),
379–393. (<a href="https://doi.org/10.1007/s10032-024-00485-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern-day Optical Music Recognition (OMR) is a fairly fragmented field. Most OMR approaches use datasets that are independent and incompatible between each other, making it difficult to both combine them and compare recognition systems built upon them. In this paper we identify the need of a common music representation language and propose the Music Tree Notation format, with the idea to construct a common endpoint for OMR research that allows coordination, reuse of technology and fair evaluation of community efforts. This format represents music as a set of primitives that group together into higher-abstraction nodes, a compromise between the expression of fully graph-based and sequential notation formats. We have also developed a specific set of OMR metrics and a typeset score dataset as a proof of concept of this idea.},
  archive      = {J_IJDAR},
  author       = {Torras, Pau and Biswas, Sanket and Fornés, Alicia},
  doi          = {10.1007/s10032-024-00485-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {379-393},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A unified representation framework for the evaluation of optical music recognition systems},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end semi-supervised approach with modulated object
queries for table detection in documents. <em>IJDAR</em>,
<em>27</em>(3), 363–378. (<a
href="https://doi.org/10.1007/s10032-024-00471-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table detection, a pivotal task in document analysis, aims to precisely recognize and locate tables within document images. Although deep learning has shown remarkable progress in this realm, it typically requires an extensive dataset of labeled data for proficient training. Current CNN-based semi-supervised table detection approaches use the anchor generation process and non-maximum suppression in their detection process, limiting training efficiency. Meanwhile, transformer-based semi-supervised techniques adopted a one-to-one match strategy that provides noisy pseudo-labels, limiting overall efficiency. This study presents an innovative transformer-based semi-supervised table detector. It improves the quality of pseudo-labels through a novel matching strategy combining one-to-one and one-to-many assignment techniques. This approach significantly enhances training efficiency during the early stages, ensuring superior pseudo-labels for further training. Our semi-supervised approach is comprehensively evaluated on benchmark datasets, including PubLayNet, ICADR-19, and TableBank. It achieves new state-of-the-art results, with a mAP of 95.7% and 97.9% on TableBank (word) and PubLaynet with 30% label data, marking a 7.4 and 7.6 point improvement over previous semi-supervised table detection approach, respectively. The results clearly show the superiority of our semi-supervised approach, surpassing all existing state-of-the-art methods by substantial margins. This research represents a significant advancement in semi-supervised table detection methods, offering a more efficient and accurate solution for practical document analysis tasks.},
  archive      = {J_IJDAR},
  author       = {Ehsan, Iqraa and Shehzadi, Tahira and Stricker, Didier and Afzal, Muhammad Zeshan},
  doi          = {10.1007/s10032-024-00471-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {363-378},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {End-to-end semi-supervised approach with modulated object queries for table detection in documents},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Table image dewarping with key element segmentation.
<em>IJDAR</em>, <em>27</em>(3), 349–362. (<a
href="https://doi.org/10.1007/s10032-024-00480-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dewarping is usually an essential step of document digitization, while the inherent semi-structured and sparse nature of tables makes table image dewarping much complex. In this paper, we propose an innovative U-Net based model to dewarp table images. Based on the segmentation of key elements, this model is easy to focus on the table structure, thereby enhancing the ultimate dewarping task. The attention mechanism is also introduced into this model to capture the global distortion, followed by image deblurring to get the final result. In addition, a new dataset containing 12,000 images is constructed for the special task of table image dewarping. The primary experiments demonstrate the proposed model excels in capturing table structure and global distortion, and achieves a 15% improvement in MS-SSIM score.},
  archive      = {J_IJDAR},
  author       = {Zhu, Ziyi and Tang, Zhi and Gao, Liangcai},
  doi          = {10.1007/s10032-024-00480-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {349-362},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Table image dewarping with key element segmentation},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate ground truth generation for semantic labeling of
historical documents with minimal human effort. <em>IJDAR</em>,
<em>27</em>(3), 335–347. (<a
href="https://doi.org/10.1007/s10032-024-00475-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have shown high performance for layout analysis of historical documents, provided that enough labeled data is available. This is not an issue for generic tasks such as image binarization, text graphics separation, or text line and text block detection but can become an impediment for more specialized tasks specific to one or a few books only. This paper addresses layout analysis of medieval books with rich and complex layouts, for which no labeled data is initially available. The proposed strategy consists of training an initial model with artificial data created to reflect the rules a deep neural network should learn. Then, the model is iteratively fine-tuned by mixing the artificial data with real data obtained by previous predictions, post-processed, and manually selected by an expert user. Such a strategy needs less human effort than manual ground truthing. The approach is qualitatively and quantitatively assessed and shows that the system converges to an accurate model that finally produces approximate ground truth stable and good enough to train a final model to solve the targeted task with high accuracy.},
  archive      = {J_IJDAR},
  author       = {Rahal, Najoua and Vögtlin, Lars and Ingold, Rolf},
  doi          = {10.1007/s10032-024-00475-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {335-347},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Approximate ground truth generation for semantic labeling of historical documents with minimal human effort},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SemiDocSeg: Harnessing semi-supervised learning for document
layout analysis. <em>IJDAR</em>, <em>27</em>(3), 317–334. (<a
href="https://doi.org/10.1007/s10032-024-00473-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document Layout Analysis (DLA) is the process of automatically identifying and categorizing the structural components (e.g. Text, Figure, Table, etc.) within a document to extract meaningful content and establish the page’s layout structure. It is a crucial stage in document parsing, contributing to their comprehension. However, traditional DLA approaches often demand a significant volume of labeled training data, and the labor-intensive task of generating high-quality annotated training data poses a substantial challenge. In order to address this challenge, we proposed a semi-supervised setting that aims to perform learning on limited annotated categories by eliminating exhaustive and expensive mask annotations. The proposed setting is expected to be generalizable to novel categories as it learns the underlying positional information through a support set and class information through Co-Occurrence that can be generalized from annotated categories to novel categories. Here, we first extract features from the input image and support set with a shared multi-scale feature acquisition backbone. Then, the extracted feature representation is fed to the transformer encoder as a query. Later on, we utilize a semantic embedding network before the decoder to capture the underlying semantic relationships and similarities between different instances, enabling the model to make accurate predictions or classifications with only a limited amount of labeled data. Extensive experimentation on competitive benchmarks like PRIMA, DocLayNet, and Historical Japanese (HJ) demonstrate that this generalized setup obtains significant performance compared to the conventional supervised approach.},
  archive      = {J_IJDAR},
  author       = {Banerjee, Ayan and Biswas, Sanket and Lladós, Josep and Pal, Umapada},
  doi          = {10.1007/s10032-024-00473-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {317-334},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {SemiDocSeg: Harnessing semi-supervised learning for document layout analysis},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Parstr: Partially autoregressive scene text recognition.
<em>IJDAR</em>, <em>27</em>(3), 303–316. (<a
href="https://doi.org/10.1007/s10032-024-00470-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An autoregressive (AR) decoder for scene text recognition (STR) requires numerous generation steps to decode a text image character by character but can yield high recognition accuracy. On the other hand, a non-autoregressive (NAR) decoder generates all characters in a single generation but suffers from a loss of recognition accuracy. This is because, unlike the former, the latter assumes that the predicted characters are conditionally independent. This paper presents a Partially Autoregressive Scene Text Recognition (PARSTR) method that unifies both AR and NAR decoding within the same model. To reduce decoding steps while maintaining recognition accuracy, we devise two decoding strategies: b-first and b-ahead, reducing the decoding steps to approximately b and by a factor of b, respectively. The experimental results demonstrate that our PARSTR models using the devised decoding strategies present a balanced compromise between efficiency and recognition accuracy compared to the fully AR and NAR decoding approaches. Specifically, the experimental results on public benchmark STR datasets demonstrate the potential to reduce decoding steps down to at most five steps and by a factor of five under the b-first and b-ahead decoding schemes, respectively, while having a slight reduction of total word recognition accuracy of less than or equal to 0.5%.},
  archive      = {J_IJDAR},
  author       = {Buoy, Rina and Iwamura, Masakazu and Srun, Sovila and Kise, Koichi},
  doi          = {10.1007/s10032-024-00470-1},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {303-316},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Parstr: Partially autoregressive scene text recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Towards reduced-complexity scene text recognition (RCSTR)
through a novel salient feature selection. <em>IJDAR</em>,
<em>27</em>(3), 289–302. (<a
href="https://doi.org/10.1007/s10032-024-00474-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of an attention mechanism has played a crucial role in many recent scene text recognition (STR) methods. It enables the capture of spatial feature dependencies (known as self-attention) and the identification of relevant features while predicting a character (known as cross-attention). However, computations and memory requirements in the self-attention and cross-attention layers increase quadratically and linearly with the feature map size, respectively, leading to a computational bottleneck in low-resource environments. But, is it necessary to attend to the entire feature maps? On the other hand, text in a natural scene is continuous and oriented in a specific direction, and it does not occupy the entire image. Therefore, utilizing only a small salient subset of features in text regions is sufficient for accurately predicting characters. Based on this salient feature selection, we propose a reduced-complexity scene text recognition framework that significantly reduces model complexities and memory requirements in the self-attention and cross-attention layers. We validate the proposed framework by employing a convolutional STR architecture with both connectionist temporal classification and transformer decoders. Through the model complexity and performance analyses on public benchmark datasets, we demonstrate that the proposed method can substantially reduce model complexities while still maintaining reasonably robust recognition accuracy.},
  archive      = {J_IJDAR},
  author       = {Buoy, Rina and Iwamura, Masakazu and Srun, Sovila and Kise, Koichi},
  doi          = {10.1007/s10032-024-00474-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {289-302},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Towards reduced-complexity scene text recognition (RCSTR) through a novel salient feature selection},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CHWmaster: Mastering chinese handwriting via sliding-window
recurrent neural networks. <em>IJDAR</em>, <em>27</em>(3), 275–288. (<a
href="https://doi.org/10.1007/s10032-024-00468-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowering machines to own the capability of writing texts as human beings has been a long-standing goal in the community. The task is challenging due to the complexity of human handwriting behaviors when writing long text lines or articles, especially for some writing systems (e.g., Chinese) that contain vast amounts of glyphs with complicated geometric appearances and structures. In this paper, we propose a novel system, CHWMaster, to mimic the behavior of an arbitrary user to write Chinese articles in the user’s personalized handwriting style. The key idea of our proposed method is to design a new model based on deep Recurrent Neural Networks with sliding windows. Both qualitative and quantitative experiments have been conducted to demonstrate that with the help of our proposed CHWMaster, for the first time, robots are able to write Chinese articles like humans, getting high-fidelity Chinese handwritten texts that are indistinguishable from real ones by simply feeding one/a few human-written text lines to fine-tune our pre-trained model.},
  archive      = {J_IJDAR},
  author       = {Li, Shilong and Tang, Shusen and Lian, Zhouhui},
  doi          = {10.1007/s10032-024-00468-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {275-288},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {CHWmaster: Mastering chinese handwriting via sliding-window recurrent neural networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating learned feature aggregators for writer retrieval.
<em>IJDAR</em>, <em>27</em>(3), 265–274. (<a
href="https://doi.org/10.1007/s10032-024-00482-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have emerged as the leading methods in natural language processing, computer vision, and multi-modal applications due to their ability to capture complex relationships and dependencies in data. In this study, we explore the potential of transformers as feature aggregators in the context of patch-based writer retrieval, with the objective of improving the quality of writer retrieval by effectively summarizing the relevant features from image patches. Our investigation underscores the complexity of leveraging transformers as feature aggregators in patch-based writer retrieval. While we have experimented with various model configurations, augmentations, and learning objectives, the performance of transformers in this task has room for improvement. This observation highlights the challenges in this domain and emphasizes the need for further research to enhance their effectiveness. By shedding light on the limitations of transformers in this context, our study contributes to the growing body of knowledge in the field of writer retrieval and provides valuable insights for future research and development in this area.},
  archive      = {J_IJDAR},
  author       = {Mattick, Alexander and Mayr, Martin and Seuret, Mathias and Kordon, Florian and Wu, Fei and Christlein, Vincent},
  doi          = {10.1007/s10032-024-00482-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {265-274},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Evaluating learned feature aggregators for writer retrieval},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural models for semantic analysis of handwritten document
images. <em>IJDAR</em>, <em>27</em>(3), 245–263. (<a
href="https://doi.org/10.1007/s10032-024-00477-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic analysis of handwritten document images offers a wide range of practical application scenarios. A sequential combination of handwritten text recognition (HTR) and a task-specific natural language processing system offers an intuitive solution in this domain. However, this HTR-based approach suffers from the problem of error propagation. An HTR-free model, which avoids explicit text recognition and solves the task end-to-end, tackles this problem, but often produces poor results. A possible reason for this is that it does not incorporate largely pre-trained semantic word embeddings, which turn out to be one of the most powerful advantages in the textual domain. In this work, we propose an HTR-based and an HTR-free model and compare them on a variety of segmentation-based handwritten document image benchmarks including semantic word spotting, named entity recognition, and question answering. Furthermore, we propose a cross-modal knowledge distillation approach to integrate semantic knowledge from textually pre-trained word embeddings into HTR-free models. In a series of experiments, we investigate optimization strategies for robust semantic word image representation. We show that the incorporation of semantic knowledge is beneficial for HTR-free approaches in achieving state-of-the-art results on a variety of benchmarks.},
  archive      = {J_IJDAR},
  author       = {Tüselmann, Oliver and Fink, Gernot A.},
  doi          = {10.1007/s10032-024-00477-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {245-263},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Neural models for semantic analysis of handwritten document images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-training for handwritten word recognition and
retrieval. <em>IJDAR</em>, <em>27</em>(3), 225–244. (<a
href="https://doi.org/10.1007/s10032-024-00484-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten text recognition and Word Retrieval, also known as Word Spotting, are traditional problems in the document analysis community. While the use of increasingly large neural network architectures has led to a steady improvement of performances it comes with the drawback of requiring manually annotated training data. This poses a tremendous problem considering their application to new document collections. To overcome this drawback, we propose a self-training approach that allows to train state-of-the-art models for HTR and word spotting. Self-training is a common technique in semi-supervised learning and usually relies on a small labeled dataset and training on pseudo-labels generated by an initial model. In this work, we show that it is feasible to train models on synthetic data that are sufficiently performant to serve as initial models for self-training. Therefore, the proposed training method does not rely on any manually annotated samples. We further investigate visual and language properties of the synthetic datasets. In order to improve performance and robustness of the self-training approach, we propose different confidence measures for both models that allow to identify and remove erroneous pseudo-labels. The presented training approach clearly outperforms other learning-free methods or adaptation strategies under the absence of manually annotated data.},
  archive      = {J_IJDAR},
  author       = {Wolf, Fabian and Fink, Gernot A.},
  doi          = {10.1007/s10032-024-00484-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {225-244},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Self-training for handwritten word recognition and retrieval},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring recursive neural networks for compact handwritten
text recognition models. <em>IJDAR</em>, <em>27</em>(3), 213–223. (<a
href="https://doi.org/10.1007/s10032-024-00481-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of deploying recognition models in specific scenarios in which memory size is relevant, such as in low-cost devices or browser-based applications. We specifically focus on developing memory-efficient approaches for Handwritten Text Recognition (HTR) by leveraging recursive networks. These networks reuse learned weights across successive layers, thus enabling the maintenance of depth, a critical factor associated with model accuracy, without an increase in memory footprint. We apply neural recursion techniques to models typically used in HTR that contain convolutional and recurrent layers. We additionally study the impact of kernel scaling, which allows the activations of these recursive layers to be modified for greater expressiveness with little cost to memory. Our experiments on various HTR benchmarks demonstrate that recursive networks are, indeed, a good alternative. It is noteworthy that these recursive networks not only preserve but in some instances also enhance accuracy, making them a promising solution for memory-efficient HTR applications. This research establishes the utility of recursive networks in addressing memory constraints in HTR models. Their ability to sustain or improve accuracy while being memory-efficient positions them as a promising solution for practical deployment, especially in contexts where memory size is a critical consideration, such as low-cost devices and browser-based applications.},
  archive      = {J_IJDAR},
  author       = {Mas-Candela, Enrique and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s10032-024-00481-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {213-223},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Exploring recursive neural networks for compact handwritten text recognition models},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial for special issue on “advanced topics in document
analysis and recognition.” <em>IJDAR</em>, <em>27</em>(3), 209–211. (<a
href="https://doi.org/10.1007/s10032-024-00494-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Barney Smith, Elisa H. and Liwicki, Marcus and Peng, Liangrui and Marinai, Simone},
  doi          = {10.1007/s10032-024-00494-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {209-211},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Editorial for special issue on “advanced topics in document analysis and recognition”},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based multiple siamese networks with primary
representation guiding for offline signature verification.
<em>IJDAR</em>, <em>27</em>(2), 195–208. (<a
href="https://doi.org/10.1007/s10032-023-00455-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of biometrics and document forensics, handwritten signatures are one of the most commonly accepted symbols. Thus, financial and commercial institutions usually use them to verify the identity of an individual. However, offline signature verification is still a challenging task due to the difficulties in discriminating the minute but significant details between genuine and skilled forged signatures. To tackle this issue, we propose a novel writer-independent offline signature verification approach using attention-based multiple siamese networks with primary representation guiding. The proposed multiple siamese networks regard the reference signature images, query signature images, and their corresponding inverse images as inputs. These images are fed to four weight-shared parallel branches, respectively. We present an efficient and reliable mutual attention module to discover prominent stroke information from both original and inverse branches. In each branch, feature maps of the first convolution are utilized to guide the combination with deeper features, named as primary representation guiding, which guides the model into concerning the shallow stroke information. The four branches are concatenated in an ordered way and are put into four contrastive pairs, which is helpful to obtain useful representations by comparing reference and query samples. Four contrastive pairs generate four preliminary decisions independently. Then, the eventual verification result is created based on the four preliminary decisions using a voting mechanism. In order to assess the performance of the proposed method, extensive experiments on four widely used public datasets are conducted. The experimental results demonstrate that the proposed method outperforms existing approaches in most cases and can be applied to various language scenarios.},
  archive      = {J_IJDAR},
  author       = {Xiong, Yu-Jie and Cheng, Song-Yang and Ren, Jian-Xin and Zhang, Yu-Jin},
  doi          = {10.1007/s10032-023-00455-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {195-208},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Attention-based multiple siamese networks with primary representation guiding for offline signature verification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SoftCTC—semi-supervised learning for text recognition using
soft pseudo-labels. <em>IJDAR</em>, <em>27</em>(2), 177–193. (<a
href="https://doi.org/10.1007/s10032-023-00452-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores semi-supervised training for sequence tasks, such as optical character recognition or automatic speech recognition. We propose a novel loss function—SoftCTC—which is an extension of CTC allowing to consider multiple transcription variants at the same time. This allows to omit the confidence-based filtering step which is otherwise a crucial component of pseudo-labeling approaches to semi-supervised learning. We demonstrate the effectiveness of our method on a challenging handwriting recognition task and conclude that SoftCTC matches the performance of a finely tuned filtering-based pipeline. We also evaluated SoftCTC in terms of computational efficiency, concluding that it is significantly more efficient than a naïve CTC-based approach for training on multiple transcription variants, and we make our GPU implementation public.},
  archive      = {J_IJDAR},
  author       = {Kišš, Martin and Hradiš, Michal and Beneš, Karel and Buchal, Petr and Kula, Michal},
  doi          = {10.1007/s10032-023-00452-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {177-193},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {SoftCTC—semi-supervised learning for text recognition using soft pseudo-labels},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptual cue-guided adaptive image downscaling for
enhanced semantic segmentation on large document images. <em>IJDAR</em>,
<em>27</em>(2), 159–175. (<a
href="https://doi.org/10.1007/s10032-023-00454-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image downscaling is an essential operation to reduce spatial complexity for various applications and is becoming increasingly important due to the growing number of solutions that rely on memory-intensive approaches, such as applying deep convolutional neural networks to semantic segmentation tasks on large images. Although conventional content-independent image downscaling can efficiently reduce complexity, it is vulnerable to losing perceptual details, which are important to preserve. Alternatively, existing content-aware downscaling severely distorts spatial structure and is not effectively applicable for segmentation tasks involving document images. In this paper, we propose a novel image downscaling approach that combines the strengths of both content-independent and content-aware strategies. The approach limits the sampling space per the content-independent strategy, adaptively relocating such sampled pixel points, and amplifying their intensities based on the local gradient and texture via the content-aware strategy. To demonstrate its effectiveness, we plug our adaptive downscaling method into a deep learning-based document image segmentation pipeline and evaluate the performance improvement. We perform the evaluation on three publicly available historical newspaper digital collections with differences in quality and quantity, comparing our method with one widely used downscaling method, Lanczos. We further demonstrate the robustness of the proposed method by using three different training scenarios: stand-alone, image-pyramid, and augmentation. The results show that training a deep convolutional neural network using images generated by the proposed method outperforms Lanczos, which relies on only content-independent strategies.},
  archive      = {J_IJDAR},
  author       = {Pack, Chulwoo and Soh, Leen-Kiat and Lorang, Elizabeth},
  doi          = {10.1007/s10032-023-00454-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {159-175},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Perceptual cue-guided adaptive image downscaling for enhanced semantic segmentation on large document images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline handwritten mathematical recognition using
adversarial learning and transformers. <em>IJDAR</em>, <em>27</em>(2),
147–158. (<a href="https://doi.org/10.1007/s10032-023-00451-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline handwritten mathematical expression recognition (HMER) is a significant area in mathematical expression recognition. However, offline HMER is often viewed as a much more complex problem than online HMER due to a lack of temporal information and variability in writing style. This paper proposes an encoder–decoder model that uses paired adversarial learning. Semantic-invariant features are extracted from handwritten mathematical expression images and their printed mathematical expression counterpart in the encoder. Learning semantic-invariant features combined with the DenseNet encoder and transformer decoder helped us to improve the expression rate from previous studies. Evaluated on the CROHME dataset, we have improved the latest CROHME 2019 test set results by 4% approx.},
  archive      = {J_IJDAR},
  author       = {Thakur, Ujjwal and Sharma, Anuj},
  doi          = {10.1007/s10032-023-00451-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {147-158},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Offline handwritten mathematical recognition using adversarial learning and transformers},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TableStrRec: Framework for table structure recognition in
data sheet images. <em>IJDAR</em>, <em>27</em>(2), 127–145. (<a
href="https://doi.org/10.1007/s10032-023-00453-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Billions of documents in data sheet format are shared between various organizations across the globe on a daily basis. The essential information in these documents is presented in tabular format. Extracting and assimilating this information can help organizations make data-driven decisions. Solutions for detecting tables in document images have been well explored. Thus, in this work, we propose TableStrRec, a deep learning-based approach to recognize the structure of such detected tables by detecting rows and columns. TableStrRec comprises two Cascade R-CNN architectures, each with a deformable backbone and Complete IOU loss to improve their detection performance. One architecture detects and classifies rows as regular rows (rows without a merged cell) and irregular rows (groups of regular rows that share a merged cell). The second architecture detects and classifies columns as regular columns (columns without a merged cell) and irregular columns (groups of regular columns that share a merged cell). Both architectures work in parallel to provide the results in a single inference. We show that utilizing TableStrRec to detect four classes of objects improves the table structure recognition performance on three public test sets. We achieve $$90.5\%$$ and $$89.6\%$$ weighted average F1 scores on the ICDAR2013 test set for rows and columns, respectively. On the TabStructDB test set, we achieve $$72.7\%$$ and $$78.5\%$$ weighted average F1 score for rows and columns, respectively. We also evaluate the proposed method under the FinTabNet dataset using the structure-only TEDS score, achieving 98.34%, which can outperform most state-of-the-art benchmark models.},
  archive      = {J_IJDAR},
  author       = {Fernandes, Johan and Xiao, Bin and Simsek, Murat and Kantarci, Burak and Khan, Shahzad and Alkheir, Ala Abu},
  doi          = {10.1007/s10032-023-00453-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {127-145},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {TableStrRec: Framework for table structure recognition in data sheet images},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The image and ground truth dataset of mongolian movable-type
newspapers for text recognition. <em>IJDAR</em>, <em>27</em>(2),
113–125. (<a href="https://doi.org/10.1007/s10032-023-00450-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OCR approaches have been widely advanced in recent years thanks to the resurgence of deep learning. However, to the best of our knowledge, there is little work on Mongolian movable-type document recognition. One major hurdle is the lack of a domain-specific well-labeled set for training robust models. This paper aims to create the first Mongolian movable type text-image dataset for OCR research. We collated 771 paragraph-level pages segmented from 34 newspapers from 1947 to 1952. For each page, word- and line-level text transcriptions and boundary annotations are recorded. It consists of 86,578 word appearances and 9711 text-line images in total. The vocabulary is 7964. The dataset was finally established from scratch through image collection, text transcription, text-image alignment and manual correction. Moreover, an official train and test set partition is defined on which the typical text segmentation and recognition experiments are tested to set the strong baselines. This dataset is available for research, and we encourage researchers to develop and test new methods using our dataset.},
  archive      = {J_IJDAR},
  author       = {Lu, Min and Bao, Feilong and Zhang, Hui and Gao, Guanglai},
  doi          = {10.1007/s10032-023-00450-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {113-125},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {The image and ground truth dataset of mongolian movable-type newspapers for text recognition},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: MRZ code extraction from visa and passport
documents using convolutional neural networks. <em>IJDAR</em>,
<em>27</em>(1), 111. (<a
href="https://doi.org/10.1007/s10032-023-00445-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Liu, Yichuan and Joren, Hailey and Gupta, Otkrist and Raviv, Dan},
  doi          = {10.1007/s10032-023-00445-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {111},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Correction to: MRZ code extraction from visa and passport documents using convolutional neural networks},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based solution for digitization of invoice
images with automatic invoice generation and labelling. <em>IJDAR</em>,
<em>27</em>(1), 97–109. (<a
href="https://doi.org/10.1007/s10032-023-00449-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the level of invoice traffic between companies has reached enormous levels. Invoices are crucial financial documents for companies, and they need to extract this information from these documents to access and control them quickly when necessary. While electronic invoices can be easily transferred to the company’s ERP system with the help of integrators, information from printed invoices must be entered into the ERP system. Information entry is generally performed manually by company employees, so the probability of error is high. The automatic recognition of information in printed invoices will reduce the possibility of error. It will also save time and money by reducing workforce requirements. This study proposes a deep learning-based solution for detecting fields in image invoices that are in high demand among businesses. The system offers an end-to-end solution, which includes a novel method for generating synthetic invoices and automatic labeling. Three invoice templates were used to evaluate the usability of the system and an adaptive fine-tuning-based solution is proposed for newly coming invoice templates. Furthermore, 6 different object detection models were compared to find the most suitable one for our problem. The system was also tested with 1022 real invoice images that were manually labeled to test real-world usage. The results indicated that the fine-tuned model achieved an accuracy that was 8.4% higher than the baseline models. In tests performed on CPU, TOOD and Cascade-RCNN models were the most successful algorithms, while YOLOv5 was the fastest running algorithm. Depending on the priority of the needs, both algorithms can be preferred for real-time usage in the detection of invoice fields. The synthetic invoice generation code is available at https://github.com/SCU-CENG/Invoice-Generation .},
  archive      = {J_IJDAR},
  author       = {Arslan, Halil and Işık, Yunus Emre and Görmez, Yasin},
  doi          = {10.1007/s10032-023-00449-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {97-109},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A deep learning-based solution for digitization of invoice images with automatic invoice generation and labelling},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multifaceted evaluation of representation of graphemes for
practically effective bangla OCR. <em>IJDAR</em>, <em>27</em>(1), 73–95.
(<a href="https://doi.org/10.1007/s10032-023-00446-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bangla Optical Character Recognition (OCR) poses a unique challenge due to the presence of hundreds of diverse conjunct characters formed by the combination of two or more letters. In this paper, we propose two novel grapheme representation methods that improve the recognition of these conjunct characters and the overall performance of OCR in Bangla. We have utilized the popular Convolutional Recurrent Neural Network architecture and implemented our grapheme representation strategies to design the final labels of the model. Due to the absence of a large-scale Bangla word-level printed dataset, we created a synthetically generated Bangla corpus containing 2 million samples that are representative and sufficiently varied in terms of fonts, domain, and vocabulary size to train our Bangla OCR model. To test the various aspects of our model, we have also created 6 test protocols. Finally, to establish the generalizability of our grapheme representation methods, we have performed training and testing on external handwriting datasets. Experimental results proved the effectiveness of our novel approach. Furthermore, our synthetically generated training dataset and the test protocols are made available to serve as benchmarks for future Bangla OCR research.},
  archive      = {J_IJDAR},
  author       = {Roy, Koushik and Hossain, Md Sazzad and Saha, Pritom Kumar and Rohan, Shadman and Ashrafi, Imranul and Rezwan, Ifty Mohammad and Rahman, Fuad and Hossain, B. M. Mainul and Kabir, Ahmedul and Mohammed, Nabeel},
  doi          = {10.1007/s10032-023-00446-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {73-95},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A multifaceted evaluation of representation of graphemes for practically effective bangla OCR},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attribute-based document image retrieval. <em>IJDAR</em>,
<em>27</em>(1), 57–71. (<a
href="https://doi.org/10.1007/s10032-023-00447-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the use of attributes for document image querying and retrieval. Existing document image retrieval techniques present several drawbacks: textual searches are limited to text, query-by-example searches require a sample query document on hand, and layout-based searches rigidly assign documents to one of several preset classes. Attributes have yet to be fully exploited in document image analysis. We describe document images based on attributes and utilize those descriptions to form a new querying paradigm for document image retrieval that addresses the above limitations: attribute-based document image retrieval (ABDIR). We create attribute-based descriptions of the documents using an expandable set of individual, independent attribute classifiers built on convolutional neural network architectures. We combine the descriptions to form queries of variable complexity which retrieve a ranked list of document images. ABDIR allows users to search for documents based on memorable visual features of their contents in a flexible way, with queries like “Find documents that have a one-column layout, are table dominant, and are colorful”, or “Find historical documents that are illuminated and have see-through artifacts”. Experiments on the recent PubLayNet and HisIR19 datasets demonstrate the system’s ability to extract various document image attributes with high accuracy, with Darknet-53 performing best, and show very promising results for document image retrieval. ABDIR is scalable and versatile: it is easy to change, add, and remove attributes, and easy to adapt queries to new domains. It provides for document image retrieval capabilities that are not possible or are impractical with other paradigms.},
  archive      = {J_IJDAR},
  author       = {Cote, Melissa and Branzan Albu, Alexandra},
  doi          = {10.1007/s10032-023-00447-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {57-71},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Attribute-based document image retrieval},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chinese text recognition enhanced by glyph and character
semantic information. <em>IJDAR</em>, <em>27</em>(1), 45–56. (<a
href="https://doi.org/10.1007/s10032-023-00444-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese text line recognition technology has been applied in a variety of scenarios. As a kind of ideographic writing, Chinese characters contain plenty of semantic information and basic components. While previous methods mainly convert each Chinese character into a discrete label to facilitate the calculation of cross-entropy loss, leaving the fine-grained glyph information (e.g. strokes and radicals) and semantic information unexploited. Concretely, glyph information is crucial for recognizing Chinese characters with similar appearances, as these characters differ only slightly in local strokes. The glyph information reflects these differences guiding the model to learn fine-grained local features. And compared to discrete category labels, the character semantic information introduces diverse visual concepts, which enriches the final character representation. This paper presents a Chinese text recognition method that exploits glyph and character semantic information to acquire effective text representations. Specifically, we propose a Glyph-Aware Decoder to identify characters by dynamically fusing the global visual features with the local stroke and radical features. And we introduce a Contrastive Visual–Textual Learning module to enhance the visual features of Chinese characters by their semantic information. Experiments show that our proposed model achieves state-of-the-art results on the Chinese text recognition benchmarks.},
  archive      = {J_IJDAR},
  author       = {Wu, Shilian and Li, Yongrui and Wang, Zengfu},
  doi          = {10.1007/s10032-023-00444-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {45-56},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Chinese text recognition enhanced by glyph and character semantic information},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chart classification: A survey and benchmarking of different
state-of-the-art methods. <em>IJDAR</em>, <em>27</em>(1), 19–44. (<a
href="https://doi.org/10.1007/s10032-023-00443-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in the number of documents with various types of charts available on the internet, automatic chart classification has become an essential task for various downstream applications such as chart data recovery, chart replenishment. This paper presents a comprehensive survey of the studies reported in the literature since 2001 from the perspective of the corpus, pre-processing techniques, feature extraction, and methodologies. Considering that the majority of the existing studies use small datasets with a smaller number of chart types and also reported varying performances, this paper implements and evaluates 44 different machine learning-based chart classification models. The evaluation is done over a large dataset curated locally and benchmarks the performances of these 44 different models over a common experimental framework. It also performs a comprehensive error analysis, identifying two core challenging issues (noise in the charts and confusing chart pairs) that affect the chart classification performances. Compared with the existing survey papers, this paper presents a more comprehensive review and experimental analysis.},
  archive      = {J_IJDAR},
  author       = {Thiyam, Jennil and Singh, Sanasam Ranbir and Bora, Prabin Kumar},
  doi          = {10.1007/s10032-023-00443-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {19-44},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Chart classification: A survey and benchmarking of different state-of-the-art methods},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BPFormNet: A lightweight block pyramid network for form
segmentation and classification. <em>IJDAR</em>, <em>27</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s10032-023-00440-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business forms with the dense text boxes have a complicated layout, diverse content, and low quality. It is a challenging task for the existing methods of form understanding to recognize form structure and to meet the requirements in real-time application scenarios. In this paper, we propose a novel multi-task lightweight block pyramid network for form segmentation and classification, named BPFormNet. According to the characteristics of the form images, we exploit the multi-scale pyramidal feature hierarchy of CNN (Convolutional Neural Network) to construct a multi-level, multi-scale block pyramid, which consists of the low-level, mid-level, and high-level convolutional blocks designed for the corresponding feature layer, and builds the semantic feature maps of multi-scale effective fusion at every level. BPFromNet leverages the interdependence between the twin task of segmentation task of form frames and classification task to improve the performance of the classification under the training strategy of small samples. Furthermore, BPFormNet performs comprehensive lightweighting from three levels: multi-level, multi-scale convolutional block combination, multi-size kernel combination, and disassembly of kernels. Experimental results on the collected image dataset of Chinese insurance forms (CIF) show that BPFormNet with the block pyramid has a strong capability of form feature representation. Comparing with some state-of-the-art (SOTA) lightweight models and their combinations, BPFormNet achieves a better performance in both segmentation and classification task than the models of single block, significantly reduces the model complexity while maintaining the model accuracy, and provides the real-time, high-quality results of form structure recognition for the downstream task of text recognition and information extraction.},
  archive      = {J_IJDAR},
  author       = {Lin, Hanyang and Zhan, Yongzhao and Wu, Chongshu},
  doi          = {10.1007/s10032-023-00440-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {BPFormNet: A lightweight block pyramid network for form segmentation and classification},
  volume       = {27},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
