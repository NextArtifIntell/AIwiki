<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---192">JRTIP - 192</h2>
<ul>
<li><details>
<summary>
(2024). Robust block-based watermarking algorithm with
parallelization using multi-level discrete wavelet transformation.
<em>JRTIP</em>, <em>21</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01559-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital era, the need to safeguard the authenticity and ownership of media content shared online via social networking sites and apps has become increasingly crucial. Digital watermarking emerges as a viable solution to address this need by providing authentication and ownership verification of digital media. Various watermarking techniques have been proposed in spatial and transformation domains, with ongoing efforts to enhance robustness against common watermarking attacks, such as filtering, noise, and transformations. In this paper, we introduce a novel block-based parallelized watermarking method. Our proposed approach hides binary watermarks within grayscale images by employing multi-level discrete wavelet transformation on image blocks in parallel. A pixel scramble mechanism is also employed to strengthen security measures. The effectiveness of the proposed method is evaluated using three quality metrics: structural similarity index measure and peak signal-to-noise ratio to assess the imperceptibility of watermarked images and normalized correlation to evaluate the robustness of our approach against standard watermarking attacks. Experimental results show that our method achieves high PSNR values of up to 48.63 dB and SSIM values above 0.99, indicating excellent imperceptibility. Additionally, NC values close to 1.0 across various attack scenarios demonstrate superior robustness compared to existing methods.},
  archive      = {J_JRTIP},
  author       = {Yadav, Akash and Goyal, Jitendra and Ahmed, Mushtaq},
  doi          = {10.1007/s11554-024-01559-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Robust block-based watermarking algorithm with parallelization using multi-level discrete wavelet transformation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal feature guided adaptive rate control for
screen content videos. <em>JRTIP</em>, <em>21</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01560-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an adaptive rate control algorithm based on screen content characteristics is proposed. It enhances the coding performance of HEVC Extensions on Screen Content Coding (HEVC-SCC) by leveraging the unique characteristics of screen content videos (SCVs). Firstly, considering the characteristics of the human visual system (HVS) and the particular spatial–temporal characteristics of SCVs, the spatial feature (SF) and temporal feature (TF) factors are extracted by Gabor filter. Then, a spatial–temporal fused feature of SVC is modeled an adaptive $$R-\lambda$$ model to guide the rate control of HEVC-SCC. More bitrates are allocated to coding tree units (CTUs) that have higher vision sensitivity, and vice versa. The experimental results demonstrate the effectiveness of the proposed STF_RC. It achieves a quality improvement of 1.70 dB in Low Delay B (LDB) and 2.55 dB in Random Access (RA) configurations. Additionally, it reduces bitrate by 21.75% in LDB and 25.42% in RA compared to the standard platform. Compared to state-of-the-art rate control algorithms for HEVC-SCC, it reduces bitrate mismatch significantly and achieves better performance in both rate-distortion and subjective quality.},
  archive      = {J_JRTIP},
  author       = {Lin, Qi and Chen, Jing},
  doi          = {10.1007/s11554-024-01560-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Spatial-temporal feature guided adaptive rate control for screen content videos},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A small object detection architecture with concatenated
detection heads and multi-head mixed self-attention mechanism.
<em>JRTIP</em>, <em>21</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01562-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel detection method is proposed to address the challenge of detecting small objects in object detection. This method augments the YOLOv8n architecture with a small object detection layer and innovatively designs a Concat-detection head to effectively extract features. Simultaneously, a new attention mechanism—Multi-Head Mixed Self-Attention (MMSA) mechanism—is introduced to enhance the feature-extraction capability of the backbone. To improve the detection sensitivity for small objects, a combination of Normalized Wasserstein Distance (NWD) and Intersection over Union (IoU) is used to calculate the localization loss, optimizing the bounding-box regression. Experimental results on the TT100K dataset show that the mean average precision (mAP@0.5) reaches 88.1%, which is a 13.5% improvement over YOLOv8n. The method’s versatility is also validated through experiments on the BDD100K dataset, where it is compared with various object-detection algorithms. The results demonstrate that this method yields significant improvements and practical value in the field of small-object detection. Detailed code can be found at https://github.com/CodeSworder/MMSA .},
  archive      = {J_JRTIP},
  author       = {Mu, Jianhong and Su, Qinghua and Wang, Xiyu and Liang, Wenhui and Xu, Sheng and Wan, Kaizheng},
  doi          = {10.1007/s11554-024-01562-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A small object detection architecture with concatenated detection heads and multi-head mixed self-attention mechanism},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Illuminate the night: Lightweight fusion and enhancement
model for extreme low-light burst images. <em>JRTIP</em>,
<em>21</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01563-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking photographs under low ambient light can be challenging due to the inability of camera sensors to gather sufficient light, resulting in dark images with increased noise and reduced image quality. Standard photography techniques and traditional enhancement methods often fail to provide satisfactory solutions for images captured under extremely low ambient light conditions. To address this problem, data-driven methods have been proposed to model complex non-linear relationships between extremely dark and long-exposure images. Recently, burst photography has become interested in improving single-image low-light image enhancement to provide more information about the scene. In this study, we propose a novel unified fusion and enhancement model inspired by recent advancements in learning-based burst image processing methods. Our model processes a burst set of raw input images across multiple scales to fuse complementary information and predict possible enhancements over the fused information, thereby producing images with longer exposure. Additionally, we introduce a new data augmentation technique, the amplification ratio scaling multiplier, for training to further improve generalization. Experimental results demonstrate that our model achieves state-of-the-art performance in the perceptual metric LPIPS while maintaining highly competitive distortion metrics PSNR and SSIM compared to existing low-light burst image enhancement techniques.},
  archive      = {J_JRTIP},
  author       = {Avşar, Hasan and Sarıgül, Mehmet and Karacan, Levent},
  doi          = {10.1007/s11554-024-01563-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Illuminate the night: Lightweight fusion and enhancement model for extreme low-light burst images},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-feature fusion for efficient inter prediction in
versatile video coding. <em>JRTIP</em>, <em>21</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01564-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Versatile Video Coding (VVC) introduces various advanced coding techniques and tools, such as QuadTree with nested Multi-type Tree (QTMT) partition structure, and outperforms High Efficiency Video Coding (HEVC) in terms of coding performance. However, the improvement of coding performance leads to an increase in coding complexity. In this paper, we propose a multi-feature fusion framework that integrates the rate-distortion-complexity optimization theory with deep learning techniques to reduce the complexity of QTMT partition for VVC inter-prediction. Firstly, the proposed framework extracts features of luminance, motion, residuals, and quantization information from video frames and then performs feature fusion through a convolutional neural network to predict the minimum partition size of Coding Units (CUs). Next, a novel rate-distortion-complexity loss function is designed to balance computational complexity and compression performance. Then, through this loss function, we can adjust various distributions of rate-distortion-complexity costs. This adjustment impacts the prediction bias of the network and sets constraints on different block partition sizes to facilitate complexity adjustment. Compared to anchor VTM $$-$$ 13.0, the proposed method saves the encoding time by 10.14% to 56.62%, with BDBR increase confined to a range of 0.31% to 6.70%. The proposed method achieves a broader range of complexity adjustments while ensuring coding performance, surpassing both traditional methods and deep learning-based methods.},
  archive      = {J_JRTIP},
  author       = {Wei, Xiaojie and Zeng, Hongji and Fang, Ying and Lin, Liqun and Chen, Weiling and Xu, Yiwen},
  doi          = {10.1007/s11554-024-01564-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-feature fusion for efficient inter prediction in versatile video coding},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial convolutional reparameterization network for
lightweight image super-resolution. <em>JRTIP</em>, <em>21</em>(6),
1–12. (<a href="https://doi.org/10.1007/s11554-024-01565-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks (CNNs) have made significant strides in single image super-resolution (SISR). However, redundancy persists in network models concerning both channels and network structures, constituting a challenge in designing lightweight super-resolution (SR) networks. Consequently, finding a balance between efficiency and performance has emerged as the focus in SR research. In response to these challenges, we propose the Partial Convolutional Reparameterization Network (PCRN) for lightweight SR. Specifically, we initially employ partial convolution to reduce channel redundancy. Subsequently, we employ a complex network structure during model training, while in the inference stage, we utilize reparameterization techniques to compress the model, thus reducing redundancy in the network structure. Moreover, we have introduced enhanced spatial attention (ESA) and efficient channel attention (ECA) modules into our approach to enhance the model’s capability to extract key information. In comparative experiments, the proposed PCRN demonstrates superior performance over other efficient SR methods.},
  archive      = {J_JRTIP},
  author       = {Zhang, Long and Wan, Yi},
  doi          = {10.1007/s11554-024-01565-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Partial convolutional reparameterization network for lightweight image super-resolution},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved YOLOv8 method for identifying empty cell and
unqualified plug seedling. <em>JRTIP</em>, <em>21</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01569-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lightweight seedling detection model with improved YOLOv8s is proposed to address the seedling identification problem in the replenishment process in industrial vegetable seedling production.The CBS module for feature extraction in Backbone and Neck has been replaced with a lightweight depthwise separable convolution (DSC) in order to reduce the number of model parameters and increase the speed of detection. Furthermore, the fifth layer of Backbone has been augmented with efficient multiscale attention (EMA), which can aggregate multi-scale spatial structure information more rapidly through the two branches of the parallel structure, thereby enhancing the extraction of multi-scale features. Ultimately, the computational complexity of the model is further reduced by enhancing the structure of the bottleneck to form the VoVGSCSP module, which replaces the C2f module in Neck. The mAP of the improved model on the test set is 96.2%, its parameters, GFLOPS, and model size are 7.88 M, 20.9, and 16.1 MB, respectively. The detection speed of the algorithm is 116.3 frames per second (FPS), which is higher than that of the original model (107.5 FPS). The results indicate that the improved model can accurately identify empty cell and unqualified seedling in the plug tray in real time and has a smaller number of parameters and GFLOPS, making it suitable for use on embedded or mobile devices for seedling replenishment and contributing to the realization of automated and unmanned seedling replenishment.},
  archive      = {J_JRTIP},
  author       = {Li, Lei and Yu, Jiajia and Lu, Yujun and Gu, Yue and Liang, Sijia and Hao, Shuai},
  doi          = {10.1007/s11554-024-01569-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved YOLOv8 method for identifying empty cell and unqualified plug seedling},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved insulator self-explosion detection method based
on group-level pruning for the YOLOv7-tiny algorithm. <em>JRTIP</em>,
<em>21</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01571-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the construction of intelligent grids, unmanned aerial vehicle have been widely employed to inspect transmission lines. The inspection process generates a large amount of data, which requires a lightweight model to reduce computational overhead. Here, we propose an improved model based on YOLOv7-tiny with group-level pruning to reduce the model size, which achieves a balance between detection accuracy and speed. Firstly, we replace the activation function with the Funnel activation function to optimize the activation domain dynamically. Second, we introduce a lightweight DFC attention mechanism to enhance the ability of backbone to extract long-range features. Finally, we use adaptively spatial feature fusion network to reduce semantic degradation during feature fusion. We group the parameters according to their dependencies and use a consistent sparse approach to obtain parameter importance. The redundant parameter groups were pruned to achieve model light-weighting. Experimental results show that the improved model achieves 95.6% detection accuracy after pruning. Compared with YOLOv7-tiny, the computational complexity is reduced by 53% and the processing speed is increased by 48.1% to 73 frames per second.},
  archive      = {J_JRTIP},
  author       = {You, Xilai and Ma, Jianqiao and Yang, Guangze},
  doi          = {10.1007/s11554-024-01571-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved insulator self-explosion detection method based on group-level pruning for the YOLOv7-tiny algorithm},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Urtnet: An unstructured feature fusion network for real-time
detection of endoscopic surgical instruments. <em>JRTIP</em>,
<em>21</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01567-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimally invasive surgery (MIS) is increasingly popular due to its smaller incisions, less pain, and faster recovery. Despite its advantages, challenges like limited visibility and reduced tactile feedback can lead to instrument and organ damage, highlighting the need for precise instrument detection and identification. Current methods face difficulties in detecting multi-scale targets and are often disrupted by blurring, occlusion, and varying lighting conditions during surgeries. Addressing these challenges, this paper introduces URTNet, a novel unstructured feature fusion network designed for the real-time detection of multi-scale surgical instruments in complex environments. Initially, the paper proposes a Stair Aggregation Network (SAN) to efficiently merge multi-scale information, minimizing detail loss in feature fusion and improving detection of blurred and obscured targets. Subsequently, a Multi-scale Feature Weighted Fusion (MFWF) approach is presented to tackle significant scale variations in detection objects and reconstruct the detection layers based on target sizes within endoscopic views. The effectiveness of URTNet is validated through tests on the public laparoscopic dataset m2cai16-tool and another dataset from Sun Yat-sen University Cancer Center, where URTNet achieved average precision scores ( $$AP_{0.5}$$ ) of 93.3% and 97.9%, surpassing other advanced methodologies.},
  archive      = {J_JRTIP},
  author       = {Peng, Cai and Li, Yunjiao and Long, Xiongbai and Zhao, Xiushun and Jiang, Xiaobing and Guo, Jing and Lou, Haifang},
  doi          = {10.1007/s11554-024-01567-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Urtnet: An unstructured feature fusion network for real-time detection of endoscopic surgical instruments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLFusionSR: A fast and lightweight fusion and
super-resolution network for infrared and visible images on edge
devices. <em>JRTIP</em>, <em>21</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01570-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, the application of image fusion and super-resolution networks is limited by the computational burden and power consumption of edge devices. To resolve this problem, we propose a fast and lightweight network for infrared and visible image fusion and super-resolution called FLFusionSR. It directly learns end-to-end mapping from the original low-resolution infrared and visible images to a single high-resolution fused image. To meet the hardware requirements and real-time processing needs of edge devices, we adopt lightweight strategies in network design. To improve network performance without increasing computational complexity, we employ an auxiliary learning strategy during network training. The running analysis shows that our proposed method achieves 30FPS running speed on an edge device with 7.5 W power consumption, meeting the real-time processing requirements. Quantitative and visual comparison experiments on two datasets demonstrate that our proposed algorithm achieves state-of-the-art performance. The code is available at https://github.com/bearxwm/FLFusionSR .},
  archive      = {J_JRTIP},
  author       = {Xue, Weimin and Liu, Yisha and He, Guojian and Wang, Fei and Zhuang, Yan},
  doi          = {10.1007/s11554-024-01570-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FLFusionSR: A fast and lightweight fusion and super-resolution network for infrared and visible images on edge devices},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time object detection method with single-domain
generalization based on YOLOv8. <em>JRTIP</em>, <em>21</em>(6), 1–12.
(<a href="https://doi.org/10.1007/s11554-024-01572-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing models for object detection are often beset by a dearth of generalizability across domains. Specifically, while these models may perform exceptionally well on a given dataset, their efficacy can plummet when confronted with novel domains that lie beyond their training purview. The single-domain generalization methods based on Faster R-CNN are constrained by the underlying strategies, which not only exhibit slow speeds and suboptimal accuracy levels but also demonstrate inadequate generalization. This paper proposes a Complementary Pseudo Multi-domain Generation Method based on YOLOv8 (Y-CPMG). The methodology fortifies the generalization prowess by fabricating a spectrum of pseudo domain information within the feature space. To elaborate, we harness the capabilities of pre-trained visual-language model, leveraging textual prompts to extract domain-specific feature enhancements. These enhancements are then amalgamated with the original images to simulate multi-domain scenarios. Building on this foundation, we delve deeper into the nuances of the real world by introducing normalization perturbation (NP) to uncover a variety of latent domain styles. This approach addresses potential limitations in visual-language models when emulating scenes of diverse styles. Empirical evaluations conducted across a spectrum of weather-diverse public datasets have demonstrated that the proposed method achieves a marked enhancement in performance for the task of domain generalization object detection. With an input dimension of 3 $$\times $$ 608 $$\times $$ 1088, the detection speed reaches 38 FPS, which represents a 65.2 $$\%$$ improvement over Faster R-CNN-based methods, fully meeting the requirements for real-time processing.},
  archive      = {J_JRTIP},
  author       = {Zhou, Yipeng and Qian, Huaming},
  doi          = {10.1007/s11554-024-01572-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time object detection method with single-domain generalization based on YOLOv8},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient denoising method for real-world noise image using
scalable convolution and channel interaction attention. <em>JRTIP</em>,
<em>21</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01575-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a three-stage image denoising method, called Scalable Convolution and Channel Interaction Attention (SC–CIA), to address the high computational cost, complexity, and suboptimal performance of traditional convolution image denoising networks when dealing with real-world noise. In the first stage, we use a variant of dynamic convolution called Scalable Convolution for shallow feature extraction. This method utilizes a set of adaptive small convolution kernels with spatial variations and applies them to full-resolution feature mapping through slicing operations. By combining bilinear interpolation and Scalable Convolution operations, it minimizes computational resources while enhancing the network’s ability to capture position and shape information from images. In addition, our method incorporates an attention mechanism for channel interaction. This mechanism groups every two channels of the input feature map, generating attention maps for each subgroup. The output features are then aggregated and rearranged to achieve channel information interaction and feature enhancement, thereby improving the model’s ability to remove real noise. Compared to various mainstream denoising networks, our method achieves excellent PSNR/SSIM performance on SIDD and DND datasets, while significantly reducing computational complexity (measured in MACs). In particular, when processing 512 $$\times$$ 512 images, our method only uses 10% of the MACs used by MIRNet, and has an inference speed that is 3.73 times faster than MIRNet. These results highlight the potential of our method for fast denoising applications.},
  archive      = {J_JRTIP},
  author       = {Li, Xiaoxia and Dong, Liugu and Wang, Li and Zhou, Yingyue},
  doi          = {10.1007/s11554-024-01575-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient denoising method for real-world noise image using scalable convolution and channel interaction attention},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BN-YOLO: A lightweight method for bird’s nest detection on
transmission lines. <em>JRTIP</em>, <em>21</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01577-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bird’s nest on power transmission lines pose a threat to the safe operation of power equipment and may even affect the stability of the entire power system. To address the challenge that traditional methods face in achieving accurate real-time detection in complex environments, this paper proposes the BN-YOLO bird’s nest detection method based on the YOLOv8s baseline model, enhancing its suitability for real-time applications in intricate settings. First, we replaced the original backbone network of YOLOv8s with a lightweight FasterNet module, thereby reducing computational burden and improving network performance. Second, the feature fusion network was redesigned, incorporating the efficient multi-scale attention module (EMA) to optimize feature fusion capabilities across different scales. Subsequently, we proposed a lighter and faster C2f structure by substituting the standard convolution in the C2f structure with partial convolution (PConv). Finally, Wise-IOUv3 was utilized as the regression loss function to mitigate the effects of low-quality annotations and accelerate network convergence. Experimental results demonstrate that on our self-constructed transmission line bird’s nest dataset, the bird’s nest detection method we proposed achieved a 97.73% score. Compared to the original YOLOv8s, the mean average precision (mAP) increased by 2.19%, and the detection speed improved from 61 FPS to 83 FPS. In addition, our method outperforms other mainstream object detection algorithms, such as SSD, DETR, and RT-DETR, providing higher detection efficiency while maintaining high accuracy. These results confirm that the proposed method can effectively detect bird’s nest targets in complex environments and fulfill the requirements for real-time inspection. This research not only enhances the accuracy and efficiency of bird’s nest detection in power transmission lines but also offers a novel solution for the safe operation and maintenance of smart grids, which holds significant practical application value.},
  archive      = {J_JRTIP},
  author       = {Xiang, Yunjie and Du, Congliu and Mei, Yan and Zhang, Liang and Du, Yutong and Liu, Aoxing},
  doi          = {10.1007/s11554-024-01577-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {BN-YOLO: A lightweight method for bird’s nest detection on transmission lines},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRMNet: More efficient bilateral networks for real-time
semantic segmentation of road scenes. <em>JRTIP</em>, <em>21</em>(6),
1–13. (<a href="https://doi.org/10.1007/s11554-024-01579-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is crucial in autonomous driving because of its accurate identification and segmentation of objects and regions. However, there is a conflict between segmentation accuracy and real-time performance on embedded devices. We propose an efficient lightweight semantic segmentation network (DRMNet) to solve these problems. Employing a streamlined bilateral structure, the model encodes semantic and spatial paths, cross-fusing features during encoding, and incorporates unique skip connections to coordinate upsampling within the semantic pathway. We design a new self-calibrated aggregate pyramid pooling module (SAPPM) at the end of the semantic branch to capture more comprehensive multi-scale semantic information and balance its extraction and inference speed. Furthermore, we designed a new feature fusion module, which guides the fusion of detail features and semantic features through attention perception, alleviating the problem of semantic information quickly covering spatial detail information. Experimental results on the CityScapes, CamVid, and NightCity datasets demonstrate the effectiveness of DRMNet. On a 2080Ti GPU, DRMNet achieves 78.6% mIoU at 88.3 FPS on the CityScapes dataset, 78.9% mIoU at 149 FPS on the CamVid dataset, and 53.5% mIoU at 160.4 FPS on the NightCity dataset. These results highlight the model’s ability to balance accuracy and real-time performance better, making it suitable for embedded devices in autonomous driving applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, Wenming and Zhang, Shaotong and Li, Yaqian and Li, Haibin and Song, Tao},
  doi          = {10.1007/s11554-024-01579-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DRMNet: More efficient bilateral networks for real-time semantic segmentation of road scenes},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An energy-efficient dehazing neural network accelerator
based on e <span class="math display"><sup>2</sup></span> AOD-net.
<em>JRTIP</em>, <em>21</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01574-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turbid media such as fog and haze seriously affects the quality of imaging for systems such as urban surveillance and satellite remote sensing. Image dehazing has become a research hotspot in the field of computer vision. Neural-network-based image dehazing has the potential of high performance, but requires high computational power and storage space, making it costly to deploy in a system with limited hardware resources, especially for edge computing systems. In this paper, we propose an energy-efficient dehazing neural network named E $$^2$$ AOD-Net, which is pruned (generalization performance rises), quantized, pipelined, and parallelized over AOD-Net. We implement E $$^2$$ AOD-Net on FPGA platform, achieving a lightweight dehazing hardware accelerator that realizes real-time dehazing. The experimental results show that the frame rate of E $$^2$$ AOD-Net hardware accelerator inference reaches 38.3 FPS, while consuming the power of 2.491 watts. The VMAF index is improved by 89.14%. The energy efficiency is 42.61 GOPS/w.},
  archive      = {J_JRTIP},
  author       = {Zhang, Zhihao and Du, Gaoming and Li, Zhenmin and Kang, Qinran and Zhao, Wenyao and Wang, Xiaolei},
  doi          = {10.1007/s11554-024-01574-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An energy-efficient dehazing neural network accelerator based on e $$^2$$ AOD-net},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced network model for PCB defect detection:
CDS-YOLO. <em>JRTIP</em>, <em>21</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01580-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces CDS-YOLO, an enhanced YOLOv5s network model, to boost accuracy and real-time performance in detecting defects in Printed Circuit Boards (PCBs). Initially, the model incorporates a Convolutional Block Attention Module (CBAM) into its architecture to heighten focus on diverse positions within the input image. Subsequently, the model employs Distribution Shifts Convolution (DSConv) to replace traditional convolution modules, reducing parameter count and boosting computational efficiency. Furthermore, the substitution of the conventional Complete Intersection over Union (CIoU) loss function with the SCYLLA Intersection over Union (SIoU) loss function optimizes bounding box prediction accuracy. Lastly, a high-resolution detection head (160 $$\times$$ 160 $$\times$$ 255) was incorporated to improve the model’s capability in identifying small-sized defects. Evaluations on the publicly available PKU-Market-PCB dataset showed that the CDS-YOLO model not only achieved an impressive mAP50 of 97.4% and a recall rate of 92.9%, but also operated with a low computational load of 13.5 GFLOPs and a fast detection speed of 117.5 FPS. The findings demonstrate that the CDS-YOLO model proficiently satisfies the stringent defect detection standards required for industrial-grade PCBs.},
  archive      = {J_JRTIP},
  author       = {Shao, Mingrui and Min, Long and Liu, Mengwen and Li, Xuelin and liu, Jingjing and Li, Xiaozhou},
  doi          = {10.1007/s11554-024-01580-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An enhanced network model for PCB defect detection: CDS-YOLO},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial generative learning and timed path optimization
for real-time visual image prediction to guide robot arm movements.
<em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01526-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time visual image prediction, crucial for directing robotic arm movements, represents a significant technique in artificial intelligence and robotics. The primary technical challenges involve the robot’s inaccurate perception and understanding of the environment, coupled with imprecise control of movements. This study proposes ForGAN-MCTS, a generative adversarial network-based action sequence prediction algorithm, aimed at refining visually guided rearrangement planning for movable objects. Initially, the algorithm unveils a scalable and robust strategy for rearrangement planning, capitalizing on the capabilities of a Monte Carlo Tree Search strategy. Secondly, to enable the robot’s successful execution of grasping maneuvers, the algorithm proposes a generative adversarial network-based real-time prediction method, employing a network trained solely on synthetic data for robust estimation of multi-object workspace states via a single uncalibrated RGB camera. The efficacy of the newly proposed algorithm is corroborated through extensive experiments conducted by using a UR-5 robotic arm. The experimental results demonstrate that the algorithm surpasses existing methods in terms of planning efficacy and processing speed. Additionally, the algorithm is robust to camera motion and can effectively mitigate the effects of external perturbations.},
  archive      = {J_JRTIP},
  author       = {Li, Xin and Ru, Changhai and Sun, Haonan},
  doi          = {10.1007/s11554-024-01526-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adversarial generative learning and timed path optimization for real-time visual image prediction to guide robot arm movements},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time visual SLAM based on semantic information and
geometric information in dynamic environment. <em>JRTIP</em>,
<em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01527-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) is the core technology enabling mobile robots to autonomously explore and perceive the environment. However, dynamic objects in the scene significantly impact the accuracy and robustness of visual SLAM systems, limiting its applicability in real-world scenarios. Hence, we propose a real-time RGB-D visual SLAM algorithm designed for indoor dynamic scenes. Our approach includes a parallel lightweight object detection thread, which leverages the YOLOv7-tiny network to detect potential moving objects and generate 2D semantic information. Subsequently, a novel dynamic feature removal strategy is introduced in the tracking thread. This strategy integrates semantic information, geometric constraints, and feature point depth-based RANSAC to effectively mitigate the influence of dynamic features. To evaluate the effectiveness of the proposed algorithms, we conducted comparative experiments using other state-of-the-art algorithms on the TUM RGB-D dataset and Bonn RGB-D dataset, as well as in real-world dynamic scenes. The results demonstrate that the algorithm maintains excellent accuracy and robustness in dynamic environments, while also exhibiting impressive real-time performance.},
  archive      = {J_JRTIP},
  author       = {Sun, Hongli and Fan, Qingwu and Zhang, Huiqing and Liu, Jiajing},
  doi          = {10.1007/s11554-024-01527-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time visual SLAM based on semantic information and geometric information in dynamic environment},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time low-light video enhancement on smartphones.
<em>JRTIP</em>, <em>21</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01532-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time low-light video enhancement on smartphones remains an open challenge due to hardware constraints such as limited sensor size and processing power. While night mode cameras have been introduced in smartphones to acquire high-quality images in light-constrained environments, their usability is restricted to static scenes as the camera must remain stationary for an extended period to leverage long exposure times or burst imaging techniques. Concurrently, significant process has been made in low-light enhancement on images coming out from the camera’s image signal processor (ISP), particularly through neural networks. These methods do not improve the image capture process itself; instead, they function as post-processing techniques to enhance the perceptual brightness and quality of captured imagery for display to human viewers. However, most neural networks are computationally intensive, making their mobile deployment either impractical or requiring considerable engineering efforts. This paper introduces VLight, a novel single-parameter low-light enhancement algorithm that enables real-time video enhancement on smartphones, along with real-time adaptation to changing lighting conditions and user-friendly fine-tuning. Operating as a custom brightness-booster on digital images, VLight provides real-time and device-agnostic enhancement directly on users’ devices. Notably, it delivers real-time low-light enhancement at up to 67 frames per second (FPS) for 4K videos locally on the smartphone.},
  archive      = {J_JRTIP},
  author       = {Zhou, Yiming and MacPhee, Callen and Gunawan, Wesley and Farahani, Ali and Jalali, Bahram},
  doi          = {10.1007/s11554-024-01532-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time low-light video enhancement on smartphones},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-friendly fast rate-distortion optimization
algorithm for AV1 encoder. <em>JRTIP</em>, <em>21</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01535-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rate distortion optimization (RDO) process aims at achieving optimal coding performance by determining the optimal coding mode according to a certain strategy in the AV1 video coding. However, the high computational complexity and strong data dependency in RDO impede real-time applications. To address these issues, a fast RDO algorithm suitable for hardware implementation is proposed. Firstly, we propose a high-frequency coefficients zero-setting approach to optimize the hardware memory occupation. Then, in the rate-distortion calculation stage, an efficient rate estimation method is proposed based on a statistical feature for the number of quantization coefficients, and the distortion estimation method is proposed by considering intrinsic features in the all-zero block. Finally, a reconstruction approximate model is proposed to solve the low parallelism issue caused by the coupling of pixel reconstruction and prediction data. Experimental results show that the proposed algorithm achieves 68.49% and 50.77% time-saving by 2.73% and 2.95% Bjøntegaard delta rate (BD-Rate) increase on average under all intra (AI) and random access (RA) configurations, respectively.},
  archive      = {J_JRTIP},
  author       = {Tang, Ran and Huang, Xiaofeng and Cui, Yan and Guo, Xinnan and Zhou, Yang and Yin, Haibing and Yan, Chenggang},
  doi          = {10.1007/s11554-024-01535-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware-friendly fast rate-distortion optimization algorithm for AV1 encoder},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA-based hardware/firmware co-design for real-time
radiometric correction onboard microsatellite. <em>JRTIP</em>,
<em>21</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01536-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images are inevitably produced with radiometric artifacts due to the photo-response non-uniformity of charge-coupled device (CCD) sensors. In situations where time constraints demand the prompt acquisition of imaging products, integrating an onboard radiometric correction system becomes essential. This paper advocates for a hardware–firmware co-design approach to achieve radiometric correction within the payload front-end electronics (FEE), leveraging the capabilities of field programmable gate array circuits (FPGA). The selection of an appropriate CCD sensor and optical device is guided by a thorough payload mission analysis, ensuring compliance with the specifications derived from Alsat-1B, the Algerian microsatellite launched in September 2016. Simulation results demonstrate that the designed FPGA firmware effectively controls the CCD sensor and configures its settings to achieve real-time radiometric correction of the acquired pixels in accordance with the mission requirements. To ensure efficient utilization during imaging operations, a hardware solution for onboard storage and in-orbit update of the radiometric coefficients has been considered for the radiometric correction system.},
  archive      = {J_JRTIP},
  author       = {Ghelamallah, Youcef and Rachedi, Azzeddine},
  doi          = {10.1007/s11554-024-01536-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-based hardware/firmware co-design for real-time radiometric correction onboard microsatellite},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating YOLOv8 and CSPBottleneck based CNN for enhanced
license plate character recognition. <em>JRTIP</em>, <em>21</em>(5),
1–12. (<a href="https://doi.org/10.1007/s11554-024-01537-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces an integrated methodology for license plate character recognition, combining YOLOv8 for segmentation and a CSPBottleneck-based CNN classifier for character recognition. The proposed approach incorporates pre-processing techniques to enhance the recognition of partial plates and augmentation methods to address challenges arising from colour diversity. Performance analysis demonstrates YOLOv8’s high segmentation accuracy and fast processing time, complemented by precise character recognition and efficient processing by the CNN classifier. The integrated system achieves an overall accuracy of 99.02% with a total processing time of 9.9 ms, offering a robust solution for automated license plate recognition (ALPR) systems. The integrated approach presented in the paper holds promise for the practical implementation of ALPR technology and further development in the field of license plate recognition systems.},
  archive      = {J_JRTIP},
  author       = {Khokhar, Sahil and Kedia, Deepak},
  doi          = {10.1007/s11554-024-01537-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Integrating YOLOv8 and CSPBottleneck based CNN for enhanced license plate character recognition},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy efficiency assessment in advanced driver assistance
systems with real-time image processing on custom xilinx DPUs.
<em>JRTIP</em>, <em>21</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01538-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement in embedded AI, driven by integrating deep neural networks (DNNs) into embedded systems for real-time image and video processing, has been notably pushed by AI-specific platforms like the AMD Xilinx Vitis AI on the MPSoC-FPGA platform. This platform utilizes a configurable Deep Processing Unit (DPU) for scalable resource utilization and operating frequencies. Our study employed a detailed methodology to assess the impact of various DPU configurations and frequencies on resource utilization and energy consumption. The findings reveal that increasing the DPU frequency enhances resource utilization efficiency and improves performance. Conversely, lower frequencies significantly reduce resource utilization, with only a borderline decrease in performance. These trade-offs are influenced not only by frequency but also by variations in DPU parameters. These findings are critical for developing energy-efficient AI-driven systems in Advanced Driver Assistance Systems (ADAS) based on real-time video processing. By leveraging the capabilities of Xilinx Vitis AI deployed on the Kria KV260 MPSoC platform, we explore the intricacies of optimizing energy efficiency through multi-task learning in real-time ADAS applications.},
  archive      = {J_JRTIP},
  author       = {Tatar, Güner and Bayar, Salih},
  doi          = {10.1007/s11554-024-01538-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Energy efficiency assessment in advanced driver assistance systems with real-time image processing on custom xilinx DPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slim-YOLO-PR_KD: An efficient pose-varied object detection
method for underground coal mine. <em>JRTIP</em>, <em>21</em>(5), 1–17.
(<a href="https://doi.org/10.1007/s11554-024-01539-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection in underground coal mine is a crucial task in the development of AI-assisted supervision systems. Due to the complex environment of the underground coal mine, limited computing resources, and the variability of object poses, the general object detection algorithms cannot provide good performance. Hence, an improved underground pose-varied object detection method named Slim-YOLO-PR_KD has been proposed. By designing an efficient pose-varied attention module (EPA) for the backbone network, providing a receive field block (RFB) module for the neck network, and optimizing the loss function, the underground pose-varied detection model YOLO-PR is obtained, which achieved good accuracy but reduced speed. For YOLO-PR, the study improved the original module by designing RFB_SK, a lightweight C2f_GSG module, a shared parameter detection head and selectively replaced modules to slim down the whole network, resulting in a lightweight detection model Slim-YOLO-PR. By using an attention guided knowledge distillation of underground object detection method and using YOLO-PR as the teacher model, the efficient pose-varied detection model Slim-YOLO-PR_KD for coal mine underground is proposed. The experimental results show that compared with the baseline model, the proposed Slim-YOLO-PR_KD has a faster detection speed, achieving higher detection accuracy while reducing model parameters and computational complexity by 42% and 46% respectively, making it capable of performing real-time underground detection tasks. Compared with other general detection models, Slim-YOLO-PR_KD exhibits excellent performance in real-time pose-varied object detection tasks in complex environments of underground coal mines.},
  archive      = {J_JRTIP},
  author       = {Mu, Huaxing and Liu, Jueting and Guan, Yanyun and Chen, Wei and Xu, Tingting and Wang, Zehua},
  doi          = {10.1007/s11554-024-01539-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Slim-YOLO-PR_KD: An efficient pose-varied object detection method for underground coal mine},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ESC-YOLO: Optimizing apple fruit recognition with efficient
spatial and channel features in YOLOX. <em>JRTIP</em>, <em>21</em>(5),
1–14. (<a href="https://doi.org/10.1007/s11554-024-01540-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate localization of apple fruits and recognition of occlusion types in complex orchard environments play an important role in precision agriculture. This work proposes an efficient fruit recognition model called Efficient Spatial and Channel Feature YOLOX (ESC-YOLO). ESC-YOLO is built upon YOLOX and fully leverages and emphasizes spatial channel information, ensuring coherence between global information and local features. The optimization strategies for the backbone network involve adopting EfficientViT as the foundational backbone, integrating Spatial and Channel Reconstruction Convolution (SCConv) into the input stem to reorganize spatial channel features and reduce redundancy, and constructing the Efficient-MBConv module, which is optimally combined with the EfficientViTBlock for feature extraction. The optimization strategies for the neck network involve utilizing the Centralized Feature Pyramid Net (CFPNet) as the neck network and employing a Simple, Parameter-Free Attention Module (SimAM) to enhance model performance. In this work, we adopted the lightweight model of the ESC-YOLO for performance evaluation, namely ESC-YOLO-S. It achieves a 4.26% improvement in Top-1 mean Average Precision (mAP) compared to YOLOX-S and significantly reduces the false and missed detections caused by various types of occlusions. Therefore, the improved model meets the requirements for high-precision identification in complex orchard environments.},
  archive      = {J_JRTIP},
  author       = {Sun, Jun and Peng, Yifei and Chen, Chen and Zhang, Bing and Wu, Zhaoqi and Jia, Yilin and Shi, Lei},
  doi          = {10.1007/s11554-024-01540-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ESC-YOLO: Optimizing apple fruit recognition with efficient spatial and channel features in YOLOX},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient spatio-temporal network for action recognition.
<em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01541-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The input tensor of video data includes temporal, spatial, and channel dimensions, crucial for extracting complementary spatial, temporal, and spatio-temporal features for video action recognition. To efficiently extract and integrate these features, we propose an efficient spatio-temporal module (ESTM) with three pathways dedicated to extracting spatial, temporal, and spatio-temporal features. Each pathway uses the Cross Global Average Pooling (CGAP) module to compress the current dimension, focusing features on the remaining two dimensions. This enhances feature extraction and recognition rates for complex actions. We also introduce a Motion Excitation Module (MEM) to enrich input features by transforming correlations between adjacent frames, reducing computational complexity. Finally, ESTM and MEM are seamlessly integrated into a 2D CNN, forming the efficient spatio-temporal network (ESTN), with minimal impact on network parameters and computational costs. Extensive experiments show that ESTN outperforms state-of-the-art methods on datasets like Something V1 &amp; V2 and HMDB51, validating its effectiveness.},
  archive      = {J_JRTIP},
  author       = {Su, Yanxiong and Zhao, Qian},
  doi          = {10.1007/s11554-024-01541-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient spatio-temporal network for action recognition},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and real-time skin lesion image segmentation using
spatial-frequency information and channel convolutional networks.
<em>JRTIP</em>, <em>21</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01542-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of skin lesions is essential for physicians to screen in dermoscopy images. However, they commonly face three main limitations: difficulty in accurately processing targets with coarse edges; frequent challenges in recovering detailed feature data; and a lack of adequate capability for the effective amalgamation of multi-scale features. To overcome these problems, we propose a skin lesion segmentation network (SFCC Net) that combines an attention mechanism and a redundancy reduction strategy. The initial step involved the design of a downsampling encoder and an encoder composed of Receptive Field (REFC) Blocks, aimed at supplementing lost details and extracting latent features. Subsequently, the Spatial-Frequency-Channel (SF) Block was employed to minimize feature redundancy and restore fine-grained information. To fully leverage previously learned features, an Up-sampling Convolution (UpC) Block was designed for information integration. The network’s performance was compared with state-of-the-art models on four public datasets. Experimental results demonstrate significant improvements in the network’s performance. On the ISIC datasets, the proposed network outperformed D-LKA Net by 4.19%, 0.19%, and 7.75% in F1, and by 2.14%, 0.51%, and 12.20% in IoU. The frame rate (FPS) of the proposed network when processing skin lesion images underscores its suitability for real-time image analysis. Additionally, the network’s generalization capability was validated on a lung dataset.},
  archive      = {J_JRTIP},
  author       = {Liu, Shangwang and Zhou, Bingyan and Lin, Yinghai and Wang, Peixia},
  doi          = {10.1007/s11554-024-01542-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient and real-time skin lesion image segmentation using spatial-frequency information and channel convolutional networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOv8s-CFB: A lightweight method for real-time detection of
apple fruits in complex environments. <em>JRTIP</em>, <em>21</em>(5),
1–13. (<a href="https://doi.org/10.1007/s11554-024-01543-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of apple-picking robots, deep learning models have become essential in apple detection. However, current detection models are often disrupted by complex backgrounds, leading to low recognition accuracy and slow speeds in natural environments. To address these issues, this study proposes an improved model, YOLOv8s-CFB, based on YOLOv8s. This model introduces partial convolution (PConv) in the backbone network, enhances the C2f module, and forms a new architecture, CSPPC, to reduce computational complexity and improve speed. Additionally, FocalModulation technology replaces the original SPPF module to enhance the model’s ability to recognize key areas. Finally, the bidirectional feature pyramid (BiFPN) is introduced to adaptively learn the importance of weights at each scale, effectively retaining multi-scale information through a bidirectional context information transmission mechanism, and improving the model’s detection ability for occluded targets. Test results show that the improved YOLOv8 network achieves better detection performance, with an average accuracy of 93.86%, a parameter volume of 8.83 M, and a detection time of 0.7 ms. The improved algorithm achieves high detection accuracy with a small weight file, making it suitable for deployment on mobile devices. Therefore, the improved model can efficiently and accurately detect apples in complex orchard environments in real time.},
  archive      = {J_JRTIP},
  author       = {Zhao, Bing and Guo, Aoran and Ma, Ruitao and Zhang, Yanfei and Gong, Jinliang},
  doi          = {10.1007/s11554-024-01543-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLOv8s-CFB: A lightweight method for real-time detection of apple fruits in complex environments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient real-time visual image adversarial
generation and processing algorithm for new energy vehicles.
<em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01544-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of deep learning in the last decade, generating and processing real-time images have become one of critical methods in intelligent driving systems for new energy vehicles. However, the real-time images captured by sensors are susceptible to variations in various environments, including different weather and lighting conditions. To enhance the real-time image generation performance for new energy vehicles in complex environments, and improve real-time visual image processing capabilities, this study proposes an energy-efficient real-time visual image adversarial generation and processing algorithm, called as ENV-GAN. It hypothesizes a shared latent domain among mixed image domains after analyzing driving situations under various weather and lighting conditions. Mappings are established between different image domains. Besides, a multi-encoder weight-sharing technique is utilized to enhances the generative adversarial network model. Additionally, the algorithm integrates an attention module to enhance the modelâ€™s image generation. Experimental results and analysis demonstrate that the new algorithm outperforms existing algorithms in tasks such as defogging, rain removal, and lighting enhancement, offering high energy efficiency and low energy consumption.},
  archive      = {J_JRTIP},
  author       = {Li, Yinghuan and Liu, Jicheng},
  doi          = {10.1007/s11554-024-01544-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Energy-efficient real-time visual image adversarial generation and processing algorithm for new energy vehicles},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO9tr: A lightweight model for pavement damage detection
utilizing a generalized efficient layer aggregation network and
attention mechanism. <em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01545-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining road pavement integrity is crucial for ensuring safe and efficient transportation. Conventional methods for assessing pavement condition are often laborious and susceptible to human error. This paper proposes YOLO9tr, a novel lightweight object detection model for pavement damage detection, leveraging the advancements of deep learning. YOLO9tr is based on the YOLOv9 architecture, incorporating a partial attention block that enhances feature extraction and attention mechanisms, leading to improved detection performance in complex scenarios. The model is trained on a comprehensive dataset comprising road damage images from multiple countries. This dataset includes an expanded set of damage categories beyond the standard four types (longitudinal cracks, transverse cracks, alligator cracks, and potholes), providing a more nuanced classification of road damage. This broadened classification range allows for a more accurate and realistic assessment of pavement conditions. Comparative analysis demonstrates YOLO9tr’s superior precision and inference speed compared to state-of-the-art models like YOLOv8, YOLOv9 and YOLOv10, achieving a balance between computational efficiency and detection accuracy. The model achieves a high frame rate of up to 136 FPS, making it suitable for real-time applications such as video surveillance and automated inspection systems. The research presents an ablation study to analyze the impact of architectural modifications and hyperparameter variations on model performance, further validating the effectiveness of the partial attention block. The results highlight YOLO9tr’s potential for practical deployment in real-time pavement condition monitoring, contributing to the development of robust and efficient solutions for maintaining safe and functional road infrastructure.},
  archive      = {J_JRTIP},
  author       = {Youwai, Sompote and Chaiyaphat, Achitaphon and Chaipetch, Pawarotorn},
  doi          = {10.1007/s11554-024-01545-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO9tr: A lightweight model for pavement damage detection utilizing a generalized efficient layer aggregation network and attention mechanism},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaptoMixNet: Detection of foreign objects on power
transmission lines under severe weather conditions. <em>JRTIP</em>,
<em>21</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01546-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of power transmission line scale, the surrounding environment is complex and susceptible to foreign objects, severely threatening its safe operation. The current algorithm lacks stability and real-time performance in small target detection and severe weather conditions. Therefore, this paper proposes a method for detecting foreign objects on power transmission lines under severe weather conditions based on AdaptoMixNet. First, an Adaptive Fusion Module (AFM) is introduced, which improves the model&#39;s accuracy and adaptability through multi-scale feature extraction, fine-grained information preservation, and enhancing context information. Second, an Adaptive Feature Pyramid Module (AEFPM) is proposed, which enhances the focus on local details while preserving global information, improving the stability and robustness of feature representation. Finally, the Neuron Expansion Recursion Adaptive Filter (CARAFE) is designed, which enhances feature extraction, adaptive filtering, and recursive mechanisms, improving detection accuracy, robustness, and computational efficiency. Experimental results show that the method of this paper exhibits excellent performance in the detection of foreign objects on power transmission lines under complex backgrounds and harsh weather conditions.},
  archive      = {J_JRTIP},
  author       = {Jia, Xinghai and Ji, Chao and Zhang, Fan and Liu, Junpeng and Gao, Mingjiang and Huang, Xinbo},
  doi          = {10.1007/s11554-024-01546-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {AdaptoMixNet: Detection of foreign objects on power transmission lines under severe weather conditions},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast encryption of color medical videos for internet of
medical things. <em>JRTIP</em>, <em>21</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11554-024-01547-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the Internet of Things (IoT), the Internet of Medical Things (IoMT) has emerged as a critical sector that enhances convenience and plays a vital role in saving lives. IoMT devices facilitate remote access and control of various medical tools, significantly improving accessibility in the healthcare field. However, the connectivity of these devices to the internet makes them vulnerable to adversarial attacks. Safeguarding medical data becomes a paramount concern, particularly when precise biometric readings are required without compromising patient safety. This paper proposes a fast encryption mechanism to protect the color information in medical videos utilized within the IoMT environment. Our approach involves scrambling medical video frames using a rapid block-splitting method combined with simple operations. Subsequently, the scrambled frames are encrypted using different keys generated from the logistic map. To ensure the practicality of our proposed method in the IoMT setting, we implement the encryption mechanism on a cost-effective Raspberry Pi platform. To evaluate the effectiveness of our proposed mechanism, we conduct comprehensive simulations and security analyses. Notably, we investigate medical test videos during the evaluation process, further validating the applicability of our method. The results confirm our proposed mechanism&#39;s robustness by hiding patterns in original videos, achieving high entropy to increase randomness in encrypted videos, reducing the correlation between adjacent pixels in encrypted videos, and resisting various attacks.},
  archive      = {J_JRTIP},
  author       = {Aldakheel, Eman Abdullah and Khafaga, Doaa Sami and Zaki, Mohamed A. and Lashin, Nabil A. and Hamza, Hanaa M. and Hosny, Khalid M.},
  doi          = {10.1007/s11554-024-01547-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast encryption of color medical videos for internet of medical things},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time foreign object detection method based on deep
learning in complex open railway environments. <em>JRTIP</em>,
<em>21</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11554-024-01548-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the current challenges of numerous background influencing factors and low detection accuracy in the open railway foreign object detection, a real-time foreign object detection method based on deep learning for open railways in complex environments is proposed. Firstly, the images of foreign objects invading the clearance collected by locomotives during long-term operation are used to create a railway foreign object dataset that fits the current situation. Then, to improve the performance of the target detection algorithm, certain improvements are made to the YOLOv7-tiny network structure. The improved algorithm enhances feature extraction capability and strengthens detection performance. By introducing a Simple, parameter-free Attention Module for convolutional neural network (SimAM) attention mechanism, the representation ability of ConvNets is improved without adding extra parameters. Additionally, drawing on the network structure of the weighted Bi-directional Feature Pyramid Network (BiFPN), the backbone network achieves cross-level feature fusion by adding edges and neck fusion. Subsequently, the feature fusion layer is improved by introducing the GhostNetV2 module, which enhances the fusion capability of different scale features and greatly reduces computational load. Furthermore, the original loss function is replaced with the Normalized Wasserstein Distance (NWD) loss function to enhance the recognition capability of small distant targets. Finally, the proposed algorithm is trained and validated, and compared with other mainstream detection algorithms based on the established railway foreign object dataset. Experimental results show that the proposed algorithm achieves applicability and real-time performance on embedded devices, with high accuracy, improved model performance, and provides precise data support for railway safety assurance.},
  archive      = {J_JRTIP},
  author       = {Zhang, Binlin and Yang, Qing and Chen, Fengkui and Gao, Dexin},
  doi          = {10.1007/s11554-024-01548-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time foreign object detection method based on deep learning in complex open railway environments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mfdd: Multi-scale attention fatigue and distracted driving
detector based on facial features. <em>JRTIP</em>, <em>21</em>(5), 1–12.
(<a href="https://doi.org/10.1007/s11554-024-01549-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the automotive industry and the continuous growth of vehicle fleets, traffic safety has become a critical global social issue. Developing detection and alert systems for fatigue and distracted driving is essential for enhancing traffic safety. Factors, such as variations in the driver’s facial details, lighting conditions, and camera pixel quality, significantly affect the accuracy of fatigue and distracted driving detection, often resulting in the low effectiveness of existing methods. This study introduces a new network designed to detect fatigue and distracted driving amidst the complex backgrounds typical within vehicles. To extract driver and facial information as well as gradient details more efficiently, we introduce the Multihead Difference Kernel Convolution Module (MDKC) and Multiscale Large Convolutional Fusion Module (MLCF) in baseline. This incorporates a blend of Multihead Mixed Convolution and Large and Small Convolutional Kernels to amplify the spatial intricacies of the backbone. To extract gradient details from different illumination and noise feature maps, we enhance the network’s neck by introducing the Adaptive Convolutional Attention Module (ACAM) in NECK, optimizing feature retention. Extensive comparative experiments validate the efficacy of our network, showcasing superior performance not only on the Fatigue and Distracted Driving Dataset but also competitive results on the public COCO dataset. Source code is available at https://github.com/SCNU-RISLAB/MFDD .},
  archive      = {J_JRTIP},
  author       = {Shi, Yulin and Cheng, Jintao and Chen, Xingming and Luo, Jiehao and Tang, Xiaoyu},
  doi          = {10.1007/s11554-024-01549-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Mfdd: Multi-scale attention fatigue and distracted driving detector based on facial features},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LGFF-YOLO: Small object detection method of UAV images based
on efficient local–global feature fusion. <em>JRTIP</em>,
<em>21</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01550-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured by Unmanned Aerial Vehicles (UAVs) play a significant role in many fields. However, with the development of UAV technology, challenges such as detecting small and dense objects against complex backgrounds have emerged. In this paper, we propose LGFF-YOLO, a detection model that integrates a novel local–global feature fusion method with the YOLOv8 baseline, specifically designed for small object detection in UAV imagery. Our innovative approach employs the Global Information Fusion Module (GIFM) and the Four-Leaf Clover Fusion Module (FLCM) to enhance the fusion of multi-scale features, improving detection accuracy without increasing model complexity. Next, we proposed the RFA-Block and LDyHead to control the total number of model parameters and improve the representation capability for small object detection. Experimental results on the VisDrone2019 dataset demonstrate a 38.3% mAP with only 4.15M parameters, a 4. 5% increase over baseline YOLOv8, while achieving 79.1 FPS for real-time detection. These advancements enhance the model’s generalization capability, balancing accuracy and speed, and significantly extend its applicability for detecting small objects in UAV images.},
  archive      = {J_JRTIP},
  author       = {Peng, Hongxing and Xie, Haopei and Liu, Huanai and Guan, Xianlu},
  doi          = {10.1007/s11554-024-01550-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LGFF-YOLO: Small object detection method of UAV images based on efficient local–global feature fusion},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMS-YOLO: An enhanced algorithm for water meter reading
recognition in complex environments. <em>JRTIP</em>, <em>21</em>(5),
1–13. (<a href="https://doi.org/10.1007/s11554-024-01551-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The disordered arrangement of water-meter pipes and the random rotation angles of their mechanical character wheels frequently result in captured water-meter images exhibiting tilt, blur, and incomplete characters. These issues complicate the detection of water-meter images, rendering traditional OCR (optical character recognition) methods inadequate for current detection requirements. Furthermore, the two-stage detection method, which involves first locating and then recognizing, proves overly cumbersome. In this paper, water-meter reading recognition is approached as an object-detection task, extracting readings using the algorithm’s Predicted Box information, establishing a water-meter dataset, and refining the algorithmic framework to improve the accuracy of recognizing incomplete characters. Utilizing YOLOv8n as the baseline, we propose GMS-YOLO, a novel object-detection algorithm that employs Grouped Multi-Scale Convolution for enhanced performance. First, by substituting the Bottleneck module’s convolution with GMSC (Grouped Multi-Scale Convolution), the model can access various scale receptive fields, thus boosting its feature-extraction prowess. Second, incorporating LSKA (Large Kernel Separable Attention) into the SPPF (Spatial Pyramid Pooling Fast) module improves the perception of fine-grained features. Finally, replacing CIoU (Generalized Intersection over Union) with the ShapeIoU bounding box loss function enhances the model’s ability to localize objects and speeds up its convergence. Evaluating a self-compiled water-meter image dataset, GMS-YOLO attained a mAP@0.5 of 92.4% and a precision of 93.2%, marking a 2.0% and 2.1% enhancement over YOLOv8n, respectively. Despite the increased computational burden, GMS-YOLO maintains an average detection time of 10 ms per image, meeting practical detection needs.},
  archive      = {J_JRTIP},
  author       = {Wang, Yu and Xiang, Xiaodong},
  doi          = {10.1007/s11554-024-01551-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GMS-YOLO: An enhanced algorithm for water meter reading recognition in complex environments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast rough mode decision algorithm and hardware architecture
design for AV1 encoder. <em>JRTIP</em>, <em>21</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01552-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance compression efficiency, the AV1 video coding standard has introduced several new intra-prediction modes, such as smooth and finer directional prediction modes. However, this addition increases computational complexity and hinders parallelized hardware implementation. In this paper, a hardware-friendly rough mode decision (RMD) algorithm and its fully pipelined hardware architecture design are proposed to address these challenges. For algorithm optimization, firstly, a novel directional mode pruning algorithm is proposed. Then, the sum of absolute transform differences (SATD) cost accumulated approximation method is adopted during the tree search. Finally, in the reconstruction stage, a reconstruction approximation model based on the DC transform is proposed to solve the low-parallelism problem. For hardware architecture design, the proposed fully pipelined hardware architecture is implemented with 28 pipeline stages. This design can process multiple prediction modes in parallel. Experimental results show that the proposed fast algorithm achieves 46.8% time savings by 1.96% Bjøntegaard delta rate (BD-Rate) increase on average under all-intra (AI) configuration. When synthesized under the 28nm UMC technology, the proposed hardware can operate at a frequency of 316.2 MHz with 1113.14 K gate count.},
  archive      = {J_JRTIP},
  author       = {Chen, Heng and Huang, Xiaofeng and Tao, Zehao and Sheng, Qinghua and Cui, Yan and Zhou, Yang and Yin, Haibing},
  doi          = {10.1007/s11554-024-01552-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast rough mode decision algorithm and hardware architecture design for AV1 encoder},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-precision real-time autonomous driving target detection
based on YOLOv8. <em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01553-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traffic scenarios, the size of targets varies significantly, and there is a limitation on computing power. This poses a significant challenge for algorithms to detect traffic targets accurately. This paper proposes a new traffic target detection method that balances accuracy and real-time performance—Deep and Filtered You Only Look Once (DF-YOLO). In response to the challenges posed by significant differences in target scales within complex scenes, we designed the Deep and Filtered Path Aggregation Network (DF-PAN). This module effectively fuses multi-scale features, enhancing the model&#39;s capability to detect multi-scale targets accurately. In response to the challenge posed by limited computational resources, we design a parameter-sharing detection head (PSD) and use Faster Neural Network (FasterNet) as the backbone network. PSD reduces computational load by parameter sharing and allows for feature extraction capability sharing across different positions. FasterNet enhances memory access efficiency, thereby maximizing computational resource utilization. The experimental results on the KITTI dataset show that our method achieves satisfactory balances between real-time and precision and reaches 90.9% mean average precision(mAP) with 77 frames/s, and the number of parameters is reduced by 28.1% and the detection accuracy is increased by 3% compared to the baseline model. We test it on the challenging BDD100K dataset and the SODA10M dataset, and the results show that DF-YOLO has excellent generalization ability.},
  archive      = {J_JRTIP},
  author       = {Liu, Huixin and Lu, Guohua and Li, Mingxi and Su, Weihua and Liu, Ziyi and Dang, Xu and Zang, Dongyuan},
  doi          = {10.1007/s11554-024-01553-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High-precision real-time autonomous driving target detection based on YOLOv8},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multi-layer interactive residual feature fusion
network for real-time traffic sign detection with stage routing
attention. <em>JRTIP</em>, <em>21</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01554-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is an important research content of Autonomous Driving Systems, which can effectively guide vehicles or driver to make correct decisions and reduce traffic accidents. The existing real-time traffic sign detectors have low detection accuracy for small objects. Therefore, we propose a novel real-time traffic sign detector based on YOLOv5 for the accurate detection of small objects. Specifically, we propose a new Multi-layer Interactive Residual Feature Fusion Network (MIRFFN) in the neck, which can effectively combine the position information of the low-layer feature maps with the semantic information of the high-layer feature maps, and refine the features by fusing different layers of feature maps. Then, we design a Residual Information Fusion (RIF) module for MIRFFN to fuse feature maps from different layers. The RIF module is composed of three residual blocks to refine spatial position information. Inspired by Bi-level Routing Attention effectively extracting small objects, we design a Stage Routing Attention (SRA) module in the backbone. The SRA modules can search the most relevant regions and enhance attention to small traffic signs in high-layer feature maps. We conduct experiments on GTSDB, TT100K, and CCTSDB2021, and achieve mAP of 96.2%, 72.7%, and 90.8%, respectively. Our method achieves 48.91 FPS on the GTSDB dataset. The experimental results show that our method can accurately perform real-time traffic sign detection.},
  archive      = {J_JRTIP},
  author       = {Zhang, Jianming and Yi, Yao and Wang, Zulou and Alqahtani, Fayez and Wang, Jin},
  doi          = {10.1007/s11554-024-01554-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Learning multi-layer interactive residual feature fusion network for real-time traffic sign detection with stage routing attention},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D reconstruction method of welding area by fusion of coding
raster and semantic segmentation network. <em>JRTIP</em>,
<em>21</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01555-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its rapid projection and capture of fringe patterns, coded structured light measurement technology is widely utilised for acquiring three-dimensional information in welding areas. To achieve real-time segmentation of the welding area for T-joint butt welding workpieces with various noise interference, this paper has developed a lightweight dual-resolution semantic segmentation network (LDRNet). This paper designed a lightweight feature extraction module that provides efficient feature representations for the network using fewer parameters and computational costs. To enhance the network’s robustness to complex environmental noise, this paper introduced a multi-scale adaptive feature extraction module that can capture information at different scales of the environment. To further improve segmentation accuracy, this paper reconstructed the traditional pyramid pooling module and combined it with the CBAM attention mechanism to enhance the focus on important features. This paper proposes a local feature constraint method to improve the phase matching accuracy. Experimental results show that this paper algorithm significantly reduces Params and FLOPs by 62.02% and 65.78%, respectively, compared to the original network DDRNet-23-Slim. Moreover, it leads to an improvement in the IOU of the welding area from 74.55% to 77.43%. Additionally, this paper’s proposed algorithm effectively reduces the generation of noise points through phase matching by approximately 93.88%. Consequently, this algorithm satisfactorily meets the requirements of practical production processes.},
  archive      = {J_JRTIP},
  author       = {Song, Limei and Xu, Baolin and Yang, Yangang and Yuan, Jiaxing and Ye, Chenchao},
  doi          = {10.1007/s11554-024-01555-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {3D reconstruction method of welding area by fusion of coding raster and semantic segmentation network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted edge-based low-cost artifacts-free high-quality
VLSI implementation for demosaicking. <em>JRTIP</em>, <em>21</em>(5),
1–19. (<a href="https://doi.org/10.1007/s11554-024-01556-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayer color filter array (CFA) is the most used color filter pattern which is employed in digital cameras to capture images, necessitating a demosaicking process for generating complete RGB images. Furthermore, the cost associated with demosaicking must not exceed the savings realized using a CFA. This paper presents a low-cost hardware architecture that produces good image quality with reduced color artifacts in the reconstructed images. In the proposed method, Green interpolation is developed by computing weighted color differences and weighted edges in the horizontal and vertical directions of 5 $$\times$$ 7 mask size. Red and Blue channel information is obtained by computing color difference values from already computed Green values and corresponding pixel values from the Bayer image. Further, two extra line buffers are used to store the previously computed intermediate Green values, which eliminates the memory requirement to store those values. Compared with the existing methods, our method produces good-quality reconstructed images without color artifacts. The proposed method utilizes less hardware than the previous methods.},
  archive      = {J_JRTIP},
  author       = {Siva, Midde Venkata and Jayakumar, E. P.},
  doi          = {10.1007/s11554-024-01556-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Weighted edge-based low-cost artifacts-free high-quality VLSI implementation for demosaicking},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RDB-YOLOv8n: Insulator defect detection based on improved
lightweight YOLOv8n model. <em>JRTIP</em>, <em>21</em>(5), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01557-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulator defect detection is pivotal for the reliable functioning of power transmission and distribution networks. This paper introduces an optimized lightweight model for insulator defect detection, RDB-YOLOv8n, which addresses the limitations of existing models including high parameter counts, extensive computational demands, slow detection speeds, low accuracy, and challenges in deployment to embedded terminals. First, the RDB-YOLOv8n model employs a novel lightweight module, C2f_RBE, in its Backbone architecture. This module replaces conventional Bottlenecks with RepViTBlocks and SE modules with EMA attention mechanisms, significantly enhancing detection efficiency and performance. Secondly, the Neck of the model incorporates the C2f_DWFB module, which substitutes Bottlenecks with FasterBlocks and introduces depth-wise separable convolutions (DWConv) over standard convolutions to ensure accuracy and robustness in complex environments. Additionally, the integration of a BiFPN structure within the Neck network further reduces the parameters and computational load of the model. while simultaneously improving feature fusion capabilities and detection efficiency. Experimental results show that the enhanced RDB-YOLOv8n model achieves a 41.2% reduction in parameters and a decrease in GFLOPs from 8.1 to 7.1, with a model size reduction of 39.1% and an increase in mAP(0.5) by 1.7%, meeting the requirement of real-time and efficient accurate detection of insulator defects.},
  archive      = {J_JRTIP},
  author       = {Jiang, Yong and Wang, Shuai and Cao, Weifeng and Liang, Wanyong and Shi, Jun and Zhou, Lintao},
  doi          = {10.1007/s11554-024-01557-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RDB-YOLOv8n: Insulator defect detection based on improved lightweight YOLOv8n model},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated stenosis detection in coronary artery disease
using yolov9c: Enhanced efficiency and accuracy in real-time
applications. <em>JRTIP</em>, <em>21</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11554-024-01558-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) is a prevalent cardiovascular condition and a leading cause of mortality. An accurate and timely diagnosis of CAD is crucial for treatment. This study aims to detect stenosis in real-time and automatically during angiographic imaging for CAD diagnosis, using the YOLOv9c model. A dataset comprising 8325 grayscale images was utilized, sourced from 100 patients diagnosed with one-vessel CAD. To enhance sensitivity and accuracy during the training, testing, and validation phases of stenosis detection, fine-tuning and augmentations were applied. The Python API, utilizing YOLO and Ultralytics libraries, was employed for these processes. The analysis revealed that the YOLOv9c model achieved remarkably high performance in both processing speed and detection accuracy, with an F1-score of 0.99 and mAP@50 of 0.99. The inference time was reduced to 18 ms, fine-tuning time to 3.5 h, and training time to 11 h. When the same dataset was tested using another significant diagnostic algorithm, SSD MobileNet V1, the YOLOv9c model outperformed it by achieving 1.36 × better F1-score and 1.42 × better mAP@50. These results indicate that the developed YOLOv9c algorithm can provide highly accurate and real-time results for stenosis detection.},
  archive      = {J_JRTIP},
  author       = {Akgül, Muhammet and Kozan, Hasan İbrahim and Akyürek, Hasan Ali and Taşdemir, Şakir},
  doi          = {10.1007/s11554-024-01558-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Automated stenosis detection in coronary artery disease using yolov9c: Enhanced efficiency and accuracy in real-time applications},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mob-psp: Modified MobileNet-v2 network for real-time
detection of tomato diseases. <em>JRTIP</em>, <em>21</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01561-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomato, as an essential food crop, is consumed worldwide, and at the same time, it is susceptible to several diseases that lead to a reduction in tomato yield. Proper diagnosis of tomato diseases is required to increase the output of tomato crops. For this purpose, this paper proposes a tomato plant disease detection algorithm based on Pyramid Scene Parsing Network (PSPNet) and deep learning. First, the training data set is augmented with data to alleviate the data imbalance problem in each category, and then the augmented images are fed into the proposed Mob-PSP network for training. The proposed network utilizes the lightweight MobileNet-V2 model as the feature extraction technique while integrating the PSPNet module to enhance the network’s detection performance. The aim is to effectively extract local and global features from plant disease images, which are being introduced in plant disease detection. This study evaluated the model on the tomato subset of the public data set PlantVillage. The experimental results demonstrate that this algorithm achieves a balance between inference speed and detection accuracy, outperforming other state-of-the-art algorithms. Additionally, compared to the baseline model Inception-V3, the inference speed is improved by 10.73 frames per second, while maintaining an average accuracy of 99.69 $$\%$$ with only 6.5M parameters.},
  archive      = {J_JRTIP},
  author       = {Qiu, Hengmiao and Yang, Jingmin and Jiang, Juan and Zhang, Wenjie},
  doi          = {10.1007/s11554-024-01561-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Mob-psp: Modified MobileNet-v2 network for real-time detection of tomato diseases},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Realistic real-time processing of anime portraits based on
generative adversarial networks. <em>JRTIP</em>, <em>21</em>(4), 1–12.
(<a href="https://doi.org/10.1007/s11554-024-01481-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, more and more brands use interesting anime characters to promote and increase brand awareness. However, real-time and interesting promotional materials are also one of the key factors to attract people, so real-time processing of anime characters has also become an effective way to enhance brand awareness. In recent years, with the rapid development of deep learning, image style conversion based on AI technology has become a much-attended artificial intelligence application, but it also suffers from the disadvantages of complex model structure, slow conversion speed, and inconspicuous identity features, which need to be improved. In view of this, this study proposes an anime portrait realization (ARF-GAN) algorithm based on generative adversarial network. This algorithm is based on the Pix2Pix architecture and also uses the U-net network structure with jump connections to connect the encoder’s feature maps directly to the decoder, which in turn enables the network to reconstruct the output data in a better way and the network architecture is more lightweight.This algorithm is based on the Pix2Pix architecture and also uses the U-net network structure with jump connections to connect the encoder’s feature maps directly to the decoder, which in turn enables the network to reconstruct the output data in a better way and the network architecture is more lightweight. It also introduces the CBAM module, which can enhance the model’s expressive and generative capabilities without increasing the model’s complexity, and improve the model’s real-time image processing capability. In addition, this paper also compensates for the problem of image blurring brought by the original architecture by introducing the deblurring module. The experimental results on CartoonFaceAB, DanBooru show that the proposed ARF-GAN has a better generative effect for the task of realizing anime portraits. And comparing with different types of image generating models, it shows better accuracy and lower complexity by using several evaluation indexes such as PSNR and SSIM. This makes it more suitable for brand advertisement promotion, so it has good application value for improving brand awareness.},
  archive      = {J_JRTIP},
  author       = {Zhu, Gaofeng and Qu, Zhiguo and Sun, Le and Liu, Yuming and Yang, Jianfeng},
  doi          = {10.1007/s11554-024-01481-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Realistic real-time processing of anime portraits based on generative adversarial networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARF-YOLOv8: A novel real-time object detection model for
UAV-captured images detection. <em>JRTIP</em>, <em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01483-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are several difficulties in the task of object detection for Unmanned Aerial Vehicle (UAV) photography images, including the small size of objects, densely distributed objects, and diverse perspectives from which the objects are captured. To tackle these challenges, we proposed a real-time algorithm named adjusting overall receptive field enhancement YOLOv8 (ARF-YOLOv8) for object detection in UAV-captured images. Our approach begins with a comprehensive restructuring of the YOLOv8 network architecture. The primary objectives are to mitigate the loss of shallow-level information and establish an optimal model receptive field. Subsequently, we designed a bibranch fusion attention module based on Coordinate Attention which is seamlessly integrated into the detection network. This module combines features processed by Coordinate Attention module with shallow-level features, facilitating the extraction of multi-level feature information. Furthermore, recognizing the influence of target size on boundary box loss, we refine the boundary box loss function CIoU Loss employed in YOLOv8. Extensive experimentation conducted on the visdrone2019 dataset provides empirical evidence supporting the superior performance of ARF-YOLOv8. In comparison to YOLOv8, our method demonstrates a noteworthy 6.86% increase in mAP (0.5:0.95) while maintaining similar detection speeds. The code is available at https://github.com/sbzeng/ARF-YOLOv8-for-uav/tree/main .},
  archive      = {J_JRTIP},
  author       = {Zeng, YaLin and Guo, DongJin and He, WeiKai and Zhang, Tian and Liu, ZhongTao},
  doi          = {10.1007/s11554-024-01483-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ARF-YOLOv8: A novel real-time object detection model for UAV-captured images detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hardware-friendly logarithmic quantization method for CNNs
and FPGA implementation. <em>JRTIP</em>, <em>21</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01484-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have been widely used in various fields due to their high accuracy and efficiency. The performance of CNNs is mainly affected by the computing capability, memory bandwidth, and flexibility of embedded devices. The high energy efficiency, computing capability, and reconfigurability of FPGAs make it a good platform for hardware acceleration in the design of CNNs. However, the increase of complexity of CNNs, requires memory while the FPGA on-chip storage is limited. Therefore, we use an improved logarithmic quantization to compress the model. This approach allows for significant reduction in bit widths while maintaining high accuracy levels, making it an effective compression method. In this work, a hardware-friendly quantization scheme is proposed, in which the weights use improved logarithmic quantization scheme, and the quantization scheme of activations use the fixed-point-to-logarithmic. The results show that the quantization model has negligible Top-1/5 accuracy loss without any retraining. In addition, we implement an acceleration engine for a heterogeneous Generalized Matrix Multiplication (GEMM) core on Zynq XC7Z020. In GEMM, the multiplier is replaced by logic shifters and adders, which achieves efficient utilization of LUT resources. We use the optimal quantization model on Zynq XC7Z020. The throughput reaches 69.7 GOPs with a power consumption of 6.008W, and the resource efficiency is 8.713 GOPs/DSP or 5.564 GOPs/kLUTs.},
  archive      = {J_JRTIP},
  author       = {Jiang, Tao and Xing, Ligang and Yu, Jinming and Qian, Junchao},
  doi          = {10.1007/s11554-024-01484-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A hardware-friendly logarithmic quantization method for CNNs and FPGA implementation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightYOLO-s: A lightweight algorithm for detecting small
targets. <em>JRTIP</em>, <em>21</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01485-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small target detection tasks are the focus and difficulty of target detection tasks. Methods to improve detection accuracy are often accompanied by drawbacks, such as a high number of parameters, computational effort, and latency. Therefore, this paper proposes the LightYOLO-S target detection algorithm based on YOLOv8s, which achieves high accuracy in small target detection tasks. First, the proposed LightC2f module reduces the overall number of parameters, computation, and inference time of the model while maintaining the same plug-and-play characteristics as the C2f module. Second, the proposed Wise-DIoU loss function, which speeds up model convergence and improves accuracy without increasing the number of parameters or computation. Third, the proposed Dynamic Sampler counts the IoU scores and classification scores of the screened training samples to adjust the sample allocation function, so that the model can obtain the best training samples during training.The results of experimental studies on the VisDrone2019 UAV aerial photography dataset and the DOTA remote sensing dataset indicate that LightYOLO-S exhibits greater accuracy and faster operation speed than the current state-of-the-art detection algorithms.},
  archive      = {J_JRTIP},
  author       = {Zihan, Liu and xu, Wu and Linyun, Zhang and Panlin, Yu},
  doi          = {10.1007/s11554-024-01485-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LightYOLO-S: A lightweight algorithm for detecting small targets},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiently adapting large pre-trained models for real-time
violence recognition in smart city surveillance. <em>JRTIP</em>,
<em>21</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01486-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the concept of smart cities has gained prominence, aiming to enhance urban efficiency, safety, and quality of life through advanced technologies. A critical component of this infrastructure is the extensive use of surveillance systems to monitor public spaces for violent behavior detection. As the scale of data and models grows, large-scale pre-trained models demonstrate remarkable capabilities across a wide range of applications. However, adapting these models for violence recognition in surveillance videos poses several challenges, including the fine-tuning cost, lack of temporal modeling, and inference overhead. In this paper, we propose an efficient recognition framework to adapt pre-trained models for violence behavior recognition, which consists of two paths, named spatial path and motion path. Our proposed framework allows for real-time parameter updating and real-time inference, which is adaptable to various ViT-based pre-trained models. Both paths adopt the pipeline of parameter-efficient fine-tuning to ensure the real-time performance of the model updating. What’s more, within the motion path, as multiple frames need to be processed to capture temporal features, the real-time performance of the model is a challenge. Considering this, to improve the efficiency of inference, we compress multiple frames into the size of a single standard image, ensuring the real-time performance of inference. Experiments on five datasets demonstrate that our framework achieves state-of-the-art performance, efficiently transferring pre-trained large models to violence behavior recognition.},
  archive      = {J_JRTIP},
  author       = {Ren, Xiaohui and Fan, Wenze and Wang, Yinghao},
  doi          = {10.1007/s11554-024-01486-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficiently adapting large pre-trained models for real-time violence recognition in smart city surveillance},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fcd-cnn: FPGA-based CU depth decision for HEVC intra encoder
using CNN. <em>JRTIP</em>, <em>21</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01487-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video compression for storage and transmission has always been a focal point for researchers in the field of image processing. Their efforts aim to reduce the data volume required for video representation while maintaining its quality. HEVC is one of the efficient standards for video compression, receiving special attention due to the increasing demand for high-resolution videos. The main step in video compression involves dividing the coding unit (CU) blocks into smaller blocks that have a uniform texture. In traditional methods, The Discrete Cosine Transform (DCT) is applied, followed by the use of RDO for decision-making on partitioning. This paper presents a novel convolutional neural network (CNN) and its hardware implementation as an alternative to DCT, aimed at speeding up partitioning and reducing the hardware resources required. The proposed hardware utilizes an efficient and lightweight CNN to partition CUs with low hardware resources in real-time applications. This CNN is trained for different Quantization Parameters (QPs) and block sizes to prevent overfitting. Furthermore, the system’s input size is fixed at $$16\times 16$$ , and other input sizes are scaled to this dimension. Loop unrolling, data reuse, and resource sharing are applied in hardware implementation to save resources. The hardware architecture is fixed for all block sizes and QPs, and only the coefficients of the CNN are changed. In terms of compression quality, the proposed hardware achieves a $$4.42\%$$ BD-BR and $$-\,0.19$$ BD-PSNR compared to HM16.5. The proposed system can process $$64\times 64$$ CU at 150 MHz and in 4914 clock cycles. The hardware resources utilized by the proposed system include 13,141 LUTs, 15,885 Flip-flops, 51 BRAMs, and 74 DSPs.},
  archive      = {J_JRTIP},
  author       = {Dehnavi, Hossein and Dehnavi, Mohammad and Klidbary, Sajad Haghzad},
  doi          = {10.1007/s11554-024-01487-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fcd-cnn: FPGA-based CU depth decision for HEVC intra encoder using CNN},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IoT-based real-time object detection system for crop
protection and agriculture field security. <em>JRTIP</em>,
<em>21</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s11554-024-01488-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In farming, clashes between humans and animals create significant challenges, risking crop yields, human well-being, and resource depletion. Farmers use traditional methods like electric fences to protect their fields but these can harm essential animals that maintain a balanced ecosystem. To address these fundamental challenges, our research presents a fresh solution harnessing the power of the Internet of Things (IoT) and deep learning. In this paper, we developed a monitoring system that takes advantage of ESP32-CAM and Raspberry Pi in collaboration with optimised YOLOv8 model. Our objective is to detect and classify objects such as animals or humans that roam around the field, providing real-time notification to the farmers by incorporating firebase cloud messaging (FCM). Initially, we have employed ultrasonic sensors that will detect any intruder movement, triggering the camera to capture an image. Further, the captured image is transmitted to a server equipped with an object detection model. Afterwards, the processed image is forwarded to FCM, responsible for managing the image and sending notifications to the farmer through an Android application. Our optimised YOLOv8 model attains an exceptional precision of 97%, recall of 96%, and accuracy of 96%. Once we achieved this optimal outcome, we integrated the model with our IoT infrastructure. This study emphasizes the effectiveness of low-power IoT devices, LoRa devices, and object detection techniques in delivering strong security solutions to the agriculture industry. These technologies hold the potential to significantly decrease crop damage while enhancing safety within the agricultural field and contribute towards wildlife conservation.},
  archive      = {J_JRTIP},
  author       = {Singh, Priya and Krishnamurthi, Rajalakshmi},
  doi          = {10.1007/s11554-024-01488-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {IoT-based real-time object detection system for crop protection and agriculture field security},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction: Driver fatigue detection based on improved
YOLOv7. <em>JRTIP</em>, <em>21</em>(4), 1. (<a
href="https://doi.org/10.1007/s11554-024-01489-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Li, Xianguo and Li, Xueyan and Shen, Zhenqian and Qian, Guangmin},
  doi          = {10.1007/s11554-024-01489-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction: Driver fatigue detection based on improved YOLOv7},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards real-time video analysis of flooded areas:
Redundancy-based accelerator for object detection models.
<em>JRTIP</em>, <em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01490-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state of Kerala in India has seen multiple instances of intense cyclones in recent years, resulting in heavy flooding. One of the biggest challenges faced by rescuers is the accessibility to flooded areas and buildings during rescue operations. In such scenarios, unmanned aerial vehicles (UAVs) can deliver reliable aerial visual data to aid planning and operations during rescue. Object detectors based on deep learning methods provide an effective solution to automate the process of detecting relevant information from image/video data. These models are complex and resource-hungry, leading to severe speed constraints during field operations. The pixel displacement algorithm (PDA), a portable and effective technique, is developed in this work to speed up object detection models on devices with limited resources, such as edge devices. This method can be integrated with all object detection models to speed up the inference time. The proposed method is combined with multiple object detection models in this work to show its effectiveness. The YOLOv4 model combined with the proposed method outperformed the AP50 performance of the YOLOv4-tiny model by 6 $$\%$$ while maintaining the same processing time. This approach gave almost 10 $$\times $$ speed improvement to Jetson Nano at an accuracy cost of $$3\%$$ when compared to YOLOv4. Further, a model to predict maximum pixel shift with respect to frame skip is proposed using parameters such as the altitude and velocity of the UAV and the tilt of the camera. Accurate prediction of pixel shift leads to a reduced search area, leading to reduced inference time. The effectiveness of the proposed model was tested against annotated locations, and it was found that the method was able to predict the search area for each test video segment with a high degree of accuracy.},
  archive      = {J_JRTIP},
  author       = {AV, Shubhasree and Sankaran, Praveen and C.V, Raghu},
  doi          = {10.1007/s11554-024-01490-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Towards real-time video analysis of flooded areas: Redundancy-based accelerator for object detection models},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selfredepth. <em>JRTIP</em>, <em>21</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01491-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources. Data-driven denoising algorithms can mitigate such problems; however, they require vast amounts of ground truth depth data. Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors. Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest highlighting a need for methods that can effectively denoise depth maps in real-time dynamic environments. This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting of full-depth maps captured with RGB-D sensors. The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence. Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms. Our results demonstrate our approach’s real-time performance on real-world datasets shows that it outperforms state-of-the-art methods in denoising and restoration performance at over 30 fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications.},
  archive      = {J_JRTIP},
  author       = {Duarte, Alexandre and Fernandes, Francisco and Pereira, João M. and Moreira, Catarina and Nascimento, Jacinto C. and Jorge, Joaquim},
  doi          = {10.1007/s11554-024-01491-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Selfredepth},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOv8n-LSLW: A lightweight method for real-time detection
of wild fishing behavior. <em>JRTIP</em>, <em>21</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01492-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the electric power industry, the laying of transmission lines covers various waters, which poses a great threat to the life safety of fishermen who intrude into high-voltage areas. To address this issue, this paper proposes a lightweight unmanned aerial vehicle (UAV) inspection algorithm. Firstly, in view of the diversity and complexity of the field environment, a complex targeted data augmentation method and an adaptive histogram equalization (AHE) method were designed. Secondly, the YOLOv8n algorithm is employed, with the design of C2f-Ghost modules and GhostConv modules to construct the light-back bone layer, aimed at improving detection accuracy and speed. Subsequently, improvements are made to the small target detection layer, and a lightweight Light Bi-directional Feature Pyramid Network (Light-BiFPN) structure is proposed. Finally, the Wise Intersection over Union (WIoU) loss function is introduced to enhance the quality of model anchor boxes. Experimental results demonstrate that the improved algorithm achieves good detection accuracy with smaller weight files, making it suitable for deployment on mobile devices and also performs well on the VisDrone2019 dataset. This algorithm plays a proactive role in safeguarding the safety of fishermen and ensuring the stable operation of power systems.},
  archive      = {J_JRTIP},
  author       = {Yan, Pengcheng and Wang, Wenchang and Li, Guodong and Zhao, Yuting and Wang, Jingbao and Wen, Ziming},
  doi          = {10.1007/s11554-024-01492-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLOv8n-LSLW: A lightweight method for real-time detection of wild fishing behavior},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU-based key-frame selection of pulmonary ultrasound images
to detect COVID-19. <em>JRTIP</em>, <em>21</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01493-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, technological advances have led to a considerable increase in computing power constraints to simulate complex phenomena in various application fields, among which are climate, physics, genomics and medical diagnosis. Often, accurate results in real time, or quasi real time, are needed, especially if related to a process requiring rapid interventions. To deal with such demands, more sophisticated approaches have been designed, including GPUs, multicore processors and hardware accelerators. Supercomputers manage high amounts of data at a very high speed; however, despite their considerable performance, their limitations are due to maintenance costs, rapid obsolescence and notable energy consumption. New processing architectures and GPUs in the medical field can provide diagnostic and therapeutic support whenever the patient is subject to risk. In this context, image processing as an aid to diagnosis, in particular pulmonary ultrasound to detect COVID-19, represents a promising diagnostic tool with the ability to discriminate between different degrees of disease. This technique has several advantages, such as no radiation exposure, low costs, the availability of follow-up tests and the ease of use even with limited resources. This work aims to identify the best approach to optimize and parallelize the selection of the most significant frames of a video which is given as the input to the classification network that will differentiate between healthy and COVID patients. Three approaches have been evaluated: histogram, entropy and ResNet-50, followed by a K-means clustering. Results highlight the third approach as the most accurate, simultaneously showing GPUs significantly lowering all processing times.},
  archive      = {J_JRTIP},
  author       = {Torti, Emanuele and Gazzoni, Marco and Marenzi, Elisa and Leporati, Francesco},
  doi          = {10.1007/s11554-024-01493-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU-based key-frame selection of pulmonary ultrasound images to detect COVID-19},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GST-YOLO: A lightweight visual detection algorithm for
underwater garbage detection. <em>JRTIP</em>, <em>21</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01494-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater cleaning work primarily relies on human labor, but applying computer vision technology to Autonomous Underwater Vehicles can enhance cleaning efficiency. Considering that existing vision detection algorithms are difficult to deploy on resource-constrained embedded devices, this paper introduces a lightweight vision detection algorithm based on an improved YOLOv8-GST-YOLO. This algorithm integrates the lightweight Ghost network and prunes the model, overcoming the drawbacks of YOLOv8’s high computational parameters and large size. It also features a GTR module and a bi-directional path aggregated feature pyramid guided by SimAM attention to enhance detection accuracy and global feature extraction capabilities. Experiments on a specially collected underwater trash image dataset show that GST-YOLO, while reducing the model size by 51% and increasing computational efficiency by 54%, improves the accuracy rate to 95.4%, surpassing the YOLOv8 algorithm. This demonstrates its potential as a crucial detection tool for underwater unmanned cleaning tasks, offering broad application prospects.},
  archive      = {J_JRTIP},
  author       = {Jiang, Longyi and Liu, Fanghua and Lv, Junwei and Liu, Binghua and Wang, Chen},
  doi          = {10.1007/s11554-024-01494-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GST-YOLO: A lightweight visual detection algorithm for underwater garbage detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning based insulator fault detection algorithm for
power transmission lines. <em>JRTIP</em>, <em>21</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01495-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the complex background of transmission lines at the present stage, which leads to the problem of low accuracy of insulator fault detection for small targets, a deep learning-based insulator fault detection algorithm for transmission lines is proposed. First, aerial images of insulators are collected using UAVs in different scenarios to establish insulator fault datasets. After that, in order to improve the detection efficiency of the target detection algorithm, certain improvements are made on the basis of the YOLOV9 algorithm. The improved algorithm enhances the feature extraction capability of the algorithm for insulator faults at a smaller computational cost by adding the GAM attention mechanism; at the same time, in order to realize the detection efficiency of small targets for insulator faults, the generalized efficient layer aggregation network (GELAN) module is improved and a new SC-GELAN module is proposed; the original loss function is replaced by the effective intersection-over-union (EIOU) loss function to minimize the difference between the aspect ratio of the predicted frame and the real frame, thereby accelerating the convergence speed of the model. Finally, the proposed algorithm is trained and tested with other target detection algorithms on the established insulator fault dataset. The experimental results and analysis show that the algorithm in this paper ensures a certain detection speed, while the algorithmic model has a higher detection accuracy, which is more suitable for UAV fault detection of insulators on transmission lines.},
  archive      = {J_JRTIP},
  author       = {Wang, Han and Yang, Qing and Zhang, Binlin and Gao, Dexin},
  doi          = {10.1007/s11554-024-01495-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning based insulator fault detection algorithm for power transmission lines},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic deep learning architecture optimization method for
edge device based on start-up latency reduction. <em>JRTIP</em>,
<em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01496-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the promising Artificial Intelligence of Things technology, deep learning algorithms are implemented on edge devices to process data locally. However, high-performance deep learning algorithms are accompanied by increased computation and parameter storage costs, leading to difficulties in implementing huge deep learning algorithms on memory and power constrained edge devices, such as smartphones and drones. Thus various compression methods are proposed, such as channel pruning. According to the analysis of low-level operations on edge devices, existing channel pruning methods have limited effect on latency optimization. Due to data processing operations, the pruned residual blocks still result in significant latency, which hinders real-time processing of CNNs on edge devices. Hence, we propose a generic deep learning architecture optimization method to achieve further acceleration on edge devices. The network is optimized in two stages, Global Constraint and Start-up Latency Reduction, and pruning of both channels and residual blocks is achieved. Optimized networks are evaluated on desktop CPU, FPGA, ARM CPU, and PULP platforms. The experimental results show that the latency is reduced by up to 70.40%, which is 13.63% higher than only applying channel pruning and achieving real-time processing in the edge device.},
  archive      = {J_JRTIP},
  author       = {Li, Qi and Li, Hengyi and Meng, Lin},
  doi          = {10.1007/s11554-024-01496-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A generic deep learning architecture optimization method for edge device based on start-up latency reduction},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rtsds: A real-time and efficient method for detecting
surface defects in strip steel. <em>JRTIP</em>, <em>21</em>(4), 1–13.
(<a href="https://doi.org/10.1007/s11554-024-01497-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of varying defect sizes, inconsistent data quality, and real-time detection challenges in steel defect detection, we propose a real-time efficient steel defect detection network (RTSD). This model employs a multi-scale feature extraction module (MSC3) and a mid-sized object detector (MidObj) to comprehensively capture texture features of defects across different scales. We incorporate a coordinate attention module (CA) and replace the spatial pyramid pooling structure (SPPF) to enhance defect localization capabilities. Additionally, we introduce the Wise-IoU (WIoU) loss function to balance attention to various quality defects. To address the real-time detection issue, we use Taylor channel pruning to reduce model complexity and employ channel-wise knowledge distillation instead of fine-tuning to mitigate the negative impacts of pruning. Experimental results show that on the NEU-DET data set, the average precision of RTSD reaches 83.5%. The model parameters, calculation amount, and size are 5.9M, 7.9 GFLOPs, and 11.9M, respectively, with an inference speed of up to 247.6 FPS. This demonstrates that our method can enhance performance while significantly reducing model complexity and computational overhead, offering a highly practical solution for industrial applications.},
  archive      = {J_JRTIP},
  author       = {Zeng, Qingtian and Wei, Daibai and Zou, Minghao},
  doi          = {10.1007/s11554-024-01497-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rtsds: A real-time and efficient method for detecting surface defects in strip steel},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel pipelined architecture of entropy filter.
<em>JRTIP</em>, <em>21</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01498-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, entropy is a measure adopted to characterize the texture information of a grayscale image, and an entropy filter is a fundamental operation used to calculate local entropy. However, this filter is computationally intensive and demands an efficient means of implementation. Additionally, with the foreseeable end of Moore’s law, there is a growing trend towards hardware offloading to increase computing power. In line with this trend, we propose a novel method for the calculation of local entropy and introduce a corresponding pipelined architecture. Under the proposed method, a sliding window of pixels undergoes three steps: sorting, adjacent difference calculation, and pipelined entropy calculation. Compared with a conventional design, implementation results on a Zynq UltraScale+ XCZU7EV-2FFVC1156 MPSoC device demonstrate that our pipelined architecture can reach a maximum throughput of handling 764.526 megapixels per second while achieving $$2.4\times$$ and $$2.9\times$$ reductions in resource utilization and $$1.1\times$$ reduction in power consumption.},
  archive      = {J_JRTIP},
  author       = {Ngo, Dat and Kang, Bongsoon},
  doi          = {10.1007/s11554-024-01498-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel pipelined architecture of entropy filter},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight safety helmet detection algorithm using improved
YOLOv5. <em>JRTIP</em>, <em>21</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01499-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the challenges faced by existing safety helmet detection algorithms when applied to complex construction site scenarios, such as poor accuracy, large number of parameters, large amount of computation and large model size, this paper proposes a lightweight safety helmet detection algorithm based on YOLOv5, which achieves a balance between lightweight and accuracy. First, the algorithm integrates the Distribution Shifting Convolution (DSConv) layer and the Squeeze-and-Excitation (SE) attention mechanism, effectively replacing the original partial convolution and C3 modules, this integration significantly enhances the capabilities of feature extraction and representation learning. Second, multi-scale feature fusion is performed on the Ghost module using skip connections, replacing certain C3 module, to achieve lightweight and maintain accuracy. Finally, adjustments have been made to the Bottleneck Attention Mechanism (BAM) to suppress irrelevant information and enhance the extraction of features in rich regions. The experimental results show that improved model improves the mean average precision (mAP) by 1.0% compared to the original algorithm, reduces the number of parameters by 22.2%, decreases the computation by 20.9%, and the model size is reduced by 20.1%, which realizes the lightweight of the detection algorithm.},
  archive      = {J_JRTIP},
  author       = {Ren, Hongge and Fan, Anni and Zhao, Jian and Song, Hairui and Liang, Xiuman},
  doi          = {10.1007/s11554-024-01499-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight safety helmet detection algorithm using improved YOLOv5},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TeleStroke: Real-time stroke detection with federated
learning and YOLOv8 on edge devices. <em>JRTIP</em>, <em>21</em>(4),
1–16. (<a href="https://doi.org/10.1007/s11554-024-01500-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke, a life-threatening medical condition, necessitates immediate intervention for optimal outcomes. Timely diagnosis and treatment play a crucial role in reducing mortality and minimizing long-term disabilities associated with strokes. This study presents a novel approach to meet these critical needs by proposing a real-time stroke detection system based on deep learning (DL) with utilization of federated learning (FL) to enhance accuracy and privacy preservation. The primary objective of this research is to develop an efficient and accurate model capable of discerning between stroke and non-stroke cases in real-time, facilitating healthcare professionals in making well-informed decisions. Traditional stroke detection methods relying on manual interpretation of medical images are time-consuming and prone to human error. DL techniques have shown promise in automating this process, yet challenges persist due to the need for extensive and diverse datasets and privacy concerns. To address these challenges, our methodology involves utilization and assessing YOLOv8 models on comprehensive datasets comprising both stroke and non-stroke based on the facial paralysis of the individuals from the images. This training process empowers the model to grasp intricate patterns and features associated with strokes, thereby enhancing its diagnostic accuracy. In addition, federated learning, a decentralized training approach, is employed to bolster privacy while preserving model performance. This approach enables the model to learn from data distributed across various clients without compromising sensitive patient information. The proposed methodology has been implemented on NVIDIA platforms, utilizing their advanced GPU capabilities to enable real-time processing and analysis. This optimized model has the potential to revolutionize stroke diagnosis and patient care, promising to save lives and elevate the quality of healthcare services in the neurology field.},
  archive      = {J_JRTIP},
  author       = {Elhanashi, Abdussalam and Dini, Pierpaolo and Saponara, Sergio and Zheng, Qinghe},
  doi          = {10.1007/s11554-024-01500-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {TeleStroke: Real-time stroke detection with federated learning and YOLOv8 on edge devices},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety helmet detection based on improved YOLOv7-tiny with
multiple feature enhancement. <em>JRTIP</em>, <em>21</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01501-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety helmets are vital protective gear for construction workers, effectively reducing head injuries and safeguarding lives. By identification of safety helmet usage, workers’ unsafe behaviors can be detected and corrected in a timely manner, reducing the possibility of accidents. Target detection methods based on computer vision can achieve fast and accurate detection regarding the wearing habits of safety helmets of workers. In this study, we propose a real-time construction-site helmet detection algorithm that improves YOLOv7-tiny to address the problems associated with automatically identifying construction-site helmets. First, the Efficient Multi-scale Attention (EMA) module is introduced at the trunk to capture the detailed information; here, the model is more focused on training to recognize the helmet-related target features. Second, the detection head is replaced with a self-attentive Dynamic Head (DyHead) for stronger feature representation. Finally, Wise-IoU (WIoU) with a dynamic nonmonotonic focusing mechanism is used as a loss function to improve the model’s ability to manage the situation of mutual occlusion between workers and enhance the detection performance. The experimental results show that the improved YOLOv7-tiny algorithm model yields 3.3, 1.5, and 5.6% improvements in the evaluation of indices of mAP@0.5, precision, and recall, respectively, while maintaining its lightweight features; this enables more accurate detection with a suitable detection speed and is more in conjunction with the needs of on-site-automated detection.},
  archive      = {J_JRTIP},
  author       = {Wang, Shuqiang and Wu, Peiyang and Wu, Qingqing},
  doi          = {10.1007/s11554-024-01501-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Safety helmet detection based on improved YOLOv7-tiny with multiple feature enhancement},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastBeltNet: A dual-branch light-weight network for
real-time conveyor belt edge detection. <em>JRTIP</em>, <em>21</em>(4),
1–11. (<a href="https://doi.org/10.1007/s11554-024-01502-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belt conveyors are widely used in multiple industries, including coal, steel, port, power, metallurgy, and chemical, etc. One major challenge faced by these industries is belt deviation, which can negatively impact production efficiency and safety. Despite previous research on improving belt edge detection accuracy, there is still a need to prioritize system efficiency and light-weight models for practical industrial applications. To meet this need, a new semantic segmentation network called FastBeltNet has been developed specifically for real-time and highly accurate conveyor belt edge line segmentation while maintaining a light-weight design. This network uses a dual-branch structure that combines a shallow spatial branch for extracting high-resolution spatial information with a context branch for deep contextual semantic information. It also incorporates the Ghost blocks, Downsample blocks, and Input Injection blocks to reduce computational load, increase processing frame rate, and enhance feature representation. Experimental results have shown that FastBeltNet has performed comparatively better than some existing methods in different real-world production settings, achieving promising performance metrics. Specifically, FastBeltNet achieves 80.49% mIoU accuracy, 99.89 FPS processing speed, 895 k parameters, 8.23 GFLOPs, and 430.95 MB peak CUDA memory use, effectively balancing accuracy and speed for industrial production.},
  archive      = {J_JRTIP},
  author       = {Zhao, Xing and Zeng, Minhao and Dong, Yanglin and Rao, Gang and Huang, Xianshan and Mo, Xutao},
  doi          = {10.1007/s11554-024-01502-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FastBeltNet: A dual-branch light-weight network for real-time conveyor belt edge detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time detection and geometric analysis algorithm for
concrete cracks based on the improved u-net model. <em>JRTIP</em>,
<em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01503-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at complex operation problems, low precision and poor robustness of traditional concrete crack detection methods, a real-time concrete crack detection and geometric analysis algorithm based on the improved U-net model is proposed. First, the efficient channel attention (ECA) module is embedded in the U-net model to reduce the loss of target information. The DenseNet network is used instead of the VGG16 network in the U-net basic model architecture, making transmitting features and gradients more effective. Then, based on the improved U-net model, the concrete crack detection experiment is performed. The experimental results indicate that the improved U-net model has 91.56% pixel accuracy (PA), 80.12% mean intersection over union (mIoU), 84.89% recall and 88.10% F1_score. The mIoU, PA, recall and F1_score of the improved U-net model increased by 17.39%, 7.82%, 2.62% and 5.10%, respectively, compared with the original model. Next, the real-time detection experiment of concrete cracks is performed based on the improved U-net model. The FPS of the improved model is the same as that of the original model and reaches 42. Finally, the geometric analysis of concrete cracks is performed based on the detection results of the improved U-net model. The area, density, length and average width information of concrete cracks are effectively extracted. The research results indicate that the detection effect of this study’s model on concrete cracks is considerably improved and that the model has good robustness. The model proposed in this study can achieve intelligent real-time and accurate identification of concrete cracks, which has broad application prospects.},
  archive      = {J_JRTIP},
  author       = {Zhang, Qian and Zhang, Fan and Liu, Hongbo and Wang, Longxuan and Chen, Zhihua and Guo, Liulu},
  doi          = {10.1007/s11554-024-01503-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection and geometric analysis algorithm for concrete cracks based on the improved U-net model},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-FGD: A fast lightweight PCB defect method based on
FasterNet and the gather-and-distribute mechanism. <em>JRTIP</em>,
<em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01504-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the electronics industry, the demand for high-quality printed circuit boards has surged. However, existing PCB defect detection methods suffer from various limitations, such as slow speeds, low accuracy, and restricted detection scope, often leading to false positives and negatives. To overcome these challenges, this paper presents YOLO-FGD, a novel detection model. YOLO-FGD replaces YOLOv5’s backbone network with FasterNet, significantly accelerating feature extraction. The Neck section adopts the Gather-and-Distribute mechanism, which enhances multiscale feature fusion for small targets through convolution and self-attention mechanisms. Integration of the C3_Faster feature extraction module effectively reduces the number of parameters and the number of FLOPs, accelerating the computations. Experiments on the PCB-DATASETS dataset show promising results: the mean average precision50 reaches 98.8%, the mean average precision50–95 reaches 57.2%, the computational load is reduced to 11.5 GFLOPs, and the model size is only 12.6 MB, meeting lightweight standards. These findings underscore the effectiveness of YOLO-FGD in efficiently detecting PCB defects, providing robust support for electronic manufacturing quality control.},
  archive      = {J_JRTIP},
  author       = {Qin, Changxin and Zhou, Zhongyu},
  doi          = {10.1007/s11554-024-01504-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-FGD: A fast lightweight PCB defect method based on FasterNet and the gather-and-distribute mechanism},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight YOLOv8 based on attention mechanism for mango
pest and disease detection. <em>JRTIP</em>, <em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01505-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because the growth of mangoes is often affected by pests and diseases, the application of object detection technology can effectively solve this problem. However, deploying object detection models on mobile devices is challenging due to resource constraints and high-efficiency requirements. To address this issue, we reduced the parameters in the target detection model, facilitating its deployment on mobile devices to detect mango pests and diseases. This study introduced the improved lightweight target detection model GAS-YOLOv8. The model’s performance was improved through the following three modifications. First, the model backbone was replaced with GhostHGNetv2, significantly reducing the model parameters. Second, the lightweight detection head AsDDet was adopted to further decrease the parameters. Finally, to increase the detection accuracy of the lightweight model without significantly increasing parameters, the C2f module was replaced with the C2f-SE module. Validation with a publicly available dataset of mango pests and diseases showed that the accuracy for insect pests increased from 97.1 to 98.6%, the accuracy for diseases increased from 91.4 to 91.7%, and the model parameters decreased by 33%. This demonstrates that the GAS-YOLOv8 model effectively addresses the issues of large computational volume and challenging deployment for the detection of mango pests and diseases.},
  archive      = {J_JRTIP},
  author       = {Wang, Jiao and Wang, Junping},
  doi          = {10.1007/s11554-024-01505-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight YOLOv8 based on attention mechanism for mango pest and disease detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel single kernel parallel image encryption scheme based
on a chaotic map. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01506-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of communication technologies has increased concerns about data security, increasing the prominence of cryptography. Images are one of the most widely shared data, and chaotic ciphers arouse significant interest from researchers, as traditional ciphers are not optimized for image encryption. Chaotic encryption schemes perform well for low-quality images but should be faster for real-time encryption of Full Ultra HD images. In this context, a novel parallel image cipher scheme is proposed to execute in GPU architectures, where the encryption procedure consists of a single kernel, making it different from previous chaotic ciphers and the AES. This new contribution enables our work to achieve a throughput of 130.8 GB/s on a GeForce RTX3070 and 251.6 GB/s on a Tesla V100 GPU, an increase of 37% compared to the AES and 43 times higher than the previously high for chaotic ciphers. The cipher’s security is also verified regarding multiple forms of attacks.},
  archive      = {J_JRTIP},
  author       = {Bezerra, Joao Inacio Moreira and Molter, Alexandre and Machado, Gustavo and Soares, Rafael Iankowski and Camargo, Vinícius Valduga de Almeida},
  doi          = {10.1007/s11554-024-01506-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel single kernel parallel image encryption scheme based on a chaotic map},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved multi-scale and knowledge distillation method
for efficient pedestrian detection in dense scenes. <em>JRTIP</em>,
<em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01507-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection in densely populated scenes, particularly in the presence of occlusions, remains a challenging issue in computer vision. Existing approaches often address detection leakage by enhancing model architectures or incorporating attention mechanisms; However, small-scale pedestrians have fewer features and are easily overfitted to the dataset and these approaches still face challenges in accurately detecting pedestrians with small target sizes. To tackle this issue, this research rethinks the occlusion region through small-scale pedestrian detection and proposes the You Only Look Once model for efficient pedestrian detection(YOLO-EPD). Firstly, we find that Standard Convolution and Dilated Convolution do not fit well with pedestrian targets with different scales due to a single receptive field, and we propose the Selective Content Aware Downsampling (SCAD) module, which is integrated into the backbone to attain enhanced feature extraction. In addition, to address the issue of missed detections resulting from insufficient feature extraction for small-scale pedestrian detection, we propose the Crowded Multi-Head Attention (CMHA) module, which makes full use of multi-layer information. Finally, for the challenge of optimizing the performance and effectiveness of small-object detection, we design Unified Channel-Task Distillation (UCTD) with channel attention and a Lightweight head (Lhead) using parameter sharing to keep it lightweight. Experimental results validate the superiority of YOLO-EPD, achieving a remarkable 91.1% Average Precision (AP) on the Widerperson dataset, while concurrently reducing parameters and computational overhead by 40%. The experimental findings demonstrate that YOLO-EPD greatly accelerates the convergence of model training and achieves better real-time performance in real-world dense scenarios.},
  archive      = {J_JRTIP},
  author       = {Xu, Yanxiang and Wen, Mi and He, Wei and Wang, Hongwei and Xue, Yunsheng},
  doi          = {10.1007/s11554-024-01507-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved multi-scale and knowledge distillation method for efficient pedestrian detection in dense scenes},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time and secure identity authentication transmission
mechanism for artificial intelligence generated image content.
<em>JRTIP</em>, <em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01508-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of generative artificial intelligence technology and large-scale pre-training models has led to the emergence of artificial intelligence generated image content (AIGIC) as an important application of natural language processing models. This has resulted in a significant shift and advancement in the way image content is created. As AIGIC requires the acquisition of substantial image datasets from user devices for training purposes, the data transmission link is highly complex, and the datasets are susceptible to illegal attacks from multiple parties during transmission, which has a detrimental impact on the integrity and real-time nature of the training data and affects the accuracy of the training results of the AIGIC model. Consequently, this paper proposed a real-time authentication mechanism to guarantee the secure transmission of AIGIC image datasets. The mechanism achieves anonymous identity protection for the user device providing the image dataset by introducing a certificate-less encryption system. In turn, an aggregated signature scheme with key negotiation algorithm is introduced to authenticate the user devices of legitimate image datasets. A performance analysis indicates that the mechanism proposed in this paper outperforms other related methods in terms of security and accuracy of AIGIC image model training results, while guaranteeing real-time transmission of AIGIC image datasets, at the same time, the time complexity is also lower, which can effectively ensure the timeliness of the algorithm.},
  archive      = {J_JRTIP},
  author       = {Feng, Xiao and Yuan, Zheng},
  doi          = {10.1007/s11554-024-01508-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and secure identity authentication transmission mechanism for artificial intelligence generated image content},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A resource-efficient partial 3D convolution for gesture
recognition. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01509-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3DCNNs have shown impressive capabilities in extracting spatiotemporal features from videos. However, in practical applications, the numerous trainable parameters in most 3DCNN models result in longer latency times. Many models attempt to improve computational speed by reducing the number of floating-point operations. However, this approach alone may not effectively reduce latency times. Therefore, this paper proposes partial 3D convolution by extracting features from only a portion of the channels to reduce memory access and latency times. Additionally, structural reparameterization was applied to simplify the inference structure of the partial convolution. This convolution can easily substitute regular convolutions and depthwise convolutions in existing models. Through verification on three datasets, Jester, EgoGesture, and NvGesture, the proposed partial 3D convolution demonstrates the following highlights: (i) low memory access, (ii) significantly lower latency compared to other models, (iii) almost unchanged accuracy. For example, when the regular convolutions of ResNeXt101 and ResNeXt50 are replaced with the proposed partial convolutions on a GPU, runtime latency is reduced by 29.6 and 29.9% respectively, with almost no change in accuracy. Furthermore, computational complexity is also reduced.},
  archive      = {J_JRTIP},
  author       = {Chen, Gongzheng and Dong, Zhenghong and Wang, Jue and Hu, Jijian},
  doi          = {10.1007/s11554-024-01509-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A resource-efficient partial 3D convolution for gesture recognition},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time water surface target detection based on improved
YOLOv7 for chengdu sand river. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01510-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been a challenge to obtain accurate detection results in a timely manner when faced with complex and changing surface target detection. Detecting targets on water surfaces in real-time can be challenging due to their rapid movement, small size, and fragmented appearance. In addition, traditional detection methods are often labor-intensive and time-consuming, especially when dealing with large water bodies such as rivers and lakes. This paper presents an improved water surface target detection algorithm that is based on the YOLOv7 (you only look once) model to enhance the performance of water surface target detection. We have enhanced the accuracy and speed of detecting surface targets by making improvements to three key structures: the network aggregation structure, the pyramid pooling structure, and the down-sampling structure. Furthermore, we implemented the model on mobile devices and designed a detection software. The software enables real-time detection through images and videos. The experimental results demonstrate that the improved model outperforms the original YOLOv7 model. It exhibits a 6.4% boost in accuracy, a 4.2% improvement in recall, a 4.1% increase in mAP, a 14.3% reduction in parameter counts, and archives the FPS of 87. The software has the ability to accurately recognize 11 typical targets on the water surface and demonstrates excellent water surface target detection capability.},
  archive      = {J_JRTIP},
  author       = {Yang, Mei and Wang, Huajun},
  doi          = {10.1007/s11554-024-01510-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time water surface target detection based on improved YOLOv7 for chengdu sand river},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DTS: Dynamic training slimming with feature sparsity for
efficient convolutional neural network. <em>JRTIP</em>, <em>21</em>(4),
1–9. (<a href="https://doi.org/10.1007/s11554-024-01511-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks have achieved remarkable progress on computer vision tasks over last years. In this paper, we proposed a dynamic training slimming with feature sparsity based on structured pruning, named DTS, for efficient and automatic channel pruning. Unlike other existing pruning methods, which require manual intervention for pruning settings for each layer, DTS can design suitable architecture width for target datasets and deployment resources by automated pruning. The proposed method can be deployed to modern CNNs and the experimental results on CIFAR, ImageNet and PASCAL VOC benchmark datasets demonstrate the effectiveness of the proposed method, which significantly exceeds the other schemes.},
  archive      = {J_JRTIP},
  author       = {Yin, Jia and Wang, Wei and Guo, Zhonghua and Ji, Yangchun},
  doi          = {10.1007/s11554-024-01511-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DTS: Dynamic training slimming with feature sparsity for efficient convolutional neural network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time medical lesion screening: Accurate and rapid
detectors. <em>JRTIP</em>, <em>21</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01512-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are highly lethal, representing 85–90% of all primary central nervous system (CNS) tumors. Magnetic resonance imaging (MRI) images are employed to identify and assess brain tumors. However, this process has historically relied heavily on the expertise of medical professionals and necessitated the involvement of a substantial number of personnel. To optimize the allocation of medical resources and improve diagnostic efficiency, this work proposes a DETR-based RPC–DETR model that utilizes the Transformer. We conducted comparative experiments using RPC–DETR with other traditional detectors on the same equipment to test the performance exhibited by the model with equal computational resources. Tested on the Br35H brain tumor dataset with 701 MRI images (500 training sets and 201 test sets), RPC–DETR surpasses the YOLO models in accuracy while utilizing fewer parameters. This advancement ensures more reliable and faster diagnosis. RPC–DETR achieves a 96% mAP with just 14M parameters, offering high accuracy in brain tumor detection with a lighter model, making it easier to implement in various medical settings.},
  archive      = {J_JRTIP},
  author       = {Shao, Dangguo and Jiang, Jie and Ma, Lei and Lai, Hua and Yi, Sanli},
  doi          = {10.1007/s11554-024-01512-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time medical lesion screening: Accurate and rapid detectors},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A full-detection association tracker with confidence
optimization for real-time multi-object tracking. <em>JRTIP</em>,
<em>21</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01513-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) aims to obtain trajectories with unique identifiers for multiple objects in a video stream. In current approaches, confidence thresholds were frequently used to perform multi-stage data association. However, these thresholds could introduce instability into the algorithm when confronted with diverse scenarios. This article proposed confidence-optimization tracker (COTracker), a full-detection association tracker based on confidence optimization. COTracker incorporated detection confidence and matching cost as covariates and modeled tracklet confidence using exponential moving average (EMA). It introduced confidence cues in data association by generating a weighting matrix containing detection and tracklet confidence. Experimental results showed that COTracker achieved 63.0 HOTA and 77.1 IDF1 on MOT17 test set. On the more crowded MOT20, it achieves 62.4 HOTA and 76.1 IDF1. Compared with threshold-based methods, COTracker showcased the ability to handle various complex scenarios without adjusting the confidence threshold. Furthermore, its outstanding tracking speed, meeting the requirements of real-time tracking, positions it with potential value in applications such as unmanned driving and drone tracking. The source codes are available at https://github.com/LiYi199983/CWTracker .},
  archive      = {J_JRTIP},
  author       = {Liu, Youyu and Zhou, Xiangxiang and Zhang, Zhendong and Li, Yi and Tao, Wanbao},
  doi          = {10.1007/s11554-024-01513-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A full-detection association tracker with confidence optimization for real-time multi-object tracking},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniTracker: Transformer-based CrossUnihead for multi-object
tracking. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01514-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, tracking-by-detection (TBD) has emerged as the predominant approach for Multi-object Tracking (MOT). Most TBD algorithms typically employ separate branch heads to handle the coarse feature representations extracted from the backbone network. However, this approach often leads to suboptimal model performance. This is due to the presence of feature conflicts among multiple tasks on one hand, and the lack of information interaction among these tasks on the other. Inspired by CSTrack and Unihead, we have combined the feature decoupling module REN with the transformer-based task interaction module CIT to form CrossUnihead which is a plug and play module. This module not only effectively achieves task-based feature decoupling, but also facilitates information interaction among different tasks. The MOT algorithm based on CrossUnihead, named UniTracker, demonstrates impressive performance on the MOT data-set when compared to other advanced methods and is capable of achieving real-time tracking.},
  archive      = {J_JRTIP},
  author       = {Wu, Fan and Zhang, Yifeng},
  doi          = {10.1007/s11554-024-01514-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {UniTracker: Transformer-based CrossUnihead for multi-object tracking},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Csb-yolo: A rapid and efficient real-time algorithm for
classroom student behavior detection. <em>JRTIP</em>, <em>21</em>(4),
1–17. (<a href="https://doi.org/10.1007/s11554-024-01515-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the integration of artificial intelligence in education has become key to enhancing the quality of teaching. This study addresses the real-time detection of student behavior in classroom environments by proposing the Classroom Student Behavior YOLO (CSB-YOLO) model. We enhance the model’s multi-scale feature fusion capability using the Bidirectional Feature Pyramid Network (BiFPN). Additionally, we have designed a novel Efficient Re-parameterized Detection Head (ERD Head) to accelerate the model’s inference speed and introduced Self-Calibrated Convolutions (SCConv) to compensate for any potential accuracy loss resulting from lightweight design. To further optimize performance, model pruning and knowledge distillation are utilized to reduce the model size and computational demands while maintaining accuracy. This makes CSB-YOLO suitable for deployment on low-performance classroom devices while maintaining robust detection capabilities. Tested on the classroom student behavior dataset SCB-DATASET3, the distilled and pruned CSB-YOLO, with only 0.72M parameters and 4.3 Giga Floating-point Operations Per Second (GFLOPs), maintains high accuracy and exhibits excellent real-time performance, making it particularly suitable for educational environments.},
  archive      = {J_JRTIP},
  author       = {Zhu, Wenqi and Yang, Zhijun},
  doi          = {10.1007/s11554-024-01515-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Csb-yolo: A rapid and efficient real-time algorithm for classroom student behavior detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Journal of real-time image processing: Fourth issue of
volume 21. <em>JRTIP</em>, <em>21</em>(4), 1–2. (<a
href="https://doi.org/10.1007/s11554-024-01516-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-024-01516-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-2},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Fourth issue of volume 21},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved YOLOv8 algorithm for small object detection in
autonomous driving. <em>JRTIP</em>, <em>21</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01517-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of visual object detection for autonomous driving, several challenges arise, such as detecting densely clustered targets, dealing with significant occlusion, and identifying small-sized targets. To address these challenges, an improved YOLOv8 algorithm for small object detection in autonomous driving (MSD-YOLO) is proposed. This algorithm incorporates several enhancements to improve the performance of detecting small and densely occluded targets. Firstly, the downsampling module is replaced with SPD-CBS (Space-to-Depth) to maintain the integrity of channel feature information. Subsequently, a multi-scale small object detection structure is designed to increase sensitivity for recognizing densely packed small objects. Additionally, DyHead (Dynamic Head) is introduced, equipped with simultaneous scale, spatial, and channel attention to ensure comprehensive perception of feature map information. In the post-processing stage, Soft-NMS (non-maximum suppression) is employed to effectively suppress redundant candidate boxes and reduce the missed detection rate of densely occluded targets. The effectiveness of these enhancements has been verified through various experiments conducted on the BDD100K autonomous driving public dataset. Experimental results indicate a significant improvement in the performance of the enhanced network. Compared to the YOLOv8n baseline model, MSD-YOLO shows a 13.7% increase in mAP50 and a 12.1% increase in mAP50:95, with only a slight increase in the number of parameters. Furthermore, the detection speed can reach 67.6 FPS, achieving a better balance between accuracy and speed.},
  archive      = {J_JRTIP},
  author       = {Cao, Jie and Zhang, Tong and Hou, Liang and Nan, Ning},
  doi          = {10.1007/s11554-024-01517-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved YOLOv8 algorithm for small object detection in autonomous driving},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight detection model for coal gangue identification
based on improved YOLOv5s. <em>JRTIP</em>, <em>21</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01518-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focusing on the issues of complex models, high computational cost, and low identification speed of existing coal gangue image identification object detection algorithms, an optimized YOLOv5s lightweight detection model for coal gangue is proposed. Using ShuffleNetV2 as the backbone network, a convolution pooling module is used at the input end instead of the original convolution module. Combining the re-parameterization idea of RepVGG and introducing depthwise separable convolution, a neck feature fusion network is constructed. And using the WIoU function as the loss function. The experimental findings indicate that the improved model maintains the same accuracy, the number of parameters is only 5.1% of the original, the computational effort is reduced to 6.3 % of the original, and the identification speed is improved by 30.9% on GPU and 4 times on CPU. This method significantly reduces model complexity and improves detection speed while maintaining detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Shang, Deyong and Lv, Zhibin and Gao, Zehua and Li, Yuntao},
  doi          = {10.1007/s11554-024-01518-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight detection model for coal gangue identification based on improved YOLOv5s},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Yolo-tla: An efficient and lightweight small object
detection model based on YOLOv5. <em>JRTIP</em>, <em>21</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s11554-024-01519-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection, a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small objects. Moreover, the extensive parameter count and computational demands of the detection models impede their deployment on equipment with limited resources. In this paper, we propose YOLO-TLA, an advanced object detection model building on YOLOv5. We first introduce an additional detection layer for small objects in the neck network pyramid architecture, thereby producing a feature map of a larger scale to discern finer features of small objects. Further, we integrate the C3CrossCovn module into the backbone network. This module uses sliding window feature extraction, which effectively minimizes both computational demand and the number of parameters, rendering the model more compact. Additionally, we have incorporated a global attention mechanism into the backbone network. This mechanism combines the channel information with global information to create a weighted feature map. This feature map is tailored to highlight the attributes of the object of interest, while effectively ignoring irrelevant details. In comparison to the baseline YOLOv5s model, our newly developed YOLO-TLA model has shown considerable improvements on the MS COCO validation dataset, with increases of 4.6% in mAP@0.5 and 4% in mAP@0.5:0.95, all while keeping the model size compact at 9.49M parameters. Further extending these improvements to the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in mAP@0.5 and mAP@0.5:0.95, respectively, with a total of 27.53M parameters. These results validate the YOLO-TLA model’s efficient and effective performance in small object detection, achieving high accuracy with fewer parameters and computational demands.},
  archive      = {J_JRTIP},
  author       = {Ji, Chun-Lin and Yu, Tao and Gao, Peng and Wang, Fei and Yuan, Ru-Yue},
  doi          = {10.1007/s11554-024-01519-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Yolo-tla: An efficient and lightweight small object detection model based on YOLOv5},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time and energy-efficient SRAM with mixed-signal
in-memory computing near CMOS sensors. <em>JRTIP</em>, <em>21</em>(4),
1–14. (<a href="https://doi.org/10.1007/s11554-024-01520-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) represents a promising approach to reducing latency and enhancing the energy efficiency of operations required for calculating convolution products of images. This study proposes a fully differential current-mode architecture for computing image convolutions across all four quadrants, intended for deep learning applications within CMOS imagers utilizing IMC near the CMOS sensor. This architecture processes analog signals provided by a CMOS sensor without the need for analog-to-digital conversion. Furthermore, it eliminates the necessity for data transfer between memory and analog operators as convolutions are computed within modified SRAM memory. The paper suggests modifying the structure of a CMOS SRAM cell by incorporating transistors capable of performing multiplications between binary (−1 or +1) weights and analog signals. Modified SRAM cells can be interconnected to sum the multiplication results obtained from individual cells. This approach facilitates connecting current inputs to different SRAM cells, offering highly scalable and parallelized calculations. For this study, a configurable module comprising nine modified SRAM cells with peripheral circuitry has been designed to calculate the convolution product on each pixel of an image using a $$3 \times 3$$ mask with binary values (−1 or 1). Subsequently, an IMC module has been designed to perform 16 convolution operations in parallel, with input currents shared among the 16 modules. This configuration enables the computation of 16 convolutions simultaneously, processing a column per cycle. A digital control circuit manages both the readout or memorization of digital weights, as well as the multiply and add operations in real-time. The architecture underwent testing by performing convolutions between binary masks of 3 × 3 values and images of 32 × 32 pixels to assess accuracy and scalability when two IMC modules are vertically integrated. Convolution weights are stored locally as 1-bit digital values. The circuit was synthesized in 180 nm CMOS technology, and simulation results indicate its capability to perform a complete convolution in 3.2 ms, achieving an efficiency of 11,522 1-b TOPS/W (1-b tera-operations per second per watt) with a similarity to ideal processing of 96%.},
  archive      = {J_JRTIP},
  author       = {Diaz-Madrid, Jose-Angel and Domenech-Asensi, Gines and Ruiz-Merino, Ramon and Zapata-Perez, Juan-Francisco},
  doi          = {10.1007/s11554-024-01520-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and energy-efficient SRAM with mixed-signal in-memory computing near CMOS sensors},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WoodGLNet: A multi-scale network integrating global and
local information for real-time classification of wood images.
<em>JRTIP</em>, <em>21</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01521-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research on image classification has combined convolutional neural networks (CNNs) and transformers to introduce inductive biases to the model, enhancing its ability to handle long-range dependencies. However, these integrated models have limitations. Standard CNNs have a static nature, restricting their convolution from dynamically adjusting to input images, thus limiting feature expression capabilities. In addition, the static nature of CNNs impedes the seamless integration between features dynamically generated by self-attention mechanisms and static features generated by convolution when combined with transformers. Furthermore, during image processing, each model stage contains abundant information that cannot be fully utilized by single-scale convolution, ultimately impacting the network’s classification performance. To tackle these challenges, we propose WoodGLNet, a real-time multi-scale pyramid network that aggregates global and local information in an input-dependent manner and facilitates feature interaction through three scales of convolution. WoodGLNet utilizes efficient multi-scale global spatial decay attention modules and input-dependent multi-scale dynamic convolutions at different stages, enhancing the network’s inductive biases and expanding the effective receptive field. In CIFAR100 and CIFAR10 image classification tasks, WoodGLNet-T achieves Top-1 accuracies of 76.34% and 92.35%, respectively, outperforming EfficientNet-B3 by 1.03 and 0.86 percentage points. WoodGLNet-S and WoodGLNet-B attain Top-1 accuracies of 77.56%, 93.66%, and 80.12%, 94.27%, respectively. The experimental subjects of this study were sourced from the Shandong Province Construction Structural Material Specimen Museum, tasked with wood testing and requiring high real-time performance. To assess WoodGLNet’s real-time detection capabilities, 20 types of precious wood from the museum were identified in real time using the WoodGLNet network. The results indicated that WoodGLNet achieved a classification accuracy of up to 99.60%, with a recognition time of 0.013 s per single image. These findings demonstrate the network’s exceptional real-time classification and generalization abilities.},
  archive      = {J_JRTIP},
  author       = {Zheng, Zhishuai and Ge, Zhedong and Tian, Zhikang and Yang, Xiaoxia and Zhou, Yucheng},
  doi          = {10.1007/s11554-024-01521-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {WoodGLNet: A multi-scale network integrating global and local information for real-time classification of wood images},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced diagnosis of common rice leaf diseases using
KERTL-BME ensemble approach. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01522-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The influence of rice leaf diseases has resulted in an annual decrease in rice mass production. This occurs mainly due to the need for more understanding in perceiving and managing rice leaf diseases. However, there has not yet been any appropriate application designed to accurately detect rice leaf diseases. This paper, we proposed a novel method called Kushner Elman Recurrent Transfer Learning-based Boyer Moore Ensemble (KERTL-BME) to detect rice leaf diseases and differentiate between healthy and diseased images. Using the KERTL-BME method, the four most common rice leaf diseases, namely Bacterial leaf blight, Brown spot, Leaf blast, and Leaf scald, are detected. First, the Kushner non-linear filter is applied to the sample images to remove noise and differentiate between measurements and expected values by pixels in the neighborhood according to time instances. This significantly improves the peak signal-to-noise ratio while preserving the edges. The transfer learning in our work uses DenseNet169 pre-trained models to extract relevant features via the Elman Recurrent Network, which improves accuracy for the rice leaf 5 disease dataset. Additionally, the ensemble of transfer learning helps to minimize generalization errors, making the proposed method more robust. Finally, Boyer–Moore majority voting is applied to minimize generalization significantly, thereby improving overall prediction accuracy and reducing prediction error promptly. The rice leaf 5 disease dataset is used for training and testing the method. Performance measures such as prediction accuracy, prediction time, prediction error, and peak signal-to-noise ratio were calculated and monitored. The designed method predicts disease-affected rice leaves with greater accuracy.},
  archive      = {J_JRTIP},
  author       = {Simhadri, Chinna Gopi and Kondaveeti, Hari Kishan},
  doi          = {10.1007/s11554-024-01522-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Advanced diagnosis of common rice leaf diseases using KERTL-BME ensemble approach},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedsNet: The real-time network for pedestrian detection
based on RT-DETR. <em>JRTIP</em>, <em>21</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01523-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the problems of complex model networks, low detection accuracy, and the detection of small targets prone to false detections and omissions in pedestrian detection, this paper proposes FedsNet, a pedestrian detection network based on RT-DETR. By constructing a new lightweight backbone network, ResFastNet, the number of parameters and computation of the model are reduced to accelerate the detection speed of pedestrian detection. Integrating the Efficient Multi-scale Attention(EMA) mechanism with the backbone network creates a new ResBlock module for improved detection of small targets. The more effective DySample has been adopted as the upsampling operator to improve the accuracy and robustness of pedestrian detection. SIoU is used as the loss function to improve the accuracy of pedestrian recognition and speed up model convergence. Experimental evaluations conducted on a self-built pedestrian detection dataset demonstrate that the average accuracy value of the FedsNet model is 91 $$\%$$ , which is a 1.7 $$\%$$ improvement over the RT-DETR model. The parameters and model volume are reduced by 15.1 $$\%$$ and 14.5 $$\%$$ , respectively. When tested on the public dataset WiderPerson, FedsNet achieved the average accuracy value of 71.3 $$\%$$ , an improvement of 1.1 $$\%$$ over the original model. In addition, the detection speed of the FedsNet network reaches 109.5 FPS and 100.3 FPS, respectively, meeting the real-time requirements of pedestrian detection.},
  archive      = {J_JRTIP},
  author       = {Peng, Hao and Chen, Shiqiang},
  doi          = {10.1007/s11554-024-01523-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FedsNet: The real-time network for pedestrian detection based on RT-DETR},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight and privacy-preserving hierarchical federated
learning mechanism for artificial intelligence-generated image content.
<em>JRTIP</em>, <em>21</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01524-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence and Big Data, the application of artificial intelligence-generated image content (AIGIC) is becoming increasingly widespread in various fields. However, the image data utilized by AIGIC is diverse and often contains sensitive personal information, characterized by heterogeneity and privacy concerns. This leads to prolonged implementation times for image data privacy protection, and a high risk of unauthorized third-party access, resulting in serious privacy breaches and security risks. To address this issue, this paper combines Hierarchical Federated Learning (HFL) with Homomorphic Encryption to first address the encryption and transmission challenges in the image processing pipeline of AIGIC. Building upon this foundation, a novel HFL group collaborative training strategy is designed to further streamline the privacy protection process of AIGIC image data, effectively masking the heterogeneity of raw image data and achieving balanced allocation of computational resources. Additionally, a model compression algorithm based on pruning is introduced to alleviate the data transmission pressure in the image encryption process. Optimization of the homomorphic encryption modulo operations significantly reduces the computational burden, enabling real-time enhancement of image data privacy protection from multiple dimensions including computational and transmission resources. To verify the effectiveness of the proposed mechanism, extensive simulation verification of the lightweight privacy protection process for AIGIC image data was performed, and a comparative analysis of the time complexity of the mechanism was conducted. Experimental results indicate substantial advantages of the proposed algorithm over traditional real-time privacy protection algorithms in AIGIC.},
  archive      = {J_JRTIP},
  author       = {Wang, Bingquan and Yang, Fangling},
  doi          = {10.1007/s11554-024-01524-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight and privacy-preserving hierarchical federated learning mechanism for artificial intelligence-generated image content},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded planogram compliance control system.
<em>JRTIP</em>, <em>21</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01525-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retail sector presents several open and challenging problems that could benefit from advanced pattern recognition and computer vision techniques. One such critical challenge is planogram compliance control. In this study, we propose a complete embedded system to tackle this issue. Our system consists of four key components as image acquisition and transfer via stand-alone embedded camera module, object detection via computer vision and deep learning methods working on single-board computers, planogram compliance control method again working on single-board computers, and energy harvesting and power management block to accompany the embedded camera modules. The image acquisition and transfer block is implemented on the ESP-EYE camera module. The object detection block is based on YOLOv5 as the deep learning method and local feature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson Orin Nano, and NVIDIA Jetson AGX Orin as single-board computers. The planogram compliance control block utilizes sequence alignment through a modified Needleman–Wunsch algorithm. This block is also working along with the object detection block on the same single-board computers. The energy harvesting and power management block consists of solar and RF energy-harvesting modules with suitable battery pack for operation. We tested the proposed embedded planogram compliance control system on two different datasets to provide valuable insights on its strengths and weaknesses. The results show that the proposed method achieves F1 scores of 0.997 and 1.0 in object detection and planogram compliance control blocks, respectively. Furthermore, we calculated that the complete embedded system can work in stand-alone form up to 2 years based on battery. This duration can be further extended with the integration of the proposed solar and RF energy-harvesting options.},
  archive      = {J_JRTIP},
  author       = {Yücel, Mehmet Erkin and Topaloğlu, Serkan and Ünsalan, Cem},
  doi          = {10.1007/s11554-024-01525-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Embedded planogram compliance control system},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EV-TIFNet: Lightweight binocular fusion network assisted by
event camera time information for 3D human pose estimation.
<em>JRTIP</em>, <em>21</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01528-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation using RGB cameras often encounters performance degradation in challenging scenarios such as motion blur or suboptimal lighting. In comparison, event cameras, endowed with a wide dynamic range, microsecond-scale temporal resolution, minimal latency, and low power consumption, demonstrate remarkable adaptability in extreme visual environments. Nevertheless, the exploitation of event cameras for pose estimation in current research has not yet fully harnessed the potential of event-driven data, and enhancing model efficiency remains an ongoing pursuit. This work focuses on devising an efficient, compact pose estimation algorithm, with special attention on optimizing the fusion of multi-view event streams for improved pose prediction accuracy. We propose EV-TIFNet, a compact dual-view interactive network, which incorporates event frames along with our custom-designed Global Spatio-Temporal Feature Maps (GTF Maps). To enhance the network’s ability to understand motion characteristics and localize keypoints, we have tailored a dedicated Auxiliary Information Extraction Module (AIE Module) for the GTF Maps. Experimental results demonstrate that our model, with a compact parameter count of 0.55 million, achieves notable advancements on the DHP19 dataset, reducing the $$\hbox {MPJPE}_{3D}$$ to 61.45 mm. Building upon the sparsity of event data, the integration of sparse convolution operators replaces a significant portion of traditional convolutional layers, leading to a reduction in computational demand by 28.3%, totalling 8.71 GFLOPs. These design choices highlight the model’s suitability and efficiency in scenarios where computational resources are limited.},
  archive      = {J_JRTIP},
  author       = {Zhao, Xin and Yang, Lianping and Huang, Wencong and Wang, Qi and Wang, Xin and Lou, Yantao},
  doi          = {10.1007/s11554-024-01528-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EV-TIFNet: Lightweight binocular fusion network assisted by event camera time information for 3D human pose estimation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-LF: A lightweight multi-scale feature fusion algorithm
for wheat spike detection. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01529-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wheat is one of the most significant crops in China, as its yield directly affects the country’s food security. Due to its dense, overlapping, and relatively fuzzy distribution, wheat spikes are prone to being missed in practical detection. Existing object detection models suffer from large model size, high computational complexity, and long computation times. Consequently, this study proposes a lightweight real-time wheat spike detection model called YOLO-LF. Initially, a lightweight backbone network is improved to reduce the model size and lower the number of parameters, thereby improving the runtime speed. Second, the structure of the neck is redesigned in the context of the wheat spike dataset to enhance the feature extraction capability of the network for wheat spikes and to achieve lightweightness. Finally, a lightweight detection head was designed to significantly reduce the FLOPs of the model and achieve further lightweighting. Experimental results on the test set indicate that the size of our model is 1.7 MB, the number of parameters is 0.76 M, and the FLOPs are 2.9, which represent reductions of 73, 74, and 64% compared to YOLOv8n, respectively. Our model demonstrates a latency of 8.6 ms and an FPS of 115 on Titan X, whereas YOLOv8n has a latency of 10.2 ms and an FPS of 97 on the same hardware. In contrast, our model is more lightweight and faster to detect, while the mAP@0.5 only decreases by 0.9%, outperforming YOLOv8 and other mainstream detection networks in overall performance. Consequently, our model can be deployed on mobile devices to provide effective assistance in the real-time detection of wheat spikes.},
  archive      = {J_JRTIP},
  author       = {Zhou, Shuren and Long, Shengzhen},
  doi          = {10.1007/s11554-024-01529-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-LF: A lightweight multi-scale feature fusion algorithm for wheat spike detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Railway rutting defects detection based on improved RT-DETR.
<em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01530-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway turnouts are critical components of the rail track system, and their defects can lead to severe safety incidents and significant property damage. The irregular distribution and varying sizes of railway-turnout defects, combined with changing environmental lighting and complex backgrounds, pose challenges for traditional detection methods, often resulting in low accuracy and poor real-time performance. To address the issue of improving the detection performance of railway-turnout defects, this study proposes a high-precision recognition model, Faster-Hilo-BiFPN-DETR (FHB-DETR), based on the RT-DETR architecture. First, we designed the Faster CGLU module based on Faster Block, which optimizes the aggregation of local and global feature information through partial convolution and gating mechanisms. This approach reduces both computational load and parameter count while enhancing feature extraction capabilities. Second, we replaced the multi-head self-attention mechanism with Hilo attention, reducing parameter count and computational load, and improving real-time performance. In terms of feature fusion, we utilized BiFPN instead of CCFF to better capture subtle defect features and optimized the weighting of feature maps through a weighted mechanism. Experimental results show that compared to RT-DETR, FHB-DETR improved mAP50 by 3.5%, reduced parameter count by 25%, and decreased computational complexity by 6%, while maintaining a high frame rate, meeting real-time performance requirements.},
  archive      = {J_JRTIP},
  author       = {Yu, Chenghai and Chen, Xiangwei},
  doi          = {10.1007/s11554-024-01530-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Railway rutting defects detection based on improved RT-DETR},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TinyCount: An efficient crowd counting network for
intelligent surveillance. <em>JRTIP</em>, <em>21</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01531-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting, the task of estimating the total number of people in an image, is essential for intelligent surveillance. Integrating a well-trained crowd counting network into edge devices, such as intelligent CCTV systems, enables its application across various domains, including the prevention of crowd collapses and urban planning. For a model to be embedded in edge devices, it requires robust performance, reduced parameter count, and faster response times. This study proposes a lightweight and powerful model called TinyCount, which has only 60k parameters. The proposed TinyCount is a fully convolutional network consisting of a feature extract module (FEM) for robust and rapid feature extraction, a scale perception module (SPM) for scale variation perception and an upsampling module (UM) that adjusts the feature map to the same size as the original image. TinyCount demonstrated competitive performance across three representative crowd counting datasets, despite utilizing approximately 3.33 to 271 times fewer parameters than other crowd counting approaches. The proposed model achieved relatively fast inference times by leveraging the MobileNetV2 architecture with dilated and transposed convolutions. The application of SEblock and findings from existing studies further proved its effectiveness. Finally, we evaluated the proposed TinyCount on multiple edge devices, including the Raspberry Pi 4, NVIDIA Jetson Nano, and NVIDIA Jetson AGX Xavier, to demonstrate its potential for practical applications.},
  archive      = {J_JRTIP},
  author       = {Lee, Hyeonbeen and Lee, Jangho},
  doi          = {10.1007/s11554-024-01531-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {TinyCount: An efficient crowd counting network for intelligent surveillance},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time detection model of electrical work safety belt
based on lightweight improved YOLOv5. <em>JRTIP</em>, <em>21</em>(4),
1–12. (<a href="https://doi.org/10.1007/s11554-024-01533-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the issue that the existing aerial work safety belt wearing detection model cannot meet the real-time operation on edge devices, this paper proposes a lightweight aerial work safety belt detection model with higher accuracy. First, the model is made lightweight by introducing Ghost convolution and model pruning. Second, for complex scenarios involving occlusion, color confusion, etc., the model’s performance is optimized by introducing the new up-sampling operator, the attention mechanism, and the feature fusion network. Lastly, the model is trained using knowledge distillation to compensate for accuracy loss resulting from the lightweight design, thereby maintain a higher accuracy. Experimental results based on the Guangdong Power Grid Intelligence Challenge safety belt wearable dataset show that, in the comparison experiments, the improved model, compared with the mainstream object detection algorithm YOU ONLY LOOK ONCE v5s (YOLOv5s), has only 8.7% of the parameters of the former with only 3.7% difference in the mean Average Precision (mAP.50) metrics and the speed is improved by 100.4%. Meanwhile, the ablation experiments show that the improved model’s parameter count is reduced by 66.9% compared with the original model, while mAP.50 decreases by only 1.9%. The overhead safety belt detection model proposed in this paper combines the model’s lightweight design, SimAM attention mechanism, Bidirectional Feature Pyramid Network feature fusion network, Carafe operator, and knowledge distillation training strategy, enabling the model to maintain lightweight and real-time performance while achieving high detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Liu, Li and Huang, Kaiye and Bai, Yuang and Zhang, Qifan and Li, Yujian},
  doi          = {10.1007/s11554-024-01533-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection model of electrical work safety belt based on lightweight improved YOLOv5},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic detection of defects in electronic plastic
packaging using deep convolutional neural networks. <em>JRTIP</em>,
<em>21</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01534-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the mainstream chip packaging technology, plastic-encapsulated chips (PEC) suffer from process defects such as delamination and voids, which seriously impact the chip&#39;s reliability. Therefore, it is urgent to detect defects promptly and accurately. However, the current manual detection methods cannot meet the application&#39;s requirements, as they are both inaccurate and inefficient. This study utilized the deep convolutional neural network (DCNN) technique to analyze PEC&#39;s scanning acoustic microscope (SAM) images and identify their internal defects. First, the SAM technology was used to collect and set up datasets of seven typical PEC defects. Then, according to the characteristics of densely packed PEC and an incredibly tiny size ratio in SAM, a PECNet network was established to detect PEC based on the traditional RetinaNet network, combining the CoTNet50 backbone network and the feature pyramid network structure. Furthermore, a PEDNet was designed to classify PEC defects based on the MobileNetV2 network, integrating cross-local connections and progressive classifiers. The experimental results demonstrated that the PECNet network&#39;s chip recognition accuracy reaches 98.6%, and its speed of a single image requires only nine milliseconds. Meanwhile, the PEDNet network&#39;s average defect classification accuracy is 97.8%, and the recognition speed of a single image is only 0.0021 s. This method provides a precise and efficient technique for defect detection in PEC.},
  archive      = {J_JRTIP},
  author       = {Ren, Wanchun and Zhu, Pengcheng and Cai, Shaofeng and Huang, Yi and Zhao, Haoran and Hama, Youji and Yan, Zhu and Zhou, Tao and Pu, Junde and Yang, Hongwei},
  doi          = {10.1007/s11554-024-01534-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Automatic detection of defects in electronic plastic packaging using deep convolutional neural networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance evaluation of all intra kvazaar and x265 HEVC
encoders on embedded system nvidia jetson platform. <em>JRTIP</em>,
<em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01429-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing demand for high-quality video requires complex coding techniques that cost resource consumption and increase encoding time which represents a challenge for real-time processing on Embedded Systems. Kvazaar and x265 encoders are two efficient implementations of the High-Efficient Video Coding (HEVC) standard. In this paper, the performance of All Intra Kvazaar and x265 encoders on the Nvidia Jetson platform was evaluated using two coding configurations; highspeed preset and high-quality preset. In our work, we used two scenarios, first, the two encoders were run on the CPU, and based on the average encoding time Kvazaar proved to be 65.44% and 69.4% faster than x265 with 1.88% and 0.6% BD-rate improvement over x265 at high-speed and high-quality preset, respectively. In the second scenario, the two encoders were run on the GPU of the Nvidia Jetson, and the results show the average encoding time under each preset is reduced by half of the CPU-based scenario. In addition, Kvazaar is 54.5% and 56.70% faster with 1.93% and 0.45% BD-rate improvement over x265 at high-speed and high-quality preset, respectively. Regarding the scalability, the two encoders on the CPU are linearly scaled up to four threads and speed remains constant afterward. On the GPU, the two encoders are scaled linearly with the number of threads. The obtained results confirmed that, Kvazaar is more efficient and that it can be used on Embedded Systems for real-time video applications due to its high speed and performance over the x265 HEVC encoder},
  archive      = {J_JRTIP},
  author       = {James, R. and Abo-Zahhad, Mohammed and Inoue, Koji and Sayed, Mohammed S.},
  doi          = {10.1007/s11554-024-01429-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance evaluation of all intra kvazaar and x265 HEVC encoders on embedded system nvidia jetson platform},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equivalent convolution strategy for the evolution
computation in parametric active contour model. <em>JRTIP</em>,
<em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01434-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric active contour model is an efficient approach for image segmentation. However, the high cost of evolution computation has restricted their potential applications to contour segmentation with long perimeter. Extensive algorithm debugging and analysis indicate that the inverse matrix calculation and the matrix multiplication are the two major reasons. In this paper, a novel simple and efficient algorithm for evolution computation is proposed. Motivated by the relationship between the eigenvalues and the entries in the circular Toeplitz matrix, each entry expression of inverse matrix is firstly derived through mathematical deduction, and then, the matrix multiplication is simplified into a more efficient convolution operation. Experimental results show that the proposed algorithm can significantly improve the computational speed by one to two orders of magnitude and is even more efficient for contour extraction with large perimeter.},
  archive      = {J_JRTIP},
  author       = {Tang, Kelun and Lang, Lin and Zhou, Xiaojun},
  doi          = {10.1007/s11554-024-01434-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Equivalent convolution strategy for the evolution computation in parametric active contour model},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slim-neck by GSConv: A lightweight-design for real-time
detector architectures. <em>JRTIP</em>, <em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01436-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection is significant for industrial and research fields. On edge devices, a giant model is difficult to achieve the real-time detecting requirement, and a lightweight model built from a large number of the depth-wise separable convolutional could not achieve the sufficient accuracy. We introduce a new lightweight convolutional technique, GSConv, to lighten the model but maintain the accuracy. The GSConv accomplishes an excellent trade-off between the accuracy and speed. Furthermore, we provide a design suggestion based on the GSConv, slim-neck (SNs), to achieve a higher computational cost-effectiveness of the real-time detectors. The effectiveness of the SNs was robustly demonstrated in over twenty sets comparative experiments. In particular, the real-time detectors of ameliorated by the SNs obtain the state-of-the-art (70.9% AP50 for the SODA10M at a speed of ~ 100 FPS on a Tesla T4) compared with the baselines. Code is available at https://github.com/alanli1997/slim-neck-by-gsconv .},
  archive      = {J_JRTIP},
  author       = {Li, Hulin and Li, Jun and Wei, Hanbing and Liu, Zheng and Zhan, Zhenfei and Ren, Qiliang},
  doi          = {10.1007/s11554-024-01436-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Slim-neck by GSConv: A lightweight-design for real-time detector architectures},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A safety helmet-wearing detection method based on
cross-layer connection. <em>JRTIP</em>, <em>21</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01437-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the current safety helmet detection methods, the feature information of the small-scale safety helmet will be lost after the network model is convolved many times, resulting in the problem of missing detection of the safety helmet. To this end, an improved target detection algorithm of YOLOv5 is used to detect the wearing of safety helmets. Firstly, a new small-scale detection layer is added to the head of the network for multi-scale feature fusion, thereby increasing the receptive field area of the feature map to improve the model’s recognition of small targets. Secondly, a cross-layer connection is designed between the feature extraction network and the feature fusion network to enhance the fine-grained features of the target in the shallow layer of the network. Thirdly, a coordinate attention (CA) module is added to the cross-layer connection to capture the global information of the image and improve the localization ability of the target. Finally, the Normalized Wasserstein Distance (NWD) is used to measure the similarity between bounding boxes, replacing the intersection over union (IoU) method. The experimental results show that the improved model achieves 95.09% of the mAP value for safety helmet-wearing detection, which has a good effect on the recognition of small-sized safety helmets of different degrees in the construction work scene.},
  archive      = {J_JRTIP},
  author       = {Dong, Gang and Zhang, Yefei and Xie, Weicheng and Huang, Yong},
  doi          = {10.1007/s11554-024-01437-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A safety helmet-wearing detection method based on cross-layer connection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online continual streaming learning for embedded space
applications. <em>JRTIP</em>, <em>21</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01438-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an online continual learning (OCL) methodology tested on hardware and validated for space applications using an object detection close-proximity operations task. The proposed OCL algorithm simulates a streaming scenario and uses experience replay to enable the model to update its knowledge without suffering catastrophic forgetting by saving past inputs in an onboard reservoir that will be sampled during updates. A stream buffer is introduced to enable online training, i.e., the ability to update the model as data is streamed, one sample at a time, rather than being available in batches. Hyperparameters such as buffer sizes, update rate, batch size, batch concatenation parameters and number of iterations per batch are all investigated to find an optimized approach for the incremental domain and streaming learning task. The algorithm is tested on a customized dataset for space applications simulating changes in visual environments that significantly impact the deployed model’s performance. Our OCL methodology uses Weighted Sampling, a novel approach which allows the system to analytically choose more useful input samples during training, the results show that a model can be updated online achieving up to 60% Average Learning while Average Forgetting can be as low as 13% all with a Model Size Efficiency of 1, meaning the model size does not increase. An additional contribution is an implementation of On-Device Continual Training for embedded applications, a hardware experiment is carried out on the Zynq 7100 FPGA where a pre-trained CNN model is updated online using our FPGA backpropagation pipeline and OCL methodology to take into account new data and satisfactorily complete the planned task in less than 5 min achieving 90 FPS.},
  archive      = {J_JRTIP},
  author       = {Mazouz, Alaa Eddine and Nguyen, Van-Tam},
  doi          = {10.1007/s11554-024-01438-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Online continual streaming learning for embedded space applications},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA-SoC implementation of YOLOv4 for flying-object
detection. <em>JRTIP</em>, <em>21</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11554-024-01440-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flying-object detection has become an increasingly attractive avenue for research, particularly with the rising prevalence of unmanned aerial vehicle (UAV). Utilizing deep learning methods offers an effective means of detection with high accuracy. Meanwhile, the demand to implement deep learning models on embedded devices is growing, fueled by the requirement for capabilities that are both real-time and power efficient. FPGA have emerged as the optimal choice for its parallelism, flexibility and energy efficiency. In this paper, we propose an FPGA-based design for YOLOv4 network to address the problem of flying-object detection. Our proposed design explores and provides a suitable solution for overcoming the challenge of limited floating-point resources while maintaining the accuracy and obtain real-time performance and energy efficiency. We have generated an appropriate dataset of flying objects for implementing, training and fine-tuning the network parameters base on this dataset, and then changing some suitable components in the YOLO networks to fit for the deployment on FPGA. Our experiments in Xilinx ZCU104 development kit show that with our implementation, the accuracy is competitive with the original model running on CPU and GPU despite the process of format conversion and model quantization. In terms of speed, the FPGA implementation with the ZCU104 kit is inferior to the ultra high-end GPU, the RTX 2080Ti, but outperforms the GTX 1650. In terms of power consumption, the FPGA implementation is significantly lower than the GPU GTX 1650 about 3 times and about 7 times lower than RTX 2080Ti. In terms of energy efficiency, FPGA is completely superior to GPU with 2–3 times more efficient than the RTX 2080Ti and 3–4 times that of the GTX 1650.},
  archive      = {J_JRTIP},
  author       = {Nguyen, Dai-Duong and Nguyen, Dang-Tuan and Le, Minh-Thuy and Nguyen, Quoc-Cuong},
  doi          = {10.1007/s11554-024-01440-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-SoC implementation of YOLOv4 for flying-object detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of convolutional neural network accelerators on
field-programmable gate array platforms: Architectures and optimization
techniques. <em>JRTIP</em>, <em>21</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11554-024-01442-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent advancements in high-performance computing, convolutional neural networks (CNNs) have achieved remarkable success in various vision tasks. However, along with improvements in model accuracy, the size and computational complexity of the models have significantly increased with the increasing number of parameters. Although graphics processing unit (GPU) platforms equipped with high-performance memory and specialized in parallel processing are commonly used for CNN processing, the significant power consumption presents challenges in their utilization on edge devices. To address these issues, research is underway to design CNN models using field-programmable gate arrays (FPGAs) as accelerators. FPGAs provide a high level of flexibility, allowing efficient optimization of convolution operations, which account for a significant portion of the CNN computations. Additionally, FPGAs are known for their low power consumption compared to GPUs, making them a promising energy-efficient platform. In this paper, we review and summarize various approaches and techniques related to the design of FPGA-based CNN accelerators. Specifically, to comprehensively study CNN accelerators, we investigate the advantages and disadvantages of various methods for optimizing CNN accelerators and previously designed efficient accelerator architectures. We expect this paper to serve as an important guideline for future hardware research in artificial intelligence.},
  archive      = {J_JRTIP},
  author       = {Hong, Hyeonseok and Choi, Dahun and Kim, Namjoon and Lee, Haein and Kang, Beomjin and Kang, Huibeom and Kim, Hyun},
  doi          = {10.1007/s11554-024-01442-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-21},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Survey of convolutional neural network accelerators on field-programmable gate array platforms: Architectures and optimization techniques},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAN-STD: Small target detection based on generative
adversarial network. <em>JRTIP</em>, <em>21</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01446-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of convolutional neural networks, significant breakthroughs have been made in deep learning-based target detection algorithms. However, existing target detection algorithms based on convolutional neural networks need to downsample the whole image to extract deep semantic information from the image, which can lead to the loss of spatial information for small targets, impressive results on large/medium-sized targets, the results are not satisfactory for small target detection. To solve the problem of small target detection and increase the detection precision of small targets, we propose an end-to-end generative adversarial network GAN-STD for small target detection in this paper. GAN-STD makes full use of the structural correlation between targets at different scales through generative adversarial networks to enhance the similar representation of small targets in the shallow feature map and large targets in the deep feature map in the feature extraction process, and reduce the difference between the representation of small targets and large targets, thus making small targets as easy to detect as large targets. In addition, for the detector to perform better localization and classification of small targets, we back-propagate the detector losses to the generator and discriminator for end-to-end training. We merge GAN-STD onto two widely used one-stage target detectors (SSD and YOLOv4) to validate the effectiveness of our proposed GAN-STD. Extensive experiments on the widely used PASCAL VOC, MS COCO, and TT100K datasets show that the proposed GAN-STD algorithm achieves excellent results for detecting small targets.},
  archive      = {J_JRTIP},
  author       = {Wang, Huilin and Qian, Huaming and Feng, Shuai},
  doi          = {10.1007/s11554-024-01446-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GAN-STD: Small target detection based on generative adversarial network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel image denoising algorithm based on least square
generative adversarial network. <em>JRTIP</em>, <em>21</em>(3), 1–18.
(<a href="https://doi.org/10.1007/s11554-024-01447-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, computer vision models have shown a significant improvement in performance on various image analysis tasks. However, these models are not robust against noisy images. There are various causes of noise including noise coming from image capturing conditions such as lighting, weather, and the noise induced by physical measurement devices such as the camera or sensor. To address the problem of noise in images, various works have been proposed. Most of these works focus on computer vision models that run on high-compute devices such as mainframes. However, with recent advances in the deployment of AI technology in mobile and edge devices, there is an increased need for models which work equally well on such devices. In this work, a novel image-denoising algorithm based on the Least Square Generative Adversarial Network (LSGAN) is proposed. It also has the ability to work in settings with lower computing access such as edge devices. The generator part of the LSGAN is a UNet-based architecture which is used to capture detailed low-level features while preserving image information. For the discriminator part of LSGAN, a fully connected network is used, which offers faster convergence, and more stability when training using the composite loss function formed with least-squares loss, visual perception loss and the mean square loss. The proposed model has been evaluated on publicly available denoising datasets including PolyU, CBSD68, DIV2K, and SIDD, under low-compute conditions. The obtained results demonstrate a considerable improvement when compared to various widely deployed works on low-computing devices.},
  archive      = {J_JRTIP},
  author       = {Mohammed, Sharfuddin Waseem and Murugan, Brindha},
  doi          = {10.1007/s11554-024-01447-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel image denoising algorithm based on least square generative adversarial network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA implementation of secret sharing for textured 3D mesh
model based on texture vertex correlation. <em>JRTIP</em>,
<em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01449-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secret sharing is an important encryption technique in the field of information security. It can ensure the security of secret information. Textured 3D (three-dimensional) mesh model is an information carrier with a large amount of non-integer data. Therefore it takes longer time and computation to share them compared to 2D (two-dimensional) digital images. In this paper, different circuit modules are designed to accelerate the generation of shares and reconstruction of secrets. The hardware architecture enables security while sharing less data. In the data preprocessing phase, the correlation of the edge points of the texture vertices is utilized to filter the texture vertices, which reduces the amount of data to be shared. In the share generation phase, random factors are added. It makes the shares more secure and does not require group reconstruction. Simulation results show that the structure is more than 70 times faster compared to the software implementation. The optimized resource utilization is reduced by more than 60 $$\%$$ . The feasibility of the hardware architecture is verified by conducting experiments on several models and analyzing the results.},
  archive      = {J_JRTIP},
  author       = {Wu, Zi-Ming and Kong, Hao and Yan, Bin and Pan, Jeng-Shyang and Yang, Hong-Mei and Ju, Zhen-Zhen},
  doi          = {10.1007/s11554-024-01449-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of secret sharing for textured 3D mesh model based on texture vertex correlation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software and hardware realizations for different designs of
chaos-based secret image sharing systems. <em>JRTIP</em>,
<em>21</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01450-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secret image sharing (SIS) conveys a secret image to mutually suspicious receivers by sending meaningless shares to the participants, and all shares must be present to recover the secret. This paper proposes and compares three systems for secret sharing, where a visual cryptography system is designed with a fast recovery scheme as the backbone for all systems. Then, an SIS system is introduced for sharing any type of image, where it improves security using the Lorenz chaotic system as the source of randomness and the generalized Arnold transform as a permutation module. The second SIS system further enhances security and robustness by utilizing SHA-256 and RSA cryptosystem. The presented architectures are implemented on a field programmable gate array (FPGA) to enhance computational efficiency and facilitate real-time processing. Detailed experimental results and comparisons between the software and hardware realizations are presented. Security analysis and comparisons with related literature are also introduced with good results, including statistical tests, differential attack measures, robustness tests against noise and crop attacks, key sensitivity tests, and performance analysis.},
  archive      = {J_JRTIP},
  author       = {Sharobim, Bishoy K. and Hosam, Muhammad and Abd-El-Hafiz, Salwa K. and Sayed, Wafaa S. and Said, Lobna A. and Radwan, Ahmed G.},
  doi          = {10.1007/s11554-024-01450-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Software and hardware realizations for different designs of chaos-based secret image sharing systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDPH: A new technique for spatial detection of path holes
from huge volume high-resolution raster images in near real-time.
<em>JRTIP</em>, <em>21</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11554-024-01451-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and repairing road defects is crucial for road safety, vehicle maintenance, and enhancing tourism on well-maintained roads. However, monitoring all roads by vehicle incurs high costs. With the widespread use of remote sensing technologies, high-resolution satellite images offer a cost-effective alternative. This study proposes a new technique, SDPH, for automated detection of damaged roads from vast, high-resolution satellite images. In the SDPH technique, satellite images are organized in a pyramid grid file system, allowing deep learning methods to efficiently process them. The images, generated as $$256\times 256$$ dimensions, are stored in a directory with explicit location information. The SDPH technique employs a two-stage object detection models, utilizing classical and modified RCNNv3, YOLOv5, and YOLOv8. Classical RCNNv3, YOLOv5, and YOLOv8 and modified RCNNv3, YOLOv5, and YOLOv8 in the first stage for identifying roads, achieving f1 scores of 0.743, 0.716, 0.710, 0.955, 0.958, and 0.954, respectively. When the YOLOv5, with the highest f1 score, was fed to the second stage; modified RCNNv3, YOLOv5, and YOLOv8 detected road defects, achieving f1 scores of 0.957,0.971 and 0.964 in the second process. When the same CNN model was used for road and road defect detection in the proposed SDPH model, classical RCNNv3, improved RCNNv3, classical YOLOv5, improved YOLOv5, classical YOLOv8, improved RCNNv8 achieved micro f1 scores of 0.752, 0.956, 0.726, 0.969, 0.720 and 0.965, respectively. In addition, these models processed 11, 10, 33, 31, 37, and 36 FPS images by performing both stage operations, respectively. Evaluations on geotiff satellite images from Kayseri Metropolitan Municipality, ranging between 20 and 40 gigabytes, demonstrated the efficiency of the SDPH technique. Notably, the modified YOLOv5 outperformed, detecting paths and defects in 0.032 s with the micro f1 score of 0.969. Fine-tuning on TileCache enhanced f1 scores and reduced computational costs across all models.},
  archive      = {J_JRTIP},
  author       = {Tasyurek, Murat},
  doi          = {10.1007/s11554-024-01451-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SDPH: A new technique for spatial detection of path holes from huge volume high-resolution raster images in near real-time},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple layers complexity allocation with dynamic control
scheme for high-efficiency video coding. <em>JRTIP</em>, <em>21</em>(3),
1–12. (<a href="https://doi.org/10.1007/s11554-024-01452-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-efficiency video coding (HEVC) has significantly improved coding efficiency; however, its quadtree (QT) structures for coding units (CU) substantially raise the overall coding complexity. This study introduces a novel complexity control scheme aimed at enhancing HEVC encoding efficiency. The proposed scheme operates across multiple layers, encompassing the group of pictures (GOP) layer, frame layer, and coding-tree unit (CTU) layer. Each coding layer is assigned a limited coding complexity based on the remaining coding time. Particularly noteworthy is the dynamic scheme implemented to activate the complexity control method. To further expedite encoding, an efficient algorithm is developed for the CTU layer. Experimental results indicate that the 0.46% and 0.98% increases in BD-rate under the target complexity are reduced to 80% and 60% of the complexity constraint, respectively. The rate-distortion performance surpasses existing state-of-the-art complexity control methods, demonstrating the effectiveness of the proposed approach in enhancing HEVC encoding efficiency.},
  archive      = {J_JRTIP},
  author       = {Fang, Jiunn-Tsair and Chen, Ju-Kai},
  doi          = {10.1007/s11554-024-01452-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multiple layers complexity allocation with dynamic control scheme for high-efficiency video coding},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time semantic segmentation network based on parallel
atrous convolution for short-term dense concatenate and attention
feature fusion. <em>JRTIP</em>, <em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01453-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of incomplete segmentation of large objects and miss-segmentation of tiny objects that is universally existing in semantic segmentation algorithms, PACAMNet, a real-time segmentation network based on short-term dense concatenate of parallel atrous convolution and fusion of attentional features is proposed, called PACAMNet. First, parallel atrous convolution is introduced to improve the short-term dense concatenate module. By adjusting the size of the atrous factor, multi-scale semantic information is obtained to ensure that the last layer of the module can also obtain rich input feature maps. Second, attention feature fusion module is proposed to align the receptive fields of deep and shallow feature maps via depth-separable convolutions with different sizes, and the channel attention mechanism is used to generate weights to effectively fuse the deep and shallow feature maps. Finally, experiments are carried out based on both Cityscapes and CamVid datasets, and the segmentation accuracy achieve 77.4% and 74.0% at the inference speeds of 98.7 FPS and 134.6 FPS, respectively. Compared with other methods, PACAMNet improves the inference speed of the model while ensuring higher segmentation accuracy, so PACAMNet achieve a better balance between segmentation accuracy and inference speed.},
  archive      = {J_JRTIP},
  author       = {Wu, Lijun and Qiu, Shangdong and Chen, Zhicong},
  doi          = {10.1007/s11554-024-01453-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time semantic segmentation network based on parallel atrous convolution for short-term dense concatenate and attention feature fusion},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). F2S-net: Learning frame-to-segment prediction for online
action detection. <em>JRTIP</em>, <em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01454-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online action detection (OAD) aims at predicting action per frame from a streaming untrimmed video in real time. Most existing approaches leverage all the historical frames in the sliding window as the temporal context of the current frame since single-frame prediction is often unreliable. However, such a manner inevitably introduces useless even noisy video content, which often misleads action classifier when recognizing the ongoing action in the current frame. To alleviate this difficulty, we propose a concise and novel F2S-Net, which can adaptively discover the contextual segments in the online sliding window, and convert current frame prediction into relevant-segment prediction. More specifically, as the current frame can be either action or background, we develop F2S-Net with a distinct two-branch structure, i.e., the action (or background) branch can exploit the action (or background) segments. Via multi-level action supervision, these two branches can complementarily enhance each other, allowing to identify the contextual segments in the sliding window to robustly predict what is ongoing. We evaluate our approach on popular OAD benchmarks, i.e., THUMOS-14, TVSeries and HDD. The extensive results show that our F2S-Net outperforms the recent state-of-the-art approaches.},
  archive      = {J_JRTIP},
  author       = {Liu, Yi and Qiao, Yu and Wang, Yali},
  doi          = {10.1007/s11554-024-01454-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {F2S-net: Learning frame-to-segment prediction for online action detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Driver fatigue detection based on improved YOLOv7.
<em>JRTIP</em>, <em>21</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01455-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fatigue driving is one of the main reasons threatening road traffic safety. Aiming at the problems of complex detection process, low accuracy, and susceptibility to light interference in the current driver fatigue detection algorithm, this paper proposes a driver Eye State detection algorithm based on YOLO, abbreviated as ES-YOLO. The algorithm optimizes the structure of YOLOv7, integrates the multi-scale features using the convolutional block attention mechanism (CBAM), and improves the attention to important spatial locations in the image. Furthermore, using the Focal-EIOU Loss instead of CIOU Loss to increase the attention on difficult samples and reduce the influence of sample class imbalance. Then, based on ES-YOLO, a driver fatigue detection method is proposed, and the driver fatigue judgment logic is designed to monitor the fatigue state in real-time and alarm in time to improve the accuracy of detection. The experiments on the public dataset CEW and the self-made dataset show that the proposed ES-YOLO obtained 99.0% and 98.8% mAP values, respectively, which are better than the compared algorithms. And this method achieves real-time and accurate detection of driver fatigue status. Source code is released in https://github.com/1066480/driver-fatigue-detection.git .},
  archive      = {J_JRTIP},
  author       = {Li, Xianguo and Li, Xueyan and Shen, Zhenqian and Qian, Guangmin},
  doi          = {10.1007/s11554-024-01455-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Driver fatigue detection based on improved YOLOv7},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing UAV tracking: A focus on discriminative
representations using contrastive instances. <em>JRTIP</em>,
<em>21</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01456-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the core challenges of achieving both high efficiency and precision in UAV tracking is crucial due to limitations in computing resources, battery capacity, and maximum load capacity on UAVs. Discriminative correlation filter (DCF)-based trackers excel in efficiency on a single CPU but lag in precision. In contrast, many lightweight deep learning (DL)-based trackers based on model compression strike a better balance between efficiency and precision. However, higher compression rates can hinder performance by diminishing discriminative representations. Given these challenges, our paper aims to enhance feature representations’ discriminative abilities through an innovative feature-learning approach. We specifically emphasize leveraging contrasting instances to achieve more distinct representations for effective UAV tracking. Our method eliminates the need for manual annotations and facilitates the creation and deployment of lightweight models. As far as our knowledge goes, we are the pioneers in exploring the possibilities of contrastive learning in UAV tracking applications. Through extensive experimentation across four UAV benchmarks, namely, UAVDT, DTB70, UAV123@10fps and VisDrone2018, We have shown that our DRCI (discriminative representation with contrastive instances) tracker outperforms current state-of-the-art UAV tracking methods, underscoring its potential to effectively tackle the persistent challenges in this field.},
  archive      = {J_JRTIP},
  author       = {Wang, Xucheng and Zeng, Dan and Li, Yongxin and Zou, Mingliang and Zhao, Qijun and Li, Shuiwang},
  doi          = {10.1007/s11554-024-01456-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhancing UAV tracking: A focus on discriminative representations using contrastive instances},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved feature extraction network in lightweight YOLOv7
model for real-time vehicle detection on low-cost hardware.
<em>JRTIP</em>, <em>21</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01457-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of unmanned aerial vehicles (UAVs) has drawn researchers to update object detection algorithms for better accuracy and computation performance. Previous works applying deep learning models for object detection applications required high graphics processing unit (GPU) computation power. Generally, object detection models suffer trade-off between accuracy and model size where the relationship is not always linear in deep learning models. Various factors such as architectural design, optimization techniques, and dataset characteristics can significantly influence the accuracy, model size, and computation cost in adopting object detection models for low-cost embedded devices. Hence, it is crucial to employ lightweight object detection models for real-time object identification for the solution to be sustainable. In this work, an improved feature extraction network is proposed by incorporating an efficient long-range aggregation network for vehicle detection (ELAN-VD) in the backbone layer. The architecture improvement in YOLOv7-tiny model is proposed to improve the accuracy of detecting small vehicles in the aerial image. Besides that, the image size output of the second and third prediction boxes is upscaled for better performance. This study showed that the proposed method yields a mean average precision (mAP) of 57.94%, which is higher than that of the conventional YOLOv7-tiny. In addition, the proposed model showed significant performance when compared to previous works, making it viable for application in low-cost embedded devices.},
  archive      = {J_JRTIP},
  author       = {Andika, Johan Lela and Khairuddin, Anis Salwa Mohd and Ramiah, Harikrishnan and Kanesan, Jeevan},
  doi          = {10.1007/s11554-024-01457-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved feature extraction network in lightweight YOLOv7 model for real-time vehicle detection on low-cost hardware},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel real-time pixel-level road crack segmentation
network. <em>JRTIP</em>, <em>21</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01458-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road crack detection plays a vital role in preserving the life of roads and ensuring driver safety. Traditional methods relying on manual observation have limitations in terms of subjectivity and inefficiency in quantifying damage. In recent years, advances in deep learning techniques have held promise for automated crack detection, but challenges, such as low contrast, small datasets, and inaccurate localization, remain. In this paper, we propose a deep learning-based pixel-level road crack segmentation network that achieves excellent performance on multiple datasets. In order to enrich the receptive fields of conventional convolutional modules, we design a residual asymmetric convolutional module for feature extraction. In addition to this, a multiple receptive field cascade module and a feature fusion module with non-local attention are proposed. Our network demonstrates superior accuracy and inference speed, achieving 55.60%, 59.01%, 75.65%, and 57.95% IoU on the CrackForest, CrackTree, CDD, and Crack500 datasets, respectively. It also has the ability to process 143 images per second. Experimental results and analysis validate the effectiveness of our approach. This work contributes to the advancement of road crack detection, providing a valuable tool for road maintenance and safety improvement.},
  archive      = {J_JRTIP},
  author       = {Wang, Rongdi and Wang, Hao and He, Zhenhao and Zhu, Jianchao and Zuo, Haiqiang},
  doi          = {10.1007/s11554-024-01458-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel real-time pixel-level road crack segmentation network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time and accurate model of instance segmentation of
foods. <em>JRTIP</em>, <em>21</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11554-024-01459-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation of foods is an important technology to ensure the food success rate of meal-assisting robotics. However, due to foods have strong intraclass variability, interclass similarity, and complex physical properties, which leads to more challenges in recognition, localization, and contour acquisition of foods. To address the above issues, this paper proposed a novel method for instance segmentation of foods. Specifically, in backbone network, deformable convolution was introduced to enhance the ability of YOLOv8 architecture to capture finer-grained spatial information, and efficient multiscale attention based on cross-spatial learning was introduced to improve sensitivity and expressiveness of multiscale inputs. In neck network, classical convolution and C2f modules were replaced by lightweight convolution GSConv and improved VoV-GSCSP aggregation module, respectively, to improve inference speed of models. We abbreviated it as the DEG-YOLOv8n-seg model. The proposed method was compared with baseline model and several state-of-the-art (SOTA) segmentation models on datasets, respectively. The results show that the DEG-YOLOv8n-seg model has higher accuracy, faster speed, and stronger robustness. Specifically, the DEG-YOLOv8n-seg model can achieve 84.6% Box_mAP@0.5 and 84.1% Mask_mAP@0.5 accuracy at 55.2 FPS and 11.1 GFLOPs. The importance of adopting data augmentation and the effectiveness of introducing deformable convolution, EMA, and VoV-GSCSP were verified by ablation experiments. Finally, the DEG-YOLOv8n-seg model was applied to experiments of food instance segmentation for meal-assisting robots. The results show that the DEG-YOLOv8n-seg can achieve better instance segmentation of foods. This work can promote the development of intelligent meal-assisting robotics technology and can provide theoretical foundations for other tasks of the computer vision field with some reference value.},
  archive      = {J_JRTIP},
  author       = {Fan, Yuhe and Zhang, Lixun and Zheng, Canxing and Zu, Yunqin and Wang, Keyi and Wang, Xingyuan},
  doi          = {10.1007/s11554-024-01459-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and accurate model of instance segmentation of foods},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). T-psd: T-shape parking slot detection with self-calibrated
convolution network. <em>JRTIP</em>, <em>21</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01460-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with a challenging autonomous parking problem in which the parking slots are with various different angles. We transform the problem of parking slot detection into center keypoint detection, representing the parking slot as a T-shape to make it robust and simple. For diverse types of parking slots, we propose a T-shape parking slot detection method, called T-PSD, to extract the T-shape center information based on a self-calibrated convolution network (SCCN). This method can concurrently obtain the entrance center confidence, the relative offsets of the paired junctions, the direction of the middle line, the occupancy and the inferred type in the parking slots. Final detection results are produced by utilizing Half-Heatmap, MultiBins and Midline-Grid to more accurately extract the center keypoint, direction and occupancy, respectively. To verify the performance of our method, we conduct experiments on the public PS2.0 dataset. The results have shown that our method outperforms state-of-the-art competitors by showing recall rate of 99.86% and precision rate of 99.82%. It is capable of achieving 65 frames per second (FPS) and satisfying a real-time detection performance. In contrast to the simultaneous detection of global and local information, our SCCN detector exclusively concentrates on the T-shape center information, which achieves comparable performance and significantly accelerates the inference time without non-maximum suppression (NMS).},
  archive      = {J_JRTIP},
  author       = {Zheng, Ruitao and Zhu, Haifei and Wu, Xinghua and Meng, Wei},
  doi          = {10.1007/s11554-024-01460-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {T-psd: T-shape parking slot detection with self-calibrated convolution network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRI-net: A model for insulator defect detection on
transmission lines in rainy backgrounds. <em>JRTIP</em>, <em>21</em>(3),
1–19. (<a href="https://doi.org/10.1007/s11554-024-01461-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission line insulators often operate in challenging weather conditions, particularly on rainy days. Continuous exposure to humidity and rain accelerates the aging process of insulators, leading to a decline in insulating material performance, the occurrence of cracks, and deformation. This situation poses a significant risk to the operation of the power system. Scene images collected on rainy days are frequently obstructed by rain lines, resulting in blurred backgrounds that significantly impact the performance of detection models. To improve the accuracy of insulator defect detection in rainy day environments, this paper proposes the DRI-Net (Derain-Insulator-net) detection model. Firstly, a dataset of insulator defects in rainy weather environments is constructed. Second, designing the de-raining model DRGAN and integrating it as an end-to-end DRGAN de-raining structural layer into the input end of the DRI-Net detection model, we significantly enhance the clarity and quality of images affected by rain, thereby reducing adverse effects such as image blurring and occlusion caused by rainwater. Finally, to enhance the lightweight performance of the model, partial convolution (PConv) and the lightweight upsampling operator CARAFE are utilized in the detection network to reduce the computational complexity of the model. The Wise-IoU bounding box regression loss function is applied to achieve faster convergence and improved detector accuracy. Experimental results demonstrate the effectiveness of the DRI-Net model in the task of rainy-day insulator defect detection, achieving an average precision MAP value of 82.65% in the established dataset. Additionally, an online detection system for rainy day insulator defects is designed in conjunction with the detection model, demonstrating practical engineering applications value.},
  archive      = {J_JRTIP},
  author       = {Ji, Chao and Gao, Mingjiang and Zhou, Siyuan and Liu, Junpeng and Zhu, Yongcan and Huang, Xinbo},
  doi          = {10.1007/s11554-024-01461-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DRI-net: A model for insulator defect detection on transmission lines in rainy backgrounds},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). <span class="math display"><em>η</em></span> -repyolo:
Real-time object detection method based on <span
class="math display"><em>η</em></span> -RepConv and YOLOv8.
<em>JRTIP</em>, <em>21</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01462-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based object detection methods often grapple with excessive model parameters, high complexity, and subpar real-time performance. In response, the YOLO series, particularly the YOLOv5s to YOLOv8s methods, has been developed by scholars to strike a balance between real-time processing and accuracy. Nevertheless, YOLOv8’s precision can fall short in certain specific applications. To address this, we introduce a real-time object detection method called $$\eta$$ -RepYOLO, which is built upon the $$\eta$$ -RepConv structure. This method is designed to maintain consistent detection speeds while improving accuracy. We begin by crafting a backbone network named $$\eta$$ -EfficientRep, which utilizes a strategically designed network unit- $$\eta$$ -RepConv and $$\eta$$ -RepC2f module, to reparameterize and subsequently generate an efficient inference model. This model achieves superior performance by extracting detailed feature maps from images. Subsequently, we propose the enhanced $$\eta$$ -RepPANet and $$\eta$$ -RepAFPN as the model’s detection neck, with the addition of the $$\eta$$ -RepC2f for optimized feature fusion, thus boosting the neck’s functionality. Our innovation continues with the development of an advanced decoupled head for detection, where the $$\eta$$ -RepConv takes the place of the traditional $$3 \times 3$$ conv, resulting in a marked increase in detection precision during the inference stage. Our proposed $$\eta$$ -RepYOLO method, when applied to distinct neck modules, $$\eta$$ -RepPANet and $$\eta$$ -RepAFPN, achieves mAP of 84.77%/85.65% on the PASCAL VOC07+12 dataset and AP of 45.3%/45.8% on the MSCOCO dataset, respectively. These figures represent a significant advancement over the YOLOv8s method. Additionally, the model parameters for $$\eta$$ -RepYOLO are reduced to 10.8M/8.8M, which is 3.6%/21.4% less than that of YOLOv8, culminating in a more streamlined detection model. The detection speeds clocked on an RTX3060 are 116 FPS/81 FPS, showcasing a substantial enhancement in comparison to YOLOv8s. In summary, our approach delivers competitive performance and presents a more lightweight alternative to the SOTA YOLO models, making it a robust choice for real-time object detection applications.},
  archive      = {J_JRTIP},
  author       = {Feng, Shuai and Qian, Huaming and Wang, Huilin and Wang, Wenna},
  doi          = {10.1007/s11554-024-01462-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {$$\eta$$ -repyolo: Real-time object detection method based on $$\eta$$ -RepConv and YOLOv8},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive complexity control for AV1 video encoder using
machine learning. <em>JRTIP</em>, <em>21</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11554-024-01463-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital videos are widely used on various platforms, including smartphones and other battery-powered mobile devices, which can suffer from energy consumption and performance constraints. Video encoders are responsible for compressing video data, enabling the use of this type of media by reducing the data rate while maintaining image quality. To promote the use of digital videos, the continuous improvement of digital video encoding standards is crucial. In this context, the Alliance for Open Media (AOM) developed the AV1 (AOMedia Video 1) format. However, the advanced tools and enhancements provided by AV1 come with a high computational cost. To address this issue, this paper presents the learning-based AV1 complexity controller (LACCO). The goal of LACCO is to dynamically optimize the encoding time of the AV1 encoder for HD 1080 and UHD 4K resolution videos. The controller achieves this goal by predicting the encoding time of future frames and classifying input videos according to their characteristics through the use of trained machine learning models. LACCO was integrated into the reference software of the AV1 encoder and its encoding time reduction ranges from 10 to 70%, with average error results ranging from 0.11 to 1.88 percentage points for HD 1080 resolution and from 0.14 to 3.33﻿ p﻿e﻿r﻿c﻿e﻿n﻿t﻿a﻿g﻿e﻿ points for UHD 4K resolution.},
  archive      = {J_JRTIP},
  author       = {Bender, Isis and Rehbein, Gustavo and Correa, Guilherme and Agostini, Luciano and Porto, Marcelo},
  doi          = {10.1007/s11554-024-01463-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adaptive complexity control for AV1 video encoder using machine learning},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-speed hardware accelerator based on brightness improved
by light-DehazeNet. <em>JRTIP</em>, <em>21</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01464-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing demand for artificial intelligence technology in today’s society, the entire industrial production system is undergoing a transformative process related to automation, reliability, and robustness, seeking higher productivity and product competitiveness. Additionally, many hardware platforms are unable to deploy complex algorithms due to limited resources. To address these challenges, this paper proposes a computationally efficient lightweight convolutional neural network called Brightness Improved by Light-DehazeNet, which removes the impact of fog and haze to reconstruct clear images. Additionally, we introduce an efficient hardware accelerator architecture based on this network for deployment on low-resource platforms. Furthermore, we present a brightness visibility restoration method to prevent brightness loss in dehazed images. To evaluate the performance of our method, extensive experiments were conducted, comparing it with various traditional and deep learning-based methods, including images with artificial synthesis and natural blur. The experimental results demonstrate that our proposed method excels in dehazing ability, outperforming other methods in comprehensive comparisons. Moreover, it achieves rapid processing speeds, with a maximum frame rate of 105 frames per second, meeting the requirements of real-time processing.},
  archive      = {J_JRTIP},
  author       = {Teng, Peiyi and Du, Gaoming and Li, Zhenmin and Wang, Xiaolei and Yin, Yongsheng},
  doi          = {10.1007/s11554-024-01464-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High-speed hardware accelerator based on brightness improved by light-DehazeNet},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive histogram equalization in constant time.
<em>JRTIP</em>, <em>21</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11554-024-01465-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Histogram Equalization (AHE) and its contrast-limited variant CLAHE are well-known and effective methods for improving the local contrast in an image. However, the fastest available implementations scale linearly with the filter mask size, which results in high execution times. This presents an obstacle in real-world applications, where large filter mask sizes are desired while maintaining low execution times. In this work, we propose an efficient algorithm for AHE that reduces the per-pixel computational complexity to $$\mathcal {O}(1)$$ . To the best of our knowledge, this is the first time that a constant-time algorithm is proposed for AHE and CLAHE. In contrast to commonly used fast implementations, our method computes the exact result for each pixel without interpolation artifacts. We benchmark and compare our method to existing algorithms. Our experiments show that our method exhibits superior execution times independent of the filter mask size, which makes AHE and CLAHE fast enough to be usable in real-world applications.},
  archive      = {J_JRTIP},
  author       = {Härtinger, Philipp and Steger, Carsten},
  doi          = {10.1007/s11554-024-01465-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adaptive histogram equalization in constant time},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time detection for miner behavior via DYS-YOLOv8n
model. <em>JRTIP</em>, <em>21</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01466-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of low real-time performance and poor algorithm accuracy in detecting miner behavior underground, we propose a high-precision real-time detection method named DSY-YOLOv8n based on the characteristics of human body behavior. This method integrates DSConv into the backbone network to enhance multi-scale feature extraction. Additionally, SCConv-C2f replaces C2f modules, reducing redundant calculations and improving model training speed. The optimization strategy of the loss function is employed, and MPDIoU is used to improve the model’s accuracy and speed. The experimental results show: (1) With almost no increase in parameters and calculation amount, the mAP50 of the DSY-YOLOv8n model is 97.4%, which is a 3.2% great improvement over the YOLOv8n model. (2) Compared to Faster-R-CNN, YOLOv5s, and YOLOv7, DYS-YOLOv8n has improved the average accuracy to varying degrees while significantly increasing the detection speed. (3) DYS-YOLOv8n meets the real-time requirements for behavioral detection in mines with a detection speed of 243FPS. In summary, the DYS-YOLOv8n offers a real-time, efficient, and lightweight method for detecting miner behavior in mines, which has high practical value.},
  archive      = {J_JRTIP},
  author       = {Xin, Fangfang and He, Xinyu and Yao, Chaoxiu and Li, Shan and Ma, Biao and Pan, Hongguang},
  doi          = {10.1007/s11554-024-01466-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time detection for miner behavior via DYS-YOLOv8n model},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time lossless image compression by dynamic huffman
coding hardware implementation. <em>JRTIP</em>, <em>21</em>(3), 1–10.
(<a href="https://doi.org/10.1007/s11554-024-01467-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the decades, implementing information technology (IT) has become increasingly common, equating to an increasing amount of data that needs to be stored, creating a massive challenge in data storage. Using a large storage capacity can solve the problem of the file size. However, this method is costly in terms of both capacity and bandwidth. One possible method is data compression, which significantly reduces the file size. With the development of IT and increasing computing capacity, data compression is becoming more and more widespread in many fields, such as broadcast television, aircraft, computer transmission, and medical imaging. In this work, we introduce an image compression algorithm based on the Huffman coding algorithm and use linear techniques to increase image compression efficiency. Besides, we replace 8-bit pixel-by-pixel compression by dividing one pixel into two 4-bit halves to save hardware capacity (because only 4-bit for each input) and optimize run time (because the number of different inputs is less). The goal is to reduce the image’s complexity, increase the data’s repetition rate, reduce the compression time, and increase the image compression efficiency. A hardware accelerator is designed and implemented on the Virtex-7 VC707 FPGA to make it work in real-time. The achieved average compression ratio is 3,467. Hardware design achieves a maximum frequency of 125 MHz.},
  archive      = {J_JRTIP},
  author       = {Lam, Duc Khai},
  doi          = {10.1007/s11554-024-01467-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time lossless image compression by dynamic huffman coding hardware implementation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Yolo-global: A real-time target detector for mineral
particles. <em>JRTIP</em>, <em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01468-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning methodologies have achieved significant advancements in mineral automatic sorting and anomaly detection. However, the limited features of minerals transported in the form of small particles pose significant challenges to accurate detection. To address this challenge, we propose a enhanced mineral particle detection algorithm based on the YOLOv8s model. Initially, a C2f-SRU block is introduced to enable the feature extraction network to more effectively process spatial redundant information. Additionally, we designed the GFF module with the aim of enhancing information propagation between non-adjacent scale features, thereby enabling deep networks to more fully leverage spatial positional information from shallower networks. Finally, we adopted the Wise-IoU loss function to optimize the detection performance of the model. We also re-designed the position of the prediction heads to achieve precise detection of small-scale targets. The experimental results substantiate the effectiveness of the algorithm, with YOLO-Global achieving a mAP@.5 of 95.8%. In comparison to the original YOLOv8s, the improved model exhibits a 2.5% increase in mAP, achieving a model inference speed of 81 fps, meeting the requirements for real-time processing and accuracy.},
  archive      = {J_JRTIP},
  author       = {Wang, Zihao and Zhou, Dong and Guo, Chengjun and Zhou, Ruihao},
  doi          = {10.1007/s11554-024-01468-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Yolo-global: A real-time target detector for mineral particles},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCSLFNet: A novel real-time pedestrian detection network
based on re-parameterized convolution and channel-spatial location
fusion attention for low-resolution infrared image. <em>JRTIP</em>,
<em>21</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01469-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel real-time infrared pedestrian detection algorithm is introduced in this study. The proposed approach leverages re-parameterized convolution and channel-spatial location fusion attention to tackle the difficulties presented by low-resolution, partial occlusion, and environmental interference in infrared pedestrian images. These factors have historically hindered the accurate detection of pedestrians using traditional algorithms. First, to tackle the problem of weak feature representation of infrared pedestrian targets caused by low resolution and partial occlusion, a new attention module that integrates channel and spatial is devised and introduced to CSPDarkNet53 to design a new backbone CSLF-DarkNet53. The designed attention model can enhance the feature expression ability of pedestrian targets and make pedestrian targets more prominent in complex backgrounds. Second, to enhance the efficiency of detection and accelerate convergence, a multi-branch decoupled detector head is designed to operate the classification and location of infrared pedestrians separately. Finally, to improve poor real-time without losing precision, we introduce the re-parameterized convolution (Repconv) using parameter identity transformation to decouple the training process and detection process. During the training procedure, to enhance the fitting ability of small convolution kernels, a multi-branch structure with convolution kernels of different scales is designed. Compared with the nice classical detection algorithms, the results of the experiment show that the proposed RCSLFNet not only detects partial occlusion infrared pedestrians in complex environments accurately but also has better real-time performance on the KAIST dataset. The mAP@0.5 reaches 86% and the detection time is 0.0081 s, 2.9% higher than the baseline.},
  archive      = {J_JRTIP},
  author       = {Hao, Shuai and Liu, Zhengqi and Ma, Xu and Wu, Yingqi and He, Tian and Li, Jiahao},
  doi          = {10.1007/s11554-024-01469-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RCSLFNet: A novel real-time pedestrian detection network based on re-parameterized convolution and channel-spatial location fusion attention for low-resolution infrared image},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware architecture optimization for high-frequency
zeroing and LFNST in h.266/VVC based on FPGA. <em>JRTIP</em>,
<em>21</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11554-024-01470-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the hardware implementation resource consumption of the two-dimensional transform component in H.266 VVC, a unified hardware structure is proposed that supports full-size Discrete Cosine Transform (DCT), Discrete Sine Transform (DST), and full-size Low-Frequency Non-Separable Transform (LFNST). This paper presents an area-efficient hardware architecture for two-dimensional transforms based on a general Regular Multiplier (RM) and a high-throughput hardware design for LFNST in the context of H.266/VVC. The first approach utilizes the high-frequency zeroing characteristics of VVC and the symmetric properties of the DCT-II matrix, allowing the RM-based architecture to use only 256 general multipliers in a fully pipelined structure with a parallelism of 16. The second approach optimizes the transpose operation of the input matrix for LFNST in a parallelism of 16 architecture, aiming to save storage and logic resources.},
  archive      = {J_JRTIP},
  author       = {Zhang, Junxiang and Sheng, Qinghua and Pan, Rui and Wang, Jiawei and Qin, Kuan and Huang, Xiaofang and Niu, Xiaoyan},
  doi          = {10.1007/s11554-024-01470-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware architecture optimization for high-frequency zeroing and LFNST in H.266/VVC based on FPGA},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA-based implementation of the VVC low-frequency
non-separable transform. <em>JRTIP</em>, <em>21</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11554-024-01471-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Versatile Video Coding (VVC) standard, released in July 2020, brings better coding performance than the High-Efficiency Video Coding (HEVC) thanks to the introduction of new coding tools. The transform module in the VVC standard incorporates the Multiple Transform Selection (MTS) concept, which relies on separable Discrete Cosine Transform (DCT)/Discrete Sine Transform (DST) kernels, and the recently introduced Low-Frequency Non-Separable Transform (LFNST). This latter serves as a secondary transform process, enhancing coding efficiency by further decorrelating residual samples. However, it introduces heightened computational complexity and substantial resource allocation demands, potentially complicating its hardware implementation. This paper introduces an effective and cost-efficient hardware architecture for LFNST. The proposed design employs additions and bit-shifting operations preserving hardware logic usage. The synthesis results for an Arria 10 10AX115N1F45E1SG FPGA device demonstrate that the logic cost is only of 26% of the available hardware resources. Additionally, the proposed design is working at 204 MHz and can process Ultra High Definition (UHD) 4K videos at up to 60 frames per second (fps).},
  archive      = {J_JRTIP},
  author       = {Belghith, Fatma and Ben Jdidia, Sonda and Abdallah, Bouthaina and Masmoudi, Nouri},
  doi          = {10.1007/s11554-024-01471-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-based implementation of the VVC low-frequency non-separable transform},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time segmentation algorithm of unstructured road scenes
based on improved BiSeNet. <em>JRTIP</em>, <em>21</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01472-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the fuzzy and complex boundaries of unstructured road scenes, as well as the high difficulty of segmentation, this paper uses BiSeNet as the benchmark model to improve the above situation and proposes a real-time segmentation model based on partial convolution. Using FasterNet based on partial convolution as the backbone network and improving it, adopting higher floating-point operations per second operators to improve the inference speed of the model; optimizing the model structure, removing inefficient spatial paths, and using shallow features of context paths to replace their roles, reducing model complexity; the Residual Atrous Spatial Pyramid Pooling Module is proposed to replace a single context embedding module in the original model, allowing better extraction of multi-scale context information and improving the accuracy of model segmentation; the feature fusion module is upgraded, the proposed Dual Attention Features Fusion Module is more helpful for the model to better understand image context through cross-level feature fusion. This paper proposes a model with a inference speed of 78.81 f/s, which meets the real-time requirements of unstructured road scene segmentation. Regarding accuracy metrics, the model in this paper excels with Mean Intersection over Union and Macro F1 at 72.63% and 83.20%, respectively, showing significant advantages over other advanced real-time segmentation models. Therefore, the real-time segmentation model based on partial convolution in this paper well meets the accuracy and speed required for segmentation tasks in complex and variable unstructured road scenes, and has reference value for the development of autonomous driving technology in unstructured road scenes. Code is available at https://github.com/BaiChunhui2001/Real-time-segmentation .},
  archive      = {J_JRTIP},
  author       = {Bai, Chunhui and Zhang, Lilian and Gao, Lutao and Peng, Lin and Li, Peishan and Yang, Linnan},
  doi          = {10.1007/s11554-024-01472-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time segmentation algorithm of unstructured road scenes based on improved BiSeNet},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOv5s-BC: An improved YOLOv5s-based method for real-time
apple detection. <em>JRTIP</em>, <em>21</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01473-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current apple detection algorithms fail to accurately differentiate obscured apples from pickable ones, thus leading to low accuracy in apple harvesting and a high rate of instances where apples are either mispicked or missed altogether. To address the issues associated with the existing algorithms, this study proposes an improved YOLOv5s-based method, named YOLOv5s-BC, for real-time apple detection, in which a series of modifications have been introduced. First, a coordinate attention block has been incorporated into the backbone module to construct a new backbone network. Second, the original concatenation operation has been replaced with a bi-directional feature pyramid network in the neck network. Finally, a new detection head has been added to the head module, enabling the detection of smaller and more distant targets within the field of view of the robot. The proposed YOLOv5s-BC model was compared to several target detection algorithms, including YOLOv5s, YOLOv4, YOLOv3, SSD, Faster R-CNN (ResNet50), and Faster R-CNN (VGG), with significant improvements of 4.6%, 3.6%, 20.48%, 23.22%, 15.27%, and 15.59% in mAP, respectively. The detection accuracy of the proposed model is also greatly enhanced over the original YOLOv5s model. The model boasts an average detection speed of 0.018 s per image, and the weight size is only 16.7 Mb with 4.7 Mb smaller than that of YOLOv8s, meeting the real-time requirements for the picking robot. Furthermore, according to the heat map, our proposed model can focus more on and learn the high-level features of the target apples, and recognize the smaller target apples better than the original YOLOv5s model. Then, in other apple orchard tests, the model can detect the pickable apples in real time and correctly, illustrating a decent generalization ability. It is noted that our model can provide technical support for the apple harvesting robot in terms of real-time target detection and harvesting sequence planning.},
  archive      = {J_JRTIP},
  author       = {Liu, Jingfan and Liu, Zhaobing},
  doi          = {10.1007/s11554-024-01473-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLOv5s-BC: An improved YOLOv5s-based method for real-time apple detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast detection of face masks in public places using
QARepVGG-YOLOv7. <em>JRTIP</em>, <em>21</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01476-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has resulted in substantial global losses. In the post-epidemic era, public health needs still advocate the correct use of medical masks in confined spaces such as hospitals and indoors. This can effectively block the spread of infectious diseases through droplets, protect personal and public health, and improve the environmental sustainability and social resilience of cities. Therefore, detecting the correct wearing of masks is crucial. This study proposes an innovative three-class mask detection model based on the QARepVGG-YOLOv7 algorithm. The model replaces the convolution module in the backbone network with the QARepVGG module and uses the quantitative friendly structure and re-parameterization characteristics of the QARepVGG module to achieve high-precision and high-efficiency target detection. To validate the effectiveness of our proposed method, we created a mask dataset of 5095 pictures, including three categories: correct use of masks, incorrect use of masks, and individuals who do not wear masks. We also employed data augmentation techniques to further balance the dataset categories. We tested YOLOv5s, YOLOv6, YOLOv7, and YOLOv8s models on self-made datasets. The results show that the QARepVGG-YOLOv7 model has the best accuracy compared with the most advanced YOLO model. Our model achieves a significantly improved mAP value of 0.946 and a faster fps of 263.2, which is 90.8 fps higher than the YOLOv7 model and a 0.5% increase in map value over the YOLOv7 model. It is a high-precision and high-efficiency mask detection model.},
  archive      = {J_JRTIP},
  author       = {Guan, Chuying and Jiang, Jiaxuan and Wang, Zhong},
  doi          = {10.1007/s11554-024-01476-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast detection of face masks in public places using QARepVGG-YOLOv7},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A MEMS-based real-time structured light 3-d measuring
architecture on FPGA. <em>JRTIP</em>, <em>21</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01477-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With its its ability to non-contact measure three-dimensional information of objects and its extremely high accuracy advantage in close range, structured light 3-D measurement is widely used in various fields. However, some application scenarios, such as measuring moving objects and performing measurements in confined spaces, impose requirements for high speed and miniaturization in structured light 3-D measurement. Therefore, we propose a real-time structured light 3-D measurement system on FPGA. This system employs a four-step phase-shifting method to compute wrapped phases, complemented Gray code for phase unwrapping, and a cubic polynomial fitting approach for calculating the 3-D coordinates of points. We have proposed the optimized pipeline structure for each module. We have also proposed an optimized on-chip buffer structure to further improve throughput. The 3-D measurement speed of the proposed method can reach 76.9 fps, when the clock frequency is 100 MHz, and the image size is 2448 $$\times$$ 2048.},
  archive      = {J_JRTIP},
  author       = {Zhou, Wenbiao and Jia, Yunfei and Fan, Luyao and Fan, Gongyu and Lu, Fengchi},
  doi          = {10.1007/s11554-024-01477-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A MEMS-based real-time structured light 3-D measuring architecture on FPGA},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering individuals with disabilities: A real-time,
cost-effective, calibration-free assistive system utilizing eye
tracking. <em>JRTIP</em>, <em>21</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01478-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent innovations in real-time eye-tracking technology enhance accessibility, offering individuals with disabilities effective computer engagement. This study presents a cost-effective, calibration-free, eye-controlled system comprising two phases: scaling and feature extraction, followed by coordinate mapping. In the first phase, the MediaPipe framework extracts features, and scaling adapts to the display size. The second phase computes parameters for accurate mapping between the user’s iris and screen coordinates. MediaPipe’s pre-trained models and optimized architecture improve system efficiency and real-time performance, reducing the need for extensive dataset training. Adaptive scaling and iris detection optimizations enhance computational efficiency and responsiveness in real-time applications. The system is budget friendly, costing $100 or less, with a user-friendly Graphical User Interface (GUI) meeting essential daily requirements for individuals with disabilities. A total of 30 participants have been recruited for system testing, including disabled and non-disabled. The system takes an average of 0.0492127 s to process a frame while video acquisition, face and facial features tracking, and iris plotting on screen take 0.0333667, 0.0086365, and 0.0082976 s per frame, respectively. The system has achieved a mean typing speed of 21.7 and 15.8 CPM (characters per minute) for non-disabled and disabled participants, respectively. In addition, the system has demonstrated an average pixel accuracy of 25.2 pixels for non-disabled individuals and 29.32 pixels for disabled individuals. A system usability test exclusively involving disabled participants yielded promising results, with an average score of 90.6. The proposed eye-controlled system operates in real time, showcasing its responsiveness and effectiveness in enhancing computer accessibility for individuals with limited mobility or disabilities.},
  archive      = {J_JRTIP},
  author       = {Chhimpa, Govind Ram and Kumar, Ajay and Garhwal, Sunita and Dhiraj},
  doi          = {10.1007/s11554-024-01478-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Empowering individuals with disabilities: A real-time, cost-effective, calibration-free assistive system utilizing eye tracking},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-computing-assisted intelligent processing of
AI-generated image content. <em>JRTIP</em>, <em>21</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01400-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence-generated image content (AIGIC) is produced through the extraction of features and patterns from a vast image dataset, requiring substantial computational resources for training. Due to the limited computational resources of terminal devices, efficiently processing and responding to AIGIC has emerged as a critical concern in current research. To address this challenge, the present paper proposed the utilization of edge computing technology. Edge computing enables the offloading of certain training tasks to edge nodes, facilitating expedited task offloading strategies that empower terminal devices to generate image content efficiently. Building upon the edge serverless architecture, this paper introduces an edge serverless computing framework based on Web Assembly (WASM). Notably, this framework effectively resolves the latency issue associated with container cold start in serverless computing. Additionally, to enhance the collaborative capabilities of edge nodes, entropy-based Proximal Policy Optimization (E-PPO2) is proposed. This algorithm enables each edge node to share global rewards, continually update parameters, and ultimately derive the optimal response strategy, thereby harnessing edge device resources to their fullest potential. Finally, the efficacy of the proposed serverless computing architecture based on WASM is demonstrated through the evaluation of 13 benchmark functions. Comparative analyses with four task offloading algorithms highlight that the E-PPO2 algorithm, proposed in this article, significantly reduces task execution latency, facilitating rapid processing and response in AIGIC scenarios.},
  archive      = {J_JRTIP},
  author       = {Wang, Suzhen and Deng, Yongchen and Hu, Lisha and Cao, Ning},
  doi          = {10.1007/s11554-023-01400-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Edge-computing-assisted intelligent processing of AI-generated image content},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient versatile video coding motion estimation
hardware. <em>JRTIP</em>, <em>21</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01402-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Versatile Video Coding (VVC) is the latest video coding standard. It provides higher compression efficiency than the previous video coding standards at the cost of significant increase in computational complexity. Motion estimation (ME) is the most time consuming and memory intensive module in VVC encoder. Therefore, in this paper, we propose an efficient VVC ME hardware. It is the first VVC ME hardware in the literature. It has real time performance with small hardware area. This efficiency is achieved by using a 64 × 64 systolic processing element array to support maximum coding tree unit (CTU) size of 128 × 128 and by using a novel memory-based sum of absolute differences (SAD) adder tree to calculate SADs of 128 × 128 CTUs. The proposed VVC ME hardware reduces memory accesses significantly by using an efficient data reuse method. It can process up to 30 4 K (3840 × 2160) video frames per second.},
  archive      = {J_JRTIP},
  author       = {Ahmad, Waqar and Mahdavi, Hossein and Hamzaoglu, Ilker},
  doi          = {10.1007/s11554-023-01402-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient versatile video coding motion estimation hardware},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel lightweight real-time traffic sign detection method
based on an embedded device and YOLOv8. <em>JRTIP</em>, <em>21</em>(2),
1–10. (<a href="https://doi.org/10.1007/s11554-023-01403-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign recognition, as one of the key steps of intelligent driving technologies, effectively avoids most traffic accidents by detecting the location and type of traffic signs in real time and providing the information to drivers or autonomous vehicles promptly. In addition, edge devices close to users has become an inevitable requirement for the development of IoT technology, real-time computing and the realization of network edge intelligence. Nowadays, the YOLO algorithm in object detection, developed to YOLOv8, is accompanied by various lightweight networks and lightweight methods to win by “fast”, so this paper will propose an algorithm, fusing Ghost module and Efficient Multi-Scale Attention Module into YOLOv8, so that the model can improve the computing speed while maintaining the original characteristics. Furthermore, we choose Raspberry Pi as the object detection device due to its many characteristics such as lightweight, low power consumption. Through experiments, the model is trained on CCTSDB dataset, and the improved algorithm is tested on Raspberry Pi 4B. The results show that for three types of traffic signs, namely prohibited, indication and warning, the recognition accuracy mAP reaches 93.5% on the poor weather test set and 82.9% on the original test set, and the inference delay of Raspberry Pi reaches 0.79 s, which is effective in actual road test experiments. The improved model’s accuracy has increased by 6.4% and 3.8% on two separate test sets compared to the original model, while the detection time has been reduced by 0.12 s. This research is of great significance to the maturation of assisted driving and autonomous driving technologies.},
  archive      = {J_JRTIP},
  author       = {Luo, Yuechen and Ci, Yusheng and Jiang, Shixin and Wei, Xiaoli},
  doi          = {10.1007/s11554-023-01403-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel lightweight real-time traffic sign detection method based on an embedded device and YOLOv8},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MED-YOLOv8s: A new real-time road crack, pothole, and patch
detection model. <em>JRTIP</em>, <em>21</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time road damage detection and assessment is crucial to ensure road safety. Traditional road damage detection methods mostly rely on manual labor, which is not only inefficient, but it is also difficult to guarantee its reliability. In this study, a road damage detection model, MED-YOLOv8s, based on YOLOv8s is proposed. MobileNetv3 is adopted as the backbone of the detection algorithm, which reduces the number of parameters and the number of computations in the process of feature extraction, enabling the model to achieve a good balance between the detection speed and the detection accuracy. The introduction of the ultralightweight attention mechanism, ECA, adapts the optimization of the correlation of channels to improve the model generalization performance. In addition, replacing the standard convolution with the DW convolution in the 21st layer of the network not only eliminates part of the redundant feature maps but also better extracts the correlation information between the feature maps. In this study, we also discuss the influence of the mix-up data augmentation weight parameter on the detection effect of the model. The experimental results show that the mAP@0.5 of the MED-YOLOv8s model proposed in this study is 95.2%, which is 1.1% higher than that of the original model, and at the same time, the calculation amount of the model is reduced by 46.2%. This method not only improves the detection accuracy but also greatly reduces the model complexity, providing a reference for subsequent model migration.},
  archive      = {J_JRTIP},
  author       = {Zhao, Minghu and Su, Yaoheng and Wang, Jiuxin and Liu, Xinru and Wang, Kaihang and Liu, Zishen and Liu, Man and Guo, Zhou},
  doi          = {10.1007/s11554-023-01405-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MED-YOLOv8s: A new real-time road crack, pothole, and patch detection model},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCX-YOLOv5: Efficient helmet detection in complex power
warehouse scenarios. <em>JRTIP</em>, <em>21</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11554-023-01406-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of the original YOLOv5 algorithm in detecting whether power company employees are wearing helmets is low due to the complex monitoring scenarios in the power warehouse and the small size of the helmets. As a result, it cannot be applied to actual operations. To address this issue, we developed the MCX-YOLOv5 helmet detection algorithm. Our model utilizes the YOLOv5 architecture and integrates a Coordinate-Spatial Attention Module (CSAM) to effectively filter the spatiotemporal data of the feature inputs. Additionally, we implement a Multi-scale Asymmetric Convolutions (MAConv) downsampling module to improve the algorithm&#39;s sensitivity to feature scale variations. To address the challenge of task information cross-coupling in coupled heads, we propose a decoupled head that is less heavy than YOLOv6 as a substitute. Our enhanced model achieved a 2.7% rise in the mean Average Precision at 50 (mAP50) and a 4.9% improvement in mAP75 on our self-developed database through multiple experiments, with just a minimal increase in parameters. Our model has yielded significant performance improvements on the Kaggle open-source Hard Hat Workers Detection dataset (HHWD), the public Safety Helmet Wearing Dataset (SHWD), and the PASCAL Visual Object Classes (VOC) dataset. These results highlight the effectiveness of our proposed algorithm in achieving higher accuracy for safety helmet wear detection in storage scenarios.},
  archive      = {J_JRTIP},
  author       = {Xu, Hongchao and Wu, Zhenyu},
  doi          = {10.1007/s11554-023-01406-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MCX-YOLOv5: Efficient helmet detection in complex power warehouse scenarios},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rep-YOLO: An efficient detection method for mine personnel.
<em>JRTIP</em>, <em>21</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of underground personnel is one of the key technologies in computer vision. However, this detection technique is susceptible to complex environments, resulting in low accuracy and slow speed. To accurately detect underground coal mine operators in complex environments, we combine the underground image features with K-means++ clustering anchor frames and propose a new Re-parameterization YOLO (Rep-YOLO) detection algorithm. First, the Criss-Cross-Vertical with Channel Attention (CVCA) mechanism is introduced at the end of the network to capture the Long-Range Dependencies (LRDs) in the image. This mechanism also emphasizes the significance of different channels to enhance image processing performance and improve the representation ability of the model. Second, the new Deep Extraction of Re-parameterization (DER) backbone network is designed, which adopts the re-parameterization structure to reduce the number of parameters and computation of the model. Additionally, each DER-block fuses different scales of features to enhance the accuracy of the model’s detection capabilities. Finally, Rep-YOLO is optimized using a slim-neck structure, which reduces the complexity of the Rep-YOLO while maintaining sufficient accuracy. The results showed that the Rep-YOLO model proposed in this paper achieved an accuracy of $$87.5\%$$ , a recall rate of $$77.2\%$$ , an Average Precision (AP) of $$83.1\%$$ , and a Frame Per Second (FPS) of 71.9. Compared to eight different models, the recall, AP50, and FPS of the Rep-YOLO model were improved. The research shows that the Rep-YOLO model can provide a real-time and efficient method for mine personnel detection. Source code is released in https://github.com/DrLSB/Rep-YOLO .},
  archive      = {J_JRTIP},
  author       = {Shao, Xiaoqiang and Liu, Shibo and Li, Xin and Lyu, Zhiyue and Li, Hao},
  doi          = {10.1007/s11554-023-01407-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rep-YOLO: An efficient detection method for mine personnel},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time detection algorithm for digital meters based on
multi-scale feature fusion and GCS. <em>JRTIP</em>, <em>21</em>(2),
1–15. (<a href="https://doi.org/10.1007/s11554-023-01408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of insufficient feature fusion, large number of network parameters and low target saliency in the current digital meter detection algorithm, a digital meter detection algorithm based on YOLOv5s is designed. First, a new feature fusion structure Bi-Directional Feature Pyramid Network Based on Multi-Scale Feature Fusion is designed to realize the full fusion between different scale feature maps and improve the detection accuracy; second, a new convolutional module Ghostconv Combined Channel Shuffle, is designed to realize the lightweight design of the network and meet the requirements of real-time substation detection tasks; finally, to improve the network’s ability to characterize the instrumented digits, the Convolutional Block Attention Module is introduced in the backbone network to further enhance the network performance. Experiments are carried out on the homemade dataset, and the experimental results show that the algorithm proposed in this paper improves the average accuracy by 2.84–98.58% compared with the original network; the amount of network parameters is reduced by 26.4%, and the detection speed is improved by 25 FPS, and the detection time for each image is only 0.012 s. Compared with other digital meter detection algorithms, the network performance and the number of parameters also have a great advantage.},
  archive      = {J_JRTIP},
  author       = {Hao, Zhaoming and Zhang, Xiaoqiong and Li, Hongyan and Xu, Meng and Zhang, Ziyang and Wang, Zhan and Wang, Weifeng},
  doi          = {10.1007/s11554-023-01408-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection algorithm for digital meters based on multi-scale feature fusion and GCS},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale feature fusion with attention mechanism for
crowded road object detection. <em>JRTIP</em>, <em>21</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowded object detection under the heavy traffic environment is always a challenging task in the field of autonomous driving and robotics, because the dense gathering of vehicles or pedestrians inevitably bring heavy occlusion. It is difficult to distinguish highly overlapped objects and predict their bounding boxes accurately, especially for small objects far down the road. To address this challenge, this paper proposes an improved YOLOv5s network integrating a multi-scale feature fusion module with attention mechanism for crowded road object detection task. Specifically, to enhance the multi-scale representation of semantic features and to model the object scale variation flexibly, we introduce an attention-guided pyramid feature fusion strategy into the YOLOv5s backbone network. Then a C3CA module is designed by embedding the coordinate attention (CA) into the concentrated-comprehensive convolution (C3) module of the original YOLOv5s, which can boost the ability of extracting distinguishing features from the overlapped objects. In addition, we add implicit detection heads (IDHs) into the original YOLOv5s’s detection head part, which helps the network to learn implicit knowledge and improves the detection accuracy. Finally, a simplified optimal transport assignment (SimOTA) and a bounding box regression loss with dynamic focusing mechanism are used to improve the detector’s overall performance. Extensive experiments on the public dataset BDD100K and our self-built crowded road object dataset (XMRD) demonstrate the superiority of our model in crowded road scenarios. The mean average precision (mAP) of our model can achieve 71.2% and 88.2% on the BDD100K and XMRD datasets, respectively, which provides an improvement of +3% over the existing state of the art models.},
  archive      = {J_JRTIP},
  author       = {Wu, Jingtao and Dai, Guojun and Zhou, Wenhui and Zhu, Xudong and Wang, Zengguan},
  doi          = {10.1007/s11554-023-01409-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-scale feature fusion with attention mechanism for crowded road object detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementation of JPEG XS entropy encoding and decoding on
FPGA. <em>JRTIP</em>, <em>21</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01410-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JPEG XS is the latest international standard for shallow compression fields launched by the International Organization for Standardization (ISO). The coding standard was officially released in 2019. The JPEG XS standard can be encoded and decoded on different devices, but there is no research on the implementation of JPEG XS entropy codec on FPGAs. This paper briefly introduces JPEG XS encoding, proposes a modular design scheme of encoder and decoder on FPGA for the entropy encoding and decoding part, and parallelizes the algorithm in JPEG XS coding standard according to the characteristics of FPGA parallelization processing, mainly including low-latency optimization design, storage space optimization design. The optimized scheme in this paper scheme enables encoding speeds of up to 4 coefficients/clock and decoding speeds of up to 2 coefficients/clock, with a 75% reduction in encoding and decoding time. The maximum clock frequency of the entropy encoder is about 222.6 MHz, and the maximum clock frequency of the entropy decoder is about 127 MHz. The design and implementation of the FPGA-based JPEG XS entropy encoding and decoding algorithm is of great significance and provides ideas for the subsequent implementation and optimization of the entire JPEG XS standard on FPGAs. This work is the first in the world to propose the design and implementation of an algorithm that can implement the JPEG XS entropy encoding and decoding process on FPGA. It creates the possibility for the effective application of JPEG XS standard in more media.},
  archive      = {J_JRTIP},
  author       = {Tian, Shuang and Song, Qinghua and He, Jialin and Wang, Yihan and Nie, Kai and Du, Gang and Bu, Ling},
  doi          = {10.1007/s11554-023-01410-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Implementation of JPEG XS entropy encoding and decoding on FPGA},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining decisions of a light-weight deep neural network
for real-time coronary artery disease classification in magnetic
resonance imaging. <em>JRTIP</em>, <em>21</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In certain healthcare settings, such as emergency or critical care units, where quick and accurate real-time analysis and decision-making are required, the healthcare system can leverage the power of artificial intelligence (AI) models to support decision-making and prevent complications. This paper investigates the optimization of healthcare AI models based on time complexity, hyper-parameter tuning, and XAI for a classification task. The paper highlights the significance of a lightweight convolutional neural network (CNN) for analysing and classifying Magnetic Resonance Imaging (MRI) in real-time and is compared with CNN-RandomForest (CNN-RF). The role of hyper-parameter is also examined in finding optimal configurations that enhance the model’s performance while efficiently utilizing the limited computational resources. Finally, the benefits of incorporating the XAI technique (e.g. GradCAM and Layer-wise Relevance Propagation) in providing transparency and interpretable explanations of AI model predictions, fostering trust, and error/bias detection are explored. Our inference time on a MacBook laptop for 323 test images of size 100x100 is only 2.6 sec, which is merely 8 milliseconds per image while providing comparable classification accuracy with the ensemble model of CNN-RF classifiers. Using the proposed model, clinicians/cardiologists can achieve accurate and reliable results while ensuring patients’ safety and answering questions imposed by the General Data Protection Regulation (GDPR). The proposed investigative study will advance the understanding and acceptance of AI systems in connected healthcare settings.},
  archive      = {J_JRTIP},
  author       = {Iqbal, Talha and Khalid, Aaleen and Ullah, Ihsan},
  doi          = {10.1007/s11554-023-01411-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Explaining decisions of a light-weight deep neural network for real-time coronary artery disease classification in magnetic resonance imaging},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VI-NeRF-SLAM: A real-time visual–inertial SLAM with NeRF
mapping. <em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In numerous robotic and autonomous driving tasks, traditional visual SLAM algorithms estimate the camera’s position in a scene through sparse feature points and express the map by estimating the depth of sparse point clouds. However, practical applications require SLAM to create dense maps in real time, overcoming the sparsity and occlusion issues of point clouds. Furthermore, it is advantageous for SLAM map to possess an auto-completion capability, where the map can automatically infer and complete the remaining 20% when the camera observes only 80% of an object. Therefore, a more dense and intelligent map representation is needed. In this paper, we propose a Visual–Inertial SLAM with Neural Radiance Fields reconstruction to address the aforementioned challenges. We integrate the traditional rule-based optimization with NeRF. This approach allows for the real-time update of NeRF local functions by rapidly estimating camera motion and sparse feature point depths to reconstruct 3D scenes. To achieve better camera poses and globally consistent map, we address the issue of IMU noise spikes resulting from rapid motion changes, along with handling pose adjustments due to loop closure fusion. Specifically, we employ a form of widening the static noise covariance to refit the dynamic noise covariance. During loop closure fusion, we treat the pose adjustment between pre- and post-loop closure as a spatiotemporal transformation, migrating NeRF parameters from pre- to post- to expedite loop closure adjustments in NeRF mapping. Moreover, we extend this method to scenarios with only grayscale images. By expanding the color channels of grayscale images and conducting linear spatial mapping, we can rapidly reconstruct 3D scenes with only grayscale images. We demonstrate the precision and speed advantages of our method in both RGB and grayscale scenes.},
  archive      = {J_JRTIP},
  author       = {Liao, DaoQing and Ai, Wei},
  doi          = {10.1007/s11554-023-01412-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {VI-NeRF-SLAM: A real-time visual–inertial SLAM with NeRF mapping},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L-SSD: Lightweight SSD target detection based on
depth-separable convolution. <em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01413-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current target detection algorithm based on deep learning has many redundant convolution calculations, which are difficult to apply to low-energy mobile devices, such as intelligent inspection robots and automatic driving. To solve this problem, we propose a lightweight target detection algorithm, L-SSD, based on depth-separable convolution. First, we chose the lightweight network MobileNetv2 as the backbone feature extraction network, and we proposed an upsampling feature fusion module (UFFM) to fuse the output feature maps of MobileNetv2. Deep semantic information is introduced into the shallow feature map to improve the feature extraction capability while reducing the complexity of the model. Second, we propose a local–global feature extraction module (LGFEM), which uses LGFEM to generate five additional feature layers to expand the feature map’s receptive field and improve the model’s detection accuracy. Then, we use an improved weighted bidirectional feature pyramid (BiFPN) for feature fusion to construct a new feature pyramid that fully utilizes the feature information between different layers. Finally, we propose asymmetric spatial attention (ASA) that enhances the expression ability of the features before BiFPN feature fusion, providing good positional information for the feature pyramid. Experimental results on the PASCAL VOC and MS COCO datasets show that the model parameters and model complexity of L-SSD are reduced by 85.9% and 96.1%, respectively, compared to SSD. A detection speed of 106 frames per second was achieved in NVIDIA GeForce RTX 3060 with detection accuracies of 73.8% and 22.4%, respectively. The optimal balance of model parameters, model complexity, detection accuracy, and speed are achieved.},
  archive      = {J_JRTIP},
  author       = {Wang, Huilin and Qian, Huaming and Feng, Shuai and Wang, Wenna},
  doi          = {10.1007/s11554-024-01413-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {L-SSD: Lightweight SSD target detection based on depth-separable convolution},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-CBAM: A lightweight network for real-time scene
segmentation. <em>JRTIP</em>, <em>21</em>(2), 1–10. (<a
href="https://doi.org/10.1007/s11554-024-01414-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic segmentation poses a significant challenge in scene parsing. Despite traditional semantic segmentation networks have made remarkable leap-forwards in semantic accuracy, the performance of inference speed remains unsatisfactory. This paper introduces the Cross-CBAM network, a novel lightweight architecture designed for real-time semantic segmentation. Specifically, a Squeeze-and-Excitation Atrous Spatial Pyramid Pooling Module (SE-ASPP) is proposed to obtain variable field-of-view and multiscale information. Additionally, we propose a Cross Convolutional Block Attention Module (CCBAM), wherein a cross-multiply operation guides low-level detail information with high-level semantic information. Unlike previous approaches that leverage attention to concentrate on the relevant information in the backbone, CCBAM utilizes cross-attention for feature fusion within the Feature Pyramid Network (FPN) structure. Extensive experiments on the Cityscapes dataset and Camvid dataset demonstrate the effectiveness of the proposed Cross-CBAM model by achieving a promising trade-off between segmentation accuracy and inference speed. On the Cityscapes test set, we achieve 73.4% mIoU with a speed of 240.9 FPS and 77.2% mIoU with a speed of 88.6 FPS on NVIDIA GTX 1080Ti.},
  archive      = {J_JRTIP},
  author       = {Zhang, Zhengbin and Xu, Zhenhao and Gu, Xingsheng and Xiong, Juan},
  doi          = {10.1007/s11554-024-01414-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Cross-CBAM: A lightweight network for real-time scene segmentation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deeplab-YOLO: A method for detecting hot-spot defects in
infrared image PV panels by combining segmentation and detection.
<em>JRTIP</em>, <em>21</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01415-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of difficult operation and maintenance of PV power plants in complex backgrounds and combined with image processing technology, a method for detecting hot spot defects in infrared image PV panels that combines segmentation and detection, Deeplab-YOLO, is proposed. In the PV panel segmentation stage, MobileNetV2 was introduced into the Deeplabv3+ model. Empty convolution in the atrous spatial pyramid pooling (ASPP) structure was improved, and established a relationship between layer-level features, the CBAM attention mechanism was combined, which achieved fast and accurate segment of PV panels and avoided false detection of hot spots. In the hot-spot recognition stage, a lightweight MobileNetV3 network was designed to replace the YOLO v5 backbone network, a small defect prediction head was added, and EIOU was used as a loss function, which improved the speed and accuracy of hot-spot detection and enhanced the performance of the YOLO v5 model. The experimental results show that the optimized Deeplabv3+ model and YOLO v5 model improve the accuracy of segmenting PV panels in images and identifying hot-spot defects by 2.61% and 0.7%, respectively, compared with the original model. This proposed method can accurately segment the PV panels and then identify different sizes of hot-spot defects on the PV panels.},
  archive      = {J_JRTIP},
  author       = {Lei, Ye and Wang, Xiaoye and An, Aimin and Guan, Haijiao},
  doi          = {10.1007/s11554-024-01415-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deeplab-YOLO: A method for detecting hot-spot defects in infrared image PV panels by combining segmentation and detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling, hardware architecture, and performance analyses of
an AEAD-based lightweight cipher. <em>JRTIP</em>, <em>21</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s11554-024-01416-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring data security and integrity is crucial for achieving the highest level of protection and performance in modern cyber-physical systems (CPS). Authenticated encryption with associated data (AEAD) is an efficient and secure way to encrypt data that ensures confidentiality and authenticity. The proposed work focuses on image encryption using the TinyJAMBU cipher within the AEAD scheme. In this paper, image encryption using the TinyJAMBU cipher with software and hardware modeling has been proposed, and image encryption evaluation over standard matrices has been performed. The hardware architecture for TinyJAMBU has been implemented on the Xilinx Virtex-7 FPGA device. The implementation results are compared with the realization of other contemporary ciphers that make TinyJAMBU-128’s implementation better in terms of look-up tables (LUTs), slice utilization, and power consumption. In the experimentation phase, the results of TinyJAMBU-128/192/256 for image encryption have been compared with existing image encryption techniques. It has been observed that, compared to other implementations, the proposed image encryption application using TinyJAMBU provides better results for PSNR, MSE, RMSE, and UACI.},
  archive      = {J_JRTIP},
  author       = {Jhawar, Kartik and Gandhi, Jugal and Shekhawat, Diksha and Upadhyay, Aniket and Harkishanka, Avadh and Chaturvedi, Nitin and Santosh, M. and Pandey, Jai Gopal},
  doi          = {10.1007/s11554-024-01416-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Modeling, hardware architecture, and performance analyses of an AEAD-based lightweight cipher},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate and real-time visual detection algorithm for
environmental perception of USVS under all-weather conditions.
<em>JRTIP</em>, <em>21</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11554-024-01417-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the intricate and ever-changing nature of the marine environment, traditional marine survey methods are subject to numerous limitations. Unmanned surface vehicles (USVs) have gained significant popularity for their role in automatically identifying and positioning targets in the ocean. To enhance the environmental perception capabilities of USVs in complex marine environments, vision-based sea surface object detection algorithms have emerged as a crucial technological approach. In response to the unique challenges of sea surface object detection tasks and the various extreme situations encountered by USVs in real sea environments, YOLOv7 was chosen as the baseline model due to its excellent trade-off between speed and accuracy. We propose Efficient Multi-Scale Pyramid Attention Networks, which enable easy and rapid multi-scale feature fusion. Additionally, we improve the boundary box loss function. Building upon these optimizations, we develop a new detector called Marit-YOLO, which consistently achieves better accuracy and efficiency than the prior art across a wide spectrum of resource constraints. Marit-YOLO achieved a 5.0% increase in the average precision (AP) value on the independently collected Ocean Buoys Dataset. During generalization validation, the AP value also saw a 7.7% increase on the open-source Singapore Maritime Dataset. The Marit-YOLO algorithm achieves a real-time inference speed of 69 frames per second on a single RTX2080, enabling accurate and real-time target detection in complex sea environments.},
  archive      = {J_JRTIP},
  author       = {Dong, Kaiyuan and Liu, Tao and Shi, Zhen and Zhang, Yang},
  doi          = {10.1007/s11554-024-01417-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accurate and real-time visual detection algorithm for environmental perception of USVS under all-weather conditions},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MLGT: Multi-local guided tracker for visual object tracking.
<em>JRTIP</em>, <em>21</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01418-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing single-stream tracking pipelines achieve good performance improvements by joint feature extraction and interaction. These tracking pipelines establish a bidirectional information flow between the template frame and the search frame, using the correlation and dynamic changes between them to improve the modeling and representation capabilities of the object, thereby improving the accuracy and robustness of tracking. However, these tracking pipelines just use the highest level semantic information of the encoder, and the low-level features are only used to compute new activations, which cannot meet the fine-grained requirements of the tracking task. To solve this issue, we propose a new approach named multi-local guided tracker (MLGT), which merges features obtained at various depths to strengthen the interaction between different semantic information. Specifically, we divide the single-stream pipeline into fixed output stages, and each stage is responsible for extracting and processing different levels of features. Then, we pass the output features into an enhanced fusion module (EFM), which incorporates a shared encoder and concatenation operation. The encoder is used to further extract the information in the joint features, and the catenation operation used to fuse features from different output stages. We conduct extensive evaluations on five datasets, among which we achieve 70.5% SUC on the LaSOT dataset, which is 1.4% higher than the existing single-stream tracker OSTrack.},
  archive      = {J_JRTIP},
  author       = {Liang, Xingzhu and Chen, Miaomiao and Liu, Erhu},
  doi          = {10.1007/s11554-024-01418-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MLGT: Multi-local guided tracker for visual object tracking},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel multiplier-less convolution core for YOLO CNN ASIC
implementation. <em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01419-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The You Only Look Once (YOLO) algorithm has a good trade-off between accuracy and execution speed in object detection. The main bottleneck of execution speed in YOLO is the optimum implementation of the Convolutional Neural Network (CNN). Reducing convolution core resources to increase parallelism can significantly increase the execution speed of the Algorithm. A new ASIC Processing Element (PE) is presented in this paper to reduce power consumption and increase speed while utilizing fewer resources. A multiplier-less convolution core is proposed by replacing multipliers with multiplexer circuits and designing a 19-input adder. Reducing the weight word length to five bits and compensating for the accuracy with the new quantization, has made the accuracy of the new architecture competitive with previous works. Compared with the traditional convolutional core, the best-proposed core has been improved by 4.44X, 4.9X, and 32% in power consumption, area, and delay, respectively. Placing the proposed core in the PE, the power consumption, FPS, and accuracy were 1.76W, 55.8, and 78%, respectively. Although the proposed 3 × 3 convolution core was evaluated using YOLOv2 and YOLOv4-tiny, it is also applicable to YOLOv7 and YOLOv8.},
  archive      = {J_JRTIP},
  author       = {Bagherzadeh, Shoorangiz and Daryanavard, Hassan and Semati, Mohammad Reza},
  doi          = {10.1007/s11554-024-01419-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel multiplier-less convolution core for YOLO CNN ASIC implementation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time and accurate deep learning-based multi-organ
nucleus segmentation in histology images. <em>JRTIP</em>,
<em>21</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01420-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated nucleus segmentation is considered the gold standard for diagnosing some severe diseases. Accurate instance segmentation of nuclei is still very challenging because of the large number of clustered nuclei, and the different appearance of nuclei for different tissue types. In this paper, a neural network is proposed for fast and accurate instance segmentation of nuclei in histopathology images. The network is inspired by the Unet and residual nets. The main contribution of the proposed model is enhancing the classification accuracy of nuclear boundaries by moderately preserving the spatial features by relatively d the size of feature maps. Then, a proposed 2D convolution layer is used instead of the conventional 3D convolution layer, the core of CNN-based architectures, where the feature maps are first compacted before being convolved by 2D kernel filters. This significantly reduces the processing time and avoids the out of memory problem of the GPU. Also, more features are extracted when getting deeper into the network without degrading the spatial features dramatically. Hence, the number of layers, required to compensate the loss of spatial features, is reduced that also reduces the processing time. The proposed approach is applied to two multi-organ datasets and evaluated by the Aggregated Jaccard Index (AJI), F1-score and the number of frames per second. Also, the formula of AJI is modified to reflect the object- and pixel-level errors more accurately. The proposed model is compared to some state-of-the-art architectures, and it shows better performance in terms of the segmentation speed and accuracy.},
  archive      = {J_JRTIP},
  author       = {Ahmed, Noha Y.},
  doi          = {10.1007/s11554-024-01420-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and accurate deep learning-based multi-organ nucleus segmentation in histology images},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMFANet: A lightweight network with efficient multi-scale
feature aggregation for real-time semantic segmentation. <em>JRTIP</em>,
<em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01421-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the performance of real-time semantic segmentation has increasingly become a research focus for real-time applications such as autonomous driving. Although large deep models have excellent segmentation results, their inference speed is slow and the models are complex, which makes them difficult to deploy in practice. To address these problems, a lightweight network with efficient multi-scale feature aggregation for real-time semantic segmentation (EMFANet) is proposed in this paper, which employs the encoder–decoder framework with efficient channel attention mechanism. In EMFANet, the effective symmetric attention residual unit (SARU) is presented to rapidly obtain large amounts of multi-scale contextual information. The lightweight multi-scale information aggregation unit (MIAU) is presented for efficient fusion of multi-scale features. Experimental results on the Cityscapes test set show that EMFANet can obtain 72.1% mean intersection over union (mIoU) and 143 FPS with only 1.03 M parameters. It also has competitive segmentation capability on the low-resolution Camvid test set with a fast inference speed of 357 FPS. EMFANet achieves an outstanding performance balance between segmentation accuracy, inference speed and model size.},
  archive      = {J_JRTIP},
  author       = {Hu, Xuegang and Ke, Yan},
  doi          = {10.1007/s11554-024-01421-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EMFANet: A lightweight network with efficient multi-scale feature aggregation for real-time semantic segmentation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal feature markers for event cameras. <em>JRTIP</em>,
<em>21</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01422-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a marker and its real-time tracking method are proposed. Different from existing technologies that recognize the markers by spatial event features, which lead to many practical problems, we discover the possibility of using temporal features. Strobe LEDs (light emitting diode) are used as markers to produce periodically flipping events, and a fast clustering-based algorithm is designed to track and recognize these markers simultaneously. Experiments demonstrate that our methods have superior speed and accuracy compared to state-of-the-arts. The markers can be stably tracked in many challenging situations, thus can be used in various visual tracking applications. The proposed method introduces a new marker and its corresponding recognition algorithm for event camera-based targets tracking, offering a reliable solution for various applications.},
  archive      = {J_JRTIP},
  author       = {You, Yue and Zhu, Mingzhu and He, Bingwei and Wang, Yihong},
  doi          = {10.1007/s11554-024-01422-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Temporal feature markers for event cameras},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network accelerator with fast buffer design for
computer vision. <em>JRTIP</em>, <em>21</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01423-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the neural networks with convolution computation is widely used for image classification and recognition. For real-time implementation, the video buffer is required to store the image temperately. However, traditional buffers like CLSB (content line shift buffer) may experience delays during the read process, particularly when encountering line breaks or image changes. As for N × N convolution, the delay time is N−1 clocks for every row changing. As the image width is W, the delay time is 2W + N clocks for every frame changing. These delays can impact the efficiency and performance of the neural network. To overcome this challenge, this paper presented novel buffer design to avoid the delay at the line ends and frame change. By proactively fetching data ahead of time, the buffer can dynamically schedule the read operation and ensure that the subsequent data are correctly placed for efficient processing. This improvement in read latency contributes to enhanced performance and better utilization of computational resources within the hardware system. Then the full convolutional network accelerator is implemented with the fast buffer design and common computational kernel to save the hardware cost based on LeNet model. The results show that the accuracy can achieve 99.1% with MNIST dataset verification. By eliminating the waiting time, the modified buffer allows for more efficient processing in the image, and the fame rate for a computer vision can achieve 46 per second, to meet the real-time requirement.},
  archive      = {J_JRTIP},
  author       = {Hsia, Shih-Chang and Zhang, Yu-Xiang},
  doi          = {10.1007/s11554-024-01423-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Neural network accelerator with fast buffer design for computer vision},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing low-light images via skip cross-attention fusion
and multi-scale lightweight transformer. <em>JRTIP</em>, <em>21</em>(2),
1–14. (<a href="https://doi.org/10.1007/s11554-024-01424-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in environments with poor lighting conditions often suffer from insufficient brightness, significant noise, and color distortion, which is highly detrimental to subsequent high-level vision tasks. Low-light image enhancement requires effective feature extraction and fusion, and the advantages of transformer and convolution in image processing are complementary. Therefore, it is an intentional exploration to combine them in image enhancement. In this paper, we propose a novel UNet-like method for enhancing low-light images. Transformer blocks are stacked to form the encoder, and convolutional blocks are utilized in the decoder. First, considering that the Transformer can effectively capture global information and convolution can obtain local information, this paper improves the lightweight Transformer by integrating multi-scale depth-wise convolution into the feedforward network to extract comprehensive features. Then, we design a Skip Cross-Attention Module to replace the traditional skip connection, which combines feature maps from different stages of the encoder and decoder. To achieve better feature fusion, this module employs two masks: the Top-k Mask and the adaptive V Channel Mask based on maximum entropy. The Top-k Mask will filter out unfavorable features by preserving the top k scores of attention map, and the V Channel Mask utilizes the corrected V channel of the image as an illumination guide for enhancement. Finally, extensive experiments on seven datasets demonstrate that our method achieves good performance in both subjective and objective evaluations. Specifically, the runtime on LOL-v2-real dataset demonstrates that our method is close to achieving real-time performance.},
  archive      = {J_JRTIP},
  author       = {Zhang, Jianming and Xing, Zi and Wu, Mingshuang and Gui, Yan and Zheng, Bin},
  doi          = {10.1007/s11554-024-01424-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhancing low-light images via skip cross-attention fusion and multi-scale lightweight transformer},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The real-time detection method for coal gangue based on
YOLOv8s-GSC. <em>JRTIP</em>, <em>21</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01425-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of complex algorithm models, poor accuracy, and low real-time performance in the coal industry&#39;s coal gangue sorting, a lightweight real-time detection method called YOLOv8s-GSC is proposed based on the characteristics of coal gangue. This method incorporates the ghost module into the YOLOv8s backbone network to reduce the network&#39;s parameter count. Additionally, a slim-neck model is used for feature fusion, and a coordinate attention module is added to the backbone network to enhance the network&#39;s feature representation capability. The experimental results show: (1) The average precision of the YOLOv8s-GSC model is 91.2%, which is a 0.6% improvement over the YOLOv8s model. The parameters and floating-point computation are reduced by 36.0% and 41.6%, respectively. (2) Compared to other models such as FasterRCNN-resnet50, SSD-VGG16, YOLOv5s, YOLOv7, YOLOv8s-Mobilenetv3, and YOLOv8s-GSConv, the average precision is improved to varying degrees. (3) The YOLOv8s-GSC model achieves a detection speed of 115FPS, meeting the real-time requirements for coal gangue detection. In conclusion, the proposed YOLOv8s-GSC model provides a lightweight, real-time, and efficient detection method for coal gangue separation in the coal industry, demonstrating high practical value.},
  archive      = {J_JRTIP},
  author       = {Chen, Kaiyun and Du, Bo and Wang, Yanwei and Wang, Guoxin and He, Junxi},
  doi          = {10.1007/s11554-024-01425-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {The real-time detection method for coal gangue based on YOLOv8s-GSC},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving small object detection via context-aware and
feature-enhanced plug-and-play modules. <em>JRTIP</em>, <em>21</em>(2),
1–16. (<a href="https://doi.org/10.1007/s11554-024-01426-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small objects is a challenging task in computer vision due to the objects only occupying a limited number of pixels and having blurred contours. These factors result in minimal discriminative features being available to effectively model the objects. In this paper, we propose three lightweight plug-and-play modules that can be seamlessly integrated into object detection algorithms, particularly those in the YOLO series, to improve the accuracy of detecting small objects. The Spatially Enhanced Convolutional Block Attention Module (SE-CBAM) is integrated into the feature extraction layer of the network to enhance the feature extraction capability of neural networks. Additionally, a Contextual Information Pooling Enhancement Module (CIE-Pool) is included at the multi-scale feature fusion stage to extract and improve object background information, which enhances the recognition rate of small objects. To improve the detection of small objects, a new layer is added to the detection head, which incorporates the shallow feature map obtained from the feature extraction network after Adaptive Feature Processing (AFP), thereby obtaining more and richer information about small objects. The efficacy of the algorithm has been evaluated on the VisDrone2021 and AI-TOD datasets. The experimental results demonstrate that the method proposed in this paper greatly improves the detection accuracy of small objects while maintaining real-time capabilities. Furthermore, it maintains high accuracy and speed even when dealing with complex background conditions and detecting small objects with high blur.},
  archive      = {J_JRTIP},
  author       = {He, Xiao and Zheng, Xiaolong and Hao, Xiyu and Jin, Heng and Zhou, Xiangming and Shao, Lihuan},
  doi          = {10.1007/s11554-024-01426-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improving small object detection via context-aware and feature-enhanced plug-and-play modules},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and implementation of hybrid (radix-8 booth and TRAM)
approximate multiplier using 15-4 approximate compressors for image
processing application. <em>JRTIP</em>, <em>21</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01427-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript proposes a low-power and high-speed hybrid approximate multiplier using 15-4 approximate compressors in partial product stage for image processing application. Initially, the most significant bits (MSB) of approximate multiplier is encoded by approximate radix-8 Booth’s (R-8B) encoding, and also least significant bits (LSB) is encoded by approximate truncated-round approximate multiplier (TRAM) encoding both are used to rounding the LSB to the adjacent power of two. Then, approximate 15-4 compressors are subjugated in partial product lessening stage to produce MSB result. Then, the hybrid approximate multiplier under 15-4 approximate compressors is carried out in the application of image processing. The proposed approach is done in MATLAB and Vivado Design Suite 2018.1 simulator, then observes that the power consumption of proposed design attains 31.814%, 23.562% lower than existing models. Similarly, the velocity attains 42.63%, 6.263% higher than the existing models.},
  archive      = {J_JRTIP},
  author       = {Immareddy, Srikanth and Sundaramoorthy, Arunmetha and Alagarsamy, Aravindhan},
  doi          = {10.1007/s11554-024-01427-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Design and implementation of hybrid (radix-8 booth and TRAM) approximate multiplier using 15-4 approximate compressors for image processing application},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research and implementation of adaptive stereo matching
algorithm based on ZYNQ. <em>JRTIP</em>, <em>21</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01428-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is an important method in computer vision for simulating human binocular vision to acquire spatial distance information. Implementing high-precision and real-time stereo-matching algorithms on hardware platforms with limited resources remains a significant challenge. Although the semi-global stereo-matching algorithm strikes a good balance between obtaining accuracy in the disparity map and computational complexity, it uses a fixed window for matching, resulting in lower matching accuracy in image regions with depth discontinuities and weak textures. To address the shortcomings of existing semi-global stereo-matching algorithms, an adaptive window semi-global stereo-matching algorithm is proposed, along with post-processing disparity optimization through left–right consistency check and median filtering. On test images provided by the Middlebury dataset, the average matching accuracy improved by 5.07% compared to traditional-matching algorithms. This algorithm is implemented on a Zynq UltraScale + chip, utilising 42,072 LUTs, 66,532 registers, and 101 BRAMs for the entire stereo-matching architecture. For images with a resolution of 1280 × 720 and 64 disparity levels, the final-processing speed can reach 54.24 fps.},
  archive      = {J_JRTIP},
  author       = {Liang, Yong and Lin, Daoqian and Chen, Zetao and Zhi, Yan and Tan, Junwen and Yang, Zhenhao and Li, Jie},
  doi          = {10.1007/s11554-024-01428-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research and implementation of adaptive stereo matching algorithm based on ZYNQ},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TinyEmergencyNet: A hardware-friendly ultra-lightweight deep
learning model for aerial scene image classification. <em>JRTIP</em>,
<em>21</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01430-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of emergency response applications, real-time situational awareness is vital. Unmanned aerial vehicles (UAVs) with imagers have emerged as crucial tools for providing timely information in such scenarios. Convolutional neural networks (CNN) are effective in image processing. However, the deployment of CNN models in UAVs faces significant challenges. The CNN models involve large number of parameters and energy-costly floating-point computations beyond the memory and power available on-board the UAVs. To address these challenges, we propose a co-design optimization approach for deploying the EmergencyNet CNN model on resource-constrained UAVs. Our strategy includes channel-wise pruning to reduce the size and optimize the network architecture. Additionally, we apply additive powers-of-two (APoT) quantization to further compress the model and enhance computational efficiency. Using channel-wise network pruning we derive TinyEmergencyNet that is only 155KB in memory size and 50% smaller than EmergencyNet. This proposed approach is evaluated on Aerial Image Disaster Event Recognition (AIDER) dataset. We have achieved an F1-score of 93.6% with 4-bit APoT quantization that closely approaches the full precision (32-bit) accuracy of 94%. Furthermore, hardware-friendly bit-shifting operations as a result of APoT quantization present an added advantage in hardware accelerator implementations. This work pioneers the joint application of channel-wise pruning and non-uniform APoT quantization on EmergencyNet, presenting a suitable solution tailored for UAV-based emergency response applications.},
  archive      = {J_JRTIP},
  author       = {Mogaka, Obed M. and Zewail, Rami and Inoue, Koji and Sayed, Mohammed S.},
  doi          = {10.1007/s11554-024-01430-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {TinyEmergencyNet: A hardware-friendly ultra-lightweight deep learning model for aerial scene image classification},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight YOLOv8 integrating FasterNet for real-time
underwater object detection. <em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01431-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a underwater target detection method that optimizes YOLOv8s to make it more suitable for real-time and underwater environments. First, a lightweight FasterNet module replaces the original backbone of YOLOv8s to reduce the computation and improve the performance of the network. Second, we modify current bi-directional feature pyramid network into a fast one by reducing unnecessary feature layers and changing the fusion method. Finally, we propose a lightweight-C2f structure by replacing the last standard convolution, bottleneck module of C2f with a GSConv and a partial convolution, respectively, to obtain a lighter and faster block. Experiments on three underwater datasets, RUOD, UTDAC2020 and URPC2022 show that the proposed method has mAP $$_{50}$$ of 86.8%, 84.3% and 84.7% for the three datasets, respectively, with a speed of 156 FPS on NVIDIA A30 GPUs, which meets the requirement of real-time detection. Compared to the YOLOv8s model, the model volume is reduced on average by 24%, and the mAP accuracy is enhanced on all three datasets.},
  archive      = {J_JRTIP},
  author       = {Guo, An and Sun, Kaiqiong and Zhang, Ziyi},
  doi          = {10.1007/s11554-024-01431-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight YOLOv8 integrating FasterNet for real-time underwater object detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time framework for HD video defogging using modified
dark channel prior. <em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01432-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foggy weather reduces the quality of video capture and seriously affects the normal work of video surveillance, remote sensing monitoring, and intelligent driving. Many methods have been proposed to remove video haze. However, under the premise of ensuring real-time performance, their defogging effect needs to be further improved. This paper improves the dark channel prior (DCP) dehazing algorithm, and designs a defogging framework that takes into account good dehazing effect and real-time processing. First, an adaptive threshold segmentation algorithm is proposed, which can well solve the serious color cast problem in brighter areas in DCP. Second, an algorithm for preserving image details using gradients is proposed, which achieves a good balance between detail preservation and computational efficiency. Then, each frame of video is evenly divided into a plurality of sub-areas, and the sub-areas are sequentially processed in a pipeline manner, which improves calculation efficiency. Finally, a high-definition real-time video defogging framework with a resolution of 1920 × 1080 and 60 frames/s is realized on the ZYNQ 7035.},
  archive      = {J_JRTIP},
  author       = {Wu, Xinchun and Chen, Xiangyu and Wang, Xiao and Zhang, Xiaojun and Yuan, Shuxuan and Sun, Biao and Huang, Xiaobing and Liu, Lintao},
  doi          = {10.1007/s11554-024-01432-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time framework for HD video defogging using modified dark channel prior},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Real-time object detection method based on YOLOv5 and
efficient mobile network. <em>JRTIP</em>, <em>21</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-024-01433-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object detection algorithm YOLOv5, which is based on deep learning, experiences inefficiencies due to an overabundance of model parameters and an overly complex structure. These drawbacks hinder its deployment on mobile devices, which are constrained by their computational capabilities and storage capacities. Addressing these limitations, we introduce a lightweight object detection algorithm that harnesses the coordinate attention (CA) mechanism in synergy with the YOLOv5 framework. Our approach embeds the CA mechanism into MobileNetv2 to create MobileNetv2-CA, thereby replacing the CSDarkNet53 as YOLOv5’s backbone network. This innovation not only trims the model’s parameter count but also maintains a competitive level of accuracy. Further amplifying performance, we propose a multi-scale fast spatial pyramid pooling (MSPPF) layer, designed to expedite and refine the model’s handling of various input image dimensions. Complementing this, we incorporate MPANet, a feature fusion network comprising optimally designed upsampling and downsample modules, along with feature extraction cells. This configuration is devised to elevate detection precision while minimizing the parameter overhead. Empirical results showcase the prowess of our methodology: we achieve a mean average precision (mAP) of 87.6% on the PASCAL VOC07+12 dataset and an average precision (AP) of 39.4% on the MS COCO dataset, with the model’s parameter size being a mere 10.1MB. When compared to the original YOLOv5, our proposed model achieves a parameter reduction of 76.9% and operates at a velocity that is 1.72 times faster, reaching 54.9 frames per second (FPS) on an NVIDIA RTX3060. Versus SOTA techniques, our method demonstrates a commendable equilibrium between accuracy and real-time performance.},
  archive      = {J_JRTIP},
  author       = {Feng, Shuai and Qian, Huaming and Wang, Huilin and Wang, Wenna},
  doi          = {10.1007/s11554-024-01433-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time object detection method based on YOLOv5 and efficient mobile network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time attention-based embedded LSTM for dynamic sign
language recognition on edge devices. <em>JRTIP</em>, <em>21</em>(2),
1–13. (<a href="https://doi.org/10.1007/s11554-024-01435-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition attempts to recognize meaningful hand gesture movements and is a significant solution for intelligent communication across societies with speech and hearing impairments. Nevertheless, understanding dynamic sign language from video-based data remains a challenging task in hand gesture recognition. However, real-time gesture recognition on low-power edge devices with limited resources has become a topic of research interest. Therefore, this work presents a memory-efficient deep-learning pipeline for identifying dynamic sign language on embedded devices. Specifically, we recover hand posture information to obtain a more discriminative 3D key point representation. Further, these properties are employed as inputs for the proposed attention-based embedded long short-term memory networks. In addition, the Indian Sign Language dataset for calendar months is also proposed. The post-training quantization is performed to reduce the model’s size to improve resource consumption at the edge. The experimental results demonstrate that the developed system has a recognition rate of 99.7% and an inference time of 500 ms on a Raspberry Pi-4 in a real-time environment. Lastly, memory profiling is performed to evaluate the performance of the model on the hardware.},
  archive      = {J_JRTIP},
  author       = {Sharma, Vaidehi and Sharma, Abhishek and Saini, Sandeep},
  doi          = {10.1007/s11554-024-01435-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time attention-based embedded LSTM for dynamic sign language recognition on edge devices},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel finetuned YOLOv8 model for real-time underwater
trash detection. <em>JRTIP</em>, <em>21</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01439-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When recognizing underwater images, problems, including poor image quality and complicated backdrops, are significant. The main problem of underwater images is the blurriness and invisibility of objects present in an image. This study presents a unique object identification design built on a YOLOv8 (You Only Look Once) framework upgraded to address these problems and further improve the models&#39; accuracy. The study also helps in identifying underwater trash. The model is a two-phase detector model. The first phase has an Underwater Image Enhancer (UIE) data augmentation technique that works with Laplacian pyramids and gamma correctness methods to enhance the underwater images. The second phase, the proposed refined, innovative YOLOv8 model for classification purposes, takes the output from the first stage as its input. The YOLOv8 model&#39;s existing feature extractor is replaced in this study with a new feature extractor technique, HEFA, that yields superior results and better detection accuracy. The introduction of the UIE and HEFA feature extractor method represents the significant novelty of this paper. The proposed model is pruned simultaneously to eliminate unnecessary parameters and further condense the model. Pruning causes the model&#39;s accuracy to decline. Thus, the transfer learning procedure is employed to raise it. The trials’ findings show that the technique can detect objects with an accuracy of 98.5% and a mAP@50 of 98.1% and that its real-time detection speed on the GPU is double that of the YOLOv8m model&#39;s baseline performance.},
  archive      = {J_JRTIP},
  author       = {Gupta, Chhaya and Gill, Nasib Singh and Gulia, Preeti and Yadav, Sangeeta and Chatterjee, Jyotir Moy},
  doi          = {10.1007/s11554-024-01439-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel finetuned YOLOv8 model for real-time underwater trash detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSDNet: A lightweight ship detection network with improved
YOLOv7. <em>JRTIP</em>, <em>21</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01441-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate ship detection is critical for maritime transportation security. Current deep learning-based object detection algorithms have made marked progress in detection accuracy. However, these models are too heavy to be applied in mobile or embedded devices with limited resources. Thus, this paper proposes a lightweight convolutional neural network shortened as LSDNet for mobile ship detection. In the proposed model, we introduce Partial Convolution into YOLOv7-tiny to reduce its parameter and computational complexity. Meanwhile, GhostConv is introduced to further achieve lightweight structure and improve detection performance. In addition, we use Mosaic-9 data-augmentation method to enhance the robustness of the model. We compared the proposed LSDNet with other approaches on a publicly available ship dataset, SeaShips7000. The experimental results show that LSDNet achieves higher accuracy than other models with less computational cost and parameters. The test results also suggest that the proposed model can meet the requirements of real-time applications.},
  archive      = {J_JRTIP},
  author       = {Lang, Cui and Yu, Xiaoyan and Rong, Xianwei},
  doi          = {10.1007/s11554-024-01441-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LSDNet: A lightweight ship detection network with improved YOLOv7},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end framework for real-time violent behavior
detection based on 2D CNNs. <em>JRTIP</em>, <em>21</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-024-01443-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Violent behavior detection (VioBD), as a special action recognition task, aims to detect violent behaviors in videos, such as mutual fighting and assault. Some progress has been made in the research of violence detection, but the existing methods have poor real-time performance and the algorithm performance is limited by the interference of complex backgrounds and the occlusion of dense crowds. To solve the above problems, we propose an end-to-end real-time violence detection framework based on 2D CNNs. First, we propose a lightweight skeletal image (SI) as the input modality, which can obtain the human body posture information and richer contextual information, and at the same time remove the background interference. As tested, at the same accuracy, the resolution of SI modality is only one-third of that of RGB modality, which greatly improves the real-time performance of model training and inference, and at the same resolution, SI modality has higher inaccuracy. Second, we also design a parallel prediction module (PPM), which can simultaneously obtain the single image detection results and the inter-frame motion information of the video, which can improve the real-time performance of the algorithm compared with the traditional “detect the image first, understand the video later&quot; mode. In addition, we propose an auxiliary parameter generation module (APGM) with both efficiency and accuracy, APGM is a 2D CNNs-based video understanding module for weighting the spatial information of the video features, processing speed can reach 30–40 frames per second, and compared with models such as CNN-LSTM (Iqrar et al., Aamir: Cnn-lstm based smart real-time video surveillance system. In: 2022 14th International Conference on Mathematics, Actuarial, Science, Computer Science and Statistics (MACS), pages 1–5. IEEE, 2022) and Ludl et al. (Cristóbal: Simple yet efficient real-time pose-based action recognition. In: 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 581–588. IEEE, 1999), the propagation effect speed can be increased by an average of $$3 \sim 20$$ frames per second per group of clips, which further improves the video motion detection efficiency and accuracy, greatly improving real-time performance. We conducted experiments on some challenging benchmarks, and RVBDN can maintain excellent speed and accuracy in long-term interactions, and are able to meet real-time requirements in methods for violence detection and spatio-temporal action detection. Finally, we update our proposed new dataset on violence detection images (violence image dataset). Dataset is available at https://github.com/ChinaZhangPeng/Violence-Image-Dataset},
  archive      = {J_JRTIP},
  author       = {Zhang, Peng and Dong, Lijia and Zhao, Xinlei and Lei, Weimin and Zhang, Wei},
  doi          = {10.1007/s11554-024-01443-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An end-to-end framework for real-time violent behavior detection based on 2D CNNs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unmanned aerial vehicle (UAV) object detection algorithm
based on keypoints representation and rotated distance-IoU loss.
<em>JRTIP</em>, <em>21</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01444-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, significant progress has been made in the research field of unmanned aerial vehicle (UAV) object detection through deep learning. The proliferation of unmanned aerial vehicles has notably facilitated the acquisition of corresponding data. However, the presence of substantial rotated objects in various orientations within UAV data sets poses challenges for traditional horizontal box object detection methods. These conventional approaches struggle to precisely locate rotated objects. Consequently, algorithms for rotated bounding-box object detection have been proposed; however, some of these existing methods exhibit issues, including periodicity of angle and exchangeability of edges. We propose a joint key point representation and rotated distance loss object detection network to solve the above problems. It is mainly composed of the key point representation module and the rotated distance-IoU loss. The key-points representation is used to indirectly represent the angle parameter of the rotated bounding box. It accomplishes this by measuring the angle between the line connecting the center point of the rotated bounding box to a specific boundary center point and the horizontal line. Next, the coordinates of the center points of anchor and the center points of its boundary are used to obtain the height dimension of the rotated bounding box and the width dimension of a rotated bounding box is introduced. Like this, the rotated bounding box can be represented by two points and a width dimension. Also, based on the traditional rotated IoU loss which does not incorporate the distance between the center point of the prediction box and the center point of ground truth in the regression process, the rotated distance-IoU loss is proposed to replace the traditional rotated IoU loss, which speeds up the convergence of the network. We have conducted extensive experiments on the DOTA data set and the DroneVehicle data set and have demonstrated the effectiveness of the proposed method.},
  archive      = {J_JRTIP},
  author       = {Zhu, Hufei and Huang, Yonghui and Xu, Ying and Zhou, Jianhong and Deng, Fuqin and Zhai, Yikui},
  doi          = {10.1007/s11554-024-01444-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Unmanned aerial vehicle (UAV) object detection algorithm based on keypoints representation and rotated distance-IoU loss},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FE-YOLO: YOLO ship detection algorithm based on feature
fusion and feature enhancement. <em>JRTIP</em>, <em>21</em>(2), 1–13.
(<a href="https://doi.org/10.1007/s11554-024-01445-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology for detecting maritime targets is crucial for realizing ship intelligence. However, traditional detection algorithms are not ideal due to the diversity of marine targets and complex background environments. Therefore, we choose YOLOv7 as the baseline and propose an end-to-end feature fusion and feature enhancement YOLO (FE-YOLO). First, we introduce channel attention and lightweight Ghostconv into the extended efficient layer aggregation network of YOLOv7, resulting in the improved extended efficient layer aggregation network (IELAN) module. This improvement enables the model to capture context information better and thus enhance the target features. Second, to enhance the network’s feature fusion capability, we design the light spatial pyramid pooling combined with the spatial channel pooling (LSPPCSPC) module and the coordinate attention feature pyramid network (CA-FPN). Furthermore, we develop an N-Loss based on normalized Wasserstein distance (NWD), effectively addressing the class imbalance issue in the ship dataset. Experimental results on the open-source Singapore maritime dataset (SMD) and SeaShips dataset demonstrate that compared to the baseline YOLOv7, FE-YOLO achieves an increase of 4.6% and 3.3% in detection accuracy, respectively.},
  archive      = {J_JRTIP},
  author       = {Cai, Shouwen and Meng, Hao and Wu, Junbao},
  doi          = {10.1007/s11554-024-01445-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FE-YOLO: YOLO ship detection algorithm based on feature fusion and feature enhancement},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IPCRGC-YOLOv7: Face mask detection algorithm based on
improved partial convolution and recursive gated convolution.
<em>JRTIP</em>, <em>21</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-024-01448-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex scenarios, current detection algorithms often face challenges such as misdetection and omission when identifying irregularities in pedestrian mask wearing. This paper introduces an enhanced detection method called IPCRGC-YOLOv7 (Improved Partial Convolution Recursive Gate Convolution-YOLOv7) as a solution. Firstly, we integrate the Partial Convolution structure into the backbone network to effectively reduce the number of model parameters. To address the problem of vanishing training gradients, we utilize the residual connection structure derived from the RepVGG network. Additionally, we introduce an efficient aggregation module, PRE-ELAN (Partially Representative Efficiency-ELAN), to replace the original Efficient Long-Range Attention Network (ELAN) structure. Next, we improve the Cross Stage Partial Network (CSPNet) module by incorporating recursive gated convolution. Introducing a new module called CSPNRGC (Cross Stage Partial Network Recursive Gated Convolution), we replace the ELAN structure in the Neck part. This enhancement allows us to achieve higher order spatial interactions across different network hierarchies. Lastly, in the loss function component, we replace the original cross-entropy loss function with Efficient-IoU to enhance loss calculation accuracy. To address the challenge of balancing the contributions of high-quality and low-quality sample weights in the loss, we propose a new loss function called Wise-EIoU (Wise-Efficient IoU). The experimental results show that the IPCRGC-YOLOv7 algorithm improves accuracy by 4.71%, recall by 5.94%, mean Average Precision (mAP@0.5) by 2.9%, and mAP@.5:.95 by 2.7% when compared to the original YOLOv7 algorithm, which can meet the requirements for mask wearing detection accuracy in practical application scenarios.},
  archive      = {J_JRTIP},
  author       = {Zhou, Huaping and Dang, Anpei and Sun, Kelei},
  doi          = {10.1007/s11554-024-01448-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {IPCRGC-YOLOv7: Face mask detection algorithm based on improved partial convolution and recursive gated convolution},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time and high-performance MobileNet accelerator based
on adaptive dataflow scheduling for image classification.
<em>JRTIP</em>, <em>21</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01378-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) models equipped with depth separable convolution (DSC) promise a lower spatial complexity while retaining high model accuracy. However, little attention has been paid to their hardware architecture. Previous studies on DSC-based CNN accelerators typically use fixed computational models for various models, leading to an imbalance between power, efficiency, and performance. To address this problem, a novel, real-time DSC-based CNN accelerator that can accommodate field-programmable gate arrays (FPGAs) of different capacities and CNNs of different sizes is proposed in this paper. Attractively, a dynamically reconfigurable computing engine and block-convolution-based adaptive dataflow scheduling mode strike a trade-off between hardware resources and the processing speed in industrial processes. The proposed MobileNet accelerator was implemented and evaluated on the Xilinx XC7020 platform. Compared to previous FPGA-based accelerators, the experimental results showed that our approach can provide 10.86 GOPS of computational performance for full HD RGB images, meeting the needs of real industrial real-time applications.},
  archive      = {J_JRTIP},
  author       = {Sang, Xiaoting and Ruan, Tao and Li, Chunlei and Li, Huanyu and Yang, Ruimin and Liu, Zhoufeng},
  doi          = {10.1007/s11554-023-01378-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and high-performance MobileNet accelerator based on adaptive dataflow scheduling for image classification},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light-deeplabv3+: A lightweight real-time semantic
segmentation method for complex environment perception. <em>JRTIP</em>,
<em>21</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01380-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current semantic segmentation methods have high accuracy. However, it has the disadvantage of high computational complexity and time consumption, which makes it difficult to meet the application requirements in complex environments. To achieve fast and accurate semantic segmentation of images, we propose a lightweight semantic segmentation method called Light-Deeplabv3+. First, a MobileNetV2Lite-SE architecture with SE module is proposed as the backbone network of the model, which can reduce the number of model parameters and improve the segmentation speed. Second, we propose an ACsc-ASPP module based on asymmetric dilated convolution block (ADCB) and scSE module to solve the semantic information loss during feature extraction. Our improvements can obtain more semantic features and improve segmentation accuracy. Finally, we propose a DSC-Blaze module to replace the original $$3\times 3$$ standard convolution. It consists of depthwise separable convolution (DSC) and Blaze module, which can improve the model segmentation speed while maintaining the receptive field. The experimental results prove that the Mean Intersection over Union (MIoU) of Light-Deeplabv3+ on the PASCAL VOC2012 dataset is 73.15 $$\%$$ , and the parameter size is only 6.291MB. Its calculation amount is only 20.883G, and the speed on the 3060Ti platform is 37.66 frames per second (FPS). Compared with traditional Deeplabv3+, Light-Deeplabv3+ can achieve efficient and accurate image segmentation results with less computational overhead, and its performance is comparable to state-of-the-art algorithms.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Qian, Huaming},
  doi          = {10.1007/s11554-023-01380-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Light-deeplabv3+: A lightweight real-time semantic segmentation method for complex environment perception},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight ship target detection algorithm based on
improved YOLOv5s. <em>JRTIP</em>, <em>21</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01381-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of ship targets is the key technology of intelligent inland waterway navigation. Given the complicated navigation environment of inland waterway ships and model detection&#39;s low accuracy and efficiency, this paper proposes an enhanced detection algorithm MGS-YOLO based on YOLOv5s. Firstly, the original backbone network is replaced by the MobileNetv3 algorithm, and the improved network parameter is only 7.54 MB. Secondly, the Gated Convolution (GnConv) structure is introduced into the original feature fusion module, which effectively improves the spatial interaction ability of feature information at different levels and further reduced the computational complexity of the model. Finally, to further improve the training speed and reasoning accuracy of the model, the SCYLLA-IoU (SIoU) is introduced into MGS-YOLO to effectively solve the problem of mismatching in the direction between the real box and the regression box. The final results show that the mean Average Precision (mAP), F1, and Average Frames Per Second (AVGFPS) of MGS-YOLO reach 0.977, 0.95, and 95.24 on the established ship dataset. It means that MGS-YOLO does not lose prediction accuracy when reducing network parameters and it has certain real-time performance. Comparing with the current representative lightweight learning models YOLOv5s, YOLOv3-tiny, YOLOv4-tiny, and YOLOv7 with good performance, the MGS-YOLO model has higher detection accuracy and efficiency and provides certain technical support for the safety detection and management of inland ships.},
  archive      = {J_JRTIP},
  author       = {Qian, Long and Zheng, Yuanzhou and Cao, Jingxin and Ma, Yong and Zhang, Yuanfeng and Liu, Xinyu},
  doi          = {10.1007/s11554-023-01381-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight ship target detection algorithm based on improved YOLOv5s},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time continuous handwritten trajectories recognition
based on a regression-based temporal pyramid network. <em>JRTIP</em>,
<em>21</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11554-023-01382-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of dynamic gesture trajectory recognition, it is difficult to real-time recognize its semantics on the continuous handwritten trajectories because of the difficulty of trajectory segment accurately. In this paper, focuses on the semantic recognition for the handwritten trajectories of continuous numeric characters, a regression-based time pyramid network real-time recognition method is proposed. Firstly, we use corner detection algorithms to obtain the corner points of the fingers, and then construct reasonable convex functions to obtain the unique fingertip point. Then, we perform hierarchical construction of the extracted fingertip trajectory features using a time pyramid, and then aggregate the features that have undergone spatial semantic modulation and temporal rate modulation. Finally, utilizing the idea of regression detection, we predict and classify the extracted trajectory features in a specialized fully connected layer with N neural nodes. According to the experimental results, our method achieved a recognition accuracy of up to 78.87%, while also achieving a recognition speed of 32.69 fps. Our method achieves a good balance between recognition accuracy and recognition speed, which indicates that our approach has significant advantages in real-time recognition of continuous handwritten trajectories.},
  archive      = {J_JRTIP},
  author       = {Jian, Chengfeng and Wang, Mengqi and Ye, Min and Zhang, Meiyu},
  doi          = {10.1007/s11554-023-01382-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time continuous handwritten trajectories recognition based on a regression-based temporal pyramid network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-MSFR: Real-time natural disaster victim detection based
on improved YOLOv5 network. <em>JRTIP</em>, <em>21</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01383-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In catastrophic occurrences, the automatic, accurate, and fast object detection of images taken by unmanned aerial vehicles (UAVs) can significantly reduce the time required for manual search and rescue. Victim detection methods, however, are not sufficiently robust for identifying partially obscured and multiscale objects against diverse backgrounds. To overcome this problem, we propose a hybrid-domain attention mechanism algorithm based on YOLOv5 with multiscale feature reuse (YOLO-MSFR). First, to solve the issue of the victim target being easily masked by a complex background, which complicates target attribute representation, a channel-space domain attention method was built, to improve target feature expression ability. Second, to address the problem of easily missed multiscale characteristics of victim targets, a multiscale feature reuse MSFR module was designed to ensure that large-scale target features are effectively expressed while enhancing small-target feature expression ability. The MSFR module was designed based on dilated convolution to solve the problem of small-target feature information loss during downsampling in the backbone network. The features were reused by cascade residuals to reduce the model training parameters and avoid the disappearance of the network deepening gradient. Finally, the efficient intersection over union (EIOU) loss function was adopted to accelerate network convergence and improve the detection performance of the network to accurately locate victims. The proposed algorithm was compared with five classical target detection algorithms on data from multiple disaster environments to verify its advantages. The experimental results show that the proposed algorithm can accurately detect multiscale victim targets in complex natural disasters, with mAP reaching 91.0%. The image detection speed at 640 × 640 resolution was 42 fps, indicating good real-time performance.},
  archive      = {J_JRTIP},
  author       = {Hao, Shuai and Zhao, Qiulin and Ma, Xu and Wu, Yingqi and Gao, Shan and Yang, Chenlu and He, Tian},
  doi          = {10.1007/s11554-023-01383-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-MSFR: Real-time natural disaster victim detection based on improved YOLOv5 network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GPU optimization workflow for real-time execution of
ultra-high frame rate computer vision applications. <em>JRTIP</em>,
<em>21</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01384-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a GPU optimization methodology for real-time execution of ultra high frame rate applications with small frame sizes. While the use of GPUs for offline processing is well-established, real-time execution remains challenging due to the lack of real-time execution guarantees, especially for embedded GPUs. Our methodology introduces guidelines and a workflow by focusing on: (a) controlling latency by means of minimization of CPU-GPU interactions; (b) computation pruning; and (c) inter/intra-kernel optimizations. Furthermore, our approach takes advantage of multi-frame processing to attain significantly higher throughput at the cost of increased latency when the application permits such trade-offs. To evaluate our optimization methodology, we applied it to the monitoring and controlling of laser powder bed fusion machines, a widely used metal additive manufacturing technique. Results show that in the considered application, the required performance could be obtained on a Jetson Xavier AGX platform, and by sacrificing latency, significantly higher throughput was achieved.},
  archive      = {J_JRTIP},
  author       = {Nourazar, Mohsen and Booth, Brian G. and Goossens, Bart},
  doi          = {10.1007/s11554-023-01384-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A GPU optimization workflow for real-time execution of ultra-high frame rate computer vision applications},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time AI-assisted visual exercise pose correctness
during rehabilitation training for musculoskeletal disorder.
<em>JRTIP</em>, <em>21</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01385-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exercise therapy is a prominent method for recovering from musculoskeletal disorders like Work-related Musculoskeletal Disorder (WMSD), Classroom-related Musculoskeletal Disorder (CMSD). This method requires the monitoring of a physiotherapist to advise the performer to do the exercise poses correctly. This process is costly and requires the presence of a physiotherapist. In this work, we plan to develop software for the real-time detection of exercise poses with the help of AI. This work replaces the physiotherapist and provides good real-time feedback on the users&#39; wrist extension and flexion exercise posture. Artificial Intelligence (AI) is the most popular and widely used technique for real-time image analysis. In real-time the user takes at least 1 s to perform the exercise pose. Our model classifies and provides feedback within 0.79 s. The frame processing rate of our model is ~ 21 frames per second. Experimentation of this framework was done through CNN DenseNet with 2-level architecture. We have experimented with three different ways of outcomes. Our mode achieved 100% accuracy with 106 samples of data and 99.86% accuracy with 2160 samples. Finally, with 144 cross-person sample datasets achieved 83.33% accuracy. This technique performs well for evaluating wrist extension exercise poses for recovery and gives participants immediate feedback on whether their wrist extension is correct or incorrect.},
  archive      = {J_JRTIP},
  author       = {Ekambaram, Dilliraj and Ponnusamy, Vijayakumar},
  doi          = {10.1007/s11554-023-01385-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time AI-assisted visual exercise pose correctness during rehabilitation training for musculoskeletal disorder},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight parameter de-redundancy demoiréing network with
adaptive wavelet distillation. <em>JRTIP</em>, <em>21</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01386-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks (CNNs) have achieved significant advancements in single image demoiréing. However, most of the existing CNN-based demoiréing methods require excessive memory usage and computational cost, which considerably limit to apply on mobile devices. Additionally, most CNN-based methods employ expensive approaches to generate similar feature maps, thereby resulting in redundant parameters within these networks. To alleviate these issues, we propose a lightweight parameter de-redundancy network (PDNet) for image demoiréing. Specifically, we present an efficient ghost block (EGB) that utilizes dilated convolution and cost-efficient operation, which significantly reduces parameters and extracts representative features. Meanwhile, we design a multi-scale shuffle fusion mechanism (MSFM) with a low amount of parameters to integrate different scales of features, which mitigates the information loss issue. To enable the lightweight network for learning latent moiré removal knowledge better, we adopt adaptive wavelet distillation to guide the network training. Experimental results validate the efficacy of our proposed method, achieving comparable or even superior results to the state-of-the-art method, while utilizing only 26.32% of parameters and 29.59% of Macs. The source code will be publicly available at https://github.com/ChenJiaCong-1005/PDNet .},
  archive      = {J_JRTIP},
  author       = {Chen, Jiacong and Mao, Qingyu and Bao, Youneng and Huang, Yingcong and Meng, Fanyang and Liang, Yongsheng},
  doi          = {10.1007/s11554-023-01386-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight parameter de-redundancy demoiréing network with adaptive wavelet distillation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time object detection method for underwater complex
environments based on FasterNet-YOLOv7. <em>JRTIP</em>, <em>21</em>(1),
1–14. (<a href="https://doi.org/10.1007/s11554-023-01387-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A FasterNet-You Only Look Once (YOLO)v7 algorithm is proposed for underwater complex environments with blurred images and complex backgrounds, which lead to difficulties in object target feature extraction and target miss detection, and to improve the fusion capability and real-time detection of small underwater targets. Before training the improved model, the original images acquired by the underwater robot are preprocessed in combination with the Underwater Image Enhancement Convolutional Neural Network (UWCNN) algorithm, which helps to identify targets accurately in the complex marine environment. First, to extract spatial features more efficiently, the algorithm uses Faster Neural Networks (FasterNet-L) as the backbone network model as well as an improved loss function, Focal Efficient Intersection over Union Loss (Focal-EIOU Loss), to reduce redundant computations and memory access, and the regression process focuses on high-quality anchor frames. Second, for the problem of poor robustness of small targets in an underwater environment, the algorithm uses the Cross-modal Transformer Attention (CoTAttention) lightweight attention mechanism to improve the original algorithm so that the detection targets are enhanced in channel and spatial dimensions. Finally, the experimental results show that the mean average precision (mAP) value of this paper&#39;s algorithm reaches 91.8% and the actual detection video frame rate reaches 83.21. FasterNet-YOLOv7 has higher detection accuracy compared with Faster Region-Based Convolutional Neural Network (Faster RCNN), Single Shot MultiBox Detection (SSD), YOLOv4, YOLOv5, and YOLOv7 models and is more accurate.},
  archive      = {J_JRTIP},
  author       = {Yang, Qing and Meng, Huijuan and Gao, Yuchen and Gao, Dexin},
  doi          = {10.1007/s11554-023-01387-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time object detection method for underwater complex environments based on FasterNet-YOLOv7},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVBS-CAT: Enhanced video background subtraction with a
controlled adaptive threshold for constrained wireless video
surveillance. <em>JRTIP</em>, <em>21</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01388-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object detection (MOD) has gained significant attention for its application in advanced video surveillance tasks. Region-of-Interest (ROI) detection algorithms are essential prerequisites for various applications, ranging from video surveillance to adaptive video coding. The simplicity and efficiency of MOD methods are critical when targeting energy-constrained systems, such as Wireless Multimedia Sensor Networks (WMSN). The challenge is always to reduce computational costs while preserving high detection accuracy. In this article, we present EVBS-CAT, an Enhanced Video Background Subtraction with a Controlled Adaptive Threshold selection method for low-cost surveillance systems. The proposed moving object detection method utilizes background subtraction (BS) with morphological operations and adaptive thresholding. We evaluate the algorithm using the Change Detection 2012 dataset. Through a computational complexity analysis of each step, we demonstrate the efficiency of the proposed MOD technique for embedded WMSN. The algorithm yields promising results compared to state-of-the-art MOD techniques in the context of embedded wireless surveillance.},
  archive      = {J_JRTIP},
  author       = {Aliouat, Ahcen and Kouadria, Nasreddine and Maimour, Moufida and Harize, Saliha},
  doi          = {10.1007/s11554-023-01388-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EVBS-CAT: Enhanced video background subtraction with a controlled adaptive threshold for constrained wireless video surveillance},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning-based fast CU size decision for
geometry videos in v-PCC. <em>JRTIP</em>, <em>21</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01389-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video-based point cloud compression (V-PCC), the geometry video and attribute video are generated through the projection of the 3D dynamic point clouds (DPCs), which can be further encoded with the High-Efficiency Video Coding (HEVC) standard. However, due to the huge amount of data in the DPC, the coding complexity is extremely high when encoding the geometry and attribute videos with V-PCC. In this work, we focus on compression for geometry videos which is completely different from natural videos. As a new coding structure has been adopted to encode the geometry video with V-PCC under the All Intra (AI) configuration, the existing methods to reduce the coding complexity are not suitable for geometry videos. Hence, we propose a fast Coding Unit (CU) size decision method for geometry videos based on unsupervised learning. As the geometry video contains regions of sharp edges and different scales, we design a novel hierarchical clustering method to determine the optimal CU sizes for encoding. In addition, an adaptive linkage threshold is introduced in the clustering process according to different quantization parameters and CU sizes. Experimental results show that, under the AI configuration, the savings in the coding time for the geometry videos from different DPCs achieved by the proposed approach range from 56.7% to 69.3% on average while only 0.1–0.5% of increments in Bjøntegaard delta total rate (BD-TotalRate) can be observed. Compared to various competing methods, the proposed approach achieves a better balance between coding complexity and encoding loss.},
  archive      = {J_JRTIP},
  author       = {Li, Yue and Huang, Jun and Wang, Chaofeng and Huang, Hongyue},
  doi          = {10.1007/s11554-023-01389-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Unsupervised learning-based fast CU size decision for geometry videos in V-PCC},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved discrete tchebichef transform approximations for
efficient image compression. <em>JRTIP</em>, <em>21</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01390-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Discrete Tchebichef Transform (DTT) has gained popularity as a signal processing tool for image and video compression due to its efficient coding and decorrelation properties. However, in the context of real-time applications and embedded systems, it is critical to develop approximate algorithms with reduced complexity and energy consumption. While three DTT approximations have been proposed to date, there is still room for further improvements. To address this gap, we propose two new low-complexity DTT approximations that employ a modified deviation metric, resulting in better compression efficiency and reduced complexity. We validate our proposed methods by implementing them on the Xilinx Virtex-6 XC6VSX475T-1FF1759-2 Field Programmable Gate Array (FPGA) through rapid prototyping. Our proposed transformations exhibit superior performance in terms of hardware resources and energy consumption, particularly for 1D 8 inputs. Furthermore, compared to the state-of-the-art DTT approximations in image compression, our proposed transformations demonstrate a quality gain of up to 2 dB. Overall, our proposed approximations provide a promising trade-off between image quality, hardware resources, and energy consumption, making them ideal for real-time applications and embedded systems.},
  archive      = {J_JRTIP},
  author       = {Mefoued, Abdelkader and Kouadria, Nasreddine and Harize, Saliha and Doghmane, Noureddine},
  doi          = {10.1007/s11554-023-01390-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved discrete tchebichef transform approximations for efficient image compression},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HW/SW co-design on embedded SoC FPGA for star tracking
optimization in space applications. <em>JRTIP</em>, <em>21</em>(1),
1–13. (<a href="https://doi.org/10.1007/s11554-023-01391-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Star trackers are crucial for satellite orientation. Improving their efficiency via reconfigurable COTS HW accommodates NewSpace missions. The current work considers SoC FPGAs to leverage both increased reprogramming and high-performance capabilities. Based on a custom sensor+FPGA system, we develop and optimize the algorithmic chain of star tracking by focusing on the acceleration of the image processing parts. We combine multiple circuit design techniques, such as low-level pipelining, word-length optimization, HW/SW co-processing, and parametric HLS+HDL coding, to fine-tune our implementation on Zynq-7020 FPGA when using real and synthetic input data. Overall, with 4-MPixel images, we achieve more than 24 FPS throughput by accelerating &gt;95% of the computation by 8.9 $$\times$$ , at system level, while preserving the original SW accuracy and meeting the real-time requirements of the application.},
  archive      = {J_JRTIP},
  author       = {Panousopoulos, Vasileios and Papaloukas, Emmanouil and Leon, Vasileios and Soudris, Dimitrios and Koumandakis, Emmanuel and Lentaris, George},
  doi          = {10.1007/s11554-023-01391-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HW/SW co-design on embedded SoC FPGA for star tracking optimization in space applications},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance verification and latency time evaluation of
hardware image processing module for appearance inspection systems using
FPGA. <em>JRTIP</em>, <em>21</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01392-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes a hardware-accelerated image processing module for visual inspection systems. These systems are essential for maintaining product quality and decreasing manual inspection. The proposed system harnesses FPGA technology to enhance the efficiency of image processing tasks, with a specific focus on filtering and labeling processes. To evaluate the performance gains achieved through hardware processing, the latency metric is put to use, which holds significant importance in real-time applications. The FPGA-based hardware processing methods have been shown to be highly effective in enhancing the performance of visual inspection systems. The experimental results prove that these methods successfully reduce latency up to the microsecond level, resulting in remarkable improvements in overall system performance. On average, the FPGA-based solution is demonstrated to be 10–100 times faster than conventional processors. Additionally, a notable advantage of this approach is its ability to synchronize processing with the inspection target flow, leveraging the camera device and sensor timing.},
  archive      = {J_JRTIP},
  author       = {Hoshino, Yukinobu and Shimasaki, Masahiro and Rathnayake, Namal and Dang, Tuan Linh},
  doi          = {10.1007/s11554-023-01392-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance verification and latency time evaluation of hardware image processing module for appearance inspection systems using FPGA},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving vertical federated broad learning system
for artificial intelligence generated image content. <em>JRTIP</em>,
<em>21</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01393-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence generated image content generates desired image content according to given instructions. It is widely applied in digital marketing, video game design, and movie production. How to efficiently detect the quality of the generated images is one of the key problems. A broad learning system is a feasible solution, because it requires no complex parameter calculation and depth expansion. However, the broad learning system is built by centralized learning, so it may not take advantage of isolated feature data, which prevents broad learning systems from detecting isolated AL-generated image content. To address the above problem, this paper proposes a privacy-preserving vertical federated broad learning system for artificial intelligence generated image content (PVF-BLS). In PVF-BLS, the vertical federated architecture is introduced into the broad learning system to utilize isolated feature data and interactive data are masked by matrix masks for security. To further improve the security of PVF-BLS, this paper proposes a secure incremental learning algorithm based on matrix masks (ILA-MM) to update PVF-BLS. In ILA-MM, the interaction data are processed by matrix masks, which makes PVF-BLS more secure. Experimental results show that PVF-BLS outperforms the traditional broad learning systems when the data are isolated.},
  archive      = {J_JRTIP},
  author       = {Li, Fengyin and Ge, Junrong and Wang, Xiaojiao and Zhao, Gang and Yu, Xilong and Li, Xinru},
  doi          = {10.1007/s11554-023-01393-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Privacy-preserving vertical federated broad learning system for artificial intelligence generated image content},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge devices-oriented surface defect segmentation by
GhostNet fusion block and global auxiliary layer. <em>JRTIP</em>,
<em>21</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01394-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an approach for the segmentation of surface defects, referred to as Efficient Surface Defect Network (ESD-Net). The proposed method uses novel components called the GhostNet Fusion Block (GFB) and the Global Auxiliary Layer (GAL) to make it edge computing-ready and to increase its performance on segmentation. The GFB algorithm employs a technique whereby it conserves and combines feature maps of reduced resolution from the original image with feature maps that have been downsampled at various resolutions. Moreover, the GAL amplifies the GFB by including comprehensive contextual information from a global perspective. The experiment shows that the proposed method outperforms state-of-the-art algorithms on SD-saliency-900, MSD, and Magnetic-tile, three public surface defect datasets with mIoU of $$82.4\%$$ , $$92.9\%$$ , and $$78.8\%$$ , respectively. Embedded device experiments have proven that ESDNet can be utilized on a wider range of cost-effective industrial devices with acceptable latency.},
  archive      = {J_JRTIP},
  author       = {Ardiyanto, Igi},
  doi          = {10.1007/s11554-023-01394-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Edge devices-oriented surface defect segmentation by GhostNet fusion block and global auxiliary layer},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speed matters, a robust infrared and visible image matching
method at real-time speed. <em>JRTIP</em>, <em>21</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01395-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching is a crucial step in executing many complex visual tasks, which involve identifying the same or similar visual patterns across various images. Matching images between infrared and visible becomes quite challenging due to the significant non-linear intensity differences. In this paper, we propose using a lightweight network for feature matching of infrared and visible images, combining global and local feature information, and reducing computational costs, enabling real-time inference on most desktop-level GPUs. To fully leverage the powerful matching capabilities of existing state-of-the-art models, we introduce knowledge distillation to obtain more robust features. Moreover, to address the issue of insufficient datasets for network training in existing methods, we propose using image style transfer techniques to synthesize paired datasets of infrared and visible. Experimental results show that our method achieves results comparable to the most advanced methods in infrared and visible image matching. Furthermore, our method has a significant advantage in inference speed, which is beneficial for tasks that require real-time completion.},
  archive      = {J_JRTIP},
  author       = {Chang, Rong and Yang, Chuanxu and Zhang, Hang and Xie, Housheng and Zhou, Chengjiang and Pan, Anning and Yang, Yang},
  doi          = {10.1007/s11554-023-01395-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Speed matters, a robust infrared and visible image matching method at real-time speed},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA implementation of compact and low-power multiplierless
architectures for DWT and IDWT. <em>JRTIP</em>, <em>21</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s11554-023-01396-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet transform is an important tool in the field of multimedia signal processing. Due to the growing demand for hardware-based design in the field of image, video, and speech signal processing, several researchers have implemented different discrete wavelet transform (DWT) architectures and measured their performances. In this paper, FPGA implementation of compact and low-power VLSI architectures for one-dimensional and two-dimensional DWT and inverse discrete wavelet transform (IDWT) based on lifting and lattice schemes have been proposed. The adder–subtractor-based design approach has been used to replace the multiplier without affecting the precision. Hardware and timing analysis of existing and proposed 1D and 2D architectures are presented here. The proposed and existing DWT and IDWT architectures are also simulated and synthesized in a Virtex-4 FPGA device. The number of slice LUTs, slice registers, clock frequency, delay, power, and power-LUTs product (PLP) of proposed DWT and IDWT architectures based on lifting and lattice schemes have been found using Xilinx Vivado synthesis tool. The proposed design is also implemented on Basys 3 Artix-7 FPGA board and synthesis results show a significant improvement in terms of area and power compared to other existing architectures.},
  archive      = {J_JRTIP},
  author       = {Jana, Jhilam and Chowdhury, Ritesh Sur and Tripathi, Sayan and Bhaumik, Jaydeb},
  doi          = {10.1007/s11554-023-01396-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of compact and low-power multiplierless architectures for DWT and IDWT},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight network for small target fall detection based on
feature fusion and dynamic convolution. <em>JRTIP</em>, <em>21</em>(1),
1–17. (<a href="https://doi.org/10.1007/s11554-023-01397-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and prompt detection of falls in the elderly holds significant importance in building a fall detection system based on artificial intelligence. However, the current research has many limitations, including poor performance in low-light conditions, missed detection for small targets, excessive parameters, and slow detection speed. This paper combines feature fusion, dynamic convolution, and the SCYLLA-IoU (SIoU) loss function to overcome these challenges. First, FasterNet is employed to ensure a balance between lightweight and accuracy. Second, the bi-directional cascaded feature pyramid network is introduced, incorporating a module to enhance feature representation and improving the perception capability for targets in dark images. Furthermore, dynamic convolution is implemented based on attention mechanisms to enhance the perception and localization accuracy for small object detection tasks. Finally, the SIOU loss function is introduced to expedite convergence speed and improve target localization accuracy. Experimental results demonstrate that the improved model outperforms the original YOLOv5s model, achieving a 6.6% increase in precision and a 15.3% enhancement in detection speed, while reducing parameter count by 24%. It exhibits superior performance compared to other networks, including Faster-R-CNN, SSD, YOLOXs, and YOLOv7.},
  archive      = {J_JRTIP},
  author       = {Zhang, Qihao and Bao, Xu and Sun, Shantong and Lin, Feng},
  doi          = {10.1007/s11554-023-01397-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight network for small target fall detection based on feature fusion and dynamic convolution},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning models for bolus segmentation in
videofluoroscopic swallow studies. <em>JRTIP</em>, <em>21</em>(1), 1–10.
(<a href="https://doi.org/10.1007/s11554-023-01398-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the benefits of the videofluoroscopic swallow study (VFSS) is the visualization of the bolus transit during the swallowing process. This X-ray imaging technique allows clinicians to observe the occurrence of penetration and aspiration of a bolus into the airway, and to characterize possible post-swallow residue. This study aims to develop and analyze deep learning models for bolus segmentation in videofluoroscopic swallow study. This study utilized various encoder–decoder-based deep learning models to automatically segment a bolus. The models were developed with 6424 VFSS images from 270 swallow studies obtained from 28 patients (15 males, mean age: 59.87 ± 14.88 years; 13 females, mean age: 57.08 ± 17.21 years) suspected of dysphagia (swallowing difficulties). The data were split at patient level with a proportion of 80%, 10%, and 10% for training, validation, and testing, respectively. Model performance was mainly evaluated by dice score coefficient (DSC) and intersection-over-union (IoU). The InceptionResNetV2 encoder in the UNet +  + architecture achieved the best performance with 81.16% of DSC and 68.29% of IoU, while the inference speed was 49.34 ms per image on a designated device. In addition, the UNet +  + with MobileNetV2 encoder achieved a considerably faster inference speed of 10.08 ms per image and slightly lower performance of 80.98% and 68.04% for DSC and IOU, respectively. Our study demonstrated effective and accurate methods of segmenting and tracking a bolus on all frames of VFSS exams in real time, indicating the potential to reduce human error and contribute objective analysis to early dysphagia diagnosis and management.},
  archive      = {J_JRTIP},
  author       = {Li, Wuqi and Mao, Shitong and Mahoney, Amanda S. and Petkovic, Sandra and Coyle, James L. and Sejdić, Ervin},
  doi          = {10.1007/s11554-023-01398-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning models for bolus segmentation in videofluoroscopic swallow studies},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved small foreign object debris detection network based
on YOLOv5. <em>JRTIP</em>, <em>21</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01399-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the challenges of detecting foreign object debris (FOD) on airport runways, where the objects are small in size and have indistinct features leading to false detections and missed detections, significant improvements were made to the YOLOv5 algorithm. First, the original YOLOv5-n model was optimized by incorporating multi-scale fusion and detection enhancements. To improve detection speed and reduce parameters, the detection head for large objects was removed. Second, the C3 module in the backbone network was replaced with the C2f module, resulting in enhanced gradient flow and improved feature representation capabilities. Additionally, the spatial pyramid pooling-fast (SPPF) module in the backbone network was refined to expand the receptive field and enhance the model’s perception of dependencies between targets and backgrounds. Furthermore, the coordinate attention (CA) mechanism was introduced in the neck layer to further enhance the model&#39;s perception of small FOD items. Lastly, the SCYLLA-IoU (SIoU) loss function was introduced to further improve the speed and accuracy of bounding box regression. Moreover, the nearest neighbor interpolation upsampling method was substituted with the lightweight Content-Aware ReAssembly of FEatures (CARAFE) upsampling operator to better exploit global information. Experimental results on the Fod_Tiny dataset, which consists of small FOD items in airports, demonstrated a significant 5.4% improvement over the baseline algorithm. To validate the generalizability of the algorithm, experiments were conducted on the Mirco_COCO dataset, resulting in a notable 1.9% improvement compared to the baseline algorithm.},
  archive      = {J_JRTIP},
  author       = {Zhang, Heng and Fu, Wei and Li, Dong and Wang, Xiaoming and Xu, Tengda},
  doi          = {10.1007/s11554-023-01399-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved small foreign object debris detection network based on YOLOv5},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on improved YOLOv8 algorithm for insulator defect
detection. <em>JRTIP</em>, <em>21</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01401-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of artificial intelligence technologies, drone aerial photography has gradually become the mainstream method for defect detection of transmission line insulators. To address the issues of slow recognition speed and low accuracy in existing detection methods, this paper proposes an insulator defect detection algorithm based on an improved YOLOv8s model. Initially, a multi-scale large-kernel attention (MLKA) module is introduced to enhance the model’s focus on features of different scales as well as low-level feature maps. In addition, by employing lightweight GSConv convolution and constructing the GSC_C2f module, the computational process is simplified and memory burden is reduced, thereby effectively improving the performance of insulator defect detection. Finally, an improved loss function using SIoU is adopted to optimize the model’s detection performance and enhance its feature extraction capability for insulator defects. Experimental results demonstrate that the improved model exhibits excellent performance in drone aerial photography for insulator defect detection, achieving an mAP of 99.22% and an FPS of 55.73 frames per second. Compared to the original YOLOv8s and YOLOv5s, the improved model’s mAP increased by 2.18% and 2.91%, respectively, and the model size is only 30.18 MB, meeting the requirements for real-time operation and accuracy.},
  archive      = {J_JRTIP},
  author       = {Zhang, Lin and Li, Boqun and Cui, Yang and Lai, Yushan and Gao, Jing},
  doi          = {10.1007/s11554-023-01401-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on improved YOLOv8 algorithm for insulator defect detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast partition algorithm in depth map intra coding unit
based on multi-deep convolution neural network. <em>JRTIP</em>,
<em>21</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01404-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimension high-efficiency video coding standard (3D-HEVC) finalized comes with a significant increase in complexity caused by the integration of depth map coding technology. This complexity is primarily triggered by the quad-tree partition of the Intra Coding Units (CU) in the depth map. A new technique utilizing deep learning is proposed, in this paper, to tackle the issue of excessive complexity aiming to predict efficiently the CU partition structure. The proposed method involves building a dataset of CU partition structure information for a depth map, creating a Multi-Deep Convolutional Neural Network (MD-CNN) model using this dataset, and then incorporating the model into the 3D-HEVC test platform. This approach reduces the 3D-HEVC video encoder complexity by 48.29% without affecting robustness, compression efficiency and video quality.},
  archive      = {J_JRTIP},
  author       = {Omran, Nacir and Maraoui, Amna and Werda, Imen and Hamdi, Belgacem},
  doi          = {10.1007/s11554-023-01404-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast partition algorithm in depth map intra coding unit based on multi-deep convolution neural network},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
