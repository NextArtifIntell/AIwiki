<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---98">COAP - 98</h2>
<ul>
<li><details>
<summary>
(2024a). Correction: Robust approximation of chance constrained
optimization with polynomial perturbation. <em>COAP</em>,
<em>89</em>(3), 1005–1006. (<a
href="https://doi.org/10.1007/s10589-024-00611-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Rao, Bo and Yang, Liu and Zhong, Suhan and Zhou, Guangming},
  doi          = {10.1007/s10589-024-00611-6},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {1005-1006},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction: Robust approximation of chance constrained optimization with polynomial perturbation},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Robust approximation of chance constrained optimization
with polynomial perturbation. <em>COAP</em>, <em>89</em>(3), 977–1003.
(<a href="https://doi.org/10.1007/s10589-024-00602-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a robust approximation method for solving chance constrained optimization (CCO) of polynomials. Assume the CCO is defined with an individual chance constraint that is affine in the decision variables. We construct a robust approximation by replacing the chance constraint with a robust constraint over an uncertainty set. When the objective function is linear or SOS-convex, the robust approximation can be equivalently transformed into linear conic optimization. Semidefinite relaxation algorithms are proposed to solve these linear conic transformations globally and their convergent properties are studied. We also introduce a heuristic method to find efficient uncertainty sets such that optimizers of the robust approximation are feasible to the original problem. Numerical experiments are given to show the efficiency of our method.},
  archive      = {J_COAP},
  author       = {Rao, Bo and Yang, Liu and Zhong, Suhan and Zhou, Guangming},
  doi          = {10.1007/s10589-024-00602-7},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {977-1003},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Robust approximation of chance constrained optimization with polynomial perturbation},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonsmooth projection-free optimization with functional
constraints. <em>COAP</em>, <em>89</em>(3), 927–975. (<a
href="https://doi.org/10.1007/s10589-024-00607-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a subgradient-based algorithm for constrained nonsmooth convex optimization that does not require projections onto the feasible set. While the well-established Frank–Wolfe algorithm and its variants already avoid projections, they are primarily designed for smooth objective functions. In contrast, our proposed algorithm can handle nonsmooth problems with general convex functional inequality constraints. It achieves an $$\epsilon $$ -suboptimal solution in $$\mathcal {O}(\epsilon ^{-2})$$ iterations, with each iteration requiring only a single (potentially inexact) Linear Minimization Oracle call and a (possibly inexact) subgradient computation. This performance is consistent with existing lower bounds. Similar performance is observed when deterministic subgradients are replaced with stochastic subgradients. In the special case where there are no functional inequality constraints, our algorithm competes favorably with a recent nonsmooth projection-free method designed for constraint-free problems. Our approach utilizes a simple separation scheme in conjunction with a new Lagrange multiplier update rule.},
  archive      = {J_COAP},
  author       = {Asgari, Kamiar and Neely, Michael J.},
  doi          = {10.1007/s10589-024-00607-2},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {927-975},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonsmooth projection-free optimization with functional constraints},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A power-like method for finding the spectral radius of a
weakly irreducible nonnegative symmetric tensor. <em>COAP</em>,
<em>89</em>(3), 895–926. (<a
href="https://doi.org/10.1007/s10589-024-00601-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Perron–Frobenius theorem says that the spectral radius of a weakly irreducible nonnegative tensor is the unique positive eigenvalue corresponding to a positive eigenvector. With this fact in mind, the purpose of this paper is to find the spectral radius and its corresponding positive eigenvector of a weakly irreducible nonnegative symmetric tensor. By transforming the eigenvalue problem into an equivalent problem of minimizing a concave function on a closed convex set, we derive a simpler and cheaper iterative method called power-like method, which is well-defined. Furthermore, we show that both sequences of the eigenvalue estimates and the eigenvector evaluations generated by the power-like method Q-linearly converge to the spectral radius and its corresponding eigenvector, respectively. To accelerate the method, we introduce a line search technique. The improved method retains the same convergence property as the original version. Plentiful numerical results show that the improved method performs quite well.},
  archive      = {J_COAP},
  author       = {Bai, Xueli and Li, Dong-Hui and Wu, Lei and Xu, Jiefeng},
  doi          = {10.1007/s10589-024-00601-8},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {895-926},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A power-like method for finding the spectral radius of a weakly irreducible nonnegative symmetric tensor},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A newton-CG based barrier-augmented lagrangian method for
general nonconvex conic optimization. <em>COAP</em>, <em>89</em>(3),
843–894. (<a href="https://doi.org/10.1007/s10589-024-00603-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider finding an approximate second-order stationary point (SOSP) of general nonconvex conic optimization that minimizes a twice differentiable function subject to nonlinear equality constraints and also a convex conic constraint. In particular, we propose a Newton-conjugate gradient (Newton-CG) based barrier-augmented Lagrangian method for finding an approximate SOSP of this problem. Under some mild assumptions, we show that our method enjoys a total inner iteration complexity of $${\widetilde{{{\,\mathrm{\mathcal {O}}\,}}}}(\epsilon ^{-11/2})$$ and an operation complexity of $${\widetilde{{{\,\mathrm{\mathcal {O}}\,}}}}(\epsilon ^{-11/2}\min \{n,\epsilon ^{-5/4}\})$$ for finding an $$(\epsilon ,\sqrt{\epsilon })$$ -SOSP of general nonconvex conic optimization with high probability. Moreover, under a constraint qualification, these complexity bounds are improved to $${\widetilde{{{\,\mathrm{\mathcal {O}}\,}}}}(\epsilon ^{-7/2})$$ and $${\widetilde{{{\,\mathrm{\mathcal {O}}\,}}}}(\epsilon ^{-7/2}\min \{n,\epsilon ^{-3/4}\})$$ , respectively. To the best of our knowledge, this is the first study on the complexity of finding an approximate SOSP of general nonconvex conic optimization. Preliminary numerical results are presented to demonstrate superiority of the proposed method over first-order methods in terms of solution quality.},
  archive      = {J_COAP},
  author       = {He, Chuan and Huang, Heng and Lu, Zhaosong},
  doi          = {10.1007/s10589-024-00603-6},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {843-894},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A newton-CG based barrier-augmented lagrangian method for general nonconvex conic optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A family of conjugate gradient methods with guaranteed
positiveness and descent for vector optimization. <em>COAP</em>,
<em>89</em>(3), 805–842. (<a
href="https://doi.org/10.1007/s10589-024-00609-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we seek a new modification way to ensure the positiveness of the conjugate parameter and, based on the Dai-Yuan (DY) method in the vector setting, propose an associated family of conjugate gradient (CG) methods with guaranteed descent for solving unconstrained vector optimization problems. Several special members of the family are analyzed and the (sufficient) descent condition is established for them (in the vector sense). Under mild conditions, a general convergence result for the CG methods with specific parameters is presented, which, in particular, covers the global convergence of the aforementioned members. Furthermore, for the purpose of comparison, we then consider the direct extension versions of some Dai-Yuan type methods which are obtained by modifying the DY method of the scalar case. These vector extensions can retrieve the classical parameters in the scalar minimization case and their descent property and global convergence are also studied under mild assumptions. Finally, numerical experiments are given to illustrate the practical behavior of all proposed methods.},
  archive      = {J_COAP},
  author       = {He, Qing-Rui and Li, Sheng-Jie and Zhang, Bo-Ya and Chen, Chun-Rong},
  doi          = {10.1007/s10589-024-00609-0},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {805-842},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A family of conjugate gradient methods with guaranteed positiveness and descent for vector optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaled-PAKKT sequential optimality condition for
multiobjective problems and its application to an augmented lagrangian
method. <em>COAP</em>, <em>89</em>(3), 769–803. (<a
href="https://doi.org/10.1007/s10589-024-00605-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the recently introduced Scaled Positive Approximate Karush–Kuhn–Tucker condition for single objective problems, we derive a sequential necessary optimality condition for multiobjective problems with equality and inequality constraints as well as additional abstract set constraints. These necessary sequential optimality conditions for multiobjective problems are subject to the same requirements as ordinary (pointwise) optimization conditions: we show that the updated Scaled Positive Approximate Karush–Kuhn–Tucker condition is necessary for a local weak Pareto point to the problem. Furthermore, we propose a variant of the classical Augmented Lagrangian method for multiobjective problems. Our theoretical framework does not require any scalarization. We also discuss the convergence properties of our algorithm with regard to feasibility and global optimality without any convexity assumption. Finally, some numerical results are given to illustrate the practical viability of the method.},
  archive      = {J_COAP},
  author       = {Carrizo, G. A. and Fazzio, N. S. and Sánchez, M. D. and Schuverdt, M. L.},
  doi          = {10.1007/s10589-024-00605-4},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {769-803},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Scaled-PAKKT sequential optimality condition for multiobjective problems and its application to an augmented lagrangian method},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultiSQP-GS: A sequential quadratic programming algorithm
via gradient sampling for nonsmooth constrained multiobjective
optimization. <em>COAP</em>, <em>89</em>(3), 729–767. (<a
href="https://doi.org/10.1007/s10589-024-00608-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a method for solving constrained nonsmooth multiobjective optimization problems which is based on a Sequential Quadratic Programming (SQP) type approach and the Gradient Sampling (GS) technique. We consider the multiobjective problems with noncovex and nonsmooth objective and constraint functions. The problem functions are assumed to be locally Lipschitz. Such problems arise in important applications, many having (weak) Pareto solutions at points of nondifferentiability of the problem functions. In our algorithm, a penalty function is applied to regularize the constraints, GS is employed to overcome the subdifferential calculation burden and make the search direction computation effective in nonsmooth regions, and SQP is used for getting a local linearization. We prove the global convergence properties of our algorithm to the stationary points which approximate (weak) Pareto front. Furthermore, we illustrate the ability and efficiency of the proposed method via a MATLAB implementation on several tests problems and compare it with some existing algorithms.},
  archive      = {J_COAP},
  author       = {Rashidi, Mehri and Soleimani-damaneh, Majid},
  doi          = {10.1007/s10589-024-00608-1},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {729-767},
  shortjournal = {Comput. Optim. Appl.},
  title        = {MultiSQP-GS: A sequential quadratic programming algorithm via gradient sampling for nonsmooth constrained multiobjective optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic zeroth order descent with structured directions.
<em>COAP</em>, <em>89</em>(3), 691–727. (<a
href="https://doi.org/10.1007/s10589-024-00616-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and analyze Structured Stochastic Zeroth order Descent (S-SZD), a finite difference approach that approximates a stochastic gradient on a set of $$l\le d$$ orthogonal directions, where d is the dimension of the ambient space. These directions are randomly chosen and may change at each step. For smooth convex functions we prove almost sure convergence of the iterates and a convergence rate on the function values of the form $$O( (d/l) k^{-c})$$ for every $$c&lt;1/2$$ , which is arbitrarily close to the one of Stochastic Gradient Descent (SGD) in terms of number of iterations (Garrigos and Gower in Handbook of convergence theorems for (stochastic) gradient methods, arXiv:2301.11235 , 2024) . Our bound shows the benefits of using l multiple directions instead of one. For non-convex functions satisfying the Polyak-Łojasiewicz condition, we establish the first convergence rates for stochastic structured zeroth order algorithms under such an assumption. We corroborate our theoretical findings with numerical simulations where the assumptions are satisfied and on the real-world problem of hyper-parameter optimization in machine learning, achieving competitive practical performance.},
  archive      = {J_COAP},
  author       = {Rando, Marco and Molinari, Cesare and Villa, Silvia and Rosasco, Lorenzo},
  doi          = {10.1007/s10589-024-00616-1},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {691-727},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic zeroth order descent with structured directions},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic moving ball approximation method for smooth
convex constrained minimization. <em>COAP</em>, <em>89</em>(3), 659–689.
(<a href="https://doi.org/10.1007/s10589-024-00612-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider constrained optimization problems with convex objective and smooth convex functional constraints. We propose a new stochastic gradient algorithm, called the Stochastic Moving Ball Approximation (SMBA) method, to solve this class of problems, where at each iteration we first take a (sub)gradient step for the objective function and then perform a projection step onto one ball approximation of a randomly chosen constraint. The computational simplicity of SMBA, which uses first-order information and considers only one constraint at a time, makes it suitable for large-scale problems with many functional constraints. We provide a convergence analysis for the SMBA algorithm using basic assumptions on the problem, that yields new convergence rates in both optimality and feasibility criteria evaluated at some average point. Our convergence proofs are novel since we need to deal properly with infeasible iterates and with quadratic upper approximations of constraints that may yield empty balls. We derive convergence rates of order $${\mathcal {O}} (k^{-1/2})$$ when the objective function is convex, and $${\mathcal {O}} (k^{-1})$$ when the objective function is strongly convex. Preliminary numerical experiments on quadratically constrained quadratic problems demonstrate the viability and performance of our method when compared to some existing state-of-the-art optimization methods and software.},
  archive      = {J_COAP},
  author       = {Singh, Nitesh Kumar and Necoara, Ion},
  doi          = {10.1007/s10589-024-00612-5},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {659-689},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stochastic moving ball approximation method for smooth convex constrained minimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexact log-domain interior-point methods for quadratic
programming. <em>COAP</em>, <em>89</em>(3), 625–658. (<a
href="https://doi.org/10.1007/s10589-024-00610-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a framework for implementing log-domain interior-point methods (LDIPMs) using inexact Newton steps. A generalized inexact iteration scheme is established that is globally convergent and locally quadratically convergent towards centered points if the residual of the inexact Newton step satisfies a set of termination criteria. Three inexact LDIPM implementations based on the conjugate gradient (CG) method are developed using this framework. In a set of computational experiments, the inexact LDIPMs demonstrate a 24–72% reduction in the total number of CG iterations required for termination relative to implementations with a fixed termination tolerance. This translates into an important computation time reduction in applications such as real-time optimization and model predictive control.},
  archive      = {J_COAP},
  author       = {Leung, Jordan and Permenter, Frank and Kolmanovsky, Ilya},
  doi          = {10.1007/s10589-024-00610-7},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {625-658},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact log-domain interior-point methods for quadratic programming},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An inexact regularized proximal newton method without line
search. <em>COAP</em>, <em>89</em>(3), 585–624. (<a
href="https://doi.org/10.1007/s10589-024-00600-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an inexact regularized proximal Newton method (IRPNM) that does not require any line search. The method is designed to minimize the sum of a twice continuously differentiable function f and a convex (possibly non-smooth and extended-valued) function $$\varphi $$ . Instead of controlling a step size by a line search procedure, we update the regularization parameter in a suitable way, based on the success of the previous iteration. The global convergence of the sequence of iterations and its superlinear convergence rate under a local Hölderian error bound assumption are shown. Notably, these convergence results are obtained without requiring a global Lipschitz property for $$ \nabla f $$ , which, to the best of the authors’ knowledge, is a novel contribution for proximal Newton methods. To highlight the efficiency of our approach, we provide numerical comparisons with an IRPNM using a line search globalization and a modern FISTA-type method.},
  archive      = {J_COAP},
  author       = {vom Dahl, Simeon and Kanzow, Christian},
  doi          = {10.1007/s10589-024-00600-9},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {585-624},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact regularized proximal newton method without line search},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COAP 2023 best paper prize. <em>COAP</em>, <em>89</em>(3),
579–583. (<a href="https://doi.org/10.1007/s10589-024-00619-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  doi          = {10.1007/s10589-024-00619-y},
  journal      = {Computational Optimization and Applications},
  month        = {12},
  number       = {3},
  pages        = {579-583},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2023 best paper prize},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Stochastic inexact augmented lagrangian
method for nonconvex expectation constrained optimization.
<em>COAP</em>, <em>89</em>(2), 575–578. (<a
href="https://doi.org/10.1007/s10589-024-00598-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Li, Zichong and Chen, Pin-Yu and Liu, Sijia and Lu, Songtao and Xu, Yangyang},
  doi          = {10.1007/s10589-024-00598-0},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {575-578},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: Stochastic inexact augmented lagrangian method for nonconvex expectation constrained optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The weighted euclidean one-center problem in <span
class="math display">ℝ<sup><em>n</em></sup></span>. <em>COAP</em>,
<em>89</em>(2), 553–574. (<a
href="https://doi.org/10.1007/s10589-024-00599-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a finite set of distinct points in $${\mathbb {R}}^n$$ and a positive weight for each point, primal and dual algorithms are developed for finding the Euclidean ball of minimum radius so that the weighted Euclidean distance between the center of the ball and each point is less than or equal to the radius of the ball. Each algorithm is based on a directional search method in which the search path at each iteration is either a ray or a two-dimensional circular arc in $${\mathbb {R}}^n$$ . At each iteration, a search path is constructed by intersecting bisectors of pairs of points, where the bisectors are either hyperplanes or n-dimensional spheres. Each search path preserves complementary slackness and primal (dual) feasibility for the primal (dual) algorithm. The step size along each search path is determined explicitly. Test problems up to 1000 dimensions and 10,000 points were solved to optimality by both primal and dual algorithms. Computational results also show that these algorithms outperform several open-source SOCP codes.},
  archive      = {J_COAP},
  author       = {Cawood, Mark E. and Dearing, P. M.},
  doi          = {10.1007/s10589-024-00599-z},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {553-574},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The weighted euclidean one-center problem in $${\mathbb {R}}^n$$},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value of risk aversion in perishable products supply chain
management. <em>COAP</em>, <em>89</em>(2), 517–552. (<a
href="https://doi.org/10.1007/s10589-024-00593-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study optimal procurement and inventory decisions for a supply chain with a single perishable product under demand uncertainty. To control risk, on the one hand, we use a risk-averse objective, and on the other hand, we utilize a chance constraint to satisfy demand with a high probability. We formulate the problem as a two-stage stochastic program with a chance constraint and risk-averse objective, where long-term decisions on pre-positioning products are made in the first stage, while recourse decisions on reallocation and emergency procurement are made in the second stage. To allow for different risk preferences, we incorporate conditional value-at-risk into the objective function and study its combination with the expectation or worst-case of the second-stage costs. To solve the resulting models, we develop various variants of the L-shaped method, based on dual and primal decomposition, and by leveraging the connection between the optimization of coherent risk measures and distributionally robust optimization. Through extensive numerical experiments, we demonstrate the value of risk aversion and present a comparative computational study on the performance of different algorithms.},
  archive      = {J_COAP},
  author       = {Pathy, Soumya Ranjan and Rahimian, Hamed},
  doi          = {10.1007/s10589-024-00593-5},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {517-552},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Value of risk aversion in perishable products supply chain management},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic stochastic projection method for multistage
stochastic variational inequalities. <em>COAP</em>, <em>89</em>(2),
485–516. (<a href="https://doi.org/10.1007/s10589-024-00594-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic approximation (SA) type methods have been well studied for solving single-stage stochastic variational inequalities (SVIs). This paper proposes a dynamic stochastic projection method (DSPM) for solving multistage SVIs. In particular, we investigate an inexact single-stage SVI and present an inexact stochastic projection method (ISPM) for solving it. Then we give the DSPM to a three-stage SVI by applying the ISPM to each stage. We show that the DSPM can achieve an $$\mathcal {O}(\frac{1}{\epsilon ^2})$$ convergence rate regarding to the total number of required scenarios for the three-stage SVI. We also extend the DSPM to the multistage SVI when the number of stages is larger than three. The numerical experiments illustrate the effectiveness and efficiency of the DSPM.},
  archive      = {J_COAP},
  author       = {Zhou, Bin and Jiang, Jie and Sun, Hailin},
  doi          = {10.1007/s10589-024-00594-4},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {485-516},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Dynamic stochastic projection method for multistage stochastic variational inequalities},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extragradient method with feasible inexact projection to
variational inequality problem. <em>COAP</em>, <em>89</em>(2), 459–484.
(<a href="https://doi.org/10.1007/s10589-024-00592-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The variational inequality problem in finite-dimensional Euclidean space is addressed in this paper, and two inexact variants of the extragradient method are proposed to solve it. Instead of computing exact projections on the constraint set, as in previous versions extragradient method, the proposed methods compute feasible inexact projections on the constraint set using a relative error criterion. The first version of the proposed method provided is a counterpart to the classic form of the extragradient method with constant steps. In order to establish its convergence we need to assume that the operator is pseudo-monotone and Lipschitz continuous, as in the standard approach. For the second version, instead of a fixed step size, the method presented finds a suitable step size in each iteration by performing a line search. Like the classical extragradient method, the proposed method does just two projections into the feasible set in each iteration. A full convergence analysis is provided, with no Lipschitz continuity assumption of the operator defining the variational inequality problem.},
  archive      = {J_COAP},
  author       = {Millán, R. Díaz and Ferreira, O. P. and Ugon, J.},
  doi          = {10.1007/s10589-024-00592-6},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {459-484},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Extragradient method with feasible inexact projection to variational inequality problem},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive regularized proximal newton-type methods for
composite optimization over the stiefel manifold. <em>COAP</em>,
<em>89</em>(2), 419–457. (<a
href="https://doi.org/10.1007/s10589-024-00595-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the proximal Newton-type method and its variants have been generalized to solve composite optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function. In this paper, we propose an adaptive quadratically regularized proximal quasi-Newton method, named ARPQN, to solve this class of problems. Under some mild assumptions, the global convergence, the local linear convergence rate and the iteration complexity of ARPQN are established. Numerical experiments and comparisons with other state-of-the-art methods indicate that ARPQN is very promising. We also propose an adaptive quadratically regularized proximal Newton method, named ARPN. It is shown the ARPN method has a local superlinear convergence rate under certain reasonable assumptions, which demonstrates attractive convergence properties of regularized proximal Newton methods.},
  archive      = {J_COAP},
  author       = {Wang, Qinsi and Yang, Wei Hong},
  doi          = {10.1007/s10589-024-00595-3},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {419-457},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An adaptive regularized proximal newton-type methods for composite optimization over the stiefel manifold},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A block-coordinate approach of multi-level optimization with
an application to physics-informed neural networks. <em>COAP</em>,
<em>89</em>(2), 385–417. (<a
href="https://doi.org/10.1007/s10589-024-00597-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-level methods are widely used for the solution of large-scale problems, because of their computational advantages and exploitation of the complementarity between the involved sub-problems. After a re-interpretation of multi-level methods from a block-coordinate point of view, we propose a multi-level algorithm for the solution of nonlinear optimization problems and analyze its evaluation complexity. We apply it to the solution of partial differential equations using physics-informed neural networks (PINNs) and consider two different types of neural architectures, a generic feedforward network and a frequency-aware network. We show that our approach is particularly effective if coupled with these specialized architectures and that this coupling results in better solutions and significant computational savings.},
  archive      = {J_COAP},
  author       = {Gratton, Serge and Mercier, Valentin and Riccietti, Elisa and Toint, Philippe L.},
  doi          = {10.1007/s10589-024-00597-1},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {385-417},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A block-coordinate approach of multi-level optimization with an application to physics-informed neural networks},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenvalue programming beyond matrices. <em>COAP</em>,
<em>89</em>(2), 361–384. (<a
href="https://doi.org/10.1007/s10589-024-00591-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we analyze and solve eigenvalue programs, which consist of the task of minimizing a function subject to constraints on the “eigenvalues” of the decision variable. Here, by making use of the FTvN systems framework introduced by Gowda, we interpret “eigenvalues” in a broad fashion going beyond the usual eigenvalues of matrices. This allows us to shed new light on classical problems such as inverse eigenvalue problems and also leads to new applications. In particular, after analyzing and developing a simple projected gradient algorithm for general eigenvalue programs, we show that eigenvalue programs can be used to express what we call vanishing quadratic constraints. A vanishing quadratic constraint requires that a given system of convex quadratic inequalities be satisfied and at least a certain number of those inequalities must be tight. As a particular case, this includes the problem of finding a point x in the intersection of m ellipsoids in such a way that x is also in the boundary of at least $$\ell $$ of the ellipsoids, for some fixed $$\ell &gt; 0$$ . At the end, we also present some numerical experiments.},
  archive      = {J_COAP},
  author       = {Ito, Masaru and Lourenço, Bruno F.},
  doi          = {10.1007/s10589-024-00591-7},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {361-384},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Eigenvalue programming beyond matrices},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Q-fully quadratic modeling and its application in a random
subspace derivative-free method. <em>COAP</em>, <em>89</em>(2), 317–360.
(<a href="https://doi.org/10.1007/s10589-024-00590-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based derivative-free optimization (DFO) methods are an important class of DFO methods that are known to struggle with solving high-dimensional optimization problems. Recent research has shown that incorporating random subspaces into model-based DFO methods has the potential to improve their performance on high-dimensional problems. However, most of the current theoretical and practical results are based on linear approximation models due to the complexity of quadratic approximation models. This paper proposes a random subspace trust-region algorithm based on quadratic approximations. Unlike most of its precursors, this algorithm does not require any special form of objective function. We study the geometry of sample sets, the error bounds for approximations, and the quality of subspaces. In particular, we provide a technique to construct Q-fully quadratic models, which is easy to analyze and implement. We present an almost-sure global convergence result of our algorithm and give an upper bound on the expected number of iterations to find a sufficiently small gradient. We also develop numerical experiments to compare the performance of our algorithm using both linear and quadratic approximation models. The numerical results demonstrate the strengths and weaknesses of using quadratic approximations.},
  archive      = {J_COAP},
  author       = {Chen, Yiwen and Hare, Warren and Wiebe, Amy},
  doi          = {10.1007/s10589-024-00590-8},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {317-360},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Q-fully quadratic modeling and its application in a random subspace derivative-free method},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full-low evaluation methods for bound and linearly
constrained derivative-free optimization. <em>COAP</em>, <em>89</em>(2),
279–315. (<a href="https://doi.org/10.1007/s10589-024-00596-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Derivative-free optimization (DFO) consists in finding the best value of an objective function without relying on derivatives. To tackle such problems, one may build approximate derivatives, using for instance finite-difference estimates. One may also design algorithmic strategies that perform space exploration and seek improvement over the current point. The first type of strategy often provides good performance on smooth problems but at the expense of more function evaluations. The second type is cheaper and typically handles non-smoothness or noise in the objective better. Recently, full-low evaluation methods have been proposed as a hybrid class of DFO algorithms that combine both strategies, respectively denoted as Full-Eval and Low-Eval. In the unconstrained case, these methods showed promising numerical performance. In this paper, we extend the full-low evaluation framework to bound and linearly constrained derivative-free optimization. We derive convergence results for an instance of this framework, that combines finite-difference quasi-Newton steps with probabilistic direct-search steps. The former are projected onto the feasible set, while the latter are defined within tangent cones identified by nearby active constraints. We illustrate the practical performance of our instance on standard linearly constrained problems, that we adapt to introduce noisy evaluations as well as non-smoothness. In all cases, our method performs favorably compared to algorithms that rely solely on Full-eval or Low-eval iterations.},
  archive      = {J_COAP},
  author       = {Royer, C. W. and Sohab, O. and Vicente, L. N.},
  doi          = {10.1007/s10589-024-00596-2},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {279-315},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Full-low evaluation methods for bound and linearly constrained derivative-free optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-monotone trust-region method with noisy oracles and
additional sampling. <em>COAP</em>, <em>89</em>(1), 247–278. (<a
href="https://doi.org/10.1007/s10589-024-00580-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a novel stochastic second-order method, within the framework of a non-monotone trust-region approach, for solving the unconstrained, nonlinear, and non-convex optimization problems arising in the training of deep neural networks. The proposed algorithm makes use of subsampling strategies that yield noisy approximations of the finite sum objective function and its gradient. We introduce an adaptive sample size strategy based on inexpensive additional sampling to control the resulting approximation error. Depending on the estimated progress of the algorithm, this can yield sample size scenarios ranging from mini-batch to full sample functions. We provide convergence analysis for all possible scenarios and show that the proposed method achieves almost sure convergence under standard assumptions for the trust-region framework. We report numerical experiments showing that the proposed algorithm outperforms its state-of-the-art counterpart in deep neural network training for image classification and regression tasks while requiring a significantly smaller number of gradient evaluations.},
  archive      = {J_COAP},
  author       = {Krejić, Nataša and Krklec Jerinkić, Nataša and Martínez, Ángeles and Yousefi, Mahsa},
  doi          = {10.1007/s10589-024-00580-w},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {247-278},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A non-monotone trust-region method with noisy oracles and additional sampling},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projected fixed-point method for vertical tensor
complementarity problems. <em>COAP</em>, <em>89</em>(1), 219–245. (<a
href="https://doi.org/10.1007/s10589-024-00581-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that the standard complementarity problem can be equivalently reformulated as a projected fixed-point equation, and this reformulation plays an important role in the theoretical and algorithmic research of complementarity problems. The vertical linear complementarity problem proposed by Cottle and Dantzig is an important generalization of the standard linear complementarity problem, and recently it has been further generalized to the case of tensors, called the vertical tensor complementarity problem (VTCP). In this paper, we give a projected fixed-point reformulation of the VTCP, and then, we design a fixed-point iteration method for solving the VTCP with a vertical block implicit Z-tensor. When there are non-positive diagonal entries in the representation subtensors of the tensor involved in the VTCP, we can reduce the computational cost of our method by solving a lower dimensional VTCP. Under the assumption that the problem under consideration is feasible, we prove that the designed method converges monotonically to a solution of the problem. The numerical results show the effectiveness of the proposed method.},
  archive      = {J_COAP},
  author       = {Zhang, Ting and Wang, Yong and Huang, Zheng-Hai},
  doi          = {10.1007/s10589-024-00581-9},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {219-245},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Projected fixed-point method for vertical tensor complementarity problems},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). T-semidefinite programming relaxation with third-order
tensors for constrained polynomial optimization. <em>COAP</em>,
<em>89</em>(1), 183–218. (<a
href="https://doi.org/10.1007/s10589-024-00582-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study T-semidefinite programming (SDP) relaxation for constrained polynomial optimization problems (POPs). T-SDP relaxation for unconstrained POPs was introduced by Zheng et al. (JGO 84:415–440, 2022). In this work, we propose a T-SDP relaxation for POPs with polynomial inequality constraints and show that the resulting T-SDP relaxation formulated with third-order tensors can be transformed into the standard SDP relaxation with block-diagonal structures. The convergence of the T-SDP relaxation to the optimal value of a given constrained POP is established under moderate assumptions as the relaxation level increases. Additionally, the feasibility and optimality of the T-SDP relaxation are discussed. Numerical results illustrate that the proposed T-SDP relaxation enhances numerical efficiency.},
  archive      = {J_COAP},
  author       = {Marumo, Hiroki and Kim, Sunyoung and Yamashita, Makoto},
  doi          = {10.1007/s10589-024-00582-8},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {183-218},
  shortjournal = {Comput. Optim. Appl.},
  title        = {T-semidefinite programming relaxation with third-order tensors for constrained polynomial optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delayed weighted gradient method with simultaneous
step-sizes for strongly convex optimization. <em>COAP</em>,
<em>89</em>(1), 151–182. (<a
href="https://doi.org/10.1007/s10589-024-00586-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Delayed Weighted Gradient Method (DWGM) is a two-step gradient algorithm that is efficient for the minimization of large scale strictly convex quadratic functions. It has orthogonality properties that make it to compete with the Conjugate Gradient (CG) method. Both methods calculate in sequence two step-sizes, CG minimizes the objective function and DWGM the gradient norm, alongside two search directions defined over first order current and previous iteration information. The objective of this work is to accelerate the recently developed extension of DWGM to nonquadratic strongly convex minimization problems. Our idea is to define the step-sizes of DWGM in a unique two dimensional convex quadratic optimization problem, calculating them simultaneously. Convergence of the resulting algorithm is analyzed. Comparative numerical experiments illustrate the effectiveness of our approach.},
  archive      = {J_COAP},
  author       = {Lara, Hugo and Aleixo, Rafael and Oviedo, Harry},
  doi          = {10.1007/s10589-024-00586-4},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {151-182},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Delayed weighted gradient method with simultaneous step-sizes for strongly convex optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonsmooth primal-dual method with interwoven PDE
constraint solver. <em>COAP</em>, <em>89</em>(1), 115–149. (<a
href="https://doi.org/10.1007/s10589-024-00587-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an efficient first-order primal-dual method for the solution of nonsmooth PDE-constrained optimization problems. We achieve this efficiency through not solving the PDE or its linearisation on each iteration of the optimization method. Instead, we run the method interwoven with a simple conventional linear system solver (Jacobi, Gauss–Seidel, conjugate gradients), always taking only one step of the linear system solver for each step of the optimization method. The control parameter is updated on each iteration as determined by the optimization method. We prove linear convergence under a second-order growth condition, and numerically demonstrate the performance on a variety of PDEs related to inverse problems involving boundary measurements.},
  archive      = {J_COAP},
  author       = {Jensen, Bjørn and Valkonen, Tuomo},
  doi          = {10.1007/s10589-024-00587-3},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {115-149},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A nonsmooth primal-dual method with interwoven PDE constraint solver},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling of constraints in multiobjective blackbox
optimization. <em>COAP</em>, <em>89</em>(1), 69–113. (<a
href="https://doi.org/10.1007/s10589-024-00588-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes the integration of two new constraint-handling approaches into the blackbox constrained multiobjective optimization algorithm DMulti-MADS, an extension of the Mesh Adaptive Direct Search (MADS) algorithm for single-objective constrained optimization. The constraints are aggregated into a single constraint violation function which is used either in a two-phase approach, where the search for a feasible point is prioritized if not available before improving the current solution set, or in a progressive barrier approach, where any trial point whose constraint violation function values are above a threshold are rejected. This threshold is progressively decreased along the iterations. As in the single-objective case, it is proved that these two variants generate feasible and/or infeasible sequences which converge either in the feasible case to a set of local Pareto optimal points or in the infeasible case to Clarke stationary points according to the constraint violation function. Computational experiments show that these two approaches are competitive with other state-of-the-art algorithms.},
  archive      = {J_COAP},
  author       = {Bigeon, Jean and Le Digabel, Sébastien and Salomon, Ludovic},
  doi          = {10.1007/s10589-024-00588-2},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {69-113},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Handling of constraints in multiobjective blackbox optimization},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The levenberg–marquardt method: An overview of modern
convergence theories and more. <em>COAP</em>, <em>89</em>(1), 33–67. (<a
href="https://doi.org/10.1007/s10589-024-00589-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Levenberg–Marquardt method is a fundamental regularization technique for the Newton method applied to nonlinear equations, possibly constrained, and possibly with singular or even nonisolated solutions. We review the literature on the subject, in particular relating to each other various convergence frameworks and results. In this process, the analysis is performed from a unified perspective, and some new results are obtained as well. We discuss smooth and piecewise smooth equations, inexact solution of subproblems, and globalization techniques. Attention is also paid to the LP-Newton method, because of its relations to the Levenberg–Marquardt method.},
  archive      = {J_COAP},
  author       = {Fischer, Andreas and Izmailov, Alexey F. and Solodov, Mikhail V.},
  doi          = {10.1007/s10589-024-00589-1},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {33-67},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The Levenberg–Marquardt method: An overview of modern convergence theories and more},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic steffensen method. <em>COAP</em>, <em>89</em>(1),
1–32. (<a href="https://doi.org/10.1007/s10589-024-00583-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is it possible for a first-order method, i.e., only first derivatives allowed, to be quadratically convergent? For univariate loss functions, the answer is yes—the Steffensen method avoids second derivatives and is still quadratically convergent like Newton method. By incorporating a specific step size we can even push its convergence order beyond quadratic to $$1+\sqrt{2} \approx 2.414$$ . While such high convergence orders are a pointless overkill for a deterministic algorithm, they become rewarding when the algorithm is randomized for problems of massive sizes, as randomization invariably compromises convergence speed. We will introduce two adaptive learning rates inspired by the Steffensen method, intended for use in a stochastic optimization setting and requires no hyperparameter tuning aside from batch size. Extensive experiments show that they compare favorably with several existing first-order methods. When restricted to a quadratic objective, our stochastic Steffensen methods reduce to randomized Kaczmarz method—note that this is not true for SGD or SLBFGS—and thus we may also view our methods as a generalization of randomized Kaczmarz to arbitrary objectives.},
  archive      = {J_COAP},
  author       = {Zhao, Minda and Lai, Zehua and Lim, Lek-Heng},
  doi          = {10.1007/s10589-024-00583-7},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic steffensen method},
  volume       = {89},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction: A bregman–kaczmarz method for nonlinear systems
of equations. <em>COAP</em>, <em>88</em>(3), 999–1000. (<a
href="https://doi.org/10.1007/s10589-024-00570-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Gower, Robert and Lorenz, Dirk A. and Winkler, Maximilian},
  doi          = {10.1007/s10589-024-00570-y},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {999-1000},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction: A Bregman–Kaczmarz method for nonlinear systems of equations},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shape optimization for interface identification in nonlocal
models. <em>COAP</em>, <em>88</em>(3), 963–997. (<a
href="https://doi.org/10.1007/s10589-024-00575-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape optimization methods have been proven useful for identifying interfaces in models governed by partial differential equations. Here we consider a class of shape optimization problems constrained by nonlocal equations which involve interface–dependent kernels. We derive a novel shape derivative associated to the nonlocal system model and solve the problem by established numerical techniques. The code for obtaining the results in this paper is published at ( https://github.com/schustermatthias/nlshape ).},
  archive      = {J_COAP},
  author       = {Schuster, Matthias and Vollmann, Christian and Schulz, Volker},
  doi          = {10.1007/s10589-024-00575-7},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {963-997},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Shape optimization for interface identification in nonlocal models},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An inexactly accelerated algorithm for nonnegative tensor CP
decomposition with the column unit constraints. <em>COAP</em>,
<em>88</em>(3), 923–962. (<a
href="https://doi.org/10.1007/s10589-024-00574-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The component separation problem in complex chemical systems is very important and challenging in chemometrics. In this paper, we study a third-order nonnegative CANDECOMP/PARAFAC decomposition model with the column unit constraints (NCPD_CU) motivated by the component separation problem. To solve the NCPD_CU model, we first explore rapid computational methods for a generalized class of three-block optimization problems, which may exhibit nonconvexity and nonsmoothness. To this end, we propose the accelerated inexact block coordinate descent (AIBCD) algorithm, where each subproblem is inexactly solved through a finite number of inner-iterations employing the alternating proximal gradient method. Additionally, the algorithm incorporates extrapolation during the outer-iterations to enhance overall efficiency. We prove that the iterative sequence generated by the algorithm converges to a stationary point under mild conditions. Subsequently, we apply this methodology to the NCPD_CU model that satisfies the specified conditions. Finally, we present numerical results using both synthetic and real-world data, showcasing the remarkable efficiency of our proposed method.},
  archive      = {J_COAP},
  author       = {Wang, Zihao and Bai, Minru},
  doi          = {10.1007/s10589-024-00574-8},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {923-962},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexactly accelerated algorithm for nonnegative tensor CP decomposition with the column unit constraints},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized alternating direction method of multipliers
for tensor complementarity problems. <em>COAP</em>, <em>88</em>(3),
903–921. (<a href="https://doi.org/10.1007/s10589-024-00579-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the tensor complementarity problem (TCP). From the perspective of non-coupled equality constraint minimization problem for the symmetric TCP, we propose a generalized alternating direction method of multipliers (G-ADMM) to solve the general TCP in which the tensor may not be symmetric. The global convergence and the $$O(\frac{1}{k})$$ convergence rate of the proposed method are proved under the assumption which is much weaker than the monotone assumption. Numerical results show that the method is efficient for solving the TCP.},
  archive      = {J_COAP},
  author       = {Liu, Kun and Zhou, Anwa and Fan, Jinyan},
  doi          = {10.1007/s10589-024-00579-3},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {903-921},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A generalized alternating direction method of multipliers for tensor complementarity problems},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonsmooth nonconvex optimization on riemannian manifolds via
bundle trust region algorithm. <em>COAP</em>, <em>88</em>(3), 871–902.
(<a href="https://doi.org/10.1007/s10589-024-00569-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops an iterative algorithm to solve nonsmooth nonconvex optimization problems on complete Riemannian manifolds. The algorithm is based on the combination of the well known trust region and bundle methods. According to the process of the most bundle methods, the objective function is approximated by a piecewise linear working model which is updated by adding cutting planes at unsuccessful trial steps. Then at each iteration, by solving a subproblem that employs the working model in the objective function subject to the trust region, a candidate descent direction is obtained. We study the algorithm from both theoretical and practical points of view and its global convergence is verified to stationary points for locally Lipschitz functions. Moreover, in order to demonstrate the reliability and efficiency, a MATLAB implementation of the proposed algorithm is prepared and results of numerical experiments are reported.},
  archive      = {J_COAP},
  author       = {Hoseini Monjezi, N. and Nobakhtian, S. and Pouryayevali, M. R.},
  doi          = {10.1007/s10589-024-00569-5},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {871-902},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonsmooth nonconvex optimization on riemannian manifolds via bundle trust region algorithm},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid inexact regularized newton and negative curvature
method. <em>COAP</em>, <em>88</em>(3), 849–870. (<a
href="https://doi.org/10.1007/s10589-024-00576-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a hybrid inexact regularized Newton and negative curvature method for solving unconstrained nonconvex problems. The descent direction is chosen based on different conditions, either the negative curvature or the inexact regularized direction. In addition, to minimize computational costs while obtaining the negative curvature, we employ a dimensionality reduction strategy to verify if the Hessian matrix exhibits negative curvatures within a three-dimensional subspace. We show that the proposed method can achieve the best-known global iteration complexity if the Hessian of the objective function is Lipschitz continuous on a certain compact set. Two simplified methods for nonconvex and strongly convex problems are analyzed as specific instances of the proposed method. We show that under the local error bound assumption with respect to the gradient, the distance between iterations generated by our proposed method and the local solution set converges to $$0$$ at a superlinear rate. Additionally, for strongly convex problems, the quadratic convergence rate can be achieved. Extensive numerical experiments show the effectiveness of the proposed method.},
  archive      = {J_COAP},
  author       = {Zhu, Hong and Xiao, Yunhai},
  doi          = {10.1007/s10589-024-00576-6},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {849-870},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A hybrid inexact regularized newton and negative curvature method},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chance-constrained programs with convex underlying
functions: A bilevel convex optimization perspective. <em>COAP</em>,
<em>88</em>(3), 819–847. (<a
href="https://doi.org/10.1007/s10589-024-00573-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chance constraints are a valuable tool for the design of safe decisions in uncertain environments; they are used to model satisfaction of a constraint with a target probability. However, because of possible non-convexity and non-smoothness, optimizing over a chance constrained set is challenging. In this paper, we consider chance constrained programs where the objective function and the constraints are convex with respect to the decision parameter. We establish an exact reformulation of such a problem as a bilevel problem with a convex lower-level. Then we leverage this bilevel formulation to propose a tractable penalty approach, in the setting of finitely supported random variables. The penalized objective is a difference-of-convex function that we minimize with a suitable bundle algorithm. We release an easy-to-use open-source python toolbox implementing the approach, with a special emphasis on fast computational subroutines.},
  archive      = {J_COAP},
  author       = {Laguel, Yassine and Malick, Jérôme and van Ackooij, Wim},
  doi          = {10.1007/s10589-024-00573-9},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {819-847},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Chance-constrained programs with convex underlying functions: A bilevel convex optimization perspective},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A boosted DC algorithm for non-differentiable DC components
with non-monotone line search. <em>COAP</em>, <em>88</em>(3), 783–818.
(<a href="https://doi.org/10.1007/s10589-024-00578-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new approach to apply the boosted difference of convex functions algorithm (BDCA) for solving non-convex and non-differentiable problems involving difference of two convex functions (DC functions). Supposing the first DC component differentiable and the second one possibly non-differentiable, the main idea of BDCA is to use the point computed by the subproblem of the DC algorithm (DCA) to define a descent direction of the objective from that point, and then a monotone line search starting from it is performed in order to find a new point which decreases the objective function when compared with the point generated by the subproblem of DCA. This procedure improves the performance of the DCA. However, if the first DC component is non-differentiable, then the direction computed by BDCA can be an ascent direction and a monotone line search cannot be performed. Our approach uses a non-monotone line search in the BDCA (nmBDCA) to enable a possible growth in the objective function values controlled by a parameter. Under suitable assumptions, we show that any cluster point of the sequence generated by the nmBDCA is a critical point of the problem under consideration and provides some iteration-complexity bounds. Furthermore, if the first DC component is differentiable, we present different iteration-complexity bounds and prove the full convergence of the sequence under the Kurdyka–Łojasiewicz property of the objective function. Some numerical experiments show that the nmBDCA outperforms the DCA, such as its monotone version.},
  archive      = {J_COAP},
  author       = {Ferreira, O. P. and Santos, E. M. and Souza, J. C. O.},
  doi          = {10.1007/s10589-024-00578-4},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {783-818},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A boosted DC algorithm for non-differentiable DC components with non-monotone line search},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An away-step frank–wolfe algorithm for constrained
multiobjective optimization. <em>COAP</em>, <em>88</em>(3), 759–781. (<a
href="https://doi.org/10.1007/s10589-024-00577-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose and analyze an away-step Frank–Wolfe algorithm designed for solving multiobjective optimization problems over polytopes. We prove that each limit point of the sequence generated by the algorithm is a weak Pareto optimal solution. Furthermore, under additional conditions, we show linear convergence of the whole sequence to a Pareto optimal solution. Numerical examples illustrate a promising performance of the proposed algorithm in problems where the multiobjective Frank–Wolfe convergence rate is only sublinear.},
  archive      = {J_COAP},
  author       = {Gonçalves, Douglas S. and Gonçalves, Max L. N. and Melo, Jefferson G.},
  doi          = {10.1007/s10589-024-00577-5},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {759-781},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An away-step Frank–Wolfe algorithm for constrained multiobjective optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global convergence of a BFGS-type algorithm for nonconvex
multiobjective optimization problems. <em>COAP</em>, <em>88</em>(3),
719–757. (<a href="https://doi.org/10.1007/s10589-024-00571-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a modified BFGS algorithm for multiobjective optimization problems with global convergence, even in the absence of convexity assumptions on the objective functions. Furthermore, we establish a local superlinear rate of convergence of the method under usual conditions. Our approach employs Wolfe step sizes and ensures that the Hessian approximations are updated and corrected at each iteration to address the lack of convexity assumption. Numerical results shows that the introduced modifications preserve the practical efficiency of the BFGS method.},
  archive      = {J_COAP},
  author       = {Prudente, L. F. and Souza, D. R.},
  doi          = {10.1007/s10589-024-00571-x},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {719-757},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Global convergence of a BFGS-type algorithm for nonconvex multiobjective optimization problems},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local convergence of primal–dual interior point methods for
nonlinear semidefinite optimization using the monteiro–tsuchiya family
of search directions. <em>COAP</em>, <em>88</em>(2), 677–718. (<a
href="https://doi.org/10.1007/s10589-024-00562-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advance of algorithms for nonlinear semidefinite optimization problems (NSDPs) is remarkable. Yamashita et al. first proposed a primal–dual interior point method (PDIPM) for solving NSDPs using the family of Monteiro–Zhang (MZ) search directions. Since then, various kinds of PDIPMs have been proposed for NSDPs, but, as far as we know, all of them are based on the MZ family. In this paper, we present a PDIPM equipped with the family of Monteiro–Tsuchiya (MT) directions, which were originally devised for solving linear semidefinite optimization problems as were the MZ family. We further prove local superlinear convergence to a Karush–Kuhn–Tucker point of the NSDP in the presence of certain general assumptions on scaling matrices, which are used in producing the MT search directions. Finally, we conduct numerical experiments to compare the efficiency among members of the MT family.},
  archive      = {J_COAP},
  author       = {Okuno, Takayuki},
  doi          = {10.1007/s10589-024-00562-y},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {677-718},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Local convergence of primal–dual interior point methods for nonlinear semidefinite optimization using the Monteiro–Tsuchiya family of search directions},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An infeasible interior-point arc-search method with
nesterov’s restarting strategy for linear programming problems.
<em>COAP</em>, <em>88</em>(2), 643–676. (<a
href="https://doi.org/10.1007/s10589-024-00561-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An arc-search interior-point method is a type of interior-point method that approximates the central path by an ellipsoidal arc, and it can often reduce the number of iterations. In this work, to further reduce the number of iterations and the computation time for solving linear programming problems, we propose two arc-search interior-point methods using Nesterov’s restarting strategy which is a well-known method to accelerate the gradient method with a momentum term. The first one generates a sequence of iterations in the neighborhood, and we prove that the proposed method converges to an optimal solution and that it is a polynomial-time method. The second one incorporates the concept of the Mehrotra-type interior-point method to improve numerical performance. The numerical experiments demonstrate that the second one reduced the number of iterations and the computational time compared to existing interior-point methods due to the momentum term.},
  archive      = {J_COAP},
  author       = {Iida, Einosuke and Yamashita, Makoto},
  doi          = {10.1007/s10589-024-00561-z},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {643-676},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An infeasible interior-point arc-search method with nesterov’s restarting strategy for linear programming problems},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An inexact regularized proximal newton method for nonconvex
and nonsmooth optimization. <em>COAP</em>, <em>88</em>(2), 603–641. (<a
href="https://doi.org/10.1007/s10589-024-00560-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the minimization of a sum of a twice continuously differentiable function f and a nonsmooth convex function. An inexact regularized proximal Newton method is proposed by an approximation to the Hessian of f involving the $$\varrho $$ th power of the KKT residual. For $$\varrho =0$$ , we justify the global convergence of the iterate sequence for the KL objective function and its R-linear convergence rate for the KL objective function of exponent 1/2. For $$\varrho \in (0,1)$$ , by assuming that cluster points satisfy a locally Hölderian error bound of order q on a second-order stationary point set and a local error bound of order $$q&gt;1\!+\!\varrho $$ on the common stationary point set, respectively, we establish the global convergence of the iterate sequence and its superlinear convergence rate with order depending on q and $$\varrho $$ . A dual semismooth Newton augmented Lagrangian method is also developed for seeking an inexact minimizer of subproblems. Numerical comparisons with two state-of-the-art methods on $$\ell _1$$ -regularized Student’s t-regressions, group penalized Student’s t-regressions, and nonconvex image restoration confirm the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Liu, Ruyu and Pan, Shaohua and Wu, Yuqia and Yang, Xiaoqi},
  doi          = {10.1007/s10589-024-00560-0},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {603-641},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact regularized proximal newton method for nonconvex and nonsmooth optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence of successive linear programming algorithms for
noisy functions. <em>COAP</em>, <em>88</em>(2), 567–601. (<a
href="https://doi.org/10.1007/s10589-024-00564-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based methods have been highly successful for solving a variety of both unconstrained and constrained nonlinear optimization problems. In real-world applications, such as optimal control or machine learning, the necessary function and derivative information may be corrupted by noise, however. Sun and Nocedal have recently proposed a remedy for smooth unconstrained problems by means of a stabilization of the acceptance criterion for computed iterates, which leads to convergence of the iterates of a trust-region method to a region of criticality (Sun and Nocedal in Math Program 66:1–28, 2023. https://doi.org/10.1007/s10107-023-01941-9 ). We extend their analysis to the successive linear programming algorithm (Byrd et al. in Math Program 100(1):27–48, 2003. https://doi.org/10.1007/s10107-003-0485-4 , SIAM J Optim 16(2):471–489, 2005. https://doi.org/10.1137/S1052623403426532 ) for unconstrained optimization problems with objectives that can be characterized as the composition of a polyhedral function with a smooth function, where the latter and its gradient may be corrupted by noise. This gives the flexibility to cover, for example, (sub)problems arising in image reconstruction or constrained optimization algorithms. We provide computational examples that illustrate the findings and point to possible strategies for practical determination of the stabilization parameter that balances the size of the critical region with a relaxation of the acceptance criterion (or descent property) of the algorithm.},
  archive      = {J_COAP},
  author       = {Hansknecht, Christoph and Kirches, Christian and Manns, Paul},
  doi          = {10.1007/s10589-024-00564-w},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {567-601},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of successive linear programming algorithms for noisy functions},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new proximal heavy ball inexact line-search algorithm.
<em>COAP</em>, <em>88</em>(2), 525–565. (<a
href="https://doi.org/10.1007/s10589-024-00565-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a novel inertial proximal-gradient method for composite optimization. The proposed method alternates between a variable metric proximal-gradient iteration with momentum and an Armijo-like linesearch based on the sufficient decrease of a suitable merit function. The linesearch procedure allows for a major flexibility on the choice of the algorithm parameters. We prove the convergence of the iterates sequence towards a stationary point of the problem, in a Kurdyka–Łojasiewicz framework. Numerical experiments on a variety of convex and nonconvex problems highlight the superiority of our proposal with respect to several standard methods, especially when the inertial parameter is selected by mimicking the Conjugate Gradient updating rule.},
  archive      = {J_COAP},
  author       = {Bonettini, S. and Prato, M. and Rebegoldi, S.},
  doi          = {10.1007/s10589-024-00565-9},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {525-565},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A new proximal heavy ball inexact line-search algorithm},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical gradient and conjugate gradient methods on flag
manifolds. <em>COAP</em>, <em>88</em>(2), 491–524. (<a
href="https://doi.org/10.1007/s10589-024-00568-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flag manifolds, sets of nested sequences of linear subspaces with fixed dimensions, are rising in numerical analysis and statistics. The current optimization algorithms on flag manifolds are based on the exponential map and parallel transport, which are expensive to compute. In this paper we propose practical optimization methods on flag manifolds without the exponential map and parallel transport. Observing that flag manifolds have a similar homogeneous structure with Grassmann and Stiefel manifolds, we generalize some typical retractions and vector transports to flag manifolds, including the Cayley-type retraction and vector transport, the QR-based and polar-based retractions, the projection-type vector transport and the projection of the differentiated polar-based retraction as a vector transport. Theoretical properties and efficient implementations of the proposed retractions and vector transports are discussed. Then we establish Riemannian gradient and Riemannian conjugate gradient algorithms based on these retractions and vector transports. Numerical results on the problem of nonlinear eigenflags demonstrate that our algorithms have a great advantage in efficiency over the existing ones.},
  archive      = {J_COAP},
  author       = {Zhu, Xiaojing and Shen, Chungen},
  doi          = {10.1007/s10589-024-00568-6},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {491-524},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Practical gradient and conjugate gradient methods on flag manifolds},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexact direct-search methods for bilevel optimization
problems. <em>COAP</em>, <em>88</em>(2), 469–490. (<a
href="https://doi.org/10.1007/s10589-024-00567-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce new direct-search schemes for the solution of bilevel optimization (BO) problems. Our methods rely on a fixed accuracy blackbox oracle for the lower-level problem, and deal both with smooth and potentially nonsmooth true objectives. We thus analyze for the first time in the literature direct-search schemes in these settings, giving convergence guarantees to approximate stationary points, as well as complexity bounds in the smooth case. We also propose the first adaptation of mesh adaptive direct-search schemes for BO. Some preliminary numerical results on a standard set of bilevel optimization problems show the effectiveness of our new approaches.},
  archive      = {J_COAP},
  author       = {Diouane, Youssef and Kungurtsev, Vyacheslav and Rinaldi, Francesco and Zeffiro, Damiano},
  doi          = {10.1007/s10589-024-00567-7},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {469-490},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact direct-search methods for bilevel optimization problems},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Riemannian preconditioned algorithms for tensor completion
via tensor ring decomposition. <em>COAP</em>, <em>88</em>(2), 443–468.
(<a href="https://doi.org/10.1007/s10589-024-00559-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Riemannian preconditioned algorithms for the tensor completion problem via tensor ring decomposition. A new Riemannian metric is developed on the product space of the mode-2 unfolding matrices of the core tensors in tensor ring decomposition. The construction of this metric aims to approximate the Hessian of the cost function by its diagonal blocks, paving the way for various Riemannian optimization methods. Specifically, we propose the Riemannian gradient descent and Riemannian conjugate gradient algorithms. We prove that both algorithms globally converge to a stationary point. In the implementation, we exploit the tensor structure and adopt an economical procedure to avoid large matrix formulation and computation in gradients, which significantly reduces the computational cost. Numerical experiments on various synthetic and real-world datasets—movie ratings, hyperspectral images, and high-dimensional functions—suggest that the proposed algorithms have better or favorably comparable performance to other candidates.},
  archive      = {J_COAP},
  author       = {Gao, Bin and Peng, Renfeng and Yuan, Ya-xiang},
  doi          = {10.1007/s10589-024-00559-7},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {443-468},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian preconditioned algorithms for tensor completion via tensor ring decomposition},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic average model methods. <em>COAP</em>,
<em>88</em>(2), 405–442. (<a
href="https://doi.org/10.1007/s10589-024-00563-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the solution of finite-sum minimization problems, such as those appearing in nonlinear least-squares or general empirical risk minimization problems. We are motivated by problems in which the summand functions are computationally expensive and evaluating all summands on every iteration of an optimization method may be undesirable. We present the idea of stochastic average model (SAM) methods, inspired by stochastic average gradient methods. SAM methods sample component functions on each iteration of a trust-region method according to a discrete probability distribution on component functions; the distribution is designed to minimize an upper bound on the variance of the resulting stochastic model. We present promising numerical results concerning an implemented variant extending the derivative-free model-based trust-region solver POUNDERS, which we name SAM-POUNDERS.},
  archive      = {J_COAP},
  author       = {Menickelly, Matt and Wild, Stefan M.},
  doi          = {10.1007/s10589-024-00563-x},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {405-442},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic average model methods},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient gradient-based optimization for reconstructing
binary images in applications to electrical impedance tomography.
<em>COAP</em>, <em>88</em>(1), 379–403. (<a
href="https://doi.org/10.1007/s10589-024-00553-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel and highly efficient computational framework for reconstructing binary-type images suitable for models of various complexity seen in diverse biomedical applications is developed and validated. Efficiency in computational speed and accuracy is achieved by combining the advantages of recently developed optimization methods that use sample solutions with customized geometry and multiscale control space reduction, all paired with gradient-based techniques. The control space is effectively reduced based on the geometry of the samples and their individual contributions. The entire 3-step computational procedure has an easy-to-follow design due to a nominal number of tuning parameters making the approach simple for practical implementation in various settings. Fairly straightforward methods for computing gradients make the framework compatible with any optimization software, including black-box ones. The performance of the complete computational framework is tested in applications to 2D inverse problems of cancer detection by electrical impedance tomography (EIT) using data from models generated synthetically and obtained from medical images showing the natural development of cancerous regions of various sizes and shapes. The results demonstrate the superior performance of the new method and its high potential for improving the overall quality of the EIT-based procedures.},
  archive      = {J_COAP},
  author       = {Arbic II, Paul R. and Bukshtynov, Vladislav},
  doi          = {10.1007/s10589-024-00553-z},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {379-403},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient gradient-based optimization for reconstructing binary images in applications to electrical impedance tomography},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A benchmark generator for scenario-based discrete
optimization. <em>COAP</em>, <em>88</em>(1), 349–378. (<a
href="https://doi.org/10.1007/s10589-024-00551-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) are a practical tool to solve non-linear problems with multiple objective functions. However, when applied to expensive black-box scenario-based optimization problems, MOEA’s performance becomes constrained due to computational or time limitations. Scenario-based optimization refers to problems that are subject to uncertainty, where each solution is evaluated over an ensemble of scenarios to reduce risks. A primary reason for MOEA’s failure is that algorithm development is challenging in these cases as many of these problems are black-box, high-dimensional, discrete, and computationally expensive. For this reason, this paper proposes a benchmark generator to create fast-to-compute scenario-based discrete test problems with different degrees of complexity. Our framework uses the structure of the Multi-Objective Knapsack Problem to create test problems that simulate characteristics of expensive scenario-based discrete problems. To validate our proposition, we tested four state-of-the-art MOEAs in 30 test instances generated with our framework, and the empirical results demonstrate that the suggested benchmark generator can analyze the ability of MOEAs in tackling expensive scenario-based discrete optimization problems.},
  archive      = {J_COAP},
  author       = {de Moraes, Matheus Bernardelli and Coelho, Guilherme Palermo},
  doi          = {10.1007/s10589-024-00551-1},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {349-378},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A benchmark generator for scenario-based discrete optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convex approximations of two-stage risk-averse mixed-integer
recourse models. <em>COAP</em>, <em>88</em>(1), 313–347. (<a
href="https://doi.org/10.1007/s10589-024-00555-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two-stage risk-averse mixed-integer recourse models with law invariant coherent risk measures. As in the risk-neutral case, these models are generally non-convex as a result of the integer restrictions on the second-stage decision variables and hence, hard to solve. To overcome this issue, we propose a convex approximation approach. We derive a performance guarantee for this approximation in the form of an asymptotic error bound, which depends on the choice of risk measure. This error bound, which extends an existing error bound for the conditional value at risk, shows that our approximation method works particularly well if the distribution of the random parameters in the model is highly dispersed. For special cases we derive tighter, non-asymptotic error bounds. Whereas our error bounds are valid only for a continuously distributed second-stage right-hand side vector, practical optimization methods often require discrete distributions. In this context, we show that our error bounds provide statistical error bounds for the corresponding (discretized) sample average approximation (SAA) model. In addition, we construct a Benders’ decomposition algorithm that uses our convex approximations in an SAA-framework and we provide a performance guarantee for the resulting algorithm solution. Finally, we perform numerical experiments which show that for certain risk measures our approach works even better than our theoretical performance guarantees suggest.},
  archive      = {J_COAP},
  author       = {van Beesten, E. Ruben and Romeijnders, Ward and Roodbergen, Kees Jan},
  doi          = {10.1007/s10589-024-00555-x},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {313-347},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convex approximations of two-stage risk-averse mixed-integer recourse models},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convex mixed-integer nonlinear programs derived from
generalized disjunctive programming using cones. <em>COAP</em>,
<em>88</em>(1), 251–312. (<a
href="https://doi.org/10.1007/s10589-024-00557-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the formulation of convex Generalized Disjunctive Programming (GDP) problems using conic inequalities leading to conic GDP problems. We then show the reformulation of conic GDPs into Mixed-Integer Conic Programming (MICP) problems through both the big-M and hull reformulations. These reformulations have the advantage that they are representable using the same cones as the original conic GDP. In the case of the hull reformulation, they require no approximation of the perspective function. Moreover, the MICP problems derived can be solved by specialized conic solvers and offer a natural extended formulation amenable to both conic and gradient-based solvers. We present the closed form of several convex functions and their respective perspectives in conic sets, allowing users to formulate their conic GDP problems easily. We finally implement a large set of conic GDP examples and solve them via the scalar nonlinear and conic mixed-integer reformulations. These examples include applications from Process Systems Engineering, Machine learning, and randomly generated instances. Our results show that the conic structure can be exploited to solve these challenging MICP problems more efficiently. Our main contribution is providing the reformulations, examples, and computational results that support the claim that taking advantage of conic formulations of convex GDP instead of their nonlinear algebraic descriptions can lead to a more efficient solution to these problems.},
  archive      = {J_COAP},
  author       = {Bernal Neira, David E. and Grossmann, Ignacio E.},
  doi          = {10.1007/s10589-024-00557-9},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {251-312},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convex mixed-integer nonlinear programs derived from generalized disjunctive programming using cones},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternative extension of the hager–zhang conjugate gradient
method for vector optimization. <em>COAP</em>, <em>88</em>(1), 217–250.
(<a href="https://doi.org/10.1007/s10589-023-00548-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Gonçalves and Prudente proposed an extension of the Hager–Zhang nonlinear conjugate gradient method for vector optimization (Comput Optim Appl 76:889–916, 2020). They initially demonstrated that directly extending the Hager–Zhang method for vector optimization may not result in descent in the vector sense, even when employing an exact line search. By utilizing a sufficiently accurate line search, they subsequently introduced a self-adjusting Hager–Zhang conjugate gradient method in the vector sense. The global convergence of this new scheme was proven without requiring regular restarts or any convex assumptions. In this paper, we propose an alternative extension of the Hager–Zhang nonlinear conjugate gradient method for vector optimization that preserves its desirable scalar property, i.e., ensuring sufficiently descent without relying on any line search or convexity assumption. Furthermore, we investigate its global convergence with the Wolfe line search under mild assumptions. Finally, numerical experiments are presented to illustrate the practical behavior of our proposed method.},
  archive      = {J_COAP},
  author       = {Hu, Qingjie and Zhu, Liping and Chen, Yu},
  doi          = {10.1007/s10589-023-00548-2},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {217-250},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Alternative extension of the Hager–Zhang conjugate gradient method for vector optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerated forward–backward algorithms for structured
monotone inclusions. <em>COAP</em>, <em>88</em>(1), 167–215. (<a
href="https://doi.org/10.1007/s10589-023-00547-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop rapidly convergent forward–backward algorithms for computing zeroes of the sum of two maximally monotone operators. A modification of the classical forward–backward method is considered, by incorporating an inertial term (closed to the acceleration techniques introduced by Nesterov), a constant relaxation factor and a correction term, along with a preconditioning process. In a Hilbert space setting, we prove the weak convergence to equilibria of the iterates $$(x_n)$$ , with worst-case rates of $$ o(n^{-1})$$ in terms of both the discrete velocity and the fixed point residual, instead of the rates of $$\mathcal {O}(n^{-1/2})$$ classically established for related algorithms. Our procedure can be also adapted to more general monotone inclusions. In particular, we propose a fast primal-dual algorithmic solution to some class of convex-concave saddle point problems. In addition, we provide a well-adapted framework for solving this class of problems by means of standard proximal-like algorithms dedicated to structured monotone inclusions. Numerical experiments are also performed so as to enlighten the efficiency of the proposed strategy.},
  archive      = {J_COAP},
  author       = {Maingé, Paul-Emile and Weng-Law, André},
  doi          = {10.1007/s10589-023-00547-3},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {167-215},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerated forward–backward algorithms for structured monotone inclusions},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A note on the convergence of deterministic gradient sampling
in nonsmooth optimization. <em>COAP</em>, <em>88</em>(1), 151–165. (<a
href="https://doi.org/10.1007/s10589-024-00552-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation of subdifferentials is one of the main tasks when computing descent directions for nonsmooth optimization problems. In this article, we propose a bisection method for weakly lower semismooth functions which is able to compute new subgradients that improve a given approximation in case a direction with insufficient descent was computed. Combined with a recently proposed deterministic gradient sampling approach, this yields a deterministic and provably convergent way to approximate subdifferentials for computing descent directions.},
  archive      = {J_COAP},
  author       = {Gebken, Bennet},
  doi          = {10.1007/s10589-024-00552-0},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {151-165},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A note on the convergence of deterministic gradient sampling in nonsmooth optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coordinate descent methods beyond smoothness and
separability. <em>COAP</em>, <em>88</em>(1), 107–149. (<a
href="https://doi.org/10.1007/s10589-024-00556-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with convex nonsmooth optimization problems. We introduce a general smooth approximation framework for the original function and apply random (accelerated) coordinate descent methods for minimizing the corresponding smooth approximations. Our framework covers the most important classes of smoothing techniques from the literature. Based on this general framework for the smooth approximation and using coordinate descent type methods we derive convergence rates in function values for the original objective. Moreover, if the original function satisfies a growth condition, then we prove that the smooth approximations also inherits this condition and consequently the convergence rates are improved in this case. We also present a relative randomized coordinate descent algorithm for solving nonseparable minimization problems with the objective function relative smooth along coordinates w.r.t. a (possibly nonseparable) differentiable function. For this algorithm we also derive convergence rates in the convex case and under the growth condition for the objective.},
  archive      = {J_COAP},
  author       = {Chorobura, Flavia and Necoara, Ion},
  doi          = {10.1007/s10589-024-00556-w},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {107-149},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Coordinate descent methods beyond smoothness and separability},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPIRAL: A superlinearly convergent incremental proximal
algorithm for nonconvex finite sum minimization. <em>COAP</em>,
<em>88</em>(1), 71–106. (<a
href="https://doi.org/10.1007/s10589-023-00550-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SPIRAL, a SuPerlinearly convergent Incremental pRoximal ALgorithm, for solving nonconvex regularized finite sum problems under a relative smoothness assumption. Each iteration of SPIRAL consists of an inner and an outer loop. It combines incremental gradient updates with a linesearch that has the remarkable property of never being triggered asymptotically, leading to superlinear convergence under mild assumptions at the limit point. Simulation results with L-BFGS directions on different convex, nonconvex, and non-Lipschitz differentiable problems show that our algorithm, as well as its adaptive variant, are competitive to the state of the art.},
  archive      = {J_COAP},
  author       = {Behmandpoor, Pourya and Latafat, Puya and Themelis, Andreas and Moonen, Marc and Patrinos, Panagiotis},
  doi          = {10.1007/s10589-023-00550-8},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {71-106},
  shortjournal = {Comput. Optim. Appl.},
  title        = {SPIRAL: A superlinearly convergent incremental proximal algorithm for nonconvex finite sum minimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A projected-search interior-point method for nonlinearly
constrained optimization. <em>COAP</em>, <em>88</em>(1), 37–70. (<a
href="https://doi.org/10.1007/s10589-023-00549-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the formulation and analysis of a new interior-point method for constrained optimization that combines a shifted primal-dual interior-point method with a projected-search method for bound-constrained optimization. The method involves the computation of an approximate Newton direction for a primal-dual penalty-barrier function that incorporates shifts on both the primal and dual variables. Shifts on the dual variables allow the method to be safely “warm started” from a good approximate solution and avoids the possibility of very large solutions of the associated path-following equations. The approximate Newton direction is used in conjunction with a new projected-search line-search algorithm that employs a flexible non-monotone quasi-Armijo line search for the minimization of each penalty-barrier function. Numerical results are presented for a large set of constrained optimization problems. For comparison purposes, results are also given for two primal-dual interior-point methods that do not use projection. The first is a method that shifts both the primal and dual variables. The second is a method that involves shifts on the primal variables only. The results show that the use of both primal and dual shifts in conjunction with projection gives a method that is more robust and requires significantly fewer iterations. In particular, the number of times that the search direction must be computed is substantially reduced. Results from a set of quadratic programming test problems indicate that the method is particularly well-suited to solving the quadratic programming subproblem in a sequential quadratic programming method for nonlinear optimization.},
  archive      = {J_COAP},
  author       = {Gill, Philip E. and Zhang, Minxin},
  doi          = {10.1007/s10589-023-00549-1},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {37-70},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A projected-search interior-point method for nonlinearly constrained optimization},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IPRSDP: A primal-dual interior-point relaxation algorithm
for semidefinite programming. <em>COAP</em>, <em>88</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10589-024-00558-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient primal-dual interior-point relaxation algorithm based on a smoothing barrier augmented Lagrangian, called IPRSDP, for solving semidefinite programming problems in this paper. The IPRSDP algorithm has three advantages over classical interior-point methods. Firstly, IPRSDP does not require the iterative points to be positive definite. Consequently, it can easily be combined with the warm-start technique used for solving many combinatorial optimization problems, which require the solutions of a series of semidefinite programming problems. Secondly, the search direction of IPRSDP is symmetric in itself, and hence the symmetrization procedure is not required any more. Thirdly, with the introduction of the smoothing barrier augmented Lagrangian function, IPRSDP can provide the explicit form of the Schur complement matrix. This enables the complexity of forming this matrix in IPRSDP to be comparable to or lower than that of many existing search directions. The global convergence of IPRSDP is established under suitable assumptions. Numerical experiments are made on the SDPLIB set, which demonstrate the efficiency of IPRSDP.},
  archive      = {J_COAP},
  author       = {Zhang, Rui-Jin and Liu, Xin-Wei and Dai, Yu-Hong},
  doi          = {10.1007/s10589-024-00558-8},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Comput. Optim. Appl.},
  title        = {IPRSDP: A primal-dual interior-point relaxation algorithm for semidefinite programming},
  volume       = {88},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A bregman–kaczmarz method for nonlinear systems of
equations. <em>COAP</em>, <em>87</em>(3), 1059–1098. (<a
href="https://doi.org/10.1007/s10589-023-00541-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new randomized method for solving systems of nonlinear equations, which can find sparse solutions or solutions under certain simple constraints. The scheme only takes gradients of component functions and uses Bregman projections onto the solution space of a Newton equation. In the special case of euclidean projections, the method is known as nonlinear Kaczmarz method. Furthermore if the component functions are nonnegative, we are in the setting of optimization under the interpolation assumption and the method reduces to SGD with the recently proposed stochastic Polyak step size. For general Bregman projections, our method is a stochastic mirror descent with a novel adaptive step size. We prove that in the convex setting each iteration of our method results in a smaller Bregman distance to exact solutions as compared to the standard Polyak step. Our generalization to Bregman projections comes with the price that a convex one-dimensional optimization problem needs to be solved in each iteration. This can typically be done with globalized Newton iterations. Convergence is proved in two classical settings of nonlinearity: for convex nonnegative functions and locally for functions which fulfill the tangential cone condition. Finally, we show examples in which the proposed method outperforms similar methods with the same memory requirements.},
  archive      = {J_COAP},
  author       = {Gower, Robert and Lorenz, Dirk A. and Winkler, Maximilian},
  doi          = {10.1007/s10589-023-00541-9},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {1059-1098},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A Bregman–Kaczmarz method for nonlinear systems of equations},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Internet traffic tensor completion with tensor nuclear norm.
<em>COAP</em>, <em>87</em>(3), 1033–1057. (<a
href="https://doi.org/10.1007/s10589-023-00545-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incomplete data is a common phenomenon in traffic network because of the high measurement cost, the failure of data collection systems and unavoidable transmission loss. Recovering the whole data from incomplete data is a very important task in internet engineering and management. In this paper, we adopt the low-rank tensor completion model equipped with tensor nuclear norm to reconstruct the internet traffic data. Besides using a low rank tensor to capture the global information of internet traffic data, we also utilize spatial correlation and periodicity to characterize the local information. The resulting model is a convex and separable optimization. Then, a proximal alternating direction method of multipliers is customized to solve the optimization problem, where all subproblems have closed-form solutions. Convergence analysis of the algorithm is given without any assumptions. Numerical experiments on Abilene and GÉANT datasets with random missing and structured loss show that the proposed model and algorithm perform better than other existing algorithms.},
  archive      = {J_COAP},
  author       = {Li, Can and Chen, Yannan and Li, Dong-Hui},
  doi          = {10.1007/s10589-023-00545-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {1033-1057},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Internet traffic tensor completion with tensor nuclear norm},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A family of barzilai-borwein steplengths from the viewpoint
of scaled total least squares. <em>COAP</em>, <em>87</em>(3), 1011–1031.
(<a href="https://doi.org/10.1007/s10589-023-00546-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Barzilai-Borwein (BB) steplengths play great roles in practical gradient methods for solving unconstrained optimization problems. Motivated by the observation that the two well-known BB steplengths correspond to the ordinary and the data least squares, respectively, we introduce a novel family of BB steplengths from the viewpoint of scaled total least squares. Numerical experiments demonstrate that high performance can be received by a carefully-selected BB steplength in the new family.},
  archive      = {J_COAP},
  author       = {Li, Shiru and Zhang, Tao and Xia, Yong},
  doi          = {10.1007/s10589-023-00546-4},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {1011-1031},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A family of barzilai-borwein steplengths from the viewpoint of scaled total least squares},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: The continuous stochastic gradient method:
Part II–application and numerics. <em>COAP</em>, <em>87</em>(3),
1009–1010. (<a
href="https://doi.org/10.1007/s10589-023-00544-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Grieshammer, Max and Pflug, Lukas and Stingl, Michael and Uihlein, Andrian},
  doi          = {10.1007/s10589-023-00544-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {1009-1010},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: the continuous stochastic gradient method: part II–application and numerics},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). The continuous stochastic gradient method: Part
II–application and numerics. <em>COAP</em>, <em>87</em>(3), 977–1008.
(<a href="https://doi.org/10.1007/s10589-023-00540-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution, we present a numerical analysis of the continuous stochastic gradient (CSG) method, including applications from topology optimization and convergence rates. In contrast to standard stochastic gradient optimization schemes, CSG does not discard old gradient samples from previous iterations. Instead, design dependent integration weights are calculated to form a convex combination as an approximation to the true gradient at the current design. As the approximation error vanishes in the course of the iterations, CSG represents a hybrid approach, starting off like a purely stochastic method and behaving like a full gradient scheme in the limit. In this work, the efficiency of CSG is demonstrated for practically relevant applications from topology optimization. These settings are characterized by both, a large number of optimization variables and an objective function, whose evaluation requires the numerical computation of multiple integrals concatenated in a nonlinear fashion. Such problems could not be solved by any existing optimization method before. Lastly, with regards to convergence rates, first estimates are provided and confirmed with the help of numerical experiments.},
  archive      = {J_COAP},
  author       = {Grieshammer, Max and Pflug, Lukas and Stingl, Michael and Uihlein, Andrian},
  doi          = {10.1007/s10589-023-00540-w},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {977-1008},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The continuous stochastic gradient method: Part II–application and numerics},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). The continuous stochastic gradient method: Part
i–convergence theory. <em>COAP</em>, <em>87</em>(3), 935–976. (<a
href="https://doi.org/10.1007/s10589-023-00542-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution, we present a full overview of the continuous stochastic gradient (CSG) method, including convergence results, step size rules and algorithmic insights. We consider optimization problems in which the objective function requires some form of integration, e.g., expected values. Since approximating the integration by a fixed quadrature rule can introduce artificial local solutions into the problem while simultaneously raising the computational effort, stochastic optimization schemes have become increasingly popular in such contexts. However, known stochastic gradient type methods are typically limited to expected risk functions and inherently require many iterations. The latter is particularly problematic, if the evaluation of the cost function involves solving multiple state equations, given, e.g., in form of partial differential equations. To overcome these drawbacks, a recent article introduced the CSG method, which reuses old gradient sample information via the calculation of design dependent integration weights to obtain a better approximation to the full gradient. While in the original CSG paper convergence of a subsequence was established for a diminishing step size, here, we provide a complete convergence analysis of CSG for constant step sizes and an Armijo-type line search. Moreover, new methods to obtain the integration weights are presented, extending the application range of CSG to problems involving higher dimensional integrals and distributed data.},
  archive      = {J_COAP},
  author       = {Grieshammer, Max and Pflug, Lukas and Stingl, Michael and Uihlein, Andrian},
  doi          = {10.1007/s10589-023-00542-8},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {935-976},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The continuous stochastic gradient method: Part i–convergence theory},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Enhancements of discretization approaches for non-convex
mixed-integer quadratically constrained quadratic programming: Part II.
<em>COAP</em>, <em>87</em>(3), 893–934. (<a
href="https://doi.org/10.1007/s10589-024-00554-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is Part II of a study on mixed-integer programming (MIP) relaxation techniques for the solution of non-convex mixed-integer quadratically constrained quadratic programs (MIQCQPs). We set the focus on MIP relaxation methods for non-convex continuous variable products where both variables are bounded and extend the well-known MIP relaxation normalized multiparametric disaggregation technique(NMDT), applying a sophisticated discretization to both variables. We refer to this approach as doubly discretized normalized multiparametric disaggregation technique (D-NMDT). In a comprehensive theoretical analysis, we underline the theoretical advantages of the enhanced method D-NMDT compared to NMDT. Furthermore, we perform a broad computational study to demonstrate its effectiveness in terms of producing tight dual bounds for MIQCQPs. Finally, we compare D-NMDT to the separable MIP relaxations from Part I and a state-of-the-art MIQCQP solver.},
  archive      = {J_COAP},
  author       = {Beach, Benjamin and Burlacu, Robert and Bärmann, Andreas and Hager, Lukas and Hildebrand, Robert},
  doi          = {10.1007/s10589-024-00554-y},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {893-934},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Enhancements of discretization approaches for non-convex mixed-integer quadratically constrained quadratic programming: Part II},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Enhancements of discretization approaches for non-convex
mixed-integer quadratically constrained quadratic programming: Part i.
<em>COAP</em>, <em>87</em>(3), 835–891. (<a
href="https://doi.org/10.1007/s10589-023-00543-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study mixed-integer programming (MIP) relaxation techniques for the solution of non-convex mixed-integer quadratically constrained quadratic programs (MIQCQPs). We present MIP relaxation methods for non-convex continuous variable products. In this paper, we consider MIP relaxations based on separable reformulation. The main focus is the introduction of the enhanced separable MIP relaxation for non-convex quadratic products of the form $$z=xy$$ , called hybrid separable (HybS). Additionally, we introduce a logarithmic MIP relaxation for univariate quadratic terms, called sawtooth relaxation, based on Beach (Beach in J Glob Optim 84:869–912, 2022). We combine the latter with HybS and existing separable reformulations to derive MIP relaxations of MIQCQPs. We provide a comprehensive theoretical analysis of these techniques, underlining the theoretical advantages of HybS compared to its predecessors. We perform a broad computational study to demonstrate the effectiveness of the enhanced MIP relaxation in terms of producing tight dual bounds for MIQCQPs. In Part II, we study MIP relaxations that extend the MIP relaxation normalized multiparametric disaggregation technique (NMDT) (Castro in J Glob Optim 64:765–784, 2015) and present a computational study which also includes the MIP relaxations from this work and compares them with a state-of-the-art of MIQCQP solvers.},
  archive      = {J_COAP},
  author       = {Beach, Benjamin and Burlacu, Robert and Bärmann, Andreas and Hager, Lukas and Hildebrand, Robert},
  doi          = {10.1007/s10589-023-00543-7},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {835-891},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Enhancements of discretization approaches for non-convex mixed-integer quadratically constrained quadratic programming: Part i},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal control problems with <span
class="math display"><em>L</em><sup>0</sup>(<em>Ω</em>)</span>
constraints: Maximum principle and proximal gradient method.
<em>COAP</em>, <em>87</em>(3), 811–833. (<a
href="https://doi.org/10.1007/s10589-023-00456-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate optimal control problems with $$L^0$$ constraints, which restrict the measure of the support of the controls. We prove necessary optimality conditions of Pontryagin maximum principle type. Here, a special control perturbation is used that respects the $$L^0$$ constraint. First, the maximum principle is obtained in integral form, which is then turned into a pointwise form. In addition, an optimization algorithm of proximal gradient type is analyzed. Under some assumptions, the sequence of iterates contains strongly converging subsequences, whose limits are feasible and satisfy a subset of the necessary optimality conditions.},
  archive      = {J_COAP},
  author       = {Wachsmuth, Daniel},
  doi          = {10.1007/s10589-023-00456-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {811-833},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimal control problems with $$L^0(\Omega )$$ constraints: Maximum principle and proximal gradient method},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy iteration for hamilton–jacobi–bellman equations with
control constraints. <em>COAP</em>, <em>87</em>(3), 785–809. (<a
href="https://doi.org/10.1007/s10589-021-00278-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy iteration is a widely used technique to solve the Hamilton Jacobi Bellman (HJB) equation, which arises from nonlinear optimal feedback control theory. Its convergence analysis has attracted much attention in the unconstrained case. Here we analyze the case with control constraints both for the HJB equations which arise in deterministic and in stochastic control cases. The linear equations in each iteration step are solved by an implicit upwind scheme. Numerical examples are conducted to solve the HJB equation with control constraints and comparisons are shown with the unconstrained cases.},
  archive      = {J_COAP},
  author       = {Kundu, Sudeep and Kunisch, Karl},
  doi          = {10.1007/s10589-021-00278-3},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {785-809},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Policy iteration for Hamilton–Jacobi–Bellman equations with control constraints},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A-posteriori reduced basis error-estimates for a
semi-discrete in space quasilinear parabolic PDE. <em>COAP</em>,
<em>87</em>(3), 755–784. (<a
href="https://doi.org/10.1007/s10589-021-00299-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove a-posteriori error-estimates for reduced-order modeling of quasilinear parabolic PDEs with non-monotone nonlinearity. We consider the solution of a semi-discrete in space equation as reference, and therefore incorporate reduced basis-, empirical interpolation-, and time-discretization-errors in our consideration. Numerical experiments illustrate our results.},
  archive      = {J_COAP},
  author       = {Hoppe, Fabian and Neitzel, Ira},
  doi          = {10.1007/s10589-021-00299-y},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {755-784},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A-posteriori reduced basis error-estimates for a semi-discrete in space quasilinear parabolic PDE},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Differentiability results and sensitivity
calculation for optimal control of incompressible two-phase
navier–stokes equations with surface tension. <em>COAP</em>,
<em>87</em>(3), 753–754. (<a
href="https://doi.org/10.1007/s10589-022-00423-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Diehl, Elisabeth and Haubner, Johannes and Ulbrich, Michael and Ulbrich, Stefan},
  doi          = {10.1007/s10589-022-00423-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {753-754},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: Differentiability results and sensitivity calculation for optimal control of incompressible two-phase Navier–Stokes equations with surface tension},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Differentiability results and sensitivity calculation for
optimal control of incompressible two-phase navier-stokes equations with
surface tension. <em>COAP</em>, <em>87</em>(3), 711–751. (<a
href="https://doi.org/10.1007/s10589-022-00415-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze optimal control problems for two-phase Navier-Stokes equations with surface tension. Based on $$L_p$$ -maximal regularity of the underlying linear problem and recent well-posedness results of the problem for sufficiently small data we show the differentiability of the solution with respect to initial and distributed controls for appropriate spaces resulting from the $$L_p$$ -maximal regularity setting. We consider first a formulation where the interface is transformed to a hyperplane. Then we deduce differentiability results for the solution in the physical coordinates. Finally, we state an equivalent Volume-of-Fluid type formulation and use the obtained differentiability results to derive rigorosly the corresponding sensitivity equations of the Volume-of-Fluid type formulation. For objective functionals involving the velocity field or the discontinuous pressure or phase indciator field we derive differentiability results with respect to controls and state formulas for the derivative. The results of the paper form an analytical foundation for stating optimality conditions, justifying the application of derivative based optimization methods and for studying the convergence of discrete sensitivity schemes based on Volume-of-Fluid discretizations for optimal control of two-phase Navier-Stokes equations.},
  archive      = {J_COAP},
  author       = {Diehl, Elisabeth and Haubner, Johannes and Ulbrich, Michael and Ulbrich, Stefan},
  doi          = {10.1007/s10589-022-00415-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {711-751},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Differentiability results and sensitivity calculation for optimal control of incompressible two-phase navier-stokes equations with surface tension},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preface to special issue on “optimal control of nonlinear
differential equations.” <em>COAP</em>, <em>87</em>(3), 707–710. (<a
href="https://doi.org/10.1007/s10589-024-00566-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Clason, Christian},
  doi          = {10.1007/s10589-024-00566-8},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {707-710},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface to special issue on “optimal control of nonlinear differential equations”},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction: On the asymptotic rate of convergence of
stochastic newton algorithms and their weighted averaged versions.
<em>COAP</em>, <em>87</em>(2), 705–706. (<a
href="https://doi.org/10.1007/s10589-023-00511-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Boyer, Claire and Godichon‑Baggioni, Antoine},
  doi          = {10.1007/s10589-023-00511-1},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {705-706},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction: On the asymptotic rate of convergence of stochastic newton algorithms and their weighted averaged versions},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nested genetic algorithm strategy for an optimal seismic
design of frames. <em>COAP</em>, <em>87</em>(2), 677–704. (<a
href="https://doi.org/10.1007/s10589-023-00523-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovative strategy for an optimal design of planar frames able to resist seismic excitations is proposed. The optimal design is performed considering the cross sections of beams and columns as design variables. The procedure is based on genetic algorithms (GA) that are performed according to a nested structure suitable to be implemented in parallel on several computing devices. In particular, this bi-level optimization involves two nested genetic algorithms. The first external one seeks the size of the structural elements of the frame which corresponds to the most performing solution associated with the highest value of an appropriate fitness function. The latter function takes into account, among other considerations, the seismic safety factor and the failure mode that are calculated by means of the second internal algorithm. The proposed procedure aims at representing a prompt performance-based design procedure which observes earthquake engineering principles, that is displacement capacity and energy dissipation, although based on a limit analysis, thus avoiding the need of performing cumbersome nonlinear analyses. The details of the proposed procedure are provided and applications to the seismic design of two frames of different size are described.},
  archive      = {J_COAP},
  author       = {Greco, A. and Cannizzaro, F. and Bruno, R. and Pluchino, A.},
  doi          = {10.1007/s10589-023-00523-x},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {677-704},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A nested genetic algorithm strategy for an optimal seismic design of frames},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equilibrium modeling and solution approaches inspired by
nonconvex bilevel programming. <em>COAP</em>, <em>87</em>(2), 641–676.
(<a href="https://doi.org/10.1007/s10589-023-00524-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for finding pure Nash equilibria have been dominated by variational inequalities and complementarity problems. Since these approaches fundamentally rely on the sufficiency of first-order optimality conditions for the players’ decision problems, they only apply as heuristic methods when the players are modeled by nonconvex optimization problems. In contrast, this work approaches Nash equilibrium using theory and methods for the global optimization of nonconvex bilevel programs. Through this perspective, we draw precise connections between Nash equilibria, feasibility for bilevel programming, the Nikaido–Isoda function, and classic arguments involving Lagrangian duality and spatial price equilibrium. Significantly, this is all in a general setting without the assumption of convexity. Along the way, we introduce the idea of minimum disequilibrium as a solution concept that reduces to traditional equilibrium when an equilibrium exists. The connections with bilevel programming and related semi-infinite programming permit us to adapt global optimization methods for those classes of problems, such as constraint generation or cutting plane methods, to the problem of finding a minimum disequilibrium solution. We propose a specific algorithm and show that this method can find a pure Nash equilibrium even when the players are modeled by mixed-integer programs. Our computational examples include practical applications like unit commitment in electricity markets.},
  archive      = {J_COAP},
  author       = {Harwood, Stuart and Trespalacios, Francisco and Papageorgiou, Dimitri and Furman, Kevin},
  doi          = {10.1007/s10589-023-00524-w},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {641-676},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Equilibrium modeling and solution approaches inspired by nonconvex bilevel programming},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexact proximal DC newton-type method for nonconvex
composite functions. <em>COAP</em>, <em>87</em>(2), 611–640. (<a
href="https://doi.org/10.1007/s10589-023-00525-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of difference-of-convex (DC) optimization problems where the objective function is the sum of a smooth function and a possibly nonsmooth DC function. The application of proximal DC algorithms to address this problem class is well-known. In this paper, we combine a proximal DC algorithm with an inexact proximal Newton-type method to propose an inexact proximal DC Newton-type method. We demonstrate global convergence properties of the proposed method. In addition, we give a memoryless quasi-Newton matrix for scaled proximal mappings and consider a two-dimensional system of semi-smooth equations that arise in calculating scaled proximal mappings. To efficiently obtain the scaled proximal mappings, we adopt a semi-smooth Newton method to inexactly solve the system. Finally, we present some numerical experiments to investigate the efficiency of the proposed method, which show that the proposed method outperforms existing methods.},
  archive      = {J_COAP},
  author       = {Nakayama, Shummin and Narushima, Yasushi and Yabe, Hiroshi},
  doi          = {10.1007/s10589-023-00525-9},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {611-640},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact proximal DC newton-type method for nonconvex composite functions},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linearly convergent bilevel optimization with single-step
inner methods. <em>COAP</em>, <em>87</em>(2), 571–610. (<a
href="https://doi.org/10.1007/s10589-023-00527-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to solving bilevel optimization problems, intermediate between solving full-system optimality conditions with a Newton-type approach, and treating the inner problem as an implicit function. The overall idea is to solve the full-system optimality conditions, but to precondition them to alternate between taking steps of simple conventional methods for the inner problem, the adjoint equation, and the outer problem. While the inner objective has to be smooth, the outer objective may be nonsmooth subject to a prox-contractivity condition. We prove linear convergence of the approach for combinations of gradient descent and forward-backward splitting with exact and inexact solution of the adjoint equation. We demonstrate good performance on learning the regularization parameter for anisotropic total variation image denoising, and the convolution kernel for image deconvolution.},
  archive      = {J_COAP},
  author       = {Suonperä, Ensio and Valkonen, Tuomo},
  doi          = {10.1007/s10589-023-00527-7},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {571-610},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Linearly convergent bilevel optimization with single-step inner methods},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast continuous time approach for non-smooth convex
optimization using tikhonov regularization technique. <em>COAP</em>,
<em>87</em>(2), 531–569. (<a
href="https://doi.org/10.1007/s10589-023-00536-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we would like to address the classical optimization problem of minimizing a proper, convex and lower semicontinuous function via the second order in time dynamics, combining viscous and Hessian-driven damping with a Tikhonov regularization term. In our analysis we heavily exploit the Moreau envelope of the objective function and its properties as well as Tikhonov regularization properties, which we extend to a nonsmooth case. We introduce the setting, which at the same time guarantees the fast convergence of the function (and Moreau envelope) values and strong convergence of the trajectories of the system to a minimal norm solution—the element of the minimal norm of all the minimizers of the objective. Moreover, we deduce the precise rates of convergence of the values for the particular choice of parameters. Various numerical examples are also included as an illustration of the theoretical results.},
  archive      = {J_COAP},
  author       = {Karapetyants, Mikhail A.},
  doi          = {10.1007/s10589-023-00536-6},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {531-569},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A fast continuous time approach for non-smooth convex optimization using tikhonov regularization technique},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A numerical-and-computational study on the impact of using
quaternions in the branch-and-prune algorithm for exact discretizable
distance geometry problems. <em>COAP</em>, <em>87</em>(2), 501–530. (<a
href="https://doi.org/10.1007/s10589-023-00526-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance geometry is a branch of Mathematics which studies geometric relations having distances as primitive elements. Its fundamental problem, the distance geometry problem, consists in determining point positions in $$\mathbb {R}^K$$ such that their Euclidean distances match those in a given list of inter-point distances. Such problem can be cast as a global optimization problem which is often tackled with continuous optimization techniques. If $$K=3$$ , it is called molecular DGP (MDGP). Under assumptions on the available distances in this list, the search space for the MDGP can be discretized so that it is able to be designed as a binary tree, giving birth to the discretized MDGP. The main method to solve it is the Branch-and-Prune Algorithm, a recursive and combinatorial tool that explores such tree in a depth-first search and whose classical formulation is based in a homogeneous matrix product that encodes one translation and two rotations. This paper presents a numerical study on the theoretical computational effort to do that task together with a quaternionic proposal as an alternative for the formulation of BP and the respective analogous numerical study, for comparing with the matrix one. Additionally, best-and-worst-case analyzes for both approaches are displayed. Finally, in order to validate the new formulation as having better computational performance for BP, a set of computational experiments are shown using artificial Lavor instances and pre-processed proteic examples which have been generated from structures withdrawn from the worldwide protein data bank.},
  archive      = {J_COAP},
  author       = {Fidalgo, Felipe and Castelani, Emerson and Philippi, Guilherme},
  doi          = {10.1007/s10589-023-00526-8},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {501-530},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A numerical-and-computational study on the impact of using quaternions in the branch-and-prune algorithm for exact discretizable distance geometry problems},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new technique to derive tight convex underestimators
(sometimes envelopes). <em>COAP</em>, <em>87</em>(2), 475–499. (<a
href="https://doi.org/10.1007/s10589-023-00534-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convex envelope value for a given function f over a region X at some point $$\textbf{x}\in X$$ can be derived by searching for the largest value at that point among affine underestimators of f over X. This can be computed by solving a maximin problem, whose exact computation, however, may be a hard task. In this paper we show that by relaxation of the inner minimization problem, duality, and, in particular, by an enlargement of the class of underestimators (thus, not only affine ones) an easier derivation of good convex understimating functions, which can also be proved to be convex envelopes in some cases, is possible. The proposed approach is mainly applied to the derivation of convex underestimators (in fact, in some cases, convex envelopes) in the quadratic case. However, some results are also presented for polynomial, ratio of polynomials, and some other separable functions over regions defined by similarly defined separable functions.},
  archive      = {J_COAP},
  author       = {Locatelli, M.},
  doi          = {10.1007/s10589-023-00534-8},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {475-499},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A new technique to derive tight convex underestimators (sometimes envelopes)},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiency of higher-order algorithms for minimizing
composite functions. <em>COAP</em>, <em>87</em>(2), 441–473. (<a
href="https://doi.org/10.1007/s10589-023-00533-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite minimization involves a collection of functions which are aggregated in a nonsmooth manner. It covers, as a particular case, smooth approximation of minimax games, minimization of max-type functions, and simple composite minimization problems, where the objective function has a nonsmooth component. We design a higher-order majorization algorithmic framework for fully composite problems (possibly nonconvex). Our framework replaces each component with a higher-order surrogate such that the corresponding error function has a higher-order Lipschitz continuous derivative. We present convergence guarantees for our method for composite optimization problems with (non)convex and (non)smooth objective function. In particular, we prove stationary point convergence guarantees for general nonconvex (possibly nonsmooth) problems and under Kurdyka–Lojasiewicz (KL) property of the objective function we derive improved rates depending on the KL parameter. For convex (possibly nonsmooth) problems we also provide sublinear convergence rates.},
  archive      = {J_COAP},
  author       = {Nabou, Yassine and Necoara, Ion},
  doi          = {10.1007/s10589-023-00533-9},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {441-473},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficiency of higher-order algorithms for minimizing composite functions},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: Stochastic projective splitting.
<em>COAP</em>, <em>87</em>(2), 439. (<a
href="https://doi.org/10.1007/s10589-023-00539-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Johnstone, Patrick R. and Eckstein, Jonathan and Flynn, Thomas and Yoo, Shinjae},
  doi          = {10.1007/s10589-023-00539-3},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {439},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: Stochastic projective splitting},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Stochastic projective splitting. <em>COAP</em>,
<em>87</em>(2), 397–437. (<a
href="https://doi.org/10.1007/s10589-023-00528-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new, stochastic variant of the projective splitting (PS) family of algorithms for inclusion problems involving the sum of any finite number of maximal monotone operators. This new variant uses a stochastic oracle to evaluate one of the operators, which is assumed to be Lipschitz continuous, and (deterministic) resolvents to process the remaining operators. Our proposal is the first version of PS with such stochastic capabilities. We envision the primary application being machine learning (ML) problems, with the method’s stochastic features facilitating “mini-batch” sampling of datasets. Since it uses a monotone operator formulation, the method can handle not only Lipschitz-smooth loss minimization, but also min–max and noncooperative game formulations, with better convergence properties than the gradient descent-ascent methods commonly applied in such settings. The proposed method can handle any number of constraints and nonsmooth regularizers via projection and proximal operators. We prove almost-sure convergence of the iterates to a solution and a convergence rate result for the expected residual, and close with numerical experiments on a distributionally robust sparse logistic regression problem.},
  archive      = {J_COAP},
  author       = {Johnstone, Patrick R. and Eckstein, Jonathan and Flynn, Thomas and Yoo, Shinjae},
  doi          = {10.1007/s10589-023-00528-6},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {397-437},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic projective splitting},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution-free algorithms for predictive stochastic
programming in the presence of streaming data. <em>COAP</em>,
<em>87</em>(2), 355–395. (<a
href="https://doi.org/10.1007/s10589-023-00529-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a fusion of concepts from stochastic programming and non-parametric statistical learning in which data is available in the form of covariates interpreted as predictors and responses. Such models are designed to impart greater agility, allowing decisions under uncertainty to adapt to the knowledge of predictors (leading indicators). This paper studies two classes of methods for such joint prediction-optimization models. One of the methods may be classified as a first-order method, whereas the other studies piecewise linear approximations. Both of these methods are based on coupling non-parametric estimation for predictive purposes, and optimization for decision-making within one unified framework. In addition, our study incorporates several non-parametric estimation schemes, including k nearest neighbors (kNN) and other standard kernel estimators. Our computational results demonstrate that the new algorithms proposed in this paper outperform traditional approaches which were not designed for streaming data applications requiring simultaneous estimation and optimization as important design features for such algorithms. For instance, coupling kNN with Stochastic Decomposition (SD) turns out to be over 40 times faster than an online version of Benders Decomposition while finding decisions of similar quality. Such computational results motivate a paradigm shift in optimization algorithms that are intended for modern streaming applications.},
  archive      = {J_COAP},
  author       = {Diao, Shuotao and Sen, Suvrajeet},
  doi          = {10.1007/s10589-023-00529-5},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {355-395},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distribution-free algorithms for predictive stochastic programming in the presence of streaming data},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiobjective BFGS method for optimization on riemannian
manifolds. <em>COAP</em>, <em>87</em>(2), 337–354. (<a
href="https://doi.org/10.1007/s10589-023-00522-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a Riemannian BFGS method to address multiobjective optimization problems with strongly retraction-convex objective functions. This method is a natural extension of the Euclidean version and produces a sequence of iterates that converges to a Pareto optimal point regardless of the initial point. The main component of our globalization strategy is a generalized Wolfe line search. Numerical experiments demonstrate the superiority and effectiveness of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Najafi, Shahabeddin and Hajarian, Masoud},
  doi          = {10.1007/s10589-023-00522-y},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {337-354},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Multiobjective BFGS method for optimization on riemannian manifolds},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An easily computable upper bound on the hoffman constant for
homogeneous inequality systems. <em>COAP</em>, <em>87</em>(1), 323–335.
(<a href="https://doi.org/10.1007/s10589-023-00514-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $$A\in {\mathbb R}^{m\times n}\setminus \{0\}$$ and $$P:=\{x:Ax\le 0\}$$ . This paper provides a procedure to compute an upper bound on the following homogeneous Hoffman constant $$\begin{aligned} H_0(A):= \sup _{u\in {\mathbb R}^n \setminus P} \frac{{{\,\textrm{dist}\,}}(u,P)}{{{\,\textrm{dist}\,}}(Au, {\mathbb R}^m_-)}. \end{aligned}$$ In sharp contrast to the intractability of computing more general Hoffman constants, the procedure described in this paper is entirely tractable and easily implementable.},
  archive      = {J_COAP},
  author       = {Peña, Javier F.},
  doi          = {10.1007/s10589-023-00514-y},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {323-335},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An easily computable upper bound on the hoffman constant for homogeneous inequality systems},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified inexact levenberg–marquardt method with the
descent property for solving nonlinear equations. <em>COAP</em>,
<em>87</em>(1), 289–322. (<a
href="https://doi.org/10.1007/s10589-023-00513-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a modified inexact Levenberg–Marquardt method with the descent property for solving nonlinear equations. A novel feature of the proposed method is that one can directly use the search direction generated by the approach to perform Armijo-type line search once the unit step size is not acceptable. We achieve this via properly controlling the level of inexactness such that the resulting search direction is automatically a descent direction for the merit function. Under the local Lipschitz continuity of the Jacobian, the global convergence of the proposed method is established, and an iteration complexity bound of $$O(1/\epsilon ^2)$$ to reach an $$\epsilon $$ -stationary solution is proved under some appropriate conditions. Moreover, with the aid of the designed inexactness condition, we establish the local superlinear rate of convergence for the proposed method under the Hölderian continuity of the Jacobian and the Hölderian local error bound condition. For some special parameters, the convergence rate is even quadratic. The numerical experiments on the underdetermined nonlinear equations illustrate the effectiveness and efficiency of the algorithm compared with a previously proposed inexact Levenberg–Marquardt method. Finally, applying it to solve the Tikhonov-regularized logistic regression shows that our proposed method is quite promising.},
  archive      = {J_COAP},
  author       = {Yin, Jianghua and Jian, Jinbao and Ma, Guodong},
  doi          = {10.1007/s10589-023-00513-z},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {289-322},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A modified inexact Levenberg–Marquardt method with the descent property for solving nonlinear equations},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed stochastic compositional optimization problems
over directed networks. <em>COAP</em>, <em>87</em>(1), 249–288. (<a
href="https://doi.org/10.1007/s10589-023-00512-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the distributed stochastic compositional optimization problems over directed communication networks in which agents privately own a stochastic compositional objective function and collaborate to minimize the sum of all objective functions. We propose a distributed stochastic compositional gradient descent method, where the gradient tracking and the stochastic correction techniques are employed to adapt to the networks’ directed structure and increase the accuracy of inner function estimation. When the objective function is smooth, the proposed method achieves the convergence rate $${\mathcal {O}}\left( k^{-1/2}\right) $$ and sample complexity $${\mathcal {O}}\left( \frac{1}{\epsilon ^2}\right) $$ for finding the ( $$\epsilon $$ )-stationary point. When the objective function is strongly convex, the convergence rate is improved to $${\mathcal {O}}\left( k^{-1}\right) $$ . Moreover, the asymptotic normality of Polyak-Ruppert averaged iterates of the proposed method is also presented. We demonstrate the empirical performance of the proposed method on model-agnostic meta-learning problem and logistic regression problem.},
  archive      = {J_COAP},
  author       = {Zhao, Shengchao and Liu, Yongchao},
  doi          = {10.1007/s10589-023-00512-0},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {249-288},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributed stochastic compositional optimization problems over directed networks},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizations of the proximal method of multipliers in
convex optimization. <em>COAP</em>, <em>87</em>(1), 219–247. (<a
href="https://doi.org/10.1007/s10589-023-00519-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proximal method of multipliers, originally introduced as a way of solving convex programming problems with inequality constraints, is a proximally stabilized alternative to the augmented Lagrangian method that is sometimes called the proximal augmented Lagrangian method. It has gained attention as a vehicle for deriving decomposition algorithms for wider formulations of problems in convex optimization than just convex programming. Here those themes are developed further. The basic algorithm is articulated in several seemingly different formats that are equivalent under exact computations, but diverge when minimization steps are executed only approximately. Stopping criteria are demonstrated to maintain convergence to a particular solution despite such approximations. Q-linear convergence is obtained from a metric regularity property of the Lagrangian mapping at the solution that acts as a mildly enhanced condition for local optimality on top of convexity and is generically available, in a sense. Moreover, all this is brought about with the proximal terms allowed to vary in their underlying metric from one iteration to the next. That generalization enables the results to be translated to the theory of the progressive decoupling algorithm, significantly adding to its versatility and providing linear convergence guarantees in its broad applicability to techniques for problem decomposition.},
  archive      = {J_COAP},
  author       = {Rockafellar, R. Tyrrell},
  doi          = {10.1007/s10589-023-00519-7},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {219-247},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Generalizations of the proximal method of multipliers in convex optimization},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From halpern’s fixed-point iterations to nesterov’s
accelerated interpretations for root-finding problems. <em>COAP</em>,
<em>87</em>(1), 181–218. (<a
href="https://doi.org/10.1007/s10589-023-00518-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive an equivalent form of Halpern’s fixed-point iteration scheme for solving a co-coercive equation (also called a root-finding problem), which can be viewed as a Nesterov’s accelerated interpretation. We show that one method is equivalent to another via a simple transformation, leading to a straightforward convergence proof for Nesterov’s accelerated scheme. Alternatively, we directly establish convergence rates of Nesterov’s accelerated variant, and as a consequence, we obtain a new convergence rate of Halpern’s fixed-point iteration. Next, we apply our results to different methods to solve monotone inclusions, where our convergence guarantees are applied. Since the gradient/forward scheme requires the co-coerciveness of the underlying operator, we derive new Nesterov’s accelerated variants for both recent extra-anchored gradient and past-extra anchored gradient methods in the literature. These variants alleviate the co-coerciveness condition by only assuming the monotonicity and Lipschitz continuity of the underlying operator. Interestingly, our new Nesterov’s accelerated interpretation of the past-extra anchored gradient method involves two past-iterate correction terms. This formulation is expected to guide us developing new Nesterov’s accelerated methods for minimax problems and their continuous views without co-coericiveness. We test our theoretical results on two numerical examples, where the actual convergence rates match well the theoretical ones up to a constant factor.},
  archive      = {J_COAP},
  author       = {Tran-Dinh, Quoc},
  doi          = {10.1007/s10589-023-00518-8},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {181-218},
  shortjournal = {Comput. Optim. Appl.},
  title        = {From halpern’s fixed-point iterations to nesterov’s accelerated interpretations for root-finding problems},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A trust-region approach for computing pareto fronts in
multiobjective optimization. <em>COAP</em>, <em>87</em>(1), 149–179. (<a
href="https://doi.org/10.1007/s10589-023-00510-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiobjective optimization is a challenging scientific area, where the conflicting nature of the different objectives to be optimized changes the concept of problem solution, which is no longer a single point but a set of points, namely the Pareto front. In a posteriori preferences approach, when the decision maker is unable to rank objectives before the optimization, it is important to develop algorithms that generate approximations to the complete Pareto front of a multiobjective optimization problem, making clear the trade-offs between the different objectives. In this work, an algorithm based on a trust-region approach is proposed to approximate the set of Pareto critical points of a multiobjective optimization problem. Derivatives are assumed to be known, allowing the computation of Taylor models for the different objective function components, which will be minimized in two main steps: the extreme point step and the scalarization step. The goal of the extreme point step is to expand the approximation to the Pareto front, by moving towards the extreme points of it, corresponding to the individual minimization of each objective function component. The scalarization step attempts to reduce the gaps on the Pareto front, by solving adequate scalarization problems. The convergence of the method is analyzed and numerical experiments are reported, indicating the relevance of each feature included in the algorithmic structure and its competitiveness, by comparison against a state-of-art multiobjective optimization algorithm.},
  archive      = {J_COAP},
  author       = {Mohammadi, A. and Custódio, A. L.},
  doi          = {10.1007/s10589-023-00510-2},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {149-179},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A trust-region approach for computing pareto fronts in multiobjective optimization},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Stochastic inexact augmented lagrangian method for
nonconvex expectation constrained optimization. <em>COAP</em>,
<em>87</em>(1), 117–147. (<a
href="https://doi.org/10.1007/s10589-023-00521-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems not only have complicated nonconvex functional constraints but also use a large number of data points. This motivates the design of efficient stochastic methods on finite-sum or expectation constrained problems. In this paper, we design and analyze stochastic inexact augmented Lagrangian methods (Stoc-iALM) to solve problems involving a nonconvex composite (i.e. smooth + nonsmooth) objective and nonconvex smooth functional constraints. We adopt the standard iALM framework and design a subroutine by using the momentum-based variance-reduced proximal stochastic gradient method (PStorm) and a postprocessing step. Under certain regularity conditions (assumed also in existing works), to reach an $$\varepsilon $$ -KKT point in expectation, we establish an oracle complexity result of $$O(\varepsilon ^{-5})$$ , which is better than the best-known $$O(\varepsilon ^{-6})$$ result. Numerical experiments on the fairness constrained problem and the Neyman–Pearson classification problem with real data demonstrate that our proposed method outperforms an existing method with the previously best-known complexity result.},
  archive      = {J_COAP},
  author       = {Li, Zichong and Chen, Pin-Yu and Liu, Sijia and Lu, Songtao and Xu, Yangyang},
  doi          = {10.1007/s10589-023-00521-z},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {117-147},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic inexact augmented lagrangian method for nonconvex expectation constrained optimization},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A successive centralized circumcentered-reflection method
for the convex feasibility problem. <em>COAP</em>, <em>87</em>(1),
83–116. (<a href="https://doi.org/10.1007/s10589-023-00516-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a successive centralization process for the circumcentered-reflection method with several control sequences for solving the convex feasibility problem in Euclidean space. Assuming that a standard error bound holds, we prove the linear convergence of the method with the most violated constraint control sequence. Moreover, under additional smoothness assumptions on the target sets, we establish the superlinear convergence. Numerical experiments confirm the efficiency of our method.},
  archive      = {J_COAP},
  author       = {Behling, Roger and Bello-Cruz, Yunier and Iusem, Alfredo and Liu, Di and Santos, Luiz-Rafael},
  doi          = {10.1007/s10589-023-00516-w},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {83-116},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A successive centralized circumcentered-reflection method for the convex feasibility problem},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local convergence analysis of augmented lagrangian method
for nonlinear semidefinite programming. <em>COAP</em>, <em>87</em>(1),
39–81. (<a href="https://doi.org/10.1007/s10589-023-00520-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The augmented Lagrangian method (ALM) has gained tremendous popularity for its elegant theory and impressive numerical performance since it was proposed by Hestenes and Powell in 1969. It has been widely used in numerous efficient solvers to improve numerical performance to solve many problems. In this paper, without requiring the uniqueness of multipliers, the local (asymptotic Q-superlinear) Q-linear convergence rate of the primal-dual sequences generated by ALM for the nonlinear semidefinite programming is established by assuming the second-order sufficient condition and the semi-isolated calmness of the Karush–Kuhn–Tucker solution under some mild conditions.},
  archive      = {J_COAP},
  author       = {Wang, Shiwei and Ding, Chao},
  doi          = {10.1007/s10589-023-00520-0},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {39-81},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Local convergence analysis of augmented lagrangian method for nonlinear semidefinite programming},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexact proximal newton methods in hilbert spaces.
<em>COAP</em>, <em>87</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10589-023-00515-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider proximal Newton methods with an inexact computation of update steps. To this end, we introduce two inexactness criteria which characterize sufficient accuracy of these update step and with the aid of these investigate global convergence and local acceleration of our method. The inexactness criteria are designed to be adequate for the Hilbert space framework we find ourselves in while traditional inexactness criteria from smooth Newton or finite dimensional proximal Newton methods appear to be inefficient in this scenario. The performance of the method and its gain in effectiveness in contrast to the exact case are showcased considering a simple model problem in function space.},
  archive      = {J_COAP},
  author       = {Pötzl, Bastian and Schiela, Anton and Jaap, Patrick},
  doi          = {10.1007/s10589-023-00515-x},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact proximal newton methods in hilbert spaces},
  volume       = {87},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
