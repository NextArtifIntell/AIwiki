<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir---41">IJMIR - 41</h2>
<ul>
<li><details>
<summary>
(2024b). DBTSF-VSOD: A decision-based two-stage framework for video
salient object detection. <em>IJMIR</em>, <em>13</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00346-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient Object Detection (SOD) plays a pivotal role in digital image processing, focusing on identifying objects in images or videos that naturally draw human attention. In the world of computer vision and image processing, these eye-catching objects are known as salient objects. The process of saliency detection holds significant importance in various computer vision applications, including video compression, automated cropping, and video summarization. Recent advancements in AI technology have witnessed the emergence of numerous deep learning-based end-to-end networks in the video saliency domain. However, these approaches predominantly concentrate on network architecture design. In this study, the author(s) introduce a decision-based two-stage framework for video saliency detection, in which the decision is taken based on frame content analysis. This approach enhances the quality of the saliency map in the overall process. In the proposed model, the process unfolds in two stages. Initially, an initial saliency map is generated. Subsequently, in the second phase, the final saliency output is produced by leveraging both the initial saliency map and an optical flow map. To evaluate the effectiveness of the proposed model extensive assessments have been carried out on widely recognized public datasets namely VOS, DAVSOD, and ViSAL. The experimental findings indicate that the proposed model delivers strong performance, exhibiting competitiveness in comparison to state-of-the-art methods on the metrics S-measure, F-measure, and MAE.},
  archive      = {J_IJMIR},
  author       = {Kumain, Sandeep Chand and Singh, Maheep and Awasthi, Lalit Kumar},
  doi          = {10.1007/s13735-024-00346-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DBTSF-VSOD: A decision-based two-stage framework for video salient object detection},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal emotion recognition using tensor decomposition
fusion and self-supervised multi-tasking. <em>IJMIR</em>,
<em>13</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s13735-024-00347-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological advancements, we can now capture rich dialogue content, tones, textual information, and visual data through tools like microphones, the internet, and cameras. However, relying solely on a single modality for emotion analysis often fails to accurately reflect the true emotional state, as this approach overlooks the dynamic correlations between different modalities. To address this, our study introduces a multimodal emotion recognition method that combines tensor decomposition fusion and self-supervised multi-task learning. This method first employs Tucker decomposition techniques to effectively reduce the model’s parameter count, lowering the risk of overfitting. Subsequently, by building a learning mechanism for both multimodal and unimodal tasks and incorporating the concept of label generation, it more accurately captures the emotional differences between modalities. We conducted extensive experiments and analyses on public datasets like CMU-MOSI and CMU-MOSEI, and the results show that our method significantly outperforms existing methods in terms of performance. The related code is open-sourced at https://github.com/ZhuJw31/MMER-TD .},
  archive      = {J_IJMIR},
  author       = {Wang, Rui and Zhu, Jiawei and Wang, Shoujin and Wang, Tao and Huang, Jingze and Zhu, Xianxun},
  doi          = {10.1007/s13735-024-00347-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-modal emotion recognition using tensor decomposition fusion and self-supervised multi-tasking},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in machine learning techniques for threat item
detection in x-ray images: A comprehensive survey. <em>IJMIR</em>,
<em>13</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s13735-024-00348-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray security screening is crucial in detecting prohibited objects and explosive materials within passenger luggage, thus ensuring the safety of airports, railway stations, and aircraft. Modern X-ray imaging systems and advanced object detection technologies enhance the overall efficiency of security screening processes by enabling the detection of dangerous items through intelligent algorithms. Indeed, recent developments in computer vision and machine learning techniques have revolutionized the field of X-ray image-based analysis, making it significantly easier to process and interpret X-ray images. This comprehensive literature survey explores the methodologies, advancements, and challenges associated with object detection by applying deep learning techniques, particularly those based on convolutional neural networks and transformers. In this paper, a taxonomy-based approach has been adopted to comprehensively elucidate the existing landscape of X-ray imaging-based algorithms for security applications and the recent strides made in computer vision technologies. Our study looks at supervised and unsupervised learning methods for CNN and transformer-based architectures, mainly focusing on tasks like image classification, object detection, instance segmentation, and anomaly detection. Furthermore, we establish a performance benchmark and delineate the evaluation criteria applied. This endeavor also entails meticulously exploring well-established X-ray datasets commonly employed to train and evaluate X-ray security imaging algorithms. This study also analyzes the available deep learning techniques, along with the problem statement, benchmark datasets, corresponding references, and their performance metric details in terms of the best-reported scores. Drawing insights from current deep learning trends and anticipating future advancements, this paper culminates in discussing the contemporary state of X-ray security imagery.},
  archive      = {J_IJMIR},
  author       = {Singh, Archana and Dhiraj},
  doi          = {10.1007/s13735-024-00348-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Advancements in machine learning techniques for threat item detection in X-ray images: A comprehensive survey},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent trends in recommender systems: A survey.
<em>IJMIR</em>, <em>13</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s13735-024-00349-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era where the number of choices is overwhelming on the internet, it is crucial to filter, prioritize and deliver relevant information to a user. A recommender system addresses this issue by recommending items that users might like from many available items. Nowadays, the prevalence of providing personalized content to users through a website has increased profoundly. The majority of such websites use recommendation models to reduce a user’s searching time. Many new recommendation models are being proposed to address the changing business requirements of eCommerce organizations. Recommender systems can be broadly classified into three categories, i.e., clustering-based, matrix-factorization-based, and deep learning-based models. Many scopes and use cases are available where recommendation models play a vital role. The advent of graph representation learning and LLMs hinders recommendation models from being more effective in promptly providing relevant suggestions. This survey comprehensively discusses various deep learning-based recommendation models available for different domains. We also discuss the pros and cons of popular recommendation models. We also discuss various open issues of recommender systems and outline a few future directions. This study also provides insight to explore novel and helpful research problems related to recommendation systems.},
  archive      = {J_IJMIR},
  author       = {Kumar, Chintoo and Chowdary, C. Ravindranath and Meena, Ashok Kumar},
  doi          = {10.1007/s13735-024-00349-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Recent trends in recommender systems: A survey},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attacks and defenses for large language models
(LLMs): Methods, frameworks &amp; challenges. <em>IJMIR</em>,
<em>13</em>(3), 1–28. (<a
href="https://doi.org/10.1007/s13735-024-00334-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have exhibited remarkable efficacy and proficiency in a wide array of NLP endeavors. Nevertheless, concerns are growing rapidly regarding the security and vulnerabilities linked to the adoption and incorporation of LLM. In this work, a systematic study focused on the most up-to-date attack and defense frameworks for the LLM is presented. This work delves into the intricate landscape of adversarial attacks on language models (LMs) and presents a thorough problem formulation. It covers a spectrum of attack enhancement techniques and also addresses methods for strengthening LLMs. This study also highlights challenges in the field, such as the assessment of offensive or defensive performance, defense and attack transferability, high computational requirements, embedding space size, and perturbation. This survey encompasses more than 200 recent papers concerning adversarial attacks and techniques. By synthesizing a broad array of attack techniques, defenses, and challenges, this paper contributes to the ongoing discourse on securing LM against adversarial threats.},
  archive      = {J_IJMIR},
  author       = {Kumar, Pranjal},
  doi          = {10.1007/s13735-024-00334-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Adversarial attacks and defenses for large language models (LLMs): Methods, frameworks &amp; challenges},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSECA: Local semantic enhancement and cross aggregation for
video-text retrieval. <em>IJMIR</em>, <em>13</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00335-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently video retrieval based on the pre-training models (e.g., CLIP) has achieved outstanding success. To further improve the search performance, most existing methods usually utilize the multi-grained contrastive fine tuning scheme. For example, frame features and word features are taken as fine-grained representations, aggregate features for frame features and [CLS] token for textual side are used as global representations. However, the above scheme still remains challenging. There are redundant and noise information in the raw output features of pre-training encoders, leading to suboptimal retrieval performance. Besides, a video usually correlates several text descriptions, while video embedding is fixed in previous works, which may also reduce the search performance. To conquer these problems, we propose a novel video-text retrieval model, named Local Semantic Enhancement and Cross Aggregation (LSECA). To be specific, we design a local semantic enhancement scheme, which utilizes global feature for video and keyword information for text to augment fine-grained semantic representations. Moreover, the cross aggregation module is proposed to enhance the interaction between video and text modalities. In this way, the local semantic enhancement scheme can increase the related representation of modalities and the developed cross aggregation module can make the representations of texts and videos more uniform. Extensive experiments on three popular text-video retrieval benchmark datasets demonstrate that our LSECA outperforms several state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Wang, Zhiwen and Zhang, Donglin and Hu, Zhikai},
  doi          = {10.1007/s13735-024-00335-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {LSECA: Local semantic enhancement and cross aggregation for video-text retrieval},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSPformer: Discovering semantic parts with token growth and
clustering for zero-shot learning. <em>IJMIR</em>, <em>13</em>(3), 1–13.
(<a href="https://doi.org/10.1007/s13735-024-00336-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved success in many computer vision tasks, but their potential in Zero-Shot Learning (ZSL) has yet to be fully explored. In this paper, a Transformer architecture is developed, termed DSPformer, which can discover semantic parts by token growth and clustering. This is achieved through two proposed methods: Adaptive Token Growth and Semantic Part Clustering. Firstly, it is observed that the background may distract models, causing the model to rely on irrelevant regions to make decisions. To alleviate this issue, the ATG is proposed to locate discriminative foreground regions and remove meaningless and even noisy backgrounds. Secondly, semantically similar parts may be distributed into different tokens. To address this problem, the SPC is proposed to group semantically consistent parts by token clustering. Extensive experiments on several challenging datasets demonstrate the effectiveness of the proposed DSPformer.},
  archive      = {J_IJMIR},
  author       = {Zhao, Peng and Wang, Qiangchang and Yin, Yilong},
  doi          = {10.1007/s13735-024-00336-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DSPformer: Discovering semantic parts with token growth and clustering for zero-shot learning},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity-based face image retrieval using sparsely
embedded deep features and binary code learning. <em>IJMIR</em>,
<em>13</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s13735-024-00337-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human face retrieval has long been established as one of the most interesting research topics in computer vision. With the recent development of deep learning, many researchers have addressed this problem by building deep hashing models to learn binary code from face images, while performing face retrieval as a classification task. Nevertheless, the performance is still unsatisfactory since these models are incapable of handling inter-class variation between multiple persons, as we need to make a class label for each identity. In this backdrop, we propose in this paper an effective deep learning-based framework for face image retrieval. The key to our framework is mainly based on the matching of face pairs, where a two-stream network, named $$\chi Net+\chi Match$$ , is designed to learn similarities in terms of person identity. Such similarities are investigated by embedding both deep local representation via face components, and deep global face representation via the whole face image. Since the similarities captured over face components are supposed to diversify due to variation in pose, expression and occlusion, we also introduce a Sparse Score Fusion layer that learns automatically the weight of each component according to its contribution to face matching. To allow fast retrieval, we farther propose a method that generates binary codes corresponding to the groups of similar faces through the hierarchical k-means, where the path down binary tree is exploited as a binary code for indexing. The final retrieval is then conducted within a privileged subset of images in the database. Our experiments on different challenging datasets show that our approach obtains outstanding results while outperforming most existing methods.},
  archive      = {J_IJMIR},
  author       = {Elboushaki, Abdessamad and Hannane, Rachida and Afdel, Karim},
  doi          = {10.1007/s13735-024-00337-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Similarity-based face image retrieval using sparsely embedded deep features and binary code learning},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human action recognition using an optical flow-gated
recurrent neural network. <em>IJMIR</em>, <em>13</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s13735-024-00338-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing various human actions in videos is considered a highly complicated problem, which has many potential applications in solving real-world problems such as human behavior analysis, artificial intelligence, video surveillance, and smart manufacturing. Therefore, designing novel approaches for automatically understanding video data is highly demanded. Towards this goal, different algorithms have been investigated, concentrating on extracting the spatial information and the temporal dependencies. However, motion feature extraction is engineered and isolated from the learning operations. In this paper, to comprehend motion features along with the spatial information and the time dependencies, an innovative attempt is made by designing a new Gated Recurrent Unit (GRU) network. Moreover, a novel deep neural network is presented using the proposed GRU to recognize human actions. Evaluations on popular datasets (YouTube2011, UCF50, UCF101, and HMDB51) not only convey the superiority of the proposed GRU in action recognition using an end-to-end learning model but also emphasize on the generalizability of the proposed method. Additionally, to show the applicability and functionality of the proposed model in solving real-world problems, an engine block assembly dataset was collected and the performance of the proposed method was measured on this dataset. Finally, the robustness of the proposed method against various kinds of noise was tested. The obtained results demonstrate the high performance of the proposed method and its robustness against noise.},
  archive      = {J_IJMIR},
  author       = {Giveki, Davar},
  doi          = {10.1007/s13735-024-00338-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Human action recognition using an optical flow-gated recurrent neural network},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A order-based content-based information retrieval system
proposal applied in 3D meshes. <em>IJMIR</em>, <em>13</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00339-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer graphics, Content-Based Informaion Retrieval (CBIR) is a database system designed to take an object as input and return a list of similar objects. Originally intended for images, CBIR systems now extend to various types of data, including sounds and three-dimensional models represented as geometrical meshes, broadening their applicability beyond images. In the field of information retrieval, it’s customary to interpret the “I” in CBIR as “Information.” Throughout this text, we adopt this interpretation. To evaluate the similarity between 3D meshes, several techniques involve transforming meshes into feature vectors and measuring the distance between these vectors. In our research, we propose leveraging an algorithm rooted in compressive sensing theory to extract features from 3D meshes. Additionally, we introduce a prototype of a 3D meshes CBIR system that utilizes an order relation, rather than a distance function, to assess the similarity between objects. We introduce an order in $${\mathbb {R}}^n$$ called the Extended Lexicographical Order (ELO), designed to incorporate all information present in the vectors being compared. Our comparative analysis includes traditional distance functions as well as classical $${\mathbb {R}}^n$$ order relations such as lexicographical and revlex. Furthermore, we employ two types of descriptors: a spectral descriptor based on compressive sensing theory, which builds upon previous work from our research group, and a harmonic spherical-based descriptor, which has already been established in the literature as a successful extractor in the context of medical models. Across all experiments, our prototype consistently outperforms traditional techniques, showcasing its efficacy in CBIR applications.},
  archive      = {J_IJMIR},
  author       = {Amorim, Thiago Kobashigawa and Biscaro, Helton Hideraldo},
  doi          = {10.1007/s13735-024-00339-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A order-based content-based information retrieval system proposal applied in 3D meshes},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mual: Enhancing multimodal sentiment analysis with
cross-modal attention and difference loss. <em>IJMIR</em>,
<em>13</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s13735-024-00340-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis has received much attention in recent years, especially in the context of the abundant multimodal data generated by social platforms. Models like CLIP, BLIP, VisualBERT are all excellent but often come with a large number of parameters and require inputs in the form of image-text pairs, which will constrain their flexibility. The integration of superior unimodal sentiment analysis models allows simultaneous processing of multimodal data, enabling arbitrary addition or removal of modalities for generalized multimodal sentiment analysis. The integrated model that preprocesses and fuses each modality can sometimes significantly improve the accuracy of sentiment analysis. Therefore, this study proposes a novel multimodal sentiment analysis approach, MuAL, based on cross-modal attention and difference loss. Cross-modal attention is used to integrate information from two modalities, and difference loss is utilized to minimize the gap between image and text information, enhancing the model’s robustness. Additionally, MuAL uses cls token to capture overall sentiment information, further eliminating noise within modalities and reducing computational expenses. The study evaluates MuAL on five real-world datasets, demonstrating superior performance over baseline methods with fewer parameters. Furthermore, considering MuAL utilizes pre-trained models as encoders, the research assesses its capability in transfer learning. Results reveal that even after freezing the parameters of the pre-trained model, MuAL outperforms the baseline on all five datasets, confirming its superior performance.},
  archive      = {J_IJMIR},
  author       = {Deng, Yang and Li, Yonghong and Xian, Sidong and Li, Laquan and Qiu, Haiyang},
  doi          = {10.1007/s13735-024-00340-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Mual: Enhancing multimodal sentiment analysis with cross-modal attention and difference loss},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D skeleton-based human motion prediction using
spatial–temporal graph convolutional network. <em>IJMIR</em>,
<em>13</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s13735-024-00341-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human motion prediction; predicting future human poses in the basis of historically observed motion sequences, is a core task in computer vision. Thus far, it has been successfully applied to both autonomous driving and human–robot interaction. Previous research work has usually employed Recurrent Neural Networks (RNNs)-based models to predict future human poses. However, as previous works have amply demonstrated, RNN-based prediction models suffer from unrealistic and discontinuous problems in human motion prediction due to the accumulation of prediction errors. To address this, we propose a feed-forward, 3D skeleton-based model for human motion prediction. This model, the Spatial–Temporal Graph Convolutional Network (ST-GCN) model, automatically learns the spatial and temporal patterns of human motion from input sequences. This model overcomes the limitations of previous research approaches. Specifically, our ST-GCN model is based on an encoder-decoder architecture. The encoder consists of 5 ST-GCN modules, with each ST-GCN module consisting of a spatial GCN layer and a 2D convolution-based TCN layer, which facilitate the encoding of the spatio-temporal dynamics of human motion. Subsequently, the decoder, consisting of 5 TCN layers, exploits the encoded spatio-temporal representation of human motion to predict future human pose. We leveraged the ST-GCN model to perform extensive experiments on various large-scale human activity 3D pose datasets (Human3.6 M, AMASS, 3DPW) while adopting MPJPE (Mean Per Joint Position Error) as the evaluation metric. The experimental results demonstrate that our ST-GCN model outperforms the baseline models in both short-term (&lt; 400 ms) and long-term (&gt; 400 ms) predictions, thus yielding the best prediction results.},
  archive      = {J_IJMIR},
  author       = {Huang, Jianying and Kang, Hoon},
  doi          = {10.1007/s13735-024-00341-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {3D skeleton-based human motion prediction using spatial–temporal graph convolutional network},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stratified graph indexing for efficient search in deep
descriptor databases. <em>IJMIR</em>, <em>13</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00342-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for unseen objects in extensive visual archives is challenging, demanding efficient indexing methods that can support meaningful similarity retrievals. This research paper presents the Stratified Graph (SG) approach for indexing similar deep descriptors by sorting them into distance-sensitive layers. The indexing algorithm incrementally constructs a bi-directional m-nearest neighbor graph within each layer, with additional 1-nearest neighbor links from outer layers, providing a distant scaling property in the graph structure. The search process starts from the innermost layer, and the same layer neighbors enhance Average Recall (AR), while the distant scaling property enhances search speed, maintaining logarithmic complexity scaling. We compare and contrast SG with six state-of-the-art retrieval methods in four deep-descriptor and two classical-descriptor databases, and we show that the SG indexing and search has smaller memory usage (up to four times) and the Mean Average Precision and AR improve up to 8% over state-of-art for all six datasets at five retrieval depths.},
  archive      = {J_IJMIR},
  author       = {Rahman, M. M. Mahabubur and Tešić, Jelena},
  doi          = {10.1007/s13735-024-00342-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Stratified graph indexing for efficient search in deep descriptor databases},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging language to visuals: Towards natural language
query-to-chart image retrieval. <em>IJMIR</em>, <em>13</em>(3), 1–11.
(<a href="https://doi.org/10.1007/s13735-024-00343-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a natural language query, mining a relevant chart image, i.e., the one that contains the answer to the query, is an overlooked problem in the literature. Our study explores this novel problem. Consider an example of retrieving relevant chart images for a query: Which Indian city has the highest annual rainfall over the past decade?. Retrieving relevant chart images for such natural language queries necessitates a deep semantic understanding of chart images. Towards addressing this problem, in this work, we make two key contributions: (a) We present a dataset, namely WebCIRD (or Web Chart Image Retrieval) for studying this problem, and (b) propose a solution viz. ChartSemBERT that offers a deeper semantic understanding of chart images for effective natural language-to-chart image retrieval. Our proposed approach yields remarkable performance improvements compared to the existing baselines, achieving R@10 as 86.9%.},
  archive      = {J_IJMIR},
  author       = {Verma, Neelu and De, Anik and Mishra, Anand},
  doi          = {10.1007/s13735-024-00343-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Bridging language to visuals: Towards natural language query-to-chart image retrieval},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal music datasets? Challenges and future goals in
music processing. <em>IJMIR</em>, <em>13</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s13735-024-00344-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term “multimodal music dataset” is often used to describe music-related datasets that represent music as a multimedia art form and multimodal experience. However, the term “multimodality” is often used differently in disciplines such as musicology, music psychology, and music technology. This paper proposes a definition of multimodality that works across different music disciplines. Many challenges are related to constructing, evaluating, and using multimodal music datasets. We provide a task-based categorization of multimodal datasets and suggest guidelines for their development. Diverse data pre-processing methods are illuminated, highlighting their contributions to transparent and reproducible music analysis. Additionally, evaluation metrics, methods, and benchmarks tailored for multimodal music processing tasks are scrutinized, empowering researchers to make informed decisions and facilitating cross-study comparisons.},
  archive      = {J_IJMIR},
  author       = {Christodoulou, Anna-Maria and Lartillot, Olivier and Jensenius, Alexander Refsum},
  doi          = {10.1007/s13735-024-00344-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multimodal music datasets? challenges and future goals in music processing},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing deep learning image classification using data
augmentation and genetic algorithm-based optimization. <em>IJMIR</em>,
<em>13</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s13735-024-00345-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, data augmentation stands out as a potent strategy to overcome the constraints imposed by limited training data, as well as unbalanced and low-quality data, ultimately enhancing model accuracy. This work introduces a novel data augmentation technique rooted in optimization, seamlessly integrating a genetic algorithm. It unlocks a broad spectrum of possibilities for generating high-quality images and creating varied and distinctive images, thereby enriching the dataset. The proposed approach allows for identifying and selecting the most significant pixels within a given image, preserving vital information crucial for efficiently training deep learning models. Experimental evaluations were carried out on two datasets, namely Kaggle Cats versus Dogs and the Chest X-ray dataset. To underscore our proposed data augmentation technique’s robustness and adaptability, we trained on different models: VGG16, VGG19, InceptionV3, EfficientNet-B0, and Vision Transformer. Our method consistently achieved peak accuracy while training these models on the specified datasets. Augmenting the Cat versus Dog dataset resulted in accuracies of 93.47%, 92.38%, 98.03%, 98.40%, and 78.61% for VGG16, VGG19, InceptionV3, EfficientNet-B0, and Vision Transformer, respectively. For the Chest X-ray dataset, the accuracies were 79.81%, 82.05%, 90.71%, 93.91% and 80.89% for VGG16, VGG19, InceptionV3, EfficientNet-B0 and Vision Transformer, respectively. For both datasets and each model, the proposed method consistently outperformed the accuracy achieved when considering the original dataset and randomly augmented datasets, underscoring the significant impact of our research.},
  archive      = {J_IJMIR},
  author       = {Boudouh, Nouara and Mokhtari, Bilal and Foufou, Sebti},
  doi          = {10.1007/s13735-024-00345-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Enhancing deep learning image classification using data augmentation and genetic algorithm-based optimization},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAF-net: Dense attention feature pyramid network for
multiscale object detection. <em>IJMIR</em>, <em>13</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s13735-024-00323-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object detection has become one of the most prominent components in computer vision. State-of-the-art object detectors now employ convolutional neural networks (CNNs) techniques alongside other deep neural network techniques to improve detection performance and accuracy. Most of the recent object detectors employ feature pyramid network (FPN) and their variants while others use combinations of attention mechanisms to achieve better performance. The open question is object detectors inconsistency between the lower layer features, their resolution receptive field and semantic information with the upper layers features in detecting objects. Although some researchers have attempted to address this issue, we exploit ideas surrounding the field and proposed a more prominent architecture called dense attention feature pyramid network (DAF-Net) for multiscale object detection. DAF-Net consists of two attention models, the spatial attention model and channel attention model. Different from other attention models, we proposed lightweight attention models which are fully data-driven then implemented a dense connected attention FPN to reduce the model’s complexity and resolve the learning of redundant feature maps. First, we developed the two attention models then used only the spatial attention model in the backbone of our network, and finally used both attention models to filter and maintain a steady flow of semantic information from lower layers to improve the model’s accuracy and efficiency. Experimental results on underwater images from the National Natural Science Foundation of China (NSFC) (Underwater Image Dataset, National Natural Science Foundation of China (NSFC). Online, retrieved from http://www.cnurpc.org/index.html ), MS COCO dataset, and PASCAL VOC dataset indicate higher accuracy and better detection results using the proposed model compared to the benchmark model YOLOX-Darknet53 (Ge in Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430). Our model achieved 70.2mAP, 48.9 mAP, and 83.9 mAP on (NSFC), MS COCO, and PASCAL VOC datasets, respectively, compared with benchmark model 68.9mAP on (NSFC), 47.7mAP on MS COCO, and 82.4mAP on PASCAL VOC.},
  archive      = {J_IJMIR},
  author       = {Achinek, Divine Njengwie and Shehu, Ibrahim Shehi and Athuman, Athuman Mohamed and Fu, Xianping},
  doi          = {10.1007/s13735-024-00323-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DAF-net: Dense attention feature pyramid network for multiscale object detection},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive spatial–temporal transfer model for unsupervised
person re-identification. <em>IJMIR</em>, <em>13</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s13735-024-00324-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, a more widespread area of computer vision research has been person re-identification (P-Reid). This technology is applied in fields such as pedestrian tracking, security, and video surveillance. Currently, person re-identification performs well when supervised with labeled data, but accuracy frequently suffers when learning unsupervised on unlabeled samples. Therefore, improving unlabeled samples model is a challenging endeavor. In order to solve this problem, we propose a progressive spatial–temporal transfer model (PSTT), which consists of three stages, including incremental tuning, spatial–temporal fusion and target domain learning. In the first stage, a high-performance multi-scale network that can initially cluster samples is obtained through triplet loss function. In the next stage, to mine spatial–temporal and visual semantic information, we introduce a fusion model that fuses the visual information extracted from the labeled dataset and the unlabeled dataset using a trained network with its spatial–temporal information. In the final stage, with the assistance of fusion model, we employ a strategy that extends learning from labeled to unlabeled samples. During the training, the fusion model is used to select labeled and unlabeled samples, and multiple meta loss function is used for transfer learning. During the testing, the fusion model is employed to enhance the accuracy of network. In the experiment, we evaluate our method on five standard P-Reid benchmarks: Market1501, DukeMTMC-ReID, CUHK03, MSMT17 and Occluded-DukeMTMC. Extensive experiments show that our proposed PSTT achieves state-of-the-art performance, exceeding the previous method by a certain margin. The source code is available at https://github.com/LiZX12/PSTT.},
  archive      = {J_IJMIR},
  author       = {Zhou, Shuren and Li, Zhixiong and Liu, Jie and Zhou, Jiarui and Zhang, Jianming},
  doi          = {10.1007/s13735-024-00324-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Progressive spatial–temporal transfer model for unsupervised person re-identification},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive multimodal video search: An extended
post-evaluation for the VBS 2022 competition. <em>IJMIR</em>,
<em>13</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00325-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CLIP-based text-to-image retrieval has proven to be very effective at the interactive video retrieval competition Video Browser Showdown 2022, where all three top-scoring teams had implemented a variant of a CLIP model in their system. Since the performance of these three systems was quite close, this post-evaluation was designed to get better insights on the differences of the systems and compare the CLIP-based text-query retrieval engines by introducing slight modifications to the original competition settings. An extended analysis of the overall results and the retrieval performance of all systems’ functionalities shows that a strong text retrieval model certainly helps, but has to be coupled with extensive browsing capabilities and other query-modalities to consistently solve known-item-search tasks in a large-scale video database.},
  archive      = {J_IJMIR},
  author       = {Schall, Konstantin and Bailer, Werner and Barthel, Kai-Uwe and Carrara, Fabio and Lokoč, Jakub and Peška, Ladislav and Schoeffmann, Klaus and Vadicamo, Lucia and Vairo, Claudio},
  doi          = {10.1007/s13735-024-00325-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Interactive multimodal video search: An extended post-evaluation for the VBS 2022 competition},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised graph reasoning distillation hashing for
multimodal hamming space search with vision-language model.
<em>IJMIR</em>, <em>13</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s13735-024-00326-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal hash technology maps high-dimensional multimodal data into hash codes, which greatly reduces the cost of data storage and improves query speed through the Hamming similarity calculation. However, existing unsupervised methods still have two key obstacles: (1) With the evolution of large multimodal models, how to efficiently distill the multimodal matching relationship of large models to train a powerful student model? (2) Existing methods do not consider other adjacencies between multimodal instances, resulting in limited similarity representation. To address these obstacles, called Unsupervised Graph Reasoning Distillation Hashing (UGRDH) is proposed. The UGRDH approach uses the CLIP as the teacher model, thus extracting fine-grained multimodal features and relations for teacher–student distillation. Specifically, the multimodal features of the teacher are used to construct a similarity–complementary relation graph matrix, and the proposed graph convolution auxiliary network performs feature aggregation guided by the relation graph matrix to generate a more discriminative hash code. In addition, a cross-attention module was designed to reason potential instance relations to enable effective teacher–student distilled learning. Finally, UGRDH greatly improves search precision while maintaining lightness. Experimental results show that our method achieves about 1.5%, 3%, and 2.8% performance improvements on MS COCO, NUS-WIDE, and MIRFlickr, respectively.},
  archive      = {J_IJMIR},
  author       = {Sun, Lina and Dong, Yumin},
  doi          = {10.1007/s13735-024-00326-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Unsupervised graph reasoning distillation hashing for multimodal hamming space search with vision-language model},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-knowledge-driven enhanced module for visible-infrared
cross-modal person re-identification. <em>IJMIR</em>, <em>13</em>(2),
1–14. (<a href="https://doi.org/10.1007/s13735-024-00327-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-identification (VI-ReID) is challenging in social security surveillance because the semantic gap between cross-modal data significantly reduces VI-ReID performance. To overcome this challenge, this paper proposes a novel Multi Knowledge-driven Enhancement Module (MKEM) for high-performance VI-ReID. It mainly focuses on explicitly learning appropriate transition modalities and effectively synthesizing them to reduce the burden of models learning vastly different cross-modal knowledge. The MKEM consists of a Visible Knowledge-driven Enhancement Module (VKEM) and an Infrared Knowledge-driven Enhancement Module (IKEM), which generate model knowledge-accumulating transition modalities for the visible and infrared modalities, respectively. To effectively leverage the transition modalities, the model needs to learn the original data distribution while accumulating knowledge of the transition modes; thus, a Diversity Loss is designed to guide the representation of the generated transition modalities to be diverse, which can facilitate the model’s knowledge accumulation. To prevent redundant knowledge accumulation, a Consistency Loss is proposed to maintain the semantic similarity between the original and modeled transitional modalities. Furthermore, we implemented a Bias Adjustment Strategy (BAS) to effectively adjust the gap between the head and tail categories. We evaluated our proposed MKEM on two VI-ReID benchmark datasets, SYSU-MM01 and RegDB, and the experimental results demonstrate that our method outperforms existing methods significantly. The source code of our proposed MKEM is available at https://github.com/SWU-CS-MediaLab/MKEM .},
  archive      = {J_IJMIR},
  author       = {Shan, Shihao and Sun, Peixin and Xiao, Guoqiang and Wu, Song},
  doi          = {10.1007/s13735-024-00327-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-knowledge-driven enhanced module for visible-infrared cross-modal person re-identification},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-specific image captioning: A comprehensive review.
<em>IJMIR</em>, <em>13</em>(2), 1–27. (<a
href="https://doi.org/10.1007/s13735-024-00328-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An image caption is a sentence summarizing the semantic details of an image. It is a blended application of computer vision and natural language processing. The earlier research addressed this domain using machine learning approaches by modeling image captioning frameworks using hand-engineered feature extraction techniques. With the resurgence of deep-learning approaches, the development of improved and efficient image captioning frameworks is on the rise. Image captioning is witnessing tremendous growth in various domains as medical, remote sensing, security, visual assistance, and multimodal search engines. In this survey, we comprehensively study the image captioning frameworks based on our proposed domain-specific taxonomy. We explore the benchmark datasets and metrics leveraged for training and evaluating image captioning models in various application domains. In addition, we also perform a comparative analysis of the reviewed models. Natural image captioning, medical image captioning, and remote sensing image captioning are currently among the most prominent application domains of image captioning. The efficacy of real-time image captioning is a challenging obstacle limiting its implementation in sensitive areas such as visual aid, remote security, and healthcare. Further challenges include the scarcity of rich domain-specific datasets, training complexity, evaluation difficulty, and a deficiency of cross-domain knowledge transfer techniques. Despite the significant contributions made, there is a need for additional efforts to develop steadfast and influential image captioning models.},
  archive      = {J_IJMIR},
  author       = {Sharma, Himanshu and Padha, Devanand},
  doi          = {10.1007/s13735-024-00328-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Domain-specific image captioning: A comprehensive review},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State of art and emerging trends on group recommender
system: A comprehensive review. <em>IJMIR</em>, <em>13</em>(2), 1–39.
(<a href="https://doi.org/10.1007/s13735-024-00329-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A group recommender system (GRS) generates suggestions for a group of individuals, considering not only each person&#39;s preferences but also factors such as social dynamics and behavior to deliver recommendations that balance personal taste and social factors. In this study, 225 papers have been analyzed from different journals and conference papers, covering a variety of literature works on group recommendation systems. The articles in the literature used for the review were published between 2010 and 2023. This overview of the literature focuses on several methods for creating group recommender systems. This review starts by providing an overview of group recommender systems, including the challenges and essential elements for their development. It then examines the existing literature on collaborative, content-based, and knowledge-based group recommendation techniques. Beyond traditional approaches, this study identifies a notable research gap in the integration of audio, image, and video recommendation systems within the group recommendation paradigm. It then discusses the research gaps found in the existing papers. The review also discusses various aggregation techniques and evaluation metrics used to evaluate these techniques. The review concludes by discussing the limitations and potential future directions for group recommendation research. This review aims to give a thorough understanding of the current status of group recommendations and to pinpoint potential areas for future study.},
  archive      = {J_IJMIR},
  author       = {Singhal, Shilpa and Pal, Kunwar},
  doi          = {10.1007/s13735-024-00329-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {State of art and emerging trends on group recommender system: A comprehensive review},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RDAT: An efficient regularized decoupled adversarial
training mechanism. <em>IJMIR</em>, <em>13</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00330-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have exposed the inherent vulnerabilities of deep neural networks. Although adversarial training has emerged as the leading strategy for adversarial defenses, it is frequently hindered by a challenging balance between maintaining accuracy on unaltered examples and enhancing model robustness. Recent efforts on decoupling network components can effectively reduce the degradation of classification accuracy, but at the cost of an unsatisfactory in robust accuracy, and may suffer from robust overfitting. In this paper, we delve into the underlying causes of this compromise, and introduce a novel framework, the Regularized Decoupled Adversarial Training Mechanism (RDAT) to effectively deal with the trade-off and overfitting. Specifically, RDAT comprises two distinct modules: Regularization module mitigates harmful perturbations by controlling the data distribution distance of examples before and after adversarial attacks. Decoupling training module separates clean and adversarial examples so that they can have special optimization strategies to avoid the suboptimal result in adversarial training. With marginal compromise on the classification accuracy, RDAT achieves remarkably better model robustness with the improvement of robust accuracy by an average of 4.47% on CIFAR-10 and 3.23% on CIFAR-100 when compared to state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Li, Yishan and Guo, Yanming and Wu, Yulun and Xie, Yuxiang and Lao, Mingrui and Yu, Tianyuan and Ruan, Yirun},
  doi          = {10.1007/s13735-024-00330-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {RDAT: An efficient regularized decoupled adversarial training mechanism},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatiotemporal bidirectional network for video salient
object detection using multiscale transfer learning. <em>IJMIR</em>,
<em>13</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00331-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video saliency prediction aims to simulate human visual attention by selecting the most pertinent and important components within a video frame or sequence. When evaluating video saliency, time and space data are essential, particularly in the presence of challenging features such as fast motion, shifting background, and nonrigid deformation. The current video saliency frameworks are highly prone to failure under the specified conditions. Moreover, it is unsuitable to perform video saliency identification by solely relying on image saliency models, disregarding the temporal information in videos. This research proposes a novel Spatiotemporal Bidirectional Network for Video Salient Object Detection using Multiscale Transfer Learning (SBMTL-Net) to solve the issue of detecting important objects in videos. The SBMTL-Net produces significant outcomes for a given sequence of frames by utilizing Multi-scale transfer learning with an encoder and decoder technique to acquire knowledge and spatially and temporally map properties. SBMTL-Net model consists of bidirectional LSTM (Long Short-Term Memory) and CNN (Convolutional Neural Network), where the VGG16 (Video Geometry Group) and VGG19 are utilized for multi-scale feature extraction of the input video frames. The performance of the proposed model has been evaluated on five publically available challenging datasets DAVIS-T, SegTrack-V2, ViSal, VOS-T and DAVSOD-T for the parameters MAE, F-measure and S-measure. The experimental results show the effectiveness of the proposed model as compared with other competitive models.},
  archive      = {J_IJMIR},
  author       = {Sharma, Gaurav and Singh, Maheep},
  doi          = {10.1007/s13735-024-00331-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A spatiotemporal bidirectional network for video salient object detection using multiscale transfer learning},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strengthening attention: Knowledge distillation via
cross-layer feature fusion for image classification. <em>IJMIR</em>,
<em>13</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00332-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved great success in computer vision, especially in image classification tasks. How to improve the generalization ability and compactness of deep neural networks has gradually attracted widespread attention from researchers. Knowledge distillation is an effective technique for model compression. It transfers general knowledge from a sophisticated teacher model to a smaller student model. Recently, some studies refine knowledge from feature maps or adopt complex attention mechanisms to better supervise students imitating teachers. However, their methods focus too much on how to improve students’ accuracy and largely overlook the associated training costs, which violates the original intention of knowledge distillation to compress the model. To achieve a balance between performance and efficiency, in this paper, we introduce a straightforward and effective distillation method to utilize the deepest feature maps to enhance shallow features. Specifically, our method performs processing only on the original feature maps without an extra assisting network. Moreover, we use cross-layer feature fusion to enhance the attention on shallow feature maps. By visualizing the features of different layers, we demonstrate the importance of the fusion operation in our method. Our experimental results on the CIFAR-100, tinyImageNet and miniImageNet datasets show that our approach outperforms previous methods, especially in the balance between performance and training cost. Further ablative studies verify the effectiveness of the design.},
  archive      = {J_IJMIR},
  author       = {Zhai, Zhongyi and Liang, Jie and Cheng, Bo and Zhao, Lingzhong and Qian, Junyan},
  doi          = {10.1007/s13735-024-00332-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Strengthening attention: Knowledge distillation via cross-layer feature fusion for image classification},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relevance equilibrium network for cross-domain few-shot
learning. <em>IJMIR</em>, <em>13</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s13735-024-00333-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning (CD-FSL) aims to develop a robust and generalizable model from a data-abundant source domain and apply it to the data-scarce target domain. An intrinsic challenge in CD-FSL is the domain shift problem, often manifested as a discrepancy in data distributions. This work addresses the domain shift problem from a model learning perspective, characterizing it in two specific aspects: over-sensitivity and excessive invariance. Specifically, we introduce a novel Relevance Equilibrium Network (ReqNet) to enhance the generalizability of few-shot models on target domain tasks. In particular, we design a Style Augmentation (StyleAug) module to diversify low-level visual styles of feature representations, alleviating the model’s over-sensitivity to class- or task-irrelevant changes. Furthermore, to mitigate the excessive invariance to features relevant to the class and task, we devise a Task Context Modeling (TCM) module that strategically employs non-local operations to incorporate comprehensive task-level information. Extensive experiments and ablation studies are conducted on eight datasets to demonstrate the competitive performance of our proposed ReqNet.},
  archive      = {J_IJMIR},
  author       = {Ji, Zhong and Kong, Xiangyu and Wang, Xuan and Liu, Xiyao},
  doi          = {10.1007/s13735-024-00333-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Relevance equilibrium network for cross-domain few-shot learning},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmented inputs for surveillance re-identification.
<em>IJMIR</em>, <em>13</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s13735-023-00309-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (Re-ID) is one of the important applications for Surveillance. However, the performance of Re-ID is dependent on the input quality, which cannot be guaranteed from the surveillance systems. We explored the technique from Gait Re-ID to address viewpoint changes. From our findings, adding horizontally mirrored image into an auxiliary pipeline can achieve a modest performance uplift in our test (0.8% net increase in mean average precision and 0.9% increase in Rank-1 accuracy) in MARS dataset. This extra pipeline can be substituted by Heterogeneous Input Triplet Loss (hiTri) for minimal performance loss. The overall performance of the proposed method outperforms state-of-the-art techniques on well-known datasets. Further investigations on other auxiliary input types are warranted.},
  archive      = {J_IJMIR},
  author       = {Kasantikul, Rangwan and Kusakunniran, Worapan},
  doi          = {10.1007/s13735-023-00309-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Augmented inputs for surveillance re-identification},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image enhancement with bi-directional normalization and
color attention-guided generative adversarial networks. <em>IJMIR</em>,
<em>13</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s13735-023-00310-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image enhancement aims to improve the quality of images from contrast, detail, and color perspectives by adjusting the color of an image to match the distribution of the high-quality domain. Since the images captured by portable devices often suffer from noise and color bias, this paper designed a novel bi-directional normalization and color attention-guided generative adversarial network (BNCAGAN) for unsupervised image enhancement. Specifically, the bi-directional normalization generator is built upon a feature encoder, an auxiliary attention classifier (AAC), a bi-directional normalization residual (BNR) module, and a feature fusion decoder. With the aid of the AAC and BNR modules, the generator can flexibly control the global style, local details, and color transformation constraints from low-quality and high-quality domains. To improve the visual effect, a spatial color correction module is proposed to assist the multi-scale discriminator in focusing on color fidelity. In addition, a mixed loss, including a content retention loss and an identity fidelity loss, can maintain the structural features to fit the high-quality domain distribution. Extensive experiments on the MIT-Adobe FiveK dataset and DSLR photograph enhancement dataset exhibit that our BNCAGAN outperforms existing methods, and it can improve both the authenticity and naturalness of low-quality images and thus can be widely used for image retrieval preprocessing to improve the understanding of image semantics. The source code is available at https://github.com/SWU-CS-MediaLab/BNCAGAN .},
  archive      = {J_IJMIR},
  author       = {Liu, Shan and Shan, Shihao and Xiao, Guoqiang and Gao, Xinbo and Wu, Song},
  doi          = {10.1007/s13735-023-00310-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Image enhancement with bi-directional normalization and color attention-guided generative adversarial networks},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-assisted attention-based cross-modal hashing.
<em>IJMIR</em>, <em>13</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s13735-023-00311-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the hottest research topics in multimedia information retrieval, cross-modal hashing has drawn widespread attention in the past decades. How to minimize the semantic gap of heterogeneous data and accurately calculate the similarity of cross-modal data is a key challenge for this task. A paradigm for tackling this problem is to map features of multi-modal data into common space. However, these approaches lack inter-modal information interaction and may not achieve satisfactory results. To overcome this problem, we propose a novel text-assisted attention-based cross-modal hashing (TAACH) method in this paper. Firstly, TAACH relies on LabelNet supervision to guide the learning of hash functions for each modality. In addition, a novel text-assisted attention mechanism is designed in our TAACH to densely integrate text features into image features, perceiving their spatial correlation and enhancing the consistency of image and text knowledge. Extensive experiments on four benchmark datasets show the effectiveness of our proposed TAACH, and it also achieves competitive performance compared to state-of-the-art methods. The source code is available at https://github.com/SWU-CS-MediaLab/TAACH .},
  archive      = {J_IJMIR},
  author       = {Yuan, Xiang and Shan, Shihao and Huo, Yuwen and Jiang, Junkai and Wu, Song},
  doi          = {10.1007/s13735-023-00311-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Text-assisted attention-based cross-modal hashing},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VERITE: A robust benchmark for multimodal misinformation
detection accounting for unimodal bias. <em>IJMIR</em>, <em>13</em>(1),
1–15. (<a href="https://doi.org/10.1007/s13735-023-00312-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia content has become ubiquitous on social media platforms, leading to the rise of multimodal misinformation (MM) and the urgent need for effective strategies to detect and prevent its spread. In recent years, the challenge of multimodal misinformation detection (MMD) has garnered significant attention by researchers and has mainly involved the creation of annotated, weakly annotated, or synthetically generated training datasets, along with the development of various deep learning MMD models. However, the problem of unimodal bias has been overlooked, where specific patterns and biases in MMD benchmarks can result in biased or unimodal models outperforming their multimodal counterparts on an inherently multimodal task, making it difficult to assess progress. In this study, we systematically investigate and identify the presence of unimodal bias in widely used MMD benchmarks, namely VMU-Twitter and COSMOS. To address this issue, we introduce the “VERification of Image-TExt pairs” (VERITE) benchmark for MMD which incorporates real-world data, excludes “asymmetric multimodal misinformation” and utilizes “modality balancing”. We conduct an extensive comparative study with a transformer-based architecture that shows the ability of VERITE to effectively address unimodal bias, rendering it a robust evaluation framework for MMD. Furthermore, we introduce a new method—termed Crossmodal HArd Synthetic MisAlignment (CHASMA)—for generating realistic synthetic training data that preserve crossmodal relations between legitimate images and false human-written captions. By leveraging CHASMA in the training process, we observe consistent and notable improvements in predictive performance on VERITE; with a 9.2% increase in accuracy. We release our code at: https://github.com/stevejpapad/image-text-verification},
  archive      = {J_IJMIR},
  author       = {Papadopoulos, Stefanos-Iordanis and Koutlis, Christos and Papadopoulos, Symeon and Petrantonakis, Panagiotis C.},
  doi          = {10.1007/s13735-023-00312-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {VERITE: A robust benchmark for multimodal misinformation detection accounting for unimodal bias},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new CNN-based semantic object segmentation for autonomous
vehicles in urban traffic scenes. <em>IJMIR</em>, <em>13</em>(1), 1–11.
(<a href="https://doi.org/10.1007/s13735-023-00313-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is the most important stage of making sense of the visual traffic scene for autonomous driving. In recent years, convolutional neural networks (CNN)-based methods for semantic segmentation of urban traffic scenes are among the trending studies. However, the methods developed in the studies carried out so far are insufficient in terms of accuracy performance criteria. In this study, a new CNN-based semantic segmentation method with higher accuracy performance is proposed. A new module, the Attentional Atrous Feature Pooling (AAFP) Module, has been developed for the proposed method. This module is located between the encoder and decoder in the general network structure and aims to obtain multi-scale information and add attentional features to large and small objects. As a result of experimental tests with the CamVid data set, an accuracy value of approximately 2% higher was achieved with a mIoU value of 70.59% compared to other state-of-art methods. Therefore, the proposed method can semantically segment objects in the urban traffic scene better than other methods.},
  archive      = {J_IJMIR},
  author       = {Doğan, Gürkan and Ergen, Burhan},
  doi          = {10.1007/s13735-023-00313-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A new CNN-based semantic object segmentation for autonomous vehicles in urban traffic scenes},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opinion convergence-based sentiment prediction of image
advertisement. <em>IJMIR</em>, <em>13</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s13735-023-00314-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel approach was proposed for the sentiment prediction of image advertisements. Unlike general multilabel problems with correct answers, image sentiment prediction is a multilabel problem that involves converging the opinions of various labelers. Therefore, an opinion convergence-based image sentiment prediction methodology was proposed to model the decision-making process of image sentiment prediction. Hypothetical labelers were generated by recombining training datasets to maximize the characteristic distance, and each prediction model was trained with each combination to represent each hypothetical labeler with distinct characteristics. The results of the experiment revealed that the proposed image sentiment prediction method outperformed other existing models with advanced architectures or considered various factors for improving the accuracy of image sentiment prediction tasks. Moreover, the effectiveness of the proposed method was verified through qualitative experiments.},
  archive      = {J_IJMIR},
  author       = {Lee, Younghoon},
  doi          = {10.1007/s13735-023-00314-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Opinion convergence-based sentiment prediction of image advertisement},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An emotion-driven, transformer-based network for multimodal
fake news detection. <em>IJMIR</em>, <em>13</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s13735-023-00315-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media is filled with multimedia data in the form of news and is heavily impacting the daily lives of the people. However, the rise of fake news is causing distress and becoming a major source of concern. Several attempts have been made in the past to detect fake news, but it still remains a challenging problem. In this study, we propose an emotion-driven framework that extracts emotions from the multimodal data to identify fake news. We use the vision transformer, which removes the irrelevant data from the images and enhances the overall classification accuracy. To the best of our knowledge, this is the first work that incorporates multimodal emotions to detect fake news in the multimodal data, comprising of images and text. We conducted several experiments on five datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset, Risdal Fake News Dataset, and Fakeddit Multimodal Dataset, and evaluated the performance of the network by using Precision, Recall, F1 scores, Accuracy, and ROC curves. We also conducted an ablation study to verify the effectiveness of different components involved in the proposed architecture. The experimental results show that the proposed architecture outperforms the state-of-the-art and other baseline methods on all the evaluation metrics.},
  archive      = {J_IJMIR},
  author       = {Yadav, Ashima and Gupta, Anika},
  doi          = {10.1007/s13735-023-00315-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {An emotion-driven, transformer-based network for multimodal fake news detection},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal retrieval based on shared proxies.
<em>IJMIR</em>, <em>13</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s13735-023-00316-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a common space that is simultaneously semantically discriminative and modality invariant stands as the primary challenge in cross-modal retrieval. Conventional approaches usually employ pairwise or triplet data relationships to learn the common space, which can only capture the data similarity locally but would be unable to effectively characterize the global geometry of the common embedding space, and thus would limit the performance of cross-modal retrieval. This paper proposes to integrate the principles of the shared proxy and neighborhood component analysis in order to learn a shared space for different modalities. The objective of this shared space is to minimize the distance between a sample’s representation and its corresponding proxy, while also maximizing the distances between a sample’s representation and the proxies that are not associated with the sample. Our proposed framework, named Cross-mOdal proXy learnIng (COXI), incorporates a cross-modal shared proxy loss, a discriminative loss, and a modality invariant loss to facilitate supervised cross-modal retrieval. Extensive experiments on benchmark datasets clearly shows that COXI outperforms state-of-the-art cross-modal retrieval techniques. Code is available on https://github.com/LigangZheng/COXI .},
  archive      = {J_IJMIR},
  author       = {Wei, Yuxin and Zheng, Ligang and Qiu, Guoping and Cai, Guocan},
  doi          = {10.1007/s13735-023-00316-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Cross-modal retrieval based on shared proxies},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multiple aggregation networks for action recognition.
<em>IJMIR</em>, <em>13</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s13735-023-00317-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in the literature, the design of pooling operations that handle action recognition, with different sources of temporal granularity in action categories, has comparatively received less attention, and existing solutions rely mainly on max or averaging operations. The latter are clearly powerless to fully exhibit the actual temporal granularity of action categories and thereby constitute a bottleneck in classification performances. In this paper, we introduce a novel hierarchical pooling design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network—which best fits a given ground-truth—is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length and resolution agnostic. Extensive experiments conducted on the challenging UCF-101, HMDB-51 and JHMDB-21 databases corroborate all these statements.},
  archive      = {J_IJMIR},
  author       = {Mazari, Ahmed and Sahbi, Hichem},
  doi          = {10.1007/s13735-023-00317-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Deep multiple aggregation networks for action recognition},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How does a kernel based on gradients of infinite-width
neural networks come to be widely used: A review of the neural tangent
kernel. <em>IJMIR</em>, <em>13</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s13735-023-00318-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural tangent kernel (NTK) was created in the context of using the limit idea to study the theory of neural network. NTKs are defined from neural network models in the infinite-width limit trained by gradient descent. Such over-parameterized models achieved good test accuracy in experiments, and the success of the NTK emphasizes not only the importance of describing neural network models in the width limit of $$h \to \infty$$ , but also the further development of deep learning theory for gradient flow in the step limit of $$\eta \to 0$$ . And NTK can be widely used in various machine learning models. This review provides a comprehensive overview of the entire development of NTKs. Firstly, the bias–variance tradeoff in statistics, the popular over-parameterization and gradient descent in deep learning, and the widely used kernel method were introduced. Secondly, the development of research on the infinite-width limit in networks and the introduction of the concept of the NTK were introduced, and the development and latest progress of NTK theory were discussed. Finally, the researches on the migrations of NTKs to neural networks of other structures and the applications of NTKs to other fields of machine learning were presented.},
  archive      = {J_IJMIR},
  author       = {Tan, Yiqiao and Liu, Haizhong},
  doi          = {10.1007/s13735-023-00318-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {How does a kernel based on gradients of infinite-width neural networks come to be widely used: A review of the neural tangent kernel},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental image retrieval method based on feature
perception and deep hashing. <em>IJMIR</em>, <em>13</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00319-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to propose an image retrieval algorithm with adaptable model and wide range of applications for large-scale datasets has become a critical technical problem in current image retrieval. This paper proposed an Incremental Image Retrieval Method Based on Feature Perception and Deep Hashing. The algorithm contains two important parts: the hash function learning part and the incremental hash code mapping part. Firstly, a module is designed called Feature Perception Module to obtain multi-scale global context-aware information. It also keeps the scale and shape of the final extracted deep features invariant. Then, a new incremental hash loss function is designed to maintain the similarity between the query image and the dataset image; the advantage of this is that it can reduce the time cost of updating the model. The experimental results show that the algorithm model can perform well in incremental image retrieval. It is shown that the algorithm can solve the current problem of low retrieval efficiency and high cost due to retraining models caused by the dramatic increase in the number of images in the image retrieval field.},
  archive      = {J_IJMIR},
  author       = {Kaiyang Liao and Lin, Jie and Yuanlin Zheng and Wang, Keer and Feng, Wen},
  doi          = {10.1007/s13735-024-00319-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Incremental image retrieval method based on feature perception and deep hashing},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A voting-based novel spatio-temporal fusion framework for
video saliency using transfer learning mechanism. <em>IJMIR</em>,
<em>13</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s13735-024-00320-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision and image processing, salient object detection in an image and video is one of the complex research problems. The most attention-grabbing object of the image or video is generally considered the most prominent object. Due to the motion effect and change in the object’s shape and structure, saliency detection in the video is much more complex than detecting a salient object in an image. In the last two decades, the researcher(s) community has proposed several methods to mimic the human visual capability to find the attention-grabbing object. The techniques presented in the literature are based on statistics or deep learning approaches. Deep learning-based methods have recently gained more attention due to robust detection results. However, it is challenging for one model to function effectively in every situation, so the overall detection accuracy gets degraded. Consequently, an ensemble technique rather than a single model may be preferable to segment salient objects accurately. In this research article, the author(s) have presented a voting-based spatio-temporal fusion framework for saliency detection in video. In the proposed framework, the author(s) used three static networks to estimate spatial saliency using the transfer learning concept and one dynamic network based on 3D convolution to learn the temporal effect. To construct the final saliency output, each spatial saliency map is fused with a temporal saliency map, and finally, a refined saliency map having spatial and temporal information is generated after pixel-wise voting. The proposed framework’s findings on four publicly available and widely used video saliency datasets are competitive in terms of S-Measure, F-Measure and mean absolute error as compared to the state-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Kumain, Sandeep Chand and Singh, Maheep and Awasthi, Lalit Kumar},
  doi          = {10.1007/s13735-024-00320-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A voting-based novel spatio-temporal fusion framework for video saliency using transfer learning mechanism},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAABNet: Depth-wise asymmetric attention bottleneck for
real-time semantic segmentation. <em>IJMIR</em>, <em>13</em>(1), 1–16.
(<a href="https://doi.org/10.1007/s13735-024-00321-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for the real-world applications such as autonomous driving and video surveillance, lightweight semantic segmentation methods achieving good trade-offs in terms of parameter size, speed and accuracy have attracted more and more attention. In this context, we propose a novel real-time semantic segmentation model. First, we design a two-branch depth-wise asymmetric attention bottleneck (DAAB) based on residual network to reduce the number of parameters and improve the inference speed. Particularly, an attention refinement module (ARM) is added in the DAAB module to make the information extracted from the two branches complement each other. Second, we design a strip pooling attention (SPA) module which combines the strip pooling module and the attention mechanism to pay more attention to strip-shaped objects and to establish long-range dependencies between discrete distributed regions, so that to address the problem of poor segmentation of strip shape objects. In addition, we also fuse information from different stages to compensate for the loss of spatial information, thus improving the ability of the network to segment small objects. Experiments on CityScapes and CamVid dataset demonstrate that the proposed method achieves impressive trade-offs in terms of parameter size, speed and accuracy. Code is available at: https://github.com/mhhz/DAABnet1 .},
  archive      = {J_IJMIR},
  author       = {Tang, Qingsong and Chen, Yingli and Zhao, Minghui and Min, Shitong and Jiang, Wuming},
  doi          = {10.1007/s13735-024-00321-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DAABNet: Depth-wise asymmetric attention bottleneck for real-time semantic segmentation},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-efficient tuning of cross-modal retrieval for a
specific database via trainable textual and visual prompts.
<em>IJMIR</em>, <em>13</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s13735-024-00322-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel cross-modal image retrieval method realized by parameter efficiently tuning a pre-trained cross-modal model is proposed in this study. Conventional cross-modal retrieval methods realize text-to-image retrieval by training cross-modal models to bring paired texts and images close in a common embedding space. However, these methods are trained on huge amounts of intentionally annotated image-text pairs, which may be unavailable in specific databases. To reduce the dependency on the amount and quality of training data, fine-tuning a pre-trained model is one approach to improve retrieval accuracy on specific personal image databases. However, this approach is parameter inefficient for separately training and retaining models for different databases. Thus, we propose a cross-modal retrieval method that uses prompt learning to solve these problems. The proposed method constructs two types of prompts, a textual prompt and a visual prompt, which are both multi-dimensional vectors. The textual and visual prompts are then concatenated with input texts and images, respectively. By optimizing the prompts to bring paired texts and images close in the common embedding space, the proposed method can improve retrieval accuracy with only a few parameters being updated. The experimental results demonstrate that the proposed method is effective for improving retrieval accuracy and outperforms conventional methods in terms of parameter efficiency.},
  archive      = {J_IJMIR},
  author       = {Zhang, Huaying and Yanagi, Rintaro and Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  doi          = {10.1007/s13735-024-00322-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Parameter-efficient tuning of cross-modal retrieval for a specific database via trainable textual and visual prompts},
  volume       = {13},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
