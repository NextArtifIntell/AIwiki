<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ida---107">IDA - 107</h2>
<ul>
<li><details>
<summary>
(2023). The interactive relationship between prosody and respiration
of computer BIOPAC systems: Shanghai dialect and mandarin. <em>IDA</em>,
<em>27</em>(S1), 181–192. (<a
href="https://doi.org/10.3233/IDA-237443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper uses the computer BIOPAC Systems tool to analyze Shanghai dialect and Mandarin which refers to the relationship between prosody and respiration in reading fable, the conclusion are as follows: 1) the mean of respiratory parameters and respiratory units was positively related, and respira tion curve on Shanghai dialect which shows the characteristics of small ups and downs is different from the Mandarin curve; 2) the reset of respiration has relationship with mute segment, and the occurrence of reset breathing place must have a quiet period, while the opposite does not happen; 3) on the situation of fable literary style with flexible feature, the text of the proficiency can significantly increase the complexity of the respiration curve, showing a more special features such as “breathless” pronunciation.},
  archive      = {J_IDA},
  author       = {Meng, Xiaohong and Yang, Zhongxiu and Li, Zhi},
  doi          = {10.3233/IDA-237443},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {181-192},
  shortjournal = {Intell. Data Anal.},
  title        = {The interactive relationship between prosody and respiration of computer BIOPAC systems: Shanghai dialect and mandarin},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retracing-efficient IoT model for identifying the
skin-related tags using automatic lumen detection. <em>IDA</em>,
<em>27</em>(S1), 161–180. (<a
href="https://doi.org/10.3233/IDA-237442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of patients with skin diseases reported a dramatic increase which is a major concern and should be addressed. The evaluation of skin is crucial to the correct diagnosis during the follow-up. Through technological advances and partnership, skin disorders can be identified and predicted. P ROBLEM: The manual detection of skin diseases may sometimes lead to misclassification due to the same intensity and color levels, which is crucial to the correct diagnosis. SOLUTION: An automated system to identify these skin diseases is applied. An IoT-based skin monitoring infrastructure is imposed that links the entire system. METHOD: In this study, a Retracing-efficient IoT model for identifying the moles, skin tags, and warts using Automatic lumen detection with the help of IoT-based Variation regularity is proposed with the technique imposed IoMT, Automatic lumen detection, Variation regularity, and trigonometric algorithm. RESULTS: The intensity and edge width based on moles, skin tags, and warts edge width heightened intensity accuracy is 56.2% on the image group with image count is 500 to 10000, and the enhanced low-level total sample accuracy is 95.9%. The pixel analysis for intensity with wavelength and intensity with time wavelength is improved from 4.2% to 54.6%, and accuracy is 70.9% formulated. Periodic classification on image count and classification accuracy image count is 87% against the 500 to 10000 image. Correlation performance analysis of lumen detection resolution image pixel and enhanced correlation performance accuracy is 23.50% on the 480 × 640 to 2336 × 3504 pixel images. CONCLUSION: The approach is tested for varying datasets, and comparative analysis is performed that reflects the effectiveness of the proposed system with high accuracy, thus contributing to the development of a perfect platform for skincare to the early detection and diagnosis of skin conditions.},
  archive      = {J_IDA},
  author       = {Vivekananda, G.N. and Almufti, Saman M. and Suresh, C. and Samsudeen, Salomi and Devarajan, Mohanarangan Veerapperumal and Srikanth, R. and Jayashree, S.},
  doi          = {10.3233/IDA-237442},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {161-180},
  shortjournal = {Intell. Data Anal.},
  title        = {Retracing-efficient IoT model for identifying the skin-related tags using automatic lumen detection},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative observation of changes in natriuretic peptides
before and after interventional therapy for congenital heart disease.
<em>IDA</em>, <em>27</em>(S1), 151–159. (<a
href="https://doi.org/10.3233/IDA-237440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OBJECTIVE: To explore changes in the plasma atrial natriuretic peptide (ANP) and brain natriuretic peptide (BNP) in patients with left-to-right shunt congenital heart disease (CHD) before and in the early stage after interventional occlusion and to evaluate the clinical significance. METHODS: Among 97 patients with left-to-right shunt CHD undergoing interventional occlusion, 34 cases had a VSD (ventricular septal defect), 35 cases had an ASD (atrial septal defect), and 28 cases had PDA (patent ductus arteriosus). Another 20 normal adults formed the control group. An ELISA was used to determine the plasma ANP and BNP levels before and on the third day after the operation to evaluate their correlations with cardiac functions and the defect size. RESULTS: The plasma ANP and BNP levels of patients with left-to-right shunt CHD were increased compared with those of the normal control group (P&lt; 0.01), and the plasma ANP and BNP levels were decreased on the third day after interventional occlusion compared with the preoperative levels (P&lt; 0.05). The plasma ANP and BNP levels were correlated with the New York Heart Association (NYHA) grade, left ventricular ejection fraction and defect diameter (P&lt; 0.05). CONCLUSION: Patients with left-to-right congenital heart disease exhibit activation of ANP and BNP, which can be alleviated in the early stage after intervention occlusion. Left-to-right shunt congenital heart disease is given priority over atrial septal defect (ASD), ventricular septal defect (VSD) and patent ductus arteriosus (PDA). Early traditional methods included repair or correction by open heart surgery under extracorporeal circulation (also known as cardiopulmonary bypass, CPB). However, interventional therapy has become a developing trend for the treatment of congenital heart disease since 1967, when Porstmann et al. [1]. reported the transcatheter closure of ASD for the first time. The application of the AMPLATZER occluder, which is a simple and feasible method, has improved the safety of the treatment and enabled the therapeutic effect to reach ideal levels. The natriuretic peptide (NP) family consists of the atrial natriuretic polypeptide (ANP), the brain natriuretic peptide, which is also known as the B type natriuretic peptide (BNP), the C type natriuretic peptide (CNP), the renal natriuretic peptide (RNP) and the D type natriuretic peptide (DNP). These family members are similar in structure, have strong natriuretic, diuretic and vasodilative effects and antagonize the activity of the renin-angiotensin-aldosterone system (RAAS) and the sympathetic nerve. Together, the natriuretic peptides sensitively and specifically reflect the ventricular function state. Although all types of congenital heart disease differ in anatomical structure, they all contain the common features of heart failure. This study detected changes in the serum ANP and BNP levels in patients with left-to-right shunt congenital heart disease before and on the third day after interventional occlusion to evaluate the early changes in left-to-right shunt congenital heart disease after interventional occlusion through neuroendocrine.},
  archive      = {J_IDA},
  author       = {Liu, Xinghui and Tan, Hongwen and Liu, Xiaoqiao and Wu, Qiang},
  doi          = {10.3233/IDA-237440},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {151-159},
  shortjournal = {Intell. Data Anal.},
  title        = {Comparative observation of changes in natriuretic peptides before and after interventional therapy for congenital heart disease},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social and economic development impact of elderly health
care products based on design ethics. <em>IDA</em>, <em>27</em>(S1),
137–150. (<a href="https://doi.org/10.3233/IDA-237439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BACKGROUND: The aging of the population is a historical stage that many countries must experience, and the current design and development of elderly health care products can no longer meet the increasing demands of the elderly. OBJECTIVE: The impact of ethical design of elderly health care products on socio-economic development is explored to provide a theoretical basis for the development direction of elderly health care products. METHODS: In this study, a questionnaire survey is conducted on 268 middle-aged people to record the subjects’ willingness to purchase elderly health care products and their reasons, concerns, satisfaction, and future demands. RESULTS: Among the subjects, 181 people have purchased elderly health care products, accounting for 67.36%; the subjects are more concerned about the quality and safety of elderly health care products, accounting for 92.56% and 91.85% respectively, followed by operability (68.46%); the problems encountered by the elderly using elderly health care products are mainly operational problems, accounting for 65.37%; and high safety (86.13%) and good quality (79.55%) are the subjects’ main demands for future development of elderly health care products. 73.61% of the 30–40 year old subjects said that the intelligent aged care products were very good; 65.89% of the 41–50 year old subjects said that the intelligent aged care products were very good; 52.67% of the 51–60 subjects thought that intelligent elderly care products were very good; and 47.82% of the subjects whose age were over 60 expressed their willingness to try intelligent elderly care products. CONCLUSIONS: Good quality and high safety are the main demands for the future development of elderly health care products. The elderly health care products manufactured based on the people-oriented design ethics concept can greatly meet the aspirations of the elderly to pursue a happy later life, and promote the vigorous development of the elderly industrial economy.},
  archive      = {J_IDA},
  author       = {Qi, Na and Zhang, Xun},
  doi          = {10.3233/IDA-237439},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {137-150},
  shortjournal = {Intell. Data Anal.},
  title        = {Social and economic development impact of elderly health care products based on design ethics},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E-health technological barriers faced by iraqi healthcare
institutions. <em>IDA</em>, <em>27</em>(S1), 115–135. (<a
href="https://doi.org/10.3233/IDA-237438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The health records management issues have detrimentally affected the Iraqi healthcare sector resultant from the inferior information technology integrity and the complicatedness of data. In order to resolve this problem, other methods of storage, management, and retrieval of health-related data can be offered by e-Health services. These aspects are important in tracking patients’ health conditions using multiple platforms at the service provider’s own convenience. However, there are numerous issues that hinder the extensive adoption of e-Health services by the health establishments in Iraq, such as issues on security and privacy, legalities connected to policies, and its implementation. The significance of the current study is its identification of the crucial aspects that will lead to the success of impacting the technical staff towards their positive acceptance and behavior with regard to the employment of e-Health information system in Iraqi hospitals. A self-administered survey was carried out on 104 technical staff from various healthcare organizations in Iraq using a simple random sampling technique. A nonparametric second-generation multivariate analysis was conducted on the compiled ordinal data by the utilization of the PLS-SEM approach. The outcomes indicated the favorable impact of several factors on the doctor’s employment of e-Health in Iraqi hospitals, comprising Availability and Affordability of the hardware and software, ICT Support Service, Network Reliability, Privacy, and Security. The results are important in assisting the comprehension of e-Health systems in the management of health data, in addition to the provision of the pertinent recommendations for policymakers to provide guidance, issue advice, directives to the healthcare professionals toward the continuous consideration of using advance information and communications technology at work.},
  archive      = {J_IDA},
  author       = {Ali, Saif Mohammed and Burhanuddin, M.A. and Yaseen, Ali Taha and Jaber, Mustafa Musa and Jassim, Mustafa Mohammed and Ali, Aseel Mohammed and Alkhayyat, Ahmed and Mohammed, Mohammed A. and Mohamad, Auday A.H.},
  doi          = {10.3233/IDA-237438},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {115-135},
  shortjournal = {Intell. Data Anal.},
  title        = {E-health technological barriers faced by iraqi healthcare institutions},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time MRI lungs images revealing using hybrid
feedforward deep neural network and convolutional neural network.
<em>IDA</em>, <em>27</em>(S1), 95–114. (<a
href="https://doi.org/10.3233/IDA-237436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focused on Real-time MRI lung images that were revealed using three grade processes by manipulating nanophotonics components, mapping by deep learning, machine learning, and pattern recognition. This research is Solving Magnetic resonance imaging of interstitial lung diseases with Hyb rid feedforward Deep Neural Network (ffDNN) and Convolutional Neural Network (CNN) architecture. The feedforward deep neural network (ffDNN) and Convolutional Neural Network (CNN) techniques are used to Solving Magnetic resonance imaging of interstitial lung diseases on the nanophotonics components, deep learning, and machine learning Platform. The Proposed semiconductor monolithic integration approach employed for bio-Magnetic resonance imaging characterization using photonic crystal “Symptomatic Image Revealing” details of the resonant monolithic. The proposed machine-learning-based approach revealed characterizing multi-parameter design space of nanophotonic components using Nano-optic imagers. The Pattern Recognition for MRI was performed for lower dimensionality. Finally, the Hybrid feedforward Deep Neural Network (ffDNN) and Convolutional Neural Network (CNN) architecture for calculating the height and size of scatterers using the inverse design of the meta-optical structure. The temporal resolution assessment of image data pixel size 280x360 hyperspectral imaging temporal resolution is 25, and magnetic resonance imaging temporal resolution is 50. The Image distribution shows that phase shift and transmission are 2.78 degrees and at 95%. The result for the inverse design using CNN returns the efficient inverse design of test data that can be designed according to the required pressure distribution. Wavelength 1000 nanometer to 1600 machine learning method absorbance 40% and ffDNN absorbance 33%.},
  archive      = {J_IDA},
  author       = {Karthick, M. and Samuel, Dinesh Jackson and Prakash, B. and Sathyaprakash, P. and Daruvuri, Nandhini and Ali, Mohammed Hasan and Aiswarya, R.S.},
  doi          = {10.3233/IDA-237436},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {95-114},
  shortjournal = {Intell. Data Anal.},
  title        = {Real-time MRI lungs images revealing using hybrid feedforward deep neural network and convolutional neural network},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital image processing for evaluating the impact of
designated nanoparticles in biomedical applications. <em>IDA</em>,
<em>27</em>(S1), 83–94. (<a
href="https://doi.org/10.3233/IDA-237435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanomaterials are finding increasingly diverse medical uses as technology advances. Researchers are constantly being introduced to new and improved methods, and these applications see widespread use for both diagnostic and therapeutic purposes. Early disease detection, efficient drug delivery, cosm etics and health care products, biosensors, miniaturisation techniques, surface improvement in implantable biomaterials, improved nanofibers in medical textiles, etc. are all examples of how biomedical nanotechnology has made a difference in the medical field. The nanoparticles are introduced deliberately for therapeutic purposes or accidentally from the environment; they will eventually reach and penetrate the human body. The exposed nanoparticles interact with human blood, which carries them to various tissues. An essential aspect of blood rheology in the microcirculation is its malleability. As a result, nanomaterial may cause structural abnormalities in erythrocytes. Echinocyte development is a typical example of an induced morphological alteration. The length of time it takes for these side effects to disappear after taking a nano medication also matters. Haemolyses could result from the dangerous concentration. In this experiment, human blood is exposed to varying concentrations of chosen nanomaterial with potential medical applications. The morphological modifications induced were studied by looking at images of erythrocyte cells. That’s a picture of a cell taken using a digital optical microscope, by the way. We used MATLAB, an image-analysis programme, to study the morphometric features. Human lymphocyte cells were used in the cytotoxic analysis.},
  archive      = {J_IDA},
  author       = {Baazeem, Rami and Maheshwary, Priti and Binjawhar, Dalal Nasser and Gulati, Kamal and Joshi, Shubham and Ojo, Stephen and Pareek, Piyush Kumar and Shukla, Prashant Kumar},
  doi          = {10.3233/IDA-237435},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {83-94},
  shortjournal = {Intell. Data Anal.},
  title        = {Digital image processing for evaluating the impact of designated nanoparticles in biomedical applications},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction and analysis of chronic epilepsy using
electroencephalographic signals on medical internet of things platform.
<em>IDA</em>, <em>27</em>(S1), 65–82. (<a
href="https://doi.org/10.3233/IDA-237434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy patients who are presently refractory may be monitored using a seizure prediction Brain-Computer Interface (BCI), which uses electrodes strategically implanted in the brain to anticipate and regulate the onset and duration of a seizure. Real-time approaches to these technologies have chall enges, as seen by seizures’ instantaneous electrographic activity. Electroencephalographic (EEG) signals are inherently non-stationary, which means that the regular and seizure signals differ significantly among people with epilepsy. Due to the restricted number of contacts on electrodes, dynamically processed and collected characteristics cannot be employed in a prediction function without causing significant processing delays. Big data can guarantee secure storage in these situations, and it has the maximum processing capability to identify, record, and analyze time in real-time to conduct the seizure event on the timetable. Seizure prediction and location for huge Scalp EEG recordings have been the focus of this study, which used wearable sensor data and deep learning to use cloud storage to develop the systems. A novel technique is suggested to avoid an epileptic seizure and discover the seizure origin from the utilized wearable sensors. Secondly, deep learning architectures called Clustered Autoencoder with Convolutional Neural Network (CAE-CNN), an expanded optimization methodology is presented based on the Principal Component Analysis (PCA), the Hierarchical Searching Algorithm (HSA), and the Medical Internet of Things (MIoT) has been established to define the suggested frameworks based on the collection of big data storage of the wearable sensors in real-time, automatic computation and storage. According to clinical trials, CAE-CNN outperforms the current wearable sensor-based treatment for unresolved chronic epilepsy patients.},
  archive      = {J_IDA},
  author       = {Hassoon, Noor Hasan and Ali, Mohammed Hasan and Jaber, Mustafa Musa and Abd, Sura Khalil and Abosinnee, Ali S. and Kareem, Z.H.},
  doi          = {10.3233/IDA-237434},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {65-82},
  shortjournal = {Intell. Data Anal.},
  title        = {Prediction and analysis of chronic epilepsy using electroencephalographic signals on medical internet of things platform},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heart sound classification using wavelet scattering
transform and support vector machine. <em>IDA</em>, <em>27</em>(S1),
47–63. (<a href="https://doi.org/10.3233/IDA-237432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OBJECTIVE: A representation of the sound recordings that are associated with the movement of the entire cardiac structure is termed the Phonocardiogram (PCG) signal. In diagnosing such diverse diseases of the heart, PCG signals are helpful. Nevertheless, as recording PCG signals are prone to severa l surrounding noises and other disturbing signals, it is a complex task. Thus, prior to being wielded for advanced processing, the PCG signal needs to be denoised. This work proposes an improved heart sound classification by utilizing two-stage Low pass filtering and Wavelet Threshold (WT) technique with subsequent Feature Extraction (FE) using Wavelet Scatter Transform and further classification utilizing the Cubic Polynomial Support Vector Machine (SVM) technique for CVD. METHOD: A computer-aided diagnosis system for CVD detection centered on PCG signal analysis is offered in this work. Initially, by heavily filtering the signal, the raw PCG signals obtained using the database were pre-processed. Then, to remove redundant information and noise, it is denoised via the WT technique. From the denoised PCG, wavelet time scattering features were extracted. After that, by employing SVMs, these features were classified for pathology. RESULTS: For the analysis, the PCG signal obtained from the Physionet dataset was considered. Heavy low-pass filtering utilizing a Low-Pass Butterworth Filter (LPBF) is entailed in the pre-processing step. This removed 98% of the noise inherently present in the signal. Further, the signal strength was ameliorated by denoising it utilizing the WT technique. Promising results with maximum noise removal of up to 99% are exhibited by the method. From the PCG, Wavelet Scattering (WS) features were extracted, which were later wielded to categorize the PCG utilizing SVMs with 99.72% accuracy for different sounds. DISCUSSION: The Classification accuracies are analogized with other classification techniques present in the literature. This technique exhibited propitious outcomes with a 3% improvement in the F1 score when weighed against the top-notch techniques. The improvement in the metrics is attributed to the usage of the pre-processing stage comprising of Low-pass filter and WT method, WS Transform (WST), and SVMs. CONCLUSION: The superiority of the proposed technique is advocated by the comparative investigation with prevailing methodologies. The system revealed that Coronary Artery Disease (CAD) can be implemented with superior methods to achieve high accuracy.},
  archive      = {J_IDA},
  author       = {Shervegar, Vishwanath Madhava},
  doi          = {10.3233/IDA-237432},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {47-63},
  shortjournal = {Intell. Data Anal.},
  title        = {Heart sound classification using wavelet scattering transform and support vector machine},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dementia prediction using novel IOTM (internet of things in
medical) architecture framework. <em>IDA</em>, <em>27</em>(S1), 29–45.
(<a href="https://doi.org/10.3233/IDA-237431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades the health care developments highly rise the level of ages of world population. This improvement was accompanied by increasing the diseases related with elder like Dementia, which Alzheimer’s disease represents the most common form. The present studies aim to design and implemen tation a medical system for improving the life of Alzheimer’s disease persons and ease the burden of their caregivers. AI is transforming the practice of medicine. It’s helping doctors diagnose patients more accurately, make predictions about patient’s future health, and recommend better treatments. AI goes beyond the foundations of deep learning to give you insight into the nuances of applying AI to medical use cases. Diagnosis is about identifying disease. By building an algorithm we can diagnosis chest X-ray and determine whether it contains disease, another algorithm that will look at brain MRIs and identify the location of tumours in those brain MRIs health of the patients lab values and their demographics and use those to predict the risk of an event. A Smart IOT Interactive Assistance is a technological device that continuously monitors the stability of an Alzheimer’s patient, indicates their position on a map, automatically reminds them to take their prescriptions, and has a call button for any emergencies they could experience during the day. The system has two components, one of which the patient wears and the other of which is an IoT platform application utilized by the caregiver. The motion processing unit sensor, GPS, heart rate sensor with microcontrollers, and LCD display were used to construct the wearable device. An Internet of Things (IoT) platform supports this device, allowing the caregiver to communicate with the patient from any location.},
  archive      = {J_IDA},
  author       = {Pavitra, B. and Singh, D. Narendar and Sharma, Sudhir Kumar and Hashmi, Mohammad Farukh},
  doi          = {10.3233/IDA-237431},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {29-45},
  shortjournal = {Intell. Data Anal.},
  title        = {Dementia prediction using novel IOTM (Internet of things in medical) architecture framework},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning models for predicting the position of the head
on an x-ray image for cephalometric analysis. <em>IDA</em>,
<em>27</em>(S1), 3–27. (<a
href="https://doi.org/10.3233/IDA-237430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cephalometric analysis is used to identify problems in the development of the skull, evaluate their treatment, and plan for possible surgical interventions. The paper aims to develop a Convolutional Neural Network that will analyze the head position on an X-ray image. It takes place in such a way that it recognizes whether the image is suitable and, if not, suggests a change in the position of the head for correction. This paper addresses the exact rotation of the head with a change in the range of a few degrees of rotation. The objective is to predict the correct head position to take an X-ray image for further Cephalometric analysis. The changes in the degree of rotations were categorized into 5 classes. Deep learning models predict the correct head position for Cephalometric analysis. An X-ray image dataset on the head is generated using CT scan images. The generated images are categorized into 5 classes based on a few degrees of rotations. A set of four deep-learning models were then used to generate the generated X-Ray images for analysis. This research work makes use of four CNN-based networks. These networks are trained on a dataset to predict the accurate head position on generated X-Ray images for analysis. Two networks of VGG-Net, one is the U-Net and the last is of the ResNet type. The experimental analysis ascertains that VGG-4 outperformed the VGG-3, U-Net, and ResNet in estimating the head position to take an X-ray on a test dataset with a measured accuracy of 98%. It is due to the incorrectly classified images are classified that are directly adjacent to the correct ones at intervals and the misclassification rate is significantly reduced.},
  archive      = {J_IDA},
  author       = {Prasanna, K. and Jyothi, Chinna Babu and Mathivanan, Sandeep Kumar and Jayagopal, Prabhu and Saif, Abdu and Samuel, Dinesh Jackson},
  doi          = {10.3233/IDA-237430},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {3-27},
  shortjournal = {Intell. Data Anal.},
  title        = {Deep learning models for predicting the position of the head on an X-ray image for cephalometric analysis},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Editorial. <em>IDA</em>, <em>27</em>(S1), 1–2. (<a
href="https://doi.org/10.3233/IDA-239006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-239006},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {S1},
  pages        = {1-2},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Note-level singing melody transcription with transformers.
<em>IDA</em>, <em>27</em>(6), 1853–1871. (<a
href="https://doi.org/10.3233/IDA-227077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing a singing melody from an audio signal in terms of the music notes’ pitch onset and offset, referred to as note-level singing melody transcription, has been studied as a critical task in the field of automatic music transcription. The task is challenging due to the different timbre and v ibrato of each vocal and the ambiguity of onset and offset of the human voice compared with other instrumental sounds. This paper proposes a note-level singing melody transcription model using sequence-to-sequence Transformers. The singing melody annotation is expressed as a monophonic melody sequence and used as a decoder sequence. Overlapping decoding is introduced to solve the problem of the context between segments being broken. Applying pitch augmentation and and adding noisy dataset with data cleansing turns out to be effective in preventing overfitting and generalizing the model performance. Ablation studies demonstrate the effects of the proposed techniques in note-level singing melody transcription, both quantitatively and qualitatively. The proposed model outperforms other models in note-level singing melody transcription performance for all the metrics considered. For fundamental frequency metrics, the voice detection performance of the proposed model is comparable to that of a vocal melody extraction model. Finally, subjective human evaluation demonstrates that the results of the proposed models are perceived as more accurate than the results of a previous study.},
  archive      = {J_IDA},
  author       = {Park, Jonggwon and Choi, Kyoyun and Oh, Seola and Kim, Leekyung and Park, Jonghun},
  doi          = {10.3233/IDA-227077},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1853-1871},
  shortjournal = {Intell. Data Anal.},
  title        = {Note-level singing melody transcription with transformers},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-head attention based candidate segment selection in QA
over hybrid data. <em>IDA</em>, <em>27</em>(6), 1839–1852. (<a
href="https://doi.org/10.3233/IDA-227032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question Answering based on Tabular and Textual data is a novel task proposed in recent years in the field of QA. At present, most QA systems return answers from a single data form, such as knowledge graphs, tables, texts. However, hybrid data including structured and unstructured data is quite per vasive in real life instead of a single form. Recent research on TAT-QA mainly suffers from the higher error of extracting supporting evidences from both tabular and textual content. This paper aimed to address the problem of failure evidence extraction from more complex and realistic hybrid data. We first proposed two types of metrics to evaluate the performance of evidence extraction on hybrid data, i.e. wrong evidence ratio (WER) and missing evidence ratio (MER). Then we utilize a candidate extractor to obtain supporting evidence related to the question. Third, an origin selector is designed to determine from where the question’s answer comes. Finally, the loss of origin selector is fused to the final loss function, which can improve the evidence extraction performance. Experimental results on the TAT-QA dataset showed that our proposed model outperforms the best baseline in terms of F1, WER and MER, which proves the effectiveness of our model.},
  archive      = {J_IDA},
  author       = {Chen, Qian and Gao, Xiaoying and Guo, Xin and Wang, Suge},
  doi          = {10.3233/IDA-227032},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1839-1852},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-head attention based candidate segment selection in QA over hybrid data},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An in-depth study on key nodes in social networks.
<em>IDA</em>, <em>27</em>(6), 1811–1838. (<a
href="https://doi.org/10.3233/IDA-227018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social network analysis, identifying the important nodes (key nodes) is a significant task in various applications. There are three most popular related tasks named influential node ranking, influence maximization, and network dismantling. Although these studies are different due to their own mo tivation, they share many similarities, which could confuse the non-domain readers and users. Moreover, few studies have explored the correlations between key nodes obtained from different tasks, hindering our further understanding of social networks. In this paper, we contribute to the field by conducting an in-depth survey of different kinds of key nodes through comparing these key nodes under our proposed framework and revealing their deep relationships. First, we clarify and formalize three existing popular studies under a uniform standard. Then we collect a group of crucial metrics and propose a fair comparison framework to analyze the features of key nodes identified by different research fields. From a large number of experiments and deep analysis on twenty real-world datasets, we not only explore correlations between key nodes derived from the three popular tasks, but also summarize insightful conclusions that explain how key nodes differ from each other and reveal their unique features for the corresponding tasks. Furthermore, we show that Shapley centrality could identify key nodes with more generality, and these nodes could also be applied to the three popular tasks simultaneously to a certain extent.},
  archive      = {J_IDA},
  author       = {Sun, Chengcheng and Wang, Zhixiao and Rui, Xiaobin and Yu, Philip S. and Sun, Lichao},
  doi          = {10.3233/IDA-227018},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1811-1838},
  shortjournal = {Intell. Data Anal.},
  title        = {An in-depth study on key nodes in social networks},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFF-SC: A multi-feature fusion method for smart contract
classification. <em>IDA</em>, <em>27</em>(6), 1781–1810. (<a
href="https://doi.org/10.3233/IDA-227186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of the smart contract can effectively reduce the search space and improve retrieval efficiency. The existing classification methods are based on natural language processing technologies. Because the processing of source code by these technologies lacks extraction and processing i n the software engineering field, there is still a lot of room for improvement in their methods of feature extraction. Therefore, this paper proposes a multi-feature fusion method for smart contract classification (MFF-SC) based on the code processing technology. From the source code perspective, source code processing method and attention mechanism are used to extract local code features. Structure-based traversal method are used to extract global code features from abstract syntax tree. Local and global code features introduce attention mechanism to generate code semantic features. From the perspective of account transaction, the feature of account transaction is extracted by using TransR. Next, the code semantic features and account transaction features generate smart contract semantic features by an attention mechanism. Finally, the smart contract semantic features are fed into a stacked denoising autoencoder and a softmax classifier for classification. Experimental results on a real dataset show that MFF-SC achieves an accuracy rate of 83.9%, compared with other baselines and variants.},
  archive      = {J_IDA},
  author       = {Tian, Gang and Wang, Xiaojin and Wang, Rui and Yu, Qiuyue and Zhao, Guangxin},
  doi          = {10.3233/IDA-227186},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1781-1810},
  shortjournal = {Intell. Data Anal.},
  title        = {MFF-SC: A multi-feature fusion method for smart contract classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of deep transfer learning approaches in
detecting and classifying brain tumor from magnetic resonance images.
<em>IDA</em>, <em>27</em>(6), 1759–1780. (<a
href="https://doi.org/10.3233/IDA-227321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Central Nervous System (CNS) is one of the most crucial parts of the human body. Brain tumor is one of the deadliest diseases that affect CNS and they should be detected earlier to avoid serious health implications. As it is one of the most dangerous types of cancer, its diagnosis is a crucial part of the healthcare sector. A brain tumor can be malignant or benign and its grade recognition is a tedious task for the radiologist. In the recent past, researchers have proposed various automatic detection and classification techniques that use different imaging modalities focusing on increased accuracy. In this paper, we have done an in-depth study of 19 different trained deep learning models like Alexnet, VGGnet, DarkNet, DenseNet, ResNet, InceptionNet, ShuffleNet, NasNet and their variants for the detection of brain tumors using deep transfer learning. The performance parameters show that NASNet-Large is outperforming others with an accuracy of 98.03% for detection and 97.87% for classification. The thresholding algorithm is used for segmenting out the tumor region if the detected output is other than normal.},
  archive      = {J_IDA},
  author       = {Deepa, P.L. and Narain, P.D. and Sreena, V.G.},
  doi          = {10.3233/IDA-227321},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1759-1780},
  shortjournal = {Intell. Data Anal.},
  title        = {Performance analysis of deep transfer learning approaches in detecting and classifying brain tumor from magnetic resonance images},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight vision transformer with symmetric modules for
vision tasks. <em>IDA</em>, <em>27</em>(6), 1741–1757. (<a
href="https://doi.org/10.3233/IDA-227205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based networks have demonstrated their powerful performance in various vision tasks. However, these transformer-based networks are heavyweight and cannot be applied to edge computing (mobile) devices. Despite that the lightweight transformer network has emerged, several problems remain, i.e., weak feature extraction ability, feature redundancy, and lack of convolutional inductive bias. To address these three problems, we propose a lightweight visual transformer (Symmetric Former, SFormer), which contains two novel modules (Symmetric Block and Symmetric FFN). Specifically, we design Symmetric Block to expand feature capacity inside the module and enhance the long-range modeling capability of attention mechanism. To increase the compactness of the model and introduce inductive bias, we introduce convolutional cheap operations to design Symmetric FFN. We compared the SFormer with existing lightweight transformers on several vision tasks. Remarkably, on the image recognition task of ImageNet [13], SFormer gains 1.2% and 1.6% accuracy improvements compared to PVTv2-b0 and Swin Transformer, respectively. On the semantic segmentation task of ADE20K [64], SFormer delivers performance improvements of 0.2% and 0.7% compared to PVTv2-b0 and Swin Transformer, respectively. On the cityscapes dataset [11], SFormer delivers performance improvements of 2.5% and 4.2% compared to PVTv2-b0 and Swin Transformer, respectively. The code is open-source and available at: https://github.com/ISCLab-Bistu/Symmetric_Former.git.},
  archive      = {J_IDA},
  author       = {Liang, Shengjun and Yu, Mingxin and Lu, Wenshuai and Ji, Xinglong and Tang, Xiongxin and Liu, Xiaolin and You, Rui},
  doi          = {10.3233/IDA-227205},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1741-1757},
  shortjournal = {Intell. Data Anal.},
  title        = {A lightweight vision transformer with symmetric modules for vision tasks},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small object detection based on attention mechanism and
enhanced network. <em>IDA</em>, <em>27</em>(6), 1725–1739. (<a
href="https://doi.org/10.3233/IDA-227154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection has a broad application prospect in image processing of unmanned aerial vehicles, autopilot and remote sensing. However, some difficulties exactly exist in small object detection, such as aggregation, occlusion and insufficient feature extraction, resulting in a great challen ge for small object detection. In this paper, we propose an improved algorithm for small object detection to address these issues. By using the spatial pyramid to extract multi-scale spatial features and by applying the multi-scale channel attention to capture the global and local semantic features, the spatial pooling pyramid and multi-scale channel attention module (SPP-MSCAM) is constructed. More importantly, the fusion of the shallower layer with higher resolution and a deeper layer with more semantic information is introduced to the neck structure for improving the sensitivity of small object features. A large number of experiments on the VisDrone2019 dataset and the NWPU VHR-10 dataset show that the proposed method significantly improves the Precision, mAP and mAP50 compared to the YOLOv5 method. Meanwhile, it still preserves a considerable real-time performance. Undoubtedly, the improved network proposed in this paper can effectively alleviate the difficulties of aggregation, occlusion and insufficient feature extraction in small object detection, which would be helpful for its potential applications in the future.},
  archive      = {J_IDA},
  author       = {Wang, Bingbing and Zhang, Fengxiang and Li, Kaipeng and Shi, Kuijie and Wang, Lei and Liu, Gang},
  doi          = {10.3233/IDA-227154},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1725-1739},
  shortjournal = {Intell. Data Anal.},
  title        = {Small object detection based on attention mechanism and enhanced network},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning bayesian multinets from labeled and unlabeled data
for knowledge representation. <em>IDA</em>, <em>27</em>(6), 1699–1723.
(<a href="https://doi.org/10.3233/IDA-227068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian network classifiers (BNCs) learned from labeled training data are expected to generalize to fit unlabeled testing data based on the independent and identically distributed (i.i.d.) assumption, whereas the asymmetric independence assertion demonstrates the uncertainty of significance of dependency or independency relationships mined from data. A highly scalable BNC should form a distinct decision boundary that can be especially tailored to specific testing instance for knowledge representation. To address the issue of asymmetric independence assertion, in this paper we propose to learn k-dependence Bayesian multinet classifiers in the framework of multistage classification. By partitioning training set and pseudo training set according to high-confidence class labels, the dependency or independency relationships can be fully mined and represented in the topologies of the committee members. Extensive experimental results indicate that the proposed algorithm achieves competitive classification performance compared to single-topology BNCs (e.g., CFWNB, AIWNB and SKDB) and ensemble BNCs (e.g., WATAN, SA2DE, ATODE and SLB) in terms of zero-one loss, root mean square error (RMSE), Friedman test and Nemenyi test.},
  archive      = {J_IDA},
  author       = {Pang, Meng and Wang, Limin and Li, Qilong and Lu, Guo and Li, Kuo},
  doi          = {10.3233/IDA-227068},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1699-1723},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning bayesian multinets from labeled and unlabeled data for knowledge representation},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-instance multi-label learning algorithm based on
radial basis functions and multi-objective particle swarm optimization.
<em>IDA</em>, <em>27</em>(6), 1681–1698. (<a
href="https://doi.org/10.3233/IDA-227042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial basis function (RBF) neural networks for Multi-Instance Multi-Label (MIML) directly can exploit the connections between instances and labels so that they can preserve useful prior information, but they only adopt Gaussian radial basis function as their RBF whose parameters are difficult to d etermine. In this paper, parameters can be obtained by multi-objective optimization methods with multi performance measures treated as objectives, specifically, parameter estimation of different RBFs by an improved multi-objective particle swarm optimization (MOPSO) is proposed where Recall rate and Precision rate are chosen to obtain the most desirable Pareto optimal solution set. Furthermore, share-learning factor is proposed to modify the particle velocity in standard MOPSO to improve the global search ability and group cooperative ability. It is experimentally demonstrated that the proposed method can estimate the reliable parameters of different RBFs, and it is also very competitive with the state of art MIML methods.},
  archive      = {J_IDA},
  author       = {Bao, Xiang and Han, Fei and Ling, Qing-Hua and Ren, Yan-Qiong},
  doi          = {10.3233/IDA-227042},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1681-1698},
  shortjournal = {Intell. Data Anal.},
  title        = {A multi-instance multi-label learning algorithm based on radial basis functions and multi-objective particle swarm optimization},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting scatter matrix on one-class support vector
machine based on low variance direction. <em>IDA</em>, <em>27</em>(6),
1663–1679. (<a href="https://doi.org/10.3233/IDA-227036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When building a performing one-class classifier, the low variance direction of the training data set might provide important information. The low variance direction of the training data set improves the Covariance-guided One-Class Support Vector Machine (COSVM), resulting in better accuracy. Howeve r, this classifier does not use data dispersion in the one class. It explicitly does not make use of target class subclass information. As a solution, we propose Scatter Covariance-guided One-Class Support Vector Machine, a novel variation of the COSVM classifier (SC-OSVM). In the kernel space, our approach makes use of subclass information to jointly decrease dispersion. Our algorithm technique is even based on a convex optimization problem that can be efficiently solved using standard numerical methods. A comparison of artificial and real-world data sets shows that SC-OSVM provides more efficient and robust solutions than normal COSVM and other contemporary one-class classifiers.},
  archive      = {J_IDA},
  author       = {Nheri, Soumaya and Ksantini, Riadh and Kaâniche, Mohamed Bécha and Bouhoula, Adel},
  doi          = {10.3233/IDA-227036},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1663-1679},
  shortjournal = {Intell. Data Anal.},
  title        = {Exploiting scatter matrix on one-class support vector machine based on low variance direction},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental density clustering framework based on dynamic
microlocal clusters. <em>IDA</em>, <em>27</em>(6), 1637–1661. (<a
href="https://doi.org/10.3233/IDA-227263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevailing development of the internet and sensors, various streaming raw data are generated continually. However, traditional clustering algorithms are unfavorable for discovering the underlying patterns of incremental data in time; clustering accuracy cannot be assured if fixed parameter s clustering algorithms are used to handle incremental data. In this paper, an Incremental-Density-Micro-Clustering (IDMC) framework is proposed to address this concern. To reduce the succeeding clustering computation, we design the Dynamic-microlocal-clustering method to merge samples from streaming data into dynamic microlocal clusters. Beyond that, the Density-center-based neighborhood search method is proposed for periodically merging microlocal clusters to global clusters automatically; at the same time, these global clusters are updated by the Dynamic-cluster-increasing method with data streaming in each period. In this way, IDMC processes sensor data with less computational time and memory, improves the clustering performance, and simplifies the parameter choosing in conventional and stream data clustering. Finally, experiments are conducted to validate the proposed clustering framework on UCI datasets and streaming data generated by IoT sensors. As a result, this work advances the state-of-the-art of incremental clustering algorithms in the field of sensors’ streaming data analysis.},
  archive      = {J_IDA},
  author       = {Zhang, Tao and Li, Decai and Dong, Jingya and He, Yuqing and Chang, Yanchun},
  doi          = {10.3233/IDA-227263},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1637-1661},
  shortjournal = {Intell. Data Anal.},
  title        = {Incremental density clustering framework based on dynamic microlocal clusters},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SARW: Similarity-aware random walk for GCN. <em>IDA</em>,
<em>27</em>(6), 1615–1636. (<a
href="https://doi.org/10.3233/IDA-227085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) is an important method for learning graph representations of nodes. For large-scale graphs, the GCN could meet with the neighborhood expansion phenomenon, which makes the model complexity high and the training time long. An efficient solution is to adopt graph samp ling techniques, such as node sampling and random walk sampling. However, the existing sampling methods still suffer from aggregating too many neighbor nodes and ignoring node feature information. Therefore, in this paper, we propose a new subgraph sampling method, namely, Similarity-Aware Random Walk (SARW), for GCN with large-scale graphs. A novel similarity index between two adjacent nodes is proposed, describing the relationship of nodes with their neighbors. Then, we design a sampling probability expression between adjacent nodes using node feature information, degree information, neighbor set information, etc. Moreover, we prove the unbiasedness of the SARW-based GCN model for node representations. The simplified version of SARW (SSARW) has a much smaller variance, which indicates the effectiveness of our subgraph sampling method in large-scale graphs for GCN learning. Experiments on six datasets show our method achieves superior performance over the state-of-the-art graph sampling approaches for the large-scale graph node classification task.},
  archive      = {J_IDA},
  author       = {Hou, Linlin and Zhang, Haixiang and Hou, Qing-Hu and Guo, Alan J.X. and Wu, Ou and Yu, Ting and Zhang, Ji},
  doi          = {10.3233/IDA-227085},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1615-1636},
  shortjournal = {Intell. Data Anal.},
  title        = {SARW: Similarity-aware random walk for GCN},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous information fusion based graph collaborative
filtering recommendation. <em>IDA</em>, <em>27</em>(6), 1595–1613. (<a
href="https://doi.org/10.3233/IDA-227025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the application of 5G, graph-based recommendation algorithms have become a research hotspot. Graph neural networks encode the graph structure information in the node representation through an iterative neighbor aggregation method, which can effectively alleviate the problem of data s parsity. In addition, more and more information graph can be used in collaborative filtering recommendation, such as user social information graph, user or item attributed information graph, etc. In this paper, we propose a novel heterogeneous information fusion based graph collaborative filtering method, which models graph data from different heterogeneous graph, and combines them together to enhance presentation learning. Through information propagation and aggregation, our model can learn the latent embeddings effectively and enhance the performance of recommendation. Experimental results on different datasets validate the outperformance of the proposed framework.},
  archive      = {J_IDA},
  author       = {Mu, Ruihui and Zeng, Xiaoqin and Zhang, Jiying},
  doi          = {10.3233/IDA-227025},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1595-1613},
  shortjournal = {Intell. Data Anal.},
  title        = {Heterogeneous information fusion based graph collaborative filtering recommendation},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HSNF: Hybrid sampling with two-step noise filtering for
imbalanced data classification. <em>IDA</em>, <em>27</em>(6), 1573–1593.
(<a href="https://doi.org/10.3233/IDA-227111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification has received much attention in machine learning, and many oversampling methods exist to solve this problem. However, these methods may suffer from insufficient noise filtering, overlap between synthetic and original samples, etc., resulting in degradation of classific ation performance. To this end, we propose a hybrid sampling with two-step noise filtering (HSNF) method in this paper, which consists of three modules. In the first module, HSNF denoises twice according to different noise discrimination mechanisms. Note that denoising mechanism is essentially based on the Euclidean distance between samples. Then in the second module, the minority class samples are divided into two categories, boundary samples and safe samples, respectively, and a portion of the boundary majority class samples are removed. In the third module, different oversampling methods are used to synthesize instances for boundary minority class samples and safe minority class samples. Experimental results on synthetic data and benchmark datasets demonstrate the effectiveness of HSNF in comparison with several popular methods. The code of HSNF will be released.},
  archive      = {J_IDA},
  author       = {Duan, Lilong and Xue, Wei and Gu, Xiaolei and Luo, Xiao and He, Yongsheng},
  doi          = {10.3233/IDA-227111},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1573-1593},
  shortjournal = {Intell. Data Anal.},
  title        = {HSNF: Hybrid sampling with two-step noise filtering for imbalanced data classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resformer: Combine quadratic linear transformation with
efficient sparse transformer for long-term series forecasting.
<em>IDA</em>, <em>27</em>(6), 1557–1572. (<a
href="https://doi.org/10.3233/IDA-227006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of deep learning, long sequence time-series forecasting (LSTF) has attracted more and more attention in power consumption prediction, traffic prediction and stock prediction. In recent studies, various improved models of Transformer are favored. While these models ha ve made breakthroughs in reducing the time and space complexity of Transformer, there are still some problems, such as the predictive power of the improved model being slightly lower than that of Transformer. And these models ignore the importance of special values in the time series. To solve these problems, we designed a more concise network named Resformer, which has four significant characteristics: (1) The fully sparse self-attention mechanism achieves O⁢(𝐿𝑙𝑜𝑔𝐿) time complexity. (2) The AMS module is used to process the special values of time series and has comparable performance on sequences dependency alignment. (3) Using quadratic linear transformation, a simple LT module is designed to replace the self-attention mechanism. It effectively reduces redundant information. (4) The DistPooling method based on data distribution is proposed to suppress redundant information and noise. A large number of experiments on real data sets show that the Resformer method is superior to the existing improved model and standard Transformer method.},
  archive      = {J_IDA},
  author       = {Chen, Gongguan and Wang, Hua and Liu, Yepeng and Zhang, Mingli and Zhang, Fan},
  doi          = {10.3233/IDA-227006},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1557-1572},
  shortjournal = {Intell. Data Anal.},
  title        = {Resformer: Combine quadratic linear transformation with efficient sparse transformer for long-term series forecasting},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Editorial. <em>IDA</em>, <em>27</em>(6), 1555–1556. (<a
href="https://doi.org/10.3233/IDA-239007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-239007},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1555-1556},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of multi-size peach in orchard using RGB-d camera
combined with an improved DEtection transformer model. <em>IDA</em>,
<em>27</em>(5), 1539–1554. (<a
href="https://doi.org/10.3233/IDA-220449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first major contribution of the paper is the proposal of using an improved DEtection Transformer network (named R2N-DETR) and Kinect-V2 camera for detecting multiple-size peaches under orchards with varied illumination and fruit occlusion. R2N-DETR model first employed Res2Net-50 to extract a f used low-high level feature map containing fine spatial features and precise semantic information of multi-size peaches from Red-Green-Blue-Depth (RGB-D) images. Second, the encoder-decoder was performed on the feature map to obtain the global context. Finally, all detected objects were detected according to each object’s global context. For the detection of 1101 RGB-D images (imaged from two orchards over three years), the R2N-DETR model achieves an average precision of 0.944 and an average detecting time of 53 ms for each image. The developed system could provide precise visual guidance for robotic picking and contribute to improving yield prediction by providing accurate fruit counting.},
  archive      = {J_IDA},
  author       = {Yang, Yu and Wang, Xin and Liu, Zhenfang and Huang, Min and Sun, Shangpeng and Zhu, Qibing},
  doi          = {10.3233/IDA-220449},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1539-1554},
  shortjournal = {Intell. Data Anal.},
  title        = {Detection of multi-size peach in orchard using RGB-D camera combined with an improved DEtection transformer model},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel deep learning framework for the identification of
tortuous vessels in plus diseased infant retinal images. <em>IDA</em>,
<em>27</em>(5), 1523–1537. (<a
href="https://doi.org/10.3233/IDA-220451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinopathy of prematurity (ROP), sometimes known as Terry syndrome, is an ophthalmic condition that affects premature babies. It is the main cause of childhood blindness and morbidity of vision throughout life. ROP frequently coexists with a disease stage known as Plus disease, which is marked by severe tortuosity and dilated retinal blood vessels. The goal of this research is to create a diagnostic technique that can discriminate between infants with Plus disease from healthy subjects. Blood vascular tortuosity is used as a prognostic indicator for the diagnosis. We examine the quantification of retinal blood vessel tortuosity and propose a computer-aided diagnosis system that can be used as a tool for ROP identification. Deep neural networks are used in the proposed approach to segment retinal blood vessels, which is followed by the prediction of tortuous vessel pixels in the segmented vessel map. Digital fundus images obtained from Retcam3TM is used for screening. We use a proprietary data set of 289 infant retinal images (89 with Plus disease and 200 healthy) from Narayana Nethralaya in Bangalore, India, to illustrate the efficacy of our methodology. The findings of this study demonstrate the reliability of the proposed method as a computer-aided diagnostic tool that can help medical professionals make an early diagnosis of ROP.},
  archive      = {J_IDA},
  author       = {Ramachandran, Sivakumar},
  doi          = {10.3233/IDA-220451},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1523-1537},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel deep learning framework for the identification of tortuous vessels in plus diseased infant retinal images},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid quantum-behaved particle swarm optimization
solution to non-convex economic load dispatch with multiple fuel types
and valve-point effects. <em>IDA</em>, <em>27</em>(5), 1503–1522. (<a
href="https://doi.org/10.3233/IDA-220415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic dispatch problems (EDPs) can be reduced to non-convex constrained optimization problems, and most of the population-based algorithms are prone to have problems of premature and falling into local optimum when solving EDPs. Therefore, this paper proposes a hybrid quantum-behaved particle sw arm optimization (HQPSO) algorithm to alleviate the above problems. In the HQPSO, the Solis and Wets local search method is used to enhance the local search ability of the QPSO so that the algorithm can find solutions that is close to optimal when the constraints are met, and two evolution operators are proposed and incorporated for the purpose of making a better balance between local search and global search abilities at the later search stage. The performance comparison is made among the HQPSO and the other ten population-based random search methods under two different experimental configurations and four different power systems in terms of solution quality, robustness, and convergence property. The experimental results show that the HQPSO improves the convergence properties of the QPSO and finally obtains the best total generation cost without violating any constraints. In addition, the HQPSO outperforms all the other algorithms on 7 cases of all 8 experimental cases in terms of global best position and mean position, which verifies the effectiveness of the algorithm.},
  archive      = {J_IDA},
  author       = {Chen, Qidong and Jun, Sun and Palade, Vasile},
  doi          = {10.3233/IDA-220415},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1503-1522},
  shortjournal = {Intell. Data Anal.},
  title        = {A hybrid quantum-behaved particle swarm optimization solution to non-convex economic load dispatch with multiple fuel types and valve-point effects},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial unsupervised domain adaptation based on
generative adversarial network for stock trend forecasting.
<em>IDA</em>, <em>27</em>(5), 1477–1502. (<a
href="https://doi.org/10.3233/IDA-220414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trend forecasting, which refers to the prediction of the rise and fall of the next day’s stock price, is a promising research field in financial time series forecasting, with a large quantity of well-performing algorithms and models being proposed. However, most of the studies focus on trend prediction for stocks with a large number of samples, while the trend prediction problem of newly listed stocks with only a small number of samples is neglected. In this work, we innovatively design a solution to the Small Sample Size (SSS) trend prediction problem of newly listed stocks. Traditional Machine Learning (ML) and Deep Learning (DL) techniques are based on the assumption that the available labeled samples are substantial, which is invalid for SSS trend prediction of newly listed stocks. In order to break out of this dilemma, we propose a novel Adversarial Unsupervised Domain Adaptation Network (AUDA-Net), based on Generative Adversarial Network (GAN), ad hoc for SSS stock trend forecasting. Different from the traditional domain adaptation algorithms, we employ a GAN model, which is trained on basis of the target stock dataset, to effectively solve the absence problem of available samples. Notably, AUDA-Net can reasonably and successfully transfer the knowledge learned from the source stock dataset to the newly listed stocks with only a few samples. The stock trend forecasting performance of our proposed AUDA-Net model has been verified through extensive experiments conducted on several real stock datasets of the U.S. stock market. Using stock trend forecasting as a case study, we show that the SSS forecasting results produced by AUDA-Net are favorably comparable to the state-of-the-art.},
  archive      = {J_IDA},
  author       = {Wei, Qiheng and Dai, Qun},
  doi          = {10.3233/IDA-220414},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1477-1502},
  shortjournal = {Intell. Data Anal.},
  title        = {Adversarial unsupervised domain adaptation based on generative adversarial network for stock trend forecasting},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-layer multi-view stacking model for credit risk
assessment. <em>IDA</em>, <em>27</em>(5), 1457–1475. (<a
href="https://doi.org/10.3233/IDA-220403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment plays a key role in determining the banking policies and commercial strategies of financial institutions. Ensemble learning approaches have been validated to be more competitive than individual classifiers and statistical techniques for default prediction. However, most resea rches focused on improving overall prediction accuracy rather than improving the identification of actual defaulted loans. In addition, model interpretability has not been paid enough attention in previous studies. To fill up these gaps, we propose a Multi-layer Multi-view Stacking Integration (MLMVS) approach to predict default risk in the P2P lending scenario. As the main innovation, our proposal explores multi-view learning and soft probability outputs to produce multi-layer integration based on stacking. An interpretable artificial intelligence tool LIME is embedded for interpreting the prediction results. We perform a comprehensive analysis of MLMVS on the Lending Club dataset and conduct comparative experiments to compare it with a number of well-known individual classifiers and ensemble classification methods, which demonstrate the superiority of MLMVS.},
  archive      = {J_IDA},
  author       = {Han, Wenfang and Gu, Xiao and Jian, Ling},
  doi          = {10.3233/IDA-220403},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1457-1475},
  shortjournal = {Intell. Data Anal.},
  title        = {A multi-layer multi-view stacking model for credit risk assessment},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A memetic-based technical indicator portfolio and parameters
optimization approach for finding trading signals to construct
transaction robot in smart city era. <em>IDA</em>, <em>27</em>(5),
1433–1456. (<a href="https://doi.org/10.3233/IDA-220755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of smart cities, the demand for personal financial services is becoming more and more importance, and personal investment suggestion is one of them. A common way to reach the goal is using a technical indicator to form trading strategy to find trading signals as trading suggest ion. However, using only a technical indicator has its limitations, a technical indicator portfolio is further utilized to generate trading signals for achieving risk aversion. To provide a more reliable trading signals, in this paper, we propose an optimization algorithm for obtaining a technical indicator portfolio and its parameters for predicting trends of target stock by using the memetic algorithm. In the proposed approach, the genetic algorithm (GA) and simulated annealing (SA) algorithm are utilized for global and local search. In global search, a technical indicator portfolio and its parameters are first encoded into a chromosome using a bit string and real numbers. Then, the initial population is generated based on the encoding scheme. Fitness value of a chromosome is evaluated by the return and risk according to the generated trading signals. In local search, SA is employed to tune parameters of indicators in chromosomes. After that, the genetic operators are continue employed to generate new offspring. Finally, the chromosome with the highest fitness value could be provided to construct transaction robot for making investment plans in smart city environment. Experiments on three real datasets with different trends were made to show the effectiveness of the proposed approach, including uptrend, consolidation, and downtrend. The total returns of them on testing datasets are 26.53% 33.48%, and 9.7% that indicate the proposed approach can not only reach risk aversion in downtrends but also have good returns in others.},
  archive      = {J_IDA},
  author       = {Chen, C.H. and Hung, S.T. and Chen, P.T. and Wang, C.S. and Chiang, R.D.},
  doi          = {10.3233/IDA-220755},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1433-1456},
  shortjournal = {Intell. Data Anal.},
  title        = {A memetic-based technical indicator portfolio and parameters optimization approach for finding trading signals to construct transaction robot in smart city era},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of an energy efficient dynamic virtual machine
consolidation model for smart cities in urban areas. <em>IDA</em>,
<em>27</em>(5), 1409–1431. (<a
href="https://doi.org/10.3233/IDA-220754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing smart cities in urban areas are becoming more intelligent day by day. Massive storage and high computational resources are required to provide smart services in urban areas. It can be provided through intelligence cloud computing. The establishment of large-scale cloud data centres is r apidly increasing to provide utility-based services in urban areas. Enormous energy consumption of data centres has a destructive effect on the environment. Due to the enormous energy consumption of data centres, a massive amount of greenhouse gases (GHG) are emitted into the environment. Virtual Machine (VM) consolidation can enable energy efficiency to reduce energy consumption of cloud data centres. The reduce energy consumption can increase the Service Level Agreement (SLA) violation. Therefore, in this research, an energy-efficient dynamic VM consolidation model has been proposed to reduce the energy consumption of cloud data centres and curb SLA violations. Novel algorithms have been proposed to accomplish the VM consolidation. A new status of any host called an almost overload host has been introduce, and determined by a novel algorithm based on the Naive Bayes Classifier Machine Learning (ML) model. A new algorithm based on the exponential binary search is proposed to perform the VM selection. Finally, a new Modified Power-Aware Best Fit Decreasing (MPABFD) VM allocation policy is proposed to allocate all VMs. The proposed model has been compared with certain well-known baseline algorithms. The comparison exhibits that the proposed model improves the energy consumption by 25% and SLA violation by 87%.},
  archive      = {J_IDA},
  author       = {Biswas, Nirmal Kr. and Banerjee, Sourav and Ghosh, Uttam and Biswas, Utpal},
  doi          = {10.3233/IDA-220754},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1409-1431},
  shortjournal = {Intell. Data Anal.},
  title        = {Design of an energy efficient dynamic virtual machine consolidation model for smart cities in urban areas},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast and distributed c4.5 algorithm for urban big data.
<em>IDA</em>, <em>27</em>(5), 1379–1408. (<a
href="https://doi.org/10.3233/IDA-220753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of information nowadays is rapidly growing. Aside from valuable information, information that is unrelated to a target or is meaningless is also growing. Big data and broader digital technologies are considered the primary components of smart city governance and planning. Big data analys is is considered to define a new era in urban planning, research, and policy. Effective data mining and pattern detection techniques are becoming very important these days. Processing such a large amount of data entails the use of data mining, a technique that clarifies the association between valid information and excludes irrelevant data to implement a practical decision tree. A large amount of data affects processing time and I/O costs during data mining. This study proposes to distribute data among multiple clients and distribute a large amount of data computation equally to improve the resource cost problem of exploration. Following that, the main server consolidates the computation results and generates the survey results. Experiment results show that the proposed algorithm is superior, thus allowing a larger amount of data to be processed while producing high-quality results.},
  archive      = {J_IDA},
  author       = {Cheng, Wan-Shu and Huang, Peng-Yu and Huang, Jheng-Yu and Chen, Ju-Chin and Lin, Kawuu W.},
  doi          = {10.3233/IDA-220753},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1379-1408},
  shortjournal = {Intell. Data Anal.},
  title        = {A fast and distributed c4.5 algorithm for urban big data},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining skyline frequent-utility patterns from big data
environment based on MapReduce framework. <em>IDA</em>, <em>27</em>(5),
1359–1377. (<a href="https://doi.org/10.3233/IDA-220756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the concentration focuses on data mining, frequent itemset mining (FIM) and high-utility itemset mining (HUIM) are commonly addressed and researched. Many related algorithms are proposed to reveal the general relationship between utility, frequency, and items in transaction databases. Although these algorithms can mine FIMs or HUIMs quickly, these algorithms merely take into account frequency or utility as a unilateral criterion for itemsets but the other factors (e.g., distance, price) could be also valuable for decision-making. A new skyline framework has been presented to mine frequent high utility patterns (SFUPs) to better support user decision-making. Several new algorithms have been proposed one after another. However, the Internet of Things (IoT), mobile Internet, and traditional Internet are generating massive amounts of data every day, and these cutting-edge standalone algorithms can not satisfy the new challenge of finding interesting patterns from this data. Big Data uses a distributed architecture in the form of cloud computing to filter and process this data to extract useful information. This paper proposes a novel parallel algorithm on Hadoop as a three-stage iterative algorithm based on MapReduce. MapReduce is used to divide the mining tasks of the whole large data set into multiple independent sub-tasks to find frequent and high utility patterns in parallel. Numerous experiments were done in this paper, and from the results, the algorithm can handle large datasets and show good performance on Hadoop clusters.},
  archive      = {J_IDA},
  author       = {Wu, Jimmy Ming-Tai and Li, Ranran and Wu, Mu-En and Lin, Jerry Chun-Wei},
  doi          = {10.3233/IDA-220756},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1359-1377},
  shortjournal = {Intell. Data Anal.},
  title        = {Mining skyline frequent-utility patterns from big data environment based on MapReduce framework},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NonPC: Non-parametric clustering algorithm with adaptive
noise detecting. <em>IDA</em>, <em>27</em>(5), 1347–1358. (<a
href="https://doi.org/10.3233/IDA-220427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based clustering performs efficiently for identifying clusters in local and nonlinear data Patterns. The existing methods face the problem of parameter selection, such as the setting of k of the k-nearest neighbor graph and the threshold in noise detection. In this paper, a non-parametric clu stering algorithm (NonPC) is proposed to tackle those inherent limitations and improve clustering performance. The weighted natural neighbor graph (wNaNG) is developed to represent the given data without any prior knowledge. What is more, the proposed NonPC method adaptively detects noise data in an unsupervised way based on some attributes extracted from wNaNG. The algorithm works without preliminary parameter settings while automatically identifying clusters with unbalanced densities, arbitrary shapes, and noises. To assess the advantages of the NonPC algorithm, extensive experiments have been conducted compared with some classic and recent clustering methods. The results demonstrate that the proposed NonPC algorithm significantly outperforms the state-of-the-art and well-known algorithms in Adjusted Rand index, Normalized Mutual Information, and Fowlkes-Mallows index aspects.},
  archive      = {J_IDA},
  author       = {Li, Lin and Chen, Xiang and Song, Chengyun},
  doi          = {10.3233/IDA-220427},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1347-1358},
  shortjournal = {Intell. Data Anal.},
  title        = {NonPC: Non-parametric clustering algorithm with adaptive noise detecting},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid classification method via keywords screening and
attention mechanisms in extreme short text. <em>IDA</em>,
<em>27</em>(5), 1331–1345. (<a
href="https://doi.org/10.3233/IDA-220417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text classification has provoked a vast amount of attention and research in recent decades. However, most existing methods only focus on the short texts that contain dozens of words like Twitter and Microblog, while pay far less attention to the extreme short texts like news headline and sear ch snippets. Meanwhile, contemporary short text classification methods that extend the features via external knowledge sources always introduce lots of useless concepts, which may be detrimental to classification performance. Moreover, unlike traditional short text classification methods, the classification results of extreme short texts are often determined by a few even one or two keywords. To address these problems, we propose a novel hybrid classification method via Keywords Screening and Attention Mechanisms in extreme short text, called KSAM. More specifically, firstly, the attention-based BiLSTM is introduced in our method to enhance the role of keywords. Secondly, we screen the keywords in the extreme short text for obtaining the true class label, and the concepts concerning the keywords are retrieved from external open knowledge sources like DBpedia. Thirdly, the attention mechanisms are introduced to acquire the weight of these retrieved concepts. Finally, conceptual information is utilized to assist the classification of the extreme short text. Extensive experiments have demonstrated the effectiveness of our method compared to other state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Zhou, Xinke and Zhu, Yi and Li, Yun and Qiang, Jipeng and Yuan, Yunhao and Wu, Xingdong and Zhang, Runmei},
  doi          = {10.3233/IDA-220417},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1331-1345},
  shortjournal = {Intell. Data Anal.},
  title        = {A hybrid classification method via keywords screening and attention mechanisms in extreme short text},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data management scheme for building internet of things based
on blockchain sharding. <em>IDA</em>, <em>27</em>(5), 1309–1330. (<a
href="https://doi.org/10.3233/IDA-220757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important part of digital building, building internet of things (BIoT) plays a positive role in promoting the construction of smart cities. Existing schemes utilize blockchain to achieve trusted data storage in BIoT. However, the full-copy storage mechanism of blockchain and the management re quirements of massive data have brought computing and storage challenges to edge nodes with limited resources. Therefore, a data management scheme for BIoT based on blockchain sharding is proposed. The scheme proposes a hybrid storage mechanism, which uses inter-planetary file system (IPFS) to ensure the integrity and availability of data outside the chain, and reduces the storage overhead of edge nodes. Based on the hybrid storage mechanism, the sharding algorithm is designed to divide the blockchain into multiple shards, and the storage overhead and computing overhead are offloaded to each shard, which effectively balances the computing and storage overhead of edge nodes. Finally, comparative analysis was made with existing schemes, and effectiveness of proposed scheme was verified from the perspectives of storage overhead, computation overhead, access delay and throughput. Results show that proposed scheme can effectively reduce storage overhead and computing overhead of edge nodes in BIoT scenario.},
  archive      = {J_IDA},
  author       = {Wang, Xu and Zheng, Wenhu and Wang, Jinlong and Xiong, Xiaoyun and Shen, Yumin and Mu, Wei and Fan, Zengliang},
  doi          = {10.3233/IDA-220757},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1309-1330},
  shortjournal = {Intell. Data Anal.},
  title        = {Data management scheme for building internet of things based on blockchain sharding},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Oversampling method based on GAN for tabular binary
classification problems. <em>IDA</em>, <em>27</em>(5), 1287–1308. (<a
href="https://doi.org/10.3233/IDA-220383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-imbalanced problems are present in many applications. A big gap in the number of samples in different classes induces classifiers to skew to the majority class and thus diminish the performance of learning and quality of obtained results. Most data level imbalanced learning approaches generate new samples only using the information associated with the minority samples through linearly generating or data distribution fitting. Different from these algorithms, we propose a novel oversampling method based on generative adversarial networks (GANs), named OS-GAN. In this method, GAN is assigned to learn the distribution characteristics of the minority class from some selected majority samples but not random noise. As a result, samples released by the trained generator carry information of both majority and minority classes. Furthermore, the central regularization makes the distribution of all synthetic samples not restricted to the domain of the minority class, which can improve the generalization of learning models or algorithms. Experimental results reported on 14 datasets and one high-dimensional dataset show that OS-GAN outperforms 14 commonly used resampling techniques in terms of G-mean, accuracy and F1-score.},
  archive      = {J_IDA},
  author       = {Yang, Jie and Jiang, Zhenhao and Pan, Tingting and Chen, Yueqi and Pedrycz, Witold},
  doi          = {10.3233/IDA-220383},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1287-1308},
  shortjournal = {Intell. Data Anal.},
  title        = {Oversampling method based on GAN for tabular binary classification problems},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved k-NN anomaly detection framework based on
locality sensitive hashing for edge computing environment. <em>IDA</em>,
<em>27</em>(5), 1267–1285. (<a
href="https://doi.org/10.3233/IDA-216461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large deployment of wireless sensor networks in various fields bring great benefits. With the increasing volume of sensor data, traditional data collection and processing schemes gradually become unable to meet the requirements in actual scenarios. As data quality is vital to data mining and value extraction, this paper presents a distributed anomaly detection framework which combines cloud computing and edge computing. The framework consists of three major components: k-nearest neighbors, locality sensitive hashing, and cosine similarity. The traditional k-nearest neighbors algorithm is improved by locality sensitive hashing in terms of computation cost and processing time. An initial anomaly detection result is given by the combination of k-nearest neighbors and locality sensitive hashing. To further improve the accuracy of anomaly detection, a second test for anomaly is provided based on cosine similarity. Extensive experiments are conducted to evaluate the performance of our proposal. Six popular methods are used for comparison. Experimental results show that our model has advantages in the aspects of accuracy, delay, and energy consumption.},
  archive      = {J_IDA},
  author       = {Gao, Cong and Chen, Yuzhe and Chen, Yanping and Wang, Zhongmin and Xia, Hong},
  doi          = {10.3233/IDA-216461},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1267-1285},
  shortjournal = {Intell. Data Anal.},
  title        = {An improved k-NN anomaly detection framework based on locality sensitive hashing for edge computing environment},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic review on recommendation systems applied to
chronic diseases. <em>IDA</em>, <em>27</em>(5), 1223–1265. (<a
href="https://doi.org/10.3233/IDA-220394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large percentage of the worldwide population is affected by chronic diseases, leading to a burden of the patient and the national healthcare systems. Recommendation systems are used for the personalization of healthcare due to their capacity of performing predictive analyses based on the patient’ s clinical data. This systematic literature review presents four research questions to provide an overall state of the art of the use of recommendation systems applied to the healthcare of patients with chronic diseases. Disease management was identified as the main purpose of the systems proposed in the literature. However, few solutions provide support to physicians in the clinical decision-making. Ontologies and rule-based systems were the artificial intelligence techniques most used in the systems since they can easily implement clinical guidelines. Current challenges of these systems include the low adherence, data sparsity, heterogeneous data, and explainability, that affect the success of the recommendation system. The results also show that there are few systems that provide support to patients with multiple chronic conditions. The findings of this literature review should be considered in the development of future recommendation systems that aim to support the management of chronic diseases.},
  archive      = {J_IDA},
  author       = {Vieira, Ana and Carneiro, João and Novais, Paulo and Corchado, Juan and Marreiros, Goreti},
  doi          = {10.3233/IDA-220394},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1223-1265},
  shortjournal = {Intell. Data Anal.},
  title        = {A systematic review on recommendation systems applied to chronic diseases},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Editorial. <em>IDA</em>, <em>27</em>(5), 1221–1222. (<a
href="https://doi.org/10.3233/IDA-239005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-239005},
  journal      = {Intelligent Data Analysis},
  month        = {10},
  number       = {5},
  pages        = {1221-1222},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new chinese text clustering algorithm based on WRD and
improved k-means. <em>IDA</em>, <em>27</em>(4), 1205–1220. (<a
href="https://doi.org/10.3233/IDA-226652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text clustering has been widely used in data mining, document management, search engines, and other fields. The K-means algorithm is a representative algorithm of text clustering. However, traditional K-means algorithm often uses Euclidean distance or cosine distance to measure the similarity betwe en texts, which is not effective in face of high-dimensional data and cannot retain enough semantic information. In response to the above problems, we combine word rotator’s distance with the K-means algorithm, and propose the WRDK-means algorithm, which use word rotator’s distance to calculate the similarity between texts and preserve more text features. Furthermore, we define a new cluster center initialization method that improves cluster instability during random initial cluster center selection. And, to solve the problem of inconsistent length between texts, we propose a new iterative approximation method of cluster centers. We selected three suitable datasets and five evaluation indicators to verify the feasibility of the proposed algorithm. Among them, the RI value of our algorithm exceeds 90%. And for Marco_F1, our scheme was about 37.77%, 23.2%, 13.06% and 20.12% better than other four methods, respectively.},
  archive      = {J_IDA},
  author       = {Cui, Zicai and Zhong, Bocheng and Bai, Chen},
  doi          = {10.3233/IDA-226652},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1205-1220},
  shortjournal = {Intell. Data Anal.},
  title        = {A new chinese text clustering algorithm based on WRD and improved K-means},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discrete equilibrium optimization algorithm for breast
cancer diagnosis. <em>IDA</em>, <em>27</em>(4), 1185–1204. (<a
href="https://doi.org/10.3233/IDA-226665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illness diagnosis is the essential step in designating a treatment. Nowadays, Technological advancements in medical equipment can produce many features to describe breast cancer disease with more comprehensive and discriminant data. Based on the patient’s medical data, several data-driven models ar e proposed for breast cancer diagnosis using learning techniques such as naive Bayes, neural networks, and SVM. However, the models generated are hardly understandable, so doctors cannot interpret them. This work aims to study breast cancer diagnosis using the associative classification technique. It generates interpretable diagnosis models. In this work, an associative classification approach for breast cancer diagnosis based on the Discrete Equilibrium Optimization Algorithm (DEOA) named Discrete Equilibrium Optimization Algorithm for Associative Classification (DEOA-AC) is proposed. DEOA-AC aims to generate accurate and interpretable diagnosis rules directly from datasets. Firstly, all features in the dataset that contains continuous values are discretized. Secondly, for each class, a new dataset is created from the original dataset and contains only the chosen class’s instances. Finally, the new proposed DEOA is called for each new dataset to generate an optimal rule set. The DEOA-AC approach is evaluated on five well-known and recently used breast cancer datasets and compared with two recently proposed and three classical breast cancer diagnosis algorithms. The comparison results show that the proposed approach can generate more accurate and interpretable diagnosis models for breast cancer than other algorithms.},
  archive      = {J_IDA},
  author       = {Haouassi, Hichem and Mahdaoui, Rafik and Chouhal, Ouahiba},
  doi          = {10.3233/IDA-226665},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1185-1204},
  shortjournal = {Intell. Data Anal.},
  title        = {A discrete equilibrium optimization algorithm for breast cancer diagnosis},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning techniques for received signal strength
indicator prediction. <em>IDA</em>, <em>27</em>(4), 1167–1184. (<a
href="https://doi.org/10.3233/IDA-226750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advances made in wireless communication technology have led to efforts to improve the quality of reception, prevent poor connections and avoid disconnections between wireless and cellular devices. One of the most important steps toward preventing communication failures is to correctly estimate the received signal strength indicator (RSSI) of a wireless device. RSSI prediction is important for addressing various challenges such as localization, power control, link quality estimation, terminal connectivity estimation, and handover decisions. In this study, we compare different machine learning (ML) techniques that can be used to predict the received signal strength values of a device, given the received signal strength values of other devices in the region. We consider various ML methods, such as multi-layer ANN, K nearest neighbors, decision trees, random forest, and the K-means based method, for the prediction challenge. We checked the accuracy level of the learning process using a real dataset provided by a major national cellular operator. Our results show that the weighted K nearest neighbors algorithm, for K = 3 neighbors, achieved, on average, the most accurate RSSI predictions. We conclude that in environments where the size of data is relatively small, and data of close geographical points is available, a method that predicts the coverage of a point using the coverage near geographical points can be more successful and more accurate compared with other ML methods.},
  archive      = {J_IDA},
  author       = {Azoulay, Rina and Edery, Eliya and Haddad, Yoram and Rozenblit, Orit},
  doi          = {10.3233/IDA-226750},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1167-1184},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning techniques for received signal strength indicator prediction},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting the implicit independence assumption for learning
directed graphical models. <em>IDA</em>, <em>27</em>(4), 1143–1165. (<a
href="https://doi.org/10.3233/IDA-226806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network classifiers (BNCs) provide a sound formalism for representing probabilistic knowledge and reasoning with uncertainty. Explicit independence assumptions can effectively and efficiently reduce the size of the search space for solving the NP-complete problem of structure learning. Str ong conditional dependencies, when added to the network topology of BNC, can relax the independence assumptions, whereas the weak ones may result in biased estimates of conditional probability and degradation in generalization performance. In this paper, we propose an extension to the k-dependence Bayesian classifier (KDB) that achieves the bias/variance trade-off by verifying the rationality of implicit independence assumptions implicated. The informational and probabilistic dependency relationships represented in the learned robust topologies will be more appropriate for fitting labeled and unlabeled data, respectively. The comprehensive experimental results on 40 UCI datasets show that our proposed algorithm achieves competitive classification performance when compared to state-of-the-art BNC learners and their efficient variants in terms of zero-one loss, root mean square error (RMSE), bias and variance.},
  archive      = {J_IDA},
  author       = {Wang, Limin and Wei, Junyang and Li, Kuo and Zhou, Jiaping},
  doi          = {10.3233/IDA-226806},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1143-1165},
  shortjournal = {Intell. Data Anal.},
  title        = {Exploiting the implicit independence assumption for learning directed graphical models},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient intrusion detection method using federated
transfer learning and support vector machine with privacy-preserving.
<em>IDA</em>, <em>27</em>(4), 1121–1141. (<a
href="https://doi.org/10.3233/IDA-226617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, network security for organizations and individuals has become more and more important, and intrusion detection systems play a key role in protecting network security. To improve intrusion detection effect, different machine learning techniques have been widely applied and achieve d exciting results. However, the premise that these methods achieve reliable results is that there are enough available and well-labeled training data, training and test data being from the same distribution. In real life, the limited label data generated by a single organization is not enough to train a reliable learning model, and the distribution of data collected by different organizations is difficult to be the same. In addition, various organizations protect their privacy and data security through data islands. Therefore, this paper proposes an efficient intrusion detection method using transfer learning and support vector machine with privacy-preserving (FETLSVMP). FETLSVMP performs aggregation of data distributed in various organizations through federated learning, then utilizes transfer learning and support vector machines build personalized models for each organization. Specifically, FETLSVMP first builds a transfer support vector machine model to solve the problem of data distribution differences among various organizations; then, under the mechanism of federated learning, the model is used for learning without sharing training data on each organization to protect data privacy; finally, the intrusion detection model is obtained with protecting the privacy of data. Experiments are carried out on NSL-KDD, KDD CUP99 and ISCX2012, the experimental results verify that the proposed method can achieve better results of detection and robust performance, especially for small samples and emerging intrusion behaviors, and have the ability to protect data privacy.},
  archive      = {J_IDA},
  author       = {Wu, Weifei and Zhang, Yanhui},
  doi          = {10.3233/IDA-226617},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1121-1141},
  shortjournal = {Intell. Data Anal.},
  title        = {An efficient intrusion detection method using federated transfer learning and support vector machine with privacy-preserving},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved hybrid structure learning strategy for bayesian
networks based on ensemble learning. <em>IDA</em>, <em>27</em>(4),
1103–1120. (<a href="https://doi.org/10.3233/IDA-226818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the application of Bayesian networks to solve practical problems, it is likely to encounter the situation that the data set is expensive and difficult to obtain in large quantities and the small data set is easy to cause the inaccuracy of Bayesian network (BN) scoring functions, which affects th e BN optimization results. Therefore, how to better learn Bayesian network structures under a small data set is an important problem we need to pay attention to and solve. This paper introduces the idea of parallel ensemble learning and proposes a new hybrid Bayesian network structure learning algorithm. The algorithm adopts the elite-based structure learner using genetic algorithm (ESL-GA) as the base learner. Firstly, the adjacency matrices of the network structures learned by ESL-GA are weighted and averaged. Then, according to the preset threshold, the edges between variables with weak dependence are filtered to obtain a fusion matrix. Finally, the fusion matrix is modified as the adjacency matrix of the integrated Bayesian network so as to obtain the final Bayesian network structure. Comparative experiments on the standard Bayesian network data sets show that the accuracy and reliability of the proposed algorithm are significantly better than other algorithms.},
  archive      = {J_IDA},
  author       = {Gao, Wenlong and Zeng, Zhimei and Ma, Xiaojie and Ke, Yongsong and Zhi, Minqian},
  doi          = {10.3233/IDA-226818},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1103-1120},
  shortjournal = {Intell. Data Anal.},
  title        = {An improved hybrid structure learning strategy for bayesian networks based on ensemble learning},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven predictive maintenance framework for railway
systems. <em>IDA</em>, <em>27</em>(4), 1087–1102. (<a
href="https://doi.org/10.3233/IDA-226811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the Industry 4.0 trend brings automation and data exchange to industrial manufacturing. Using computational systems and IoT devices allows businesses to collect and deal with vast volumes of sensorial and business process data. The growing and proliferation of big data and machine learning technologies enable strategic decisions based on the analyzed data. This study suggests a data-driven predictive maintenance framework for the air production unit (APU) system of a train of Metro do Porto. The proposed method assists in detecting failures and errors in machinery before they reach critical stages. We present an anomaly detection model following an unsupervised approach, combining the Half-Space-trees method with One Class K Nearest Neighbor, adapted to deal with data streams. We evaluate and compare our approach with the Half-Space-Trees method applied without the One Class K Nearest Neighbor combination. Our model produced few type-I errors, significantly increasing the value of precision when compared to the Half-Space-Trees model. Our proposal achieved high anomaly detection performance, predicting most of the catastrophic failures of the APU train system.},
  archive      = {J_IDA},
  author       = {Meira, Jorge and Veloso, Bruno and Bolón-Canedo, Verónica and Marreiros, Goreti and Alonso-Betanzos, Amparo and Gama, João},
  doi          = {10.3233/IDA-226811},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1087-1102},
  shortjournal = {Intell. Data Anal.},
  title        = {Data-driven predictive maintenance framework for railway systems},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel and balanced SVM algorithm on spark for
data-intensive computing. <em>IDA</em>, <em>27</em>(4), 1065–1086. (<a
href="https://doi.org/10.3233/IDA-226774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a machine learning with excellent classification performance, which has been widely used in various fields such as data mining, text classification, face recognition and etc. However, when data volume scales to a certain level, the computational time becomes too long and the efficiency becomes low. To address this issue, we propose a parallel balanced SVM algorithm based on Spark, named PB-SVM, which is optimized on the basis of the traditional Cascade SVM algorithm. PB-SVM contains three parts, i.e., Clustering Equal Division, Balancing Shuffle and Iteration Termination, which solves the problems of data skew of Cascade SVM and the large difference between local support vector and global support vector. We implement PB-SVM in AliCloud Spark distributed cluster with five kinds of public datasets. Our experimental results show that in the two-classification test on the dataset covtype, compared with MLlib-SVM and Cascade SVM on Spark, PB-SVM improves efficiency by 38.9% and 75.4%, and the accuracy is improved by 7.16% and 8.38%. Moreover, in the multi-classification test, compared with Cascade SVM on Spark on the dataset covtype, PB-SVM improves efficiency and accuracy by 94.8% and 18.26% respectively.},
  archive      = {J_IDA},
  author       = {Li, Jianjiang and Shi, Jinliang and Liu, Zhiguo and Feng, Can},
  doi          = {10.3233/IDA-226774},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1065-1086},
  shortjournal = {Intell. Data Anal.},
  title        = {A parallel and balanced SVM algorithm on spark for data-intensive computing},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature evolvable learning with image streams. <em>IDA</em>,
<em>27</em>(4), 1047–1063. (<a
href="https://doi.org/10.3233/IDA-226799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature Evolvable Stream Learning (FESL) has received extensive attentions during the past few years where old features could vanish and new features could appear when learning with streaming data. Existing FESL algorithms are mainly designed for simple datasets with low-dimension features, neverth eless they are ineffective to deal with complex streams such as image sequences. Such crux lies in two facts: (1) the shallow model, which is supported to be feasible for the low-dimension streams, fails to reveal the complex nonlinear patterns of images, and (2) the linear mapping used to recover the vanished features from the new ones is inadequate to reconstruct the old features of image streams. In response, this paper explores a new online learning paradigm: Feature Evolvable Learning with Image Streams (FELIS) which attempts to make the online learners less restrictive and more applicable. In particular, we present a novel ensemble residual network (ERN), in which the prediction is weighted combination of classifiers learnt by the feature representations from several residual blocks, such that the learning is able to start with a shallow network that enjoys fast convergence, and then gradually switch to a deeper model when more data has been received to learn more complex hypotheses. Moreover, we amend the first residual block of ERN as an autoencoder, and then proposed a latent representation mapping (LRM) approach to exploit the relationship between the previous and current feature space of the image streams via minimizing the discrepancy of the latent representations from the two different feature spaces. We carried out experiments on both virtual and real scenarios over large-scale images, and the experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_IDA},
  author       = {Zheng, Tianxiang and Wang, Xianmin and Chen, Yixiang and Yu, Fujia and Li, Jing},
  doi          = {10.3233/IDA-226799},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1047-1063},
  shortjournal = {Intell. Data Anal.},
  title        = {Feature evolvable learning with image streams},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FairAW – additive weighting without discrimination.
<em>IDA</em>, <em>27</em>(4), 1023–1045. (<a
href="https://doi.org/10.3233/IDA-226898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing awareness of the societal impact of decision-making, fairness has become an important issue. More specifically, in many real-world situations, decision-makers can unintentionally discriminate a certain group of individuals based on either inherited or appropriated attributes, such as g ender, age, race, or religion. In this paper, we introduce a post-processing technique, called fair additive weighting (FairAW) for achieving group and individual fairness in multi-criteria decision-making methods. The methodology is based on changing the score of an alternative by imposing fair criteria weights. This is achieved through minimization of differences in scores of individuals subject to fairness constraint. The proposed methodology can be successfully used in multi-criteria decision-making methods where the additive weighting is used to evaluate scores of individuals. Moreover, we tested the method both on synthetic and real-world data, and compared it to Disparate Impact Remover and FA*IR methods that are commonly used in achieving fair scoring of individuals. The obtained results showed that FairAW manages to achieve group fairness in terms of statistical parity, while also retaining individual fairness. Additionally, our approach managed to obtain the best equality in scoring between discriminated and privileged groups.},
  archive      = {J_IDA},
  author       = {Radovanović, Sandro and Petrović, Andrija and Dodevska, Zorica and Delibašić, Boris},
  doi          = {10.3233/IDA-226898},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1023-1045},
  shortjournal = {Intell. Data Anal.},
  title        = {FairAW – additive weighting without discrimination},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RegRL-KG: Learning an l1 regularized reinforcement agent for
keyphrase generation. <em>IDA</em>, <em>27</em>(4), 1003–1021. (<a
href="https://doi.org/10.3233/IDA-226561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyphrase generation (KG) aims at condensing the content from the source text to the target concise phrases. Though many KG algorithms have been proposed, most of them are tailored into deep learning settings with various specially designed strategies and may fail in solving the bias exposure probl em. Reinforcement Learning (RL), a class of control optimization techniques, are well suited to compensate for some of the limitations of deep learning methods. Nevertheless, RL methods typically suffer from four core difficulties in keyphrase generation: environment interaction and effective exploration, complex action control, reward design, and task-specific obstacle. To tackle this difficult but significant task, we present RegRL-KG, including actor-critic based-reinforcement learning control and L1 policy regularization under the first principle of minimizing the maximum likelihood estimation (MLE) criterion by a sequence-to-sequence (Seq2Seq) deep learnining model, for efficient keyphrase generation. The agent utilizes an actor-critic network to control the generated probability distribution and employs L1 policy regularization to solve the bias exposure problem. Extensive experiments show that our method brings improvement in terms of the evaluation metrics on five scientific article benchmark datasets.},
  archive      = {J_IDA},
  author       = {Yao, Yu and Yang, Peng and Zhao, Guangzhen and Leng, Juncheng},
  doi          = {10.3233/IDA-226561},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1003-1021},
  shortjournal = {Intell. Data Anal.},
  title        = {RegRL-KG: Learning an l1 regularized reinforcement agent for keyphrase generation},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Active ordinal classification by querying relative
information. <em>IDA</em>, <em>27</em>(4), 977–1002. (<a
href="https://doi.org/10.3233/IDA-226899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting and learning with auxiliary information is a way to further reduce the labeling cost of active learning. This paper studies the problem of active learning for ordinal classification by querying low-cost relative information (instance-pair relation information) through pairwise queries. T wo challenges in this study that arise are how to train an ordinal classifier with absolute information (labeled data) and relative information simultaneously and how to select appropriate query pairs for querying. To solve the first problem, we convert the absolute and relative information into the class interval-labeled training instances form by introducing a class interval concept and two reasoning rules. Then, we design a new ordinal classification model for learning with the class interval-labeled training instances. For query pair selection, we specify that each query pair consists of an unlabeled instance and a labeled instance. The unlabeled instance is selected by a margin-based critical instance selection method, and the corresponding labeled instance is selected based on an expected cost minimization strategy. Extensive experiments on twelve public datasets validate that the proposed method is superior to the state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {He, Deniu},
  doi          = {10.3233/IDA-226899},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {977-1002},
  shortjournal = {Intell. Data Anal.},
  title        = {Active ordinal classification by querying relative information},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe co-training for semi-supervised regression.
<em>IDA</em>, <em>27</em>(4), 959–975. (<a
href="https://doi.org/10.3233/IDA-226718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-training is a popular semi-supervised learning method. The learners exchange pseudo-labels obtained from different views to reduce the accumulation of errors. One of the key issues is how to ensure the quality of pseudo-labels. However, the pseudo-labels obtained during the co-training process m ay be inaccurate. In this paper, we propose a safe co-training (SaCo) algorithm for regression with two new characteristics. First, the safe labeling technique obtains pseudo-labels that are certified by both views to ensure their reliability. It differs from popular techniques of using two views to assign pseudo-labels to each other. Second, the label dynamic adjustment strategy updates the previous pseudo-labels to keep them up-to-date. These pseudo-labels are predicted using the augmented training data. Experiments are conducted on twelve datasets commonly used for regression testing. Results show that SaCo is superior to other co-training style regression algorithms and state-of-the-art semi-supervised regression algorithms.},
  archive      = {J_IDA},
  author       = {Liu, Liyan and Huang, Peng and Yu, Hong and Min, Fan},
  doi          = {10.3233/IDA-226718},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {959-975},
  shortjournal = {Intell. Data Anal.},
  title        = {Safe co-training for semi-supervised regression},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory personalization privacy preservation method based
on multi-sensitivity attribute generalization and local suppression.
<em>IDA</em>, <em>27</em>(4), 935–957. (<a
href="https://doi.org/10.3233/IDA-226892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast-developing mobile location-aware services generate an enormous volume of trajectory data while adding value to people’s lives. However, trajectory data contains not only location information, but also sensitive personal information. If the original trajectory data is published directly, it cou ld result in serious privacy leaks. Most of the existing privacy-preserving trajectory publishing methods only protect the location information or set the same privacy preservation levels for all moving objects. To meet the users’ personalized privacy requirements and ensure the utility of trajectory location and sensitive information, we propose a trajectory personalized privacy preservation method based on multi-sensitivity attribute generalization and local suppression. First, we set different security levels for each trajectory by calculating the correlation between sensitive attributes to establish a sensitive attribute classification tree. Second, we generalized sensitive attributes based on privacy preservation levels for each trajectory, the trajectory data still at risk of privacy leakage after generalization was locally suppressed. Finally, an anonymized trajectory dataset was generated. Experimental results on real datasets demonstrated that our method could improve data availability while preserving privacy.},
  archive      = {J_IDA},
  author       = {Yu, Qingying and Yang, Feng and Xiao, Zhenxing and Gong, Shan and Sun, Liping and Chen, Chuanming},
  doi          = {10.3233/IDA-226892},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {935-957},
  shortjournal = {Intell. Data Anal.},
  title        = {Trajectory personalization privacy preservation method based on multi-sensitivity attribute generalization and local suppression},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filter pruning via feature map clustering. <em>IDA</em>,
<em>27</em>(4), 911–933. (<a
href="https://doi.org/10.3233/IDA-226810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of network compression algorithms, deep neural networks can be applied on low-power embedded systems and mobile devices such as drones, satellites, and smartphones. Filter pruning is a sub-direction of network compression research, which reduces memory and computational consumption by reducing the number of parameters of model filters. Previous works utilized the “more-simple-less-important” criterion for pruning filters. That is, filters with the smaller norm or more sparse weights in the network are preferentially pruned. In this paper, we found that feature maps are not fully positively correlated with the sparsity of filter weights by observing the visualization of feature maps and the corresponding filters. Hence, we came up with the idea that the priority of filter pruning should be determined by redundancy rather than sparsity. The redundancy of a filter is the measure of whether the output of the filter is repeated with other filters. Based on this, we defined a criterion called redundancy index to rank the filters and introduced it into our filter pruning strategy. Extensive experiments demonstrate the effectiveness of our approach on different model architectures, including VGGNet, GoogleNet, DenseNet, and ResNet. The models compressed with our strategy surpass the state-of-the-art in terms of Floating Point Operations Per Second (FLOPs), parameters reduction, and classification accuracy.},
  archive      = {J_IDA},
  author       = {Li, Wei and He, Yongxing and Zhang, Xiaoyu and Tang, Yongchuan},
  doi          = {10.3233/IDA-226810},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {911-933},
  shortjournal = {Intell. Data Anal.},
  title        = {Filter pruning via feature map clustering},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic mutual information-based feature selection for
multi-label learning. <em>IDA</em>, <em>27</em>(4), 891–909. (<a
href="https://doi.org/10.3233/IDA-226666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classification problems, feature selection is used to identify important input features to reduce the dimensionality of the input space while improving or maintaining classification performance. Traditional feature selection algorithms are designed to handle single-label learning, but classifica tion problems have recently emerged in multi-label domain. In this study, we propose a novel feature selection algorithm for classifying multi-label data. This proposed method is based on dynamic mutual information, which can handle redundancy among features controlling the input space. We compare the proposed method with some existing problem transformation and algorithm adaptation methods applied to real multi-label datasets using the metrics of multi-label accuracy and hamming loss. The results show that the proposed method demonstrates more stable and better performance for nearly all multi-label datasets.},
  archive      = {J_IDA},
  author       = {Kim, Kyung-Jun and Jun, Chi-Hyuck},
  doi          = {10.3233/IDA-226666},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {891-909},
  shortjournal = {Intell. Data Anal.},
  title        = {Dynamic mutual information-based feature selection for multi-label learning},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Editorial. <em>IDA</em>, <em>27</em>(4), 887–889. (<a
href="https://doi.org/10.3233/IDA-230004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-230004},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {887-889},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Erratum. <em>IDA</em>, <em>27</em>(3), 885. (<a
href="https://doi.org/10.3233/IDA-230950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-230950},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {885},
  shortjournal = {Intell. Data Anal.},
  title        = {Erratum},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficacy of the confinement policies on the COVID-19 spread
dynamics in the early period of the pandemic. <em>IDA</em>,
<em>27</em>(3), 855–884. (<a
href="https://doi.org/10.3233/IDA-216444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spread dynamics and the confinement policies of COVID-19 exhibit different patterns for different countries. Numerous factors affect such patterns within each country. Examining these factors, and analyzing the confinement practices allow government authorities to implement effective policies in th e future. In addition, they help the authorities to distribute healthcare resources optimally without overwhelming their systems. In this empirical study, we use a clustering-based approach, Hierarchical Cluster Analysis (HCA) on time-series data to capture the spread patterns at various countries. We particularly investigate the confinement policies adopted by different countries and their impact on the spread patterns of COVID-19. We limit our investigation to the early period of the pandemic, because many governments tried to respond rapidly and aggressively in the beginning. Moreover, these governments adopted diverse confinement policies based on trial-and-error in the beginning of the pandemic. We found that implementations of the same confinement policies may exhibit different results in different countries. Specifically, lockdowns become less effective in densely populated regions, because of the reluctance to comply with social distancing measures. Lack of testing, contact tracing, and social awareness in some countries forestall people from self-isolation and maintaining social distance. Large labor camps with unhealthy living conditions also aid in high community transmissions in countries depending on foreign labor. Distrust in government policies and fake news instigate the spread in both developed and under-developed countries. Large social gatherings play a vital role in causing rapid outbreaks almost everywhere. An early and rapid response at the early period of the pandemic is necessary to contain the spread, yet it is not always sufficient.},
  archive      = {J_IDA},
  author       = {Hassan, Mehedi and Haque, Md. Enamul and Tozal, Mehmet Engin},
  doi          = {10.3233/IDA-216444},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {855-884},
  shortjournal = {Intell. Data Anal.},
  title        = {Efficacy of the confinement policies on the COVID-19 spread dynamics in the early period of the pandemic},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Positive public opinion guidance model based on dual
learning in social network. <em>IDA</em>, <em>27</em>(3), 833–853. (<a
href="https://doi.org/10.3233/IDA-226602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet public opinion is closely related to our life in social network. The wanton growth of some negative public opinions has an extremely serious impact on the social stability and national security. After the guidance of government manually, some negative public opinion is well controlled and people’s life gain more positive energy. How to use Internet technology to automatically and promptly guide public opinion events and reduce the harm to society is currently challenging research. Therefore, in this paper, we propose a positive public opinion guidance model based on dual learning for negative Internet public opinion, hereinafter denoted to as the dual-PPOG model. Firstly, we use the Fast Unfolding algorithm to divide social networks into the public opinion guidance communities. In these communities, we detect the positive opinion guider and negative opinion receiver by our defined PageRank (PR) variant. Secondly, inspired by dual learning, we construct the public opinion guidance model and evaluate whether the guidance is successful through the feedback signal. Through the repeated guidance of the positive opinion guider to the negative opinion receiver, the public opinion guidance is successful. This is the main process for the dual positive public opinion mechanism. Finally, we guide the remaining nodes based on the opinion dynamics. The experiment demonstrates beneficial effects of our proposed model of dual-PPOG. Experimental results on three real-world datasets intercepted from Twitter, E-mail and Facebook show that the model of dual-PPOG can capture useful information in the network topology. Compared with the methods of HK, AE, Random and AIA on the three datasets from small to large in scale, the percentage of positive opinion increased by 4%, 6.9%, and 2.7% respectively, which shows our approach achieve significant improvements and effectiveness compared to all baselines.},
  archive      = {J_IDA},
  author       = {Lyu, Binyan and Du, Yajun and Jiang, Jiajian and Hu, JinRong and Li, Hui},
  doi          = {10.3233/IDA-226602},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {833-853},
  shortjournal = {Intell. Data Anal.},
  title        = {Positive public opinion guidance model based on dual learning in social network},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning binary codes for fast image retrieval with sparse
discriminant analysis and deep autoencoders. <em>IDA</em>,
<em>27</em>(3), 809–831. (<a
href="https://doi.org/10.3233/IDA-226687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval with relevant feedback on large and high-dimensional image databases is a challenging task. In this paper, we propose an image retrieval method, called BCFIR (Binary Codes for Fast Image Retrieval). BCFIR utilizes sparse discriminant analysis to select the most important original fe ature set, and solve the small class problem in the relevance feedback. Besides, to increase the retrieval performance on large-scale image databases, in addition to BCFIR mapping real-valued features to short binary codes, it also applies a bagging learning strategy to improve the ability general capabilities of autoencoders. In addition, our proposed method also takes advantage of both labeled and unlabeled samples to improve the retrieval precision. The experimental results on three databases demonstrate that the proposed method obtains competitive precision compared with other state-of-the-art image retrieval methods.},
  archive      = {J_IDA},
  author       = {Hong, Son An and Huu, Quynh Nguyen and Viet, Dung Cu and Thi Thuy, Quynh Dao and Quoc, Tao Ngo},
  doi          = {10.3233/IDA-226687},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {809-831},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning binary codes for fast image retrieval with sparse discriminant analysis and deep autoencoders},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on community evolution based on node influence and
multi-attribute fusion. <em>IDA</em>, <em>27</em>(3), 791–807. (<a
href="https://doi.org/10.3233/IDA-216485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the problem of low discrimination accuracy of evolutionary events in dynamic social networks, a community evolution model (EMNI) based on node influence and multi-attribute fusion is proposed. Firstly, the topological structure information of nodes is obtained by random walk and local clustering coefficient, and the influence of nodes is evaluated according to the topological structure of nodes. Secondly, in order to improve the accuracy of discriminating community similarity, a community similarity discrimination method based on multi-attribute fusion is proposed. The model EMNI combined the characteristics of community stability and community difference, and redefined seven evolutionary events. Finally, the effectiveness of the EMNI model in identifying community evolution events is verified on different data sets. The experimental results show that the EMNI model is better than GED, PECT and SGCI, which is able to identify more evolutionary events and the distribution of events is also more balanced.},
  archive      = {J_IDA},
  author       = {Chen, Jing and Zhao, Haitong and Liu, Mingxin and Liu, Miaomiao},
  doi          = {10.3233/IDA-216485},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {791-807},
  shortjournal = {Intell. Data Anal.},
  title        = {Research on community evolution based on node influence and multi-attribute fusion},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum-behaved particle swarm optimization with dynamic
grouping searching strategy. <em>IDA</em>, <em>27</em>(3), 769–789. (<a
href="https://doi.org/10.3233/IDA-226753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum-behaved particle swarm optimization (QPSO) algorithm, a variant of particle swarm optimization (PSO), has been proven to be an effective tool to solve various of optimization problems. However, like other PSO variants, it often suffers a premature convergence, especially when solving co mplex optimization problems. Considering this issue, this paper proposes a hybrid QPSO with dynamic grouping searching strategy, named QPSO-DGS. During the search process, the particle swarm is dynamically grouped into two subpopulations, which are assigned to implement the exploration and exploitation search, respectively. In each subpopulation, a comprehensive learning strategy is used for each particle to adjust its personal best position with a certain probability. Besides, a modified opposition-based computation is employed to improve the swarm diversity. The experimental comparison is conducted between the QPSO-DGS and other seven state-of-art PSO variants on the CEC’2013 test suit. The experimental results show that QPSO-DGS has a promising performance in terms of the solution accuracy and the convergence speed on the majority of these test functions, and especially on multimodal problems.},
  archive      = {J_IDA},
  author       = {You, Qi and Sun, Jun and Palade, Vasile and Pan, Feng},
  doi          = {10.3233/IDA-226753},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {769-789},
  shortjournal = {Intell. Data Anal.},
  title        = {Quantum-behaved particle swarm optimization with dynamic grouping searching strategy},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel framework to enhance the performance of training
distributed deep neural networks. <em>IDA</em>, <em>27</em>(3), 753–768.
(<a href="https://doi.org/10.3233/IDA-226710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many attempts to implement deep neural network (DNN) distributed training frameworks. In these attempts, Apache Spark was used to develop the frameworks. Each framework has its advantages and disadvantages and needs further improvements. In the process of using Apache Spark to implement d istributed training systems, we ran into some obstacles that significantly affect the performance of the systems and programming thinking. This is the reason why we developed our own distributed training framework, called Distributed Deep Learning Framework (DDLF), which is completely independent of Apache Spark. Our proposed framework can overcome the obstacles and is highly scalable. DDLF helps to develop applications that train DNN in a distributed environment (referred to as distributed training) in a simple, natural, and flexible way. In this paper, we will analyze the obstacles when implementing a distributed training system on Apache Spark and present solutions to overcome them in DDLF. We also present the features of DDLF and how to implement a distributed DNN training application on this framework. In addition, we conduct experiments by training a Convolutional Neural Network (CNN) model with datasets MNIST and CIFAR-10 in Apache Spark cluster and DDLF cluster to demonstrate the flexibility and effectiveness of DDLF.},
  archive      = {J_IDA},
  author       = {Phan, Trung and Do, Phuc},
  doi          = {10.3233/IDA-226710},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {753-768},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel framework to enhance the performance of training distributed deep neural networks},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-guided and attribute-manipulated similarity
learning network for fashion image retrieval. <em>IDA</em>,
<em>27</em>(3), 733–751. (<a
href="https://doi.org/10.3233/IDA-226740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the similarity between fashion items is essential for many fashion-related tasks. Most methods based on global or local image similarity cannot meet the fine-grained retrieval requirements related to attributes. We are the first to clearly distinguish the concepts of attribute name and the ir values and divide fashion retrieval tasks that combine images and text into: attribute-guided retrieval and attribute-manipulated retrieval. We propose a hierarchical attribute-aware embedding network (HAEN) that takes images and attributes as input, learns multiple attribute-specific embedding spaces, and measures fine-grained similarity in the corresponding spaces. It can accurately map different attributes to the corresponding areas of the image, thereby facilitating the feature fusion of two different modalities of text and image, including enhancement and replacement. Then on this basis, we propose three attribute-manipulated similarity learning methods, HAEN_Avg, HAEN_Rec, and HAEN_Cmb. With comprehensive validation on two real-world fashion datasets, we demonstrate that our methods can effectively leverage semantic knowledge to improve image retrieval performance, including attribute-guided and attribute-manipulated retrieval tasks.},
  archive      = {J_IDA},
  author       = {Wan, Yongquan and Yan, Cairong and Zou, Guobing and Zhang, Bofeng},
  doi          = {10.3233/IDA-226740},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {733-751},
  shortjournal = {Intell. Data Anal.},
  title        = {Attribute-guided and attribute-manipulated similarity learning network for fashion image retrieval},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-community shortcut detection based on network
representation learning and structural features. <em>IDA</em>,
<em>27</em>(3), 709–732. (<a
href="https://doi.org/10.3233/IDA-216513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As social networks continue to expand, an increasing number of people prefer to use social networks to post their comments and express their feelings, and as a result, the information contained in social networks has grown explosively. The effective extraction of valuable information from social ne tworks has attracted the attention of many researchers. It can mine hidden information from social networks and promote the development of social network structures. At present, many ranking node approaches, such as structural hole spanners and opinion leaders, are widely adopted to extract valuable information and knowledge. However, approaches for analyzing edge influences are seldom considered. In this study, we proposed an edge PageRank to mine shortcuts (these edges without direct mutual friends) that are located among communities and play an important role in the spread of public opinion. We first used a network-embedding algorithm to order the spanners and determine the direction of every edge. Then, we transferred the graphs of social networks into edge graphs according to the ordering. We considered the nodes and edges of the graphs of the social networks as edges and nodes of the edge graphs, respectively. Finally, we improved the PageRank algorithm on the edge graph to obtained the edge ranking and extracted the shortcuts of social networks. The experimental results for five different sizes of social networks, such as email, YouTube, DBLP-L, DBLP-M, and DBLP-S, verify whether the inferred shortcut is indeed more useful for information dissemination, and the utility of three sets of edges inferred by different methods is compared, namely, the edge inferred by ER, the edge inferred by the Jaccard index. The ER approach improves by approximately 10%, 9.9%, and 8.3% on DBLP, YouTube, and Orkut. Our method is more effective than the edge ranked by the Jaccard index.},
  archive      = {J_IDA},
  author       = {Hu, Ruilin and Du, Yajun and Hu, Jingrong and Li, Hui},
  doi          = {10.3233/IDA-216513},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {709-732},
  shortjournal = {Intell. Data Anal.},
  title        = {Cross-community shortcut detection based on network representation learning and structural features},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling multi-attribute and implicit relationship factors
with self-supervised learning for recommender systems. <em>IDA</em>,
<em>27</em>(3), 691–708. (<a
href="https://doi.org/10.3233/IDA-226576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions of users and items can be naturally modeled as a user-item bipartite graph in recommender systems, and emerging research is devoted to exploring user-item graphs for collaborative filtering methods. In reality, user-item interaction usually stems from more complex underlying factors, s uch as the users’ specific preferences. A user-item bipartite graph could be used to understand the differences in motivation. However, existing research has not clearly proposed and modeled the factors that affect the differences, ignoring the similarities between user pairs and item pairs, preventing them from capturing fine-grained user preferences more effectively. In addition to the two points mentioned above, most GNN-based models for recommendation have the following two limitations: First, the model’s accuracy depends on the number of observed interactions in the dataset. Secondly, node representations are vulnerable to noisy interactions. This work has developed a novel recommendation model called “Multi-Attribute and Implicit Relationship Factors With Self-Supervised Learning for Collaborative filtering” (MIS-CF), which explicitly proposes and models multi-attribute and implicit relationship factors for collaborative filtering recommendation. Meanwhile, an auxiliary self-supervised learning task is designed to help the downstream task optimize the node representation. MIS-CF aggregates multi-attribute spaces through the user-item bipartite graph and establishes user-user and item-item graphs to model the similar relationship information of neighbor pairs through a memory model. The self-supervised learning task generates contrastive learning via self-discrimination, thus mining the rich auxiliary signals within the data, improving the accuracy and robustness of our model. Moreover, the sparse regularizer is utilized to alleviate the overfitting problem. Extensive experimental results on three public datasets not only show the significant performance and robustness gain of the proposed model but also prove the effectiveness and interpretability of fine-grained implicit factors modeling.},
  archive      = {J_IDA},
  author       = {Hu, Linfeng and Wen, Junhao and Zhang, Hanwen and Zhou, Wei and Wang, Hongyu},
  doi          = {10.3233/IDA-226576},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {691-708},
  shortjournal = {Intell. Data Anal.},
  title        = {Modeling multi-attribute and implicit relationship factors with self-supervised learning for recommender systems},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A prototype selection technique based on relative density
and density peaks clustering for k nearest neighbor classification.
<em>IDA</em>, <em>27</em>(3), 675–690. (<a
href="https://doi.org/10.3233/IDA-226730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k-nearest neighbor classifier (KNN) is one of the most famous classification models due to its straightforward implementation and an error bounded by twice the Bayes error. However, it usually degrades because of noise and the high cost in computing the distance between different samples. In this c ontext, hybrid prototype selection techniques have been postulated as a good solution and developed. Yet, they have the following issues: (a) adopted edition methods are susceptible to harmful samples around tested samples; (b) they retain too many internal samples, which contributes little to the classification of KNN classifier and (or) leading to the low reduction; (c) they rely on many parameters. The main contributions of our work are that (a) a novel competitive hybrid prototype selection technique based on relative density and density peaks clustering (PST-RD-DP) are proposed against the above issues at the same time; (b) a new edition method based on relative density and distance (EMRDD) in PST-RD-DP is first proposed to remove harmful samples and smooth the class boundary; (c) a new condensing method based on relative density and density peaks clustering (CMRDDPC) in PST-RD-DP is second proposed to retain representative borderline samples. Intensive experiments prove that PST-RD-DP outperforms 6 popular hybrid prototype selection techniques on extensive real data sets in weighing accuracy and reduction of the KNN classifier. Besides, the running time of PST-RD-DP is also acceptable.},
  archive      = {J_IDA},
  author       = {Xiang, Lina},
  doi          = {10.3233/IDA-226730},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {675-690},
  shortjournal = {Intell. Data Anal.},
  title        = {A prototype selection technique based on relative density and density peaks clustering for k nearest neighbor classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local morphological patterns for time series classification.
<em>IDA</em>, <em>27</em>(3), 653–674. (<a
href="https://doi.org/10.3233/IDA-216548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key problem of time series classification is the similarity measure between time series. In recent years, efficient and accurate similarity measurement methods of time series have attracted extensive attention from researchers. According to the different similarity measure strategies, the exist ing time series classification methods can be roughly divided into shape-based (original value) methods and structure-based (symbol transformation) methods. Shape-based methods usually use Euclidean distance (ED), dynamic time warping (DTW), or other methods to measure the global similarity between sequences. The disadvantage of these methods is that their measurement process does not necessarily achieve local sensible matchings of time series, which leads to a decrease in their accuracy and interpretability. To better capture the local information of the sequence, the structure-based methods discretize or symbolize the local value of the time sequence, which leads to the loss of the original information of the sequence. To address these problems, this paper proposes a novel similarity measurement method named dynamic time warping based on the local morphological pattern (MPDTW), which first decomposes the local subsequences of time series using discrete wavelet transforms for extracting the local structure information. Then, the decomposed subsequence will be encoded by the morphological pattern. Finally, the ED between points and their local structure difference based on morphological pattern will be weighted and applied to the DTW algorithm to measure the similarity between sequences. Experiments have been carried out on the classification tasks of the UCR datasets and the results show that our method outperforms the existing baselines.},
  archive      = {J_IDA},
  author       = {Hao, Shilei and Wang, Zhihai and Yuan, Jidong},
  doi          = {10.3233/IDA-216548},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {653-674},
  shortjournal = {Intell. Data Anal.},
  title        = {Local morphological patterns for time series classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering-based improved adaptive synthetic minority
oversampling technique for imbalanced data classification. <em>IDA</em>,
<em>27</em>(3), 635–652. (<a
href="https://doi.org/10.3233/IDA-226612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic Minority Oversampling Technique (SMOTE) and some extensions based on it are popularly used to balance imbalanced data. In this study, we concentrate on solving overfitting of the classification model caused by choosing instances to oversample that increase the occurrence of overlaps with the majority class. Our method called Clustering-based Improved Adaptive Synthetic Minority Oversampling Technique (CI-ASMOTE1) decomposes minority instances into sub-clusters according to their connectivity in the feature space and then selects minority sub-clusters which are relatively close to the decision boundary as the candidate regions to oversample. After application of CI-ASMOTE1, new minority instances are only synthesized within each connected region of the selected sub-clusters. Considering the diversity of the synthetic instances in each selected sub-cluster, CI-ASMOTE2 is put forward to extend CI-ASMOTE1 by keeping all features of those instances in the feature space as different as possible. The experimental evaluation shows that CI-ASMOTE1 and CI-ASMOTE2 improve SMOTE and its extensions, especially in the occurrence of overlaps between the minority instances and the majority instances.},
  archive      = {J_IDA},
  author       = {Jin, Dian and Xie, Dehong and Liu, Di and Gong, Murong},
  doi          = {10.3233/IDA-226612},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {635-652},
  shortjournal = {Intell. Data Anal.},
  title        = {Clustering-based improved adaptive synthetic minority oversampling technique for imbalanced data classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Active learning for ordinal classification on incomplete
data. <em>IDA</em>, <em>27</em>(3), 613–634. (<a
href="https://doi.org/10.3233/IDA-226664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing active learning algorithms typically assume that the data provided are complete. Nonetheless, data with missing values are common in real-world applications, and active learning on incomplete data is less studied. This paper studies the problem of active learning for ordinal classification on incomplete data. Although cutting-edge imputation methods can be used to impute the missing values before commencing active learning, inaccurately imputed instances are unavoidable and may degrade the ordinal classifier’s performance once labeled. Therefore, the crucial question in this work is how to reduce the negative impact of imprecisely filled instances on active learning. First, to avoid selecting filled instances with high imputation imprecision, we propose penalizing the query selection with a novel imputation uncertainty measure that combines a feature-level imputation uncertainty and a knowledge-level imputation uncertainty. Second, to mitigate the adverse influence of potentially labeled imprecisely imputed instances, we suggest using a diversity-based uncertainty sampling strategy to select query instances in specified candidate instance regions. Extensive experiments on nine public ordinal classification datasets with varying value missing rates show that the proposed approach outperforms several baseline methods.},
  archive      = {J_IDA},
  author       = {He, Deniu},
  doi          = {10.3233/IDA-226664},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {613-634},
  shortjournal = {Intell. Data Anal.},
  title        = {Active learning for ordinal classification on incomplete data},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An agglomerative hierarchical clustering approach to
identify coexisting bacteria in groups of bacterial vaginosis patients.
<em>IDA</em>, <em>27</em>(3), 583–611. (<a
href="https://doi.org/10.3233/IDA-216488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polymicrobial syndromes such as Bacterial Vaginosis (BV), where there is a great diversity of microorganisms and causal connotations, turn it into a disease with complex dynamics in the bacteria’s coexistence in groups of patients. The main aim of this study was to explore a dataset of patients wit h BV to determine a more informed number of groups to create for further analysis of bacteria’s coexistence. The Agglomerative Hierarchical Clustering (AHC) algorithm was applied to a BV dataset from an urban population in southeastern Mexico consisting of 201 patient records with 59 patient attributes and three classes (BV-positive, BV-negative, BV-indeterminate). In the clustering results obtained, it is possible to identify different remarkable groups of patients. The most prevalent coexisting bacteria among patients with BV were Atopobium + Gardnerella vaginalis with 37.50%, Atopobium + Megasphaera with 15.68% in the first experiment. Whereas, in the second experiment, the coexisting bacteria were Atopobium + Megasphaera + Mycoplasma hominis with 33.33% and Atopobium + Gardnerella vaginalis + Mycoplasma hominis with 25%. Finally, we provided evidence that via the AHC algorithm, it was possible to identify an optimal number of clusters with high intra-similarity and inter-dissimilarity. Furthermore, this approach allowed us to create a clustering model that helps analyze the complex dynamics between bacteria in groups of patients with BV.},
  archive      = {J_IDA},
  author       = {Hernández-Gómez, Henry Jesús and Canul-Reich, Juana and Hernández-Ocaña, Betania and de la Cruz Hernández, Erick},
  doi          = {10.3233/IDA-216488},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {583-611},
  shortjournal = {Intell. Data Anal.},
  title        = {An agglomerative hierarchical clustering approach to identify coexisting bacteria in groups of bacterial vaginosis patients},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Editorial. <em>IDA</em>, <em>27</em>(3), 579–581. (<a
href="https://doi.org/10.3233/IDA-230003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-230003},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {579-581},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-granularity user interest modeling and interest drift
detection. <em>IDA</em>, <em>27</em>(2), 555–577. (<a
href="https://doi.org/10.3233/IDA-216517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the advent of Web 2.0 culture, there as been an explosion of data on the internet. The traditional service model based on the search engine can no longer meet the increasing demand for personalized service. Taking the Douban film review platform as an example in this paper, we propose a metho d to model user preferences and detect preference drift. Based on a hierarchical topic tree and tilted time window, we design a hierarchical classification tree, named HAT-tree, to maintain the history of the user’s preferences at multi-topic and multi-time granularity. We identify the user’s primary historical preferences, predict their future primary preferences and also detect user preference drift. The proposed algorithm can find the user’s long-term and short-term preferences, detect the user’s explicit and implicit preference drift, and highlight the importance of the user’s more recent preferences. Many experiments are carried out on multiple data sets, and the experimental results show that the proposed method is more accurate than other similar algorithms of user preference drift detection.},
  archive      = {J_IDA},
  author       = {Chen, Hui and Huang, Jian and Deng, Qingshan and Wang, Jing and Kong, Leilei and Deng, Xiaozheng},
  doi          = {10.3233/IDA-216517},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {555-577},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-granularity user interest modeling and interest drift detection},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining rifle shooting factors through multi-sensor body
tracking 1. <em>IDA</em>, <em>27</em>(2), 535–554. (<a
href="https://doi.org/10.3233/IDA-216457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a lack of data-driven training instructions for sports shooters, as instruction has commonly been based on subjective assessments. Many studies have correlated body posture and balance to shooting performance in rifle shooting tasks, but have mostly focused on single aspects of postural control. This study has focused on finding relevant rifle shooting factors by examining the entire body over sequences of time. A data collection was performed with 13 human participants carrying out live rifle shooting scenarios while being recorded with multiple body tracking sensors. A pre-processing pipeline produced a novel skeleton sequence representation, which was used to train a transformer model. The predictions from this model could be explained on a per sample basis using the attention mechanism, and visualised in an interactive format for humans to interpret. It was possible to separate the different phases of a shooting scenario from body posture with a high classification accuracy (80%). Shooting performance could be detected to an extent by separating participants using their strong and weak shooting hand. The dataset and pre-processing pipeline, as well as the techniques for generating explainable predictions presented in this study have laid the groundwork for future research in the sports shooting domain.},
  archive      = {J_IDA},
  author       = {Flyckt, Jonatan and Andersson, Filip and Westphal, Florian and Månsson, Andreas and Lavesson, Niklas},
  doi          = {10.3233/IDA-216457},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {535-554},
  shortjournal = {Intell. Data Anal.},
  title        = {Explaining rifle shooting factors through multi-sensor body tracking 1},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic gaussian deep belief network design and stock market
application. <em>IDA</em>, <em>27</em>(2), 519–534. (<a
href="https://doi.org/10.3233/IDA-216340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock price forecasting has been an important topic for investors, researchers, and analysts. In this paper, a prediction model of Dynamic Gaussian Deep Belief Network (DGDBN) is proposed. Generally, the network structure of traditional Deep Belief Network (DBN) determines the performance of its ti me series prediction. Most previous research uses artificial experience to adjust the network structure, it is difficult to ensure performance and time efficiency by constantly trying. In addition, the accuracy of the traditional DBN stacked by binary Restricted Boltzmann Machines(RBM) needs to be improved when solving the time series problem. The DGDBN designed in this paper contains two points: The first point is to add Gaussian noise to the RBM. The second point is to realize the increase or decrease branch algorithm of hidden layer structure according to the connection weights and average percentage error (MAPE). Finally, the forecast for the stocks of United Technologies Corporation and Unisys Corp, DGDBN is compared with DBN and LSTM. The root means square error (RMSE) increases by 15% and 65%. The interesting thing we found is that the number of neurons in the last layer of the DGDBN network has a greater effect than other layers.},
  archive      = {J_IDA},
  author       = {Xi, Shuyue and Xu, Xiaozhong},
  doi          = {10.3233/IDA-216340},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {519-534},
  shortjournal = {Intell. Data Anal.},
  title        = {Dynamic gaussian deep belief network design and stock market application},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning application in the ex-combatant
demobilization process on the colombian armed conflict. <em>IDA</em>,
<em>27</em>(2), 501–517. (<a
href="https://doi.org/10.3233/IDA-216397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research explores the potential of supervised machine learning models to support the decision-making process in demobilizing ex-combatants in the peace process in Colombia. Recent works apply machine learning in analyzing crime and national security; however, there are no previous studies in t he specific contexts of demobilization in an armed conflict. Therefore, the present paper makes a significant contribution by training and evaluating four machine learning models, using a database composed of 52,139 individuals and 21 variables. From the obtained results, it was possible to conclude that the XGBoost algorithm is the most suitable for predicting the future status of an ex-combatant. The XGBoost presented an AUC score of 0.964 in the cross-validation stage and an AUC of 0.952 in the test stage, evidencing the high reliability of the model.},
  archive      = {J_IDA},
  author       = {Delahoz-Domínguez, Enrique and Carrillo-Naranjo, Jonathan and Camelo-Guarín, Alicia and Zuluaga-Ortiz, Rohemi},
  doi          = {10.3233/IDA-216397},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {501-517},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning application in the ex-combatant demobilization process on the colombian armed conflict},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating deep neural network with logic rules for credit
scoring. <em>IDA</em>, <em>27</em>(2), 483–500. (<a
href="https://doi.org/10.3233/IDA-216460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring is an important topic in financial activities and bankruptcy prediction that has been extensively explored using deep neural network (DNN) methods. DNN-based credit scoring models rely heavily on a large amount of labeled data. The accuracy of DNN-based credit assessment models relie s heavily on large amounts of labeled data. However, purely data-driven learning makes it difficult to encode human intent to guide the model to capture the desired patterns and leads to low transparency of the model. Therefore, the Probabilistic Soft Logic Posterior Regularization (PSLPR) framework is proposed for integrating prior knowledge of logic rule with neural network. First, the PSLPR framework calculates the rule satisfaction distance for each instance using a probabilistic soft logic formula. Second, the logic rules are integrated into the posterior distribution of the DNN output to form a logic output. Finally, a novel discrepancy loss which measures the difference between the real label and the logic output is used to incorporate logic rules into the parameters of the neural network. Extensive experiments were conducted on two datasets, the Australian credit dataset and the credit card customer default dataset. To evaluate the obtained systems, several performance metrics were used, including PCC, Recall, F1 and AUC. The results show that compared to the standard DNN model, the four evaluation metrics are increased by 7.14%, 14.29%, 8.15%, and 5.43% respectively on the Australian credit dataset.},
  archive      = {J_IDA},
  author       = {Li, Zhanli and Zhang, Xinyu and Deng, Fan and Zhang, Yun},
  doi          = {10.3233/IDA-216460},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {483-500},
  shortjournal = {Intell. Data Anal.},
  title        = {Integrating deep neural network with logic rules for credit scoring},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrusion detection algorithom based on transfer extreme
learning machine. <em>IDA</em>, <em>27</em>(2), 463–482. (<a
href="https://doi.org/10.3233/IDA-216475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection can effectively detect malicious attacks in computer networks, which has always been a research hotspot in field of network security. At present, most of the existing intrusion detection methods are based on traditional machine learning algorithms. These methods need enough avai lable intrusion detection training samples, training and test data meet the assumption of independent and identically distributed, at the same time have the disadvantages of low detection accuracy for small samples and new emerging attacks, slow speed of establishment model and high cost. To solve the above problems, this paper proposes an intrusion detection algorithm-TrELM based on transfer learning and extreme machine. TrELM is no longer limited by the assumptions of traditional machine learning. TrELM utilizes the idea of transfer learning to transfer a large number of historical intrusion detection samples related to target domain to target domain with a small number of intrusion detection samples. With the existing historical knowledge, quickly build a high-quality target learning model to effectively improve the detection effect and efficiency of small samples and new emerging intrusion detection behaviors. Experiments are carried out on NSL-KDD, KDD99 and ISCX2012 data sets. The experimental results show that the algorithm can improve the detection accuracy, especially for unknown and small samples.},
  archive      = {J_IDA},
  author       = {Wang, Kunpeng and Li, Jingmei and Wu, Weifei},
  doi          = {10.3233/IDA-216475},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {463-482},
  shortjournal = {Intell. Data Anal.},
  title        = {Intrusion detection algorithom based on transfer extreme learning machine},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A domain density peak clustering algorithm based on natural
neighbor. <em>IDA</em>, <em>27</em>(2), 443–462. (<a
href="https://doi.org/10.3233/IDA-216541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is as an efficient algorithm due for the cluster centers can be found quickly. However, this approach has some disadvantages. Firstly, it is sensitive to the cutoff distance; secondly, the neighborhood information of the data is not considered when calculating the loc al density; thirdly, during allocation, one assignment error may cause more errors. Considering these problems, this study proposes a domain density peak clustering algorithm based on natural neighbor (NDDC). At first, natural neighbor is introduced innovatively to obtain the neighborhood of each point. Then, based on the natural neighbors, several new methods are proposed to calculate corresponding metrics of the points to identify the centers. At last, this study proposes a new two-step assignment strategy to reduce the probability of data misclassification. A series of experiments are conducted that the NDDC offers higher accuracy and robustness than other methods.},
  archive      = {J_IDA},
  author       = {Chen, Di and Du, Tao and Zhou, Jin and Shen, Tianyu},
  doi          = {10.3233/IDA-216541},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {443-462},
  shortjournal = {Intell. Data Anal.},
  title        = {A domain density peak clustering algorithm based on natural neighbor},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SUWAN: A supervised clustering algorithm with attributed
networks. <em>IDA</em>, <em>27</em>(2), 423–441. (<a
href="https://doi.org/10.3233/IDA-216436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing area of study for economists and sociologists is the varying organizational structures between business networks. The use of network science makes it possible to identify the determinants of the performance of these business networks. In this work we look for the determinants of inter -firm performance. On one hand, a new method of supervised clustering with attributed networks is proposed, SUWAN, with the aim at obtaining class-uniform clusters of the turnover, while minimizing the number of clusters. This method deals with representative-based supervised clustering, where a set of initial representatives is randomly chosen. One of the innovative aspects of SUWAN is that we use a supervised clustering algorithm to attributed networks that can be accomplished through a combination of weights between the matrix of distances of nodes and their attributes when defining the clusters. As a benchmark, we use Subgroup Discovery on attributed network data. Subgroup Discovery focuses on detecting subgroups described by specific patterns that are interesting with respect to some target concept and a set of explaining features. On the other hand, in order to analyze the impact of the network’s topology on the group’s performance, some network topology measures, and the group total turnover were exploited. The proposed methodologies are applied to an inter-organizational network, the EuroGroups Register, a central register that contains statistical information on business networks from European countries.},
  archive      = {J_IDA},
  author       = {Santos, Bárbara and Campos, Pedro},
  doi          = {10.3233/IDA-216436},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {423-441},
  shortjournal = {Intell. Data Anal.},
  title        = {SUWAN: A supervised clustering algorithm with attributed networks},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic dataset generator for anomaly detection in a
university environment. <em>IDA</em>, <em>27</em>(2), 417–422. (<a
href="https://doi.org/10.3233/IDA-216511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a recently developed synthetic dataset generator, which contains anonymised data from the Prague University of Economics and Business information system logs. The generator is opensource and is able to scale this data time-wise and also perform injection of the data with cyb erattackers’ behaviour patterns. The anonymised data still contains user behaviour patterns; therefore, individual anomalous behaviour can be detected. Different types of real attack behaviour patterns in the university environment have been selected; they are used to demonstrate attackers’ behaviour in synthetically created system logs. The mentioned features allow other researchers to benchmark their anomaly detection algorithms with complex data.},
  archive      = {J_IDA},
  author       = {Strnad, Pavel and Švarc, Lukáš and Berka, Petr},
  doi          = {10.3233/IDA-216511},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {417-422},
  shortjournal = {Intell. Data Anal.},
  title        = {Synthetic dataset generator for anomaly detection in a university environment},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A relief-PGS algorithm for feature selection and data
classification. <em>IDA</em>, <em>27</em>(2), 399–415. (<a
href="https://doi.org/10.3233/IDA-216493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a supervised learning algorithm, Support Vector Machine (SVM) is very popularly used for classification. However, the traditional SVM is error-prone because of easy to fall into local optimal solution. To overcome the problem, a new SVM algorithm based on Relief algorithm and particle swarm opti mization-genetic algorithm (Relief-PGS) is proposed for feature selection and data classification, where the penalty factor and kernel function of SVM and the extracted feature of Relief algorithm are encoded as the particles of particle swarm optimization-genetic algorithm (PSO-GA) and optimized by iteratively searching for optimal subset of features. To evaluate the quality of features, Relief algorithm is used to screen the feature set to reduce the irrelevant features and effectively select the feature subset from multiple attributes. The advantage of Relief-PGS algorithm is that it can optimize both feature subset selection and SVM parameters including the penalty factor and the kernel parameter simultaneously. Numerical experimental results indicated that the classification accuracy and efficiency of Relief-PGS are superior to those of other algorithms including traditional SVM, PSO-GA-SVM, Relief-SVM, ACO-SVM, etc.},
  archive      = {J_IDA},
  author       = {Wang, Youming and Han, Jiali and Zhang, Tianqi},
  doi          = {10.3233/IDA-216493},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {399-415},
  shortjournal = {Intell. Data Anal.},
  title        = {A relief-PGS algorithm for feature selection and data classification},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving multi-label learning by modeling local label and
feature correlations. <em>IDA</em>, <em>27</em>(2), 379–398. (<a
href="https://doi.org/10.3233/IDA-216404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning deals with the problem that each instance is associated with multiple labels simultaneously, and many methods have been proposed by modeling label correlations in a global way to improve the performance of multi-label learning. However, the local label correlations and the infl uence of feature correlations are not fully exploited for multi-label learning. In real applications, different examples may share different label correlations, and similarly, different feature correlations are also shared by different data subsets. In this paper, a method is proposed for multi-label learning by modeling local label correlations and local feature correlations. Specifically, the data set is first divided into several subsets by a clustering method. Then, the local label and feature correlations, and the multi-label classifiers are modeled based on each data subset respectively. In addition, a novel regularization is proposed to model the consistency between classifiers corresponding to different data subsets. Experimental results on twelve real-word multi-label data sets demonstrate the effectiveness of the proposed method.},
  archive      = {J_IDA},
  author       = {Cheng, Qianqian and Huang, Jun and Zhang, Huiyi and Chen, Sibao and Zheng, Xiao},
  doi          = {10.3233/IDA-216404},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {379-398},
  shortjournal = {Intell. Data Anal.},
  title        = {Improving multi-label learning by modeling local label and feature correlations},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A grouping feature selection method based on feature
interaction. <em>IDA</em>, <em>27</em>(2), 361–377. (<a
href="https://doi.org/10.3233/IDA-226551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature interaction is crucial in the process of feature selection. In this paper, a grouping feature selection method based on feature interaction (GFS-NPIS) is proposed. Firstly, a new evaluation function measuring feature interaction is proposed. Secondly, a grouping strategy based on approximat e Markov blanket is used to remove strong redundant features. Lastly, a new feature selection method called as GFS-NPIS is given. In order to verify the effectiveness of our method, we compare GFS-NPIS with other eight representative ones on three classifiers (SVM, KNN and CART). The experimental results on fifteen public data sets show that GFS-NPIS outperforms others in terms of classification accuracy and Macro-F1.},
  archive      = {J_IDA},
  author       = {Zhou, Hongfang and An, Lei and Zhu, Rourou},
  doi          = {10.3233/IDA-226551},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {361-377},
  shortjournal = {Intell. Data Anal.},
  title        = {A grouping feature selection method based on feature interaction},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel feature selection method considering feature
interaction in neighborhood rough set. <em>IDA</em>, <em>27</em>(2),
345–359. (<a href="https://doi.org/10.3233/IDA-216447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has been shown to be a highly valuable strategy in data mining, pattern recognition, and machine learning. However, the majority of proposed feature selection methods do not account for feature interaction while calculating feature correlations. Interactive features are those feat ures that have less individual relevance with the class, but can provide more joint information for the class when combined with other features. Inspired by it, a novel feature selection algorithm considering feature relevance, redundancy, and interaction in neighborhood rough set is proposed. First of all, a new method of information measurement called neighborhood symmetric uncertainty is proposed, to measure what proportion data a feature contains regarding category label. Afterwards, a new objective evaluation function of the interactive selection is developed. Then a novel feature selection algorithm named (NSUNCMI) based on measuring feature correlation, redundancy and interactivity is proposed. The results on the nine universe datasets and five representative feature selection algorithms indicate that NSUNCMI reduces the dimensionality of feature space efficiently and offers the best average classification accuracy.},
  archive      = {J_IDA},
  author       = {Wang, Wenjing and Guo, Min and Han, Tongtong and Ning, Shiyong},
  doi          = {10.3233/IDA-216447},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {345-359},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel feature selection method considering feature interaction in neighborhood rough set},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-training algorithm based on density peaks combining
globally adaptive multi-local noise filter. <em>IDA</em>,
<em>27</em>(2), 323–343. (<a
href="https://doi.org/10.3233/IDA-226575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-training algorithm highlights the speed of training a supervised classifier through small labeled samples and large unlabeled samples. Despite its long considerable success, self-training algorithm has suffered from mislabeled samples. Local noise filters are designed to detect mislabeled samp les. However, two major problem with this kind of application are: (a) Current local noise filters have not treated the spatial distribution of the nearest neighbors in different classes in much detail. (b) They are being disadvantaged when mislabeled samples are located in overlapping areas of different classes. Here, we develop an integrated architecture – self-training algorithm based on density peaks combining globally adaptive multi-local noise filter (STDP-GAMLNF), to improve detecting efficiency. Firstly, the spatial structure of the data set is revealed by density peak clustering, and it is used for empowering self-training to label unlabeled samples. In the meantime, after each epoch of labeling, GAMLNF can comprehensively judge whether a sample is a mislabeled sample from multiple classes or not, and it will reduce the influence of edge samples effectively. The corresponding experimental results conducted on eighteen UCI data sets demonstrate that GAMLNF is not sensitive to the value of the neighbor parameter k, and it is capable of adaptively finding the appropriate number of neighbors of each class.},
  archive      = {J_IDA},
  author       = {Li, Shuaijun and Lu, Jia},
  doi          = {10.3233/IDA-226575},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {323-343},
  shortjournal = {Intell. Data Anal.},
  title        = {Self-training algorithm based on density peaks combining globally adaptive multi-local noise filter},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Editorial. <em>IDA</em>, <em>27</em>(2), 319–321. (<a
href="https://doi.org/10.3233/IDA-230002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-230002},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {319-321},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding reinforced structural hole spanners in social
networks via node embedding. <em>IDA</em>, <em>27</em>(1), 297–318. (<a
href="https://doi.org/10.3233/IDA-226836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying structural hole spanners that benefit from acting as bridges between communities is a core study in social network analysis. Existing methods for identification mainly focus on measuring the ability of users to control information propagation by bridging holes, while ignoring the impact of reinforcement of the holes themselves on the benefits of bridging spanners. A recent sociological study shows that the more reinforced a hole is, the more likely it is to bring high benefits to its spanners. In this paper, we propose a node embedding-based method ReHSe for identifying reinforced structural hole spanners in social networks. Specifically, an integrated embedding method is devised to extract features encoding reinforcement properties of nodes into a low-dimensional space. Further, to improve the robustness and accuracy of identification, an incremental learning strategy based on a reserved set is employed to train a scoring network in this subspace, to find top-k reinforced hole spanners. Extensive experimental results show that the performance of hole spanners identified by the proposed method outperforms several existing methods.},
  archive      = {J_IDA},
  author       = {Li, Mengshi and Huang, Feihu and Peng, Jian},
  doi          = {10.3233/IDA-226836},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {297-318},
  shortjournal = {Intell. Data Anal.},
  title        = {Finding reinforced structural hole spanners in social networks via node embedding},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently mining maximal l-reachability co-location
patterns from spatial data sets. <em>IDA</em>, <em>27</em>(1), 269–295.
(<a href="https://doi.org/10.3233/IDA-216515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A co-location pattern is a set of spatial features that are strongly correlated in space. However, some of these patterns could be neglected if the prevalence metrics are based solely on the clique (or star) relationship. Hence, the l-reachability co-location pattern is proposed by introducing the l-reachability clique where the members of each instance pair can be reachable to each other in a given step length l. Because the average size of l-reachability co-location patterns tends to be longer, maximal l-reachability co-location pattern mining is researched in this paper. First, some sparsification strategies are introduced to shorten star neighborhood lists of instances in an updated graph called the l-reachability neighbor relationship graph, and then, they are grouped by their corresponding patterns. Second, candidate maximal l-reachability co-location patterns are iteratively detected in a size-independent way on bi-graphs that contain group keys and their intersection sets. Third, the prevalence of each candidate maximal l-reachability co-location pattern is checked in a binary search way with a natural l-reachability clique called the ⌊l/2⌋-reachability neighborhood list. Finally, the effectiveness and efficiency of our model and algorithms are analyzed by extensive comparison experiments on synthetic and real-world spatial data sets.},
  archive      = {J_IDA},
  author       = {Zou, Muquan and Wang, Lizhen and Wu, Pingping and Tran, Vanha},
  doi          = {10.3233/IDA-216515},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {269-295},
  shortjournal = {Intell. Data Anal.},
  title        = {Efficiently mining maximal l-reachability co-location patterns from spatial data sets},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal motif-based attentional graph convolutional network
for dynamic link prediction. <em>IDA</em>, <em>27</em>(1), 241–268. (<a
href="https://doi.org/10.3233/IDA-216169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic link prediction is an important component of the dynamic network analysis with many real-world applications. Currently, most advancements focus on analyzing link-defined neighborhoods with graph convolutional networks (GCN), while ignoring the influence of higher-order structural and tempor al interacting features on link formation. Therefore, based on recent progress in modeling temporal graphs, we propose a novel temporal motif-based attentional graph convolutional network model (TMAGCN) for dynamic link prediction. As dynamic graphs usually contain periodical patterns, we first propose a temporal motif matrix construction method to capture higher-order structural and temporal features, then introduce a spatial convolution operation following a temporal motif-attention mechanism to encode these features into node embeddings. Furthermore, we design two methods to combine multiple temporal motif-based attentions, a dynamic attention-based method and a reinforcement learning-based method, to allow each individual node to make the most of the relevant motif-based neighborhood to propagate and aggregate information in the graph convolutional layers. Experimental results on various real-world datasets demonstrate that the proposed model is superior to state-of-the-art baselines on the dynamic link prediction task. It also reveals that temporal motif can manifest the essential dynamic mechanism of the network.},
  archive      = {J_IDA},
  author       = {Wu, Zheng and Chen, Hongchang and Zhang, Jianpeng and Pei, Yulong and Huang, Zishuo},
  doi          = {10.3233/IDA-216169},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {241-268},
  shortjournal = {Intell. Data Anal.},
  title        = {Temporal motif-based attentional graph convolutional network for dynamic link prediction},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian estimation of decay parameters in hawkes processes.
<em>IDA</em>, <em>27</em>(1), 223–240. (<a
href="https://doi.org/10.3233/IDA-216283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hawkes processes with exponential kernels are a ubiquitous tool for modeling and predicting event times. However, estimating their decay parameter is challenging, and there is a remarkable variability among decay parameter estimates. Moreover, this variability increases substantially in cases of a small number of realizations of the process or due to sudden changes to a system under study, for example, in the presence of exogenous shocks. In this work, we demonstrate that these estimation difficulties relate to the noisy, non-convex shape of the Hawkes process’ log-likelihood as a function of the decay. To address uncertainty in the estimates, we propose to use a Bayesian approach to learn more about likely decay values. We show that our approach alleviates the decay estimation problem across a range of experiments with synthetic and real-world data. With our work, we support researchers and practitioners in their applications of Hawkes processes in general and in their interpretation of Hawkes process parameters in particular.},
  archive      = {J_IDA},
  author       = {Santos, Tiago and Lemmerich, Florian and Helic, Denis},
  doi          = {10.3233/IDA-216283},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {223-240},
  shortjournal = {Intell. Data Anal.},
  title        = {Bayesian estimation of decay parameters in hawkes processes},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive active learning algorithm with informativeness
and representativeness. <em>IDA</em>, <em>27</em>(1), 199–222. (<a
href="https://doi.org/10.3233/IDA-216418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning focuses on selecting a small subset of the most valuable instances for labeling to learn a highly accurate model. Considering informativeness and representativeness of unlabeled instances is significant for a query, some works have been done about combing informativeness and represe ntativeness criteria. However, most of them are generally in a fixed manner to balance these criteria, and difficult to find suitable sampling strategies and weights of informativeness and representativeness for various datasets. In this paper, an adaptive active learning method ALIR is proposed to address these limitations. Firstly, an adaptive active learning framework is represented, in which the weight of informativeness and representativeness criteria can be dynamically updated by the feedback of previous learning processes. Secondly, by formulating the active learning as a Markov decision process, ALIR can adaptively select the suitable sampling strategies according to the reward of the learning process. Finally, extensive experimental results over several benchmark datasets and two real classification datasets demonstrate that ALIR outperforms several state-of-the-art methods. Different from traditional active learning algorithms, ALIR can adaptively select sampling strategies and adjust the weights simultaneously, which helps it more feasible in the application.},
  archive      = {J_IDA},
  author       = {Lv, Qiuyue and Dong, Minggang},
  doi          = {10.3233/IDA-216418},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {199-222},
  shortjournal = {Intell. Data Anal.},
  title        = {An adaptive active learning algorithm with informativeness and representativeness},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guided node graph convolutional networks for repository
recommendation. <em>IDA</em>, <em>27</em>(1), 181–198. (<a
href="https://doi.org/10.3233/IDA-216250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) has been widely used in the field of recommender systems. There are some nodes in KG that guide the occurrence of interaction behaviors. We call them guided nodes. However, the current application doesn’t take into account the guided nodes in KG. We explore the utility of guide d nodes in KG. It is applied in repository recommendations. In this paper, we propose an end-to-end framework, namely Guided Node Graph Convolutional Network (GNGCN), which effectively captures the connections between entities by mining the influence of related nodes. We extract samples of each entity in KG as their guided nodes and then combine the information and bias of the guided nodes when computing the representation of a given entity. The guided nodes can be extended to multiple hops. We evaluate our model on a real-world Github dataset named Github-SKG and music recommendation dataset, and the experimental results show that the method outperforms the recommendation baselines and our model is much lighter than others.},
  archive      = {J_IDA},
  author       = {Tan, Guoqiang and Shi, Yuliang and Wang, Jihu and Li, Hui and Chen, Zhiyong and Wang, Xinjun},
  doi          = {10.3233/IDA-216250},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {181-198},
  shortjournal = {Intell. Data Anal.},
  title        = {Guided node graph convolutional networks for repository recommendation},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple independent losses scheduling: A simple training
method for deep neural networks. <em>IDA</em>, <em>27</em>(1), 165–180.
(<a href="https://doi.org/10.3233/IDA-216401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, various loss functions have been proposed to boost the performance of deep neural networks. Every loss function has its own specific theoretical motivation, and can easily learn its preference features of training data compared with other loss functions. Thus, combining multiple lo ss functions to capture more data features becomes an attractive idea for model performance improvement. In this paper, instead of using a single loss function or a linear weighted sum of multiple loss functions, we present the method named Multiple Independent Losses Scheduling (MILS), which allows multiple loss functions to independently participate in the training process according to their performance. Specifically, for all candidate loss functions, one loss function will be predefined as the primary loss function before training, and the other loss functions will play auxiliary roles for possible contributions to improve the model performance. In order to avoid auxiliary loss functions bringing a negative effect on the model performance in the training process, we developed a simple but effective performance-based scheduling algorithm to prevent auxiliary loss functions from dragging down the model performance. Extensive experiments using various deep architectures on various recognition benchmarks demonstrate our scheme is simple, robust, lightweight, and effective for typical classification tasks.},
  archive      = {J_IDA},
  author       = {Deng, Jiali and Gong, Haigang and Wang, Xiaomin and Liu, Minghui and Xie, Tianshu and Cheng, Xuan and Liu, Ming and Huang, Wanqing},
  doi          = {10.3233/IDA-216401},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {165-180},
  shortjournal = {Intell. Data Anal.},
  title        = {Multiple independent losses scheduling: A simple training method for deep neural networks},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inferring student social link from spatiotemporal behavior
data via entropy-based analyzing model. <em>IDA</em>, <em>27</em>(1),
137–163. (<a href="https://doi.org/10.3233/IDA-216318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social link is an important index to understand master students’ mental health and social ability in educational management. Extracting hidden social strength from students’ rich daily life behaviors has also become an attractive research hotspot. Devices with positioning functions record many stud ents’ spatiotemporal behavior data, which can infer students’ social links. However, under the guidance of school regulations, students’ daily activities have a certain regularity and periodicity. Traditional methods usually compare the co-occurrence frequency of two users to infer social association but do not consider the location-intensive and time-sensitive in campus scenes. Aiming at the campus environment, a Spatiotemporal Entropy-Based Analyzing (S-EBA) model for inferring students’ social strength is proposed. The model is based on students’ multi-source heterogeneous behavioral data to calculate the frequency of co-occurrence under the influence of time intervals. Then, the three features of diversity, spatiotemporal hotspot and behavior similarity are introduced to calculate social strength. Experiments show that our method is superior to the traditional methods under many evaluating criteria. The inferred social strength is used as the weight of the edge to construct a social network further to analyze its important impact on students’ education management.},
  archive      = {J_IDA},
  author       = {Li, Mengran and Zhang, Yong and Li, Xiaoyong and Lin, Xuanqi and Yin, Baocai},
  doi          = {10.3233/IDA-216318},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {137-163},
  shortjournal = {Intell. Data Anal.},
  title        = {Inferring student social link from spatiotemporal behavior data via entropy-based analyzing model},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wide &amp; deep generative adversarial networks for
recommendation system. <em>IDA</em>, <em>27</em>(1), 121–136. (<a
href="https://doi.org/10.3233/IDA-216400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) has achieved great success in computer vision like Image Inpainting, Image Super-Resolution. Many researchers apply it to improve the effectiveness of recommendation system. However, GANs-based methods obtain users’ preferences using a single Neural Network fr amework in generative model, which may not be fully mined. Furthermore, most GANs-based algorithms adopt cross-entropy loss to get pair-wise bias, but these methods don’t reveal global data distribution loss when data are sparse. Those problems will influence the performance of the algorithm and result in poor accuracy. To address these problems, we introduce Wide &amp; Deep Generative Adversarial Networks for Recommendation System (a.k.a W &amp; DGAN) in this paper. On the one hand, we employ Wide &amp; Deep Learning as a generative model capable of extracting both explicit and implicit information of user preferences. Furthermore, we combine Cross-Entropy loss in G with Wasserstein loss in D to get data distribution, then, the joint loss will be to receive the training information feedback from data distribution. Empirical results on three public benchmarks show that W&amp;DGAN significantly outperforms state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Li, Jianhong and Li, Jianhua and Wang, Chengjun and Zhao, Xin},
  doi          = {10.3233/IDA-216400},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {121-136},
  shortjournal = {Intell. Data Anal.},
  title        = {Wide &amp;amp; deep generative adversarial networks for recommendation system},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative optimization with PSO for named entity
recognition-based applications. <em>IDA</em>, <em>27</em>(1), 103–120.
(<a href="https://doi.org/10.3233/IDA-216483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) as a crucial technology is widely used in many application scenarios, including information extraction, information retrieval, text summarization, and machine translation assisted in AI-based smart communication and networking systems. As people pay more and more atte ntion to NER, it has gradually become an independent and important research field. Currently, most of the NER models need to manually adjust their hyper-parameters, which is not only time-consuming and laborious, but also easy to fall into a local optimal situation. To deal with such problem, this paper proposes a machine learning-guided model to achieve NER, where the hyper-parameters of model are automatically adjusted to improve the computational performance. Specifically, the proposed model is implemented by using bi-directional encoder representation from transformers (BERT) and conditional random field (CRF). Meanwhile, the collaborative computing paradigm is also fused in the model, while utilizing the particle swarm optimization (PSO) to automatically search for the best value of hyper-parameters in a collaborative way. The experimental results demonstrate the satisfactory performance of our proposed model.},
  archive      = {J_IDA},
  author       = {Peng, Qiaojuan and Luo, Xiong and Shen, Hailun and Huang, Ziyang and Chen, Maojian},
  doi          = {10.3233/IDA-216483},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {103-120},
  shortjournal = {Intell. Data Anal.},
  title        = {Collaborative optimization with PSO for named entity recognition-based applications},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting traffic crash severity using hybrid of balanced
bagging classification and light gradient boosting machine.
<em>IDA</em>, <em>27</em>(1), 79–101. (<a
href="https://doi.org/10.3233/IDA-216398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accident severity prediction is a hot topic of research aimed at ensuring road safety as well as taking precautionary measures for anticipated future road crashes. In the past decades, both classical statistical methods and machine learning algorithms have been used to predict traffic crash severit y. However, most of these models suffer from several drawbacks including low accuracy, and lack of interpretability for people. To address these issues, this paper proposed a hybrid of Balanced Bagging Classification (BBC) and Light Gradient Boosting Machine (LGBM) to improve the accuracy of crash severity prediction and eliminate the issues of bias and variance. To the best of the author’s knowledge, this is one of the pioneer studies which explores the application of BBC-LGBM to predict traffic crash severity. On the accident dataset of Great Britain (UK) from 2013 to 2019, the proposed model has demonstrated better performance when compared with other models such as Gaussian Naïve Bayes (GNB), Support vector machines (SVM), and Random Forest (RF). More specifically, the proposed model managed to achieve better performance among all metrics for the testing dataset (accuracy = 77.7%, precision = 75%, recall = 73%, F1-Score = 68%). Moreover, permutation importance is used to interpret the results and analyze the importance of each factor influencing crash severity. The accuracy-enhanced model is significant to several stakeholders including drivers for early alarm and government departments, insurance companies, and even hospitals for the services concerned about human lives and property damage in road crashes.},
  archive      = {J_IDA},
  author       = {Niyogisubizo, Jovial and Liao, Lyuchao and Zou, Fumin and Han, Guangjie and Nziyumva, Eric and Li, Ben and Lin, Yuyuan},
  doi          = {10.3233/IDA-216398},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {79-101},
  shortjournal = {Intell. Data Anal.},
  title        = {Predicting traffic crash severity using hybrid of balanced bagging classification and light gradient boosting machine},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LSEC: Large-scale spectral ensemble clustering.
<em>IDA</em>, <em>27</em>(1), 59–77. (<a
href="https://doi.org/10.3233/IDA-216240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental problem in machine learning is ensemble clustering, that is, combining multiple base clusterings to obtain improved clustering result. However, most of the existing methods are unsuitable for large-scale ensemble clustering tasks owing to efficiency bottlenecks. In this paper, we prop ose a large-scale spectral ensemble clustering (LSEC) method to balance efficiency and effectiveness. In LSEC, a large-scale spectral clustering-based efficient ensemble generation framework is designed to generate various base clusterings with low computational complexity. Thereafter, all the base clusterings are combined using a bipartite graph partition-based consensus function to obtain improved consensus clustering results. The LSEC method achieves a lower computational complexity than most existing ensemble clustering methods. Experiments conducted on ten large-scale datasets demonstrate the efficiency and effectiveness of the LSEC method. The MATLAB code of the proposed method and experimental datasets are available at https://github.com/Li-Hongmin/MyPaperWithCode.},
  archive      = {J_IDA},
  author       = {Li, Hongmin and Ye, Xiucai and Imakura, Akira and Sakurai, Tetsuya},
  doi          = {10.3233/IDA-216240},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {59-77},
  shortjournal = {Intell. Data Anal.},
  title        = {LSEC: Large-scale spectral ensemble clustering},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial-temporal trajectory anomaly detection based on an
improved spectral clustering algorithm. <em>IDA</em>, <em>27</em>(1),
31–58. (<a href="https://doi.org/10.3233/IDA-216185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of wireless communication technology, when users use wireless networks to meet various needs, wireless networks also record a large number of users’ spatial-temporal trajectory data. In order to better pay attention to the healthy development of students and promote the informa tion construction on campus, a spectral clustering algorithm based on the multi-scale threshold and density combined with shared nearest neighbors (MSTDSNN-SC) is proposed. Firstly, it improves the affinity distance function based on the shortest time dis-tance-shortest time distance sub-sequence (STD-STDSS) by adding location popularity and uses this model to construct the initial adjacency matrix. Then it introduces the covariance scale threshold and spatial scale threshold to perform 0–1 processing on the adjacency matrix to obtain more accurate sample similarity. Next, it constructs an eigenvector space by eigenvalue decom-position of the adjacency matrix. Finally, it uses DBSCAN clustering algorithm with shared nearest neighbors to avoid to manually determine the number of clusters. Taking Internet usage data on campus as an example, multiple clustering algorithms are used for anomaly detection and four evaluation metrics are applied to estimate the clustering results. MSTDSNN-SC algorithm reflects better clustering performance. Furthermore, the abnormal trajectories list is verified to be effective and credible.},
  archive      = {J_IDA},
  author       = {Guo, Yishan and Liu, Mandan},
  doi          = {10.3233/IDA-216185},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {31-58},
  shortjournal = {Intell. Data Anal.},
  title        = {Spatial-temporal trajectory anomaly detection based on an improved spectral clustering algorithm},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing outlier-free histograms with variable bin-width
based on distance minimization. <em>IDA</em>, <em>27</em>(1), 5–29. (<a
href="https://doi.org/10.3233/IDA-216316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method of constructing a variable bin width histogram that can accommodate the unbalanced distribution of the samples yet retaining, as a whole, the good aspect of both equal width (EW) and equal-area (EA) histograms that are being used popularly for data visualization and analysis . We formulate this as an optimal change point detection problem in which the bin boundaries are determined by minimizing the sum of the absolute error or the squared error in each bin. The former is based on Distance Minimization (DM) and new, and the latter is based on Variance Minimization (VM) and is considered the state-of-the-art. The constructed histograms can effectively be used to detect and visualize hidden outliers/anomalies by applying the interquartile range method in each bin. The final histograms are obtained by adjusting bin boundaries and heights accordingly after removing the detected outliers/anomalies. We further propose a method to annotate the constructed bins if the data for annotation is given for each sample as a set of nominal variables, using z-score with respect to their distribution within each bin. We applied our method to both real vinyl greenhouse datasets and two different sets of three synthetic datasets, and confirmed that both DM and VM methods work as intended, both can represent the sample distribution with a smaller number of bins than those by EW and EA methods, The use of interquartile range method can detect anomalies as well as outliers, and the terms selected for annotation are interpretable and reasonable. EW and EA methods have contrasting properties. DM and VM methods lie in between, but the former is closer to EA method and the latter to EW method. DM method runs substantially faster than VM method and performs slightly better than VM method in outlier detection and annotation tasks.},
  archive      = {J_IDA},
  author       = {Fushimi, Takayasu and Saito, Kazumi and Motoda, Hiroshi},
  doi          = {10.3233/IDA-216316},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {5-29},
  shortjournal = {Intell. Data Anal.},
  title        = {Constructing outlier-free histograms with variable bin-width based on distance minimization},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023g). Editorial. <em>IDA</em>, <em>27</em>(1), 1–3. (<a
href="https://doi.org/10.3233/IDA-230001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-230001},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {27},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
