<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---27">SW - 27</h2>
<ul>
<li><details>
<summary>
(2023). Sem@ k: Is my knowledge graph embedding model
semantic-aware? <em>SW</em>, <em>14</em>(6), 1273–1309. (<a
href="https://doi.org/10.3233/SW-233508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using knowledge graph embedding models (KGEMs) is a popular approach for predicting links in knowledge graphs (KGs). Traditionally, the performance of KGEMs for link prediction is assessed using rank-based metrics, which evaluate their ability to give high scores to ground-truth entities. However, the literature claims that the KGEM evaluation procedure would benefit from adding supplementary dimensions to assess. That is why, in this paper, we extend our previously introduced metric Sem@ K that measures the capability of models to predict valid entities w.r.t. domain and range constraints. In particular, we consider a broad range of KGs and take their respective characteristics into account to propose different versions of Sem@ K . We also perform an extensive study to qualify the abilities of KGEMs as measured by our metric. Our experiments show that Sem@ K provides a new perspective on KGEM quality. Its joint analysis with rank-based metrics offers different conclusions on the predictive power of models. Regarding Sem@ K , some KGEMs are inherently better than others, but this semantic superiority is not indicative of their performance w.r.t. rank-based metrics. In this work, we generalize conclusions about the relative performance of KGEMs w.r.t. rank-based and semantic-oriented metrics at the level of families of models. The joint analysis of the aforementioned metrics gives more insight into the peculiarities of each model. This work paves the way for a more comprehensive evaluation of KGEM adequacy for specific downstream tasks.},
  archive      = {J_SW},
  author       = {Hubert, Nicolas and Monnin, Pierre and Brun, Armelle and Monticolo, Davy},
  doi          = {10.3233/SW-233508},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1273-1309},
  shortjournal = {Semantic Web},
  title        = {Sem@ k: Is my knowledge graph embedding model semantic-aware?},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using semantic story maps to describe a territory beyond its
map. <em>SW</em>, <em>14</em>(6), 1255–1272. (<a
href="https://doi.org/10.3233/SW-233485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents the Story Map Building and Visualizing Tool (SMBVT) that allows users to create story maps within a collaborative environment and a usable Web interface. It is entirely open-source and published as a free-to-use solution. It uses Semantic Web technologies in the back-end system to represent stories through a reference ontology for representing narratives. It builds up a user-shared semantic knowledge base that automatically interconnects all stories and seamlessly enables collaborative story building. Finally, it operates within an Open-Science oriented e-Infrastructure, which enables data and information sharing within communities of narrators, and adds multi-tenancy, multi-user, security, and access-control facilities. SMBVT represents narratives as a network of spatiotemporal events related by semantic relations and standardizes the event descriptions by assigning internationalized resource identifiers (IRIs) to the event components , i.e., the entities that take part in the event (e.g., persons, objects, places, concepts). The tool automatically saves the collected knowledge as a Web Ontology Language (OWL) graph and openly publishes it as Linked Open Data. This feature allows connecting the story events to other knowledge bases. To evaluate and demonstrate our tool, we used it to describe the Apuan Alps territory in Tuscany (Italy). Based on a user-test evaluation, we assessed the tool’s effectiveness at building story maps and the ability of the produced story to describe the territory beyond the map.},
  archive      = {J_SW},
  author       = {Bartalesi, Valentina and Coro, Gianpaolo and Lenzi, Emanuele and Pratelli, Nicolò and Pagano, Pasquale and Felici, Francesco and Moretti, Michele and Brunori, Gianluca},
  doi          = {10.3233/SW-233485},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1255-1272},
  shortjournal = {Semantic Web},
  title        = {Using semantic story maps to describe a territory beyond its map},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Focused categorization power of ontologies: General
framework and study on simple existential concept expressions.
<em>SW</em>, <em>14</em>(6), 1209–1253. (<a
href="https://doi.org/10.3233/SW-233401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When reusing existing ontologies for publishing a dataset in RDF (or developing a new ontology), preference may be given to those providing extensive subcategorization for important classes (denoted as focus classes). The subcategories may consist not only of named classes but also of compound class expressions. We define the notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable initial experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms (so-called FCE patterns). The characteristics of the chosen concept expression language and associated FCE patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression pattern frequency in class definitions; second, the occurrence of FCE patterns in the Tbox of ontologies; and last, for class expressions generated from the Tbox of ontologies (through the FCE patterns); their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression patterns. The complementary analyses are then compared and summarized. To allow for further experimentation, a web-based prototype was also implemented, which covers the whole process of ontology reuse from keyword-based ontology search through the FCP computation to the selection of ontologies and their enrichment with new concepts built from compound expressions.},
  archive      = {J_SW},
  author       = {Svátek, Vojtěch and Zamazal, Ondřej and Nguyen, Viet Bach and Ivánek, Jiří and Kľuka, Ján and Vacura, Miroslav},
  doi          = {10.3233/SW-233401},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1209-1253},
  shortjournal = {Semantic Web},
  title        = {Focused categorization power of ontologies: General framework and study on simple existential concept expressions},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifiable integrity for linked data on the web.
<em>SW</em>, <em>14</em>(6), 1167–1207. (<a
href="https://doi.org/10.3233/SW-233409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to publish Linked Data on the Web with quantifiable integrity using Web technologies, and in which rational agents are incentivised to contribute to the integrity of the link network. To this end, we introduce self-verifying resource representations, that include Linked Data Signatures whose signature value is used as a suffix in the resource’s URI. Links among such representations, typically managed as web documents, contribute therefore to preserving the integrity of the resulting document graphs. To quantify how well a document’s integrity can be relied on, we introduce the notion of trust scores and present an interpretation based on hubs and authorities. In addition, we present how specific agent behaviour may be induced by the choice of trust score regarding their optimisation, e.g., in general but also using a heuristic strategy called Additional Reach Strategy (ARS). We discuss our approach in a three-fold evaluation: First, we evaluate the effect of different graph metrics as trust scores on induced agent behaviour and resulting evolution of the document graph. We show that trust scores based on hubs and authorities induce agent behaviour that contributes to integrity preservation in the document graph. Next, we evaluate different heuristics for agents to optimise trust scores when general optimisation strategies are not applicable. We show that ARS outperforms other potential optimisation strategies. Last, we evaluate the whole approach by examining the resilience of integrity preservation in a document graph when resources are deleted. To this end, we propose a simulation system based on the Watts–Strogatz model for simulating a social network. We show that our approach produces a document graph that can recover from such attacks or failures in the document graph.},
  archive      = {J_SW},
  author       = {Braun, Christoph H.-J. and Käfer, Tobias},
  doi          = {10.3233/SW-233409},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1167-1207},
  shortjournal = {Semantic Web},
  title        = {Quantifiable integrity for linked data on the web},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing SPARQL queries over decentralized knowledge
graphs. <em>SW</em>, <em>14</em>(6), 1121–1165. (<a
href="https://doi.org/10.3233/SW-233438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the Web of Data in principle offers access to a wide range of interlinked data, the architecture of the Semantic Web today relies mostly on the data providers to maintain access to their data through SPARQL endpoints. Several studies, however, have shown that such endpoints often experience downtime, meaning that the data they maintain becomes inaccessible. While decentralized systems based on Peer-to-Peer (P2P) technology have previously shown to increase the availability of knowledge graphs, even when a large proportion of the nodes fail, processing queries in such a setup can be an expensive task since data necessary to answer a single query might be distributed over multiple nodes. In this paper, we therefore propose an approach to optimizing SPARQL queries over decentralized knowledge graphs, called Lothbrok . While there are potentially many aspects to consider when optimizing such queries, we focus on three aspects: cardinality estimation, locality awareness, and data fragmentation. We empirically show that Lothbrok is able to achieve significantly faster query processing performance compared to the state of the art when processing challenging queries as well as when the network is under high load.},
  archive      = {J_SW},
  author       = {Aebeloe, Christian and Montoya, Gabriela and Hose, Katja},
  doi          = {10.3233/SW-233438},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1121-1165},
  shortjournal = {Semantic Web},
  title        = {Optimizing SPARQL queries over decentralized knowledge graphs},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DegreEmbed: Incorporating entity embedding into logic rule
learning for knowledge graph reasoning. <em>SW</em>, <em>14</em>(6),
1099–1119. (<a href="https://doi.org/10.3233/SW-233413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for inferring on KGs. Specifically, we study the problem of predicting missing links in heterogeneous KGs from the perspective of the degree of nodes. Experimentally, we demonstrate that our DegreEmbed model outperforms the state-of-the-art methods on real world datasets and the rules mined by our model are of high quality and interpretability.},
  archive      = {J_SW},
  author       = {Li, Haotian and Liu, Hongri and Wang, Yao and Xin, Guodong and Wei, Yuliang},
  doi          = {10.3233/SW-233413},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1099-1119},
  shortjournal = {Semantic Web},
  title        = {DegreEmbed: Incorporating entity embedding into logic rule learning for knowledge graph reasoning},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A conceptual model for ontology quality assessment.
<em>SW</em>, <em>14</em>(6), 1051–1097. (<a
href="https://doi.org/10.3233/SW-233393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of methods, tools, and techniques in ontology development, ontologies have emerged in various fields such as machine learning, robotics, biomedical informatics, agricultural informatics, crowdsourcing, database management, and the Internet of Things. Nevertheless, the nonexistence of a universally agreed methodology for specifying and evaluating the quality of an ontology hinders the success of ontology-based systems in such fields as the quality of each component is required for the overall quality of a system and in turn impacts the usability in use. Moreover, a number of anomalies in definitions of ontology quality concepts are visible, and in addition to that, the ontology quality assessment is limited only to a certain set of characteristics in practice even though some other significant characteristics have to be considered for the specified use-case. Thus, in this research, a comprehensive analysis was performed to uncover the existing contributions specifically on ontology quality models, characteristics, and the associated measures of these characteristics. Consequently, the characteristics identified through this review were classified with the associated aspects of the ontology evaluation space. Furthermore, the formalized definitions for each quality characteristic are provided through this study from the ontological perspective based on the accepted theories and standards. Additionally, a thorough analysis of the extent to which the existing works have covered the quality evaluation aspects is presented and the areas further to be investigated are outlined.},
  archive      = {J_SW},
  author       = {Wilson, R.S.I. and Goonetillake, J.S. and Indika, W.A. and Ginige, Athula},
  doi          = {10.3233/SW-233393},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {1051-1097},
  shortjournal = {Semantic Web},
  title        = {A conceptual model for ontology quality assessment},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conjunctive query answering over unrestricted OWL 2
ontologies. <em>SW</em>, <em>14</em>(6), 997–1050. (<a
href="https://doi.org/10.3233/SW-233382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conjunctive Query (CQ) answering is a primary reasoning task over knowledge bases. However, when considering expressive description logics, query answering can be computationally very expensive; reasoners for CQ answering, although heavily optimized, often sacrifice expressive power of the input ontology or completeness of the computed answers in order to achieve tractability and scalability for the problem. In this work, we present a hybrid query answering architecture that combines various services to provide a CQ answering service for OWL. Specifically, it combines scalable CQ answering services for tractable languages with a CQ answering service for a more expressive language approaching the full OWL 2. If the query can be fully answered by one of the tractable services, then that service is used, to ensure maximum performance. Otherwise, the tractable services are used to compute lower and upper bound approximations. The union of the lower bounds and the intersection of the upper bounds are then compared. If the bounds do not coincide, then the “gap” answers are checked using the “full” service. These techniques led to the development of two new systems: (i) RSAComb, an efficient implementation of a new tractable answering service for RSA ( role safety acyclic ) (ii) ACQuA, a reference implementation of the proposed hybrid architecture combining RSAComb, PAGOdA, and HermiT to provide a CQ answering service for OWL. Our extensive evaluation shows how the additional computational cost introduced by reasoning over a more expressive language like RSA can still provide a significant improvement compared to relying on a fully-fledged reasoner. Additionally, we show how ACQuA can reliably match the performance of PAGOdA, a state-of-the-art CQ answering system that uses a similar approach, and can significantly improve performance when PAGOdA extensively relies on the underlying fully-fledged reasoner.},
  archive      = {J_SW},
  author       = {Igne, Federico and Germano, Stefano and Horrocks, Ian},
  doi          = {10.3233/SW-233382},
  journal      = {Semantic Web},
  month        = {12},
  number       = {6},
  pages        = {997-1050},
  shortjournal = {Semantic Web},
  title        = {Conjunctive query answering over unrestricted OWL&amp;nbsp;2 ontologies},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CiTIzen-centric DAta pLatform (TIDAL): Sharing distributed
personal data in a privacy-preserving manner for health research.
<em>SW</em>, <em>14</em>(5), 977–996. (<a
href="https://doi.org/10.3233/SW-223220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing personal data sharing tools and standards in conformity with data protection regulations is essential to empower citizens to control and share their health data with authorized parties for any purpose they approve. This can be, among others, for primary use in healthcare, or secondary use for research to improve human health and well-being. Ensuring that citizens are able to make fine-grained decisions about how their personal health data can be used and shared will significantly encourage citizens to participate in more health-related research. In this paper, we propose a ciTIzen-centric DatA pLatform (TIDAL) to give individuals ownership of their own data, and connect them with researchers to donate the use of their personal data for research while being in control of the entire data life cycle, including data access, storage and analysis. We recognize that most existing technologies focus on one particular aspect such as personal data storage, or suffer from executing data analysis over a large number of participants, or face challenges of low data quality and insufficient data interoperability. To address these challenges, the TIDAL platform integrates a set of components for requesting subsets of RDF (Resource Description Framework) data stored in personal data vaults based on SOcial LInked Data (Solid) technology and analyzing them in a privacy-preserving manner. We demonstrate the feasibility and efficiency of the TIDAL platform by conducting a set of simulation experiments using three different pod providers ( Inrupt , Solidcommunity , Self-hosted Server). On each pod provider, we evaluated the performance of TIDAL by querying and analyzing personal health data with varying scales of participants and configurations. The reasonable total time consumption and a linear correlation between the number of pods and variables on all pod providers show the feasibility and potential to implement and use the TIDAL platform in practice. TIDAL facilitates individuals to access their personal data in a fine-grained manner and to make their own decision on their data. Researchers are able to reach out to individuals and send them digital consent directly for using personal data for health-related research. TIDAL can play an important role to connect citizens, researchers, and data organizations to increase the trust placed by citizens in the processing of personal data.},
  archive      = {J_SW},
  author       = {Sun, Chang and Gallofré Ocaña, Marc and van Soest, Johan and Dumontier, Michel},
  doi          = {10.3233/SW-223220},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {977-996},
  shortjournal = {Semantic Web},
  title        = {CiTIzen-centric DAta pLatform (TIDAL): Sharing distributed personal data in a privacy-preserving manner for health research},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graphs for enhancing transparency in health data
ecosystems 1. <em>SW</em>, <em>14</em>(5), 943–976. (<a
href="https://doi.org/10.3233/SW-223294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tailoring personalized treatments demands the analysis of a patient’s characteristics, which may be scattered over a wide variety of sources. These features include family history, life habits, comorbidities, and potential treatment side effects. Moreover, the analysis of the services visited the most by a patient before a new diagnosis, as well as the type of requested tests, may uncover patterns that contribute to earlier disease detection and treatment effectiveness. Built on knowledge-driven ecosystems, we devise DE4LungCancer, a health data ecosystem of data sources for lung cancer. In this data ecosystem, knowledge extracted from heterogeneous sources, e.g., clinical records, scientific publications, and pharmacological data, is integrated into knowledge graphs. Ontologies describe the meaning of the combined data, and mapping rules enable the declarative definition of the transformation and integration processes. DE4LungCancer is assessed regarding the methods followed for data quality assessment and curation. Lastly, the role of controlled vocabularies and ontologies in health data management is discussed, as well as their impact on transparent knowledge extraction and analytics. This paper presents the lessons learned in the DE4LungCancer development. It demonstrates the transparency level supported by the proposed knowledge-driven ecosystem, in the context of the lung cancer pilots of the EU H2020-funded project BigMedilytic, the ERA PerMed funded project P4-LUCAT, and the EU H2020 projects CLARIFY and iASiS.},
  archive      = {J_SW},
  author       = {Aisopos, Fotis and Jozashoori, Samaneh and Niazmand, Emetis and Purohit, Disha and Rivas, Ariam and Sakor, Ahmad and Iglesias, Enrique and Vogiatzis, Dimitrios and Menasalvas, Ernestina and Rodriguez Gonzalez, Alejandro and Vigueras, Guillermo and Gomez-Bravo, Daniel and Torrente, Maria and Hernández López, Roberto and Provencio Pulla, Mariano and Dalianis, Athanasios and Triantafillou, Anna and Paliouras, Georgios and Vidal, Maria-Esther},
  doi          = {10.3233/SW-223294},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {943-976},
  shortjournal = {Semantic Web},
  title        = {Knowledge graphs for enhancing transparency in health data ecosystems 1},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware query derivation for IoT data streams with
DIVIDE enabling privacy by design. <em>SW</em>, <em>14</em>(5), 893–941.
(<a href="https://doi.org/10.3233/SW-223281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Internet of Things (IoT) sensor data from heterogeneous sources with domain knowledge and context information in real-time is a challenging task in IoT healthcare data management applications that can be solved with semantics. Existing IoT platforms often have issues with preserving the privacy of patient data. Moreover, configuring and managing context-aware stream processing queries in semantic IoT platforms requires much manual, labor-intensive effort. Generic queries can deal with context changes but often lead to performance issues caused by the need for expressive real-time semantic reasoning. In addition, query window parameters are part of the manual configuration and cannot be made context-dependent. To tackle these problems, this paper presents DIVIDE, a component for a semantic IoT platform that adaptively derives and manages the queries of the platform’s stream processing components in a context-aware and scalable manner, and that enables privacy by design. By performing semantic reasoning to derive the queries when context changes are observed, their real-time evaluation does require any reasoning. The results of an evaluation on a homecare monitoring use case demonstrate how activity detection queries derived with DIVIDE can be evaluated in on average less than 3.7 seconds and can therefore successfully run on low-end IoT devices.},
  archive      = {J_SW},
  author       = {De Brouwer, Mathias and Steenwinckel, Bram and Fang, Ziye and Stojchevska, Marija and Bonte, Pieter and De Turck, Filip and Van Hoecke, Sofie and Ongenae, Femke},
  doi          = {10.3233/SW-223281},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {893-941},
  shortjournal = {Semantic Web},
  title        = {Context-aware query derivation for IoT data streams with DIVIDE enabling privacy by design},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empowering machine learning models with contextual knowledge
for enhancing the detection of eating disorders in social media posts.
<em>SW</em>, <em>14</em>(5), 873–892. (<a
href="https://doi.org/10.3233/SW-223269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks have become information dissemination channels, where announcements are posted frequently; they also serve as frameworks for debates in various areas (e.g., scientific, political, and social). In particular, in the health area, social networks represent a channel to communicate and disseminate novel treatments’ success; they also allow ordinary people to express their concerns about a disease or disorder. The Artificial Intelligence (AI) community has developed analytical methods to uncover and predict patterns from posts that enable it to explain news about a particular topic, e.g., mental disorders expressed as eating disorders or depression. Albeit potentially rich while expressing an idea or concern, posts are presented as short texts, preventing, thus, AI models from accurately encoding these posts’ contextual knowledge. We propose a hybrid approach where knowledge encoded in community-maintained knowledge graphs (e.g., Wikidata) is combined with deep learning to categorize social media posts using existing classification models. The proposed approach resorts to state-of-the-art named entity recognizers and linkers (e.g., Falcon 2.0) to extract entities in short posts and link them to concepts in knowledge graphs. Then, knowledge graph embeddings (KGEs) are utilized to compute latent representations of the extracted entities, which result in vector representations of the posts that encode these entities’ contextual knowledge extracted from the knowledge graphs. These KGEs are combined with contextualized word embeddings (e.g., BERT) to generate a context-based representation of the posts that empower prediction models. We apply our proposed approach in the health domain to detect whether a publication is related to an eating disorder (e.g., anorexia or bulimia) and uncover concepts within the discourse that could help healthcare providers diagnose this type of mental disorder. We evaluate our approach on a dataset of 2,000 tweets about eating disorders. Our experimental results suggest that combining contextual knowledge encoded in word embeddings with the one built from knowledge graphs increases the reliability of the predictive models. The ambition is that the proposed method can support health domain experts in discovering patterns that may forecast a mental disorder, enhancing early detection and more precise diagnosis towards personalized medicine.},
  archive      = {J_SW},
  author       = {Benítez-Andrades, José Alberto and García-Ordás, María Teresa and Russo, Mayra and Sakor, Ahmad and Fernandes Rotger, Luis Daniel and Vidal, Maria-Esther},
  doi          = {10.3233/SW-223269},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {873-892},
  shortjournal = {Semantic Web},
  title        = {Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media&amp;nbsp;posts},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Terminology and ontology development for semantic
annotation: A use case on sepsis and adverse events. <em>SW</em>,
<em>14</em>(5), 811–871. (<a
href="https://doi.org/10.3233/SW-223226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotations enrich text corpora and provide necessary labels for natural language processing studies. To reason and infer underlying implicit knowledge captured by labels, an ontology is needed to provide a semantically annotated corpus with structured domain knowledge. Utilizing a corpus of adverse event documents annotated for sepsis-related signs and symptoms as a use case, this paper details how a terminology and corresponding ontology were developed. The Annotated Adverse Event NOte TErminology (AAENOTE) represents annotated documents and assists annotators in annotating text. In contrast, the complementary Catheter Infection Indications Ontology (CIIO) is intended for clinician use and captures domain knowledge needed to reason and infer implicit information from data. The approach taken makes ontology development understandable and accessible to domain experts without formal ontology training.},
  archive      = {J_SW},
  author       = {Yan, Melissa Y. and Gustad, Lise Tuset and Høvik, Lise Husby and Nytrø, Øystein},
  doi          = {10.3233/SW-223226},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {811-871},
  shortjournal = {Semantic Web},
  title        = {Terminology and ontology development for semantic annotation: A use case on sepsis and adverse events},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the usability of a semantic environmental health
data framework: Approach and study. <em>SW</em>, <em>14</em>(5),
787–810. (<a href="https://doi.org/10.3233/SW-223212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental exposures transported across air, land and water can affect our health making us more susceptible to developing a disease. Therefore, researchers need to face the complex task of integrating environmental exposures and linking them to health events with the relevant spatiotemporal and health context for individuals or populations. We present a usability evaluation approach and study of a semantic framework (i.e. Knowledge Graph, Methodology and User Interface) to enable Health Data Researchers (HDR) to link particular health events with environmental data for rare disease research. The usability study includes 17 HDRs with expertise in health data related to Anti-Neutrophil Cytoplasmic Antibody (ANCA)-associated vasculitis (AAV) in Ireland and Kawasaki Disease in Japan, and with no previous practical experience in using Semantic Web (SW) technologies. The evaluation results are promising in that they indicate that the framework is useful in allowing researchers themselves to link health and environmental data whilst hiding the complexities of SW technologies. As a result of this work, we also discuss the limitations of the approach together with the applicability to other domains. Beyond the direct impact on environmental health studies, the description of the evaluation approach can guide researchers in making SW technologies more accessible to domain experts through usability studies.},
  archive      = {J_SW},
  author       = {Navarro-Gallinad, Albert and Orlandi, Fabrizio and Scott, Jennifer and Little, Mark and O’Sullivan, Declan},
  doi          = {10.3233/SW-223212},
  journal      = {Semantic Web},
  month        = {5},
  number       = {5},
  pages        = {787-810},
  shortjournal = {Semantic Web},
  title        = {Evaluating the usability of a semantic environmental health data framework: Approach and study},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Security approaches for electronic health data handling
through the semantic web: A scoping review. <em>SW</em>, <em>14</em>(4),
771–784. (<a href="https://doi.org/10.3233/SW-223088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integration of health information systems are crucial to advance the effective delivery of healthcare for individuals and communities across organizational boundaries. Semantic Web technologies may be used to connect, correlate, and integrate heterogeneous datasets spread over the internet. However, when working with sensitive data, such as health data, security mechanisms are needed. A scoping review of the literature was undertaken to provide a broad view of security mechanisms applied to, or along with, Semantic Web technologies that could allow its use with health data. Searches were conducted in the most relevant databases for the scope of this work. The findings were classified according to the main objective and features presented by each solution. Twenty-six studies were included in the review. They introduced mechanisms that addressed several security attributes, such as authentication, authorization, integrity, availability, confidentiality, privacy, and provenance. These mechanisms support access control frameworks, semantic and functional interoperability infrastructures, and privacy compliance solutions. The findings suggest that the application and use of Semantic Web technologies is still growing, with the healthcare area being particularly interested. The main security mechanisms for Semantic Web technologies, the key security attributes and properties, and the main gaps in the literature were identified, helping to understand the technical needs to mitigate the risks of handling personal health information over the Semantic Web. Also, this research has shown that complex and robust solutions are available to successfully address several security properties and features, depending on the context that the electronic health data is being managed.},
  archive      = {J_SW},
  author       = {Costa Lima, Vinícius and Alves, Domingos and Andrade Bernardi, Filipe and Charters Lopes Rijo, Rui Pedro},
  doi          = {10.3233/SW-223088},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {771-784},
  shortjournal = {Semantic Web},
  title        = {Security approaches for electronic health data handling through the semantic web: A&amp;nbsp;scoping review},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic web technologies and bias in artificial
intelligence: A systematic literature review. <em>SW</em>,
<em>14</em>(4), 745–770. (<a
href="https://doi.org/10.3233/SW-223041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bias in Artificial Intelligence (AI) is a critical and timely issue due to its sociological, economic and legal impact, as decisions made by biased algorithms could lead to unfair treatment of specific individuals or groups. Multiple surveys have emerged to provide a multidisciplinary view of bias or to review bias in specific areas such as social sciences, business research, criminal justice, or data mining. Given the ability of Semantic Web (SW) technologies to support multiple AI systems, we review the extent to which semantics can be a “tool” to address bias in different algorithmic scenarios. We provide an in-depth categorisation and analysis of bias assessment, representation, and mitigation approaches that use SW technologies. We discuss their potential in dealing with issues such as representing disparities of specific demographics or reducing data drifts, sparsity, and missing values. We find research works on AI bias that apply semantics mainly in information retrieval, recommendation and natural language processing applications and argue through multiple use cases that semantics can help deal with technical, sociological, and psychological challenges.},
  archive      = {J_SW},
  author       = {Reyero Lobo, Paula and Daga, Enrico and Alani, Harith and Fernandez, Miriam},
  doi          = {10.3233/SW-223041},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {745-770},
  shortjournal = {Semantic Web},
  title        = {Semantic web technologies and bias in artificial intelligence: A systematic literature&amp;nbsp;review},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital humanities on the semantic web: Sampo model and
portal series. <em>SW</em>, <em>14</em>(4), 729–744. (<a
href="https://doi.org/10.3233/SW-223034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cultural heritage (CH) contents are typically strongly interlinked, but published in heterogeneous, distributed local data silos, making it difficult to utilize the data on a global level. Furthermore, the content is usually available only for humans to read, and not as data for Digital Humanities (DH) analyses and application development. This application report addresses these problems by presenting a collaborative publication model for CH Linked Data and six design principles for creating shared data services and semantic portals for DH research and applications. This Sampo model has evolved gradually in 2002–2021 through lessons learned when developing the Sampo series of linked data services and semantic portals in use, including MuseumFinland (2004), CultureSampo (2009), BookSampo (2011), WarSampo (2015), Norssit Alumni (2017), U.S. Congress Prosopographer (2018), NameSampo (2019), BiographySampo (2019), WarVictimSampo 1914–1922 (2019), MMM (2020), AcademySampo (2021), FindSampo (2021), WarMemoirSampo (2021), and LetterSampo (2022). These Semantic Web applications surveyed in this paper cover a wide range of application domains in CH and have attracted up to millions of users on the Semantic Web, suggesting feasibility of the proposed Sampo model. This work shows a shift of focus in research on CH semantic portals from data aggregation and exploration systems (1. generation systems) to systems supporting DH research (2. generation systems) with data analytic tools, and finally to automatic knowledge discovery and Artificial Intelligence (3. generation systems).},
  archive      = {J_SW},
  author       = {Hyvönen, Eero},
  doi          = {10.3233/SW-223034},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {729-744},
  shortjournal = {Semantic Web},
  title        = {Digital humanities on the semantic web: Sampo model and portal series},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blue brain nexus: An open, secure, scalable system for
knowledge graph management and data-driven science. <em>SW</em>,
<em>14</em>(4), 697–727. (<a
href="https://doi.org/10.3233/SW-222974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data-driven science often consists of iterative cycles of data discovery, acquisition, preparation, analysis, model building and validation leading to knowledge discovery as well as dissemination at scale. The unique challenges of building and simulating the whole rodent brain in the Swiss EPFL Blue Brain Project (BBP) required a solution to managing large-scale highly heterogeneous data, and tracking their provenance to ensure quality, reproducibility and attribution throughout these iterative cycles. Here, we describe Blue Brain Nexus (BBN), an ecosystem of open source, domain agnostic, scalable, extensible data and knowledge graph management systems built by BBP to address these challenges. BBN builds on open standards and interoperable semantic web technologies to enable the creation and management of secure RDF-based knowledge graphs validated by W3C SHACL. BBN supports a spectrum of (meta)data modeling and representation formats including JSON and JSON-LD as well as more formally specified SHACL-based schemas enabling domain model-driven runtime API. With its streaming event-based architecture, BBN supports asynchronous building and maintenance of multiple extensible indices to ensure high performance search capabilities and enable analytics. We present four use cases and applications of BBN to large-scale data integration and dissemination challenges in computational modeling, neuroscience, psychiatry and open linked data.},
  archive      = {J_SW},
  author       = {Sy, Mohameth François and Roman, Bogdan and Kerrien, Samuel and Mendez, Didac Montero and Genet, Henry and Wajerowicz, Wojciech and Dupont, Michaël and Lavriushev, Ian and Machon, Julien and Pirman, Kenneth and Neela Mana, Dhanesh and Stafeeva, Natalia and Kaufmann, Anna-Kristin and Lu, Huanxiang and Lurie, Jonathan and Fonta, Pierre-Alexandre and Martinez, Alejandra Garcia Rojas and Ulbrich, Alexander D. and Lindqvist, Carolina and Jimenez, Silvia and Rotenberg, David and Markram, Henry and Hill, Sean L.},
  doi          = {10.3233/SW-222974},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {697-727},
  shortjournal = {Semantic Web},
  title        = {Blue brain nexus: An open, secure, scalable system for knowledge graph management and data-driven science},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Publishing public transport data on the web with the
linked connections framework. <em>SW</em>, <em>14</em>(4), 659–693. (<a
href="https://doi.org/10.3233/SW-223116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publishing transport data on the Web for consumption by others poses several challenges for data publishers. In addition to planned schedules, access to live schedule updates (e.g. delays or cancellations) and historical data is fundamental to enable reliable applications and to support machine learning use cases. However publishing such dynamic data further increases the computational burden for data publishers, resulting in often unavailable historical data and live schedule updates for most public transport networks. In this paper we apply and extend the current Linked Connections approach for static data to also support cost-efficient live and historical public transport data publishing on the Web. Our contributions include (i) a reference specification and system architecture to support cost-efficient publishing of dynamic public transport schedules and historical data; (ii) empirical evaluations on route planning query performance based on data fragmentation size, publishing costs and a comparison with a traditional route planning engine such as OpenTripPlanner; (iii) an analysis of potential correlations of query performance with particular public transport network characteristics such as size, average degree, density, clustering coefficient and average connection duration. Results confirm that fragmentation size influences route planning query performance and converges on an optimal fragment size per network. Size (stops), density and connection duration also show correlation with route planning query performance. Our approach proves to be more cost-efficient and in some cases outperforms OpenTripPlanner when supporting the earliest arrival time route planning use case. Moreover, the cost of publishing live and historical schedules remains in the same order of magnitude for server-side resources compared to publishing planned schedules only. Yet, further optimizations are needed for larger networks (&gt;1000 stops) to be useful in practice. Additional dataset fragmentation strategies (e.g. geospatial) may be studied for designing more scalable and performant Web api s that adapt to particular use cases, not only limited to the public transport domain.},
  archive      = {J_SW},
  author       = {Rojas, Julián Andrés and Delva, Harm and Colpaert, Pieter and Verborgh, Ruben},
  doi          = {10.3233/SW-223116},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {659-693},
  shortjournal = {Semantic Web},
  title        = {Publishing public transport data on the web with the Linked&amp;nbsp;Connections framework},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applying the LOT methodology to a public bus transport
ontology aligned with transmodel: Challenges and results. <em>SW</em>,
<em>14</em>(4), 639–657. (<a
href="https://doi.org/10.3233/SW-210451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an ontology that describes the domain of Public Transport by bus, which is common in cities around the world. This ontology is aligned to Transmodel, a reference model which is available as a UML specification and which was developed to foster interoperability of data about transport systems across Europe. The alignment with this non-ontological resource required the adaptation of the Linked Open Terms (LOT) methodology, which has been used by our team as the methodological framework for the development of many ontologies used for the publication of open city data. The ontology is structured into three main modules: (1) agencies, operators and the lines that they manage, (2) lines, routes, stops and journey patterns, and (3) planned vehicle journeys with their timetables and service calendars. Besides reusing Transmodel concepts, the ontology also reuses common ontology design patterns from GeoSPARQL and the SOSA ontology. As part of the LOT data-driven validation stage, RDF data has been generated taking as input the GTFS feeds (General Transit Feed Specification) provided by the Madrid public bus transport provider (EMT). Mapping rules from structured data sources to RDF were developed using the RDF Mapping Language (RML) to generate RDF data, and queries corresponding to competency questions were tested.},
  archive      = {J_SW},
  author       = {Ruckhaus, Edna and Anton-Bravo, Adolfo and Scrocca, Mario and Corcho, Oscar},
  doi          = {10.3233/SW-210451},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {639-657},
  shortjournal = {Semantic Web},
  title        = {Applying the LOT methodology to a public bus transport ontology aligned with transmodel: Challenges and results},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urban IoT ontologies for sharing and electric mobility.
<em>SW</em>, <em>14</em>(4), 617–638. (<a
href="https://doi.org/10.3233/SW-210445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.},
  archive      = {J_SW},
  author       = {Scrocca, Mario and Baroni, Ilaria and Celino, Irene},
  doi          = {10.3233/SW-210445},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {617-638},
  shortjournal = {Semantic Web},
  title        = {Urban IoT ontologies for sharing and electric mobility},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial of transport data on the web. <em>SW</em>,
<em>14</em>(4), 613–616. (<a
href="https://doi.org/10.3233/SW-223278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Chaves-Fraga, David and Colpaert, Pieter and Sadeghi, Mersedeh and Comerio, Marco},
  doi          = {10.3233/SW-223278},
  journal      = {Semantic Web},
  month        = {4},
  number       = {4},
  pages        = {613-616},
  shortjournal = {Semantic Web},
  title        = {Editorial of transport data on the web},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methodologies for publishing linked open government data on
the web: A systematic mapping and a unified process model. <em>SW</em>,
<em>14</em>(3), 585–610. (<a
href="https://doi.org/10.3233/SW-222896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the beginning of the release of open data by many countries, different methodologies for publishing linked data have been proposed. However, they seem not to be adopted by early studies exploring linked data for different reasons. In this work, we conducted a systematic mapping in the literature to synthesize the different approaches around the following topics: common steps, associated tools and practices, quality assessment validations, and evaluation of the methodology. The findings show a core set of activities, based on the linked data principles, but with additional critical steps for practical use in scale. Furthermore, although a fair amount of quality issues are reported in the literature, very few of these methodologies embed validation steps in their process. We describe an integrated overview of the different activities and how they can be executed with appropriate tools. We also present research challenges that need to be addressed in future works in this area.},
  archive      = {J_SW},
  author       = {Penteado, Bruno Elias and Maldonado, José Carlos and Isotani, Seiji},
  doi          = {10.3233/SW-222896},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {585-610},
  shortjournal = {Semantic Web},
  title        = {Methodologies for publishing linked open government data on the web: A systematic mapping and a unified process model},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A strategy for archives metadata representation on CIDOC-CRM
and knowledge discovery. <em>SW</em>, <em>14</em>(3), 553–584. (<a
href="https://doi.org/10.3233/SW-222798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a strategy for the semantic migration of Portuguese National Archives records into CIDOC-CRM standard, an ontology developed for museums, within the context of the EPISA project. The approach to automatically populate the CIDOC-CRM is based on Mapping Description Rules to semantically translate the archives descriptive information into CIDOC-CRM representation. The compliance of the CIDOC-CRM model recommendations guarantees that the populated CIDOC-CRM ontology of archives descriptive information verifies interoperability, and could be linked and integrated with other populated CIDOC-CRM ontologies. In the information modelling, requirements on the mapping representation, due to the intent of interpreting natural language text to automatically extract information of metadata text fields and to interpret natural language queries, are taken into account. To automatically interpret the Mapping Description Rules, OWL API was used to obtain the set of assertions that represents the information in the target ontology and two datasets are available with some migration examples. The exploration of the knowledge representation is done through some Description Logic queries to highlight the advantages of having this new representation of the National Archives. The evaluation of the resulting representation can be done automatically proving its correctness for the metadata that has a direct representation in CIDOC-CRM.},
  archive      = {J_SW},
  author       = {Melo, Dora and Rodrigues, Irene Pimenta and Varagnolo, Davide},
  doi          = {10.3233/SW-222798},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {553-584},
  shortjournal = {Semantic Web},
  title        = {A strategy for archives metadata representation on CIDOC-CRM and knowledge discovery},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building spatio-temporal knowledge graphs from vectorized
topographic historical maps. <em>SW</em>, <em>14</em>(3), 527–549. (<a
href="https://doi.org/10.3233/SW-222918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical maps provide rich information for researchers in many areas, including the social and natural sciences. These maps contain detailed documentation of a wide variety of natural and human-made features and their changes over time, such as changes in transportation networks or the decline of wetlands or forest areas. Analyzing changes over time in such maps can be labor-intensive for a scientist, even after the geographic features have been digitized and converted to a vector format. Knowledge Graphs (KGs) are the appropriate representations to store and link such data and support semantic and temporal querying to facilitate change analysis. KGs combine expressivity, interoperability, and standardization in the Semantic Web stack, thus providing a strong foundation for querying and analysis. In this paper, we present an automatic approach to convert vector geographic features extracted from multiple historical maps into contextualized spatio-temporal KGs. The resulting graphs can be easily queried and visualized to understand the changes in different regions over time. We evaluate our technique on railroad networks and wetland areas extracted from the United States Geological Survey (USGS) historical topographic maps for several regions over multiple map sheets and editions. We also demonstrate how the automatically constructed linked data (i.e., KGs) enable effective querying and visualization of changes over different points in time.},
  archive      = {J_SW},
  author       = {Shbita, Basel and Knoblock, Craig A. and Duan, Weiwei and Chiang, Yao-Yi and Uhl, Johannes H. and Leyk, Stefan},
  doi          = {10.3233/SW-222918},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {527-549},
  shortjournal = {Semantic Web},
  title        = {Building spatio-temporal knowledge graphs from vectorized topographic historical maps},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characteristic sets profile features: Estimation and
application to SPARQL query planning. <em>SW</em>, <em>14</em>(3),
491–526. (<a href="https://doi.org/10.3233/SW-222903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF dataset profiling is the task of extracting a formal representation of a dataset’s features. Such features may cover various aspects of the RDF dataset ranging from information on licensing and provenance to statistical descriptors of the data distribution and its semantics. In this work, we focus on the characteristics sets profile features that capture both structural and semantic information of an RDF dataset, making them a valuable resource for different downstream applications. While previous research demonstrated the benefits of characteristic sets in centralized and federated query processing, access to these fine-grained statistics is taken for granted. However, especially in federated query processing, computing this profile feature is challenging as it can be difficult and/or costly to access and process the entire data from all federation members. We address this shortcoming by introducing the concept of a profile feature estimation and propose a sampling-based approach to generate estimations for the characteristic sets profile feature. In addition, we showcase the applicability of these feature estimations in federated querying by proposing a query planning approach that is specifically designed to leverage these feature estimations. In our first experimental study, we intrinsically evaluate our approach on the representativeness of the feature estimation. The results show that even small samples of just 0.5 % of the original graph’s entities allow for estimating both structural and statistical properties of the characteristic sets profile features. Our second experimental study extrinsically evaluates the estimations by investigating their applicability in our query planner using the well-known FedBench benchmark. The results of the experiments show that the estimated profile features allow for obtaining efficient query plans.},
  archive      = {J_SW},
  author       = {Heling, Lars and Acosta, Maribel},
  doi          = {10.3233/SW-222903},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {491-526},
  shortjournal = {Semantic Web},
  title        = {Characteristic sets profile features: Estimation and application to SPARQL query planning},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modular ontology modeling. <em>SW</em>, <em>14</em>(3),
459–489. (<a href="https://doi.org/10.3233/SW-222886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reusing ontologies for new purposes, or adapting them to new use-cases, is frequently difficult. In our experiences, we have found this to be the case for several reasons: (i) differing representational granularity in ontologies and in use-cases, (ii) lacking conceptual clarity in potentially reusable ontologies, (iii) lack and difficulty of adherence to good modeling principles, and (iv) a lack of reuse emphasis and process support available in ontology engineering tooling. In order to address these concerns, we have developed the Modular Ontology Modeling (MOMo) methodology, and its supporting tooling infrastructure, CoModIDE (the Comprehensive Modular Ontology IDE – “commodity”). MOMo builds on the established eXtreme Design methodology, and like it emphasizes modular development and design pattern reuse; but crucially adds the extensive use of graphical schema diagrams, and tooling that support them, as vehicles for knowledge elicitation from experts. In this paper, we present the MOMo workflow in detail, and describe several useful resources for executing it. In particular, we provide a thorough and rigorous evaluation of CoModIDE in its role of supporting the MOMo methodology’s graphical modeling paradigm. We find that CoModIDE significantly improves approachability of such a paradigm, and that it displays a high usability.},
  archive      = {J_SW},
  author       = {Shimizu, Cogan and Hammar, Karl and Hitzler, Pascal},
  doi          = {10.3233/SW-222886},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {459-489},
  shortjournal = {Semantic Web},
  title        = {Modular ontology modeling},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
