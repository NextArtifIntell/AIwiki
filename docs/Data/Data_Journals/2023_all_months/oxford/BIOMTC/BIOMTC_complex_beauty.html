<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---337">BIOMTC - 337</h2>
<ul>
<li><details>
<summary>
(2023). Acknowledgments referees 2023. <em>BIOMTC</em>,
<em>79</em>(4), 4017–4019. (<a
href="https://doi.org/10.1111/biom.13929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13929},
  journal      = {Biometrics},
  number       = {4},
  pages        = {4017-4019},
  shortjournal = {Biometrics},
  title        = {Acknowledgments referees 2023},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bioinformatics methods: From omics to next generation
sequencing by shili lin, denise scholtens, sujay datta, boca raton, FL:
Chapman &amp; hall. 2023. Pp. 350. ISBN 9781498765152. <em>BIOMTC</em>,
<em>79</em>(4), 4014–4016. (<a
href="https://doi.org/10.1111/biom.13917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yu-Chiao Chiu},
  doi          = {10.1111/biom.13917},
  journal      = {Biometrics},
  number       = {4},
  pages        = {4014-4016},
  shortjournal = {Biometrics},
  title        = {Bioinformatics methods: from omics to next generation sequencing by shili lin, denise scholtens, sujay datta, boca raton, FL: chapman &amp; hall. 2023. pp. 350. ISBN 9781498765152.},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Statistical inference and machine learning for big data by
mayer alvo, springer cham. 2022. Pp. 431. EUR 129.99. ISBN-13:
978-3-031-06783-9. <em>BIOMTC</em>, <em>79</em>(4), 4013. (<a
href="https://doi.org/10.1111/biom.13883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13883},
  journal      = {Biometrics},
  number       = {4},
  pages        = {4013},
  shortjournal = {Biometrics},
  title        = {Statistical inference and machine learning for big data by mayer alvo, springer cham. 2022. pp. 431. EUR 129.99. ISBN-13: 978-3-031-06783-9.},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “robust approach to combining multiple markers
to improve surrogacy.” <em>BIOMTC</em>, <em>79</em>(4), 4012. (<a
href="https://doi.org/10.1111/biom.13928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13928},
  journal      = {Biometrics},
  number       = {4},
  pages        = {4012},
  shortjournal = {Biometrics},
  title        = {Correction to “Robust approach to combining multiple markers to improve surrogacy”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to analyze continuous and discrete repeated measures in
small-sample cross-over trials? <em>BIOMTC</em>, <em>79</em>(4),
3998–4011. (<a href="https://doi.org/10.1111/biom.13920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To optimize the use of data from a small number of subjects in rare disease trials, an at first sight advantageous design is the repeated measures cross-over design. However, it is unclear how these within-treatment period and within-subject clustered data are best analyzed in small-sample trials. In a real-data simulation study based upon a recent epidermolysis bullosa simplex trial using this design, we compare non-parametric marginal models, generalized pairwise comparison models, GEE-type models and parametric model averaging for both repeated binary and count data. The recommendation of which methodology to use in rare disease trials with a repeated measures cross-over design depends on the type of outcome and the number of time points the treatment has an effect on. The non-parametric marginal model testing the treatment–time-interaction effect is suitable for detecting between group differences in the shapes of the longitudinal profiles. For binary outcomes with the treatment effect on a single time point, the parametric model averaging method is recommended, while in the other cases the unmatched generalized pairwise comparison methodology is recommended. Both provide an easily interpretable effect size measure, and do not require exclusion of periods or subjects due to incompleteness.},
  archive      = {J_BIOMTC},
  author       = {Johan Verbeeck and Martin Geroldinger and Konstantin Thiel and Andrew Craig Hooker and Sebastian Ueckert and Mats Karlsson and Arne Cornelius Bathke and Johann Wolfgang Bauer and Geert Molenberghs and Georg Zimmermann},
  doi          = {10.1111/biom.13920},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3998-4011},
  shortjournal = {Biometrics},
  title        = {How to analyze continuous and discrete repeated measures in small-sample cross-over trials?},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient algorithms for building representative matched
pairs with enhanced generalizability. <em>BIOMTC</em>, <em>79</em>(4),
3981–3997. (<a href="https://doi.org/10.1111/biom.13919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent efforts center on assessing the ability of real-world evidence (RWE) generated from non-randomized, observational data to produce results compatible with those from randomized controlled trials (RCTs). One noticeable endeavor is the RCT DUPLICATE initiative. To better reconcile findings from an observational study and an RCT, or two observational studies based on different databases, it is desirable to eliminate differences between study populations. We outline an efficient, network-flow-based statistical matching algorithm that designs well-matched pairs from observational data that resemble the covariate distributions of a target population, for instance, the target-RCT-eligible population in the RCT DUPLICATE initiative studies or a generic population of scientific interest. We demonstrate the usefulness of the method by revisiting the inconsistency regarding a cardioprotective effect of the hormone replacement therapy (HRT) in the Women&#39;s Health Initiative (WHI) clinical trial and corresponding observational study. We found that the discrepancy between the trial and observational study persisted in a design that adjusted for the difference in study populations&#39; cardiovascular risk profile, but seemed to disappear in a study design that further adjusted for the difference in HRT initiation age and previous estrogen-plus-progestin use. The proposed method is integrated into the R package match2C .},
  archive      = {J_BIOMTC},
  author       = {Bo Zhang},
  doi          = {10.1111/biom.13919},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3981-3997},
  shortjournal = {Biometrics},
  title        = {Efficient algorithms for building representative matched pairs with enhanced generalizability},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A second evidence factor for a second control group.
<em>BIOMTC</em>, <em>79</em>(4), 3968–3980. (<a
href="https://doi.org/10.1111/biom.13921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational study of the effects caused by a treatment, a second control group is used in an effort to detect bias from unmeasured covariates, and the investigator is content if no evidence of bias is found. This strategy is not entirely satisfactory: two control groups may differ significantly, yet the difference may be too small to invalidate inferences about the treatment, or the control groups may not differ yet nonetheless fail to provide a tangible strengthening of the evidence of a treatment effect. Is a firmer conclusion possible? Is there a way to analyze a second control group such that the data might report measurably strengthened evidence of cause and effect, that is, insensitivity to larger unmeasured biases? Evidence factor analyses are not commonly used with a second control group: most analyses compare the treated group to each control group, but analyses of that kind are partially redundant; so, they do not constitute evidence factors. An alternative analysis is proposed here, one that does yield two evidence factors, and with a carefully designed test statistic, is capable of extracting strong evidence from the second factor. The new technical work here concerns the development of a test statistic with high design sensitivity and high Bahadur efficiency in a sensitivity analysis for the second factor. A study of binge drinking as a cause of high blood pressure is used as an illustration.},
  archive      = {J_BIOMTC},
  author       = {Paul R. Rosenbaum},
  doi          = {10.1111/biom.13921},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3968-3980},
  shortjournal = {Biometrics},
  title        = {A second evidence factor for a second control group},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proportional incidence rate model for aggregated data to
study the vaccine effectiveness against COVID-19 hospital and ICU
admissions. <em>BIOMTC</em>, <em>79</em>(4), 3954–3967. (<a
href="https://doi.org/10.1111/biom.13915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a proportional incidence model that estimates vaccine effectiveness (VE) at the population level using conditional likelihood for aggregated data. Our model assumes that the population counts of clinical outcomes for an infectious disease arise from a superposition of Poisson processes with different vaccination statuses. The intensity function in the model is calculated as the product of per capita incidence rate and the at-risk population size, both of which are time-dependent. We formulate a log-linear regression model with respect to the relative risk, defined as the ratio between the per capita incidence rates of vaccinated and unvaccinated individuals. In the regression analysis, we treat the baseline incidence rate as a nuisance parameter, similar to the Cox proportional hazard model in survival analysis. We then apply the proposed models and methods to age-stratified weekly counts of COVID-19–related hospital and ICU admissions among adults in Ontario, Canada. The data spanned from 2021 to February 2022, encompassing the Omicron era and the rollout of booster vaccine doses. We also discuss the limitations and confounding effects while advocating for the necessity of more comprehensive and up-to-date individual-level data that document the clinical outcomes and measure potential confounders.},
  archive      = {J_BIOMTC},
  author       = {Ping Yan and Muhammad Abu Shadeque Mullah and Ashleigh Tuite},
  doi          = {10.1111/biom.13915},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3954-3967},
  shortjournal = {Biometrics},
  title        = {A proportional incidence rate model for aggregated data to study the vaccine effectiveness against COVID-19 hospital and ICU admissions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Melding wildlife surveys to improve conservation inference.
<em>BIOMTC</em>, <em>79</em>(4), 3941–3953. (<a
href="https://doi.org/10.1111/biom.13903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated models are a popular tool for analyzing species of conservation concern. Species of conservation concern are often monitored by multiple entities that generate several datasets. Individually, these datasets may be insufficient for guiding management due to low spatio-temporal resolution, biased sampling, or large observational uncertainty. Integrated models provide an approach for assimilating multiple datasets in a coherent framework that can compensate for these deficiencies. While conventional integrated models have been used to assimilate count data with surveys of survival, fecundity, and harvest, they can also assimilate ecological surveys that have differing spatio-temporal regions and observational uncertainties. Motivated by independent aerial and ground surveys of lesser prairie-chicken, we developed an integrated modeling approach that assimilates density estimates derived from surveys with distinct sources of observational error into a joint framework that provides shared inference on spatio-temporal trends. We model these data using a Bayesian Markov melding approach and apply several data augmentation strategies for efficient sampling. In a simulation study, we show that our integrated model improved predictive performance relative to models for analyzing the surveys independently. We use the integrated model to facilitate prediction of lesser prairie-chicken density at unsampled regions and perform a sensitivity analysis to quantify the inferential cost associated with reduced survey effort.},
  archive      = {J_BIOMTC},
  author       = {Justin J. Van Ee and Christian A. Hagen and David C. Pavlacky Jr. and Kent A. Fricke and Matthew D. Koslovsky and Mevin B. Hooten},
  doi          = {10.1111/biom.13903},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3941-3953},
  shortjournal = {Biometrics},
  title        = {Melding wildlife surveys to improve conservation inference},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of length-biased and partly interval-censored
survival data with mismeasured covariates. <em>BIOMTC</em>,
<em>79</em>(4), 3929–3940. (<a
href="https://doi.org/10.1111/biom.13898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyze the length-biased and partly interval-censored data, whose challenges primarily come from biased sampling and interfere induced by interval censoring. Unlike existing methods that focus on low-dimensional data and assume the covariates to be precisely measured, sometimes researchers may encounter high-dimensional data subject to measurement error, which are ubiquitous in applications and make estimation unreliable. To address those challenges, we explore a valid inference method for handling high-dimensional length-biased and interval-censored survival data with measurement error in covariates under the accelerated failure time model. We primarily employ the SIMEX method to correct for measurement error effects and propose the boosting procedure to do variable selection and estimation. The proposed method is able to handle the case that the dimension of covariates is larger than the sample size and enjoys appealing features that the distributions of the covariates are left unspecified.},
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen and Bangxu Qiu},
  doi          = {10.1111/biom.13898},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3929-3940},
  shortjournal = {Biometrics},
  title        = {Analysis of length-biased and partly interval-censored survival data with mismeasured covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size and power determination for multiparameter
evaluation in nonlinear regression models with potential stratification.
<em>BIOMTC</em>, <em>79</em>(4), 3916–3928. (<a
href="https://doi.org/10.1111/biom.13897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size and power determination are crucial design considerations for biomedical studies intending to formally test the effects of key variables on an outcome. Other known prognostic factors may exist, necessitating the use of techniques for covariate adjustment when conducting this evaluation. Moreover, the main interest often includes assessing the impact of more than one variable on an outcome, such as multiple treatments or risk factors. Regression models are frequently employed for these purposes, formalizing this assessment as a test of multiple regression parameters. But, the presence of multiple variables of primary interest and correlation between covariates can complicate sample size/power calculation. Given the paucity of available sample size formulas for this context, these calculations are often performed via simulation, which can be both time-consuming as well as demanding extensive probability modeling. We propose a simpler, general approach to sample size and power determination that may be applied when testing multiple parameters in commonly used regression models, including generalized linear models as well as ordinary and stratified versions of the Cox and Fine–Gray models. Through both rigorous simulations and theoretical derivations, we demonstrate the formulas&#39; accuracy in producing sample sizes that will meet the type I error rate and power specifications of the study design.},
  archive      = {J_BIOMTC},
  author       = {Michael J. Martens and Soyoung Kim and Kwang Woo Ahn},
  doi          = {10.1111/biom.13897},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3916-3928},
  shortjournal = {Biometrics},
  title        = {Sample size and power determination for multiparameter evaluation in nonlinear regression models with potential stratification},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dirichlet process mixture models for the analysis of
repeated attempt designs. <em>BIOMTC</em>, <em>79</em>(4), 3907–3915.
(<a href="https://doi.org/10.1111/biom.13894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal studies, it is not uncommon to make multiple attempts to collect a measurement after baseline. Recording whether these attempts are successful provides useful information for the purposes of assessing missing data assumptions. This is because measurements from subjects who provide the data after numerous failed attempts may differ from those who provide the measurement after fewer attempts. Previous models for these designs were parametric and/or did not allow sensitivity analysis. For the former, there are always concerns about model misspecification and for the latter, sensitivity analysis is essential when conducting inference in the presence of missing data. Here, we propose a new approach which minimizes issues with model misspecification by using Bayesian nonparametrics for the observed data distribution. We also introduce a novel approach for identification and sensitivity analysis. We re-analyze the repeated attempts data from a clinical trial involving patients with severe mental illness and conduct simulations to better understand the properties of our approach.},
  archive      = {J_BIOMTC},
  author       = {Michael J. Daniels and Minji Lee and Wei Feng},
  doi          = {10.1111/biom.13894},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3907-3915},
  shortjournal = {Biometrics},
  title        = {Dirichlet process mixture models for the analysis of repeated attempt designs},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing time-invariant dynamic surveillance rules for
optimal monitoring schedules. <em>BIOMTC</em>, <em>79</em>(4),
3895–3906. (<a href="https://doi.org/10.1111/biom.13911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic surveillance rules (DSRs) are sequential surveillance decision rules informing monitoring schedules in clinical practice, which can adapt over time according to a patient&#39;s evolving characteristics. In many clinical applications, it is desirable to identify and implement optimal time-invariant DSRs, where the parameters indexing the decision rules are shared across different decision points. We propose a new criterion for DSRs that accounts for benefit-cost tradeoff during the course of disease surveillance. We develop two methods to estimate the time-invariant DSRs optimizing the proposed criterion, and establish asymptotic properties for the estimated parameters of biomarkers indexing the DSRs. The first approach estimates the optimal decision rules for each individual at every stage via regression modeling, and then estimates the time-invariant DSRs via a classification procedure with the estimated time-varying decision rules as the response. The second approach proceeds by optimizing a relaxation of the empirical objective, where a surrogate function is utilized to facilitate computation. Extensive simulation studies are conducted to demonstrate the superior performances of the proposed methods. The methods are further applied to the Canary Prostate Active Surveillance Study (PASS).},
  archive      = {J_BIOMTC},
  author       = {Xinyuan Dong and Yingye Zheng and Daniel W. Lin and Lisa Newcomb and Ying-Qi Zhao},
  doi          = {10.1111/biom.13911},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3895-3906},
  shortjournal = {Biometrics},
  title        = {Constructing time-invariant dynamic surveillance rules for optimal monitoring schedules},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pathological imaging-assisted cancer gene–environment
interaction analysis. <em>BIOMTC</em>, <em>79</em>(4), 3883–3894. (<a
href="https://doi.org/10.1111/biom.13873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene–environment (G–E) interactions have important implications for cancer outcomes and phenotypes beyond the main G and E effects. Compared to main-effect-only analysis, G–E interaction analysis more seriously suffers from a lack of information caused by higher dimensionality, weaker signals, and other factors. It is also uniquely challenged by the “main effects, interactions” variable selection hierarchy. Effort has been made to bring in additional information to assist cancer G–E interaction analysis. In this study, we take a strategy different from the existing literature and borrow information from pathological imaging data. Such data are a “byproduct” of biopsy, enjoys broad availability and low cost, and has been shown as informative for modeling prognosis and other cancer outcomes/phenotypes in recent studies. Building on penalization, we develop an assisted estimation and variable selection approach for G–E interaction analysis. The approach is intuitive, can be effectively realized, and has competitive performance in simulation. We further analyze The Cancer Genome Atlas (TCGA) data on lung adenocarcinoma (LUAD). The outcome of interest is overall survival, and for G variables, we analyze gene expressions. Assisted by pathological imaging data, our G–E interaction analysis leads to different findings with competitive prediction performance and stability.},
  archive      = {J_BIOMTC},
  author       = {Kuangnan Fang and Jingmao Li and Qingzhao Zhang and Yaqing Xu and Shuangge Ma},
  doi          = {10.1111/biom.13873},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3883-3894},
  shortjournal = {Biometrics},
  title        = {Pathological imaging-assisted cancer gene–environment interaction analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A case study of glucose levels during sleep using multilevel
fast function on scalar regression inference. <em>BIOMTC</em>,
<em>79</em>(4), 3873–3882. (<a
href="https://doi.org/10.1111/biom.13878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous glucose monitors (CGMs) are increasingly used to measure blood glucose levels and provide information about the treatment and management of diabetes. Our motivating study contains CGM data during sleep for 174 study participants with type II diabetes mellitus measured at a 5-min frequency for an average of 10 nights. We aim to quantify the effects of diabetes medications and sleep apnea severity on glucose levels. Statistically, this is an inference question about the association between scalar covariates and functional responses observed at multiple visits (sleep periods). However, many characteristics of the data make analyses difficult, including (1) nonstationary within-period patterns; (2) substantial between-period heterogeneity, non-Gaussianity, and outliers; and (3) large dimensionality due to the number of study participants, sleep periods, and time points. For our analyses, we evaluate and compare two methods: fast univariate inference (FUI) and functional additive mixed models (FAMMs). We extend FUI and introduce a new approach for testing the hypotheses of no effect and time invariance of the covariates. We also highlight areas for further methodological development for FAMM. Our study reveals that (1) biguanide medication and sleep apnea severity significantly affect glucose trajectories during sleep and (2) the estimated effects are time invariant.},
  archive      = {J_BIOMTC},
  author       = {Renat Sergazinov and Andrew Leroux and Erjia Cui and Ciprian Crainiceanu and R. Nisha Aurora and Naresh M. Punjabi and Irina Gaynanova},
  doi          = {10.1111/biom.13878},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3873-3882},
  shortjournal = {Biometrics},
  title        = {A case study of glucose levels during sleep using multilevel fast function on scalar regression inference},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional cross-design synthesis estimators for
generalizability in medicaid. <em>BIOMTC</em>, <em>79</em>(4),
3859–3872. (<a href="https://doi.org/10.1111/biom.13863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While much of the causal inference literature has focused on addressing internal validity biases, both internal and external validity are necessary for unbiased estimates in a target population of interest. However, few generalizability approaches exist for estimating causal quantities in a target population that is not well-represented by a randomized study but is reflected when additionally incorporating observational data. To generalize to a target population represented by a union of these data, we propose a novel class of conditional cross-design synthesis estimators that combine randomized and observational data, while addressing their estimates&#39; respective biases—lack of overlap and unmeasured confounding. These methods enable estimating the causal effect of managed care plans on health care spending among Medicaid beneficiaries in New York City, which requires obtaining estimates for the 7\% of beneficiaries randomized to a plan and 93\% who choose a plan, who do not resemble randomized beneficiaries. Our new estimators include outcome regression, propensity weighting, and double robust approaches. All use the covariate overlap between the randomized and observational data to remove potential unmeasured confounding bias. Applying these methods, we find substantial heterogeneity in spending effects across managed care plans. This has major implications for our understanding of Medicaid, where this heterogeneity has previously been hidden. Additionally, we demonstrate that unmeasured confounding rather than lack of overlap poses a larger concern in this setting.},
  archive      = {J_BIOMTC},
  author       = {Irina Degtiar and Tim Layton and Jacob Wallace and Sherri Rose},
  doi          = {10.1111/biom.13863},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3859-3872},
  shortjournal = {Biometrics},
  title        = {Conditional cross-design synthesis estimators for generalizability in medicaid},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised convex clustering. <em>BIOMTC</em>,
<em>79</em>(4), 3846–3858. (<a
href="https://doi.org/10.1111/biom.13860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering has long been a popular unsupervised learning approach to identify groups of similar objects and discover patterns from unlabeled data in many applications. Yet, coming up with meaningful interpretations of the estimated clusters has often been challenging precisely due to their unsupervised nature. Meanwhile, in many real-world scenarios, there are some noisy supervising auxiliary variables, for instance, subjective diagnostic opinions, that are related to the observed heterogeneity of the unlabeled data. By leveraging information from both supervising auxiliary variables and unlabeled data, we seek to uncover more scientifically interpretable group structures that may be hidden by completely unsupervised analyses. In this work, we propose and develop a new statistical pattern discovery method named supervised convex clustering (SCC) that borrows strength from both information sources and guides towards finding more interpretable patterns via a joint convex fusion penalty. We develop several extensions of SCC to integrate different types of supervising auxiliary variables, to adjust for additional covariates, and to find biclusters. We demonstrate the practical advantages of SCC through simulations and a case study on Alzheimer&#39;s disease genomics. Specifically, we discover new candidate genes as well as new subtypes of Alzheimer&#39;s disease that can potentially lead to better understanding of the underlying genetic mechanisms responsible for the observed heterogeneity of cognitive decline in older adults.},
  archive      = {J_BIOMTC},
  author       = {Minjie Wang and Tianyi Yao and Genevera I. Allen},
  doi          = {10.1111/biom.13860},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3846-3858},
  shortjournal = {Biometrics},
  title        = {Supervised convex clustering},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A synthetic data integration framework to leverage external
summary-level information from heterogeneous populations.
<em>BIOMTC</em>, <em>79</em>(4), 3831–3845. (<a
href="https://doi.org/10.1111/biom.13852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing need for flexible general frameworks that integrate individual-level data with external summary information for improved statistical inference. External information relevant for a risk prediction model may come in multiple forms, through regression coefficient estimates or predicted values of the outcome variable. Different external models may use different sets of predictors and the algorithm they used to predict the outcome Y given these predictors may or may not be known. The underlying populations corresponding to each external model may be different from each other and from the internal study population. Motivated by a prostate cancer risk prediction problem where novel biomarkers are measured only in the internal study, this paper proposes an imputation-based methodology, where the goal is to fit a target regression model with all available predictors in the internal study while utilizing summary information from external models that may have used only a subset of the predictors. The method allows for heterogeneity of covariate effects across the external populations. The proposed approach generates synthetic outcome data in each external population, uses stacked multiple imputation to create a long dataset with complete covariate information. The final analysis of the stacked imputed data is conducted by weighted regression. This flexible and unified approach can improve statistical efficiency of the estimated coefficients in the internal study, improve predictions by utilizing even partial information available from models that use a subset of the full set of covariates used in the internal study, and provide statistical inference for the external population with potentially different covariate effects from the internal population.},
  archive      = {J_BIOMTC},
  author       = {Tian Gu and Jeremy Michael George Taylor and Bhramar Mukherjee},
  doi          = {10.1111/biom.13852},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3831-3845},
  shortjournal = {Biometrics},
  title        = {A synthetic data integration framework to leverage external summary-level information from heterogeneous populations},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling COVID-19 contact-tracing using the ratio regression
capture–recapture approach. <em>BIOMTC</em>, <em>79</em>(4), 3818–3830.
(<a href="https://doi.org/10.1111/biom.13842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact-tracing is one of the most effective tools in infectious disease outbreak control. A capture–recapture approach based upon ratio regression is suggested to estimate the completeness of case detection. Ratio regression has been recently developed as flexible tool for count data modeling and has proved to be successful in the capture–recapture setting. The methodology is applied here to Covid-19 contact tracing data from Thailand. A simple weighted straight line approach is used which includes the Poisson and geometric distribution as special cases. For the case study data of contact tracing for Thailand, a completeness of 83\% could be found with a 95\% confidence interval of 74\%–93\%.},
  archive      = {J_BIOMTC},
  author       = {Dankmar Böhning and Rattana Lerdsuwansri and Patarawan Sangnawakij},
  doi          = {10.1111/biom.13842},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3818-3830},
  shortjournal = {Biometrics},
  title        = {Modeling COVID-19 contact-tracing using the ratio regression capture–recapture approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating population size: The importance of model and
estimator choice. <em>BIOMTC</em>, <em>79</em>(4), 3803–3817. (<a
href="https://doi.org/10.1111/biom.13828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimator and model choice when estimating abundance from capture–recapture data. Our work is motivated by a mark–recapture distance sampling example, where model and estimator choice led to unexpectedly large disparities in the estimates. To understand these differences, we look at three estimation strategies (maximum likelihood estimation, conditional maximum likelihood estimation, and Bayesian estimation) for both binomial and Poisson models. We show that assuming the data have a binomial or multinomial distribution introduces implicit and unnoticed assumptions that are not addressed when fitting with maximum likelihood estimation. This can have an important effect in finite samples, particularly if our data arise from multiple populations. We relate these results to those of restricted maximum likelihood in linear mixed effects models.},
  archive      = {J_BIOMTC},
  author       = {Matthew R. Schofield and Richard J. Barker and William A. Link and Heloise Pavanato},
  doi          = {10.1111/biom.13828},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3803-3817},
  shortjournal = {Biometrics},
  title        = {Estimating population size: The importance of model and estimator choice},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inference for a principal stratum estimand on
recurrent events truncated by death. <em>BIOMTC</em>, <em>79</em>(4),
3792–3802. (<a href="https://doi.org/10.1111/biom.13831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are often important endpoints in randomized clinical trials. For example, the number of recurrent disease-related hospitalizations may be considered as a clinically meaningful endpoint in cardiovascular studies. In some settings, the recurrent event process may be terminated by an event such as death, which makes it more challenging to define and estimate a causal treatment effect on recurrent event endpoints. In this paper, we focus on the principal stratum estimand, where the treatment effect of interest on recurrent events is defined among subjects who would be alive regardless of the assigned treatment. For the estimation of the principal stratum effect in randomized clinical trials, we propose a Bayesian approach based on a joint model of the recurrent event and death processes with a frailty term accounting for within-subject correlation. We also present Bayesian posterior predictive check procedures for assessing the model fit. The proposed approaches are demonstrated in the randomized Phase III chronic heart failure trial PARAGON-HF (NCT01920711).},
  archive      = {J_BIOMTC},
  author       = {Tianmeng Lyu and Björn Bornkamp and Guenther Mueller-Velten and Heinz Schmidli},
  doi          = {10.1111/biom.13831},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3792-3802},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for a principal stratum estimand on recurrent events truncated by death},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nonparametric test of group distributional differences for
hierarchically clustered functional data. <em>BIOMTC</em>,
<em>79</em>(4), 3778–3791. (<a
href="https://doi.org/10.1111/biom.13846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological sex and gender are critical variables in biomedical research, but are complicated by the presence of sex-specific natural hormone cycles, such as the estrous cycle in female rodents, typically divided into phases. A common feature of these cycles are fluctuating hormone levels that induce sex differences in many behaviors controlled by the electrophysiology of neurons, such as neuronal membrane potential in response to electrical stimulus, typically summarized using a priori defined metrics. In this paper, we propose a method to test for differences in the electrophysiological properties across estrous cycle phase without first defining a metric of interest. We do this by modeling membrane potential data in the frequency domain as realizations of a bivariate process, also depending on the electrical stimulus, by adopting existing methods for longitudinal functional data. We are then able to extract the main features of the bivariate signals through a set of basis function coefficients. We use these coefficients for testing, adapting methods for multivariate data to account for an induced hierarchical structure that is a product of the experimental design. We illustrate the performance of the proposed approach in simulations and then apply the method to experimental data.},
  archive      = {J_BIOMTC},
  author       = {Alexander S. Long and Brian J. Reich and Ana-Maria Staicu and John Meitzen},
  doi          = {10.1111/biom.13846},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3778-3791},
  shortjournal = {Biometrics},
  title        = {A nonparametric test of group distributional differences for hierarchically clustered functional data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing clustered continuous response variables with
ordinal regression models. <em>BIOMTC</em>, <em>79</em>(4), 3764–3777.
(<a href="https://doi.org/10.1111/biom.13904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous response data are regularly transformed to meet regression modeling assumptions. However, approaches taken to identify the appropriate transformation can be ad hoc and can increase model uncertainty. Further, the resulting transformations often vary across studies leading to difficulties with synthesizing and interpreting results. When a continuous response variable is measured repeatedly within individuals or when continuous responses arise from clusters, analyses have the additional challenge caused by within-individual or within-cluster correlations. We extend a widely used ordinal regression model, the cumulative probability model (CPM), to fit clustered, continuous response data using generalized estimating equations for ordinal responses. With the proposed approach, estimates of marginal model parameters, cumulative distribution functions , expectations, and quantiles conditional on covariates can be obtained without pretransformation of the response data. While computational challenges arise with large numbers of distinct values of the continuous response variable, we propose feasible and computationally efficient approaches to fit CPMs under commonly used working correlation structures. We study finite sample operating characteristics of the estimators via simulation and illustrate their implementation with two data examples. One studies predictors of CD4:CD8 ratios in a cohort living with HIV, and the other investigates the association of a single nucleotide polymorphism and lung function decline in a cohort with early chronic obstructive pulmonary disease.},
  archive      = {J_BIOMTC},
  author       = {Yuqi Tian and Bryan E. Shepherd and Chun Li and Donglin Zeng and Jonathan S. Schildcrout},
  doi          = {10.1111/biom.13904},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3764-3777},
  shortjournal = {Biometrics},
  title        = {Analyzing clustered continuous response variables with ordinal regression models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear multilevel joint model for individual lesion
kinetics and survival to characterize intra-individual heterogeneity in
patients with advanced cancer. <em>BIOMTC</em>, <em>79</em>(4),
3752–3763. (<a href="https://doi.org/10.1111/biom.13912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In advanced cancer patients, tumor burden is calculated using the sum of the longest diameters (SLD) of the target lesions, a measure that lumps all lesions together and ignores intra-patient heterogeneity. Here, we used a rich dataset of 342 metastatic bladder cancer patients treated with a novel immunotherapy agent to develop a Bayesian multilevel joint model that can quantify heterogeneity in lesion dynamics and measure their impact on survival. Using a nonlinear model of tumor growth inhibition, we estimated that dynamics differed greatly among lesions, and inter-lesion variability accounted for 21\% and 28\% of the total variance in tumor shrinkage and treatment effect duration, respectively. Next, we investigated the impact of individual lesion dynamics on survival. Lesions located in the liver and in the bladder had twice as much impact on the instantaneous risk of death compared to those located in the lung or the lymph nodes. Finally, we evaluated the utility of individual lesion follow-up for dynamic predictions. Consistent with results at the population level, the individual lesion model outperformed a model relying only on SLD, especially at early landmark times and in patients with liver or bladder target lesions. Our results show that an individual lesion model can characterize the heterogeneity in tumor dynamics and its impact on survival in advanced cancer patients.},
  archive      = {J_BIOMTC},
  author       = {Marion Kerioui and Maxime Beaulieu and Solène Desmée and Julie Bertrand and François Mercier and Jin Y. Jin and René Bruno and Jérémie Guedj},
  doi          = {10.1111/biom.13912},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3752-3763},
  shortjournal = {Biometrics},
  title        = {Nonlinear multilevel joint model for individual lesion kinetics and survival to characterize intra-individual heterogeneity in patients with advanced cancer},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correcting for bias due to mismeasured exposure history in
longitudinal studies with continuous outcomes. <em>BIOMTC</em>,
<em>79</em>(4), 3739–3751. (<a
href="https://doi.org/10.1111/biom.13877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiologists are often interested in estimating the effect of functions of time-varying exposure histories in relation to continuous outcomes, for example, cognitive function. However, the individual exposure measurements that constitute the history upon which an exposure history function is constructed are usually mismeasured. To obtain unbiased estimates of the effects for mismeasured functions in longitudinal studies, a method incorporating main and validation studies was developed. Simulation studies under several realistic assumptions were conducted to assess its performance compared to standard analysis, and we found that the proposed method has good performance in terms of finite sample bias reduction and nominal confidence interval coverage. We applied it to a study of long-term exposure to PM 2.5 $\text{PM}_{2.5}$ , in relation to cognitive decline in the Nurses&#39; Health Study Previously, it was found that the 2-year decline in the standard measure of cognition was 0.018 (95\% CI, −0.034 to −0.001) units worse per 10 μ g/m 3 $\mu \text{g/m}^3$ increase in PM 2.5 $\text{PM}_{2.5}$ exposure. After correction, the estimated impact of PM 2.5 $\text{PM}_{2.5}$ on cognitive decline increased to 0.027 (95\% CI, −0.059 to 0.005) units lower per 10 μ g/m 3 $\mu \text{g/m}^3$ increase. To put this into perspective, effects of this magnitude are about 2/3 of those found in our data associated with each additional year of aging: 0.044 (95\% CI, −0.047 to −0.040) units per 1 year older after applying our correction method.},
  archive      = {J_BIOMTC},
  author       = {Jiachen Cai and Ning Zhang and Xin Zhou and Donna Spiegelman and Molin Wang},
  doi          = {10.1111/biom.13877},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3739-3751},
  shortjournal = {Biometrics},
  title        = {Correcting for bias due to mismeasured exposure history in longitudinal studies with continuous outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal mediation analysis using high-dimensional image
mediator bounded in irregular domain with an application to breast
cancer. <em>BIOMTC</em>, <em>79</em>(4), 3728–3738. (<a
href="https://doi.org/10.1111/biom.13847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is the primary breast cancer screening strategy. Recent methods have been developed using the mammogram image to improve breast cancer risk prediction. However, it is unclear on the extent to which the effect of risk factors on breast cancer risk is mediated through tissue features summarized in mammogram images and the extent to which it is through other pathways. While mediation analysis has been conducted using mammographic density (a summary measure within the image), the mammogram image is not necessarily well described by a single summary measure and, in addition, such a measure provides no spatial information about the relationship between the exposure risk factor and the risk of breast cancer. Thus, to better understand the role of the mammogram images that provide spatial information about the state of the breast tissue that is causally predictive of the future occurrence of breast cancer, we propose a novel method of causal mediation analysis using mammogram image mediator while accommodating the irregular shape of the breast. We apply the proposed method to data from the Joanne Knight Breast Health Cohort and leverage new insights on the decomposition of the total association between risk factor and breast cancer risk that was mediated by the texture of the underlying breast tissue summarized in the mammogram image.},
  archive      = {J_BIOMTC},
  author       = {Shu Jiang and Graham A. Colditz},
  doi          = {10.1111/biom.13847},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3728-3738},
  shortjournal = {Biometrics},
  title        = {Causal mediation analysis using high-dimensional image mediator bounded in irregular domain with an application to breast cancer},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding influential subjects in a network using a causal
framework. <em>BIOMTC</em>, <em>79</em>(4), 3715–3727. (<a
href="https://doi.org/10.1111/biom.13841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers across a wide array of disciplines are interested in finding the most influential subjects in a network. In a network setting, intervention effects and health outcomes can spill over from one node to another through network ties, and influential subjects are expected to have a greater impact than others. For this reason, network research in public health has attempted to maximize health and behavioral changes by intervening on a subset of influential subjects. Although influence is often defined only implicitly in most of the literature, the operative notion of influence is inherently causal in many cases: influential subjects are those we should intervene on to achieve the greatest overall effect across the entire network. In this work, we define a causal notion of influence using potential outcomes. We review existing influence measures, such as node centrality, that largely rely on the particular features of the network structure and/or on certain diffusion models that predict the pattern of information or diseases spreads through network ties. We provide simulation studies to demonstrate when popular centrality measures can agree with our causal measure of influence. As an illustrative example, we apply several popular centrality measures to the HIV risk network in the Transmission Reduction Intervention Project and demonstrate the assumptions under which each centrality can represent the causal influence of each participant in the study.},
  archive      = {J_BIOMTC},
  author       = {Youjin Lee and Ashley L. Buchanan and Elizabeth L. Ogburn and Samuel R. Friedman and M. Elizabeth Halloran and Natallia V. Katenka and Jing Wu and Georgios K. Nikolopoulos},
  doi          = {10.1111/biom.13841},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3715-3727},
  shortjournal = {Biometrics},
  title        = {Finding influential subjects in a network using a causal framework},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Study design for restricted mean time analysis of recurrent
events and death. <em>BIOMTC</em>, <em>79</em>(4), 3701–3714. (<a
href="https://doi.org/10.1111/biom.13923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean time in favor (RMT-IF) of treatment has just been added to the analytic toolbox for composite endpoints of recurrent events and death. To help practitioners design new trials based on this method, we develop tools to calculate the sample size and power. Specifically, we formulate the outcomes as a multistate Markov process with a sequence of transient states for recurrent events and an absorbing state for death. The transition intensities, in this case the instantaneous risks of another nonfatal event or death, are assumed to be time-homogeneous but nonetheless allowed to depend on the number of past events. Using the properties of Coxian distributions, we derive the RMT-IF effect size under the alternative hypothesis as a function of the treatment-to-control intensity ratios along with the baseline intensities, the latter of which can be easily estimated from historical data. We also reduce the variance of the nonparametric RMT-IF estimator to calculable terms under a standard set-up for censoring. Simulation studies show that the resulting formulas provide accurate approximation to the sample size and power in realistic settings. For illustration, a past cardiovascular trial with recurrent-hospitalization and mortality outcomes is analyzed to generate the parameters needed to design a future trial. The procedures are incorporated into the rmt package along with the original methodology on the Comprehensive R Archive Network (CRAN).},
  archive      = {J_BIOMTC},
  author       = {Lu Mao},
  doi          = {10.1111/biom.13923},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3701-3714},
  shortjournal = {Biometrics},
  title        = {Study design for restricted mean time analysis of recurrent events and death},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of dynamic restricted mean survival time based on
pseudo-observations. <em>BIOMTC</em>, <em>79</em>(4), 3690–3700. (<a
href="https://doi.org/10.1111/biom.13891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical follow-up studies with a time-to-event end point, the difference in the restricted mean survival time (RMST) is a suitable substitute for the hazard ratio (HR). However, the RMST only measures the survival of patients over a period of time from the baseline and cannot reflect changes in life expectancy over time. Based on the RMST, we study the conditional restricted mean survival time (cRMST) by estimating life expectancy in the future according to the time that patients have survived, reflecting the dynamic survival status of patients during follow-up. In this paper, we introduce the estimation method of cRMST based on pseudo-observations, the statistical inference concerning the difference between two cRMSTs (cRMSTd), and the establishment of the robust dynamic prediction model using the landmark method. Simulation studies are conducted to evaluate the statistical properties of these methods. The results indicate that the estimation of the cRMST is accurate, and the dynamic RMST model has high accuracy in coefficient estimation and good predictive performance. In addition, an example of patients with chronic kidney disease who received renal transplantations is employed to illustrate that the dynamic RMST model can predict patients’ expected survival times from any prediction time, considering the time-dependent covariates and time-varying effects of covariates.},
  archive      = {J_BIOMTC},
  author       = {Zijing Yang and Chengfeng Zhang and Yawen Hou and Zheng Chen},
  doi          = {10.1111/biom.13891},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3690-3700},
  shortjournal = {Biometrics},
  title        = {Analysis of dynamic restricted mean survival time based on pseudo-observations},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imputation-based q-learning for optimizing dynamic treatment
regimes with right-censored survival outcome. <em>BIOMTC</em>,
<em>79</em>(4), 3676–3689. (<a
href="https://doi.org/10.1111/biom.13872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Q-learning has been one of the most commonly used methods for optimizing dynamic treatment regimes (DTRs) in multistage decision-making. Right-censored survival outcome poses a significant challenge to Q-Learning due to its reliance on parametric models for counterfactual estimation which are subject to misspecification and sensitive to missing covariates. In this paper, we propose an imputation-based Q-learning (IQ-learning) where flexible nonparametric or semiparametric models are employed to estimate optimal treatment rules for each stage and then weighted hot-deck multiple imputation (MI) and direct-draw MI are used to predict optimal potential survival times. Missing data are handled using inverse probability weighting and MI, and the nonrandom treatment assignment among the observed is accounted for using a propensity-score approach. We investigate the performance of IQ-learning via extensive simulations and show that it is more robust to model misspecification than existing Q-Learning methods, imputes only plausible potential survival times contrary to parametric models and provides more flexibility in terms of baseline hazard shape. Using IQ-learning, we developed an optimal DTR for leukemia treatment based on a randomized trial with observational follow-up that motivated this study.},
  archive      = {J_BIOMTC},
  author       = {Lingyun Lyu and Yu Cheng and Abdus S. Wahed},
  doi          = {10.1111/biom.13872},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3676-3689},
  shortjournal = {Biometrics},
  title        = {Imputation-based Q-learning for optimizing dynamic treatment regimes with right-censored survival outcome},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent trajectory models for spatio-temporal dynamics in
alaskan ecosystems. <em>BIOMTC</em>, <em>79</em>(4), 3664–3675. (<a
href="https://doi.org/10.1111/biom.13832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alaskan landscape has undergone substantial changes in recent decades, most notably the expansion of shrubs and trees across the Arctic. We developed a Bayesian hierarchical model to quantify the impact of climate change on the structural transformation of ecosystems using remotely sensed imagery. We used latent trajectory processes to model dynamic state probabilities that evolve annually, from which we derived transition probabilities between ecotypes. Our latent trajectory model accommodates temporal irregularity in survey intervals and uses spatio-temporally heterogeneous climate drivers to infer rates of land cover transitions. We characterized multi-scale spatial correlation induced by plot and subplot arrangements in our study system. We also developed a Pólya–Gamma sampling strategy to improve computation. Our model facilitates inference on the response of ecosystems to shifts in the climate and can be used to predict future land cover transitions under various climate scenarios.},
  archive      = {J_BIOMTC},
  author       = {Xinyi Lu and Mevin B. Hooten and Ann M. Raiho and David K. Swanson and Carl A. Roland and Sarah E. Stehn},
  doi          = {10.1111/biom.13832},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3664-3675},
  shortjournal = {Biometrics},
  title        = {Latent trajectory models for spatio-temporal dynamics in alaskan ecosystems},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial modeling of mycobacterium tuberculosis transmission
with dyadic genetic relatedness data. <em>BIOMTC</em>, <em>79</em>(4),
3650–3663. (<a href="https://doi.org/10.1111/biom.13836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding factors that contribute to the increased likelihood of pathogen transmission between two individuals is important for infection control. However, analyzing measures of pathogen relatedness to estimate these associations is complicated due to correlation arising from the presence of the same individual across multiple dyadic outcomes, potential spatial correlation caused by unmeasured transmission dynamics, and the distinctive distributional characteristics of some of the outcomes. We develop two novel hierarchical Bayesian spatial methods for analyzing dyadic pathogen genetic relatedness data, in the form of patristic distances and transmission probabilities, that simultaneously address each of these complications. Using individual-level spatially correlated random effect parameters, we account for multiple sources of correlation between the outcomes as well as other important features of their distribution. Through simulation, we show the limitations of existing approaches in terms of estimating key associations of interest, and the ability of the new methodology to correct for these issues across datasets with different levels of correlation. All methods are applied to Mycobacterium tuberculosis data from the Republic of Moldova, where we identify previously unknown factors associated with disease transmission and, through analysis of the random effect parameters, key individuals, and areas with increased transmission activity. Model comparisons show the importance of the new methodology in this setting. The methods are implemented in the R package GenePair .},
  archive      = {J_BIOMTC},
  author       = {Joshua L. Warren and Melanie H. Chitwood and Benjamin Sobkowiak and Caroline Colijn and Ted Cohen},
  doi          = {10.1111/biom.13836},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3650-3663},
  shortjournal = {Biometrics},
  title        = {Spatial modeling of mycobacterium tuberculosis transmission with dyadic genetic relatedness data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatially adaptive calibrations of airbox PM2.5 data.
<em>BIOMTC</em>, <em>79</em>(4), 3637–3649. (<a
href="https://doi.org/10.1111/biom.13819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Taiwan air quality monitoring network (TAQMN) and the AirBox network both monitor PM 2.5 in Taiwan. The TAQMN, managed by Taiwan&#39;s Environmental Protection Administration (EPA), provides high-quality PM 2.5 measurements at 77 monitoring stations. The AirBox network launched more recently consists of low-cost, small internet-of-things (IoT) microsensors (i.e., AirBoxes) at thousands of locations. While the AirBox network provides broad spatial coverage, its measurements are unreliable and require calibrations. However, applying a universal calibration procedure to all AirBoxes does not work well because the calibration line varies with local factors, including the chemical compositions of PM 2.5 , which are not homogeneous in space. Therefore, different calibrations are needed at different locations to adapt to their local environments. Unfortunately, AirBoxes and EPA locations are misaligned, challenging the calibration task. In this paper, we propose a spatial model with spatially varying coefficients to account for the heterogeneity in the data. Our method gives spatially adaptive calibrations of AirBoxes and produces accurate PM 2.5 concentration estimates with their error bars at any location, incorporating two types of measurements. In addition, the proposed method is robust to outliers, requires no colocated data, and provides calibration formulas for new AirBoxes once they are added to the network. We illustrate our approach using hourly PM 2.5 data in 2020. After the calibration, the results show that the PM 2.5 prediction improves by about 38\%–68\% in root-mean-squared prediction error. Once the calibration formulas are established, we can obtain reliable PM 2.5 values even if we ignore EPA data.},
  archive      = {J_BIOMTC},
  author       = {ShengLi Tzeng and Chi-Wei Lai and Hsin-Cheng Huang},
  doi          = {10.1111/biom.13819},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3637-3649},
  shortjournal = {Biometrics},
  title        = {Spatially adaptive calibrations of airbox PM2.5 data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian causal inference for observational studies with
missingness in covariates and outcomes. <em>BIOMTC</em>, <em>79</em>(4),
3624–3636. (<a href="https://doi.org/10.1111/biom.13918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are a pervasive issue in observational studies using electronic health records or patient registries. It presents unique challenges for statistical inference, especially causal inference. Inappropriately handling missing data in causal inference could potentially bias causal estimation. Besides missing data problems, observational health data structures typically have mixed-type variables - continuous and categorical covariates - whose joint distribution is often too complex to be modeled by simple parametric models. The existence of missing values in covariates and outcomes makes the causal inference even more challenging, while most standard causal inference approaches assume fully observed data or start their works after imputing missing values in a separate preprocessing stage. To address these problems, we introduce a Bayesian nonparametric causal model to estimate causal effects with missing data. The proposed approach can simultaneously impute missing values, account for multiple outcomes, and estimate causal effects under the potential outcomes framework. We provide three simulation studies to show the performance of our proposed method under complicated data settings whose features are similar to our case studies. For example, Simulation Study 3 assumes the case where missing values exist in both outcomes and covariates. Two case studies were conducted applying our method to evaluate the comparative effectiveness of treatments for chronic disease management in juvenile idiopathic arthritis and cystic fibrosis.},
  archive      = {J_BIOMTC},
  author       = {Huaiyu Zang and Hang J. Kim and Bin Huang and Rhonda Szczesniak},
  doi          = {10.1111/biom.13918},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3624-3636},
  shortjournal = {Biometrics},
  title        = {Bayesian causal inference for observational studies with missingness in covariates and outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic enrichment of bayesian small-sample, sequential,
multiple assignment randomized trial design using natural history data:
A case study from duchenne muscular dystrophy. <em>BIOMTC</em>,
<em>79</em>(4), 3612–3623. (<a
href="https://doi.org/10.1111/biom.13887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Duchenne muscular dystrophy (DMD) and other rare diseases, recruiting patients into clinical trials is challenging. Additionally, assigning patients to long-term, multi-year placebo arms raises ethical and trial retention concerns. This poses a significant challenge to the traditional sequential drug development paradigm. In this paper, we propose a small-sample, sequential, multiple assignment, randomized trial (snSMART) design that combines dose selection and confirmatory assessment into a single trial. This multi-stage design evaluates the effects of multiple doses of a promising drug and re-randomizes patients to appropriate dose levels based on their Stage 1 dose and response. Our proposed approach increases the efficiency of treatment effect estimates by (i) enriching the placebo arm with external control data, and (ii) using data from all stages. Data from external control and different stages are combined using a robust meta-analytic combined (MAC) approach to consider the various sources of heterogeneity and potential selection bias. We reanalyze data from a DMD trial using the proposed method and external control data from the Duchenne Natural History Study (DNHS). Our method&#39;s estimators show improved efficiency compared to the original trial. Also, the robust MAC-snSMART method most often provides more accurate estimators than the traditional analytic method. Overall, the proposed methodology provides a promising candidate for efficient drug development in DMD and other rare diseases.},
  archive      = {J_BIOMTC},
  author       = {Sidi Wang and Kelley M. Kidwell and Satrajit Roychoudhury},
  doi          = {10.1111/biom.13887},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3612-3623},
  shortjournal = {Biometrics},
  title        = {Dynamic enrichment of bayesian small-sample, sequential, multiple assignment randomized trial design using natural history data: A case study from duchenne muscular dystrophy},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse bayesian modeling of hierarchical independent
component analysis: Reliable estimation of individual differences in
brain networks. <em>BIOMTC</em>, <em>79</em>(4), 3599–3611. (<a
href="https://doi.org/10.1111/biom.13867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent component analysis (ICA) is one of the leading approaches for studying brain functional networks. There is increasing interest in neuroscience studies to investigate individual differences in brain networks and their association with demographic characteristics and clinical outcomes. In this work, we develop a sparse Bayesian group hierarchical ICA model that offers significant improvements over existing ICA techniques for identifying covariate effects on the brain network. Specifically, we model the population-level ICA source signals for brain networks using a Dirichlet process mixture. To reliably capture individual differences on brain networks, we propose sparse estimation of the covariate effects in the hierarchical ICA model via a horseshoe prior. Through extensive simulation studies, we show that our approach performs considerably better in detecting covariate effects in comparison with the leading group ICA methods. We then perform an ICA decomposition of a between-subject meditation study. Our method is able to identify significant effects related to meditative practice in brain regions that are consistent with previous research into the default mode network, whereas other group ICA approaches find few to no effects.},
  archive      = {J_BIOMTC},
  author       = {Joshua Lukemire and Giuseppe Pagnoni and Ying Guo},
  doi          = {10.1111/biom.13867},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3599-3611},
  shortjournal = {Biometrics},
  title        = {Sparse bayesian modeling of hierarchical independent component analysis: Reliable estimation of individual differences in brain networks},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian design of multi-regional clinical trials with
time-to-event endpoints. <em>BIOMTC</em>, <em>79</em>(4), 3586–3598. (<a
href="https://doi.org/10.1111/biom.13820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sponsors often rely on multi-regional clinical trials (MRCTs) to introduce new treatments more rapidly into the global market. Many commonly used statistical methods do not account for regional differences, and small regional sample sizes frequently result in lower estimation quality of region-specific treatment effects. The International Council for Harmonization E17 guidelines suggest consideration of methods that allow for information borrowing across regions to improve estimation. In response to these guidelines, we develop a novel methodology to estimate global and region-specific treatment effects from MRCTs with time-to-event endpoints using Bayesian model averaging (BMA). This approach accounts for the possibility of heterogeneous treatment effects between regions, and we discuss how to assess the consistency of these effects using posterior model probabilities. We obtain posterior samples of the treatment effects using a Laplace approximation, and we show through simulation studies that the proposed modeling approach estimates region-specific treatment effects with lower mean squared error than a Cox proportional hazards model while resulting in a similar rejection rate of the global treatment effect. We then apply the BMA approach to data from the LEADER trial, an MRCT designed to evaluate the cardiovascular safety of an anti-diabetic treatment.},
  archive      = {J_BIOMTC},
  author       = {Nathan William Bean and Joseph George Ibrahim and Matthew Austin Psioda},
  doi          = {10.1111/biom.13820},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3586-3598},
  shortjournal = {Biometrics},
  title        = {Bayesian design of multi-regional clinical trials with time-to-event endpoints},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating designs for comparative experiments with two
blocking factors. <em>BIOMTC</em>, <em>79</em>(4), 3574–3585. (<a
href="https://doi.org/10.1111/biom.13913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often, comparative experiments involve a single treatment factor and two blocking factors, for example, augmented row–column, two-phase, and incomplete row–column experiments. These experiments are widely used in agriculture. Finding good designs for these experiments is a major challenge when the number of treatments is large and the blocking structure is complex. In this paper, we first propose a new search algorithm that is combined with efficient update formulae, so that optimal designs with two blocking factors can be found within a reasonable time. Second, we compare augmented row–column designs generated with our new method to those obtained from CycDesigN , DiGGer , and the OPTEX procedure of SAS in terms of computing times as well as the quality of solutions. Third, we illustrate our proposed approach with four applications. We show an example where our efficient update formulae work while existing update formulae cannot be applied, and we use our search framework to generate augmented row–column, two-phase, and incomplete row–column designs. We end the paper with a conclusion along with suggestions for potential applications.},
  archive      = {J_BIOMTC},
  author       = {Nha Vo-Thanh and Hans-Peter Piepho},
  doi          = {10.1111/biom.13913},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3574-3585},
  shortjournal = {Biometrics},
  title        = {Generating designs for comparative experiments with two blocking factors},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stochastic block ising model for multi-layer networks with
inter-layer dependence. <em>BIOMTC</em>, <em>79</em>(4), 3564–3573. (<a
href="https://doi.org/10.1111/biom.13885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection has attracted tremendous interests in network analysis, which aims at finding group of nodes with similar characteristics. Various detection methods have been developed to detect homogeneous communities in multi-layer networks, where inter-layer dependence is a widely acknowledged but severely under-investigated issue. In this paper, we propose a novel stochastic block Ising model (SBIM) to incorporate the inter-layer dependence to help with community detection in multi-layer networks. The community structure is modeled by the stochastic block model (SBM) and the inter-layer dependence is incorporated via the popular Ising model. Furthermore, we develop an efficient variational EM algorithm to tackle the resultant optimization task and establish the asymptotic consistency of the proposed method. Extensive simulated examples and a real example on gene co-expression multi-layer network data are also provided to demonstrate the advantage of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Jingnan Zhang and Chengye Li and Junhui Wang},
  doi          = {10.1111/biom.13885},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3564-3573},
  shortjournal = {Biometrics},
  title        = {A stochastic block ising model for multi-layer networks with inter-layer dependence},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On interquantile smoothness of censored quantile regression
with induced smoothing. <em>BIOMTC</em>, <em>79</em>(4), 3549–3563. (<a
href="https://doi.org/10.1111/biom.13892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression has emerged as a useful and effective tool in modeling survival data, especially for cases where noises demonstrate heterogeneity. Despite recent advancements, non-smooth components involved in censored quantile regression estimators may often yield numerically unstable results, which, in turn, lead to potentially self-contradicting conclusions. We propose an estimating equation-based approach to obtain consistent estimators of the regression coefficients of interest via the induced smoothing technique to circumvent the difficulty. Our proposed estimator can be shown to be asymptotically equivalent to its original unsmoothed version, whose consistency and asymptotic normality can be readily established. Extensions to handle functional covariate data and recurrent event data are also discussed. To alleviate the heavy computational burden of bootstrap-based variance estimation, we also propose an efficient resampling procedure that reduces the computational time considerably. Our numerical studies demonstrate that our proposed estimator provides substantially smoother model parameter estimates across different quantile levels and can achieve better statistical efficiency compared to a plain estimator under various finite-sample settings. The proposed method is also illustrated via four survival datasets, including the HMO (health maintenance organizations) HIV (human immunodeficiency virus) data, the primary biliary cirrhosis (PBC) data, and so forth.},
  archive      = {J_BIOMTC},
  author       = {Zexi Cai and Tony Sit},
  doi          = {10.1111/biom.13892},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3549-3563},
  shortjournal = {Biometrics},
  title        = {On interquantile smoothness of censored quantile regression with induced smoothing},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A seasonality-adjusted sequential test for vaccine safety
surveillance. <em>BIOMTC</em>, <em>79</em>(4), 3533–3548. (<a
href="https://doi.org/10.1111/biom.13829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-market active safety monitoring is important for the timely capture of safety signals associated with exposure to a new vaccine or drug. The group sequential analysis is a common method employed in safety surveillance. Specifically, it compares the post-vaccination incidence of adverse event (AE) in a vaccinated population with a pre-specified reference level by sequentially conducting hypothesis testing during the surveillance. When the number of AEs is “too high”, a safety signal is identified. If the null hypothesis is never rejected, the vaccine is considered safe. Such an approach does not account for either the variation in determining the reference risk from a control population or the seasonality effect. Furthermore, not rejecting the null could be due to a lack of power and cannot always be interpreted as proof of safety. In this paper, we proposed a new group sequential test procedure fully accounting for both seasonality and variation from the historical controls. More importantly, we proposed to construct a confidence interval for the relative AE risk between the exposed and control groups at the end of the study, which can be used to quantify the safety of the vaccine. The proposed method is illustrated via real-data examples on anaphylaxis and examined by extensive simulation studies.},
  archive      = {J_BIOMTC},
  author       = {Rex Shen and Keran Moll and Ying Lu and Lu Tian},
  doi          = {10.1111/biom.13829},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3533-3548},
  shortjournal = {Biometrics},
  title        = {A seasonality-adjusted sequential test for vaccine safety surveillance},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Detecting the spatial clustering of exposure–response
relationships with estimation error: A novel spatial scan statistic.
<em>BIOMTC</em>, <em>79</em>(4), 3522–3532. (<a
href="https://doi.org/10.1111/biom.13861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting the spatial clustering of the exposure–response relationship (ERR) between environmental risk factors and health-related outcomes plays important roles in disease control and prevention, such as identifying highly sensitive regions, exploring the causes of heterogeneous ERRs, and designing region-specific health intervention measures. However, few studies have focused on this issue. A possible reason is that the commonly used cluster-detecting tool, spatial scan statistics, cannot be used for multivariate spatial datasets with estimation error, such as the ERR, which is often defined by a vector with its covariance estimated by a regression model. Such spatial datasets have been produced in abundance in the last decade, which suggests the importance of developing a novel cluster-detecting tool applicable for multivariate datasets with estimation error. In this work, by extending the classic scan statistic, we developed a novel spatial scan statistic called the estimation-error-based scan statistic (EESS), which is applicable for both univariate and multivariate datasets with estimation error. Then, a two-stage analytic process was proposed to detect the spatial clustering of ERRs in practical studies. A published motivating example and a simulation study were used to validate the performance of EESS. The results show that the clusters detected by EESS can efficiently reflect the clustering heterogeneity and yield more accurate ERR estimates by adjusting for such heterogeneity.},
  archive      = {J_BIOMTC},
  author       = {Wei Wang and Sheng Li and Tao Zhang and Fei Yin and Yue Ma},
  doi          = {10.1111/biom.13861},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3522-3532},
  shortjournal = {Biometrics},
  title        = {Detecting the spatial clustering of exposure–response relationships with estimation error: A novel spatial scan statistic},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing data in complicated 3D domains: Smoothing,
semiparametric regression, and functional principal component analysis.
<em>BIOMTC</em>, <em>79</em>(4), 3510–3521. (<a
href="https://doi.org/10.1111/biom.13845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a family of methods for the analysis of data observed at locations scattered in three-dimensional (3D) domains, with possibly complicated shapes. The proposed family of methods includes smoothing, regression, and functional principal component analysis for functional signals defined over (possibly nonconvex) 3D domains, appropriately complying with the nontrivial shape of the domain. This constitutes an important advance with respect to the literature, because the available methods to analyze data observed in 3D domains rely on Euclidean distances, which are inappropriate when the shape of the domain influences the phenomenon under study. The common building block of the proposed methods is a nonparametric regression model with differential regularization. We derive the asymptotic properties of the methods and show, through simulation studies, that they are superior to the available alternatives for the analysis of data in 3D domains, even when considering domains with simple shapes. We finally illustrate an application to a neurosciences study, with neuroimaging signals from functional magnetic resonance imaging, measuring neural activity in the gray matter, a nonconvex volume with a highly complicated structure.},
  archive      = {J_BIOMTC},
  author       = {Eleonora Arnone and Luca Negri and Ferruccio Panzica and Laura M. Sangalli},
  doi          = {10.1111/biom.13845},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3510-3521},
  shortjournal = {Biometrics},
  title        = {Analyzing data in complicated 3D domains: Smoothing, semiparametric regression, and functional principal component analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). False discovery rate-controlled multiple testing for union
null hypotheses: A knockoff-based approach. <em>BIOMTC</em>,
<em>79</em>(4), 3497–3509. (<a
href="https://doi.org/10.1111/biom.13848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False discovery rate (FDR) controlling procedures provide important statistical guarantees for replicability in signal identification based on multiple hypotheses testing. In many fields of study, FDR controling procedures are used in high-dimensional (HD) analyses to discover features that are truly associated with the outcome. In some recent applications, data on the same set of candidate features are independently collected in multiple different studies. For example, gene expression data are collected at different facilities and with different cohorts, to identify the genetic biomarkers of multiple types of cancers. These studies provide us with opportunities to identify signals by considering information from different sources (with potential heterogeneity) jointly. This paper is about how to provide FDR control guarantees for the tests of union null hypotheses of conditional independence. We present a knockoff-based variable selection method ( Simultaneous knockoffs ) to identify mutual signals from multiple independent datasets, providing exact FDR control guarantees under finite sample settings. This method can work with very general model settings and test statistics. We demonstrate the performance of this method with extensive numerical studies and two real-data examples.},
  archive      = {J_BIOMTC},
  author       = {Ran Dai and Cheng Zheng},
  doi          = {10.1111/biom.13848},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3497-3509},
  shortjournal = {Biometrics},
  title        = {False discovery rate-controlled multiple testing for union null hypotheses: A knockoff-based approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiresolution categorical regression for interpretable
cell-type annotation. <em>BIOMTC</em>, <em>79</em>(4), 3485–3496. (<a
href="https://doi.org/10.1111/biom.13926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many categorical response regression applications, the response categories admit a multiresolution structure. That is, subsets of the response categories may naturally be combined into coarser response categories. In such applications, practitioners are often interested in estimating the resolution at which a predictor affects the response category probabilities. In this paper, we propose a method for fitting the multinomial logistic regression model in high dimensions that addresses this problem in a unified and data-driven way. Our method allows practitioners to identify which predictors distinguish between coarse categories but not fine categories, which predictors distinguish between fine categories, and which predictors are irrelevant. For model fitting, we propose a scalable algorithm that can be applied when the coarse categories are defined by either overlapping or nonoverlapping sets of fine categories. Statistical properties of our method reveal that it can take advantage of this multiresolution structure in a way existing estimators cannot. We use our method to model cell-type probabilities as a function of a cell&#39;s gene expression profile (i.e., cell-type annotation). Our fitted model provides novel biological insights which may be useful for future automated and manual cell-type annotation methodology.},
  archive      = {J_BIOMTC},
  author       = {Aaron J. Molstad and Keshav Motwani},
  doi          = {10.1111/biom.13926},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3485-3496},
  shortjournal = {Biometrics},
  title        = {Multiresolution categorical regression for interpretable cell-type annotation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Competition-based control of the false discovery proportion.
<em>BIOMTC</em>, <em>79</em>(4), 3472–3484. (<a
href="https://doi.org/10.1111/biom.13830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Barber and Candès laid the theoretical foundation for a general framework for false discovery rate (FDR) control based on the notion of “knockoffs.” A closely related FDR control methodology has long been employed in the analysis of mass spectrometry data, referred to there as “target–decoy competition” (TDC). However, any approach that aims to control the FDR, which is defined as the expected value of the false discovery proportion (FDP), suffers from a problem. Specifically, even when successfully controlling the FDR at level α, the FDP in the list of discoveries can significantly exceed α. We offer FDP-SD, a new procedure that rigorously controls the FDP in the knockoff/TDC competition setup by guaranteeing that the FDP is bounded by α at a desired confidence level. Compared with the recently published framework of Katsevich and Ramdas, FDP-SD generally delivers more power and often substantially so in simulated and real data.},
  archive      = {J_BIOMTC},
  author       = {Dong Luo and Arya Ebadi and Kristen Emery and Yilun He and William Stafford Noble and Uri Keich},
  doi          = {10.1111/biom.13830},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3472-3484},
  shortjournal = {Biometrics},
  title        = {Competition-based control of the false discovery proportion},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional inference in cis-mendelian randomization using
weak genetic factors. <em>BIOMTC</em>, <em>79</em>(4), 3458–3471. (<a
href="https://doi.org/10.1111/biom.13888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a widely used method to estimate the causal effect of an exposure on an outcome by using genetic variants as instrumental variables. MR analyses that use variants from only a single genetic region ( cis -MR) encoding the protein target of a drug are able to provide supporting evidence for drug target validation. This paper proposes methods for cis -MR inference that use many correlated variants to make robust inferences even in situations, where those variants have only weak effects on the exposure. In particular, we exploit the highly structured nature of genetic correlations in single gene regions to reduce the dimension of genetic variants using factor analysis. These genetic factors are then used as instrumental variables to construct tests for the causal effect of interest. Since these factors may often be weakly associated with the exposure, size distortions of standard t -tests can be severe. Therefore, we consider two approaches based on conditional testing. First, we extend results of commonly-used identification-robust tests for the setting where estimated factors are used as instruments. Second, we propose a test which appropriately adjusts for first-stage screening of genetic factors based on their relevance. Our empirical results provide genetic evidence to validate cholesterol-lowering drug targets aimed at preventing coronary heart disease.},
  archive      = {J_BIOMTC},
  author       = {Ashish Patel and Dipender Gill and Paul Newcombe and Stephen Burgess},
  doi          = {10.1111/biom.13888},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3458-3471},
  shortjournal = {Biometrics},
  title        = {Conditional inference in cis-mendelian randomization using weak genetic factors},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse estimation in semiparametric finite mixture of
varying coefficient regression models. <em>BIOMTC</em>, <em>79</em>(4),
3445–3457. (<a href="https://doi.org/10.1111/biom.13870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite mixture of regressions (FMR) are commonly used to model heterogeneous effects of covariates on a response variable in settings where there are unknown underlying subpopulations. FMRs, however, cannot accommodate situations where covariates&#39; effects also vary according to an “index” variable—known as finite mixture of varying coefficient regression (FM-VCR). Although complex, this situation occurs in real data applications: the osteocalcin (OCN) data analyzed in this manuscript presents a heterogeneous relationship where the effect of a genetic variant on OCN in each hidden subpopulation varies over time. Oftentimes, the number of covariates with varying coefficients also presents a challenge: in the OCN study, genetic variants on the same chromosome are considered jointly. The relative proportions of hidden subpopulations may also change over time. Nevertheless, existing methods cannot provide suitable solutions for accommodating all these features in real data applications. To fill this gap, we develop statistical methodologies based on regularized local-kernel likelihood for simultaneous parameter estimation and variable selection in sparse FM-VCR models. We study large-sample properties of the proposed methods. We then carry out a simulation study to evaluate the performance of various penalties adopted for our regularized approach and ascertain the ability of a BIC-type criterion for estimating the number of subpopulations. Finally, we applied the FM-VCR model to analyze the OCN data and identified several covariates, including genetic variants, that have age-dependent effects on OCN.},
  archive      = {J_BIOMTC},
  author       = {Abbas Khalili and Farhad Shokoohi and Masoud Asgharian and Shili Lin},
  doi          = {10.1111/biom.13870},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3445-3457},
  shortjournal = {Biometrics},
  title        = {Sparse estimation in semiparametric finite mixture of varying coefficient regression models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible joint modeling of mean and dispersion for the
directional tuning of neuronal spike counts. <em>BIOMTC</em>,
<em>79</em>(4), 3431–3444. (<a
href="https://doi.org/10.1111/biom.13882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of how the number of spikes in a middle temporal visual area (MT/V5) neuron is tuned to the direction of a visual stimulus has attracted considerable attention over the years, but recent studies suggest that the variability of the number of spikes might also be influenced by the directional stimulus. This entails that Poisson regression models are not adequate for this type of data, as the observations usually present over/underdispersion (or both) with respect to the Poisson distribution. This paper makes use of the double exponential family and presents a flexible model to estimate, jointly, the mean and dispersion functions, accounting for the effect of a circular covariate. The empirical performance of the proposal is explored via simulations and an application to a neurological data set is shown.},
  archive      = {J_BIOMTC},
  author       = {María Alonso-Pena and Irène Gijbels and Rosa M. Crujeiras},
  doi          = {10.1111/biom.13882},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3431-3444},
  shortjournal = {Biometrics},
  title        = {Flexible joint modeling of mean and dispersion for the directional tuning of neuronal spike counts},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Longitudinal incremental propensity score interventions for
limited resource settings. <em>BIOMTC</em>, <em>79</em>(4), 3418–3430.
(<a href="https://doi.org/10.1111/biom.13859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-life treatments are of limited supply and cannot be provided to all individuals in the population. For example, patients on the liver transplant waiting list usually cannot be assigned a liver transplant immediately at the time they reach highest priority because a suitable organ is not immediately available. In settings with limited supply, investigators are often interested in the effects of treatment strategies in which a limited proportion of patients receive an organ at a given time, that is, treatment regimes satisfying resource constraints. Here, we describe an estimand that allows us to define causal effects of treatment strategies that satisfy resource constraints: incremental propensity score interventions (IPSIs) for limited resources. IPSIs flexibly constrain time-varying resource utilization through proportional scaling of patients&#39; natural propensities for treatment, thereby preserving existing propensity rank ordering compared to the status quo . We derive a simple class of inverse-probability-weighted estimators, and we apply one such estimator to evaluate the effect of restricting or expanding utilization of “increased risk” liver organs to treat patients with end-stage liver disease.},
  archive      = {J_BIOMTC},
  author       = {Aaron L. Sarvet and Kerollos N. Wanis and Jessica G. Young and Roberto Hernandez-Alejandro and Mats J. Stensrud},
  doi          = {10.1111/biom.13859},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3418-3430},
  shortjournal = {Biometrics},
  title        = {Longitudinal incremental propensity score interventions for limited resource settings},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining mixed effects hidden markov models with latent
alternating recurrent event processes to model diurnal active–rest
cycles. <em>BIOMTC</em>, <em>79</em>(4), 3402–3417. (<a
href="https://doi.org/10.1111/biom.13865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected from wearable devices can shed light on an individual&#39;s pattern of behavioral and circadian routine. Phone use can be modeled as alternating processes, between the state of active use and the state of being idle. Markov chains and alternating recurrent event models are commonly used to model state transitions in cases such as these, and the incorporation of random effects can be used to introduce diurnal effects. While state labels can be derived prior to modeling dynamics, this approach omits informative regression covariates that can influence state memberships. We instead propose an alternating recurrent event proportional hazards (PH) regression to model the transitions between latent states. We propose an expectation–maximization algorithm for imputing latent state labels and estimating parameters. We show that our E-step simplifies to the hidden Markov model (HMM) forward–backward algorithm, allowing us to recover an HMM with logistic regression transition probabilities. In addition, we show that PH modeling of discrete-time transitions implicitly penalizes the logistic regression likelihood and results in shrinkage estimators for the relative risk. This new estimator favors an extended stay in a state and is useful for modeling diurnal rhythms. We derive asymptotic distributions for our parameter estimates and compare our approach against competing methods through simulation as well as in a digital phenotyping study that followed smartphone use in a cohort of adolescents with mood disorders.},
  archive      = {J_BIOMTC},
  author       = {Benny Ren and Ian Barnett},
  doi          = {10.1111/biom.13865},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3402-3417},
  shortjournal = {Biometrics},
  title        = {Combining mixed effects hidden markov models with latent alternating recurrent event processes to model diurnal active–rest cycles},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous selection and inference for varying
coefficients with zero regions: A soft-thresholding approach.
<em>BIOMTC</em>, <em>79</em>(4), 3388–3401. (<a
href="https://doi.org/10.1111/biom.13900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varying coefficient models have been used to explore dynamic effects in many scientific areas, such as in medicine, finance, and epidemiology. As most existing models ignore the existence of zero regions, we propose a new soft-thresholded varying coefficient model, where the coefficient functions are piecewise smooth with zero regions. Our new modeling approach enables us to perform variable selection, detect the zero regions of selected variables, obtain point estimates of the varying coefficients with zero regions, and construct a new type of sparse confidence intervals that accommodate zero regions. We prove the asymptotic properties of the estimator, based on which we draw statistical inference. Our simulation study reveals that the proposed sparse confidence intervals achieve the desired coverage probability. We apply the proposed method to analyze a large-scale preoperative opioid study.},
  archive      = {J_BIOMTC},
  author       = {Yuan Yang and Ziyang Pan and Jian Kang and Chad Brummett and Yi Li},
  doi          = {10.1111/biom.13900},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3388-3401},
  shortjournal = {Biometrics},
  title        = {Simultaneous selection and inference for varying coefficients with zero regions: A soft-thresholding approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous and error-prone longitudinal data analysis via
functional calibration. <em>BIOMTC</em>, <em>79</em>(4), 3374–3387. (<a
href="https://doi.org/10.1111/biom.13866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many longitudinal settings, time-varying covariates may not be measured at the same time as responses and are often prone to measurement error. Naive last-observation-carried-forward methods incur estimation biases, and existing kernel-based methods suffer from slow convergence rates and large variations. To address these challenges, we propose a new functional calibration approach to efficiently learn longitudinal covariate processes based on sparse functional data with measurement error. Our approach, stemming from functional principal component analysis, calibrates the unobserved synchronized covariate values from the observed asynchronous and error-prone covariate values, and is broadly applicable to asynchronous longitudinal regression with time-invariant or time-varying coefficients. For regression with time-invariant coefficients, our estimator is asymptotically unbiased, root-n consistent, and asymptotically normal; for time-varying coefficient models, our estimator has the optimal varying coefficient model convergence rate with inflated asymptotic variance from the calibration. In both cases, our estimators present asymptotic properties superior to the existing methods. The feasibility and usability of the proposed methods are verified by simulations and an application to the Study of Women&#39;s Health Across the Nation, a large-scale multisite longitudinal study on women&#39;s health during midlife.},
  archive      = {J_BIOMTC},
  author       = {Xinyue Chang and Yehua Li and Yi Li},
  doi          = {10.1111/biom.13866},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3374-3387},
  shortjournal = {Biometrics},
  title        = {Asynchronous and error-prone longitudinal data analysis via functional calibration},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-level structured functional analysis for genome-wide
association studies. <em>BIOMTC</em>, <em>79</em>(4), 3359–3373. (<a
href="https://doi.org/10.1111/biom.13871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genome-wide association studies (GWAS) have led to great successes in identifying genotype–phenotype associations for complex human diseases. In such studies, the high dimensionality of single nucleotide polymorphisms (SNPs) often makes analysis difficult. Functional analysis, which interprets SNPs densely distributed in a chromosomal region as a continuous process rather than discrete observations, has emerged as a promising avenue for overcoming the high dimensionality challenges. However, the majority of the existing functional studies continue to be individual SNP based and are unable to sufficiently account for the intricate underpinning structures of SNP data. SNPs are often found in groups (e.g., genes or pathways) and have a natural group structure. Additionally, these SNP groups can be highly correlated with coordinated biological functions and interact in a network. Motivated by these unique characteristics of SNP data, we develop a novel bi-level structured functional analysis method and investigate disease-associated genetic variants at the SNP level and SNP group level simultaneously. The penalization technique is adopted for bi-level selection and also to accommodate the group-level network structure. Both the estimation and selection consistency properties are rigorously established. The superiority of the proposed method over alternatives is shown through extensive simulation studies. A type 2 diabetes SNP data application yields some biologically intriguing results.},
  archive      = {J_BIOMTC},
  author       = {Mengyun Wu and Fan Wang and Yeheng Ge and Shuangge Ma and Yang Li},
  doi          = {10.1111/biom.13871},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3359-3373},
  shortjournal = {Biometrics},
  title        = {Bi-level structured functional analysis for genome-wide association studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent deformation models for multivariate functional data
and time-warping separability. <em>BIOMTC</em>, <em>79</em>(4),
3345–3358. (<a href="https://doi.org/10.1111/biom.13851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate functional data present theoretical and practical complications that are not found in univariate functional data. One of these is a situation where the component functions of multivariate functional data are positive and are subject to mutual time warping. That is, the component processes exhibit a common shape but are subject to systematic phase variation across their domains in addition to subject-specific time warping, where each subject has its own internal clock. This motivates a novel model for multivariate functional data that connect such mutual time warping to a latent-deformation-based framework by exploiting a novel time-warping separability assumption. This separability assumption allows for meaningful interpretation and dimension reduction. The resulting latent deformation model is shown to be well suited to represent commonly encountered functional vector data. The proposed approach combines a random amplitude factor for each component with population-based registration across the components of a multivariate functional data vector and includes a latent population function, which corresponds to a common underlying trajectory. We propose estimators for all components of the model, enabling implementation of the proposed data-based representation for multivariate functional data and downstream analyses such as Fréchet regression. Rates of convergence are established when curves are fully observed or observed with measurement error. The usefulness of the model, interpretations, and practical aspects are illustrated in simulations and with application to multivariate human growth curves and multivariate environmental pollution data.},
  archive      = {J_BIOMTC},
  author       = {Cody Carroll and Hans-Georg Müller},
  doi          = {10.1111/biom.13851},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3345-3358},
  shortjournal = {Biometrics},
  title        = {Latent deformation models for multivariate functional data and time-warping separability},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Homogeneity tests of covariance for high-dimensional
functional data with applications to event segmentation.
<em>BIOMTC</em>, <em>79</em>(4), 3332–3344. (<a
href="https://doi.org/10.1111/biom.13844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider inference problems for high-dimensional (HD) functional data with a dense number of T repeated measurements taken for a large number of p variables from a small number of n experimental units. The spatial and temporal dependence, high dimensionality, and dense number of repeated measurements pose theoretical and computational challenges. This paper has two aims; our first aim is to solve the theoretical and computational challenges in testing equivalence among covariance matrices from HD functional data. The second aim is to provide computationally efficient and tuning-free tools with guaranteed stochastic error control. The weak convergence of the stochastic process formed by the test statistics is established under the “large p , large T , and small n ” setting. If the null is rejected, we further show that the locations of the change points can be estimated consistently. The estimator&#39;s rate of convergence is shown to depend on the data dimension, sample size, number of repeated measurements, and signal-to-noise ratio. We also show that our proposed computation algorithms can significantly reduce the computation time and are applicable to real-world data with a large number of HD-repeated measurements (e.g., functional magnetic resonance imaging (fMRI) data). Simulation results demonstrate both the finite sample performance and computational effectiveness of our proposed procedures. We observe that the empirical size of the test is well controlled at the nominal level, and the locations of multiple change points can be accurately identified. An application to fMRI data demonstrates that our proposed methods can identify event boundaries in the preface of the television series Sherlock . Code to implement the procedures is available in an R package named TechPhD .},
  archive      = {J_BIOMTC},
  author       = {Ping-Shou Zhong},
  doi          = {10.1111/biom.13844},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3332-3344},
  shortjournal = {Biometrics},
  title        = {Homogeneity tests of covariance for high-dimensional functional data with applications to event segmentation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear function-on-scalar regression via functional
universal approximation. <em>BIOMTC</em>, <em>79</em>(4), 3319–3331. (<a
href="https://doi.org/10.1111/biom.13838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider general nonlinear function-on-scalar (FOS) regression models, where the functional response depends on multiple scalar predictors in a general unknown nonlinear form. Existing methods either assume specific model forms (e.g., additive models) or directly estimate the nonlinear function in a space with dimension equal to the number of scalar predictors, which can only be applied to models with a few scalar predictors. To overcome these shortcomings, motivated by the classic universal approximation theorem used in neural networks, we develop a functional universal approximation theorem which can be used to approximate general nonlinear FOS maps and can be easily adopted into the framework of functional data analysis. With this theorem and utilizing smoothness regularity, we develop a novel method to fit the general nonlinear FOS regression model and make predictions. Our new method does not make any specific assumption on the model forms, and it avoids the direct estimation of nonlinear functions in a space with dimension equal to the number of scalar predictors. By estimating a sequence of bivariate functions, our method can be applied to models with a relatively large number of scalar predictors. The good performance of the proposed method is demonstrated by empirical studies on various simulated and real datasets.},
  archive      = {J_BIOMTC},
  author       = {Ruiyan Luo and Xin Qi},
  doi          = {10.1111/biom.13838},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3319-3331},
  shortjournal = {Biometrics},
  title        = {Nonlinear function-on-scalar regression via functional universal approximation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent factor model for multivariate functional data.
<em>BIOMTC</em>, <em>79</em>(4), 3307–3318. (<a
href="https://doi.org/10.1111/biom.13924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multivariate functional data, a functional latent factor model is proposed, extending the traditional latent factor model for multivariate data. The proposed model uses unobserved stochastic processes to induce the dependence among the different functions, and thus, for a large number of functions, may provide a more parsimonious and interpretable characterization of the otherwise complex dependencies between the functions. Sufficient conditions are provided to establish the identifiability of the proposed model. The performance of the proposed model is assessed through simulation studies and an application to electroencephalography data.},
  archive      = {J_BIOMTC},
  author       = {Ruonan Li and Luo Xiao},
  doi          = {10.1111/biom.13924},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3307-3318},
  shortjournal = {Biometrics},
  title        = {Latent factor model for multivariate functional data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian functional data analysis over dependent regions and
its application for identification of differentially methylated regions.
<em>BIOMTC</em>, <em>79</em>(4), 3294–3306. (<a
href="https://doi.org/10.1111/biom.13902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a Bayesian functional data analysis for observations measured as extremely long sequences. Splitting the sequence into several small windows with manageable lengths, the windows may not be independent especially when they are neighboring each other. We propose to utilize Bayesian smoothing splines to estimate individual functional patterns within each window and to establish transition models for parameters involved in each window to address the dependence structure between windows. The functional difference of groups of individuals at each window can be evaluated by the Bayes factor based on Markov Chain Monte Carlo samples in the analysis. In this paper, we examine the proposed method through simulation studies and apply it to identify differentially methylated genetic regions in TCGA lung adenocarcinoma data.},
  archive      = {J_BIOMTC},
  author       = {Suvo Chatterjee and Shrabanti Chowdhury and Duchwan Ryu and Sanjib Basu},
  doi          = {10.1111/biom.13902},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3294-3306},
  shortjournal = {Biometrics},
  title        = {Bayesian functional data analysis over dependent regions and its application for identification of differentially methylated regions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional bayesian networks for discovering causality from
multivariate functional data. <em>BIOMTC</em>, <em>79</em>(4),
3279–3293. (<a href="https://doi.org/10.1111/biom.13922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate functional data arise in a wide range of applications. One fundamental task is to understand the causal relationships among these functional objects of interest. In this paper, we develop a novel Bayesian network (BN) model for multivariate functional data where conditional independencies and causal structure are encoded by a directed acyclic graph. Specifically, we allow the functional objects to deviate from Gaussian processes, which is the key to unique causal structure identification even when the functions are measured with noises. A fully Bayesian framework is designed to infer the functional BN model with natural uncertainty quantification through posterior summaries. Simulation studies and real data examples demonstrate the practical utility of the proposed model.},
  archive      = {J_BIOMTC},
  author       = {Fangting Zhou and Kejun He and Kunbo Wang and Yanxun Xu and Yang Ni},
  doi          = {10.1111/biom.13922},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3279-3293},
  shortjournal = {Biometrics},
  title        = {Functional bayesian networks for discovering causality from multivariate functional data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian model selection for generalized linear mixed
models. <em>BIOMTC</em>, <em>79</em>(4), 3266–3278. (<a
href="https://doi.org/10.1111/biom.13896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian model selection approach for generalized linear mixed models (GLMMs). We consider covariance structures for the random effects that are widely used in areas such as longitudinal studies, genome-wide association studies, and spatial statistics. Since the random effects cannot be integrated out of GLMMs analytically, we approximate the integrated likelihood function using a pseudo-likelihood approach. Our Bayesian approach assumes a flat prior for the fixed effects and includes both approximate reference prior and half-Cauchy prior choices for the variances of random effects. Since the flat prior on the fixed effects is improper, we develop a fractional Bayes factor approach to obtain posterior probabilities of the several competing models. Simulation studies with Poisson GLMMs with spatial random effects and overdispersion random effects show that our approach performs favorably when compared to widely used competing Bayesian methods including deviance information criterion and Watanabe–Akaike information criterion. We illustrate the usefulness and flexibility of our approach with three case studies including a Poisson longitudinal model, a Poisson spatial model, and a logistic mixed model. Our proposed approach is implemented in the R package GLMMselect that is available on CRAN.},
  archive      = {J_BIOMTC},
  author       = {Shuangshuang Xu and Marco A. R. Ferreira and Erica M. Porter and Christopher T. Franck},
  doi          = {10.1111/biom.13896},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3266-3278},
  shortjournal = {Biometrics},
  title        = {Bayesian model selection for generalized linear mixed models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric adjustment of confounding.
<em>BIOMTC</em>, <em>79</em>(4), 3252–3265. (<a
href="https://doi.org/10.1111/biom.13833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of observational studies increasingly confronts the challenge of determining which of a possibly high-dimensional set of available covariates are required to satisfy the assumption of ignorable treatment assignment for estimation of causal effects. We propose a Bayesian nonparametric approach that simultaneously (1) prioritizes inclusion of adjustment variables in accordance with existing principles of confounder selection; (2) estimates causal effects in a manner that permits complex relationships among confounders, exposures, and outcomes; and (3) provides causal estimates that account for uncertainty in the nature of confounding. The proposal relies on specification of multiple Bayesian additive regression trees models, linked together with a common prior distribution that accrues posterior selection probability to covariates on the basis of association with both the exposure and the outcome of interest. A set of extensive simulation studies demonstrates that the proposed method performs well relative to similarly-motivated methodologies in a variety of scenarios. We deploy the method to investigate the causal effect of emissions from coal-fired power plants on ambient air pollution concentrations, where the prospect of confounding due to local and regional meteorological factors introduces uncertainty around the confounding role of a high-dimensional set of measured variables. Ultimately, we show that the proposed method produces more efficient and more consistent results across adjacent years than alternative methods, lending strength to the evidence of the causal relationship between SO 2 emissions and ambient particulate pollution.},
  archive      = {J_BIOMTC},
  author       = {Chanmin Kim and Mauricio Tec and Corwin Zigler},
  doi          = {10.1111/biom.13833},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3252-3265},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric adjustment of confounding},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian zero-inflated dirichlet-multinomial regression
model for multivariate compositional count data. <em>BIOMTC</em>,
<em>79</em>(4), 3239–3251. (<a
href="https://doi.org/10.1111/biom.13853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dirichlet-multinomial (DM) distribution plays a fundamental role in modern statistical methodology development and application. Recently, the DM distribution and its variants have been used extensively to model multivariate count data generated by high-throughput sequencing technology in omics research due to its ability to accommodate the compositional structure of the data as well as overdispersion. A major limitation of the DM distribution is that it is unable to handle excess zeros typically found in practice which may bias inference. To fill this gap, we propose a novel Bayesian zero-inflated DM model for multivariate compositional count data with excess zeros. We then extend our approach to regression settings and embed sparsity-inducing priors to perform variable selection for high-dimensional covariate spaces. Throughout, modeling decisions are made to boost scalability without sacrificing interpretability or imposing limiting assumptions. Extensive simulations and an application to a human gut microbiome dataset are presented to compare the performance of the proposed method to existing approaches. We provide an accompanying R package with a user-friendly vignette to apply our method to other datasets.},
  archive      = {J_BIOMTC},
  author       = {Matthew D. Koslovsky},
  doi          = {10.1111/biom.13853},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3239-3251},
  shortjournal = {Biometrics},
  title        = {A bayesian zero-inflated dirichlet-multinomial regression model for multivariate compositional count data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A double-robust test for high-dimensional gene coexpression
networks conditioning on clinical information. <em>BIOMTC</em>,
<em>79</em>(4), 3227–3238. (<a
href="https://doi.org/10.1111/biom.13890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been increasingly appealing to evaluate whether expression levels of two genes in a gene coexpression network are still dependent given samples&#39; clinical information, in which the conditional independence test plays an essential role. For enhanced robustness regarding model assumptions, we propose a class of double-robust tests for evaluating the dependence of bivariate outcomes after controlling for known clinical information. Although the proposed test relies on the marginal density functions of bivariate outcomes given clinical information, the test remains valid as long as one of the density functions is correctly specified. Because of the closed-form variance formula, the proposed test procedure enjoys computational efficiency without requiring a resampling procedure or tuning parameters. We acknowledge the need to infer the conditional independence network with high-dimensional gene expressions, and further develop a procedure for multiple testing by controlling the false discovery rate. Numerical results show that our method accurately controls both the type-I error and false discovery rate, and it provides certain levels of robustness regarding model misspecification. We apply the method to a gastric cancer study with gene expression data to understand the associations between genes belonging to the transforming growth factor β signaling pathway given cancer-stage information.},
  archive      = {J_BIOMTC},
  author       = {Maomao Ding and Ruosha Li and Jin Qin and Jing Ning},
  doi          = {10.1111/biom.13890},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3227-3238},
  shortjournal = {Biometrics},
  title        = {A double-robust test for high-dimensional gene coexpression networks conditioning on clinical information},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instability of inverse probability weighting methods and a
remedy for nonignorable missing data. <em>BIOMTC</em>, <em>79</em>(4),
3215–3226. (<a href="https://doi.org/10.1111/biom.13881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability weighting (IPW) methods are commonly used to analyze nonignorable missing data (NIMD) under the assumption of a logistic model for the missingness probability. However, solving IPW equations numerically may involve nonconvergence problems when the sample size is moderate and the missingness probability is high. Moreover, those equations often have multiple roots, and identifying the best root is challenging. Therefore, IPW methods may have low efficiency or even produce biased results. We identify the pitfall in these methods pathologically: they involve the estimation of a moment-generating function (MGF), and such functions are notoriously unstable in general. As a remedy, we model the outcome distribution given the covariates of the completely observed individuals semiparametrically. After forming an induced logistic regression (LR) model for the missingness status of the outcome and covariate, we develop a maximum conditional likelihood method to estimate the underlying parameters. The proposed method circumvents the estimation of an MGF and hence overcomes the instability of IPW methods. Our theoretical and simulation results show that the proposed method outperforms existing competitors greatly. Two real data examples are analyzed to illustrate the advantages of our method. We conclude that if only a parametric LR is assumed but the outcome regression model is left arbitrary, then one has to be cautious in using any of the existing statistical methods in problems involving NIMD.},
  archive      = {J_BIOMTC},
  author       = {Pengfei Li and Jing Qin and Yukun Liu},
  doi          = {10.1111/biom.13881},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3215-3226},
  shortjournal = {Biometrics},
  title        = {Instability of inverse probability weighting methods and a remedy for nonignorable missing data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-censoring model for multivariate nonignorable
nonmonotone missing data. <em>BIOMTC</em>, <em>79</em>(4), 3203–3214.
(<a href="https://doi.org/10.1111/biom.13916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an itemwise modeling approach called “self-censoring” for multivariate nonignorable nonmonotone missing data, where the missingness process of each outcome can be affected by its own value and associated with missingness indicators of other outcomes, while conditionally independent of the other outcomes. The self-censoring model complements previous graphical approaches for the analysis of multivariate nonignorable missing data. It is identified under a completeness condition stating that any variability in one outcome can be captured by variability in the other outcomes among complete cases. For estimation, we propose a suite of semiparametric estimators including doubly robust estimators that deliver valid inferences under partial misspecification of the full-data distribution. We also provide a novel and flexible global sensitivity analysis procedure anchored at the self-censoring. We evaluate the performance of the proposed methods with simulations and apply them to analyze a study about the effect of highly active antiretroviral therapy on preterm delivery of HIV-positive mothers.},
  archive      = {J_BIOMTC},
  author       = {Yilin Li and Wang Miao and Ilya Shpitser and Eric J. Tchetgen Tchetgen},
  doi          = {10.1111/biom.13916},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3203-3214},
  shortjournal = {Biometrics},
  title        = {A self-censoring model for multivariate nonignorable nonmonotone missing data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individualized causal discovery with latent trajectory
embedded bayesian networks. <em>BIOMTC</em>, <em>79</em>(4), 3191–3202.
(<a href="https://doi.org/10.1111/biom.13843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks have been widely used to generate causal hypotheses from multivariate data. Despite their popularity, the vast majority of existing causal discovery approaches make the strong assumption of a (partially) homogeneous sampling scheme. However, such assumption can be seriously violated, causing significant biases when the underlying population is inherently heterogeneous. To this end, we propose a novel causal Bayesian network model, termed BN-LTE, that embeds heterogeneous samples onto a low-dimensional manifold and builds Bayesian networks conditional on the embedding. This new framework allows for more precise network inference by improving the estimation resolution from the population level to the observation level. Moreover, while causal Bayesian networks are in general not identifiable with purely observational, cross-sectional data due to Markov equivalence, with the blessing of causal effect heterogeneity, we prove that the proposed BN-LTE is uniquely identifiable under relatively mild assumptions. Through extensive experiments, we demonstrate the superior performance of BN-LTE in causal structure learning as well as inferring observation-specific gene regulatory networks from observational data.},
  archive      = {J_BIOMTC},
  author       = {Fangting Zhou and Kejun He and Yang Ni},
  doi          = {10.1111/biom.13843},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3191-3202},
  shortjournal = {Biometrics},
  title        = {Individualized causal discovery with latent trajectory embedded bayesian networks},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropy balancing for causal generalization with target
sample summary information. <em>BIOMTC</em>, <em>79</em>(4), 3179–3190.
(<a href="https://doi.org/10.1111/biom.13825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on estimating the average treatment effect (ATE) of a target population when individual-level data from a source population and summary-level data (e.g., first or second moments of certain covariates) from the target population are available. In the presence of the heterogeneous treatment effect, the ATE of the target population can be different from that of the source population when distributions of treatment effect modifiers are dissimilar in these two populations, a phenomenon also known as covariate shift. Many methods have been developed to adjust for covariate shift, but most require individual covariates from a representative target sample. We develop a weighting approach based on the summary-level information from the target sample to adjust for possible covariate shift in effect modifiers. In particular, weights of the treated and control groups within a source sample are calibrated by the summary-level information of the target sample. Our approach also seeks additional covariate balance between the treated and control groups in the source sample. We study the asymptotic behavior of the corresponding weighted estimator for the target population ATE under a wide range of conditions. The theoretical implications are confirmed in simulation studies and a real-data application.},
  archive      = {J_BIOMTC},
  author       = {Rui Chen and Guanhua Chen and Menggang Yu},
  doi          = {10.1111/biom.13825},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3179-3190},
  shortjournal = {Biometrics},
  title        = {Entropy balancing for causal generalization with target sample summary information},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transportability of causal inference under random dynamic
treatment regimes for kidney–pancreas transplantation. <em>BIOMTC</em>,
<em>79</em>(4), 3165–3178. (<a
href="https://doi.org/10.1111/biom.13899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A difficult decision for patients in need of kidney–pancreas transplant is whether to seek a living kidney donor or wait to receive both organs from one deceased donor. The framework of dynamic treatment regimes (DTRs) can inform this choice, but a patient-relevant strategy such as “wait for deceased-donor transplant” is ill-defined because there are multiple versions of treatment (i.e., wait times, organ qualities). Existing DTR methods average over the distribution of treatment versions in the data, estimating survival under a “representative intervention.” This is undesirable if transporting inferences to a target population such as patients today, who experience shorter wait times thanks to evolutions in allocation policy. We, therefore, propose the concept of a generalized representative intervention (GRI): a random DTR that assigns treatment version by drawing from the distribution among strategy compliers in the target population (e.g., patients today). We describe an inverse-probability-weighted product-limit estimator of survival under a GRI that performs well in simulations and can be implemented in standard statistical software. For continuous treatments (e.g., organ quality), weights are reformulated to depend on probabilities only, not densities. We apply our method to a national database of kidney–pancreas transplant candidates from 2001–2020 to illustrate that variability in transplant rate across years and centers results in qualitative differences in the optimal strategy for patient survival.},
  archive      = {J_BIOMTC},
  author       = {Grace R. Lyden and David M. Vock and Erika S. Helgeson and Erik B. Finger and Arthur J. Matas and Jon J. Snyder},
  doi          = {10.1111/biom.13899},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3165-3178},
  shortjournal = {Biometrics},
  title        = {Transportability of causal inference under random dynamic treatment regimes for kidney–pancreas transplantation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prior and posterior checking of implicit causal assumptions.
<em>BIOMTC</em>, <em>79</em>(4), 3153–3164. (<a
href="https://doi.org/10.1111/biom.13886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference practitioners have increasingly adopted machine learning techniques with the aim of producing principled uncertainty quantification for causal effects while minimizing the risk of model misspecification. Bayesian nonparametric approaches have attracted attention as well, both for their flexibility and their promise of providing natural uncertainty quantification. Priors on high-dimensional or nonparametric spaces, however, can often unintentionally encode prior information that is at odds with substantive knowledge in causal inference—specifically, the regularization required for high-dimensional Bayesian models to work can indirectly imply that the magnitude of the confounding is negligible. In this paper, we explain this problem and provide tools for (i) verifying that the prior distribution does not encode an inductive bias away from confounded models and (ii) verifying that the posterior distribution contains sufficient information to overcome this issue if it exists. We provide a proof-of-concept on simulated data from a high-dimensional probit-ridge regression model, and illustrate on a Bayesian nonparametric decision tree ensemble applied to a large medical expenditure survey.},
  archive      = {J_BIOMTC},
  author       = {Antonio R. Linero},
  doi          = {10.1111/biom.13886},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3153-3164},
  shortjournal = {Biometrics},
  title        = {Prior and posterior checking of implicit causal assumptions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved inference for doubly robust estimators of
heterogeneous treatment effects. <em>BIOMTC</em>, <em>79</em>(4),
3140–3152. (<a href="https://doi.org/10.1111/biom.13837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a doubly robust approach to characterizing treatment effect heterogeneity in observational studies. We develop a frequentist inferential procedure that utilizes posterior distributions for both the propensity score and outcome regression models to provide valid inference on the conditional average treatment effect even when high-dimensional or nonparametric models are used. We show that our approach leads to conservative inference in finite samples or under model misspecification and provides a consistent variance estimator when both models are correctly specified. In simulations, we illustrate the utility of these results in difficult settings such as high-dimensional covariate spaces or highly flexible models for the propensity score and outcome regression. Lastly, we analyze environmental exposure data from NHANES to identify how the effects of these exposures vary by subject-level characteristics.},
  archive      = {J_BIOMTC},
  author       = {Heejun Shin and Joseph Antonelli},
  doi          = {10.1111/biom.13837},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3140-3152},
  shortjournal = {Biometrics},
  title        = {Improved inference for doubly robust estimators of heterogeneous treatment effects},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and flexible estimation of natural direct and
indirect effects under intermediate confounding and monotonicity
constraints. <em>BIOMTC</em>, <em>79</em>(4), 3126–3139. (<a
href="https://doi.org/10.1111/biom.13850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural direct and indirect effects are mediational estimands that decompose the average treatment effect and describe how outcomes would be affected by contrasting levels of a treatment through changes induced in mediator values (in the case of the indirect effect) or not through induced changes in the mediator values (in the case of the direct effect). Natural direct and indirect effects are not generally point-identified in the presence of a treatment-induced confounder; however, they may be identified if one is willing to assume monotonicity between the treatment and the treatment-induced confounder. We argue that this assumption may be reasonable in the relatively common encouragement-design trial setting, where the intervention is randomized treatment assignment and the treatment-induced confounder is whether or not treatment was actually taken/adhered to. We develop efficiency theory for the natural direct and indirect effects under this monotonicity assumption, and use it to propose a nonparametric, multiply robust estimator. We demonstrate the finite sample properties of this estimator using a simulation study, and apply it to data from the Moving to Opportunity Study to estimate the natural direct and indirect effects of being randomly assigned to receive a Section 8 housing voucher—the most common form of federal housing assistance—on risk developing any mood or externalizing disorder among adolescent boys, possibly operating through various school and community characteristics.},
  archive      = {J_BIOMTC},
  author       = {Kara E. Rudolph and Nicholas Williams and Iván Díaz},
  doi          = {10.1111/biom.13850},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3126-3139},
  shortjournal = {Biometrics},
  title        = {Efficient and flexible estimation of natural direct and indirect effects under intermediate confounding and monotonicity constraints},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semiparametric cox–aalen transformation model with
censored data. <em>BIOMTC</em>, <em>79</em>(4), 3111–3125. (<a
href="https://doi.org/10.1111/biom.13895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a broad class of so-called Cox–Aalen transformation models that incorporate both multiplicative and additive covariate effects on the baseline hazard function within a transformation. The proposed models provide a highly flexible and versatile class of semiparametric models that include the transformation models and the Cox–Aalen model as special cases. Specifically, it extends the transformation models by allowing potentially time-dependent covariates to work additively on the baseline hazard and extends the Cox–Aalen model through a predetermined transformation function. We propose an estimating equation approach and devise an expectation-solving (ES) algorithm that involves fast and robust calculations. The resulting estimator is shown to be consistent and asymptotically normal via modern empirical process techniques. The ES algorithm yields a computationally simple method for estimating the variance of both parametric and nonparametric estimators. Finally, we demonstrate the performance of our procedures through extensive simulation studies and applications in two randomized, placebo-controlled human immunodeficiency virus (HIV) prevention efficacy trials. The data example shows the utility of the proposed Cox–Aalen transformation models in enhancing statistical power for discovering covariate effects.},
  archive      = {J_BIOMTC},
  author       = {Xi Ning and Yinghao Pan and Yanqing Sun and Peter B. Gilbert},
  doi          = {10.1111/biom.13895},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3111-3125},
  shortjournal = {Biometrics},
  title        = {A semiparametric Cox–Aalen transformation model with censored data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensuring valid inference for cox hazard ratios after
variable selection. <em>BIOMTC</em>, <em>79</em>(4), 3096–3110. (<a
href="https://doi.org/10.1111/biom.13889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of how to best select variables for confounding adjustment forms one of the key challenges in the evaluation of exposure effects in observational studies, and has been the subject of vigorous recent activity in causal inference. A major drawback of routine procedures is that there is no finite sample size at which they are guaranteed to deliver exposure effect estimators and associated confidence intervals with adequate performance. In this work, we will consider this problem when inferring conditional causal hazard ratios from observational studies under the assumption of no unmeasured confounding. The major complication that we face with survival data is that the key confounding variables may not be those that explain the censoring mechanism. In this paper, we overcome this problem using a novel and simple procedure that can be implemented using off-the-shelf software for penalized Cox regression. In particular, we will propose tests of the null hypothesis that the exposure has no effect on the considered survival endpoint, which are uniformly valid under standard sparsity conditions. Simulation results show that the proposed methods yield valid inferences even when covariates are high-dimensional.},
  archive      = {J_BIOMTC},
  author       = {Kelly Van Lancker and Oliver Dukes and Stijn Vansteelandt},
  doi          = {10.1111/biom.13889},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3096-3110},
  shortjournal = {Biometrics},
  title        = {Ensuring valid inference for cox hazard ratios after variable selection},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group variable selection for the cox model with
interval-censored failure time data. <em>BIOMTC</em>, <em>79</em>(4),
3082–3095. (<a href="https://doi.org/10.1111/biom.13879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group variable selection is often required in many areas, and for this many methods have been developed under various situations. Unlike the individual variable selection, the group variable selection can select the variables in groups, and it is more efficient to identify both important and unimportant variables or factors by taking into account the existing group structure. In this paper, we consider the situation where one only observes interval-censored failure time data arising from the Cox model, for which there does not seem to exist an established method. More specifically, a penalized sieve maximum likelihood variable selection and estimation procedure is proposed and the oracle property of the proposed method is established. Also, an extensive simulation study is performed and suggests that the proposed approach works well in practical situations. An application of the method to a set of real data is provided.},
  archive      = {J_BIOMTC},
  author       = {Yuxiang Wu and Hui Zhao and Jianguo Sun},
  doi          = {10.1111/biom.13879},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3082-3095},
  shortjournal = {Biometrics},
  title        = {Group variable selection for the cox model with interval-censored failure time data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated failure time regression model for
illness–death data: A frailty approach. <em>BIOMTC</em>, <em>79</em>(4),
3066–3081. (<a href="https://doi.org/10.1111/biom.13880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a new model and estimation procedure for the illness–death survival data where the hazard functions follow accelerated failure time (AFT) models. A shared frailty variate induces positive dependence among failure times of a subject for handling the unobserved dependency between the nonterminal and the terminal failure times given the observed covariates. The motivation behind the proposed modeling approach is to leverage the well-known interpretability advantage of AFT models with respect to the observed covariates, while also benefiting from the simple and intuitive interpretation of the hazard functions. A semiparametric maximum likelihood estimation procedure is developed via a kernel smoothed-aided expectation-maximization algorithm, and variances are estimated by weighted bootstrap. We consider existing frailty-based illness–death models and place particular emphasis on highlighting the contribution of our current research. The breast cancer data of the Rotterdam tumor bank are analyzed using the proposed as well as existing illness–death models. The results are contrasted and evaluated based on a new graphical goodness-of-fit procedure. Simulation results and data analysis nicely demonstrate the practical utility of the shared frailty variate with the AFT regression model under the illness–death framework.},
  archive      = {J_BIOMTC},
  author       = {Lea Kats and Malka Gorfine},
  doi          = {10.1111/biom.13880},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3066-3081},
  shortjournal = {Biometrics},
  title        = {An accelerated failure time regression model for illness–death data: A frailty approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information criteria for detecting change-points in the cox
proportional hazards model. <em>BIOMTC</em>, <em>79</em>(4), 3050–3065.
(<a href="https://doi.org/10.1111/biom.13855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards model, commonly used in clinical trials, assumes proportional hazards. However, it does not hold when, for example, there is a delayed onset of the treatment effect. In such a situation, an acute change in the hazard ratio function is expected to exist. This paper considers the Cox model with change-points and derives Akaike information criterion (AIC)-type information criteria for detecting those change-points. The change-point model does not allow for conventional statistical asymptotics due to its irregularity, thus a formal AIC that penalizes twice the number of parameters would not be analytically derived, and using it would clearly give overfitting analysis results. Therefore, we will construct specific asymptotics using the partial likelihood estimation method in the Cox model with change-points, and propose information criteria based on the original derivation method for AIC. If the partial likelihood is used in the estimation, information criteria with penalties much larger than twice the number of parameters could be obtained in an explicit form. Numerical experiments confirm that the proposed criteria are clearly superior in terms of the original purpose of AIC, which are to provide an estimate that is close to the true structure. We also apply the proposed criterion to actual clinical trial data to indicate that it will easily lead to different results from the formal AIC.},
  archive      = {J_BIOMTC},
  author       = {Ryoto Ozaki and Yoshiyuki Ninomiya},
  doi          = {10.1111/biom.13855},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3050-3065},
  shortjournal = {Biometrics},
  title        = {Information criteria for detecting change-points in the cox proportional hazards model},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of time-specific intervention effects on
continuously distributed time-to-event outcomes by targeted maximum
likelihood estimation. <em>BIOMTC</em>, <em>79</em>(4), 3038–3049. (<a
href="https://doi.org/10.1111/biom.13856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers targeted maximum likelihood estimation (TMLE) of treatment effects on absolute risk and survival probabilities in classical time-to-event settings characterized by right-censoring and competing risks. TMLE is a general methodology combining flexible ensemble learning and semiparametric efficiency theory in a two-step procedure for substitution estimation of causal parameters. We specialize and extend the continuous-time TMLE methods for competing risks settings, proposing a targeting algorithm that iteratively updates cause-specific hazards to solve the efficient influence curve equation for the target parameter. As part of the work, we further detail and implement the recently proposed highly adaptive lasso estimator for continuous-time conditional hazards with L 1 -penalized Poisson regression. The resulting estimation procedure benefits from relying solely on very mild nonparametric restrictions on the statistical model, thus providing a novel tool for machine-learning-based semiparametric causal inference for continuous-time time-to-event data. We apply the methods to a publicly available dataset on follicular cell lymphoma where subjects are followed over time until disease relapse or death without relapse. The data display important time-varying effects that can be captured by the highly adaptive lasso. In our simulations that are designed to imitate the data, we compare our methods to a similar approach based on random survival forests and to the discrete-time TMLE.},
  archive      = {J_BIOMTC},
  author       = {Helene C. W. Rytgaard and Frank Eriksson and Mark J. van der Laan},
  doi          = {10.1111/biom.13856},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3038-3049},
  shortjournal = {Biometrics},
  title        = {Estimation of time-specific intervention effects on continuously distributed time-to-event outcomes by targeted maximum likelihood estimation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric failure time: Time-to-event machine learning
with heteroskedastic bayesian additive regression trees and low
information omnibus dirichlet process mixtures. <em>BIOMTC</em>,
<em>79</em>(4), 3023–3037. (<a
href="https://doi.org/10.1111/biom.13857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many popular survival models rely on restrictive parametric, or semiparametric, assumptions that could provide erroneous predictions when the effects of covariates are complex. Modern advances in computational hardware have led to an increasing interest in flexible Bayesian nonparametric methods for time-to-event data such as Bayesian additive regression trees (BART). We propose a novel approach that we call nonparametric failure time (NFT) BART in order to increase the flexibility beyond accelerated failure time (AFT) and proportional hazard models. NFT BART has three key features: (1) a BART prior for the mean function of the event time logarithm; (2) a heteroskedastic BART prior to deduce a covariate-dependent variance function; and (3) a flexible nonparametric error distribution using Dirichlet process mixtures (DPM). Our proposed approach widens the scope of hazard shapes including nonproportional hazards, can be scaled up to large sample sizes, naturally provides estimates of uncertainty via the posterior and can be seamlessly employed for variable selection. We provide convenient, user-friendly, computer software that is freely available as a reference implementation. Simulations demonstrate that NFT BART maintains excellent performance for survival prediction especially when AFT assumptions are violated by heteroskedasticity. We illustrate the proposed approach on a study examining predictors for mortality risk in patients undergoing hematopoietic stem cell transplant (HSCT) for blood-borne cancer, where heteroskedasticity and nonproportional hazards are likely present.},
  archive      = {J_BIOMTC},
  author       = {Rodney A. Sparapani and Brent R. Logan and Martin J. Maiers and Purushottam W. Laud and Robert E. McCulloch},
  doi          = {10.1111/biom.13857},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3023-3037},
  shortjournal = {Biometrics},
  title        = {Nonparametric failure time: Time-to-event machine learning with heteroskedastic bayesian additive regression trees and low information omnibus dirichlet process mixtures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Additive subdistribution hazards regression for competing
risks data in case-cohort studies. <em>BIOMTC</em>, <em>79</em>(4),
3010–3022. (<a href="https://doi.org/10.1111/biom.13821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival data analysis, a competing risk is an event whose occurrence precludes or alters the chance of the occurrence of the primary event of interest. In large cohort studies with long-term follow-up, there are often competing risks. Further, if the event of interest is rare in such large studies, the case-cohort study design is widely used to reduce the cost and achieve the same efficiency as a cohort study. The conventional additive hazards modeling for competing risks data in case-cohort studies involves the cause-specific hazard function, under which direct assessment of covariate effects on the cumulative incidence function, or the subdistribution, is not possible. In this paper, we consider an additive hazard model for the subdistribution of a competing risk in case-cohort studies. We propose estimating equations based on inverse probability weighting methods for the estimation of the model parameters. Consistency and asymptotic normality of the proposed estimators are established. The performance of the proposed methods in finite samples is examined through simulation studies and the proposed approach is applied to a case-cohort dataset from the Sister Study.},
  archive      = {J_BIOMTC},
  author       = {Adane F. Wogu and Haolin Li and Shanshan Zhao and Hazel B. Nichols and Jianwen Cai},
  doi          = {10.1111/biom.13821},
  journal      = {Biometrics},
  number       = {4},
  pages        = {3010-3022},
  shortjournal = {Biometrics},
  title        = {Additive subdistribution hazards regression for competing risks data in case-cohort studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying and estimating effects of sustained
interventions under parallel trends assumptions. <em>BIOMTC</em>,
<em>79</em>(4), 2998–3009. (<a
href="https://doi.org/10.1111/biom.13862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research questions in public health and medicine concern sustained interventions in populations defined by substantive priorities. Existing methods to answer such questions typically require a measured covariate set sufficient to control confounding, which can be questionable in observational studies. Differences-in-differences rely instead on the parallel trends assumption, allowing for some types of time-invariant unmeasured confounding. However, most existing difference-in-differences implementations are limited to point treatments in restricted subpopulations. We derive identification results for population effects of sustained treatments under parallel trends assumptions. In particular, in settings where all individuals begin follow-up with exposure status consistent with the treatment plan of interest but may deviate at later times, a version of Robins&#39; g-formula identifies the intervention-specific mean under stable unit treatment value assumption, positivity, and parallel trends. We develop consistent asymptotically normal estimators based on inverse-probability weighting, outcome regression, and a double robust estimator based on targeted maximum likelihood. Simulation studies confirm theoretical results and support the use of the proposed estimators at realistic sample sizes. As an example, the methods are used to estimate the effect of a hypothetical federal stay-at-home order on all-cause mortality during the COVID-19 pandemic in spring 2020 in the United States.},
  archive      = {J_BIOMTC},
  author       = {Audrey Renson and Michael G. Hudgens and Alexander P. Keil and Paul N. Zivich and Allison E. Aiello},
  doi          = {10.1111/biom.13862},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2998-3009},
  shortjournal = {Biometrics},
  title        = {Identifying and estimating effects of sustained interventions under parallel trends assumptions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining transmission rate variations and forecasting
epidemic spread in multiple regions with a semiparametric mixed effects
SIR model. <em>BIOMTC</em>, <em>79</em>(4), 2987–2997. (<a
href="https://doi.org/10.1111/biom.13901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transmission rate is a central parameter in mathematical models of infectious disease. Its pivotal role in outbreak dynamics makes estimating the current transmission rate and uncovering its dependence on relevant covariates a core challenge in epidemiological research as well as public health policy evaluation. Here, we develop a method for flexibly inferring a time-varying transmission rate parameter, modeled as a function of covariates and a smooth Gaussian process (GP). The transmission rate model is further embedded in a hierarchy to allow information borrowing across parallel streams of regional incidence data. Crucially, the method makes use of optional vaccination data as a first step toward modeling of endemic infectious diseases. Computational techniques borrowed from the Bayesian spatial analysis literature enable fast and reliable posterior computation. Simulation studies reveal that the method recovers true covariate effects at nominal coverage levels. We analyze data from the COVID-19 pandemic and validate forecast intervals on held-out data. User-friendly software is provided to enable practitioners to easily deploy the method in public health research.},
  archive      = {J_BIOMTC},
  author       = {David A. Buch and James E. Johndrow and David B. Dunson},
  doi          = {10.1111/biom.13901},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2987-2997},
  shortjournal = {Biometrics},
  title        = {Explaining transmission rate variations and forecasting epidemic spread in multiple regions with a semiparametric mixed effects SIR model},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal sampling for positive only electronic health record
data. <em>BIOMTC</em>, <em>79</em>(4), 2974–2986. (<a
href="https://doi.org/10.1111/biom.13824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying a patient&#39;s disease/health status from electronic medical records is a frequently encountered task in electronic health records (EHR) related research, and estimation of a classification model often requires a benchmark training data with patients&#39; known phenotype statuses. However, assessing a patient&#39;s phenotype is costly and labor intensive, hence a proper selection of EHR records as a training set is desired. We propose a procedure to tailor the best training subsample with limited sample size for a classification model, minimizing its mean-squared phenotyping/classification error (MSE). Our approach incorporates “positive only” information, an approximation of the true disease status without false alarm, when it is available. In addition, our sampling procedure is applicable for training a chosen classification model which can be misspecified. We provide theoretical justification on its optimality in terms of MSE. The performance gain from our method is illustrated through simulation and a real-data example, and is found often satisfactory under criteria beyond MSE.},
  archive      = {J_BIOMTC},
  author       = {Seong-H. Lee and Yanyuan Ma and Ying Wei and Jinbo Chen},
  doi          = {10.1111/biom.13824},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2974-2986},
  shortjournal = {Biometrics},
  title        = {Optimal sampling for positive only electronic health record data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining observational and experimental datasets using
shrinkage estimators. <em>BIOMTC</em>, <em>79</em>(4), 2961–2973. (<a
href="https://doi.org/10.1111/biom.13827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of combining data from observational and experimental sources to draw causal conclusions. To derive combined estimators with desirable properties, we extend results from the Stein shrinkage literature. Our contributions are threefold. First, we propose a generic procedure for deriving shrinkage estimators in this setting, making use of a generalized unbiased risk estimate. Second, we develop two new estimators, prove finite sample conditions under which they have lower risk than an estimator using only experimental data, and show that each achieves a notion of asymptotic optimality. Third, we draw connections between our approach and results in sensitivity analysis, including proposing a method for evaluating the feasibility of our estimators.},
  archive      = {J_BIOMTC},
  author       = {Evan T.R. Rosenman and Guillaume Basse and Art B. Owen and Mike Baiocchi},
  doi          = {10.1111/biom.13827},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2961-2973},
  shortjournal = {Biometrics},
  title        = {Combining observational and experimental datasets using shrinkage estimators},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient data integration scheme for synthesizing
information from multiple secondary datasets for the parameter inference
of the main analysis. <em>BIOMTC</em>, <em>79</em>(4), 2947–2960. (<a
href="https://doi.org/10.1111/biom.13858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many observational studies and clinical trials collect various secondary outcomes that may be highly correlated with the primary endpoint. These secondary outcomes are often analyzed in secondary analyses separately from the main data analysis. However, these secondary outcomes can be used to improve the estimation precision in the main analysis. We propose a method called multiple information borrowing (MinBo) that borrows information from secondary data (containing secondary outcomes and covariates) to improve the efficiency of the main analysis. The proposed method is robust against model misspecification of the secondary data. Both theoretical and case studies demonstrate that MinBo outperforms existing methods in terms of efficiency gain. We apply MinBo to data from the Atherosclerosis Risk in Communities study to assess risk factors for hypertension.},
  archive      = {J_BIOMTC},
  author       = {Chixiang Chen and Ming Wang and Shuo Chen},
  doi          = {10.1111/biom.13858},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2947-2960},
  shortjournal = {Biometrics},
  title        = {An efficient data integration scheme for synthesizing information from multiple secondary datasets for the parameter inference of the main analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical nuclear norm penalization for multi-view data
integration. <em>BIOMTC</em>, <em>79</em>(4), 2933–2946. (<a
href="https://doi.org/10.1111/biom.13893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of data collected on the same set of samples from multiple sources (i.e., multi-view data) has prompted significant development of data integration methods based on low-rank matrix factorizations. These methods decompose signal matrices from each view into the sum of shared and individual structures, which are further used for dimension reduction, exploratory analyses, and quantifying associations across views. However, existing methods have limitations in modeling partially-shared structures due to either too restrictive models, or restrictive identifiability conditions. To address these challenges, we propose a new formulation for signal structures that include partially-shared signals based on grouping the views into so-called hierarchical levels with identifiable guarantees under suitable conditions. The proposed hierarchy leads us to introduce a new penalty, hierarchical nuclear norm (HNN), for signal estimation. In contrast to existing methods, HNN penalization avoids scores and loadings factorization of the signals and leads to a convex optimization problem, which we solve using a dual forward–backward algorithm. We propose a simple refitting procedure to adjust the penalization bias and develop an adapted version of bi-cross-validation for selecting tuning parameters. Extensive simulation studies and analysis of the genotype-tissue expression data demonstrate the advantages of our method over existing alternatives.},
  archive      = {J_BIOMTC},
  author       = {Sangyoon Yi and Raymond Ka Wai Wong and Irina Gaynanova},
  doi          = {10.1111/biom.13893},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2933-2946},
  shortjournal = {Biometrics},
  title        = {Hierarchical nuclear norm penalization for multi-view data integration},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relative contrast estimation and inference for treatment
recommendation. <em>BIOMTC</em>, <em>79</em>(4), 2920–2932. (<a
href="https://doi.org/10.1111/biom.13826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When there are resource constraints, it may be necessary to rank individualized treatment benefits to facilitate the prioritization of assigning different treatments. Most existing literature on individualized treatment rules targets absolute conditional treatment effect differences as a metric for the benefit. However, there can be settings where relative differences may better represent such benefit. In this paper, we consider modeling such relative differences formed as scale-invariant contrasts between the conditional treatment effects. By showing that all scale-invariant contrasts are monotonic transformations of each other, we posit a single index model for a particular relative contrast. We then characterize semiparametric estimating equations, including the efficient score, to estimate index parameters. To achieve semiparametric efficiency, we propose a two-step approach that minimizes a doubly robust loss function for initial estimation and then performs a one-step efficiency augmentation procedure. Careful theoretical and numerical studies are provided to show the superiority of our proposed approach.},
  archive      = {J_BIOMTC},
  author       = {Muxuan Liang and Menggang Yu},
  doi          = {10.1111/biom.13826},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2920-2932},
  shortjournal = {Biometrics},
  title        = {Relative contrast estimation and inference for treatment recommendation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DROID: Dose-ranging approach to optimizing dose in oncology
drug development. <em>BIOMTC</em>, <em>79</em>(4), 2907–2919. (<a
href="https://doi.org/10.1111/biom.13840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of targeted therapy, there has been increasing concern about the development of oncology drugs based on the “more is better” paradigm, developed decades ago for chemotherapy. Recently, the US Food and Drug Administration (FDA) initiated Project Optimus to reform the dose optimization and dose selection paradigm in oncology drug development. To accommodate this paradigm shifting, we propose a d ose- r anging approach to o ptim i zing d ose (DROID) for oncology trials with targeted drugs. DROID leverages the well-established dose-ranging study framework, which has been routinely used to develop non-oncology drugs for decades, and bridges it with established oncology dose-finding designs to optimize the dose of oncology drugs. DROID consists of two seamlessly connected stages. In the first stage, patients are sequentially enrolled and adaptively assigned to investigational doses to establish the therapeutic dose range (TDR), defined as the range of doses with acceptable toxicity and efficacy profiles, and the recommended phase 2 dose set (RP2S). In the second stage, patients are randomized to the doses in RP2S to assess the dose–response relationship and identify the optimal dose. The simulation study shows that DROID substantially outperforms the conventional approach, providing a new paradigm to efficiently optimize the dose of targeted oncology drugs. DROID aligns with the approach of a randomized, parallel dose-response trial design recommended by the FDA in the Guidance on Optimizing the Dosage of Human Prescription Drugs and Biological Products for the Treatment of Oncologic Diseases.},
  archive      = {J_BIOMTC},
  author       = {Beibei Guo and Ying Yuan},
  doi          = {10.1111/biom.13840},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2907-2919},
  shortjournal = {Biometrics},
  title        = {DROID: Dose-ranging approach to optimizing dose in oncology drug development},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate-adjusted response-adaptive designs based on
semiparametric approaches. <em>BIOMTC</em>, <em>79</em>(4), 2895–2906.
(<a href="https://doi.org/10.1111/biom.13849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider theoretical and practical issues for innovatively using a large number of covariates in clinical trials to achieve various design objectives without model misspecification. Specifically, we propose a new family of semiparametric covariate-adjusted response-adaptive randomization (CARA) designs and we use the target maximum likelihood estimation (TMLE) to analyze the correlated data from CARA designs. Our approach can flexibly achieve multiple objectives and correctly incorporate the effect of a large number of covariates on the responses without model misspecification. We also obtain the consistency and asymptotic normality of the target parameters, allocation probabilities, and allocation proportions. Numerical studies demonstrate that our approach has advantages over existing approaches, even when the data-generating distribution is complicated.},
  archive      = {J_BIOMTC},
  author       = {Hai Zhu and Hongjian Zhu},
  doi          = {10.1111/biom.13849},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2895-2906},
  shortjournal = {Biometrics},
  title        = {Covariate-adjusted response-adaptive designs based on semiparametric approaches},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interim monitoring of sequential multiple assignment
randomized trials using partial information. <em>BIOMTC</em>,
<em>79</em>(4), 2881–2894. (<a
href="https://doi.org/10.1111/biom.13854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequential multiple assignment randomized trial (SMART) is the gold standard trial design to generate data for the evaluation of multistage treatment regimes. As with conventional (single-stage) randomized clinical trials, interim monitoring allows early stopping; however, there are few methods for principled interim analysis in SMARTs. Because SMARTs involve multiple stages of treatment, a key challenge is that not all enrolled participants will have progressed through all treatment stages at the time of an interim analysis. Wu et al. (2021) propose basing interim analyses on an estimator for the mean outcome under a given regime that uses data only from participants who have completed all treatment stages. We propose an estimator for the mean outcome under a given regime that gains efficiency by using partial information from enrolled participants regardless of their progression through treatment stages. Using the asymptotic distribution of this estimator, we derive associated Pocock and O&#39;Brien-Fleming testing procedures for early stopping. In simulation experiments, the estimator controls type I error and achieves nominal power while reducing expected sample size relative to the method of Wu et al. (2021). We present an illustrative application of the proposed estimator based on a recent SMART evaluating behavioral pain interventions for breast cancer patients.},
  archive      = {J_BIOMTC},
  author       = {Cole Manschot and Eric Laber and Marie Davidian},
  doi          = {10.1111/biom.13854},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2881-2894},
  shortjournal = {Biometrics},
  title        = {Interim monitoring of sequential multiple assignment randomized trials using partial information},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression-based multiple treatment effect estimation under
covariate-adaptive randomization. <em>BIOMTC</em>, <em>79</em>(4),
2869–2880. (<a href="https://doi.org/10.1111/biom.13925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization methods are widely used in clinical trials to balance baseline covariates. Recent studies have shown the validity of using regression-based estimators for treatment effects without imposing functional form requirements on the true data generation model. These studies have had limitations in certain scenarios; for example, in the case of multiple treatment groups, these studies did not consider additional covariates or assumed that the allocation ratios were the same across strata. To address these limitations, we develop a stratum-common estimator and a stratum-specific estimator under multiple treatments. We derive the asymptotic behaviors of these estimators and propose consistent nonparametric estimators for asymptotic variances. To determine their efficiency, we compare the estimators with the stratified difference-in-means estimator as the benchmark. We find that the stratum-specific estimator guarantees efficiency gains, regardless of whether the allocation ratios across strata are the same or different. Our conclusions were also validated by simulation studies and a real clinical trial example.},
  archive      = {J_BIOMTC},
  author       = {Yujia Gu and Hanzhong Liu and Wei Ma},
  doi          = {10.1111/biom.13925},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2869-2880},
  shortjournal = {Biometrics},
  title        = {Regression-based multiple treatment effect estimation under covariate-adaptive randomization},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAM: Self-adapting mixture prior to dynamically borrow
information from historical data in clinical trials. <em>BIOMTC</em>,
<em>79</em>(4), 2857–2868. (<a
href="https://doi.org/10.1111/biom.13927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture priors provide an intuitive way to incorporate historical data while accounting for potential prior-data conflict by combining an informative prior with a noninformative prior. However, prespecifying the mixing weight for each component remains a crucial challenge. Ideally, the mixing weight should reflect the degree of prior-data conflict, which is often unknown beforehand, posing a significant obstacle to the application and acceptance of mixture priors. To address this challenge, we introduce self-adapting mixture (SAM) priors that determine the mixing weight using likelihood ratio test statistics or Bayes factors. SAM priors are data-driven and self-adapting, favoring the informative (noninformative) prior component when there is little (substantial) evidence of prior-data conflict. Consequently, SAM priors achieve dynamic information borrowing. We demonstrate that SAM priors exhibit desirable properties in both finite and large samples and achieve information-borrowing consistency. Moreover, SAM priors are easy to compute, data-driven, and calibration-free, mitigating the risk of data dredging. Numerical studies show that SAM priors outperform existing methods in adopting prior-data conflicts effectively. We developed R package “SAMprior” and web application that are freely available at CRAN and www.trialdesign.org to facilitate the use of SAM priors.},
  archive      = {J_BIOMTC},
  author       = {Peng Yang and Yuansong Zhao and Lei Nie and Jonathon Vallejo and Ying Yuan},
  doi          = {10.1111/biom.13927},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2857-2868},
  shortjournal = {Biometrics},
  title        = {SAM: Self-adapting mixture prior to dynamically borrow information from historical data in clinical trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stabilized direct learning for efficient estimation of
individualized treatment rules. <em>BIOMTC</em>, <em>79</em>(4),
2843–2856. (<a href="https://doi.org/10.1111/biom.13818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the field of precision medicine has seen many advancements. Significant focus has been placed on creating algorithms to estimate individualized treatment rules (ITRs), which map from patient covariates to the space of available treatments with the goal of maximizing patient outcome. Direct learning (D-Learning) is a recent one-step method which estimates the ITR by directly modeling the treatment–covariate interaction. However, when the variance of the outcome is heterogeneous with respect to treatment and covariates, D-Learning does not leverage this structure. Stabilized direct learning (SD-Learning), proposed in this paper, utilizes potential heteroscedasticity in the error term through a residual reweighting which models the residual variance via flexible machine learning algorithms such as XGBoost and random forests. We also develop an internal cross-validation scheme which determines the best residual model among competing models. SD-Learning improves the efficiency of D-Learning estimates in binary and multi-arm treatment scenarios. The method is simple to implement and an easy way to improve existing algorithms within the D-Learning family, including original D-Learning, Angle-based D-Learning (AD-Learning), and Robust D-learning (RD-Learning). We provide theoretical properties and justification of the optimality of SD-Learning. Head-to-head performance comparisons with D-Learning methods are provided through simulations, which demonstrate improvement in terms of average prediction error (APE), misclassification rate, and empirical value, along with a data analysis of an acquired immunodeficiency syndrome (AIDS) randomized clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Kushal S. Shah and Haoda Fu and Michael R. Kosorok},
  doi          = {10.1111/biom.13818},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2843-2856},
  shortjournal = {Biometrics},
  title        = {Stabilized direct learning for efficient estimation of individualized treatment rules},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating optimal individualized treatment rules with
multistate processes. <em>BIOMTC</em>, <em>79</em>(4), 2830–2842. (<a
href="https://doi.org/10.1111/biom.13864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate process data are common in studies of chronic diseases such as cancer. These data are ideal for precision medicine purposes as they can be leveraged to improve more refined health outcomes, compared to standard survival outcomes, as well as incorporate patient preferences regarding quantity versus quality of life. However, there are currently no methods for the estimation of optimal individualized treatment rules with such data. In this paper, we propose a nonparametric outcome weighted learning approach for this problem in randomized clinical trial settings. The theoretical properties of the proposed methods, including Fisher consistency and asymptotic normality of the estimated expected outcome under the estimated optimal individualized treatment rule, are rigorously established. A consistent closed-form variance estimator is provided and methodology for the calculation of simultaneous confidence intervals is proposed. Simulation studies show that the proposed methodology and inference procedures work well even with small-sample sizes and high rates of right censoring. The methodology is illustrated using data from a randomized clinical trial on the treatment of metastatic squamous-cell carcinoma of the head and neck.},
  archive      = {J_BIOMTC},
  author       = {Giorgos Bakoyannis},
  doi          = {10.1111/biom.13864},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2830-2842},
  shortjournal = {Biometrics},
  title        = {Estimating optimal individualized treatment rules with multistate processes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing treatment allocation in randomized clinical
trials by leveraging baseline covariates. <em>BIOMTC</em>,
<em>79</em>(4), 2815–2829. (<a
href="https://doi.org/10.1111/biom.13914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of optimizing treatment allocation for statistical efficiency in randomized clinical trials. Optimal allocation has been studied previously for simple treatment effect estimators such as the sample mean difference, which are not fully efficient in the presence of baseline covariates. More efficient estimators can be obtained by incorporating covariate information, and modern machine learning methods make it increasingly feasible to approach full efficiency. Accordingly, we derive the optimal allocation ratio by maximizing the design efficiency of a randomized trial, assuming that an efficient estimator will be used for analysis. We then expand the scope of optimization by considering covariate-dependent randomization (CDR), which has some flavor of an observational study but provides the same level of scientific rigor as a standard randomized trial. We describe treatment effect estimators that are consistent, asymptotically normal, and (nearly) efficient under CDR, and derive the optimal propensity score by maximizing the design efficiency of a CDR trial (under the assumption that an efficient estimator will be used for analysis). Our optimality results translate into optimal designs that improve upon standard practice. Real-world examples and simulation results demonstrate that the proposed designs can produce substantial efficiency improvements in realistic settings.},
  archive      = {J_BIOMTC},
  author       = {Wei Zhang and Zhiwei Zhang and Aiyi Liu},
  doi          = {10.1111/biom.13914},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2815-2829},
  shortjournal = {Biometrics},
  title        = {Optimizing treatment allocation in randomized clinical trials by leveraging baseline covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder to discussions on “optimal test procedures for
multiple hypotheses controlling the familywise expected loss.”
<em>BIOMTC</em>, <em>79</em>(4), 2811–2814. (<a
href="https://doi.org/10.1111/biom.13905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Willi Maurer and Frank Bretz and Xiaolei Xun},
  doi          = {10.1111/biom.13905},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2811-2814},
  shortjournal = {Biometrics},
  title        = {Rejoinder to discussions on “Optimal test procedures for multiple hypotheses controlling the familywise expected loss”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “optimal test procedures for multiple
hypotheses controlling the familywise expected loss” by willi maurer,
frank bretz, and xiaolei xun. <em>BIOMTC</em>, <em>79</em>(4),
2806–2810. (<a href="https://doi.org/10.1111/biom.13909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This comment builds on the familywise expected loss (FWEL) framework suggested by Maurer, Bretz, and Xun in 2022. By representing the populationwise error rate (PWER) as FWEL, it is illustrated how the FWEL framework can be extended to clinical trials with multiple and overlapping populations and the PWER can be generalized to more general losses. The comment also addresses the question of how to deal with midtrial changes in the posttrial risks and related losses that are caused by data-driven decisions. Focusing on multiarm trials with the possibility of dropping treatments midtrial, we suggest to switch from control of the unconditional expected loss to control of the conditional expected loss that is related to the actual risks and is conditional on the sample event that causes the change in the risks. The problem and here suggested solution is also motivated with a sequence of independent trials for a hitherto incurable disease which ends when an efficient treatment is found. No multiplicity adjustment is applied in this case and we show how this can be justified by the consideration of the changing out-trial risks and with control of conditional type I error rates and losses.},
  archive      = {J_BIOMTC},
  author       = {Werner Brannath},
  doi          = {10.1111/biom.13909},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2806-2810},
  shortjournal = {Biometrics},
  title        = {Discussion on “Optimal test procedures for multiple hypotheses controlling the familywise expected loss” by willi maurer, frank bretz, and xiaolei xun},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “optimal test procedures for multiple
hypotheses controlling the familywise expected loss” by willi maurer,
frank bretz, and xiaolei xun. <em>BIOMTC</em>, <em>79</em>(4),
2802–2805. (<a href="https://doi.org/10.1111/biom.13910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide commentary on the paper by Willi Maurer, Frank Bretz, and Xiaolei Xun entitled, “Optimal test procedures for multiple hypotheses controlling for the familywise expected loss.” The authors provide an excellent discussion of the multiplicity problem in clinical trials and propose a novel approach based on a decision-theoretic framework that incorporates loss functions that can vary across multiple hypotheses in a family. We provide some considerations for the practical use of the authors&#39; proposed methods as well as some alternative methods that may also be of interest in this setting.},
  archive      = {J_BIOMTC},
  author       = {L.M. LaVange and E.M. Alt and J.G. Ibrahim},
  doi          = {10.1111/biom.13910},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2802-2805},
  shortjournal = {Biometrics},
  title        = {Discussion of “Optimal test procedures for multiple hypotheses controlling the familywise expected loss” by willi maurer, frank bretz, and xiaolei xun},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “optimal test procedures for multiple
hypotheses controlling the familywise expected loss” by willi maurer,
frank bretz, and xiaolei xun. <em>BIOMTC</em>, <em>79</em>(4),
2798–2801. (<a href="https://doi.org/10.1111/biom.13908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sudipto Banerjee},
  doi          = {10.1111/biom.13908},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2798-2801},
  shortjournal = {Biometrics},
  title        = {Discussion of “Optimal test procedures for multiple hypotheses controlling the familywise expected loss” by willi maurer, frank bretz, and xiaolei xun},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “optimal test procedures for multiple
hypotheses controlling the familywise expected loss” by willi maurer,
frank bretz, and xiaolei xun. <em>BIOMTC</em>, <em>79</em>(4),
2794–2797. (<a href="https://doi.org/10.1111/biom.13906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss three issues. In the first part, we discuss the criteria emphasized by Maurer, Bretz, and Xun, warning that it modifies the per comparison error rate that does not address the concerns raised by multiple testing. In the second part, we strengthen the optimality results developed in the paper, based on our recent results. In the third part, we highlight the potentially important role that the use of weights may have in practice and discuss the difficulties in assigning weights that convey the importance in the gain and loss functions, especially as it pertains to multiple endpoints.},
  archive      = {J_BIOMTC},
  author       = {Yoav Benjamini and Ruth Heller and Abba Krieger and Saharon Rosset},
  doi          = {10.1111/biom.13906},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2794-2797},
  shortjournal = {Biometrics},
  title        = {Discussion on “Optimal test procedures for multiple hypotheses controlling the familywise expected loss” by willi maurer, frank bretz, and xiaolei xun},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Optimal test procedures for multiple hypotheses controlling
the familywise expected loss. <em>BIOMTC</em>, <em>79</em>(4),
2781–2793. (<a href="https://doi.org/10.1111/biom.13907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of testing multiple null hypotheses, where a decision to reject or retain must be made for each one and embedding incorrect decisions into a real-life context may inflict different losses. We argue that traditional methods controlling the Type I error rate may be too restrictive in this situation and that the standard familywise error rate may not be appropriate. Using a decision-theoretic approach, we define suitable loss functions for a given decision rule, where incorrect decisions can be treated unequally by assigning different loss values. Taking expectation with respect to the sampling distribution of the data allows us to control the familywise expected loss instead of the conventional familywise error rate. Different loss functions can be adopted, and we search for decision rules that satisfy certain optimality criteria within a broad class of decision rules for which the expected loss is bounded by a fixed threshold under any parameter configuration. We illustrate the methods with the problem of establishing efficacy of a new medicinal treatment in non-overlapping subgroups of patients.},
  archive      = {J_BIOMTC},
  author       = {Willi Maurer and Frank Bretz and Xiaolei Xun},
  doi          = {10.1111/biom.13907},
  journal      = {Biometrics},
  number       = {4},
  pages        = {2781-2793},
  shortjournal = {Biometrics},
  title        = {Optimal test procedures for multiple hypotheses controlling the familywise expected loss},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fundamentals of high-dimensional statistics: With exercises
and r labs by johannes lederer, springer international publishing, 2021.
Pp. 355. ISBN: 978-3-030-73791-7. <em>BIOMTC</em>, <em>79</em>(3),
2772–2773. (<a href="https://doi.org/10.1111/biom.13884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13884},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2772-2773},
  shortjournal = {Biometrics},
  title        = {Fundamentals of high-dimensional statistics: with exercises and r labs by johannes lederer, springer international publishing, 2021. pp. 355. ISBN: 978-3-030-73791-7},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mendelian randomization: Methods for causal inference using
genetic variants 2nd edition by stephen burgess and simon g. Thompson.
New york: Chapman &amp; hall. Https://doi.org/10.1201/9780429324352.
<em>BIOMTC</em>, <em>79</em>(3), 2771–2772. (<a
href="https://doi.org/10.1111/biom.13874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chia-Yen Chen},
  doi          = {10.1111/biom.13874},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2771-2772},
  shortjournal = {Biometrics},
  title        = {Mendelian randomization: methods for causal inference using genetic variants 2nd edition by stephen burgess and simon g. thompson. new york: chapman &amp; hall. https://doi.org/10.1201/9780429324352},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probability and random variables: Theory and applications by
iickho song, so ryoung park, seokho yoon (2022). Springer cham. ISBN:
978-3-030-97678-1; 978-3-030-97679-8 (eBook).
Https://doi.org/10.1007/978-3-030-97679-8. <em>BIOMTC</em>,
<em>79</em>(3), 2770. (<a
href="https://doi.org/10.1111/biom.13875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen-Po Liao},
  doi          = {10.1111/biom.13875},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2770},
  shortjournal = {Biometrics},
  title        = {Probability and random variables: theory and applications by iickho song, so ryoung park, seokho yoon (2022). springer cham. ISBN: 978-3-030-97678-1; 978-3-030-97679-8 (eBook). https://doi.org/10.1007/978-3-030-97679-8},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hospital profiling using bayesian decision theory.
<em>BIOMTC</em>, <em>79</em>(3), 2757–2769. (<a
href="https://doi.org/10.1111/biom.13798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For evaluating the quality of care provided by hospitals, special interest lies in the identification of performance outliers. The classification of healthcare providers as outliers or non-outliers is a decision under uncertainty, because the true quality is unknown and can only be inferred from an observed result of a quality indicator. We propose to embed the classification of healthcare providers into a Bayesian decision theoretical framework that enables the derivation of optimal decision rules with respect to the expected decision consequences. We propose paradigmatic utility functions for two typical purposes of hospital profiling: the external reporting of healthcare quality and the initiation of change in care delivery . We make use of funnel plots to illustrate and compare the resulting optimal decision rules and argue that sensitivity and specificity of the resulting decision rules should be analyzed. We then apply the proposed methodology to the area of hip replacement surgeries by analyzing data from 1,277 hospitals in Germany which performed over 180,000 such procedures in 2017. Our setting illustrates that the classification of outliers can be highly dependent upon the underlying utilities. We conclude that analyzing the classification of hospitals as a decision theoretic problem helps to derive transparent and justifiable decision rules. The methodology for classifying quality indicator results is implemented in an R package (iqtigbdt) and is available on GitHub.},
  archive      = {J_BIOMTC},
  author       = {Johannes Hengelbrock and Johannes Rauh and Jona Cederbaum and Maximilian Kähler and Michael Höhle},
  doi          = {10.1111/biom.13798},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2757-2769},
  shortjournal = {Biometrics},
  title        = {Hospital profiling using bayesian decision theory},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sensitivity analysis approach for the causal hazard ratio
in randomized and observational studies. <em>BIOMTC</em>,
<em>79</em>(3), 2743–2756. (<a
href="https://doi.org/10.1111/biom.13797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hazard ratio (HR) is often reported as the main causal effect when studying survival data. Despite its popularity, the HR suffers from an unclear causal interpretation. As already pointed out in the literature, there is a built-in selection bias in the HR, because similarly to the truncation by death problem, the HR conditions on post-treatment survival. A recently proposed alternative, inspired by the Survivor Average Causal Effect, is the causal HR, defined as the ratio between hazards across treatment groups among the study participants that would have survived regardless of their treatment assignment. We discuss the challenge in identifying the causal HR and present a sensitivity analysis identification approach in randomized controlled trials utilizing a working frailty model. We further extend our framework to adjust for potential confounders using inverse probability of treatment weighting. We present a Cox-based and a flexible non-parametric kernel-based estimation under right censoring. We study the finite-sample properties of the proposed estimation methods through simulations. We illustrate the utility of our framework using two real-data examples.},
  archive      = {J_BIOMTC},
  author       = {Rachel Axelrod and Daniel Nevo},
  doi          = {10.1111/biom.13797},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2743-2756},
  shortjournal = {Biometrics},
  title        = {A sensitivity analysis approach for the causal hazard ratio in randomized and observational studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent multinomial models for extended batch-mark data.
<em>BIOMTC</em>, <em>79</em>(3), 2732–2742. (<a
href="https://doi.org/10.1111/biom.13789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch marking is common and useful for many capture–recapture studies where individual marks cannot be applied due to various constraints such as timing, cost, or marking difficulty. When batch marks are used, observed data are not individual capture histories but a set of counts including the numbers of individuals first marked, marked individuals that are recaptured, and individuals captured but released without being marked (applicable to some studies) on each capture occasion. Fitting traditional capture–recapture models to such data requires one to identify all possible sets of capture–recapture histories that may lead to the observed data, which is computationally infeasible even for a small number of capture occasions. In this paper, we propose a latent multinomial model to deal with such data, where the observed vector of counts is a non-invertible linear transformation of a latent vector that follows a multinomial distribution depending on model parameters. The latent multinomial model can be fitted efficiently through a saddlepoint approximation based maximum likelihood approach. The model framework is very flexible and can be applied to data collected with different study designs. Simulation studies indicate that reliable estimation results are obtained for all parameters of the proposed model. We apply the model to analysis of golden mantella data collected using batch marks in Central Madagascar.},
  archive      = {J_BIOMTC},
  author       = {Wei Zhang and Simon J. Bonner and Rachel S. McCrea},
  doi          = {10.1111/biom.13789},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2732-2742},
  shortjournal = {Biometrics},
  title        = {Latent multinomial models for extended batch-mark data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pattern-based clustering of daily weigh-in trajectories
using dynamic time warping. <em>BIOMTC</em>, <em>79</em>(3), 2719–2731.
(<a href="https://doi.org/10.1111/biom.13773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Smart”-scales are a new tool for frequent monitoring of weight change as well as weigh-in behavior. These scales give researchers the opportunity to discover patterns in the frequency that individuals weigh themselves over time, and how these patterns are associated with overall weight loss. Our motivating data come from an 18-month behavioral weight loss study of 55 adults classified as overweight or obese who were instructed to weigh themselves daily. Adherence to daily weigh-in routines produces a binary times series for each subject, indicating whether a participant weighed in on a given day. To characterize weigh-in by time-invariant patterns rather than overall adherence, we propose using hierarchical clustering with dynamic time warping (DTW). We perform an extensive simulation study to evaluate the performance of DTW compared to Euclidean and Jaccard distances to recover underlying patterns in adherence time series. In addition, we compare cluster performance using cluster validation indices (CVIs) under the single, average, complete, and Ward linkages and evaluate how internal and external CVIs compare for clustering binary time series. We apply conclusions from the simulation to cluster our real data and summarize observed weigh-in patterns. Our analysis finds that the adherence trajectory pattern is significantly associated with weight loss.},
  archive      = {J_BIOMTC},
  author       = {Samantha Bothwell and Alex Kaizer and Ryan Peterson and Danielle Ostendorf and Victoria Catenacci and Julia Wrobel},
  doi          = {10.1111/biom.13773},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2719-2731},
  shortjournal = {Biometrics},
  title        = {Pattern-based clustering of daily weigh-in trajectories using dynamic time warping},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Associating somatic mutation with clinical outcomes through
kernel regression and optimal transport. <em>BIOMTC</em>,
<em>79</em>(3), 2705–2718. (<a
href="https://doi.org/10.1111/biom.13769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Somatic mutations in cancer patients are inherently sparse and potentially high dimensional. Cancer patients may share the same set of deregulated biological processes perturbed by different sets of somatically mutated genes. Therefore, when assessing the associations between somatic mutations and clinical outcomes, gene-by-gene analysis is often under-powered because it does not capture the complex disease mechanisms shared across cancer patients. Rather than testing genes one by one, an intuitive approach is to aggregate somatic mutation data of multiple genes to assess their joint association with clinical outcomes. The challenge is how to aggregate such information. Building on the optimal transport method, we propose a principled approach to estimate the similarity of somatic mutation profiles of multiple genes between tumor samples, while accounting for gene–gene similarities defined by gene annotations or empirical mutational patterns. Using such similarities, we can assess the associations between somatic mutations and clinical outcomes by kernel regression. We have applied our method to analyze somatic mutation data of 17 cancer types and identified at least five cancer types, where somatic mutations are associated with overall survival, progression-free interval, or cytolytic activity.},
  archive      = {J_BIOMTC},
  author       = {Paul Little and Li Hsu and Wei Sun},
  doi          = {10.1111/biom.13769},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2705-2718},
  shortjournal = {Biometrics},
  title        = {Associating somatic mutation with clinical outcomes through kernel regression and optimal transport},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delivering spatially comparable inference on the risks of
multiple severities of respiratory disease from spatially misaligned
disease count data. <em>BIOMTC</em>, <em>79</em>(3), 2691–2704. (<a
href="https://doi.org/10.1111/biom.13739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population-level disease risk varies between communities, and public health professionals are interested in mapping this spatial variation to monitor the locations of high-risk areas and the magnitudes of health inequalities. Almost all of these risk maps relate to a single severity of disease outcome, such as hospitalization, which thus ignores any cases of disease of a different severity, such as a mild case treated in a primary care setting. These spatially-varying risk maps are estimated from spatially aggregated disease count data, but the set of areal units to which these disease counts relate often varies by severity. Thus, the statistical challenge is to provide spatially comparable inference from multiple sets of spatially misaligned disease count data, and an additional complexity is that the spatial extents of the areal units for some severities are partially unknown. This paper thus proposes a novel spatial realignment approach for multivariate misaligned count data, and applies it to the first study delivering spatially comparable inference for multiple severities of the same disease. Inference is via a novel spatially smoothed data augmented MCMC algorithm, and the methods are motivated by a new study of respiratory disease risk in Scotland in 2017.},
  archive      = {J_BIOMTC},
  author       = {Duncan Lee and Craig Anderson},
  doi          = {10.1111/biom.13739},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2691-2704},
  shortjournal = {Biometrics},
  title        = {Delivering spatially comparable inference on the risks of multiple severities of respiratory disease from spatially misaligned disease count data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network on interval-censored data with application to
the prediction of alzheimer’s disease. <em>BIOMTC</em>, <em>79</em>(3),
2677–2690. (<a href="https://doi.org/10.1111/biom.13734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a progressive and polygenic disorder that affects millions of individuals each year. Given that there have been few effective treatments yet for AD, it is highly desirable to develop an accurate model to predict the full disease progression profile based on an individual&#39;s genetic characteristics for early prevention and clinical management. This work uses data composed of all four phases of the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) study, including 1740 individuals with 8 million genetic variants. We tackle several challenges in this data, characterized by large-scale genetic data, interval-censored outcome due to intermittent assessments, and left truncation in one study phase (ADNIGO). Specifically, we first develop a semiparametric transformation model on interval-censored and left-truncated data and estimate parameters through a sieve approach. Then we propose a computationally efficient generalized score test to identify variants associated with AD progression. Next, we implement a novel neural network on interval-censored data (NN-IC) to construct a prediction model using top variants identified from the genome-wide test. Comprehensive simulation studies show that the NN-IC outperforms several existing methods in terms of prediction accuracy. Finally, we apply the NN-IC to the full ADNI data and successfully identify subgroups with differential progression risk profiles. Data used in the preparation of this article were obtained from the ADNI database.},
  archive      = {J_BIOMTC},
  author       = {Tao Sun and Ying Ding},
  doi          = {10.1111/biom.13734},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2677-2690},
  shortjournal = {Biometrics},
  title        = {Neural network on interval-censored data with application to the prediction of alzheimer&#39;s disease},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prioritizing candidate peptides for cancer vaccines through
predicting peptide presentation by HLA-i proteins. <em>BIOMTC</em>,
<em>79</em>(3), 2664–2676. (<a
href="https://doi.org/10.1111/biom.13717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer (treatment) vaccines that are made of neoantigens, or peptides unique to tumor cells due to somatic mutations, have emerged as a promising method to reinvigorate the immune response against cancer. A key step to prioritizing neoantigens for cancer vaccines is computationally predicting which neoantigens are presented on the cell surface by a human leukocyte antigen (HLA). We propose to address this challenge by training a neural network using mass spectrometry (MS) data composed of peptides presented by at least one of several HLAs of a subject. We embed the neural network within a mixture model and train the neural network by maximizing the likelihood of the mixture model. After evaluating our method using data sets where the peptide presentation status was known, we applied it to analyze somatic mutations of 60 melanoma patients and identified a group of neoantigens more immunogenic in tumor cells than in normal cells. Moreover, neoantigen burden estimated by our method was significantly associated with a measurement of the immune system activity, suggesting these neoantigens could induce an immune response.},
  archive      = {J_BIOMTC},
  author       = {Laura Y. Zhou and Fei Zou and Wei Sun},
  doi          = {10.1111/biom.13717},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2664-2676},
  shortjournal = {Biometrics},
  title        = {Prioritizing candidate peptides for cancer vaccines through predicting peptide presentation by HLA-I proteins},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiwave validation sampling for error-prone electronic
health records. <em>BIOMTC</em>, <em>79</em>(3), 2649–2663. (<a
href="https://doi.org/10.1111/biom.13713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health record (EHR) data are increasingly used for biomedical research, but these data have recognized data quality challenges. Data validation is necessary to use EHR data with confidence, but limited resources typically make complete data validation impossible. Using EHR data, we illustrate prospective, multiwave, two-phase validation sampling to estimate the association between maternal weight gain during pregnancy and the risks of her child developing obesity or asthma. The optimal validation sampling design depends on the unknown efficient influence functions of regression coefficients of interest. In the first wave of our multiwave validation design, we estimate the influence function using the unvalidated (phase 1) data to determine our validation sample; then in subsequent waves, we re-estimate the influence function using validated (phase 2) data and update our sampling. For efficiency, estimation combines obesity and asthma sampling frames while calibrating sampling weights using generalized raking. We validated 996 of 10,335 mother-child EHR dyads in six sampling waves. Estimated associations between childhood obesity/asthma and maternal weight gain, as well as other covariates, are compared to naïve estimates that only use unvalidated data. In some cases, estimates markedly differ, underscoring the importance of efficient validation sampling to obtain accurate estimates incorporating validated data.},
  archive      = {J_BIOMTC},
  author       = {Bryan E. Shepherd and Kyunghee Han and Tong Chen and Aihua Bian and Shannon Pugh and Stephany N. Duda and Thomas Lumley and William J. Heerman and Pamela A. Shaw},
  doi          = {10.1111/biom.13713},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2649-2663},
  shortjournal = {Biometrics},
  title        = {Multiwave validation sampling for error-prone electronic health records},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Misdiagnosis-related harm quantification through mixture
models and harm measures. <em>BIOMTC</em>, <em>79</em>(3), 2633–2648.
(<a href="https://doi.org/10.1111/biom.13759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating and monitoring misdiagnosis-related harm is crucial for improving health care. However, this effort has traditionally focused on the chart review process, which is labor intensive, potentially unstable, and does not scale well. To monitor medical institutes&#39; diagnostic performance and identify areas for improvement in a timely fashion, researchers proposed to leverage the relationship between symptoms and diseases based on electronic health records or claim data. Specifically, the elevated disease risk following a false-negative diagnosis can be used to signal potential harm. However, off-the-shelf statistical methods do not fully accommodate the data structure of a well-hypothesized risk pattern and thus fail to address the unique challenges adequately. To fill these gaps, we proposed a mixture regression model and its associated goodness-of-fit testing. We further proposed harm measures and profiling analysis procedures to quantify, evaluate, and compare misdiagnosis-related harm across institutes with potentially different patient population compositions. We studied the performance of the proposed methods through simulation studies. We then illustrated the methods through data analyses on stroke occurrence data from the Taiwan Longitudinal Health Insurance Database. From the analyses, we quantitatively evaluated risk factors for being harmed due to misdiagnosis, which unveiled some insights for health care quality research. We also compared general and special care hospitals in Taiwan and observed better diagnostic performance in special care hospitals using various new evaluation measures.},
  archive      = {J_BIOMTC},
  author       = {Yuxin Zhu and Zheyu Wang and David Newman-Toker},
  doi          = {10.1111/biom.13759},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2633-2648},
  shortjournal = {Biometrics},
  title        = {Misdiagnosis-related harm quantification through mixture models and harm measures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric distributed lag quantile regression for
modeling time-dependent exposure mixtures. <em>BIOMTC</em>,
<em>79</em>(3), 2619–2632. (<a
href="https://doi.org/10.1111/biom.13702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying time-dependent exposure mixtures has gained increasing attentions in environmental health research. When a scalar outcome is of interest, distributed lag (DL) models have been employed to characterize the exposures effects distributed over time on the mean of final outcome. However, there is a methodological gap on investigating time-dependent exposure mixtures with different quantiles of outcome. In this paper, we introduce semiparametric partial-linear single-index (PLSI) DL quantile regression, which can describe the DL effects of time-dependent exposure mixtures on different quantiles of outcome and identify susceptible periods of exposures. We consider two time-dependent exposure settings: discrete and functional, when exposures are measured in a small number of time points and at dense time grids, respectively. Spline techniques are used to approximate the nonparametric DL function and single-index link function, and a profile estimation algorithm is proposed. Through extensive simulations, we demonstrate the performance and value of our proposed models and inference procedures. We further apply the proposed methods to study the effects of maternal exposures to ambient air pollutants of fine particulate and nitrogen dioxide on birth weight in New York University Children&#39;s Health and Environment Study (NYU CHES).},
  archive      = {J_BIOMTC},
  author       = {Yuyan Wang and Akhgar Ghassabian and Bo Gu and Yelena Afanasyeva and Yiwei Li and Leonardo Trasande and Mengling Liu},
  doi          = {10.1111/biom.13702},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2619-2632},
  shortjournal = {Biometrics},
  title        = {Semiparametric distributed lag quantile regression for modeling time-dependent exposure mixtures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial dependence modeling of latent susceptibility and
time to joint damage in psoriatic arthritis. <em>BIOMTC</em>,
<em>79</em>(3), 2605–2618. (<a
href="https://doi.org/10.1111/biom.13770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Important scientific insights into chronic diseases affecting several organ systems can be gained from modeling spatial dependence of sites experiencing damage progression. We describe models and methods for studying spatial dependence of joint damage in psoriatic arthritis (PsA). Since a large number of joints may remain unaffected even among individuals with a long disease history, spatial dependence is first modeled in latent joint-specific indicators of susceptibility. Among susceptible joints, a Gaussian copula is adopted for dependence modeling of times to damage. Likelihood and composite likelihoods are developed for settings, where individuals are under intermittent observation and progression times are subject to type K interval censoring. Two-stage estimation procedures help mitigate the computational burden arising when a large number of processes (i.e., joints) are under consideration. Simulation studies confirm that the proposed methods provide valid inference, and an application to the motivating data from the University of Toronto Psoriatic Arthritis Clinic yields important insights which can help physicians distinguish PsA from arthritic conditions with different dependence patterns.},
  archive      = {J_BIOMTC},
  author       = {Fangya Mao and Richard J. Cook},
  doi          = {10.1111/biom.13770},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2605-2618},
  shortjournal = {Biometrics},
  title        = {Spatial dependence modeling of latent susceptibility and time to joint damage in psoriatic arthritis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infinite hidden markov models for multiple multivariate time
series with missing data. <em>BIOMTC</em>, <em>79</em>(3), 2592–2604.
(<a href="https://doi.org/10.1111/biom.13715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure to air pollution is associated with increased morbidity and mortality. Recent technological advancements permit the collection of time-resolved personal exposure data. Such data are often incomplete with missing observations and exposures below the limit of detection, which limit their use in health effects studies. In this paper, we develop an infinite hidden Markov model for multiple asynchronous multivariate time series with missing data. Our model is designed to include covariates that can inform transitions among hidden states. We implement beam sampling, a combination of slice sampling and dynamic programming, to sample the hidden states, and a Bayesian multiple imputation algorithm to impute missing data. In simulation studies, our model excels in estimating hidden states and state-specific means and imputing observations that are missing at random or below the limit of detection. We validate our imputation approach on data from the Fort Collins Commuter Study. We show that the estimated hidden states improve imputations for data that are missing at random compared to existing approaches. In a case study of the Fort Collins Commuter Study, we describe the inferential gains obtained from our model including improved imputation of missing data and the ability to identify shared patterns in activity and exposure among repeated sampling days for individuals and among distinct individuals.},
  archive      = {J_BIOMTC},
  author       = {Lauren Hoskovec and Matthew D. Koslovsky and Kirsten Koehler and Nicholas Good and Jennifer L. Peel and John Volckens and Ander Wilson},
  doi          = {10.1111/biom.13715},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2592-2604},
  shortjournal = {Biometrics},
  title        = {Infinite hidden markov models for multiple multivariate time series with missing data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and robust approaches for analysis of sequential
multiple assignment randomized trials: Illustration using the ADAPT-r
trial. <em>BIOMTC</em>, <em>79</em>(3), 2577–2591. (<a
href="https://doi.org/10.1111/biom.13808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized intervention strategies, in particular those that modify treatment based on a participant&#39;s own response, are a core component of precision medicine approaches. Sequential multiple assignment randomized trials (SMARTs) are growing in popularity and are specifically designed to facilitate the evaluation of sequential adaptive strategies, in particular those embedded within the SMART. Advances in efficient estimation approaches that are able to incorporate machine learning while retaining valid inference can allow for more precise estimates of the effectiveness of these embedded regimes. However, to the best of our knowledge, such approaches have not yet been applied as the primary analysis in SMART trials. In this paper, we present a robust and efficient approach using targeted maximum likelihood estimation (TMLE) for estimating and contrasting expected outcomes under the dynamic regimes embedded in a SMART, together with generating simultaneous confidence intervals for the resulting estimates. We contrast this method with two alternatives (G-computation and inverse probability weighting estimators). The precision gains and robust inference achievable through the use of TMLE to evaluate the effects of embedded regimes are illustrated using both outcome-blind simulations and a real-data analysis from the Adaptive Strategies for Preventing and Treating Lapses of Retention in Human Immunodeficiency Virus (HIV) Care (ADAPT-R) trial (NCT02338739), a SMART with a primary aim of identifying strategies to improve retention in HIV care among people living with HIV in sub-Saharan Africa.},
  archive      = {J_BIOMTC},
  author       = {Lina M. Montoya and Michael R. Kosorok and Elvin H. Geng and Joshua Schwab and Thomas A. Odeny and Maya L. Petersen},
  doi          = {10.1111/biom.13808},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2577-2591},
  shortjournal = {Biometrics},
  title        = {Efficient and robust approaches for analysis of sequential multiple assignment randomized trials: Illustration using the ADAPT-R trial},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design considerations for two-stage enrichment clinical
trials. <em>BIOMTC</em>, <em>79</em>(3), 2565–2576. (<a
href="https://doi.org/10.1111/biom.13805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When there is a predictive biomarker, enrichment can focus the clinical trial on a benefiting subpopulation. We describe a two-stage enrichment design, in which the first stage is designed to efficiently estimate a threshold and the second stage is a “phase III-like” trial on the enriched population. The goal of this paper is to explore design issues: sample size in Stages 1 and 2, and re-estimation of the Stage 2 sample size following Stage 1. By treating these as separate trials, we can gain insight into how the predictive nature of the biomarker specifically impacts the sample size. We also show that failure to adequately estimate the threshold can have disastrous consequences in the second stage. While any bivariate model could be used, we assume a continuous outcome and continuous biomarker, described by a bivariate normal model. The correlation coefficient between the outcome and biomarker is the key to understanding the behavior of the design, both for predictive and prognostic biomarkers. Through a series of simulations we illustrate the impact of model misspecification, consequences of poor threshold estimation, and requisite sample sizes that depend on the predictive nature of the biomarker. Such insight should be helpful in understanding and designing enrichment trials.},
  archive      = {J_BIOMTC},
  author       = {Rosamarie Frieri and William Fisher Rosenberger and Nancy Flournoy and Zhantao Lin},
  doi          = {10.1111/biom.13805},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2565-2576},
  shortjournal = {Biometrics},
  title        = {Design considerations for two-stage enrichment clinical trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing exposure-time treatment effect heterogeneity in
stepped-wedge cluster randomized trials. <em>BIOMTC</em>,
<em>79</em>(3), 2551–2564. (<a
href="https://doi.org/10.1111/biom.13803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stepped-wedge cluster randomized trial (CRT) is a unidirectional crossover study in which timings of treatment initiation for clusters are randomized. Because the timing of treatment initiation is different for each cluster, an emerging question is whether the treatment effect depends on the exposure time, namely, the time duration since the initiation of treatment. Existing approaches for assessing exposure-time treatment effect heterogeneity either assume a parametric functional form of exposure time or model the exposure time as a categorical variable, in which case the number of parameters increases with the number of exposure-time periods, leading to a potential loss in efficiency. In this article, we propose a new model formulation for assessing treatment effect heterogeneity over exposure time. Rather than a categorical term for each level of exposure time, the proposed model includes a random effect to represent varying treatment effects by exposure time. This allows for pooling information across exposure-time periods and may result in more precise average and exposure-time-specific treatment effect estimates. In addition, we develop an accompanying permutation test for the variance component of the heterogeneous treatment effect parameters. We conduct simulation studies to compare the proposed model and permutation test to alternative methods to elucidate their finite-sample operating characteristics, and to generate practical guidance on model choices for assessing exposure-time treatment effect heterogeneity in stepped-wedge CRTs.},
  archive      = {J_BIOMTC},
  author       = {Lara Maleyeff and Fan Li and Sebastien Haneuse and Rui Wang},
  doi          = {10.1111/biom.13803},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2551-2564},
  shortjournal = {Biometrics},
  title        = {Assessing exposure-time treatment effect heterogeneity in stepped-wedge cluster randomized trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correcting delayed reporting of COVID-19 using the
generalized-dirichlet-multinomial method. <em>BIOMTC</em>,
<em>79</em>(3), 2537–2550. (<a
href="https://doi.org/10.1111/biom.13810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has highlighted delayed reporting as a significant impediment to effective disease surveillance and decision-making. In the absence of timely data, statistical models which account for delays can be adopted to nowcast and forecast cases or deaths. We discuss the four key sources of systematic and random variability in available data for COVID-19 and other diseases, and critically evaluate current state-of-the-art methods with respect to appropriately separating and capturing this variability. We propose a general hierarchical approach to correcting delayed reporting of COVID-19 and apply this to daily English hospital deaths, resulting in a flexible prediction tool which could be used to better inform pandemic decision-making. We compare this approach to competing models with respect to theoretical flexibility and quantitative metrics from a 15-month rolling prediction experiment imitating a realistic operational scenario. Based on consistent leads in predictive accuracy, bias, and precision, we argue that this approach is an attractive option for correcting delayed reporting of COVID-19 and future epidemics.},
  archive      = {J_BIOMTC},
  author       = {Oliver Stoner and Alba Halliday and Theo Economou},
  doi          = {10.1111/biom.13810},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2537-2550},
  shortjournal = {Biometrics},
  title        = {Correcting delayed reporting of COVID-19 using the generalized-dirichlet-multinomial method},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Age-related model for estimating the symptomatic and
asymptomatic transmissibility of COVID-19 patients. <em>BIOMTC</em>,
<em>79</em>(3), 2525–2536. (<a
href="https://doi.org/10.1111/biom.13814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of age-dependent transmissibility of COVID-19 patients is critical for effective policymaking. Although the transmissibility of symptomatic cases has been extensively studied, asymptomatic infection is understudied due to limited data. Using a dataset with reliably distinguished symptomatic and asymptomatic statuses of COVID-19 cases, we propose an ordinary differential equation model that considers age-dependent transmissibility in transmission dynamics. Under a Bayesian framework, multi-source information is synthesized in our model for identifying transmissibility. A shrinkage prior among age groups is also adopted to improve the estimation behavior of transmissibility from age-structured data. The added values of accounting for age-dependent transmissibility are further evaluated through simulation studies. In real-data analysis, we compare our approach with two basic models using the deviance information criterion (DIC) and its extension. We find that the proposed model is more flexible for our epidemic data. Our results also suggest that the transmissibility of asymptomatic infections is significantly lower (on average, 76.45\% with a credible interval (27.38\%, 88.65\%)) than that of symptomatic cases. In both symptomatic and asymptomatic patients, the transmissibility mainly increases with age. Patients older than 30 years are more likely to develop symptoms with higher transmissibility. We also find that the transmission burden of asymptomatic cases is lower than that of symptomatic patients.},
  archive      = {J_BIOMTC},
  author       = {Jianbin Tan and Ye Shen and Yang Ge and Leonardo Martinez and Hui Huang},
  doi          = {10.1111/biom.13814},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2525-2536},
  shortjournal = {Biometrics},
  title        = {Age-related model for estimating the symptomatic and asymptomatic transmissibility of COVID-19 patients},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing COVID-19 incidences longitudinally per economic
sector against the background of preventive measures and vaccination.
<em>BIOMTC</em>, <em>79</em>(3), 2516–2524. (<a
href="https://doi.org/10.1111/biom.13766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the COVID-19 pandemic, workplace transmission plays an important role. For this type of transmission, the longitudinal 14-day incidence curve of SARS-CoV-2 infections per economic sector is a proxy. In Belgium, a census of confirmed 14-day incidences per NACE-BEL sector level three is available from September 2020 until June 2021, encompassing two waves of infections. However, these high-dimensional data, with a relatively small number of NACE-BEL sectors, are challenging to analyze. We propose a nonlinear Gaussian–Gaussian model that combines parametric and semi-parametric elements to describe the incidence curves with a small set of meaningful parameters. These parameters are further analyzed with conventional statistical methods, such as CCA and linear models, to provide insight into predictive characteristics of the first wave for the second wave. Those nonlinear models classify economic sectors into three groups: sectors with two regular waves of infections, sectors with only a first wave and sectors with a more irregular profile, which may indicate a clear effect of COVID-19 vaccination. The Gaussian–Gaussian model thus allows for analyzing and comparing incidence curves and to bring out key characteristics of such curves. Finally, we consider in which other settings the proposed approach could be applied, together with possible pitfalls.},
  archive      = {J_BIOMTC},
  author       = {Florian Stijven and Johan Verbeeck and Geert Molenberghs},
  doi          = {10.1111/biom.13766},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2516-2524},
  shortjournal = {Biometrics},
  title        = {Comparing COVID-19 incidences longitudinally per economic sector against the background of preventive measures and vaccination},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast bayesian inference for large occupancy datasets.
<em>BIOMTC</em>, <em>79</em>(3), 2503–2515. (<a
href="https://doi.org/10.1111/biom.13816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the study of species&#39; occurrence has benefited from the increased availability of large-scale citizen-science data. While abundance data from standardized monitoring schemes are biased toward well-studied taxa and locations, opportunistic data are available for many taxonomic groups, from a large number of locations and across long timescales. Hence, these data provide opportunities to measure species&#39; changes in occurrence, particularly through the use of occupancy models, which account for imperfect detection. These opportunistic datasets can be substantially large, numbering hundreds of thousands of sites, and hence present a challenge from a computational perspective, especially within a Bayesian framework. In this paper, we develop a unifying framework for Bayesian inference in occupancy models that account for both spatial and temporal autocorrelation. We make use of the Pólya-Gamma scheme, which allows for fast inference, and incorporate spatio-temporal random effects using Gaussian processes (GPs), for which we consider two efficient approximations: subset of regressors and nearest neighbor GPs. We apply our model to data on two UK butterfly species, one common and widespread and one rare, using records from the Butterflies for the New Millennium database, producing occupancy indices spanning 45 years. Our framework can be applied to a wide range of taxa, providing measures of variation in species&#39; occurrence, which are used to assess biodiversity change.},
  archive      = {J_BIOMTC},
  author       = {Alex Diana and Emily Beth Dennis and Eleni Matechou and Byron John Treharne Morgan},
  doi          = {10.1111/biom.13816},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2503-2515},
  shortjournal = {Biometrics},
  title        = {Fast bayesian inference for large occupancy datasets},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian sample size calculations for comparing two
strategies in SMART studies. <em>BIOMTC</em>, <em>79</em>(3), 2489–2502.
(<a href="https://doi.org/10.1111/biom.13813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the management of most chronic conditions characterized by the lack of universally effective treatments, adaptive treatment strategies (ATSs) have grown in popularity as they offer a more individualized approach. As a result, sequential multiple assignment randomized trials (SMARTs) have gained attention as the most suitable clinical trial design to formalize the study of these strategies. While the number of SMARTs has increased in recent years, sample size and design considerations have generally been carried out in frequentist settings. However, standard frequentist formulae require assumptions on interim response rates and variance components. Misspecifying these can lead to incorrect sample size calculations and correspondingly inadequate levels of power. The Bayesian framework offers a straightforward path to alleviate some of these concerns. In this paper, we provide calculations in a Bayesian setting to allow more realistic and robust estimates that account for uncertainty in inputs through the ‘two priors’ approach. Additionally, compared to the standard frequentist formulae, this methodology allows us to rely on fewer assumptions, integrate pre-trial knowledge, and switch the focus from the standardized effect size to the MDD. The proposed methodology is evaluated in a thorough simulation study and is implemented to estimate the sample size for a full-scale SMART of an internet-based adaptive stress management intervention on cardiovascular disease patients using data from its pilot study conducted in two Canadian provinces.},
  archive      = {J_BIOMTC},
  author       = {Armando Turchetta and Erica E. M. Moodie and David A. Stephens and Sylvie D. Lambert},
  doi          = {10.1111/biom.13813},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2489-2502},
  shortjournal = {Biometrics},
  title        = {Bayesian sample size calculations for comparing two strategies in SMART studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian hierarchical quantile regression with application
to characterizing the immune architecture of lung cancer.
<em>BIOMTC</em>, <em>79</em>(3), 2474–2488. (<a
href="https://doi.org/10.1111/biom.13774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successful development and implementation of precision immuno-oncology therapies requires a deeper understanding of the immune architecture at a patient level. T-cell receptor (TCR) repertoire sequencing is a relatively new technology that enables monitoring of T-cells, a subset of immune cells that play a central role in modulating immune response. These immunologic relationships are complex and are governed by various distributional aspects of an individual patient&#39;s tumor profile. We propose Bayesian QUANTIle regression for hierarchical COvariates (QUANTICO) that allows simultaneous modeling of hierarchical relationships between multilevel covariates, conducts explicit variable selection, estimates quantile and patient-specific coefficient effects, to induce individualized inference. We show QUANTICO outperforms existing approaches in multiple simulation scenarios. We demonstrate the utility of QUANTICO to investigate the effect of TCR variables on immune response in a cohort of lung cancer patients. At population level, our analyses reveal the mechanistic role of T-cell proportion on the immune cell abundance, with tumor mutation burden as an important factor modulating this relationship. At a patient level, we find several outlier patients based on their quantile-specific coefficient functions, who have higher mutational rates and different smoking history.},
  archive      = {J_BIOMTC},
  author       = {Priyam Das and Christine B. Peterson and Yang Ni and Alexandre Reuben and Jiexin Zhang and Jianjun Zhang and Kim-Anh Do and Veerabhadran Baladandayuthapani},
  doi          = {10.1111/biom.13774},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2474-2488},
  shortjournal = {Biometrics},
  title        = {Bayesian hierarchical quantile regression with application to characterizing the immune architecture of lung cancer},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian treatment screening and selection using
subgroup-specific utilities of response and toxicity. <em>BIOMTC</em>,
<em>79</em>(3), 2458–2473. (<a
href="https://doi.org/10.1111/biom.13738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian design is proposed for randomized phase II clinical trials that screen multiple experimental treatments compared to an active control based on ordinal categorical toxicity and response. The underlying model and design account for patient heterogeneity characterized by ordered prognostic subgroups. All decision criteria are subgroup specific, including interim rules for dropping unsafe or ineffective treatments, and criteria for selecting optimal treatments at the end of the trial. The design requires an elicited utility function of the two outcomes that varies with the subgroups. Final treatment selections are based on posterior mean utilities. The methodology is illustrated by a trial of targeted agents for metastatic renal cancer, which motivated the design methodology. In the context of this application, the design is evaluated by computer simulation, including comparison to three designs that conduct separate trials within subgroups, or conduct one trial while ignoring subgroups, or base treatment selection on estimated response rates while ignoring toxicity.},
  archive      = {J_BIOMTC},
  author       = {Juhee Lee and Peter F. Thall and Pavlos Msaouel},
  doi          = {10.1111/biom.13738},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2458-2473},
  shortjournal = {Biometrics},
  title        = {Bayesian treatment screening and selection using subgroup-specific utilities of response and toxicity},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent state space model for estimating brain dynamics
from electroencephalogram (EEG) data. <em>BIOMTC</em>, <em>79</em>(3),
2444–2457. (<a href="https://doi.org/10.1111/biom.13742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern neuroimaging technologies have substantially advanced the measurement of brain activity. Electroencephalogram (EEG) as a noninvasive neuroimaging technique measures changes in electrical voltage on the scalp induced by brain cortical activity. With its high temporal resolution, EEG has emerged as an increasingly useful tool to study brain connectivity. Challenges with modeling EEG signals of complex brain activity include interactions among unknown sources, low signal-to-noise ratio, and substantial between-subject heterogeneity. In this work, we propose a state space model that jointly analyzes multichannel EEG signals and learns dynamics of different sources corresponding to brain cortical activity. Our model borrows strength from spatially correlated measurements and uses low-dimensional latent states to explain all observed channels. The model can account for patient heterogeneity and quantify the effect of a subject&#39;s covariates on the latent space. The EM algorithm, Kalman filtering, and bootstrap resampling are used to fit the state space model and provide comparisons between patient diagnostic groups. We apply the developed approach to a case-control study of alcoholism and reveal significant attenuation of brain activity in response to visual stimuli in alcoholic subjects compared to healthy controls.},
  archive      = {J_BIOMTC},
  author       = {Qinxia Wang and Ji Meng Loh and Xiaofu He and Yuanjia Wang},
  doi          = {10.1111/biom.13742},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2444-2457},
  shortjournal = {Biometrics},
  title        = {A latent state space model for estimating brain dynamics from electroencephalogram (EEG) data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-dimensional mediation model for a neuroimaging
mediator: Integrating clinical, neuroimaging, and neurocognitive data to
mitigate late effects in pediatric cancer. <em>BIOMTC</em>,
<em>79</em>(3), 2430–2443. (<a
href="https://doi.org/10.1111/biom.13729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pediatric cancer treatment, especially for brain tumors, can have profound and complicated late effects. With the survival rates increasing because of improved detection and treatment, a more comprehensive understanding of the impact of current treatments on neurocognitive function and brain structure is critically needed. A frontline medulloblastoma clinical trial (SJMB03) has collected data, including treatment, clinical, neuroimaging, and cognitive variables. Advanced methods for modeling and integrating these data are critically needed to understand the mediation pathway from the treatment through brain structure to neurocognitive outcomes. We propose an integrative Bayesian mediation analysis approach to model jointly a treatment exposure, a high-dimensional structural neuroimaging mediator, and a neurocognitive outcome and to uncover the mediation pathway. The high-dimensional imaging-related coefficients are modeled via a binary Ising–Gaussian Markov random field prior (BI-GMRF), addressing the sparsity, spatial dependency, and smoothness and increasing the power to detect brain regions with mediation effects. Numerical simulations demonstrate the estimation accuracy, power, and robustness. For the SJMB03 study, the BI-GMRF method has identified white matter microstructure that is damaged by cancer-directed treatment and impacts late neurocognitive outcomes. The results provide guidance on improving treatment planning to minimize long-term cognitive sequela for pediatric brain tumor patients.},
  archive      = {J_BIOMTC},
  author       = {Jade Xiaoqing Wang and Yimei Li and Wilburn E. Reddick and Heather M. Conklin and John O. Glass and Arzu Onar-Thomas and Amar Gajjar and Cheng Cheng and Zhao-Hua Lu},
  doi          = {10.1111/biom.13729},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2430-2443},
  shortjournal = {Biometrics},
  title        = {A high-dimensional mediation model for a neuroimaging mediator: Integrating clinical, neuroimaging, and neurocognitive data to mitigate late effects in pediatric cancer},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated analysis of low-field brain MRI in cerebral
malaria. <em>BIOMTC</em>, <em>79</em>(3), 2417–2429. (<a
href="https://doi.org/10.1111/biom.13708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central challenge of medical imaging studies is to extract biomarkers that characterize disease pathology or outcomes. Modern automated approaches have found tremendous success in high-resolution, high-quality magnetic resonance images. These methods, however, may not translate to low-resolution images acquired on magnetic resonance imaging (MRI) scanners with lower magnetic field strength. In low-resource settings where low-field scanners are more common and there is a shortage of radiologists to manually interpret MRI scans, it is critical to develop automated methods that can augment or replace manual interpretation, while accommodating reduced image quality. We present a fully automated framework for translating radiological diagnostic criteria into image-based biomarkers, inspired by a project in which children with cerebral malaria (CM) were imaged using low-field 0.35 Tesla MRI. We integrate multiatlas label fusion, which leverages high-resolution images from another sample as prior spatial information, with parametric Gaussian hidden Markov models based on image intensities, to create a robust method for determining ventricular cerebrospinal fluid volume. We also propose normalized image intensity and texture measurements to determine the loss of gray-to-white matter tissue differentiation and sulcal effacement. These integrated biomarkers have excellent classification performance for determining severe brain swelling due to CM.},
  archive      = {J_BIOMTC},
  author       = {Danni Tu and Manu S. Goyal and Jordan D. Dworkin and Samuel Kampondeni and Lorenna Vidal and Eric Biondo-Savin and Sandeep Juvvadi and Prashant Raghavan and Jennifer Nicholas and Karen Chetcuti and Kelly Clark and Timothy Robert-Fitzgerald and Theodore D. Satterthwaite and Paul Yushkevich and Christos Davatzikos and Guray Erus and Nicholas J. Tustison and Douglas G. Postels and Terrie E. Taylor and Dylan S. Small and Russell T. Shinohara},
  doi          = {10.1111/biom.13708},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2417-2429},
  shortjournal = {Biometrics},
  title        = {Automated analysis of low-field brain MRI in cerebral malaria},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistent estimation of the number of communities via
regularized network embedding. <em>BIOMTC</em>, <em>79</em>(3),
2404–2416. (<a href="https://doi.org/10.1111/biom.13815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network analysis plays an important role in numerous application domains including biomedicine. Estimation of the number of communities is a fundamental and critical issue in network analysis. Most existing studies assume that the number of communities is known a priori, or lack of rigorous theoretical guarantee on the estimation consistency. In this paper, we propose a regularized network embedding model to simultaneously estimate the community structure and the number of communities in a unified formulation. The proposed model equips network embedding with a novel composite regularization term, which pushes the embedding vector toward its center and pushes similar community centers collapsed with each other. A rigorous theoretical analysis is conducted, establishing asymptotic consistency in terms of community detection and estimation of the number of communities. Extensive numerical experiments have also been conducted on both synthetic networks and brain functional connectivity network, which demonstrate the superior performance of the proposed method compared with existing alternatives.},
  archive      = {J_BIOMTC},
  author       = {Mingyang Ren and Sanguo Zhang and Junhui Wang},
  doi          = {10.1111/biom.13815},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2404-2416},
  shortjournal = {Biometrics},
  title        = {Consistent estimation of the number of communities via regularized network embedding},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for the dimension of a regression relationship
using pseudo-covariates. <em>BIOMTC</em>, <em>79</em>(3), 2394–2403. (<a
href="https://doi.org/10.1111/biom.13812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data analysis using dimension reduction methods, the main goal is to summarize how the response is related to the covariates through a few linear combinations. One key issue is to determine the number of independent, relevant covariate combinations, which is the dimension of the sufficient dimension reduction (SDR) subspace. In this work, we propose an easily-applied approach to conduct inference for the dimension of the SDR subspace, based on augmentation of the covariate set with simulated pseudo-covariates. Applying the partitioning principal to the possible dimensions, we use rigorous sequential testing to select the dimensionality, by comparing the strength of the signal arising from the actual covariates to that appearing to arise from the pseudo-covariates. We show that under a “uniform direction” condition, our approach can be used in conjunction with several popular SDR methods, including sliced inverse regression. In these settings, the test statistic asymptotically follows a beta distribution and therefore is easily calibrated. Moreover, the family-wise type I error rate of our sequential testing is rigorously controlled. Simulation studies and an analysis of newborn anthropometric data demonstrate the robustness of the proposed approach, and indicate that the power is comparable to or greater than the alternatives.},
  archive      = {J_BIOMTC},
  author       = {Shih-Hao Huang and Kerby Shedden and Hsin-wen Chang},
  doi          = {10.1111/biom.13812},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2394-2403},
  shortjournal = {Biometrics},
  title        = {Inference for the dimension of a regression relationship using pseudo-covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the area under the ROC curve when transporting a
prediction model to a target population. <em>BIOMTC</em>,
<em>79</em>(3), 2382–2393. (<a
href="https://doi.org/10.1111/biom.13796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose methods for estimating the area under the receiver operating characteristic (ROC) curve (AUC) of a prediction model in a target population that differs from the source population that provided the data used for original model development. If covariates that are associated with model performance, as measured by the AUC, have a different distribution in the source and target populations, then AUC estimators that only use data from the source population will not reflect model performance in the target population. Here, we provide identification results for the AUC in the target population when outcome and covariate data are available from the sample of the source population, but only covariate data are available from the sample of the target population. In this setting, we propose three estimators for the AUC in the target population and show that they are consistent and asymptotically normal. We evaluate the finite-sample performance of the estimators using simulations and use them to estimate the AUC in a nationally representative target population from the National Health and Nutrition Examination Survey for a lung cancer risk prediction model developed using source population data from the National Lung Screening Trial.},
  archive      = {J_BIOMTC},
  author       = {Bing Li and Constantine Gatsonis and Issa J. Dahabreh and Jon A. Steingrimsson},
  doi          = {10.1111/biom.13796},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2382-2393},
  shortjournal = {Biometrics},
  title        = {Estimating the area under the ROC curve when transporting a prediction model to a target population},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical inference and power analysis for direct and
spillover effects in two-stage randomized experiments. <em>BIOMTC</em>,
<em>79</em>(3), 2370–2381. (<a
href="https://doi.org/10.1111/biom.13782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage randomized experiments become an increasingly popular experimental design for causal inference when the outcome of one unit may be affected by the treatment assignments of other units in the same cluster. In this paper, we provide a methodological framework for general tools of statistical inference and power analysis for two-stage randomized experiments. Under the randomization-based framework, we consider the estimation of a new direct effect of interest as well as the average direct and spillover effects studied in the literature. We provide unbiased estimators of these causal quantities and their conservative variance estimators in a general setting. Using these results, we then develop hypothesis testing procedures and derive sample size formulas. We theoretically compare the two-stage randomized design with the completely randomized and cluster randomized designs, which represent two limiting designs. Finally, we conduct simulation studies to evaluate the empirical performance of our sample size formulas. For empirical illustration, the proposed methodology is applied to the randomized evaluation of the Indian National Health Insurance Program. An open-source software package is available for implementing the proposed methodology.},
  archive      = {J_BIOMTC},
  author       = {Zhichao Jiang and Kosuke Imai and Anup Malani},
  doi          = {10.1111/biom.13782},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2370-2381},
  shortjournal = {Biometrics},
  title        = {Statistical inference and power analysis for direct and spillover effects in two-stage randomized experiments},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CEDAR: Communication efficient distributed analysis for
regressions. <em>BIOMTC</em>, <em>79</em>(3), 2357–2369. (<a
href="https://doi.org/10.1111/biom.13786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) offer great promises for advancing precision medicine and, at the same time, present significant analytical challenges. Particularly, it is often the case that patient-level data in EHRs cannot be shared across institutions (data sources) due to government regulations and/or institutional policies. As a result, there are growing interests about distributed learning over multiple EHRs databases without sharing patient-level data. To tackle such challenges, we propose a novel communication efficient method that aggregates the optimal estimates of external sites, by turning the problem into a missing data problem. In addition, we propose incorporating posterior samples of remote sites, which can provide partial information on the missing quantities and improve efficiency of parameter estimates while having the differential privacy property and thus reducing the risk of information leaking. The proposed approach, without sharing the raw patient level data, allows for proper statistical inference. We provide theoretical investigation for the asymptotic properties of the proposed method for statistical inference as well as differential privacy, and evaluate its performance in simulations and real data analyses in comparison with several recently developed methods.},
  archive      = {J_BIOMTC},
  author       = {Changgee Chang and Zhiqi Bu and Qi Long},
  doi          = {10.1111/biom.13786},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2357-2369},
  shortjournal = {Biometrics},
  title        = {CEDAR: Communication efficient distributed analysis for regressions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How well can fine balance work for covariate balancing.
<em>BIOMTC</em>, <em>79</em>(3), 2346–2356. (<a
href="https://doi.org/10.1111/biom.13771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine balance is a matching technique to improve covariate balance in observational studies. It constrains a match to have identical distributions for some covariates without restricting who is matched to whom. However, despite its wide application and excellent performance in practice, there is very little theory indicating when the method is likely to succeed or fail and to what extent it can remove covariate imbalance. In order to answer these questions, this paper studies the limits of what is possible for covariate balancing using fine balance and near-fine balance. The investigations suggest that given the distributions of the treated and control groups, in large samples, the maximum achievable balance by using fine balance only depends on the matching ratio (ie, the ratio of the sample size of the control group to that of the treated group). In addition, the results indicate how to estimate this matching ratio threshold without knowledge of the true distributions in finite samples. The findings are also illustrated by numerical studies in this paper.},
  archive      = {J_BIOMTC},
  author       = {Ruoqi Yu},
  doi          = {10.1111/biom.13771},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2346-2356},
  shortjournal = {Biometrics},
  title        = {How well can fine balance work for covariate balancing},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying brain hierarchical structures associated with
alzheimer’s disease using a regularized regression method with tree
predictors. <em>BIOMTC</em>, <em>79</em>(3), 2333–2345. (<a
href="https://doi.org/10.1111/biom.13775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain segmentation at different levels is generally represented as hierarchical trees. Brain regional atrophy at specific levels was found to be marginally associated with Alzheimer&#39;s disease outcomes. In this study, we propose an ℓ 1 -type regularization for predictors that follow a hierarchical tree structure. Considering a tree as a directed acyclic graph, we interpret the model parameters from a path analysis perspective. Under this concept, the proposed penalty regulates the total effect of each predictor on the outcome. With regularity conditions, it is shown that under the proposed regularization, the estimator of the model coefficient is consistent in ℓ 2 -norm and the model selection is also consistent. When applied to a brain sMRI dataset acquired from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI), the proposed approach identifies brain regions where atrophy in these regions demonstrates the declination in memory. With regularization on the total effects, the findings suggest that the impact of atrophy on memory deficits is localized from small brain regions, but at various levels of brain segmentation. Data used in preparation of this paper were obtained from the ADNI database.},
  archive      = {J_BIOMTC},
  author       = {Yi Zhao and Bingkai Wang and Chin-Fu Liu and Andreia V. Faria and Michael I. Miller and Brian S. Caffo and Xi Luo},
  doi          = {10.1111/biom.13775},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2333-2345},
  shortjournal = {Biometrics},
  title        = {Identifying brain hierarchical structures associated with alzheimer&#39;s disease using a regularized regression method with tree predictors},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Microbiome subcommunity learning with logistic-tree normal
latent dirichlet allocation. <em>BIOMTC</em>, <em>79</em>(3), 2321–2332.
(<a href="https://doi.org/10.1111/biom.13772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-membership (MM) models such as latent Dirichlet allocation (LDA) have been applied to microbiome compositional data to identify latent subcommunities of microbial species. These subcommunities are informative for understanding the biological interplay of microbes and for predicting health outcomes. However, microbiome compositions typically display substantial cross-sample heterogeneities in subcommunity compositions—that is, the variability in the proportions of microbes in shared subcommunities across samples—which is not accounted for in prior analyses. As a result, LDA can produce inference, which is highly sensitive to the specification of the number of subcommunities and often divides a single subcommunity into multiple artificial ones. To address this limitation, we incorporate the logistic-tree normal (LTN) model into LDA to form a new MM model. This model allows cross-sample variation in the composition of each subcommunity around some “centroid” composition that defines the subcommunity. Incorporation of auxiliary Pólya-Gamma variables enables a computationally efficient collapsed blocked Gibbs sampler to carry out Bayesian inference under this model. By accounting for such heterogeneity, our new model restores the robustness of the inference in the specification of the number of subcommunities and allows meaningful subcommunities to be identified.},
  archive      = {J_BIOMTC},
  author       = {Patrick LeBlanc and Li Ma},
  doi          = {10.1111/biom.13772},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2321-2332},
  shortjournal = {Biometrics},
  title        = {Microbiome subcommunity learning with logistic-tree normal latent dirichlet allocation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On generalized latent factor modeling and inference for
high-dimensional binomial data. <em>BIOMTC</em>, <em>79</em>(3),
2311–2320. (<a href="https://doi.org/10.1111/biom.13768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore a hierarchical generalized latent factor model for discrete and bounded response variables and in particular, binomial responses. Specifically, we develop a novel two-step estimation procedure and the corresponding statistical inference that is computationally efficient and scalable for the high dimension in terms of both the number of subjects and the number of features per subject. We also establish the validity of the estimation procedure, particularly the asymptotic properties of the estimated effect size and the latent structure, as well as the estimated number of latent factors. The results are corroborated by a simulation study and for illustration, the proposed methodology is applied to analyze a dataset in a gene–environment association study.},
  archive      = {J_BIOMTC},
  author       = {Ting Fung Ma and Fangfang Wang and Jun Zhu},
  doi          = {10.1111/biom.13768},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2311-2320},
  shortjournal = {Biometrics},
  title        = {On generalized latent factor modeling and inference for high-dimensional binomial data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting distributional copula regression. <em>BIOMTC</em>,
<em>79</em>(3), 2298–2310. (<a
href="https://doi.org/10.1111/biom.13765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing complex dependence structures between outcome variables (e.g., study endpoints) is of high relevance in contemporary biomedical data problems and medical research. Distributional copula regression provides a flexible tool to model the joint distribution of multiple outcome variables by disentangling the marginal response distributions and their dependence structure. In a regression setup, each parameter of the copula model, that is, the marginal distribution parameters and the copula dependence parameters, can be related to covariates via structured additive predictors. We propose a framework to fit distributional copula regression via model-based boosting, which is a modern estimation technique that incorporates useful features like an intrinsic variable selection mechanism, parameter shrinkage and the capability to fit regression models in high-dimensional data setting, that is, situations with more covariates than observations. Thus, model-based boosting does not only complement existing Bayesian and maximum-likelihood based estimation frameworks for this model class but rather enables unique intrinsic mechanisms that can be helpful in many applied problems. The performance of our boosting algorithm for copula regression models with continuous margins is evaluated in simulation studies that cover low- and high-dimensional data settings and situations with and without dependence between the responses. Moreover, distributional copula boosting is used to jointly analyze and predict the length and the weight of newborns conditional on sonographic measurements of the fetus before delivery together with other clinical variables.},
  archive      = {J_BIOMTC},
  author       = {Nicolai Hans and Nadja Klein and Florian Faschingbauer and Michael Schneider and Andreas Mayr},
  doi          = {10.1111/biom.13765},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2298-2310},
  shortjournal = {Biometrics},
  title        = {Boosting distributional copula regression},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concave likelihood-based regression with finite-support
response variables. <em>BIOMTC</em>, <em>79</em>(3), 2286–2297. (<a
href="https://doi.org/10.1111/biom.13760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified framework for likelihood-based regression modeling when the response variable has finite support. Our work is motivated by the fact that, in practice, observed data are discrete and bounded. The proposed methods assume a model which includes models previously considered for interval-censored variables with log-concave distributions as special cases. The resulting log-likelihood is concave, which we use to establish asymptotic normality of its maximizer as the number of observations n tends to infinity with the number of parameters d fixed, and rates of convergence of L 1 -regularized estimators when the true parameter vector is sparse and d and n both tend to infinity with log ( d ) / n → 0 $\log (d) / n \rightarrow 0$ . We consider an inexact proximal Newton algorithm for computing estimates and give theoretical guarantees for its convergence. The range of possible applications is wide, including but not limited to survival analysis in discrete time, the modeling of outcomes on scored surveys and questionnaires, and, more generally, interval-censored regression. The applicability and usefulness of the proposed methods are illustrated in simulations and data examples.},
  archive      = {J_BIOMTC},
  author       = {K.O. Ekvall and M. Bottai},
  doi          = {10.1111/biom.13760},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2286-2297},
  shortjournal = {Biometrics},
  title        = {Concave likelihood-based regression with finite-support response variables},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Segmented correspondence curve regression for quantifying
covariate effects on the reproducibility of high-throughput experiments.
<em>BIOMTC</em>, <em>79</em>(3), 2272–2285. (<a
href="https://doi.org/10.1111/biom.13757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput biological experiments are essential tools for identifying biologically interesting candidates in large-scale omics studies. The results of a high-throughput biological experiment rely heavily on the operational factors chosen in its experimental and data-analytic procedures. Understanding how these operational factors influence the reproducibility of the experimental outcome is critical for selecting the optimal parameter settings and designing reliable high-throughput workflows. However, the influence of an operational factor may differ between strong and weak candidates in a high-throughput experiment, complicating the selection of parameter settings. To address this issue, we propose a novel segmented regression model, called segmented correspondence curve regression, to assess the influence of operational factors on the reproducibility of high-throughput experiments. Our model dissects the heterogeneous effects of operational factors on strong and weak candidates, providing a principled way to select operational parameters. Based on this framework, we also develop a sup-likelihood ratio test for the existence of heterogeneity. Simulation studies show that our estimation and testing procedures yield well-calibrated type I errors and are substantially more powerful in detecting and locating the differences in reproducibility across workflows than the existing method. Using this model, we investigated an important design question for ChIP-seq experiments: How many reads should one sequence to obtain reliable results in a cost-effective way? Our results reveal new insights into the impact of sequencing depth on the binding-site identification reproducibility, helping biologists determine the most cost-effective sequencing depth to achieve sufficient reproducibility for their study goals.},
  archive      = {J_BIOMTC},
  author       = {Feipeng Zhang and Qunhua Li},
  doi          = {10.1111/biom.13757},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2272-2285},
  shortjournal = {Biometrics},
  title        = {Segmented correspondence curve regression for quantifying covariate effects on the reproducibility of high-throughput experiments},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating tree-based dynamic treatment regimes using
observational data with restricted treatment sequences. <em>BIOMTC</em>,
<em>79</em>(3), 2260–2271. (<a
href="https://doi.org/10.1111/biom.13754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regime (DTR) is a sequence of decision rules that provide guidance on how to treat individuals based on their static and time-varying status. Existing observational data are often used to generate hypotheses about effective DTRs. A common challenge with observational data, however, is the need for analysts to consider “restrictions” on the treatment sequences. Such restrictions may be necessary for settings where (1) one or more treatment sequences that were offered to individuals when the data were collected are no longer considered viable in practice, (2) specific treatment sequences are no longer available, or (3) the scientific focus of the analysis concerns a specific type of treatment sequences (eg, “stepped-up” treatments). To address this challenge, we propose a restricted tree–based reinforcement learning (RT-RL) method that searches for an interpretable DTR with the maximum expected outcome, given a (set of) user-specified restriction(s), which specifies treatment options (at each stage) that ought not to be considered as part of the estimated tree-based DTR. In simulations, we evaluate the performance of RT-RL versus the standard approach of ignoring the partial data for individuals not following the (set of) restriction(s). The method is illustrated using an observational data set to estimate a two-stage stepped-up DTR for guiding the level of care placement for adolescents with substance use disorder.},
  archive      = {J_BIOMTC},
  author       = {Nina Zhou and Lu Wang and Daniel Almirall},
  doi          = {10.1111/biom.13754},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2260-2271},
  shortjournal = {Biometrics},
  title        = {Estimating tree-based dynamic treatment regimes using observational data with restricted treatment sequences},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous cluster structure learning and estimation of
heterogeneous graphs for matrix-variate fMRI data. <em>BIOMTC</em>,
<em>79</em>(3), 2246–2259. (<a
href="https://doi.org/10.1111/biom.13753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models play an important role in neuroscience studies, particularly in brain connectivity analysis. Typically, observations/samples are from several heterogenous groups and the group membership of each observation/sample is unavailable, which poses a great challenge for graph structure learning. In this paper, we propose a method which can achieve Simultaneous Clustering and Estimation of Heterogeneous Graphs (briefly denoted as SCEHG) for matrix-variate functional magnetic resonance imaging (fMRI) data. Unlike the conventional clustering methods which rely on the mean differences of various groups, the proposed SCEHG method fully exploits the group differences of conditional dependence relationships among brain regions for learning cluster structure. In essence, by constructing individual-level between-region network measures, we formulate clustering as penalized regression with grouping and sparsity pursuit, which transforms the unsupervised learning into supervised learning. A modified difference of convex programming with the alternating direction method of multipliers (DC-ADMM) algorithm is proposed to solve the corresponding optimization problem. We also propose a generalized criterion to specify the number of clusters. Extensive simulation studies illustrate the superiority of the SCEHG method over some state-of-the-art methods in terms of both clustering and graph recovery accuracy. We also apply the SCEHG procedure to analyze fMRI data associated with attention-deficit hyperactivity disorder (ADHD), which illustrates its empirical usefulness.},
  archive      = {J_BIOMTC},
  author       = {Dong Liu and Changwei Zhao and Yong He and Lei Liu and Ying Guo and Xinsheng Zhang},
  doi          = {10.1111/biom.13753},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2246-2259},
  shortjournal = {Biometrics},
  title        = {Simultaneous cluster structure learning and estimation of heterogeneous graphs for matrix-variate fMRI data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional data analysis with covariate-dependent mean and
covariance structures. <em>BIOMTC</em>, <em>79</em>(3), 2232–2245. (<a
href="https://doi.org/10.1111/biom.13744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis has emerged as a powerful tool in response to the ever-increasing resources and efforts devoted to collecting information about response curves or anything that varies over a continuum. However, limited progress has been made with regard to linking the covariance structures of response curves to external covariates, as most functional models assume a common covariance structure. We propose a new functional regression model with covariate-dependent mean and covariance structures. Particularly, by allowing variances of random scores to be covariate-dependent, we identify eigenfunctions for each individual from the set of eigenfunctions that govern the variation patterns across all individuals, resulting in high interpretability and prediction power. We further propose a new penalized quasi-likelihood procedure that combines regularization and B-spline smoothing for model selection and estimation and establish the convergence rate and asymptotic normality of the proposed estimators. The utility of the developed method is demonstrated via simulations, as well as an analysis of the Avon Longitudinal Study of Parents and Children concerning parental effects on the growth curves of their offspring, which yields biologically interesting results.},
  archive      = {J_BIOMTC},
  author       = {Chenlin Zhang and Huazhen Lin and Li Liu and Jin Liu and Yi Li},
  doi          = {10.1111/biom.13744},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2232-2245},
  shortjournal = {Biometrics},
  title        = {Functional data analysis with covariate-dependent mean and covariance structures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized propensity score approach to causal inference
with spatial interference. <em>BIOMTC</em>, <em>79</em>(3), 2220–2231.
(<a href="https://doi.org/10.1111/biom.13745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many spatial phenomena exhibit interference, where exposures at one location may affect the response at other locations. Because interference violates the stable unit treatment value assumption, standard methods for causal inference do not apply. We propose a new causal framework to recover direct and spill-over effects in the presence of spatial interference, taking into account that exposures at nearby locations are more influential than exposures at locations further apart. Under the no unmeasured confounding assumption, we show that a generalized propensity score is sufficient to remove all measured confounding. To reduce dimensionality issues, we propose a Bayesian spline-based regression model accounting for a sufficient set of variables for the generalized propensity score. A simulation study demonstrates the accuracy and coverage properties. We apply the method to estimate the causal effect of wildland fires on air pollution in the Western United States over 2005–2018.},
  archive      = {J_BIOMTC},
  author       = {A. Giffin and B. J. Reich and S. Yang and A. G. Rappold},
  doi          = {10.1111/biom.13745},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2220-2231},
  shortjournal = {Biometrics},
  title        = {Generalized propensity score approach to causal inference with spatial interference},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mendelian randomization mixed-scale treatment effect robust
identification and estimation for causal inference. <em>BIOMTC</em>,
<em>79</em>(3), 2208–2219. (<a
href="https://doi.org/10.1111/biom.13735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard Mendelian randomization (MR) analysis can produce biased results if the genetic variant defining an instrumental variable (IV) is confounded and/or has a horizontal pleiotropic effect on the outcome of interest not mediated by the treatment variable. We provide novel identification conditions for the causal effect of a treatment in the presence of unmeasured confounding by leveraging a possibly invalid IV for which both the IV independence and exclusion restriction assumptions may be violated. The proposed Mendelian randomization mixed-scale treatment effect robust identification (MR MiSTERI) approach relies on (i) an assumption that the treatment effect does not vary with the possibly invalid IV on the additive scale; (ii) that the confounding bias does not vary with the possibly invalid IV on the odds ratio scale; and (iii) that the residual variance for the outcome is heteroskedastic with respect to the possibly invalid IV. Although assumptions (i) and (ii) have, respectively, appeared in the IV literature, assumption (iii) has not; we formally establish that their conjunction can identify a causal effect even with an invalid IV. MR MiSTERI is shown to be particularly advantageous in the presence of pervasive heterogeneity of pleiotropic effects on the additive scale. We propose a simple and consistent three-stage estimator that can be used as a preliminary estimator to a carefully constructed efficient one-step-update estimator. In order to incorporate multiple, possibly correlated, and weak invalid IVs, a common challenge in MR studies, we develop a MAny Weak Invalid Instruments (MR MaWII MiSTERI) approach for strengthened identification and improved estimation accuracy. Both simulation studies and UK Biobank data analysis results demonstrate the robustness of the proposed methods.},
  archive      = {J_BIOMTC},
  author       = {Zhonghua Liu and Ting Ye and Baoluo Sun and Mary Schooling and Eric Tchetgen Tchetgen},
  doi          = {10.1111/biom.13735},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2208-2219},
  shortjournal = {Biometrics},
  title        = {Mendelian randomization mixed-scale treatment effect robust identification and estimation for causal inference},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing weak nulls in matched observational studies.
<em>BIOMTC</em>, <em>79</em>(3), 2196–2207. (<a
href="https://doi.org/10.1111/biom.13741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop sensitivity analyses for the sample average treatment effect in matched observational studies while allowing unit-level treatment effects to vary. The methods may be applied to studies using any optimal without-replacement matching algorithm. In contrast to randomized experiments and to paired observational studies, we show for general matched designs that over a large class of test statistics, any procedure bounding the worst-case expectation while allowing for arbitrary effect heterogeneity must be unnecessarily conservative if treatment effects are actually constant across individuals. We present a sensitivity analysis which bounds the worst-case expectation while allowing for effect heterogeneity, and illustrate why it is generally conservative if effects are constant. An alternative procedure is presented that is asymptotically sharp if treatment effects are constant, and that is valid for testing the sample average effect under additional restrictions which may be deemed benign by practitioners. Simulations demonstrate that this alternative procedure results in a valid sensitivity analysis for the weak null hypothesis under a host of reasonable data-generating processes. The procedures allow practitioners to assess robustness of estimated sample average treatment effects to hidden bias while allowing for effect heterogeneity in matched observational studies.},
  archive      = {J_BIOMTC},
  author       = {Colin B. Fogarty},
  doi          = {10.1111/biom.13741},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2196-2207},
  shortjournal = {Biometrics},
  title        = {Testing weak nulls in matched observational studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel penalized inverse-variance weighted estimator for
mendelian randomization with applications to COVID-19 outcomes.
<em>BIOMTC</em>, <em>79</em>(3), 2184–2195. (<a
href="https://doi.org/10.1111/biom.13732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization utilizes genetic variants as instrumental variables (IVs) to estimate the causal effect of an exposure variable on an outcome of interest even in the presence of unmeasured confounders. However, the popular inverse-variance weighted (IVW) estimator could be biased in the presence of weak IVs, a common challenge in MR studies. In this article, we develop a novel penalized inverse-variance weighted (pIVW) estimator, which adjusts the original IVW estimator to account for the weak IV issue by using a penalization approach to prevent the denominator of the pIVW estimator from being close to zero. Moreover, we adjust the variance estimation of the pIVW estimator to account for the presence of balanced horizontal pleiotropy. We show that the recently proposed debiased IVW (dIVW) estimator is a special case of our proposed pIVW estimator. We further prove that the pIVW estimator has smaller bias and variance than the dIVW estimator under some regularity conditions. We also conduct extensive simulation studies to demonstrate the performance of the proposed pIVW estimator. Furthermore, we apply the pIVW estimator to estimate the causal effects of five obesity-related exposures on three coronavirus disease 2019 (COVID-19) outcomes. Notably, we find that hypertensive disease is associated with an increased risk of hospitalized COVID-19; and peripheral vascular disease and higher body mass index are associated with increased risks of COVID-19 infection, hospitalized COVID-19, and critically ill COVID-19.},
  archive      = {J_BIOMTC},
  author       = {Siqi Xu and Peng Wang and Wing Kam Fung and Zhonghua Liu},
  doi          = {10.1111/biom.13732},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2184-2195},
  shortjournal = {Biometrics},
  title        = {A novel penalized inverse-variance weighted estimator for mendelian randomization with applications to COVID-19 outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general modeling framework for open wildlife populations
based on the polya tree prior. <em>BIOMTC</em>, <em>79</em>(3),
2171–2183. (<a href="https://doi.org/10.1111/biom.13756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildlife monitoring for open populations can be performed using a number of different survey methods. Each survey method gives rise to a type of data and, in the last five decades, a large number of associated statistical models have been developed for analyzing these data. Although these models have been parameterized and fitted using different approaches, they have all been designed to either model the pattern with which individuals enter and/or exit the population, or to estimate the population size by accounting for the corresponding observation process, or both. However, existing approaches rely on a predefined model structure and complexity, either by assuming that parameters linked to the entry and exit pattern (EEP) are specific to sampling occasions, or by employing parametric curves to describe the EEP. Instead, we propose a novel Bayesian nonparametric framework for modeling EEPs based on the Polya tree (PT) prior for densities. Our Bayesian nonparametric approach avoids overfitting when inferring EEPs, while simultaneously allowing more flexibility than is possible using parametric curves. Finally, we introduce the replicate PT prior for defining classes of models for these data allowing us to impose constraints on the EEPs, when required. We demonstrate our new approach using capture–recapture, count, and ring-recovery data for two different case studies.},
  archive      = {J_BIOMTC},
  author       = {Alex Diana and Eleni Matechou and Jim Griffin and Todd Arnold and Simone Tenan and Stefano Volponi},
  doi          = {10.1111/biom.13756},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2171-2183},
  shortjournal = {Biometrics},
  title        = {A general modeling framework for open wildlife populations based on the polya tree prior},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Center-augmented ℓ2-type regularization for subgroup
learning. <em>BIOMTC</em>, <em>79</em>(3), 2157–2170. (<a
href="https://doi.org/10.1111/biom.13725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing methods for subgroup analysis can be roughly divided into two categories: finite mixture models (FMM) and regularization methods with an ℓ 1 -type penalty. In this paper, by introducing the group centers and ℓ 2 -type penalty in the loss function, we propose a novel center-augmented regularization (CAR) method; this method can be regarded as a unification of the regularization method and FMM and hence exhibits higher efficiency and robustness and simpler computations than the existing methods. In particular, its computational complexity is reduced from the O ( n 2 ) $O(n^2)$ of the conventional pairwise-penalty method to only O ( n K ) $O(nK)$ , where n is the sample size and K is the number of subgroups. The asymptotic normality of CAR is established, and the convergence of the algorithm is proven. CAR is applied to a dataset from a multicenter clinical trial, Buprenorphine in the Treatment of Opiate Dependence; a larger R 2 is produced and three additional significant variables are identified compared to those of the existing methods.},
  archive      = {J_BIOMTC},
  author       = {Ye He and Ling Zhou and Yingcun Xia and Huazhen Lin},
  doi          = {10.1111/biom.13725},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2157-2170},
  shortjournal = {Biometrics},
  title        = {Center-augmented ℓ2-type regularization for subgroup learning},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Double reduction estimation and equilibrium tests in natural
autopolyploid populations. <em>BIOMTC</em>, <em>79</em>(3), 2143–2156.
(<a href="https://doi.org/10.1111/biom.13722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many bioinformatics pipelines include tests for equilibrium. Tests for diploids are well studied and widely available, but extending these approaches to autopolyploids is hampered by the presence of double reduction, the comigration of sister chromatid segments into the same gamete during meiosis. Though a hindrance for equilibrium tests, double reduction rates are quantities of interest in their own right, as they provide insights about the meiotic behavior of autopolyploid organisms. Here, we develop procedures to (i) test for equilibrium while accounting for double reduction, and (ii) estimate the double reduction rate given equilibrium. To do so, we take two approaches: a likelihood approach, and a novel U -statistic minimization approach that we show generalizes the classical equilibrium χ 2 test in diploids. For small sample sizes and uncertain genotypes, we further develop a bootstrap procedure based on our U -statistic to test for equilibrium. We validate our methods on both simulated and real data.},
  archive      = {J_BIOMTC},
  author       = {David Gerard},
  doi          = {10.1111/biom.13722},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2143-2156},
  shortjournal = {Biometrics},
  title        = {Double reduction estimation and equilibrium tests in natural autopolyploid populations},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pair-switching rerandomization. <em>BIOMTC</em>,
<em>79</em>(3), 2127–2142. (<a
href="https://doi.org/10.1111/biom.13712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rerandomization discards assignments with covariates unbalanced in the treatment and control groups to improve estimation and inference efficiency. However, the acceptance-rejection sampling method used in rerandomization is computationally inefficient. As a result, it is time-consuming for rerandomization to draw numerous independent assignments, which are necessary for performing Fisher randomization tests and constructing randomization-based confidence intervals. To address this problem, we propose a pair-switching rerandomization (PSRR) method to draw balanced assignments efficiently. We obtain the unbiasedness and variance reduction of the difference-in-means estimator and show that the Fisher randomization tests are valid under PSRR. Moreover, we propose an exact approach to invert Fisher randomization tests to confidence intervals, which is faster than the existing methods. In addition, our method is applicable to both nonsequentially and sequentially randomized experiments. We conduct comprehensive simulation studies to compare the finite-sample performance of the proposed method with that of classical rerandomization. Simulation results indicate that PSRR leads to comparable power of Fisher randomization tests and is 3–23 times faster than classical rerandomization. Finally, we apply the PSRR method to analyze two clinical trial datasets, both of which demonstrate the advantages of our method.},
  archive      = {J_BIOMTC},
  author       = {Ke Zhu and Hanzhong Liu},
  doi          = {10.1111/biom.13712},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2127-2142},
  shortjournal = {Biometrics},
  title        = {Pair-switching rerandomization},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general framework for subgroup detection via one-step
value difference estimation. <em>BIOMTC</em>, <em>79</em>(3), 2116–2126.
(<a href="https://doi.org/10.1111/biom.13711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent statistical methodology for precision medicine has focused on either identification of subgroups with enhanced treatment effects or estimating optimal treatment decision rules so that treatment is allocated in a way that maximizes, on average, predefined patient outcomes. Less attention has been given to subgroup testing, which involves evaluation of whether at least a subgroup of the population benefits from an investigative treatment, compared to some control or standard of care. In this work, we propose a general framework for testing for the existence of a subgroup with enhanced treatment effects based on the difference of the estimated value functions under an estimated optimal treatment regime and a fixed regime that assigns everyone to the same treatment. Our proposed test does not require specification of the parametric form of the subgroup and allows heterogeneous treatment effects within the subgroup. The test applies to cases when the outcome of interest is either a time-to-event or a (uncensored) scalar, and is valid at the exceptional law. To demonstrate the empirical performance of the proposed test, we study the type I error and power of the test statistics in simulations and also apply our test to data from a Phase III trial in patients with hematological malignancies.},
  archive      = {J_BIOMTC},
  author       = {Dana Johnson and Wenbin Lu and Marie Davidian},
  doi          = {10.1111/biom.13711},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2116-2126},
  shortjournal = {Biometrics},
  title        = {A general framework for subgroup detection via one-step value difference estimation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic analysis of irregularly or sparsely sampled curves.
<em>BIOMTC</em>, <em>79</em>(3), 2103–2115. (<a
href="https://doi.org/10.1111/biom.13706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide statistical analysis methods for samples of curves in two or more dimensions, where the image, but not the parameterization of the curves, is of interest and suitable alignment/registration is thus necessary. Examples are handwritten letters, movement paths, or object outlines. We focus in particular on the computation of (smooth) means and distances, allowing, for example, classification or clustering. Existing parameterization invariant analysis methods based on the elastic distance of the curves modulo parameterization, using the square-root-velocity framework, have limitations in common realistic settings where curves are irregularly and potentially sparsely observed. We propose using spline curves to model smooth or polygonal (Fréchet) means of open or closed curves with respect to the elastic distance and show identifiability of the spline model modulo parameterization. We further provide methods and algorithms to approximate the elastic distance for irregularly or sparsely observed curves, via interpreting them as polygons. We illustrate the usefulness of our methods on two datasets. The first application classifies irregularly sampled spirals drawn by Parkinson&#39;s patients and healthy controls, based on the elastic distance to a mean spiral curve computed using our approach. The second application clusters sparsely sampled GPS tracks based on the elastic distance and computes smooth cluster means to find new paths on the Tempelhof field in Berlin. All methods are implemented in the R-package “elasdics” and evaluated in simulations.},
  archive      = {J_BIOMTC},
  author       = {Lisa Steyer and Almond Stöcker and Sonja Greven},
  doi          = {10.1111/biom.13706},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2103-2115},
  shortjournal = {Biometrics},
  title        = {Elastic analysis of irregularly or sparsely sampled curves},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusting for publication bias in meta-analysis via inverse
probability weighting using clinical trial registries. <em>BIOMTC</em>,
<em>79</em>(3), 2089–2102. (<a
href="https://doi.org/10.1111/biom.13822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publication bias is a major concern in conducting systematic reviews and meta-analyses. Various sensitivity analysis or bias-correction methods have been developed based on selection models, and they have some advantages over the widely used trim-and-fill bias-correction method. However, likelihood methods based on selection models may have difficulty in obtaining precise estimates and reasonable confidence intervals, or require a rather complicated sensitivity analysis process. Herein, we develop a simple publication bias adjustment method by utilizing the information on conducted but still unpublished trials from clinical trial registries. We introduce an estimating equation for parameter estimation in the selection function by regarding the publication bias issue as a missing data problem under the missing not at random assumption. With the estimated selection function, we introduce the inverse probability weighting (IPW) method to estimate the overall mean across studies. Furthermore, the IPW versions of heterogeneity measures such as the between-study variance and the I 2 measure are proposed. We propose methods to construct confidence intervals based on asymptotic normal approximation as well as on parametric bootstrap. Through numerical experiments, we observed that the estimators successfully eliminated bias, and the confidence intervals had empirical coverage probabilities close to the nominal level. On the other hand, the confidence interval based on asymptotic normal approximation is much wider in some scenarios than the bootstrap confidence interval. Therefore, the latter is recommended for practical use.},
  archive      = {J_BIOMTC},
  author       = {Ao Huang and Kosuke Morikawa and Tim Friede and Satoshi Hattori},
  doi          = {10.1111/biom.13822},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2089-2102},
  shortjournal = {Biometrics},
  title        = {Adjusting for publication bias in meta-analysis via inverse probability weighting using clinical trial registries},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying alert concentrations using a model-based
bootstrap approach. <em>BIOMTC</em>, <em>79</em>(3), 2076–2088. (<a
href="https://doi.org/10.1111/biom.13799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of alert concentrations, where a pre-specified threshold of the response variable is exceeded, is an important goal of concentration–response studies. The traditional approach is based on investigating the measured concentrations and attaining statistical significance of the alert concentration by using a multiple t -test procedure. In this paper, we propose a new model-based method to identify alert concentrations, based on fitting a concentration–response curve and constructing a simultaneous confidence band for the difference of the response of a concentration compared to the control. In order to obtain these confidence bands, we use a bootstrap approach which can be applied to any functional form of the concentration–response curve. This particularly offers the possibility to investigate also those situations where the concentration–response relationship is not monotone and, moreover, to detect alerts at concentrations which were not measured during the study, providing a highly flexible framework for the problem at hand.},
  archive      = {J_BIOMTC},
  author       = {Kathrin Möllenhoff and Kirsten Schorning and Franziska Kappenberg},
  doi          = {10.1111/biom.13799},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2076-2088},
  shortjournal = {Biometrics},
  title        = {Identifying alert concentrations using a model-based bootstrap approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric scanning tests of homogeneity for hierarchical
models with continuous covariates. <em>BIOMTC</em>, <em>79</em>(3),
2063–2075. (<a href="https://doi.org/10.1111/biom.13801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications of hierarchical models, there is often interest in evaluating the inherent heterogeneity in view of observed data. When the underlying hypothesis involves parameters resting on the boundary of their support space such as variances and mixture proportions, it is a usual practice to entertain testing procedures that rely on common heterogeneity assumptions. Such procedures, albeit omnibus for general alternatives, may entail a substantial loss of power for specific alternatives such as heterogeneity varying with covariates. We introduce a novel and flexible approach that uses covariate information to improve the power to detect heterogeneity, without imposing unnecessary restrictions. With continuous covariates, the approach does not impose a regression model relating heterogeneity parameters to covariates or rely on arbitrary discretizations. Instead, a scanning approach requiring continuous dichotomizations of the covariates is proposed. Empirical processes resulting from these dichotomizations are then used to construct the test statistics, with limiting null distributions shown to be functionals of tight random processes. We illustrate our proposals and results on a popular class of two-component mixture models, followed by simulation studies and applications to two real datasets in cancer and caries research.},
  archive      = {J_BIOMTC},
  author       = {David Todem and Wei-Wen Hsu and KyungMann Kim},
  doi          = {10.1111/biom.13801},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2063-2075},
  shortjournal = {Biometrics},
  title        = {Nonparametric scanning tests of homogeneity for hierarchical models with continuous covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequentist model averaging for undirected gaussian
graphical models. <em>BIOMTC</em>, <em>79</em>(3), 2050–2062. (<a
href="https://doi.org/10.1111/biom.13758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in information technologies have made network data increasingly frequent in a spectrum of big data applications, which is often explored by probabilistic graphical models. To precisely estimate the precision matrix, we propose an optimal model averaging estimator for Gaussian graphs. We prove that the proposed estimator is asymptotically optimal when candidate models are misspecified. The consistency and the asymptotic distribution of model averaging estimator, and the weight convergence are also studied when at least one correct model is included in the candidate set. Furthermore, numerical simulations and a real data analysis on yeast genetic data are conducted to illustrate that the proposed method is promising.},
  archive      = {J_BIOMTC},
  author       = {Huihang Liu and Xinyu Zhang},
  doi          = {10.1111/biom.13758},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2050-2062},
  shortjournal = {Biometrics},
  title        = {Frequentist model averaging for undirected gaussian graphical models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile regression for nonignorable missing data with its
application of analyzing electronic medical records. <em>BIOMTC</em>,
<em>79</em>(3), 2036–2049. (<a
href="https://doi.org/10.1111/biom.13723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, there has been growing enthusiasm for using electronic medical records (EMRs) for biomedical research. Quantile regression estimates distributional associations, providing unique insights into the intricacies and heterogeneity of the EMR data. However, the widespread nonignorable missing observations in EMR often obscure the true associations and challenge its potential for robust biomedical discoveries. We propose a novel method to estimate the covariate effects in the presence of nonignorable missing responses under quantile regression. This method imposes no parametric specifications on response distributions, which subtly uses implicit distributions induced by the corresponding quantile regression models. We show that the proposed estimator is consistent and asymptotically normal. We also provide an efficient algorithm to obtain the proposed estimate and a randomly weighted bootstrap approach for statistical inferences. Numerical studies, including an empirical analysis of real-world EMR data, are used to assess the proposed method&#39;s finite-sample performance compared to existing literature.},
  archive      = {J_BIOMTC},
  author       = {Aiai Yu and Yujie Zhong and Xingdong Feng and Ying Wei},
  doi          = {10.1111/biom.13723},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2036-2049},
  shortjournal = {Biometrics},
  title        = {Quantile regression for nonignorable missing data with its application of analyzing electronic medical records},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust approach for electronic health record–based
case-control studies with contaminated case pools. <em>BIOMTC</em>,
<em>79</em>(3), 2023–2035. (<a
href="https://doi.org/10.1111/biom.13721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider analyses of case-control studies assembled from electronic health records (EHRs) where the pool of cases is contaminated by patients who are ineligible for the study. These ineligible patients, referred to as “false cases,” should be excluded from the analyses if known. However, the true outcome status of a patient in the case pool is unknown except in a subset whose size may be arbitrarily small compared to the entire pool. To effectively remove the influence of the false cases on estimating odds ratio parameters defined by a working association model of the logistic form, we propose a general strategy to adaptively impute the unknown case status without requiring a correct phenotyping model to help discern the true and false case statuses. Our method estimates the target parameters as the solution to a set of unbiased estimating equations constructed using all available data. It outperforms existing methods by achieving robustness to mismodeling the relationship between the outcome status and covariates of interest, as well as improved estimation efficiency. We further show that our estimator is root- n -consistent and asymptotically normal. Through extensive simulation studies and analysis of real EHR data, we demonstrate that our method has desirable robustness to possible misspecification of both the association and phenotyping models, along with statistical efficiency superior to the competitors.},
  archive      = {J_BIOMTC},
  author       = {Guorong Dai and Yanyuan Ma and Jill Hasler and Jinbo Chen and Raymond J. Carroll},
  doi          = {10.1111/biom.13721},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2023-2035},
  shortjournal = {Biometrics},
  title        = {A robust approach for electronic health record–based case-control studies with contaminated case pools},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semiparametric joint model for cluster size and
subunit-specific interval-censored outcomes. <em>BIOMTC</em>,
<em>79</em>(3), 2010–2022. (<a
href="https://doi.org/10.1111/biom.13795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered data frequently arise in biomedical studies, where observations, or subunits, measured within a cluster are associated. The cluster size is said to be informative, if the outcome variable is associated with the number of subunits in a cluster. In most existing work, the informative cluster size issue is handled by marginal approaches based on within-cluster resampling, or cluster-weighted generalized estimating equations. Although these approaches yield consistent estimation of the marginal models, they do not allow estimation of within-cluster associations and are generally inefficient. In this paper, we propose a semiparametric joint model for clustered interval-censored event time data with informative cluster size. We use a random effect to account for the association among event times of the same cluster as well as the association between event times and the cluster size. For estimation, we propose a sieve maximum likelihood approach and devise a computationally-efficient expectation-maximization algorithm for implementation. The estimators are shown to be strongly consistent, with the Euclidean components being asymptotically normal and achieving semiparametric efficiency. Extensive simulation studies are conducted to evaluate the finite-sample performance, efficiency and robustness of the proposed method. We also illustrate our method via application to a motivating periodontal disease dataset.},
  archive      = {J_BIOMTC},
  author       = {Chun Yin Lee and Kin Yau Wong and Kwok Fai Lam and Dipankar Bandyopadhyay},
  doi          = {10.1111/biom.13795},
  journal      = {Biometrics},
  number       = {3},
  pages        = {2010-2022},
  shortjournal = {Biometrics},
  title        = {A semiparametric joint model for cluster size and subunit-specific interval-censored outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric estimation of the transformation model by
leveraging external aggregate data in the presence of population
heterogeneity. <em>BIOMTC</em>, <em>79</em>(3), 1996–2009. (<a
href="https://doi.org/10.1111/biom.13778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging information in aggregate data from external sources to improve estimation efficiency and prediction accuracy with smaller scale studies has drawn a great deal of attention in recent years. Yet, conventional methods often either ignore uncertainty in the external information or fail to account for the heterogeneity between internal and external studies. This article proposes an empirical likelihood-based framework to improve the estimation of the semiparametric transformation models by incorporating information about the t -year subgroup survival probability from external sources. The proposed estimation procedure incorporates an additional likelihood component to account for uncertainty in the external information and employs a density ratio model to characterize population heterogeneity. We establish the consistency and asymptotic normality of the proposed estimator and show that it is more efficient than the conventional pseudopartial likelihood estimator without combining information. Simulation studies show that the proposed estimator yields little bias and outperforms the conventional approach even in the presence of information uncertainty and heterogeneity. The proposed methodologies are illustrated with an analysis of a pancreatic cancer study.},
  archive      = {J_BIOMTC},
  author       = {Yu-Jen Cheng and Yen-Chun Liu and Chang-Yu Tsai and Chiung-Yu Huang},
  doi          = {10.1111/biom.13778},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1996-2009},
  shortjournal = {Biometrics},
  title        = {Semiparametric estimation of the transformation model by leveraging external aggregate data in the presence of population heterogeneity},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining parametric and nonparametric models to estimate
treatment effects in observational studies. <em>BIOMTC</em>,
<em>79</em>(3), 1986–1995. (<a
href="https://doi.org/10.1111/biom.13776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing causal inference in observational studies requires we assume confounding variables are correctly adjusted for. In settings with few discrete-valued confounders, standard models can be employed. However, as the number of confounders increases these models become less feasible as there are fewer observations available for each unique combination of confounding variables. In this paper, we propose a new model for estimating treatment effects in observational studies that incorporates both parametric and nonparametric outcome models. By conceptually splitting the data, we can combine these models while maintaining a conjugate framework, allowing us to avoid the use of Markov chain Monte Carlo (MCMC) methods. Approximations using the central limit theorem and random sampling allow our method to be scaled to high-dimensional confounders. Through simulation studies we show our method can be competitive with benchmark models while maintaining efficient computation, and illustrate the method on a large epidemiological health survey.},
  archive      = {J_BIOMTC},
  author       = {Daniel Daly-Grafstein and Paul Gustafson},
  doi          = {10.1111/biom.13776},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1986-1995},
  shortjournal = {Biometrics},
  title        = {Combining parametric and nonparametric models to estimate treatment effects in observational studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multidimensional adaptive p-splines with application to
neurons’ activity studies. <em>BIOMTC</em>, <em>79</em>(3), 1972–1985.
(<a href="https://doi.org/10.1111/biom.13755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The receptive field (RF) of a visual neuron is the region of the space that elicits neuronal responses. It can be mapped using different techniques that allow inferring its spatial and temporal properties. Raw RF maps (RFmaps) are usually noisy, making it difficult to obtain and study important features of the RF. A possible solution is to smooth them using P-splines. Yet, raw RFmaps are characterized by sharp transitions in both space and time. Their analysis thus asks for spatiotemporal adaptive P-spline models, where smoothness can be locally adapted to the data. However, the literature lacks proposals for adaptive P-splines in more than two dimensions. Furthermore, the extra flexibility afforded by adaptive P-spline models is obtained at the cost of a high computational burden, especially in a multidimensional setting. To fill these gaps, this work presents a novel anisotropic locally adaptive P-spline model in two (e.g., space) and three (space and time) dimensions. Estimation is based on the recently proposed SOP (Separation of Overlapping Precision matrices) method, which provides the speed we look for. Besides the spatiotemporal analysis of the neuronal activity data that motivated this work, the practical performance of the proposal is evaluated through simulations, and comparisons with alternative methods are reported.},
  archive      = {J_BIOMTC},
  author       = {María Xosé Rodríguez-Álvarez and María Durbán and Paul H.C. Eilers and Dae-Jin Lee and Francisco Gonzalez},
  doi          = {10.1111/biom.13755},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1972-1985},
  shortjournal = {Biometrics},
  title        = {Multidimensional adaptive P-splines with application to neurons&#39; activity studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint semiparametric models for case-cohort designs.
<em>BIOMTC</em>, <em>79</em>(3), 1959–1971. (<a
href="https://doi.org/10.1111/biom.13728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase studies such as case-cohort and nested case-control studies are widely used cost-effective sampling strategies. In the first phase, the observed failure/censoring time and inexpensive exposures are collected. In the second phase, a subgroup of subjects is selected for measurements of expensive exposures based on the information from the first phase. One challenging issue is how to utilize all the available information to conduct efficient regression analyses of the two-phase study data. This paper proposes a joint semiparametric modeling of the survival outcome and the expensive exposures. Specifically, we assume a class of semiparametric transformation models and a semiparametric density ratio model for the survival outcome and the expensive exposures, respectively. The class of semiparametric transformation models includes the proportional hazards model and the proportional odds model as special cases. The density ratio model is flexible in modeling multivariate mixed-type data. We develop efficient likelihood-based estimation and inference procedures and establish the large sample properties of the nonparametric maximum likelihood estimators. Extensive numerical studies reveal that the proposed methods perform well under practical settings. The proposed methods also appear to be reasonably robust under various model mis-specifications. An application to the National Wilms Tumor Study is provided.},
  archive      = {J_BIOMTC},
  author       = {Weibin Zhong and Guoqing Diao},
  doi          = {10.1111/biom.13728},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1959-1971},
  shortjournal = {Biometrics},
  title        = {Joint semiparametric models for case-cohort designs},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor response quantile regression with neuroimaging data.
<em>BIOMTC</em>, <em>79</em>(3), 1947–1958. (<a
href="https://doi.org/10.1111/biom.13809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting neuroimaging data in the form of tensors (i.e. multidimensional arrays) has become more common in mental health studies, driven by an increasing interest in studying the associations between neuroimaging phenotypes and clinical disease manifestation. Motivated by a neuroimaging study of post-traumatic stress disorder (PTSD) from the Grady Trauma Project, we study a tensor response quantile regression framework, which enables novel analyses that confer a detailed view of the potentially heterogeneous association between a neuroimaging phenotype and relevant clinical predictors. We adopt a sensible low-rank structure to represent the association of interest, and propose a simple two-step estimation procedure which is easy to implement with existing software. We provide rigorous theoretical justifications for the intuitive two-step procedure. Simulation studies demonstrate good performance of the proposed method with realistic sample sizes in neuroimaging studies. We conduct the proposed tensor response quantile regression analysis of the motivating PTSD study to investigate the association between fMRI resting-state functional connectivity and PTSD symptom severity. Our results uncover non-homogeneous effects of PTSD symptoms on brain functional connectivity, which cannot be captured by existing tensor response methods.},
  archive      = {J_BIOMTC},
  author       = {Bo Wei and Limin Peng and Ying Guo and Amita Manatunga and Jennifer Stevens},
  doi          = {10.1111/biom.13809},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1947-1958},
  shortjournal = {Biometrics},
  title        = {Tensor response quantile regression with neuroimaging data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient targeted learning of heterogeneous treatment
effects for multiple subgroups. <em>BIOMTC</em>, <em>79</em>(3),
1934–1946. (<a href="https://doi.org/10.1111/biom.13800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical science, analyzing treatment effect heterogeneity plays an essential role in assisting personalized medicine. The main goals of analyzing treatment effect heterogeneity include estimating treatment effects in clinically relevant subgroups and predicting whether a patient subpopulation might benefit from a particular treatment. Conventional approaches often evaluate the subgroup treatment effects via parametric modeling and can thus be susceptible to model mis-specifications. In this paper, we take a model-free semiparametric perspective and aim to efficiently evaluate the heterogeneous treatment effects of multiple subgroups simultaneously under the one-step targeted maximum-likelihood estimation (TMLE) framework. When the number of subgroups is large, we further expand this path of research by looking at a variation of the one-step TMLE that is robust to the presence of small estimated propensity scores in finite samples. From our simulations, our method demonstrates substantial finite sample improvements compared to conventional methods. In a case study, our method unveils the potential treatment effect heterogeneity of rs12916-T allele (a proxy for statin usage) in decreasing Alzheimer&#39;s disease risk.},
  archive      = {J_BIOMTC},
  author       = {Waverly Wei and Maya Petersen and Mark J van der Laan and Zeyu Zheng and Chong Wu and Jingshen Wang},
  doi          = {10.1111/biom.13800},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1934-1946},
  shortjournal = {Biometrics},
  title        = {Efficient targeted learning of heterogeneous treatment effects for multiple subgroups},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Change-plane analysis for subgroup detection with a
continuous treatment. <em>BIOMTC</em>, <em>79</em>(3), 1920–1933. (<a
href="https://doi.org/10.1111/biom.13762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and characterizing subgroups with differential effects of a binary treatment has been widely studied and led to improvements in patient outcomes and population risk management. Under the setting of a continuous treatment, however, such investigations remain scarce. We propose a semiparametric change-plane model and consequently a doubly robust test statistic for assessing the existence of two subgroups with differential treatment effects under a continuous treatment. The proposed testing procedure is valid when either the baseline function for the covariate effects or the generalized propensity score function for the continuous treatment is correctly specified. The asymptotic distributions of the test statistic under the null and local alternative hypotheses are established. When the null hypothesis of no subgroup is rejected, the change-plane parameters that define the subgroups can be estimated. This paper provides a unified framework of the change-plane method to handle various types of outcomes, including the exponential family of distributions and time-to-event outcomes. Additional extensions with nonparametric estimation approaches are also provided. We evaluate the performance of our proposed methods through extensive simulation studies under various scenarios. An application to the Health Effects of Arsenic Longitudinal Study with a continuous environmental exposure of arsenic is presented.},
  archive      = {J_BIOMTC},
  author       = {Peng Jin and Wenbin Lu and Yu Chen and Mengling Liu},
  doi          = {10.1111/biom.13762},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1920-1933},
  shortjournal = {Biometrics},
  title        = {Change-plane analysis for subgroup detection with a continuous treatment},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal multiple testing and design in clinical trials.
<em>BIOMTC</em>, <em>79</em>(3), 1908–1919. (<a
href="https://doi.org/10.1111/biom.13726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central goal in designing clinical trials is to find the test that maximizes power (or equivalently minimizes required sample size) for finding a false null hypothesis subject to the constraint of type I error. When there is more than one test, such as in clinical trials with multiple endpoints, the issues of optimal design and optimal procedures become more complex. In this paper, we address the question of how such optimal tests should be defined and how they can be found. We review different notions of power and how they relate to study goals, and also consider the requirements of type I error control and the nature of the procedures. This leads us to an explicit optimization problem with objective and constraints that describe its specific desiderata. We present a complete solution for deriving optimal procedures for two hypotheses, which have desired monotonicity properties, and are computationally simple. For some of the optimization formulations this yields optimal procedures that are identical to existing procedures, such as Hommel&#39;s procedure or the procedure of Bittman et al. (2009), while for other cases it yields completely novel and more powerful procedures than existing ones. We demonstrate the nature of our novel procedures and their improved power extensively in a simulation and on the APEX study (Cohen et al., 2016).},
  archive      = {J_BIOMTC},
  author       = {Ruth Heller and Abba Krieger and Saharon Rosset},
  doi          = {10.1111/biom.13726},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1908-1919},
  shortjournal = {Biometrics},
  title        = {Optimal multiple testing and design in clinical trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrasting principal stratum and hypothetical strategy
estimands in multi-period crossover trials with incomplete data.
<em>BIOMTC</em>, <em>79</em>(3), 1896–1907. (<a
href="https://doi.org/10.1111/biom.13777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complete case analyses of complete crossover designs provide an opportunity to make comparisons based on patients who can tolerate all treatments. It is argued that this provides a means of estimating a principal stratum strategy estimand, something which is difficult to do in parallel group trials. While some trial users will consider this a relevant aim, others may be interested in hypothetical strategy estimands, that is, the effect that would be found if all patients completed the trial. Whether these estimands differ importantly is a question of interest to the different users of the trial results. This paper derives the difference between principal stratum strategy and hypothetical strategy estimands, where the former is estimated by a complete-case analysis of the crossover design, and a model for the dropout process is assumed. Complete crossover designs, that is, those where all treatments appear in all sequences, and which compare t treatments over p periods with respect to a continuous outcome are considered. Numerical results are presented for Williams designs with four and six periods. Results from a trial of obstructive sleep apnoea-hypopnoea (TOMADO) are also used for illustration. The results demonstrate that the percentage difference between the estimands is modest, exceeding 5\% only when the trial has been severely affected by dropouts or if the within-subject correlation is low.},
  archive      = {J_BIOMTC},
  author       = {John N.S. Matthews and Sofia Bazakou and Robin Henderson and Linda D. Sharples},
  doi          = {10.1111/biom.13777},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1896-1907},
  shortjournal = {Biometrics},
  title        = {Contrasting principal stratum and hypothetical strategy estimands in multi-period crossover trials with incomplete data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous functional linear regression models for
longitudinal data in reproducing kernel hilbert space. <em>BIOMTC</em>,
<em>79</em>(3), 1880–1895. (<a
href="https://doi.org/10.1111/biom.13767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of longitudinal neuroimaging studies, we study the longitudinal functional linear regression model under asynchronous data setting for modeling the association between clinical outcomes and functional (or imaging) covariates. In the asynchronous data setting, both covariates and responses may be measured at irregular and mismatched time points, posing methodological challenges to existing statistical methods. We develop a kernel weighted loss function with roughness penalty to obtain the functional estimator and derive its representer theorem. The rate of convergence, a Bahadur representation, and the asymptotic pointwise distribution of the functional estimator are obtained under the reproducing kernel Hilbert space framework. We propose a penalized likelihood ratio test to test the nullity of the functional coefficient, derive its asymptotic distribution under the null hypothesis, and investigate the separation rate under the alternative hypotheses. Simulation studies are conducted to examine the finite-sample performance of the proposed procedure. We apply the proposed methods to the analysis of multitype data obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) study, which reveals significant association between 21 regional brain volume density curves and the cognitive function. Data used in preparation of this paper were obtained from the ADNI database (adni.loni.usc.edu).},
  archive      = {J_BIOMTC},
  author       = {Ting Li and Huichen Zhu and Tengfei Li and Hongtu Zhu},
  doi          = {10.1111/biom.13767},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1880-1895},
  shortjournal = {Biometrics},
  title        = {Asynchronous functional linear regression models for longitudinal data in reproducing kernel hilbert space},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grouped generalized estimating equations for longitudinal
data analysis. <em>BIOMTC</em>, <em>79</em>(3), 1868–1879. (<a
href="https://doi.org/10.1111/biom.13718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equation (GEE) is widely adopted for regression modeling for longitudinal data, taking account of potential correlations within the same subjects. Although the standard GEE assumes common regression coefficients among all the subjects, such an assumption may not be realistic when there is potential heterogeneity in regression coefficients among subjects. In this paper, we develop a flexible and interpretable approach, called grouped GEE analysis, to modeling longitudinal data with allowing heterogeneity in regression coefficients. The proposed method assumes that the subjects are divided into a finite number of groups and subjects within the same group share the same regression coefficient. We provide a simple algorithm for grouping subjects and estimating the regression coefficients simultaneously, and show the asymptotic properties of the proposed estimator. The number of groups can be determined by the cross validation with averaging method. We demonstrate the proposed method through simulation studies and an application to a real data set.},
  archive      = {J_BIOMTC},
  author       = {Tsubasa Ito and Shonosuke Sugasawa},
  doi          = {10.1111/biom.13718},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1868-1879},
  shortjournal = {Biometrics},
  title        = {Grouped generalized estimating equations for longitudinal data analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subset selection for linear mixed models. <em>BIOMTC</em>,
<em>79</em>(3), 1853–1867. (<a
href="https://doi.org/10.1111/biom.13707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed models (LMMs) are instrumental for regression analysis with structured dependence, such as grouped, clustered, or multilevel data. However, selection among the covariates—while accounting for this structured dependence—remains a challenge. We introduce a Bayesian decision analysis for subset selection with LMMs. Using a Mahalanobis loss function that incorporates the structured dependence, we derive optimal linear coefficients for (i) any given subset of variables and (ii) all subsets of variables that satisfy a cardinality constraint. Crucially, these estimates inherit shrinkage or regularization and uncertainty quantification from the underlying Bayesian model, and apply for any well-specified Bayesian LMM. More broadly, our decision analysis strategy deemphasizes the role of a single “best” subset, which is often unstable and limited in its information content, and instead favors a collection of near-optimal subsets. This collection is summarized by key member subsets and variable-specific importance metrics. Customized subset search and out-of-sample approximation algorithms are provided for more scalable computing. These tools are applied to simulated data and a longitudinal physical activity dataset, and demonstrate excellent prediction, estimation, and selection ability.},
  archive      = {J_BIOMTC},
  author       = {Daniel R. Kowal},
  doi          = {10.1111/biom.13707},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1853-1867},
  shortjournal = {Biometrics},
  title        = {Subset selection for linear mixed models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solutions for surrogacy validation with longitudinal
outcomes for a gene therapy. <em>BIOMTC</em>, <em>79</em>(3), 1840–1852.
(<a href="https://doi.org/10.1111/biom.13720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valid surrogate endpoints S can be used as a substitute for a true outcome of interest T to measure treatment efficacy in a clinical trial. We propose a causal inference approach to validate a surrogate by incorporating longitudinal measurements of the true outcomes using a mixed modeling approach, and we define models and quantities for validation that may vary across the study period using principal surrogacy criteria. We consider a surrogate-dependent treatment efficacy curve that allows us to validate the surrogate at different time points. We extend these methods to accommodate a delayed-start treatment design where all patients eventually receive the treatment. Not all parameters are identified in the general setting. We apply a Bayesian approach for estimation and inference, utilizing more informative prior distributions for selected parameters. We consider the sensitivity of these prior assumptions as well as assumptions of independence among certain counterfactual quantities conditional on pretreatment covariates to improve identifiability. We examine the frequentist properties (bias of point and variance estimates, credible interval coverage) of a Bayesian imputation method. Our work is motivated by a clinical trial of a gene therapy where the functional outcomes are measured repeatedly throughout the trial.},
  archive      = {J_BIOMTC},
  author       = {Emily K. Roberts and Michael R. Elliott and Jeremy M. G. Taylor},
  doi          = {10.1111/biom.13720},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1840-1852},
  shortjournal = {Biometrics},
  title        = {Solutions for surrogacy validation with longitudinal outcomes for a gene therapy},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Adaptive bayesian sum of trees model for
covariate-dependent spectral analysis. <em>BIOMTC</em>, <em>79</em>(3),
1826–1839. (<a href="https://doi.org/10.1111/biom.13763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a flexible and adaptive nonparametric method for estimating the association between multiple covariates and power spectra of multiple time series. The proposed approach uses a Bayesian sum of trees model to capture complex dependencies and interactions between covariates and the power spectrum, which are often observed in studies of biomedical time series. Local power spectra corresponding to terminal nodes within trees are estimated nonparametrically using Bayesian penalized linear splines. The trees are considered to be random and fit using a Bayesian backfitting Markov chain Monte Carlo (MCMC) algorithm that sequentially considers tree modifications via reversible-jump MCMC techniques. For high-dimensional covariates, a sparsity-inducing Dirichlet hyperprior on tree splitting proportions is considered, which provides sparse estimation of covariate effects and efficient variable selection. By averaging over the posterior distribution of trees, the proposed method can recover both smooth and abrupt changes in the power spectrum across multiple covariates. Empirical performance is evaluated via simulations to demonstrate the proposed method&#39;s ability to accurately recover complex relationships and interactions. The proposed methodology is used to study gait maturation in young children by evaluating age-related changes in power spectra of stride interval time series in the presence of other covariates.},
  archive      = {J_BIOMTC},
  author       = {Yakun Wang and Zeda Li and Scott A. Bruce},
  doi          = {10.1111/biom.13763},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1826-1839},
  shortjournal = {Biometrics},
  title        = {Adaptive bayesian sum of trees model for covariate-dependent spectral analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian regression analysis of skewed tensor responses.
<em>BIOMTC</em>, <em>79</em>(3), 1814–1825. (<a
href="https://doi.org/10.1111/biom.13743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor regression analysis is finding vast emerging applications in a variety of clinical settings, including neuroimaging, genomics, and dental medicine. The motivation for this paper is a study of periodontal disease (PD) with an order-3 tensor response: multiple biomarkers measured at prespecified tooth–sites within each tooth, for each participant. A careful investigation would reveal considerable skewness in the responses, in addition to response missingness. To mitigate the shortcomings of existing analysis tools, we propose a new Bayesian tensor response regression method that facilitates interpretation of covariate effects on both marginal and joint distributions of highly skewed tensor responses, and accommodates missing-at-random responses under a closure property of our tensor model. Furthermore, we present a prudent evaluation of the overall covariate effects while identifying their possible variations on only a sparse subset of the tensor components. Our method promises Markov chain Monte Carlo (MCMC) tools that are readily implementable. We illustrate substantial advantages of our proposal over existing methods via simulation studies and application to a real data set derived from a clinical study of PD. The R package BSTN available in GitHub implements our model.},
  archive      = {J_BIOMTC},
  author       = {Inkoo Lee and Debajyoti Sinha and Qing Mai and Xin Zhang and Dipankar Bandyopadhyay},
  doi          = {10.1111/biom.13743},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1814-1825},
  shortjournal = {Biometrics},
  title        = {Bayesian regression analysis of skewed tensor responses},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrative bayesian models using post-selective inference:
A case study in radiogenomics. <em>BIOMTC</em>, <em>79</em>(3),
1801–1813. (<a href="https://doi.org/10.1111/biom.13740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative analyses based on statistically relevant associations between genomics and a wealth of intermediary phenotypes (such as imaging) provide vital insights into their clinical relevance in terms of the disease mechanisms. Estimates for uncertainty in the resulting integrative models are however unreliable unless inference accounts for the selection of these associations with accuracy. In this paper, we develop selection-aware Bayesian methods, which (1) counteract the impact of model selection bias through a “selection-aware posterior” in a flexible class of integrative Bayesian models post a selection of promising variables via ℓ 1 -regularized algorithms; (2) strike an inevitable trade-off between the quality of model selection and inferential power when the same data set is used for both selection and uncertainty estimation. Central to our methodological development, a carefully constructed conditional likelihood function deployed with a reparameterization mapping provides tractable updates when gradient-based Markov chain Monte Carlo (MCMC) sampling is used for estimating uncertainties from the selection-aware posterior. Applying our methods to a radiogenomic analysis, we successfully recover several important gene pathways and estimate uncertainties for their associations with patient survival times.},
  archive      = {J_BIOMTC},
  author       = {Snigdha Panigrahi and Shariq Mohammed and Arvind Rao and Veerabhadran Baladandayuthapani},
  doi          = {10.1111/biom.13740},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1801-1813},
  shortjournal = {Biometrics},
  title        = {Integrative bayesian models using post-selective inference: A case study in radiogenomics},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tractable bayes of skew-elliptical link models for
correlated binary data. <em>BIOMTC</em>, <em>79</em>(3), 1788–1800. (<a
href="https://doi.org/10.1111/biom.13731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated binary response data with covariates are ubiquitous in longitudinal or spatial studies. Among the existing statistical models, the most well-known one for this type of data is the multivariate probit model, which uses a Gaussian link to model dependence at the latent level. However, a symmetric link may not be appropriate if the data are highly imbalanced. Here, we propose a multivariate skew-elliptical link model for correlated binary responses, which includes the multivariate probit model as a special case. Furthermore, we perform Bayesian inference for this new model and prove that the regression coefficients have a closed-form unified skew-elliptical posterior with an elliptical prior. The new methodology is illustrated by an application to COVID-19 data from three different counties of the state of California, USA. By jointly modeling extreme spikes in weekly new cases, our results show that the spatial dependence cannot be neglected. Furthermore, the results also show that the skewed latent structure of our proposed model improves the flexibility of the multivariate probit model and provides a better fit to our highly imbalanced dataset.},
  archive      = {J_BIOMTC},
  author       = {Zhongwei Zhang and Reinaldo B. Arellano-Valle and Marc G. Genton and Raphaël Huser},
  doi          = {10.1111/biom.13731},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1788-1800},
  shortjournal = {Biometrics},
  title        = {Tractable bayes of skew-elliptical link models for correlated binary data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian multivariate mixture model for high throughput
spatial transcriptomics. <em>BIOMTC</em>, <em>79</em>(3), 1775–1787. (<a
href="https://doi.org/10.1111/biom.13727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High throughput spatial transcriptomics (HST) is a rapidly emerging class of experimental technologies that allow for profiling gene expression in tissue samples at or near single-cell resolution while retaining the spatial location of each sequencing unit within the tissue sample. Through analyzing HST data, we seek to identify sub-populations of cells within a tissue sample that may inform biological phenomena. Existing computational methods either ignore the spatial heterogeneity in gene expression profiles, fail to account for important statistical features such as skewness, or are heuristic-based network clustering methods that lack the inferential benefits of statistical modeling. To address this gap, we develop SPRUCE: a Bayesian spatial multivariate finite mixture model based on multivariate skew-normal distributions, which is capable of identifying distinct cellular sub-populations in HST data. We further implement a novel combination of Pólya–Gamma data augmentation and spatial random effects to infer spatially correlated mixture component membership probabilities without relying on approximate inference techniques. Via a simulation study, we demonstrate the detrimental inferential effects of ignoring skewness or spatial correlation in HST data. Using publicly available human brain HST data, SPRUCE outperforms existing methods in recovering expertly annotated brain layers. Finally, our application of SPRUCE to human breast cancer HST data indicates that SPRUCE can distinguish distinct cell populations within the tumor microenvironment. An R package spruce for fitting the proposed models is available through The Comprehensive R Archive Network.},
  archive      = {J_BIOMTC},
  author       = {Carter Allen and Yuzhou Chang and Brian Neelon and Won Chang and Hang J. Kim and Zihai Li and Qin Ma and Dongjun Chung},
  doi          = {10.1111/biom.13727},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1775-1787},
  shortjournal = {Biometrics},
  title        = {A bayesian multivariate mixture model for high throughput spatial transcriptomics},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-level bayesian interaction analysis for survival data
incorporating pathway information. <em>BIOMTC</em>, <em>79</em>(3),
1761–1774. (<a href="https://doi.org/10.1111/biom.13811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic interactions play an important role in the progression of complex diseases, providing explanation of variations in disease phenotype missed by main genetic effects. Comparatively, there are fewer studies on survival time, given its challenging characteristics such as censoring. In recent biomedical research, two-level analysis of both genes and their involved pathways has received much attention and been demonstrated as more effective than single-level analysis. However, such analysis is usually limited to main effects. Pathways are not isolated, and their interactions have also been suggested to have important contributions to the prognosis of complex diseases. In this paper, we develop a novel two-level Bayesian interaction analysis approach for survival data. This approach is the first to conduct the analysis of lower-level gene–gene interactions and higher-level pathway–pathway interactions simultaneously. Significantly advancing from the existing Bayesian studies based on the Markov Chain Monte Carlo (MCMC) technique, we propose a variational inference framework based on the accelerated failure time model with effective priors to accommodate two-level selection as well as censoring. Its computational efficiency is much desirable for high-dimensional interaction analysis. We examine performance of the proposed approach using extensive simulation. The application to TCGA melanoma and lung adenocarcinoma data leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.},
  archive      = {J_BIOMTC},
  author       = {Xing Qin and Shuangge Ma and Mengyun Wu},
  doi          = {10.1111/biom.13811},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1761-1774},
  shortjournal = {Biometrics},
  title        = {Two-level bayesian interaction analysis for survival data incorporating pathway information},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Nonparametric inference of general while-alive estimands
for recurrent events. <em>BIOMTC</em>, <em>79</em>(3), 1749–1760. (<a
href="https://doi.org/10.1111/biom.13709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the treatment effect on recurrent events like hospitalization in the presence of death has long challenged statisticians and clinicians alike. Traditional inference on the cumulative frequency unjustly penalizes survivorship as longer survivors also tend to experience more adverse events. Expanding a recently suggested idea of the “while-alive” event rate, we consider a general class of such estimands that adjust for the length of survival without losing causal interpretation. Given a user-specified loss function that allows for arbitrary weighting, we define as estimand the average loss experienced per unit time alive within a target period and use the ratio of this loss rate to measure the effect size. Scaling the loss rate by the width of the corresponding time window gives us an alternative, and sometimes more photogenic, way of showing the data. To make inferences, we construct a nonparametric estimator for the loss rate through the cumulative loss and the restricted mean survival time and derive its influence function in closed form for variance estimation and testing. As simulations and analysis of real data from a heart failure trial both show, the while-alive approach corrects for the false attenuation of treatment effect due to patients living longer under treatment, with increased statistical power as a result. The proposed methods are implemented in the R-package WA , which is publicly available from the Comprehensive R Archive Network (CRAN).},
  archive      = {J_BIOMTC},
  author       = {Lu Mao},
  doi          = {10.1111/biom.13709},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1749-1760},
  shortjournal = {Biometrics},
  title        = {Nonparametric inference of general while-alive estimands for recurrent events},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). General independent censoring in event-driven trials with
staggered entry. <em>BIOMTC</em>, <em>79</em>(3), 1737–1748. (<a
href="https://doi.org/10.1111/biom.13710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized clinical trials with time-to-event endpoints are frequently stopped after a prespecified number of events has been observed. This practice leads to dependent data and nonrandom censoring, which can in general not be solved by conditioning on the underlying baseline information. In case of staggered study entry, matters are complicated substantially. The present paper demonstrates that the study design at hand entails general independent censoring in the counting process sense, provided that the analysis is based on study time information only. To illustrate that the filtrations must not use abundant information, we simulated data of event-driven trials and evaluated them by means of Cox regression models with covariates for the calendar times. The Breslow curves of the cumulative baseline hazard showed considerable deviations, which implies that the analysis is disturbed by conditioning on the calendar time variables. A second simulation study further revealed that Efron&#39;s classical bootstrap, unlike the (martingale-based) wild bootstrap, may lead to biased results in the given setting, as the assumption of random censoring is violated. This is exemplified by an analysis of data on immunotherapy in patients with advanced, previously treated nonsmall cell lung cancer.},
  archive      = {J_BIOMTC},
  author       = {Jasmin Rühl and Jan Beyersmann and Sarah Friedrich},
  doi          = {10.1111/biom.13710},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1737-1748},
  shortjournal = {Biometrics},
  title        = {General independent censoring in event-driven trials with staggered entry},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model uncertainty quantification in cox regression.
<em>BIOMTC</em>, <em>79</em>(3), 1726–1736. (<a
href="https://doi.org/10.1111/biom.13823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider covariate selection and the ensuing model uncertainty aspects in the context of Cox regression. The perspective we take is probabilistic, and we handle it within a Bayesian framework. One of the critical elements in variable/model selection is choosing a suitable prior for model parameters. Here, we derive the so-called conventional prior approach and propose a comprehensive implementation that results in an automatic procedure. Our simulation studies and real applications show improvements over existing literature. For the sake of reproducibility but also for its intrinsic interest for practitioners, a web application requiring minimum statistical knowledge implements the proposed approach.},
  archive      = {J_BIOMTC},
  author       = {Gonzalo García-Donato and Stefano Cabras and María Eugenia Castellanos},
  doi          = {10.1111/biom.13823},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1726-1736},
  shortjournal = {Biometrics},
  title        = {Model uncertainty quantification in cox regression},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An information ratio-based goodness-of-fit test for copula
models on censored data. <em>BIOMTC</em>, <em>79</em>(3), 1713–1725. (<a
href="https://doi.org/10.1111/biom.13807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copula is a popular method for modeling the dependence among marginal distributions in multivariate censored data. As many copula models are available, it is essential to check if the chosen copula model fits the data well for analysis. Existing approaches to testing the fitness of copula models are mainly for complete or right-censored data. No formal goodness-of-fit (GOF) test exists for interval-censored or recurrent events data. We develop a general GOF test for copula-based survival models using the information ratio (IR) to address this research gap. It can be applied to any copula family with a parametric form, such as the frequently used Archimedean, Gaussian, and D-vine families. The test statistic is easy to calculate, and the test procedure is straightforward to implement. We establish the asymptotic properties of the test statistic. The simulation results show that the proposed test controls the type-I error well and achieves adequate power when the dependence strength is moderate to high. Finally, we apply our method to test various copula models in analyzing multiple real datasets. Our method consistently separates different copula models for all these datasets in terms of model fitness.},
  archive      = {J_BIOMTC},
  author       = {Tao Sun and Yu Cheng and Ying Ding},
  doi          = {10.1111/biom.13807},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1713-1725},
  shortjournal = {Biometrics},
  title        = {An information ratio-based goodness-of-fit test for copula models on censored data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-parametric estimation of the age-at-onset distribution
from a cross-sectional sample. <em>BIOMTC</em>, <em>79</em>(3),
1701–1712. (<a href="https://doi.org/10.1111/biom.13804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a simple and innovative non-parametric approach to estimate the age-of-onset distribution for a disease from a cross-sectional sample of the population that includes individuals with prevalent disease. First, we estimate the joint distribution of two event times, the age of disease onset and the survival time after disease onset. We accommodate that individuals had to be alive at the time of the study by conditioning on their survival until the age at sampling. We propose a computationally efficient expectation–maximization (EM) algorithm and derive the asymptotic properties of the resulting estimates. From these joint probabilities we then obtain non-parametric estimates of the age-at-onset distribution by marginalizing over the survival time after disease onset to death. The method accommodates categorical covariates and can be used to obtain unbiased estimates of the covariate distribution in the source population. We show in simulations that our method performs well in finite samples even under large amounts of truncation for prevalent cases. We apply the proposed method to data from female participants in the Washington Ashkenazi Study to estimate the age-at-onset distribution of breast cancer associated with carrying BRCA1 or BRCA2 mutations.},
  archive      = {J_BIOMTC},
  author       = {S. Mandal and J. Qin and R.M. Pfeiffer},
  doi          = {10.1111/biom.13804},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1701-1712},
  shortjournal = {Biometrics},
  title        = {Non-parametric estimation of the age-at-onset distribution from a cross-sectional sample},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved semiparametric estimation of the proportional rate
model with recurrent event data. <em>BIOMTC</em>, <em>79</em>(3),
1686–1700. (<a href="https://doi.org/10.1111/biom.13788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its robustness properties, marginal interpretations, and ease of implementation, the pseudo-partial likelihood method proposed in the seminal papers of Pepe and Cai and Lin et al. has become the default approach for analyzing recurrent event data with Cox-type proportional rate models. However, the construction of the pseudo-partial score function ignores the dependency among recurrent events and thus can be inefficient. An attempt to investigate the asymptotic efficiency of weighted pseudo-partial likelihood estimation found that the optimal weight function involves the unknown variance–covariance process of the recurrent event process and may not have closed-form expression. Thus, instead of deriving the optimal weights, we propose to combine a system of pre-specified weighted pseudo-partial score equations via the generalized method of moments and empirical likelihood estimation. We show that a substantial efficiency gain can be easily achieved without imposing additional model assumptions. More importantly, the proposed estimation procedures can be implemented with existing software. Theoretical and numerical analyses show that the empirical likelihood estimator is more appealing than the generalized method of moments estimator when the sample size is sufficiently large. An analysis of readmission risk in colorectal cancer patients is presented to illustrate the proposed methodology.},
  archive      = {J_BIOMTC},
  author       = {Ming-Yueh Huang and Chiung-Yu Huang},
  doi          = {10.1111/biom.13788},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1686-1700},
  shortjournal = {Biometrics},
  title        = {Improved semiparametric estimation of the proportional rate model with recurrent event data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Marginal proportional hazards models for clustered
interval-censored data with time-dependent covariates. <em>BIOMTC</em>,
<em>79</em>(3), 1670–1685. (<a
href="https://doi.org/10.1111/biom.13787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Botswana Combination Prevention Project was a cluster-randomized HIV prevention trial whose follow-up period coincided with Botswana&#39;s national adoption of a universal test and treat strategy for HIV management. Of interest is whether, and to what extent, this change in policy modified the preventative effects of the study intervention. To address such questions, we adopt a stratified proportional hazards model for clustered interval-censored data with time-dependent covariates and develop a composite expectation maximization algorithm that facilitates estimation of model parameters without placing parametric assumptions on either the baseline hazard functions or the within-cluster dependence structure. We show that the resulting estimators for the regression parameters are consistent and asymptotically normal. We also propose and provide theoretical justification for the use of the profile composite likelihood function to construct a robust sandwich estimator for the variance. We characterize the finite-sample performance and robustness of these estimators through extensive simulation studies. Finally, we conclude by applying this stratified proportional hazards model to a re-analysis of the Botswana Combination Prevention Project, with the national adoption of a universal test and treat strategy now modeled as a time-dependent covariate.},
  archive      = {J_BIOMTC},
  author       = {Kaitlyn Cook and Wenbin Lu and Rui Wang},
  doi          = {10.1111/biom.13787},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1670-1685},
  shortjournal = {Biometrics},
  title        = {Marginal proportional hazards models for clustered interval-censored data with time-dependent covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized estimation of frailty-based illness–death models
for semi-competing risks. <em>BIOMTC</em>, <em>79</em>(3), 1657–1669.
(<a href="https://doi.org/10.1111/biom.13761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-competing risks refer to the time-to-event analysis setting, where the occurrence of a non-terminal event is subject to whether a terminal event has occurred, but not vice versa. Semi-competing risks arise in a broad range of clinical contexts, including studies of preeclampsia, a condition that may arise during pregnancy and for which delivery is a terminal event. Models that acknowledge semi-competing risks enable investigation of relationships between covariates and the joint timing of the outcomes, but methods for model selection and prediction of semi-competing risks in high dimensions are lacking. Moreover, in such settings researchers commonly analyze only a single or composite outcome, losing valuable information and limiting clinical utility—in the obstetric setting, this means ignoring valuable insight into timing of delivery after preeclampsia has onset. To address this gap, we propose a novel penalized estimation framework for frailty-based illness–death multi-state modeling of semi-competing risks. Our approach combines non-convex and structured fusion penalization, inducing global sparsity as well as parsimony across submodels. We perform estimation and model selection via a pathwise routine for non-convex optimization, and prove statistical error rate results in this setting. We present a simulation study investigating estimation error and model selection performance, and a comprehensive application of the method to joint risk modeling of preeclampsia and timing of delivery using pregnancy data from an electronic health record.},
  archive      = {J_BIOMTC},
  author       = {Harrison T. Reeder and Junwei Lu and Sebastien Haneuse},
  doi          = {10.1111/biom.13761},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1657-1669},
  shortjournal = {Biometrics},
  title        = {Penalized estimation of frailty-based illness–death models for semi-competing risks},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum likelihood estimation in the additive hazards model.
<em>BIOMTC</em>, <em>79</em>(3), 1646–1656. (<a
href="https://doi.org/10.1111/biom.13764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazards model specifies the effect of covariates on the hazard in an additive way, in contrast to the popular Cox model, in which it is multiplicative. As the non-parametric model, additive hazards offer a very flexible way of modeling time-varying covariate effects. It is most commonly estimated by ordinary least squares. In this paper, we consider the case where covariates are bounded, and derive the maximum likelihood estimator under the constraint that the hazard is non-negative for all covariate values in their domain. We show that the maximum likelihood estimator may be obtained by separately maximizing the log-likelihood contribution of each event time point, and we show that the maximizing problem is equivalent to fitting a series of Poisson regression models with an identity link under non-negativity constraints. We derive an analytic solution to the maximum likelihood estimator. We contrast the maximum likelihood estimator with the ordinary least-squares estimator in a simulation study and show that the maximum likelihood estimator has smaller mean squared error than the ordinary least-squares estimator. An illustration with data on patients with carcinoma of the oropharynx is provided.},
  archive      = {J_BIOMTC},
  author       = {Chengyuan Lu and Jelle Goeman and Hein Putter},
  doi          = {10.1111/biom.13764},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1646-1656},
  shortjournal = {Biometrics},
  title        = {Maximum likelihood estimation in the additive hazards model},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint inference for competing risks data using multiple
endpoints. <em>BIOMTC</em>, <em>79</em>(3), 1635–1645. (<a
href="https://doi.org/10.1111/biom.13752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing risks data are commonly encountered in randomized clinical trials and observational studies. This paper considers the situation where the ending statuses of competing events have different clinical interpretations and/or are of simultaneous interest. In clinical trials, often more than one competing event has meaningful clinical interpretations even though the trial effects of different events could be different or even opposite to each other. In this paper, we develop estimation procedures and inferential properties for the joint use of multiple cumulative incidence functions (CIFs). Additionally, by incorporating longitudinal marker information, we develop estimation and inference procedures for weighted CIFs and related metrics. The proposed methods are applied to a COVID-19 in-patient treatment clinical trial, where the outcomes of COVID-19 hospitalization are either death or discharge from the hospital, two competing events with completely different clinical implications.},
  archive      = {J_BIOMTC},
  author       = {Jiyang Wen and Chen Hu and Mei-Cheng Wang},
  doi          = {10.1111/biom.13752},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1635-1645},
  shortjournal = {Biometrics},
  title        = {Joint inference for competing risks data using multiple endpoints},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concordance indices with left-truncated and right-censored
data. <em>BIOMTC</em>, <em>79</em>(3), 1624–1634. (<a
href="https://doi.org/10.1111/biom.13714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of time-to-event analysis, a primary objective is to model the risk of experiencing a particular event in relation to a set of observed predictors. The Concordance Index (C-Index) is a statistic frequently used in practice to assess how well such models discriminate between various risk levels in a population. However, the properties of conventional C-Index estimators when applied to left-truncated time-to-event data have not been well studied, despite the fact that left-truncation is commonly encountered in observational studies. We show that the limiting values of the conventional C-Index estimators depend on the underlying distribution of truncation times, which is similar to the situation with right-censoring as discussed in Uno et al. (2011) [On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in Medicine 30(10), 1105–1117]. We develop a new C-Index estimator based on inverse probability weighting (IPW) that corrects for this limitation, and we generalize this estimator to settings with left-truncated and right-censored data. The proposed IPW estimators are highly robust to the underlying truncation distribution and often outperform the conventional methods in terms of bias, mean squared error, and coverage probability. We apply these estimators to evaluate a predictive survival model for mortality among patients with end-stage renal disease.},
  archive      = {J_BIOMTC},
  author       = {Nicholas Hartman and Sehee Kim and Kevin He and John D. Kalbfleisch},
  doi          = {10.1111/biom.13714},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1624-1634},
  shortjournal = {Biometrics},
  title        = {Concordance indices with left-truncated and right-censored data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimension reduction for integrative survival analysis.
<em>BIOMTC</em>, <em>79</em>(3), 1610–1623. (<a
href="https://doi.org/10.1111/biom.13736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a constrained maximum partial likelihood estimator for dimension reduction in integrative (e.g., pan-cancer) survival analysis with high-dimensional predictors. We assume that for each population in the study, the hazard function follows a distinct Cox proportional hazards model. To borrow information across populations, we assume that each of the hazard functions depend only on a small number of linear combinations of the predictors (i.e., “factors”). We estimate these linear combinations using an algorithm based on “distance-to-set” penalties. This allows us to impose both low-rankness and sparsity on the regression coefficient matrix estimator. We derive asymptotic results that reveal that our estimator is more efficient than fitting a separate proportional hazards model for each population. Numerical experiments suggest that our method outperforms competitors under various data generating models. We use our method to perform a pan-cancer survival analysis relating protein expression to survival across 18 distinct cancer types. Our approach identifies six linear combinations, depending on only 20 proteins, which explain survival across the cancer types. Finally, to validate our fitted model, we show that our estimated factors can lead to better prediction than competitors on four external datasets.},
  archive      = {J_BIOMTC},
  author       = {Aaron J. Molstad and Rohit K. Patra},
  doi          = {10.1111/biom.13736},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1610-1623},
  shortjournal = {Biometrics},
  title        = {Dimension reduction for integrative survival analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural cumulative survival models for estimation of
treatment effects accounting for treatment switching in randomized
experiments. <em>BIOMTC</em>, <em>79</em>(3), 1597–1609. (<a
href="https://doi.org/10.1111/biom.13704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment switching in a randomized controlled trial occurs when a patient in one treatment arm switches to another arm during follow-up. This can occur at the point of disease progression, whereby patients in the control arm may be offered the experimental treatment. It is widely known that failure to account for treatment switching can seriously bias the estimated treatment causal effect. In this paper, we aim to account for the potential impact of treatment switching in a reanalysis evaluating the treatment effect of nucleoside reverse transcriptase inhibitors (NRTIs) on a safety outcome (time to first severe or worse sign or symptom) in participants receiving a new antiretroviral regimen that either included or omitted NRTIs in the optimized treatment that includes or omits NRTIs trial. We propose an estimator of a treatment causal effect for a censored time to event outcome under a structural cumulative survival model that leverages randomization as an instrumental variable to account for selective treatment switching. We establish that the proposed estimator is uniformly consistent and asymptotically Gaussian, with a consistent variance estimator and confidence intervals given, whose finite-sample performance is evaluated via extensive simulations. An R package ‘ivsacim’ implementing all proposed methods is freely available on R CRAN. Results indicate that adding NRTIs versus omitting NRTIs to a new optimized treatment regime may increase the risk for a safety outcome.},
  archive      = {J_BIOMTC},
  author       = {Andrew Ying and Eric J. Tchetgen Tchetgen},
  doi          = {10.1111/biom.13704},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1597-1609},
  shortjournal = {Biometrics},
  title        = {Structural cumulative survival models for estimation of treatment effects accounting for treatment switching in randomized experiments},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principles of biostatistics (3rd ed) marcello pagano,
kimberlee gauvreau, heather mattie (2022). Boca raton, FL: CRC press.
<em>BIOMTC</em>, <em>79</em>(2), 1589–1590. (<a
href="https://doi.org/10.1111/biom.13876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chuhsing Kate Hsiao},
  doi          = {10.1111/biom.13876},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1589-1590},
  shortjournal = {Biometrics},
  title        = {Principles of biostatistics (3rd ed) marcello pagano, kimberlee gauvreau, heather mattie (2022). boca raton, FL: CRC press},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Writing grant proposals in epidemiology, preventive
medicine, and biostatistics lisa chasan-taber, CRC press: Boca raton FL.
2022. Https://doi.org/10.1201/9781003155140. <em>BIOMTC</em>,
<em>79</em>(2), 1587–1589. (<a
href="https://doi.org/10.1111/biom.13868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {James S. Hodges},
  doi          = {10.1111/biom.13868},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1587-1589},
  shortjournal = {Biometrics},
  title        = {Writing grant proposals in epidemiology, preventive medicine, and biostatistics lisa chasan-taber, CRC press: boca raton FL. 2022. https://doi.org/10.1201/9781003155140},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistics in the public interest: In memory of stephen e.
Fienberg alicia l. Carriquiry, judith m. Tanur, william f. Eddy,
margaret l. Smykla (eds.), new york city: Springer. 2022.
<em>BIOMTC</em>, <em>79</em>(2), 1586–1587. (<a
href="https://doi.org/10.1111/biom.13869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {David Banks},
  doi          = {10.1111/biom.13869},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1586-1587},
  shortjournal = {Biometrics},
  title        = {Statistics in the public interest: in memory of stephen e. fienberg alicia l. carriquiry, judith m. tanur, william f. eddy, margaret l. smykla (Eds.), new york city: springer. 2022.},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for set-based effects in genetic association
studies with interval-censored outcomes. <em>BIOMTC</em>,
<em>79</em>(2), 1573–1585. (<a
href="https://doi.org/10.1111/biom.13636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid acceleration of genetic data collection in biomedical settings has recently resulted in the rise of genetic compendiums filled with rich longitudinal disease data. One common feature of these data sets is their plethora of interval-censored outcomes. However, very few tools are available for the analysis of genetic data sets with interval-censored outcomes, and in particular, there is a lack of methodology available for set-based inference. Set-based inference is used to associate a gene, biological pathway, or other genetic construct with outcomes and is one of the most popular strategies in genetics research. This work develops three such tests for interval-censored settings beginning with a variance components test for interval-censored outcomes, the interval-censored sequence kernel association test (ICSKAT). We also provide the interval-censored version of the Burden test, and then we integrate ICSKAT and Burden to construct the interval censored sequence kernel association test—optimal (ICSKATO) combination. These tests unlock set-based analysis of interval-censored data sets with analogs of three highly popular set-based tools commonly applied to continuous and binary outcomes. Simulation studies illustrate the advantages of the developed methods over ad hoc alternatives, including protection of the type I error rate at very low levels and increased power. The proposed approaches are applied to the investigation that motivated this study, an examination of the genes associated with bone mineral density deficiency and fracture risk.},
  archive      = {J_BIOMTC},
  author       = {Ryan Sun and Liang Zhu and Yimei Li and Yutaka Yasui and Leslie Robison},
  doi          = {10.1111/biom.13636},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1573-1585},
  shortjournal = {Biometrics},
  title        = {Inference for set-based effects in genetic association studies with interval-censored outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible copula model for integrating correlated multi-omics
data from single-cell experiments. <em>BIOMTC</em>, <em>79</em>(2),
1559–1572. (<a href="https://doi.org/10.1111/biom.13701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advances in technologies to profile multi-omics data at the single-cell level, integrative multi-omics data analysis has been increasingly popular. It is increasingly common that information such as methylation changes, chromatin accessibility, and gene expression are jointly collected in a single-cell experiment. In biomedical studies, it is often of interest to study the associations between various data types and to examine how these associations might change according to other factors such as cell types and gene regulatory components. However, since each data type usually has a distinct marginal distribution, joint analysis of these changes of associations using multi-omics data is statistically challenging. In this paper, we propose a flexible copula-based framework to model covariate-dependent correlation structures independent of their marginals. In addition, the proposed approach could jointly combine a wide variety of univariate marginal distributions, either discrete or continuous, including the class of zero-inflated distributions. The performance of the proposed framework is demonstrated through a series of simulation studies. Finally, it is applied to a set of experimental data to investigate the dynamic relationship between single-cell RNA sequencing, chromatin accessibility, and DNA methylation at different germ layers during mouse gastrulation.},
  archive      = {J_BIOMTC},
  author       = {Zichen Ma and Shannon W. Davis and Yen-Yi Ho},
  doi          = {10.1111/biom.13701},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1559-1572},
  shortjournal = {Biometrics},
  title        = {Flexible copula model for integrating correlated multi-omics data from single-cell experiments},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplifying the estimation of diagnostic testing accuracy
over time for high specificity tests in the absence of a gold standard.
<em>BIOMTC</em>, <em>79</em>(2), 1546–1558. (<a
href="https://doi.org/10.1111/biom.13689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many different methods for evaluating diagnostic test results in the absence of a gold standard have been proposed. In this paper, we discuss how one common method, a maximum likelihood estimate for a latent class model found via the Expectation-Maximization (EM) algorithm can be applied to longitudinal data where test sensitivity changes over time. We also propose two simplified and nonparametric methods which use data-based indicator variables for disease status and compare their accuracy to the maximum likelihood estimation (MLE) results. We find that with high specificity tests, the performance of simpler approximations may be just as high as the MLE.},
  archive      = {J_BIOMTC},
  author       = {Clara Drew and Moses Badio and Dehkontee Dennis and Lisa Hensley and Elizabeth Higgs and Michael Sneller and Mosoka Fallah and Cavan Reilly},
  doi          = {10.1111/biom.13689},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1546-1558},
  shortjournal = {Biometrics},
  title        = {Simplifying the estimation of diagnostic testing accuracy over time for high specificity tests in the absence of a gold standard},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Increasing efficiency and reducing bias when assessing HPV
vaccination efficacy by using nontargeted HPV strains. <em>BIOMTC</em>,
<em>79</em>(2), 1534–1545. (<a
href="https://doi.org/10.1111/biom.13663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of vaccine efficacy often record both the incidence of vaccine-targeted virus strains (primary outcome) and the incidence of nontargeted strains (secondary outcome). However, standard estimates of vaccine efficacy on targeted strains ignore the data on nontargeted strains. Assuming nontargeted strains are unaffected by vaccination, we regard the secondary outcome as a negative control outcome and show how using such data can (i) increase the precision of the estimated vaccine efficacy against targeted strains in randomized trials and (ii) reduce confounding bias of that same estimate in observational studies. For objective (i), we augment the primary outcome estimating equation with a function of the secondary outcome that is unbiased for zero. For objective (ii), we jointly estimate the treatment effects on the primary and secondary outcomes. If the bias induced by the unmeasured confounders is similar for both types of outcomes, as is plausible for factors that influence the general risk of infection, then we can use the estimated efficacy against the secondary outcomes to remove the bias from estimated efficacy against the primary outcome. We demonstrate the utility of these approaches in studies of HPV vaccines that only target a few highly carcinogenic strains. In this example, using nontargeted strains increased precision in randomized trials modestly but reduced bias in observational studies substantially.},
  archive      = {J_BIOMTC},
  author       = {Lola Etievant and Joshua N. Sampson and Mitchell H. Gail},
  doi          = {10.1111/biom.13663},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1534-1545},
  shortjournal = {Biometrics},
  title        = {Increasing efficiency and reducing bias when assessing HPV vaccination efficacy by using nontargeted HPV strains},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric count data regression for self-reported
mental health. <em>BIOMTC</em>, <em>79</em>(2), 1520–1533. (<a
href="https://doi.org/10.1111/biom.13617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘‘For how many days during the past 30 days was your mental health not good?” The responses to this question measure self-reported mental health and can be linked to important covariates in the National Health and Nutrition Examination Survey (NHANES). However, these count variables present major distributional challenges: The data are overdispersed, zero-inflated, bounded by 30, and heaped in 5- and 7-day increments. To address these challenges—which are especially common for health questionnaire data—we design a semiparametric estimation and inference framework for count data regression. The data-generating process is defined by simultaneously transforming and rounding ( star ) a latent Gaussian regression model. The transformation is estimated nonparametrically and the rounding operator ensures the correct support for the discrete and bounded data. Maximum likelihood estimators are computed using an expectation-maximization (EM) algorithm that is compatible with any continuous data model estimable by least squares. star regression includes asymptotic hypothesis testing and confidence intervals, variable selection via information criteria, and customized diagnostics. Simulation studies validate the utility of this framework. Using star regression, we identify key factors associated with self-reported mental health and demonstrate substantial improvements in goodness-of-fit compared to existing count data regression models.},
  archive      = {J_BIOMTC},
  author       = {Daniel R. Kowal and Bohan Wu},
  doi          = {10.1111/biom.13617},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1520-1533},
  shortjournal = {Biometrics},
  title        = {Semiparametric count data regression for self-reported mental health},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hierarchical model for analyzing multisite
individual-level disease surveillance data from multiple systems.
<em>BIOMTC</em>, <em>79</em>(2), 1507–1519. (<a
href="https://doi.org/10.1111/biom.13647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive surveillance systems are widely used to monitor diseases occurrence over wide spatial areas due to their cost-effectiveness and integration into broadly distributed healthcare systems. However, such systems are generally associated with imperfect ascertainment of disease cases and with heterogeneous capture probabilities arising from factors such as differential access to care. Augmenting passive surveillance systems with other surveillance efforts provides a way to estimate the true number of incident cases. We develop a hierarchical modeling framework for analyzing data from multiple surveillance systems that allows for individual-level covariate-dependent heterogeneous capture probabilities, and borrows information across surveillance sites to improve estimation of the true number of incident cases. Inference is carried out via a two-stage Bayesian procedure. Simulation studies illustrated superior performance of the proposed approach with respect to bias, root mean square error, and coverage compared to a model that does not borrow information across sites. We applied the proposed model to data from three surveillance systems reporting pulmonary tuberculosis (PTB) cases in a major center of ongoing transmission in China. The analysis yielded bias-corrected estimates of PTB cases from the passive system and led to the identification of risk factors associated with PTB rates, as well as factors influencing the operating characteristics of the implemented surveillance systems.},
  archive      = {J_BIOMTC},
  author       = {Yuzi Zhang and Howard H. Chang and Qu Cheng and Philip A. Collender and Ting Li and Jinge He and Justin V. Remais},
  doi          = {10.1111/biom.13647},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1507-1519},
  shortjournal = {Biometrics},
  title        = {A hierarchical model for analyzing multisite individual-level disease surveillance data from multiple systems},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating treatment effects in group sequential
multivariate longitudinal studies with covariate adjustment.
<em>BIOMTC</em>, <em>79</em>(2), 1496–1506. (<a
href="https://doi.org/10.1111/biom.13659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jeffries et al. (2018) investigated testing for a treatment difference in the setting of a randomized clinical trial with a single outcome measured longitudinally over a series of common follow-up times while adjusting for covariates. That paper examined the null hypothesis of no difference at any follow-up time versus the alternative of a difference for at least one follow-up time. We extend those results here by considering multivariate outcome measurements, where each individual outcome is examined at common follow-up times. We consider the case where there is interest in first testing for a treatment difference in a global function of the outcomes (e.g., weighted average or sum) with subsequent interest in examining the individual outcomes, should the global function show a treatment difference. Testing is conducted for each follow-up time and may be performed in the setting of a group sequential trial. Testing procedures are developed to determine follow-up times for which a global treatment difference exists and which individual combinations of outcome and follow-up time show evidence of a difference while controlling for multiplicity in outcomes, follow-up, and interim analyses. These approaches are examined in a study evaluating the effects of tissue plasminogen activator on longitudinally obtained stroke severity measurements.},
  archive      = {J_BIOMTC},
  author       = {Neal O. Jeffries and James F. Troendle and Nancy L. Geller},
  doi          = {10.1111/biom.13659},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1496-1506},
  shortjournal = {Biometrics},
  title        = {Evaluating treatment effects in group sequential multivariate longitudinal studies with covariate adjustment},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A repeated measures approach to pooled and calibrated
biomarker data. <em>BIOMTC</em>, <em>79</em>(2), 1485–1495. (<a
href="https://doi.org/10.1111/biom.13618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Participant-level meta-analysis across multiple studies increases the sample size for pooled analyses, thereby improving precision in effect estimates and enabling subgroup analyses. For analyses involving biomarker measurements as an exposure of interest, investigators must first calibrate the data to address measurement variability arising from usage of different laboratories and/or assays. In practice, the calibration process involves reassaying a random subset of biospecimens from each study at a central laboratory and fitting models that relate the study-specific “local” and central laboratory measurements. Previous work in this area treats the calibration process from the perspective of measurement error techniques and imputes the estimated central laboratory value among individuals with only a local laboratory measurement. In this work, we propose a repeated measures method to calibrate biomarker measurements pooled from multiple studies with study-specific calibration subsets. We account for correlation between measurements made on the same person and between measurements made at the same laboratory. We demonstrate that the repeated measures approach provides valid inference, and compare it to existing calibration approaches grounded in measurement error techniques in an example describing the association between circulating vitamin D and stroke.},
  archive      = {J_BIOMTC},
  author       = {Abigail Sloan and Chao Cheng and Bernard Rosner and Regina G. Ziegler and Stephanie A. Smith-Warner and Molin Wang},
  doi          = {10.1111/biom.13618},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1485-1495},
  shortjournal = {Biometrics},
  title        = {A repeated measures approach to pooled and calibrated biomarker data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging a surrogate outcome to improve inference on a
partially missing target outcome. <em>BIOMTC</em>, <em>79</em>(2),
1472–1484. (<a href="https://doi.org/10.1111/biom.13629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample sizes vary substantially across tissues in the Genotype-Tissue Expression (GTEx) project, where considerably fewer samples are available from certain inaccessible tissues, such as the substantia nigra (SSN), than from accessible tissues, such as blood. This severely limits power for identifying tissue-specific expression quantitative trait loci (eQTL) in undersampled tissues. Here we propose Surrogate Phenotype Regression Analysis ( Spray ) for leveraging information from a correlated surrogate outcome (eg, expression in blood) to improve inference on a partially missing target outcome (eg, expression in SSN). Rather than regarding the surrogate outcome as a proxy for the target outcome, Spray jointly models the target and surrogate outcomes within a bivariate regression framework. Unobserved values of either outcome are treated as missing data. We describe and implement an expectation conditional maximization algorithm for performing estimation in the presence of bilateral outcome missingness. Spray estimates the same association parameter estimated by standard eQTL mapping and controls the type I error even when the target and surrogate outcomes are truly uncorrelated. We demonstrate analytically and empirically, using simulations and GTEx data, that in comparison with marginally modeling the target outcome, jointly modeling the target and surrogate outcomes increases estimation precision and improves power.},
  archive      = {J_BIOMTC},
  author       = {Zachary R. McCaw and Sheila M. Gaynor and Ryan Sun and Xihong Lin},
  doi          = {10.1111/biom.13629},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1472-1484},
  shortjournal = {Biometrics},
  title        = {Leveraging a surrogate outcome to improve inference on a partially missing target outcome},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian platform trial design to simultaneously evaluate
multiple drugs in multiple indications with mixed endpoints.
<em>BIOMTC</em>, <em>79</em>(2), 1459–1471. (<a
href="https://doi.org/10.1111/biom.13694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of targeted therapies and immunotherapies, the traditional drug development paradigm of testing one drug at a time in one indication has become increasingly inefficient. Motivated by a real-world application, we propose a master-protocol–based Bayesian platform trial design with mixed endpoints (PDME) to simultaneously evaluate multiple drugs in multiple indications, where different subsets of efficacy measures (eg, objective response and landmark progression-free survival) may be used by different indications as single or multiple endpoints. We propose a Bayesian hierarchical model to accommodate mixed endpoints and reflect the trial structure of indications that are nested within treatments. We develop a two-stage approach that first clusters the indications into homogeneous subgroups and then applies the Bayesian hierarchical model to each subgroup to achieve precision information borrowing. Patients are enrolled in a group-sequential way and adaptively assigned to treatments according to their efficacy estimates. At each interim analysis, the posterior probabilities that the treatment effect exceeds prespecified clinically relevant thresholds are used to drop ineffective treatments and “graduate” effective treatments. Simulations show that the PDME design has desirable operating characteristics compared to existing method.},
  archive      = {J_BIOMTC},
  author       = {Yujie Zhao and Rui (Sammi) Tang and Yeting Du and Ying Yuan},
  doi          = {10.1111/biom.13694},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1459-1471},
  shortjournal = {Biometrics},
  title        = {A bayesian platform trial design to simultaneously evaluate multiple drugs in multiple indications with mixed endpoints},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian model with application for adaptive platform
trials having temporal changes. <em>BIOMTC</em>, <em>79</em>(2),
1446–1458. (<a href="https://doi.org/10.1111/biom.13680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal changes exist in clinical trials. Over time, shifts in patients&#39; characteristics, trial conduct, and other features of a clinical trial may occur. In typical randomized clinical trials, temporal effects, that is, the impact of temporal changes on clinical outcomes and study analysis, are largely mitigated by randomization and usually need not be explicitly addressed. However, temporal effects can be a serious obstacle for conducting clinical trials with complex designs, including the adaptive platform trials that are gaining popularity in recent medical product development. In this paper, we introduce a Bayesian robust prior for mitigating temporal effects based on a hidden Markov model, and propose a particle filtering algorithm for computation. We conduct simulation studies to evaluate the performance of the proposed method and provide illustration examples based on trials of Ebola virus disease therapeutics and hemostat in vascular surgery.},
  archive      = {J_BIOMTC},
  author       = {Chenguang Wang and Min Lin and Gary L. Rosner and Guoxing Soon},
  doi          = {10.1111/biom.13680},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1446-1458},
  shortjournal = {Biometrics},
  title        = {A bayesian model with application for adaptive platform trials having temporal changes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An alternative metric for evaluating the potential patient
benefit of response-adaptive randomization procedures. <em>BIOMTC</em>,
<em>79</em>(2), 1433–1445. (<a
href="https://doi.org/10.1111/biom.13673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning a two-arm group sequential clinical trial with a binary primary outcome that has severe implications for quality of life (e.g., mortality), investigators may strive to find the design that maximizes in-trial patient benefit. In such cases, Bayesian response-adaptive randomization (BRAR) is often considered because it can alter the allocation ratio throughout the trial in favor of the treatment that is currently performing better. Although previous studies have recommended using fixed randomization over BRAR based on patient benefit metrics calculated from the realized trial sample size, these previous comparisons have been limited by failures to hold type I and II error rates constant across designs or consider the impacts on all individuals directly affected by the design choice. In this paper, we propose a metric for comparing designs with the same type I and II error rates that reflects expected outcomes among individuals who would participate in the trial if enrollment is open when they become eligible. We demonstrate how to use the proposed metric to guide the choice of design in the context of two recent trials in persons suffering out of hospital cardiac arrest. Using computer simulation, we demonstrate that various implementations of group sequential BRAR offer modest improvements with respect to the proposed metric relative to conventional group sequential monitoring alone.},
  archive      = {J_BIOMTC},
  author       = {Jennifer Proper and Thomas A. Murray},
  doi          = {10.1111/biom.13673},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1433-1445},
  shortjournal = {Biometrics},
  title        = {An alternative metric for evaluating the potential patient benefit of response-adaptive randomization procedures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and analysis of two-phase studies with multivariate
longitudinal data. <em>BIOMTC</em>, <em>79</em>(2), 1420–1432. (<a
href="https://doi.org/10.1111/biom.13616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase studies are crucial when outcome and covariate data are available in a first-phase sample (e.g., a cohort study), but costs associated with retrospective ascertainment of a novel exposure limit the size of the second-phase sample, in whom the exposure is collected. For longitudinal outcomes, one class of two-phase studies stratifies subjects based on an outcome vector summary (e.g., an average or a slope over time) and oversamples subjects in the extreme value strata while undersampling subjects in the medium-value stratum. Based on the choice of the summary, two-phase studies for longitudinal data can increase efficiency of time-varying and/or time-fixed exposure parameter estimates. In this manuscript, we extend efficient, two-phase study designs to multivariate longitudinal continuous outcomes, and we detail two analysis approaches. The first approach is a multiple imputation analysis that combines complete data from subjects selected for phase two with the incomplete data from those not selected. The second approach is a conditional maximum likelihood analysis that is intended for applications where only data from subjects selected for phase two are available. Importantly, we show that both approaches can be applied to secondary analyses of previously conducted two-phase studies. We examine finite sample operating characteristics of the two approaches and use the Lung Health Study (Connett et al. (1993), Controlled Clinical Trials , 14, 3S–19S) to examine genetic associations with lung function decline over time.},
  archive      = {J_BIOMTC},
  author       = {Chiara Di Gravio and Ran Tao and Jonathan S. Schildcrout},
  doi          = {10.1111/biom.13616},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1420-1432},
  shortjournal = {Biometrics},
  title        = {Design and analysis of two-phase studies with multivariate longitudinal data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing intervention effects in a randomized trial within
a social network. <em>BIOMTC</em>, <em>79</em>(2), 1409–1419. (<a
href="https://doi.org/10.1111/biom.13606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of social networks provide unique opportunities to assess the causal effects of interventions that may impact more of the population than just those intervened on directly. Such effects are sometimes called peer or spillover effects, and may exist in the presence of interference, that is, when one individual&#39;s treatment affects another individual&#39;s outcome. Randomization-based inference (RI) methods provide a theoretical basis for causal inference in randomized studies, even in the presence of interference. In this article, we consider RI of the intervention effect in the eX-FLU trial, a randomized study designed to assess the effect of a social distancing intervention on influenza-like-illness transmission in a connected network of college students. The approach considered enables inference about the effect of the social distancing intervention on the per-contact probability of influenza-like-illness transmission in the observed network. The methods allow for interference between connected individuals and for heterogeneous treatment effects. The proposed methods are evaluated empirically via simulation studies, and then applied to data from the eX-FLU trial.},
  archive      = {J_BIOMTC},
  author       = {Shaina J. Alexandria and Michael G. Hudgens and Allison E. Aiello},
  doi          = {10.1111/biom.13606},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1409-1419},
  shortjournal = {Biometrics},
  title        = {Assessing intervention effects in a randomized trial within a social network},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian functional data model for surveys collected under
informative sampling with application to mortality estimation using
NHANES. <em>BIOMTC</em>, <em>79</em>(2), 1397–1408. (<a
href="https://doi.org/10.1111/biom.13696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data are often extremely high-dimensional and exhibit strong dependence structures but can often prove valuable for both prediction and inference. The literature on functional data analysis is well developed; however, there has been very little work involving functional data in complex survey settings. Motivated by physical activity monitor data from the National Health and Nutrition Examination Survey (NHANES), we develop a Bayesian model for functional covariates that can properly account for the survey design. Our approach is intended for non-Gaussian data and can be applied in multivariate settings. In addition, we make use of a variety of Bayesian modeling techniques to ensure that the model is fit in a computationally efficient manner. We illustrate the value of our approach through two simulation studies as well as an example of mortality estimation using NHANES data.},
  archive      = {J_BIOMTC},
  author       = {Paul A. Parker and Scott H. Holan},
  doi          = {10.1111/biom.13696},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1397-1408},
  shortjournal = {Biometrics},
  title        = {A bayesian functional data model for surveys collected under informative sampling with application to mortality estimation using NHANES},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric analysis of restricted mean survival
time. <em>BIOMTC</em>, <em>79</em>(2), 1383–1396. (<a
href="https://doi.org/10.1111/biom.13622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) evaluates the expectation of survival time truncated by a prespecified time point, because the mean survival time in the presence of censoring is typically not estimable. The frequentist inference procedure for RMST has been widely advocated for comparison of two survival curves, while research from the Bayesian perspective is rather limited. For the RMST of both right- and interval-censored data, we propose Bayesian nonparametric estimation and inference procedures. By assigning a mixture of Dirichlet processes (MDP) prior to the distribution function, we can estimate the posterior distribution of RMST. We also explore another Bayesian nonparametric approach using the Dirichlet process mixture model and make comparisons with the frequentist nonparametric method. Simulation studies demonstrate that the Bayesian nonparametric RMST under diffuse MDP priors leads to robust estimation and under informative priors it can incorporate prior knowledge into the nonparametric estimator. Analysis of real trial examples demonstrates the flexibility and interpretability of the Bayesian nonparametric RMST for both right- and interval-censored data.},
  archive      = {J_BIOMTC},
  author       = {Chenyang Zhang and Guosheng Yin},
  doi          = {10.1111/biom.13622},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1383-1396},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric analysis of restricted mean survival time},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric analysis for the detection of spikes
in noisy calcium imaging data. <em>BIOMTC</em>, <em>79</em>(2),
1370–1382. (<a href="https://doi.org/10.1111/biom.13626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in miniaturized fluorescence microscopy have made it possible to investigate neuronal responses to external stimuli in awake behaving animals through the analysis of intracellular calcium signals. An ongoing challenge is deconvolving the temporal signals to extract the spike trains from the noisy calcium signals&#39; time series. In this article, we propose a nested Bayesian finite mixture specification that allows the estimation of spiking activity and, simultaneously, reconstructing the distributions of the calcium transient spikes&#39; amplitudes under different experimental conditions. The proposed model leverages two nested layers of random discrete mixture priors to borrow information between experiments and discover similarities in the distributional patterns of neuronal responses to different stimuli. Furthermore, the spikes&#39; intensity values are also clustered within and between experimental conditions to determine the existence of common (recurring) response amplitudes. Simulation studies and the analysis of a dataset from the Allen Brain Observatory show the effectiveness of the method in clustering and detecting neuronal activities.},
  archive      = {J_BIOMTC},
  author       = {Laura D&#39;Angelo and Antonio Canale and Zhaoxia Yu and Michele Guindani},
  doi          = {10.1111/biom.13626},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1370-1382},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric analysis for the detection of spikes in noisy calcium imaging data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised two-dimensional functional principal component
analysis with time-to-event outcomes and mammogram imaging data.
<em>BIOMTC</em>, <em>79</em>(2), 1359–1369. (<a
href="https://doi.org/10.1111/biom.13611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screening mammography aims to identify breast cancer early and secondarily measures breast density to classify women at higher or lower than average risk for future breast cancer in the general population. Despite the strong association of individual mammography features to breast cancer risk, the statistical literature on mammogram imaging data is limited. While functional principal component analysis (FPCA) has been studied in the literature for extracting image-based features, it is conducted independently of the time-to-event response variable. With the consideration of building a prognostic model for precision prevention, we present a set of flexible methods, supervised FPCA (sFPCA) and functional partial least squares (FPLS), to extract image-based features associated with the failure time while accommodating the added complication from right censoring. Throughout the article, we hope to demonstrate that one method is favored over the other under different clinical setups. The proposed methods are applied to the motivating data set from the Joanne Knight Breast Health cohort at Siteman Cancer Center. Our approaches not only obtain the best prediction performance compared to the benchmark model, but also reveal different risk patterns within the mammograms.},
  archive      = {J_BIOMTC},
  author       = {Shu Jiang and Jiguo Cao and Bernard Rosner and Graham A. Colditz},
  doi          = {10.1111/biom.13611},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1359-1369},
  shortjournal = {Biometrics},
  title        = {Supervised two-dimensional functional principal component analysis with time-to-event outcomes and mammogram imaging data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder: A formal causal interpretation of the
case-crossover design. <em>BIOMTC</em>, <em>79</em>(2), 1351–1358. (<a
href="https://doi.org/10.1111/biom.13746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We thank the discussants for their insightful commentary and questions. In our rejoinder, we extend our analysis to additional settings, control strategies, and sources of bias relevant to how case-crossover is often used in practice, as suggested by multiple discussants. In particular, we consider: control exposures that follow occurrence of events, settings with shared exposure trajectories (which are common in case-crossover studies of effects of air pollution), bias due to non-transient treatment effects, removing bias due to time trends in treatment using control subjects, and extending our results to the continuous time setting. We also take the opportunity to clarify an easily misinterpreted comment we made about collapsibility, which we thank Andersen and Martinussen for highlighting. Throughout, in the spirit of the discussants. contributions, we iterate between general bias formulas and simulations and numerical studies from illustrative simplified scenarios to build insight.},
  archive      = {J_BIOMTC},
  author       = {Zach Shahn and Miguel A. Hernán and James M. Robins},
  doi          = {10.1111/biom.13746},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1351-1358},
  shortjournal = {Biometrics},
  title        = {Rejoinder: A formal causal interpretation of the case-crossover design},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “a formal causal interpretation of the
case-crossover design” by zach shahn, miguel a. Hernán, and james m.
robins. <em>BIOMTC</em>, <em>79</em>(2), 1349–1350. (<a
href="https://doi.org/10.1111/biom.13750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has always been clear that the case-crossover design works, for some definition of “works,” but some of the details have been surprisingly elusive, and it is good to see more of them nailed down by Shahn et al. My interest in case-crossover analyses has mostly been in the context of air pollution epidemiology mentioned at the end of the paper. The air pollution setting is distinctive for several reasons: as the exposure variable is plausibly exogenous, it is possible to use control times after the case time, the effects of interest are quite small, and the same measured exposure series is shared over many—perhaps all—of the cohort.},
  archive      = {J_BIOMTC},
  author       = {Thomas Lumley},
  doi          = {10.1111/biom.13750},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1349-1350},
  shortjournal = {Biometrics},
  title        = {Discussion on “A formal causal interpretation of the case-crossover design” by zach shahn, miguel a. hernán, and james m. robins},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “a formal causal interpretation of the
case-crossover design” by zach shahn, miguel a. Hernan, and james m.
robins. <em>BIOMTC</em>, <em>79</em>(2), 1346–1348. (<a
href="https://doi.org/10.1111/biom.13747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shahn, Hernan, and Robins give conditions under which estimates from a case-crossover analysis converge to the desired causal relative risk times a bias factor, and they discuss conditions needed to have small bias. To simplify the problem, we discuss only two exposure times and rely on randomized exposure assignments, thereby avoiding the need for potential outcome notation. We identify many, but not all, of the conditions discussed by Shahn et al. in this simple analysis.},
  archive      = {J_BIOMTC},
  author       = {Ruth M. Pfeiffer and Mitchell H. Gail},
  doi          = {10.1111/biom.13747},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1346-1348},
  shortjournal = {Biometrics},
  title        = {Discussion of “A formal causal interpretation of the case-crossover design” by zach shahn, miguel a. hernan, and james m. robins},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “a formal causal interpretation of the
case-crossover design.” <em>BIOMTC</em>, <em>79</em>(2), 1344–1345. (<a
href="https://doi.org/10.1111/biom.13748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discussion on “A formal causal interpretation of the case-crossover design” by Zach Shahn, Miguel A. Hernan, and James M. Robins},
  archive      = {J_BIOMTC},
  author       = {Per Kragh Andersen and Torben Martinussen},
  doi          = {10.1111/biom.13748},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1344-1345},
  shortjournal = {Biometrics},
  title        = {Discussion of “A formal causal interpretation of the case-crossover design”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A formal causal interpretation of the case-crossover
design. <em>BIOMTC</em>, <em>79</em>(2), 1330–1343. (<a
href="https://doi.org/10.1111/biom.13749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-crossover design of Maclure is widely used in epidemiology and other fields to study causal effects of transient treatments on acute outcomes. However, its validity and causal interpretation have only been justified under informal conditions. Here, we place the design in a formal counterfactual framework for the first time. Doing so helps to clarify its assumptions and interpretation. In particular, when the treatment effect is nonnull, we identify a previously unnoticed bias arising from strong common causes of the outcome at different person-times. We analyze this bias and demonstrate its potential importance with simulations. We also use our derivation of the limit of the case-crossover estimator to analyze its sensitivity to treatment effect heterogeneity, a violation of one of the informal criteria for validity. The upshot of this work for practitioners is that, while the case-crossover design can be useful for testing the causal null hypothesis in the presence of baseline confounders, extra caution is warranted when using the case-crossover design for point estimation of causal effects.},
  archive      = {J_BIOMTC},
  author       = {Zach Shahn and Miguel A. Hernán and James M. Robins},
  doi          = {10.1111/biom.13749},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1330-1343},
  shortjournal = {Biometrics},
  title        = {A formal causal interpretation of the case-crossover design},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). It’s all relative: Regression analysis with compositional
predictors. <em>BIOMTC</em>, <em>79</em>(2), 1318–1329. (<a
href="https://doi.org/10.1111/biom.13703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional data reside in a simplex and measure fractions or proportions of parts to a whole. Most existing regression methods for such data rely on log-ratio transformations that are inadequate or inappropriate in modeling high-dimensional data with excessive zeros and hierarchical structures. Moreover, such models usually lack a straightforward interpretation due to the interrelation between parts of a composition. We develop a novel relative-shift regression framework that directly uses proportions as predictors. The new framework provides a paradigm shift for regression analysis with compositional predictors and offers a superior interpretation of how shifting concentration between parts affects the response. New equi-sparsity and tree-guided regularization methods and an efficient smoothing proximal gradient algorithm are developed to facilitate feature aggregation and dimension reduction in regression. A unified finite-sample prediction error bound is derived for the proposed regularized estimators. We demonstrate the efficacy of the proposed methods in extensive simulation studies and a real gut microbiome study. Guided by the taxonomy of the microbiome data, the framework identifies important taxa at different taxonomic levels associated with the neurodevelopment of preterm infants.},
  archive      = {J_BIOMTC},
  author       = {Gen Li and Yan Li and Kun Chen},
  doi          = {10.1111/biom.13703},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1318-1329},
  shortjournal = {Biometrics},
  title        = {It&#39;s all relative: Regression analysis with compositional predictors},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Translocation detection from hi-c data via scan statistics.
<em>BIOMTC</em>, <em>79</em>(2), 1306–1317. (<a
href="https://doi.org/10.1111/biom.13724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent Hi-C technology enables more comprehensive chromosomal conformation research, including the detection of structural variations, especially translocations. In this paper, we formulate the interchromosomal translocation detection as a problem of scan clustering in a spatial point process. We then develop TranScan, a new translocation detection method through scan statistics with the control of false discovery. The simulation shows that TranScan is more powerful than an existing sophisticated scan clustering method, especially under strong signal situations. Evaluation of TranScan against current translocation detection methods on realistic breakpoint simulations generated from real data suggests better discriminative power under the receiver-operating characteristic curve. Power analysis also highlights TranScan&#39;s consistent outperformance when sequencing depth and heterozygosity rate is varied. Comparatively, Type I error rate is lowest when evaluated using a karyotypically normal cell line. Both the simulation and real data analysis indicate that TranScan has great potentials in interchromosomal translocation detection using Hi-C data.},
  archive      = {J_BIOMTC},
  author       = {Anthony Cheng and Disheng Mao and Yuping Zhang and Joseph Glaz and Zhengqing Ouyang},
  doi          = {10.1111/biom.13724},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1306-1317},
  shortjournal = {Biometrics},
  title        = {Translocation detection from hi-C data via scan statistics},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power analysis for cluster randomized trials with continuous
coprimary endpoints. <em>BIOMTC</em>, <em>79</em>(2), 1293–1305. (<a
href="https://doi.org/10.1111/biom.13692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pragmatic trials evaluating health care interventions often adopt cluster randomization due to scientific or logistical considerations. Systematic reviews have shown that coprimary endpoints are not uncommon in pragmatic trials but are seldom recognized in sample size or power calculations. While methods for power analysis based on K ( K ≥ 2 $K\ge 2$ ) binary coprimary endpoints are available for cluster randomized trials (CRTs), to our knowledge, methods for continuous coprimary endpoints are not yet available. Assuming a multivariate linear mixed model (MLMM) that accounts for multiple types of intraclass correlation coefficients among the observations in each cluster, we derive the closed-form joint distribution of K treatment effect estimators to facilitate sample size and power determination with different types of null hypotheses under equal cluster sizes. We characterize the relationship between the power of each test and different types of correlation parameters. We further relax the equal cluster size assumption and approximate the joint distribution of the K treatment effect estimators through the mean and coefficient of variation of cluster sizes. Our simulation studies with a finite number of clusters indicate that the predicted power by our method agrees well with the empirical power, when the parameters in the MLMM are estimated via the expectation-maximization algorithm. An application to a real CRT is presented to illustrate the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Siyun Yang and Mirjam Moerbeek and Monica Taljaard and Fan Li},
  doi          = {10.1111/biom.13692},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1293-1305},
  shortjournal = {Biometrics},
  title        = {Power analysis for cluster randomized trials with continuous coprimary endpoints},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cross-validation statistical framework for asymmetric data
integration. <em>BIOMTC</em>, <em>79</em>(2), 1280–1292. (<a
href="https://doi.org/10.1111/biom.13685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of biobanks and large public clinical data sets enables their integration with a smaller amount of locally gathered data for the purposes of parameter estimation and model prediction. However, public data sets may be subject to context-dependent confounders and the protocols behind their generation are often opaque; naively integrating all external data sets equally can bias estimates and lead to spurious conclusions. Weighted data integration is a potential solution, but current methods still require subjective specifications of weights and can become computationally intractable. Under the assumption that local data are generated from the set of unknown true parameters, we propose a novel weighted integration method based upon using the external data to minimize the local data leave-one-out cross validation (LOOCV) error. We demonstrate how the optimization of LOOCV errors for linear and Cox proportional hazards models can be rewritten as functions of external data set integration weights. Significant reductions in estimation error and prediction error are shown using simulation studies mimicking the heterogeneity of clinical data as well as a real-world example using kidney transplant patients from the Scientific Registry of Transplant Recipients.},
  archive      = {J_BIOMTC},
  author       = {Lam Tran and Kevin He and Di Wang and Hui Jiang},
  doi          = {10.1111/biom.13685},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1280-1292},
  shortjournal = {Biometrics},
  title        = {A cross-validation statistical framework for asymmetric data integration},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Score test for missing at random or not under logistic
missingness models. <em>BIOMTC</em>, <em>79</em>(2), 1268–1279. (<a
href="https://doi.org/10.1111/biom.13666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are frequently encountered in various disciplines and can be divided into three categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). Valid statistical approaches to missing data depend crucially on correct identification of the underlying missingness mechanism. Although the problem of testing whether this mechanism is MCAR or MAR has been extensively studied, there has been very little research on testing MAR versus MNAR. A critical challenge that is faced when dealing with this problem is the issue of model identification under MNAR. In this paper, under a logistic model for the missing probability, we develop two score tests for the problem of whether the missingness mechanism is MAR or MNAR under a parametric model and a semiparametric location model on the regression function. The implementation of the score tests circumvents the identification issue as it requires only parameter estimation under the null MAR assumption. Our simulations and analysis of human immunodeficiency virus data show that the score tests have well-controlled type I errors and desirable powers.},
  archive      = {J_BIOMTC},
  author       = {Hairu Wang and Zhiping Lu and Yukun Liu},
  doi          = {10.1111/biom.13666},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1268-1279},
  shortjournal = {Biometrics},
  title        = {Score test for missing at random or not under logistic missingness models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous time-interaction processes for population size
estimation, with an application to drug dealing in italy.
<em>BIOMTC</em>, <em>79</em>(2), 1254–1267. (<a
href="https://doi.org/10.1111/biom.13662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a time-interaction point process where the occurrence of an event can increase (self-excitement) or reduce (self-correction) the probability of future events. Self-excitement and self-correction are allowed to be triggered by the same event, at different timescales; other effects such as those of covariates, unobserved heterogeneity, and temporal dependence are also allowed in the model. We focus on capture-recapture data, as our work is motivated by an original example about the estimation of the total number of drug dealers in Italy. To do so, we derive a conditional likelihood formulation where only subjects with at least one capture are involved in the inference process. The result is a novel and flexible continuous-time population size estimator. A simulation study and the analysis of our motivating example illustrate the validity of our approach in several scenarios.},
  archive      = {J_BIOMTC},
  author       = {Linda Altieri and Alessio Farcomeni and Danilo Alunni Fegatelli},
  doi          = {10.1111/biom.13662},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1254-1267},
  shortjournal = {Biometrics},
  title        = {Continuous time-interaction processes for population size estimation, with an application to drug dealing in italy},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust functional principal component analysis via a
functional pairwise spatial sign operator. <em>BIOMTC</em>,
<em>79</em>(2), 1239–1253. (<a
href="https://doi.org/10.1111/biom.13695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional principal component analysis (FPCA) has been widely used to capture major modes of variation and reduce dimensions in functional data analysis. However, standard FPCA based on the sample covariance estimator does not work well if the data exhibits heavy-tailedness or outliers. To address this challenge, a new robust FPCA approach based on a functional pairwise spatial sign (PASS) operator, termed PASS FPCA, is introduced. We propose robust estimation procedures for eigenfunctions and eigenvalues. Theoretical properties of the PASS operator are established, showing that it adopts the same eigenfunctions as the standard covariance operator and also allows recovering ratios between eigenvalues. We also extend the proposed procedure to handle functional data measured with noise. Compared to existing robust FPCA approaches, the proposed PASS FPCA requires weaker distributional assumptions to conserve the eigenspace of the covariance function. Specifically, existing work are often built upon a class of functional elliptical distributions, which requires inherently symmetry. In contrast, we introduce a class of distributions called the weakly functional coordinate symmetry (weakly FCS), which allows for severe asymmetry and is much more flexible than the functional elliptical distribution family. The robustness of the PASS FPCA is demonstrated via extensive simulation studies, especially its advantages in scenarios with nonelliptical distributions. The proposed method was motivated by and applied to analysis of accelerometry data from the Objective Physical Activity and Cardiovascular Health Study, a large-scale epidemiological study to investigate the relationship between objectively measured physical activity and cardiovascular health among older women.},
  archive      = {J_BIOMTC},
  author       = {Guangxing Wang and Sisheng Liu and Fang Han and Chong-Zhi Di},
  doi          = {10.1111/biom.13695},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1239-1253},
  shortjournal = {Biometrics},
  title        = {Robust functional principal component analysis via a functional pairwise spatial sign operator},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional group bridge for simultaneous regression and
support estimation. <em>BIOMTC</em>, <em>79</em>(2), 1226–1238. (<a
href="https://doi.org/10.1111/biom.13684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by studying differential brain activities to multiple experimental condition presentations in intracranial electroencephalography (iEEG) experiments. Contrasting effects of experimental conditions are often zero in most regions and nonzero in some local regions, yielding locally sparse functions. Such studies are essentially a function-on-scalar regression problem, with interest being focused not only on estimating nonparametric functions but also on recovering the function supports. We propose a weighted group bridge approach for simultaneous function estimation and support recovery in function-on-scalar mixed effect models, while accounting for heterogeneity present in functional data. We use B-splines to transform sparsity of functions to its sparse vector counterpart of increasing dimension, and propose a fast nonconvex optimization algorithm using nested alternative direction method of multipliers (ADMM) for estimation. Large sample properties are established. In particular, we show that the estimated coefficient functions are rate optimal in the minimax sense under the L 2 norm and resemble a phase transition phenomenon. For support estimation, we derive a convergence rate under the L ∞ $L_{\infty }$ norm that leads to a selection consistency property under δ-sparsity, and obtain a result under strict sparsity using a simple sufficient regularity condition. An adjusted extended Bayesian information criterion is proposed for parameter tuning. The developed method is illustrated through simulations and an application to a novel iEEG data set to study multisensory integration.},
  archive      = {J_BIOMTC},
  author       = {Zhengjia Wang and John Magnotti and Michael S. Beauchamp and Meng Li},
  doi          = {10.1111/biom.13684},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1226-1238},
  shortjournal = {Biometrics},
  title        = {Functional group bridge for simultaneous regression and support estimation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving trial generalizability using observational
studies. <em>BIOMTC</em>, <em>79</em>(2), 1213–1225. (<a
href="https://doi.org/10.1111/biom.13609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementary features of randomized controlled trials (RCTs) and observational studies (OSs) can be used jointly to estimate the average treatment effect of a target population. We propose a calibration weighting estimator that enforces the covariate balance between the RCT and OS, therefore improving the trial-based estimator&#39;s generalizability. Exploiting semiparametric efficiency theory, we propose a doubly robust augmented calibration weighting estimator that achieves the efficiency bound derived under the identification assumptions. A nonparametric sieve method is provided as an alternative to the parametric approach, which enables the robust approximation of the nuisance functions and data-adaptive selection of outcome predictors for calibration. We establish asymptotic results and confirm the finite sample performances of the proposed estimators by simulation experiments and an application on the estimation of the treatment effect of adjuvant chemotherapy for early-stage non-small-cell lung patients after surgery.},
  archive      = {J_BIOMTC},
  author       = {Dasom Lee and Shu Yang and Lin Dong and Xiaofei Wang and Donglin Zeng and Jianwen Cai},
  doi          = {10.1111/biom.13609},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1213-1225},
  shortjournal = {Biometrics},
  title        = {Improving trial generalizability using observational studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A compound decision approach to covariance matrix
estimation. <em>BIOMTC</em>, <em>79</em>(2), 1201–1212. (<a
href="https://doi.org/10.1111/biom.13686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance matrix estimation is a fundamental statistical task in many applications, but the sample covariance matrix is suboptimal when the sample size is comparable to or less than the number of features. Such high-dimensional settings are common in modern genomics, where covariance matrix estimation is frequently employed as a method for inferring gene networks. To achieve estimation accuracy in these settings, existing methods typically either assume that the population covariance matrix has some particular structure, for example, sparsity, or apply shrinkage to better estimate the population eigenvalues. In this paper, we study a new approach to estimating high-dimensional covariance matrices. We first frame covariance matrix estimation as a compound decision problem. This motivates defining a class of decision rules and using a nonparametric empirical Bayes g -modeling approach to estimate the optimal rule in the class. Simulation results and gene network inference in an RNA-seq experiment in mouse show that our approach is comparable to or can outperform a number of state-of-the-art proposals.},
  archive      = {J_BIOMTC},
  author       = {Huiqin Xin and Sihai Dave Zhao},
  doi          = {10.1111/biom.13686},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1201-1212},
  shortjournal = {Biometrics},
  title        = {A compound decision approach to covariance matrix estimation},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decomposition of variation of mixed variables by a latent
mixed gaussian copula model. <em>BIOMTC</em>, <em>79</em>(2), 1187–1200.
(<a href="https://doi.org/10.1111/biom.13660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biomedical studies collect data of mixed types of variables from multiple groups of subjects. Some of these studies aim to find the group-specific and the common variation among all these variables. Even though similar problems have been studied by some previous works, their methods mainly rely on the Pearson correlation, which cannot handle mixed data. To address this issue, we propose a latent mixed Gaussian copula (LMGC) model that can quantify the correlations among binary, ordinal, continuous, and truncated variables in a unified framework. We also provide a tool to decompose the variation into the group-specific and the common variation over multiple groups via solving a regularized M -estimation problem. We conduct extensive simulation studies to show the advantage of our proposed method over the Pearson correlation-based methods. We also demonstrate that by jointly solving the M -estimation problem over multiple groups, our method is better than decomposing the variation group by group. We also apply our method to a Chlamydia trachomatis genital tract infection study to demonstrate how it can be used to discover informative biomarkers that differentiate patients.},
  archive      = {J_BIOMTC},
  author       = {Yutong Liu and Toni Darville and Xiaojing Zheng and Quefeng Li},
  doi          = {10.1111/biom.13660},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1187-1200},
  shortjournal = {Biometrics},
  title        = {Decomposition of variation of mixed variables by a latent mixed gaussian copula model},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for nonparanormal partial correlation via
regularized rank-based nodewise regression. <em>BIOMTC</em>,
<em>79</em>(2), 1173–1186. (<a
href="https://doi.org/10.1111/biom.13624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial correlation is a common tool in studying conditional dependence for Gaussian distributed data. However, partial correlation being zero may not be equivalent to conditional independence under non-Gaussian distributions. In this paper, we propose a statistical inference procedure for partial correlations under the high-dimensional nonparanormal (NPN) model where the observed data are normally distributed after certain monotone transformations. The NPN partial correlation is the partial correlation of the normal transformed data under the NPN model, which is a more general measure of conditional dependence. We estimate the NPN partial correlations by regularized nodewise regression based on the empirical ranks of the original data. A multiple testing procedure is proposed to identify the nonzero NPN partial correlations. The proposed method can be carried out by a simple coordinate descent algorithm for lasso optimization. It is easy-to-implement and computationally more efficient compared to the existing methods for estimating NPN graphical models. Theoretical results are developed to show the asymptotic normality of the proposed estimator and to justify the proposed multiple testing procedure. Numerical simulations and a case study on brain imaging data demonstrate the utility of the proposed procedure and evaluate its performance compared to the existing methods. Data used in preparation of this article were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database.},
  archive      = {J_BIOMTC},
  author       = {Haoyan Hu and Yumou Qiu},
  doi          = {10.1111/biom.13624},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1173-1186},
  shortjournal = {Biometrics},
  title        = {Inference for nonparanormal partial correlation via regularized rank-based nodewise regression},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The generalized fisher’s combination and accurate p-value
calculation under dependence. <em>BIOMTC</em>, <em>79</em>(2),
1159–1172. (<a href="https://doi.org/10.1111/biom.13634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining dependent tests of significance has broad applications but the related p -value calculation is challenging. For Fisher&#39;s combination test, current p -value calculation methods (eg, Brown&#39;s approximation) tend to inflate the type I error rate when the desired significance level is substantially less than 0.05. The problem could lead to significant false discoveries in big data analyses. This paper provides two main contributions. First, it presents a general family of Fisher type statistics, referred to as the GFisher, which covers many classic statistics, such as Fisher&#39;s combination, Good&#39;s statistic, Lancaster&#39;s statistic, weighted Z-score combination, and so forth. The GFisher allows a flexible weighting scheme, as well as an omnibus procedure that automatically adapts proper weights and the statistic-defining parameters to a given data. Second, the paper presents several new p -value calculation methods based on two novel ideas: moment-ratio matching and joint-distribution surrogating. Systematic simulations show that the new calculation methods are more accurate under multivariate Gaussian, and more robust under the generalized linear model and the multivariate t -distribution. The applications of the GFisher and the new p -value calculation methods are demonstrated by a gene-based single nucleotide polymorphism (SNP)-set association study. Relevant computation has been implemented to an R package GFisher available on the Comprehensive R Archive Network.},
  archive      = {J_BIOMTC},
  author       = {Hong Zhang and Zheyang Wu},
  doi          = {10.1111/biom.13634},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1159-1172},
  shortjournal = {Biometrics},
  title        = {The generalized fisher&#39;s combination and accurate p-value calculation under dependence},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimated quadratic inference function for correlated
failure time data. <em>BIOMTC</em>, <em>79</em>(2), 1145–1158. (<a
href="https://doi.org/10.1111/biom.13633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An estimated quadratic inference function method is proposed for correlated failure time data with auxiliary covariates. The proposed method makes efficient use of the auxiliary information for the incomplete exposure covariates and preserves the property of the quadratic inference function method that requires the covariates to be completely observed. It can improve the estimation efficiency and easily deal with the situation when the cluster size is large. The proposed estimator which minimizes the estimated quadratic inference function is shown to be consistent and asymptotically normal. A chi-squared test based on the estimated quadratic inference function is proposed to test hypotheses about the regression parameters. The small-sample performance of the proposed method is investigated through extensive simulation studies. The proposed method is then applied to analyze the Study of Left Ventricular Dysfunction (SOLVD) data as an illustration.},
  archive      = {J_BIOMTC},
  author       = {Feifei Yan and Yanyan Liu and Jianwen Cai and Haibo Zhou},
  doi          = {10.1111/biom.13633},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1145-1158},
  shortjournal = {Biometrics},
  title        = {Estimated quadratic inference function for correlated failure time data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact-corrected confidence interval for risk difference in
noninferiority binomial trials. <em>BIOMTC</em>, <em>79</em>(2),
1133–1144. (<a href="https://doi.org/10.1111/biom.13688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel confidence interval estimator is proposed for the risk difference in noninferiority binomial trials. The proposed confidence interval, which is dependent on the prespecified noninferiority margin, is consistent with an exact unconditional test that preserves the type-I error and has improved power, particularly for smaller sample sizes, compared to the confidence interval by Chan and Zhang. The improved performance of the proposed confidence interval is theoretically justified and demonstrated with simulations and examples. An R package is also distributed that implements the proposed methods along with other confidence interval estimators.},
  archive      = {J_BIOMTC},
  author       = {Nour Hawila and Arthur Berg},
  doi          = {10.1111/biom.13688},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1133-1144},
  shortjournal = {Biometrics},
  title        = {Exact-corrected confidence interval for risk difference in noninferiority binomial trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain selection and familywise error rate for functional
data: A unified framework. <em>BIOMTC</em>, <em>79</em>(2), 1119–1132.
(<a href="https://doi.org/10.1111/biom.13669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data are smooth, often continuous, random curves, which can be seen as an extreme case of multivariate data with infinite dimensionality. Just as componentwise inference for multivariate data naturally performs feature selection, subsetwise inference for functional data performs domain selection. In this paper, we present a unified testing framework for domain selection on populations of functional data. In detail, p -values of hypothesis tests performed on pointwise evaluations of functional data are suitably adjusted for providing control of the familywise error rate (FWER) over a family of subsets of the domain. We show that several state-of-the-art domain selection methods fit within this framework and differ from each other by the choice of the family over which the control of the FWER is provided. In the existing literature, these families are always defined a priori. In this work, we also propose a novel approach, coined thresholdwise testing, in which the family of subsets is instead built in a data-driven fashion. The method seamlessly generalizes to multidimensional domains in contrast to methods based on a priori defined families. We provide theoretical results with respect to consistency and control of the FWER for the methods within the unified framework. We illustrate the performance of the methods within the unified framework on simulated and real data examples and compare their performance with other existing methods.},
  archive      = {J_BIOMTC},
  author       = {Konrad Abramowicz and Alessia Pini and Lina Schelin and Sara Sjöstedt de Luna and Aymeric Stamm and Simone Vantini},
  doi          = {10.1111/biom.13669},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1119-1132},
  shortjournal = {Biometrics},
  title        = {Domain selection and familywise error rate for functional data: A unified framework},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A note on familywise error rate for a primary and secondary
endpoint. <em>BIOMTC</em>, <em>79</em>(2), 1114–1118. (<a
href="https://doi.org/10.1111/biom.13668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hung et al. (2007) considered the problem of controlling the type I error rate for a primary and secondary endpoint in a clinical trial using a gatekeeping approach in which the secondary endpoint is tested only if the primary endpoint crosses its monitoring boundary. They considered a two-look trial and showed by simulation that the naive method of testing the secondary endpoint at full level α at the time the primary endpoint reaches statistical significance does not control the familywise error rate at level α. Tamhane et al. (2010) derived analytic expressions for familywise error rate and power and confirmed the inflated error rate of the naive approach. Nonetheless, many people mistakenly believe that the closure principle can be used to prove that the naive procedure controls the familywise error rate. The purpose of this note is to explain in greater detail why there is a problem with the naive approach and show that the degree of alpha inflation can be as high as that of unadjusted monitoring of a single endpoint.},
  archive      = {J_BIOMTC},
  author       = {Michael A. Proschan and Dean A. Follmann},
  doi          = {10.1111/biom.13668},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1114-1118},
  shortjournal = {Biometrics},
  title        = {A note on familywise error rate for a primary and secondary endpoint},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Closed testing with globaltest, with application in
metabolomics. <em>BIOMTC</em>, <em>79</em>(2), 1103–1113. (<a
href="https://doi.org/10.1111/biom.13693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Globaltest is a powerful test for the global null hypothesis that there is no association between a group of features and a response of interest, which is popular in pathway testing in metabolomics. Evaluating multiple feature sets, however, requires multiple testing correction. In this paper, we propose a multiple testing method, based on closed testing, specifically designed for the Globaltest. The proposed method controls the familywise error rate simultaneously over all possible feature sets, and therefore allows post hoc inference, that is, the researcher may choose feature sets of interest after seeing the data without jeopardizing error control. To circumvent the exponential computation time of closed testing, we derive a novel shortcut that allows exact closed testing to be performed on the scale of metabolomics data. An R package ctgt is available on comprehensive R archive network for the implementation of the shortcut procedure, with applications on several real metabolomics data examples.},
  archive      = {J_BIOMTC},
  author       = {Ningning Xu and Aldo Solari and Jelle J. Goeman},
  doi          = {10.1111/biom.13693},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1103-1113},
  shortjournal = {Biometrics},
  title        = {Closed testing with globaltest, with application in metabolomics},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Zero-inflated poisson models with measurement error in the
response. <em>BIOMTC</em>, <em>79</em>(2), 1089–1102. (<a
href="https://doi.org/10.1111/biom.13657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-inflated count data arise frequently from genomics studies. Analysis of such data is often based on a mixture model which facilitates excess zeros in combination with a Poisson distribution, and various inference methods have been proposed under such a model. Those analysis procedures, however, are challenged by the presence of measurement error in count responses. In this article, we propose a new measurement error model to describe error-contaminated count data. We show that ignoring the measurement error effects in the analysis may generally lead to invalid inference results, and meanwhile, we identify situations where ignoring measurement error can still yield consistent estimators. Furthermore, we propose a Bayesian method to address the effects of measurement error under the zero-inflated Poisson model and discuss the identifiability issues. We develop a data-augmentation algorithm that is easy to implement. Simulation studies are conducted to evaluate the performance of the proposed method. We apply our method to analyze the data arising from a prostate adenocarcinoma genomic study.},
  archive      = {J_BIOMTC},
  author       = {Qihuang Zhang and Grace Y. Yi},
  doi          = {10.1111/biom.13657},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1089-1102},
  shortjournal = {Biometrics},
  title        = {Zero-inflated poisson models with measurement error in the response},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Generalized network structured models with mixed responses
subject to measurement error and misclassification. <em>BIOMTC</em>,
<em>79</em>(2), 1073–1088. (<a
href="https://doi.org/10.1111/biom.13623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research of complex associations between a gene network and multiple responses has attracted increasing attention. A great challenge in analyzing genetic data is posited by the presence of the genetic network that is typically unknown. Moreover, mismeasurement of responses introduces additional complexity to distort usual inferential procedures. In this paper, we consider the problem with mixed binary and continuous responses that are subject to mismeasurement and associated with complex structured covariates. We first start with the case where data are precisely measured. We propose a generalized network structured model and develop a two-step inferential procedure. In the first step, we employ a Gaussian graphical model to facilitate the covariates network structure, and in the second step, we incorporate the estimated graphical structure of covariates and develop an estimating equation method. Furthermore, we extend the development to accommodating mismeasured responses. We consider two cases where the information on mismeasurement is either known or estimated from a validation sample. Theoretical results are established and numerical studies are conducted to evaluate the finite sample performance of the proposed methods. We apply the proposed method to analyze the outbred Carworth Farms White mice data arising from a genome-wide association study.},
  archive      = {J_BIOMTC},
  author       = {Qihuang Zhang and Grace Y. Yi},
  doi          = {10.1111/biom.13623},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1073-1088},
  shortjournal = {Biometrics},
  title        = {Generalized network structured models with mixed responses subject to measurement error and misclassification},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and robust methods for causally interpretable
meta-analysis: Transporting inferences from multiple randomized trials
to a target population. <em>BIOMTC</em>, <em>79</em>(2), 1057–1072. (<a
href="https://doi.org/10.1111/biom.13716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present methods for causally interpretable meta-analyses that combine information from multiple randomized trials to draw causal inferences for a target population of substantive interest. We consider identifiability conditions, derive implications of the conditions for the law of the observed data, and obtain identification results for transporting causal inferences from a collection of independent randomized trials to a new target population in which experimental data may not be available. We propose an estimator for the potential outcome mean in the target population under each treatment studied in the trials. The estimator uses covariate, treatment, and outcome data from the collection of trials, but only covariate data from the target population sample. We show that it is doubly robust in the sense that it is consistent and asymptotically normal when at least one of the models it relies on is correctly specified. We study the finite sample properties of the estimator in simulation studies and demonstrate its implementation using data from a multicenter randomized trial.},
  archive      = {J_BIOMTC},
  author       = {Issa J. Dahabreh and Sarah E. Robertson and Lucia C. Petito and Miguel A. Hernán and Jon A. Steingrimsson},
  doi          = {10.1111/biom.13716},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1057-1072},
  shortjournal = {Biometrics},
  title        = {Efficient and robust methods for causally interpretable meta-analysis: Transporting inferences from multiple randomized trials to a target population},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Posttreatment confounding in causal mediation studies: A
cutting-edge problem and a novel solution via sensitivity analysis.
<em>BIOMTC</em>, <em>79</em>(2), 1042–1056. (<a
href="https://doi.org/10.1111/biom.13705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In causal mediation studies that decompose an average treatment effect into indirect and direct effects, examples of posttreatment confounding are abundant. In the presence of treatment-by-mediator interactions, past research has generally considered it infeasible to adjust for a posttreatment confounder of the mediator–outcome relationship due to incomplete information: for any given individual, a posttreatment confounder is observed under the actual treatment condition while missing under the counterfactual treatment condition. This paper proposes a new sensitivity analysis strategy for handling posttreatment confounding and incorporates it into weighting-based causal mediation analysis. The key is to obtain the conditional distribution of the posttreatment confounder under the counterfactual treatment as a function of not only pretreatment covariates but also its counterpart under the actual treatment. The sensitivity analysis then generates a bound for the natural indirect effect and that for the natural direct effect over a plausible range of the conditional correlation between the posttreatment confounder under the actual and that under the counterfactual conditions. Implemented through either imputation or integration, the strategy is suitable for binary as well as continuous measures of posttreatment confounders. Simulation results demonstrate major strengths and potential limitations of this new solution. A reanalysis of the National Evaluation of Welfare-to-Work Strategies (NEWWS) Riverside data reveals that the initial analytic results are sensitive to omitted posttreatment confounding.},
  archive      = {J_BIOMTC},
  author       = {Guanglei Hong and Fan Yang and Xu Qin},
  doi          = {10.1111/biom.13705},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1042-1056},
  shortjournal = {Biometrics},
  title        = {Posttreatment confounding in causal mediation studies: A cutting-edge problem and a novel solution via sensitivity analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric inverse-probability-weighted estimators based
on the highly adaptive lasso. <em>BIOMTC</em>, <em>79</em>(2),
1029–1041. (<a href="https://doi.org/10.1111/biom.13719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse-probability-weighted estimators are the oldest and potentially most commonly used class of procedures for the estimation of causal effects. By adjusting for selection biases via a weighting mechanism, these procedures estimate an effect of interest by constructing a pseudopopulation in which selection biases are eliminated. Despite their ease of use, these estimators require the correct specification of a model for the weighting mechanism, are known to be inefficient, and suffer from the curse of dimensionality. We propose a class of nonparametric inverse-probability-weighted estimators in which the weighting mechanism is estimated via undersmoothing of the highly adaptive lasso, a nonparametric regression function proven to converge at nearly n − 1 / 3 $ n^{-1/3}$ -rate to the true weighting mechanism. We demonstrate that our estimators are asymptotically linear with variance converging to the nonparametric efficiency bound. Unlike doubly robust estimators, our procedures require neither derivation of the efficient influence function nor specification of the conditional outcome model. Our theoretical developments have broad implications for the construction of efficient inverse-probability-weighted estimators in large statistical models and a variety of problem settings. We assess the practical performance of our estimators in simulation studies and demonstrate use of our proposed methodology with data from a large-scale epidemiologic study.},
  archive      = {J_BIOMTC},
  author       = {Ashkan Ertefaie and Nima S. Hejazi and Mark J. van der Laan},
  doi          = {10.1111/biom.13719},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1029-1041},
  shortjournal = {Biometrics},
  title        = {Nonparametric inverse-probability-weighted estimators based on the highly adaptive lasso},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric estimation of the causal effect of a
stochastic threshold-based intervention. <em>BIOMTC</em>,
<em>79</em>(2), 1014–1028. (<a
href="https://doi.org/10.1111/biom.13690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying a biomarker or treatment-dose threshold that marks a specified level of risk is an important problem, especially in clinical trials. In view of this goal, we consider a covariate-adjusted threshold-based interventional estimand, which happens to equal the binary treatment–specific mean estimand from the causal inference literature obtained by dichotomizing the continuous biomarker or treatment as above or below a threshold. The unadjusted version of this estimand was considered in Donovan et al. . Expanding upon Stitelman et al. , we show that this estimand, under conditions, identifies the expected outcome of a stochastic intervention that sets the treatment dose of all participants above the threshold. We propose a novel nonparametric efficient estimator for the covariate-adjusted threshold-response function for the case of informative outcome missingness, which utilizes machine learning and targeted minimum-loss estimation (TMLE). We prove the estimator is efficient and characterize its asymptotic distribution and robustness properties. Construction of simultaneous 95\% confidence bands for the threshold-specific estimand across a set of thresholds is discussed. In the Supporting Information, we discuss how to adjust our estimator when the biomarker is missing at random, as occurs in clinical trials with biased sampling designs, using inverse probability weighting. Efficiency and bias reduction of the proposed estimator are assessed in simulations. The methods are employed to estimate neutralizing antibody thresholds for virologically confirmed dengue risk in the CYD14 and CYD15 dengue vaccine trials.},
  archive      = {J_BIOMTC},
  author       = {Lars van der Laan and Wenbo Zhang and Peter B. Gilbert},
  doi          = {10.1111/biom.13690},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1014-1028},
  shortjournal = {Biometrics},
  title        = {Nonparametric estimation of the causal effect of a stochastic threshold-based intervention},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric and semiparametric estimation with
sequentially truncated survival data. <em>BIOMTC</em>, <em>79</em>(2),
1000–1013. (<a href="https://doi.org/10.1111/biom.13678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational cohort studies with complex sampling schemes, truncation arises when the time to event of interest is observed only when it falls below or exceeds another random time, that is, the truncation time. In more complex settings, observation may require a particular ordering of event times; we refer to this as sequential truncation . Estimators of the event time distribution have been developed for simple left-truncated or right-truncated data. However, these estimators may be inconsistent under sequential truncation. We propose nonparametric and semiparametric maximum likelihood estimators for the distribution of the event time of interest in the presence of sequential truncation, under two truncation models. We show the equivalence of an inverse probability weighted estimator and a product limit estimator under one of these models. We study the large sample properties of the proposed estimators and derive their asymptotic variance estimators. We evaluate the proposed methods through simulation studies and apply the methods to an Alzheimer&#39;s disease study. We have developed an R package, seqTrun , for implementation of our method.},
  archive      = {J_BIOMTC},
  author       = {Rebecca A. Betensky and Jing Qian and Jingyao Hou},
  doi          = {10.1111/biom.13678},
  journal      = {Biometrics},
  number       = {2},
  pages        = {1000-1013},
  shortjournal = {Biometrics},
  title        = {Nonparametric and semiparametric estimation with sequentially truncated survival data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection in regression-based estimation of dynamic
treatment regimes. <em>BIOMTC</em>, <em>79</em>(2), 988–999. (<a
href="https://doi.org/10.1111/biom.13608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) consist of a sequence of decision rules, one per stage of intervention, that aim to recommend effective treatments for individual patients according to patient information history. DTRs can be estimated from models which include interactions between treatment and a (typically small) number of covariates which are often chosen a priori. However, with increasingly large and complex data being collected, it can be difficult to know which prognostic factors might be relevant in the treatment rule. Therefore, a more data-driven approach to select these covariates might improve the estimated decision rules and simplify models to make them easier to interpret. We propose a variable selection method for DTR estimation using penalized dynamic weighted least squares. Our method has the strong heredity property, that is, an interaction term can be included in the model only if the corresponding main terms have also been selected. We show our method has both the double robustness property and the oracle property theoretically; and the newly proposed method compares favorably with other variable selection approaches in numerical studies. We further illustrate the proposed method on data from the Sequenced Treatment Alternatives to Relieve Depression study.},
  archive      = {J_BIOMTC},
  author       = {Zeyu Bian and Erica E. M. Moodie and Susan M. Shortreed and Sahir Bhatnagar},
  doi          = {10.1111/biom.13608},
  journal      = {Biometrics},
  number       = {2},
  pages        = {988-999},
  shortjournal = {Biometrics},
  title        = {Variable selection in regression-based estimation of dynamic treatment regimes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of the odds ratio in a proportional odds model
with censored time-lagged outcome in a randomized clinical trial.
<em>BIOMTC</em>, <em>79</em>(2), 975–987. (<a
href="https://doi.org/10.1111/biom.13603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many randomized clinical trials of therapeutics for COVID-19, the primary outcome is an ordinal categorical variable, and interest focuses on the odds ratio (OR; active agent vs control) under the assumption of a proportional odds model. Although at the final analysis the outcome will be determined for all subjects, at an interim analysis, the status of some participants may not yet be determined, for example, because ascertainment of the outcome may not be possible until some prespecified follow-up time. Accordingly, the outcome from these subjects can be viewed as censored. A valid interim analysis can be based on data only from those subjects with full follow-up; however, this approach is inefficient, as it does not exploit additional information that may be available on those for whom the outcome is not yet available at the time of the interim analysis. Appealing to the theory of semiparametrics, we propose an estimator for the OR in a proportional odds model with censored, time-lagged categorical outcome that incorporates additional baseline and time-dependent covariate information and demonstrate that it can result in considerable gains in efficiency relative to simpler approaches. A byproduct of the approach is a covariate-adjusted estimator for the OR based on the full data that would be available at a final analysis.},
  archive      = {J_BIOMTC},
  author       = {Anastasios A. Tsiatis and Marie Davidian and Shannon T. Holloway},
  doi          = {10.1111/biom.13603},
  journal      = {Biometrics},
  number       = {2},
  pages        = {975-987},
  shortjournal = {Biometrics},
  title        = {Estimation of the odds ratio in a proportional odds model with censored time-lagged outcome in a randomized clinical trial},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random projection ensemble classification with
high-dimensional time series. <em>BIOMTC</em>, <em>79</em>(2), 964–974.
(<a href="https://doi.org/10.1111/biom.13679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time-series (MTS) data are prevalent in diverse domains and often high dimensional. We propose new random projection ensemble classifiers with high-dimensional MTS. The method first applies dimension reduction in the time domain via randomly projecting the time-series variables into some low-dimensional space, followed by measuring the disparity via some novel base classifier between the data and the candidate generating processes in the projected space. Our contributions are twofold: (i) We derive optimal weighted majority voting schemes for pooling information from the base classifiers for multiclass classification and (ii) we introduce new base frequency-domain classifiers based on Whittle likelihood (WL), Kullback-Leibler (KL) divergence, eigen-distance (ED), and Chernoff (CH) divergence. Both simulations for binary and multiclass problems, and an Electroencephalogram (EEG) application demonstrate the efficacy of the proposed methods in constructing accurate classifiers with high-dimensional MTS.},
  archive      = {J_BIOMTC},
  author       = {Fuli Zhang and Kung-Sik Chan},
  doi          = {10.1111/biom.13679},
  journal      = {Biometrics},
  number       = {2},
  pages        = {964-974},
  shortjournal = {Biometrics},
  title        = {Random projection ensemble classification with high-dimensional time series},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general framework of nonparametric feature selection in
high-dimensional data. <em>BIOMTC</em>, <em>79</em>(2), 951–963. (<a
href="https://doi.org/10.1111/biom.13664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric feature selection for high-dimensional data is an important and challenging problem in the fields of statistics and machine learning. Most of the existing methods for feature selection focus on parametric or additive models which may suffer from model misspecification. In this paper, we propose a new framework to perform nonparametric feature selection for both regression and classification problems. Under this framework, we learn prediction functions through empirical risk minimization over a reproducing kernel Hilbert space. The space is generated by a novel tensor product kernel, which depends on a set of parameters that determines the importance of the features. Computationally, we minimize the empirical risk with a penalty to estimate the prediction and kernel parameters simultaneously. The solution can be obtained by iteratively solving convex optimization problems. We study the theoretical property of the kernel feature space and prove the oracle selection property and Fisher consistency of our proposed method. Finally, we demonstrate the superior performance of our approach compared to existing methods via extensive simulation studies and applications to two real studies.},
  archive      = {J_BIOMTC},
  author       = {Hang Yu and Yuanjia Wang and Donglin Zeng},
  doi          = {10.1111/biom.13664},
  journal      = {Biometrics},
  number       = {2},
  pages        = {951-963},
  shortjournal = {Biometrics},
  title        = {A general framework of nonparametric feature selection in high-dimensional data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering high-dimensional data via feature selection.
<em>BIOMTC</em>, <em>79</em>(2), 940–950. (<a
href="https://doi.org/10.1111/biom.13665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional clustering analysis is a challenging problem in statistics and machine learning, with broad applications such as the analysis of microarray data and RNA-seq data. In this paper, we propose a new clustering procedure called spectral clustering with feature selection (SC-FS), where we first obtain an initial estimate of labels via spectral clustering, then select a small fraction of features with the largest R -squared with these labels, that is, the proportion of variation explained by group labels, and conduct clustering again using selected features. Under mild conditions, we prove that the proposed method identifies all informative features with high probability and achieves the minimax optimal clustering error rate for the sparse Gaussian mixture model. Applications of SC-FS to four real-world datasets demonstrate its usefulness in clustering high-dimensional data.},
  archive      = {J_BIOMTC},
  author       = {Tianqi Liu and Yu Lu and Biqing Zhu and Hongyu Zhao},
  doi          = {10.1111/biom.13665},
  journal      = {Biometrics},
  number       = {2},
  pages        = {940-950},
  shortjournal = {Biometrics},
  title        = {Clustering high-dimensional data via feature selection},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Screening methods for linear errors-in-variables models in
high dimensions. <em>BIOMTC</em>, <em>79</em>(2), 926–939. (<a
href="https://doi.org/10.1111/biom.13628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray studies, in order to identify genes associated with an outcome of interest, usually produce noisy measurements for a large number of gene expression features from a small number of subjects. One common approach to analyzing such high-dimensional data is to use linear errors-in-variables (EIV) models; however, current methods for fitting such models are computationally expensive. In this paper, we present two efficient screening procedures, namely, corrected penalized marginal screening (PMSc) and corrected sure independence screening (SISc), to reduce the number of variables for final model building. Both screening procedures are based on fitting corrected marginal regression models relating the outcome to each contaminated covariate separately, which can be computed efficiently even with a large number of features. Under mild conditions, we show that these procedures achieve screening consistency and reduce the number of features substantially, even when the number of covariates grows exponentially with sample size. In addition, if the true covariates are weakly correlated, we show that PMSc can achieve full variable selection consistency. Through a simulation study and an analysis of gene expression data for bone mineral density of Norwegian women, we demonstrate that the two new screening procedures make estimation of linear EIV models computationally scalable in high-dimensional settings, and improve finite sample estimation and selection performance compared with estimators that do not employ a screening stage.},
  archive      = {J_BIOMTC},
  author       = {Linh H. Nghiem and Francis K.C. Hui and Samuel Müller and A.H. Welsh},
  doi          = {10.1111/biom.13628},
  journal      = {Biometrics},
  number       = {2},
  pages        = {926-939},
  shortjournal = {Biometrics},
  title        = {Screening methods for linear errors-in-variables models in high dimensions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint gene network construction by single-cell RNA
sequencing data. <em>BIOMTC</em>, <em>79</em>(2), 915–925. (<a
href="https://doi.org/10.1111/biom.13645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to differential gene expression analysis at the single-gene level, gene regulatory network (GRN) analysis depicts complex transcriptomic interactions among genes for better understandings of underlying genetic architectures of human diseases and traits. Recent advances in single-cell RNA sequencing (scRNA-seq) allow constructing GRNs at a much finer resolution than bulk RNA-seq and microarray data. However, scRNA-seq data are inherently sparse, which hinders the direct application of the popular Gaussian graphical models (GGMs). Furthermore, most existing approaches for constructing GRNs with scRNA-seq data only consider gene networks under one condition. To better understand GRNs across different but related conditions at single-cell resolution, we propose to construct Joint Gene Networks with scRNA-seq data (JGNsc) under the GGMs framework. To facilitate the use of GGMs, JGNsc first proposes a hybrid imputation procedure that combines a Bayesian zero-inflated Poisson model with an iterative low-rank matrix completion step to efficiently impute zero-inflated counts resulted from technical artifacts. JGNsc then transforms the imputed data via a nonparanormal transformation, based on which joint GGMs are constructed. We demonstrate JGNsc and assess its performance using synthetic data. The application of JGNsc on two cancer clinical studies of medulloblastoma and glioblastoma gains novel insights in addition to confirming well-known biological results.},
  archive      = {J_BIOMTC},
  author       = {Meichen Dong and Yiping He and Yuchao Jiang and Fei Zou},
  doi          = {10.1111/biom.13645},
  journal      = {Biometrics},
  number       = {2},
  pages        = {915-925},
  shortjournal = {Biometrics},
  title        = {Joint gene network construction by single-cell RNA sequencing data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra-high dimensional variable selection for doubly robust
causal inference. <em>BIOMTC</em>, <em>79</em>(2), 903–914. (<a
href="https://doi.org/10.1111/biom.13625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference has been increasingly reliant on observational studies with rich covariate information. To build tractable causal procedures, such as the doubly robust estimators, it is imperative to first extract important features from high or even ultra-high dimensional data. In this paper, we propose causal ball screening for confounder selection from modern ultra-high dimensional data sets. Unlike the familiar task of variable selection for prediction modeling, our confounder selection procedure aims to control for confounding while improving efficiency in the resulting causal effect estimate. Previous empirical and theoretical studies suggest excluding causes of the treatment that are not confounders. Motivated by these results, our goal is to keep all the predictors of the outcome in both the propensity score and outcome regression models. A distinctive feature of our proposal is that we use an outcome model-free procedure for propensity score model selection, thereby maintaining double robustness in the resulting causal effect estimator. Our theoretical analyses show that the proposed procedure enjoys a number of properties, including model selection consistency and pointwise normality. Synthetic and real data analysis show that our proposal performs favorably with existing methods in a range of realistic settings. Data used in preparation of this paper were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database.},
  archive      = {J_BIOMTC},
  author       = {Dingke Tang and Dehan Kong and Wenliang Pan and Linbo Wang},
  doi          = {10.1111/biom.13625},
  journal      = {Biometrics},
  number       = {2},
  pages        = {903-914},
  shortjournal = {Biometrics},
  title        = {Ultra-high dimensional variable selection for doubly robust causal inference},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An eigenvalue ratio approach to inferring population
structure from whole genome sequencing data. <em>BIOMTC</em>,
<em>79</em>(2), 891–902. (<a
href="https://doi.org/10.1111/biom.13691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference of population structure from genetic data plays an important role in population and medical genetics studies. With the advancement and decreasing cost of sequencing technology, the increasingly available whole genome sequencing data provide much richer information about the underlying population structure. The traditional method originally developed for array-based genotype data for computing and selecting top principal components (PCs) that capture population structure may not perform well on sequencing data for two reasons. First, the number of genetic variants p is much larger than the sample size n in sequencing data such that the sample-to-marker ratio n / p $n/p$ is nearly zero, violating the assumption of the Tracy-Widom test used in their method. Second, their method might not be able to handle the linkage disequilibrium well in sequencing data. To resolve those two practical issues, we propose a new method called ERStruct to determine the number of top informative PCs based on sequencing data. More specifically, we propose to use the ratio of consecutive eigenvalues as a more robust test statistic, and then we approximate its null distribution using modern random matrix theory. Both simulation studies and applications to two public data sets from the HapMap 3 and the 1000 Genomes Projects demonstrate the empirical performance of our ERStruct method.},
  archive      = {J_BIOMTC},
  author       = {Yuyang Xu and Zhonghua Liu and Jianfeng Yao},
  doi          = {10.1111/biom.13691},
  journal      = {Biometrics},
  number       = {2},
  pages        = {891-902},
  shortjournal = {Biometrics},
  title        = {An eigenvalue ratio approach to inferring population structure from whole genome sequencing data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature screening with latent responses. <em>BIOMTC</em>,
<em>79</em>(2), 878–890. (<a
href="https://doi.org/10.1111/biom.13658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel feature screening method is proposed to examine the correlation between latent responses and potential predictors in ultrahigh-dimensional data analysis. First, a confirmatory factor analysis (CFA) model is used to characterize latent responses through multiple observed variables. The expectation-maximization algorithm is employed to estimate the parameters in the CFA model. Second, R-Vector (RV) correlation is used to measure the dependence between the multivariate latent responses and covariates of interest. Third, a feature screening procedure is proposed on the basis of an unbiased estimator of the RV coefficient. The sure screening property of the proposed screening procedure is established under certain mild conditions. Monte Carlo simulations are conducted to assess the finite-sample performance of the feature screening procedure. The proposed method is applied to an investigation of the relationship between psychological well-being and the human genome.},
  archive      = {J_BIOMTC},
  author       = {Congran Yu and Wenwen Guo and Xinyuan Song and Hengjian Cui},
  doi          = {10.1111/biom.13658},
  journal      = {Biometrics},
  number       = {2},
  pages        = {878-890},
  shortjournal = {Biometrics},
  title        = {Feature screening with latent responses},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multisource single-cell data integration by MAW barycenter
for gaussian mixture models. <em>BIOMTC</em>, <em>79</em>(2), 866–877.
(<a href="https://doi.org/10.1111/biom.13630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key challenge encountered in single-cell data clustering is to combine clustering results of data sets acquired from multiple sources. We propose to represent the clustering result of each data set by a Gaussian mixture model (GMM) and produce an integrated result based on the notion of Wasserstein barycenter. However, the precise barycenter of GMMs, a distribution on the same sample space, is computationally infeasible to solve. Importantly, the barycenter of GMMs may not be a GMM containing a reasonable number of components. We thus propose to use the minimized aggregated Wasserstein (MAW) distance to approximate the Wasserstein metric and develop a new algorithm for computing the barycenter of GMMs under MAW. Recent theoretical advances further justify using the MAW distance as an approximation for the Wasserstein metric between GMMs. We also prove that the MAW barycenter of GMMs has the same expectation as the Wasserstein barycenter. Our proposed algorithm for clustering integration scales well with the data dimension and the number of mixture components, with complexity independent of data size. We demonstrate that the new method achieves better clustering results on several single-cell RNA-seq data sets than some other popular methods.},
  archive      = {J_BIOMTC},
  author       = {Lin Lin and Wei Shi and Jianbo Ye and Jia Li},
  doi          = {10.1111/biom.13630},
  journal      = {Biometrics},
  number       = {2},
  pages        = {866-877},
  shortjournal = {Biometrics},
  title        = {Multisource single-cell data integration by MAW barycenter for gaussian mixture models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating cell type composition using isoform expression
one gene at a time. <em>BIOMTC</em>, <em>79</em>(2), 854–865. (<a
href="https://doi.org/10.1111/biom.13614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human tissue samples are often mixtures of heterogeneous cell types, which can confound the analyses of gene expression data derived from such tissues. The cell type composition of a tissue sample may itself be of interest and is needed for proper analysis of differential gene expression. A variety of computational methods have been developed to estimate cell type proportions using gene-level expression data. However, RNA isoforms can also be differentially expressed across cell types, and isoform-level expression could be equally or more informative for determining cell type origin than gene-level expression. We propose a new computational method, IsoDeconvMM, which estimates cell type fractions using isoform-level gene expression data. A novel and useful feature of IsoDeconvMM is that it can estimate cell type proportions using only a single gene, though in practice we recommend aggregating estimates of a few dozen genes to obtain more accurate results. We demonstrate the performance of IsoDeconvMM using a unique data set with cell type–specific RNA-seq data across more than 135 individuals. This data set allows us to evaluate different methods given the biological variation of cell type–specific gene expression data across individuals. We further complement this analysis with additional simulations.},
  archive      = {J_BIOMTC},
  author       = {Hillary M. Heiling and Douglas R. Wilson and Naim U. Rashid and Wei Sun and Joseph G. Ibrahim},
  doi          = {10.1111/biom.13614},
  journal      = {Biometrics},
  number       = {2},
  pages        = {854-865},
  shortjournal = {Biometrics},
  title        = {Estimating cell type composition using isoform expression one gene at a time},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-trait prediction accuracy of summary statistics in
genome-wide association studies. <em>BIOMTC</em>, <em>79</em>(2),
841–853. (<a href="https://doi.org/10.1111/biom.13661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, univariate models have widely been used as a workhorse tool for quickly producing marginal estimators; and this is true even when in a high-dimensional dense setting, in which many features are “true,” but weak signals. Genome-wide association studies (GWAS) epitomize this type of setting. Although the GWAS marginal estimator is popular, it has long been criticized for ignoring the correlation structure of genetic variants (i.e., the linkage disequilibrium [LD] pattern). In this paper, we study the effects of LD pattern on the GWAS marginal estimator and investigate whether or not additionally accounting for the LD can improve the prediction accuracy of complex traits. We consider a general high-dimensional dense setting for GWAS and study a class of ridge-type estimators, including the popular marginal estimator and the best linear unbiased prediction (BLUP) estimator as two special cases. We show that the performance of GWAS marginal estimator depends on the LD pattern through the first three moments of its eigenvalue distribution. Furthermore, we uncover that the relative performance of GWAS marginal and BLUP estimators highly depends on the ratio of GWAS sample size over the number of genetic variants. Particularly, our finding reveals that the marginal estimator can easily become near-optimal within this class when the sample size is relatively small, even though it ignores the LD pattern. On the other hand, BLUP estimator has substantially better performance than the marginal estimator as the sample size increases toward the number of genetic variants, which is typically in millions. Therefore, adjusting for the LD (such as in the BLUP) is most needed when GWAS sample size is large. We illustrate the importance of our results by using the simulated data and real GWAS.},
  archive      = {J_BIOMTC},
  author       = {Bingxin Zhao and Fei Zou and Hongtu Zhu},
  doi          = {10.1111/biom.13661},
  journal      = {Biometrics},
  number       = {2},
  pages        = {841-853},
  shortjournal = {Biometrics},
  title        = {Cross-trait prediction accuracy of summary statistics in genome-wide association studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint fairness model with applications to risk predictions
for underrepresented populations. <em>BIOMTC</em>, <em>79</em>(2),
826–840. (<a href="https://doi.org/10.1111/biom.13632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data collection for predictive modeling, underrepresentation of certain groups, based on gender, race/ethnicity, or age, may yield less accurate predictions for these groups. Recently, this issue of fairness in predictions has attracted significant attention, as data-driven models are increasingly utilized to perform crucial decision-making tasks. Existing methods to achieve fairness in the machine learning literature typically build a single prediction model in a manner that encourages fair prediction performance for all groups. These approaches have two major limitations: (i) fairness is often achieved by compromising accuracy for some groups; (ii) the underlying relationship between dependent and independent variables may not be the same across groups. We propose a joint fairness model (JFM) approach for logistic regression models for binary outcomes that estimates group-specific classifiers using a joint modeling objective function that incorporates fairness criteria for prediction. We introduce an accelerated smoothing proximal gradient algorithm to solve the convex objective function, and present the key asymptotic properties of the JFM estimates. Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group&#39;s sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for underrepresented older patients diagnosed with coronavirus disease 2019 (COVID-19).},
  archive      = {J_BIOMTC},
  author       = {Hyungrok Do and Shinjini Nandi and Preston Putzel and Padhraic Smyth and Judy Zhong},
  doi          = {10.1111/biom.13632},
  journal      = {Biometrics},
  number       = {2},
  pages        = {826-840},
  shortjournal = {Biometrics},
  title        = {A joint fairness model with applications to risk predictions for underrepresented populations},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selective prediction-set models with coverage rate
guarantees. <em>BIOMTC</em>, <em>79</em>(2), 811–825. (<a
href="https://doi.org/10.1111/biom.13612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current approach to using machine learning (ML) algorithms in healthcare is to either require clinician oversight for every use case or use their predictions without any human oversight. We explore a middle ground that lets ML algorithms abstain from making a prediction to simultaneously improve their reliability and reduce the burden placed on human experts. To this end, we present a general penalized loss minimization framework for training selective prediction-set (SPS) models, which choose to either output a prediction set or abstain. The resulting models abstain when the outcome is difficult to predict accurately, such as on subjects who are too different from the training data, and achieve higher accuracy on those they do give predictions for. We then introduce a model-agnostic, statistical inference procedure for the coverage rate of an SPS model that ensembles individual models trained using K-fold cross-validation. We find that SPS ensembles attain prediction-set coverage rates closer to the nominal level and have narrower confidence intervals for its marginal coverage rate. We apply our method to train neural networks that abstain more for out-of-sample images on the MNIST digit prediction task and achieve higher predictive accuracy for ICU patients compared to existing approaches.},
  archive      = {J_BIOMTC},
  author       = {Jean Feng and Arjun Sondhi and Jessica Perry and Noah Simon},
  doi          = {10.1111/biom.13612},
  journal      = {Biometrics},
  number       = {2},
  pages        = {811-825},
  shortjournal = {Biometrics},
  title        = {Selective prediction-set models with coverage rate guarantees},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing for heterogeneity in the utility of a surrogate
marker. <em>BIOMTC</em>, <em>79</em>(2), 799–810. (<a
href="https://doi.org/10.1111/biom.13600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies that require long-term and/or costly follow-up of participants to evaluate a treatment, there is often interest in identifying and using a surrogate marker to evaluate the treatment effect. While several statistical methods have been proposed to evaluate potential surrogate markers, available methods generally do not account for or address the potential for a surrogate to vary in utility or strength by patient characteristics. Previous work examining surrogate markers has indicated that there may be such heterogeneity, that is, that a surrogate marker may be useful (with respect to capturing the treatment effect on the primary outcome) for some subgroups, but not for others. This heterogeneity is important to understand, particularly if the surrogate is to be used in a future trial to replace the primary outcome. In this paper, we propose an approach and estimation procedures to measure the surrogate strength as a function of a baseline covariate W and thus examine potential heterogeneity in the utility of the surrogate marker with respect to W . Within a potential outcome framework, we quantify the surrogate strength/utility using the proportion of treatment effect on the primary outcome that is explained by the treatment effect on the surrogate. We propose testing procedures to test for evidence of heterogeneity, examine finite sample performance of these methods via simulation, and illustrate the methods using AIDS clinical trial data.},
  archive      = {J_BIOMTC},
  author       = {Layla Parast and Tianxi Cai and Lu Tian},
  doi          = {10.1111/biom.13600},
  journal      = {Biometrics},
  number       = {2},
  pages        = {799-810},
  shortjournal = {Biometrics},
  title        = {Testing for heterogeneity in the utility of a surrogate marker},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust approach to combining multiple markers to improve
surrogacy. <em>BIOMTC</em>, <em>79</em>(2), 788–798. (<a
href="https://doi.org/10.1111/biom.13677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying effective and valid surrogate markers to make inference about a treatment effect on long-term outcomes is an important step in improving the efficiency of clinical trials. Replacing a long-term outcome with short-term and/or cheaper surrogate markers can potentially shorten study duration and reduce trial costs. There is sizable statistical literature on methods to quantify the effectiveness of a single surrogate marker. Both parametric and nonparametric approaches have been well developed for different outcome types. However, when there are multiple markers available, methods for combining markers to construct a composite marker with improved surrogacy remain limited. In this paper, building on top of the optimal transformation framework of Wang et al. (2020), we propose a novel calibrated model fusion approach to optimally combine multiple markers to improve surrogacy. Specifically, we obtain two initial estimates of optimal composite scores of the markers based on two sets of models with one set approximating the underlying data distribution and the other directly approximating the optimal transformation function. We then estimate an optimal calibrated combination of the two estimated scores which ensures both validity of the final combined score and optimality with respect to the proportion of treatment effect explained by the final combined score. This approach is unique in that it identifies an optimal combination of the multiple surrogates without strictly relying on parametric assumptions while borrowing modeling strategies to avoid fully nonparametric estimation which is subject to the curse of dimensionality. Our identified optimal transformation can also be used to directly quantify the surrogacy of this identified combined score. Theoretical properties of the proposed estimators are derived, and the finite sample performance of the proposed method is evaluated through simulation studies. We further illustrate the proposed method using data from the Diabetes Prevention Program study.},
  archive      = {J_BIOMTC},
  author       = {Xuan Wang and Layla Parast and Larry Han and Lu Tian and Tianxi Cai},
  doi          = {10.1111/biom.13677},
  journal      = {Biometrics},
  number       = {2},
  pages        = {788-798},
  shortjournal = {Biometrics},
  title        = {Robust approach to combining multiple markers to improve surrogacy},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coherent modeling of longitudinal causal effects on binary
outcomes. <em>BIOMTC</em>, <em>79</em>(2), 775–787. (<a
href="https://doi.org/10.1111/biom.13687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyses of biomedical studies often necessitate modeling longitudinal causal effects. The current focus on personalized medicine and effect heterogeneity makes this task even more challenging. Toward this end, structural nested mean models (SNMMs) are fundamental tools for studying heterogeneous treatment effects in longitudinal studies. However, when outcomes are binary, current methods for estimating multiplicative and additive SNMM parameters suffer from variation dependence between the causal parameters and the noncausal nuisance parameters. This leads to a series of difficulties in interpretation, estimation, and computation. These difficulties have hindered the uptake of SNMMs in biomedical practice, where binary outcomes are very common. We solve the variation dependence problem for the binary multiplicative SNMM via a reparameterization of the noncausal nuisance parameters. Our novel nuisance parameters are variation independent of the causal parameters, and hence allow for coherent modeling of heterogeneous effects from longitudinal studies with binary outcomes. Our parameterization also provides a key building block for flexible doubly robust estimation of the causal parameters. Along the way, we prove that an additive SNMM with binary outcomes does not admit a variation independent parameterization, thereby justifying the restriction to multiplicative SNMMs.},
  archive      = {J_BIOMTC},
  author       = {Linbo Wang and Xiang Meng and Thomas S. Richardson and James M. Robins},
  doi          = {10.1111/biom.13687},
  journal      = {Biometrics},
  number       = {2},
  pages        = {775-787},
  shortjournal = {Biometrics},
  title        = {Coherent modeling of longitudinal causal effects on binary outcomes},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based clustering of high-dimensional longitudinal data
via regularization. <em>BIOMTC</em>, <em>79</em>(2), 761–774. (<a
href="https://doi.org/10.1111/biom.13672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model-based clustering method for high-dimensional longitudinal data via regularization in this paper. This study was motivated by the Trial of Activity in Adolescent Girls (TAAG), which aimed to examine multilevel factors related to the change of physical activity by following up a cohort of 783 girls over 10 years from adolescence to early adulthood. Our goal is to identify the intrinsic grouping of subjects with similar patterns of physical activity trajectories and the most relevant predictors within each group. The previous analyses conducted clustering and variable selection in two steps, while our new method can perform the tasks simultaneously. Within each cluster, a linear mixed-effects model (LMM) is fitted with a doubly penalized likelihood to induce sparsity for parameter estimation and effect selection. The large-sample joint properties are established, allowing the dimensions of both fixed and random effects to increase at an exponential rate of the sample size, with a general class of penalty functions. Assuming subjects are drawn from a Gaussian mixture distribution, model effects and cluster labels are estimated via a coordinate descent algorithm nested inside the Expectation-Maximization (EM) algorithm. Bayesian Information Criterion (BIC) is used to determine the optimal number of clusters and the values of tuning parameters. Our numerical studies show that the new method has satisfactory performance and is able to accommodate complex data with multilevel and/or longitudinal effects.},
  archive      = {J_BIOMTC},
  author       = {Luoying Yang and Tong Tong Wu},
  doi          = {10.1111/biom.13672},
  journal      = {Biometrics},
  number       = {2},
  pages        = {761-774},
  shortjournal = {Biometrics},
  title        = {Model-based clustering of high-dimensional longitudinal data via regularization},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multikink quantile regression for longitudinal data with
application to progesterone data analysis. <em>BIOMTC</em>,
<em>79</em>(2), 747–760. (<a
href="https://doi.org/10.1111/biom.13667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by investigating the relationship between progesterone and the days in a menstrual cycle in a longitudinal study, we propose a multikink quantile regression model for longitudinal data analysis. It relaxes the linearity condition and assumes different regression forms in different regions of the domain of the threshold covariate. In this paper, we first propose a multikink quantile regression for longitudinal data. Two estimation procedures are proposed to estimate the regression coefficients and the kink points locations: one is a computationally efficient profile estimator under the working independence framework while the other one considers the within-subject correlations by using the unbiased generalized estimation equation approach. The selection consistency of the number of kink points and the asymptotic normality of two proposed estimators are established. Second, we construct a rank score test based on partial subgradients for the existence of the kink effect in longitudinal studies. Both the null distribution and the local alternative distribution of the test statistic have been derived. Simulation studies show that the proposed methods have excellent finite sample performance. In the application to the longitudinal progesterone data, we identify two kink points in the progesterone curves over different quantiles and observe that the progesterone level remains stable before the day of ovulation, then increases quickly in 5 to 6 days after ovulation and then changes to stable again or drops slightly.},
  archive      = {J_BIOMTC},
  author       = {Chuang Wan and Wei Zhong and Wenyang Zhang and Changliang Zou},
  doi          = {10.1111/biom.13667},
  journal      = {Biometrics},
  number       = {2},
  pages        = {747-760},
  shortjournal = {Biometrics},
  title        = {Multikink quantile regression for longitudinal data with application to progesterone data analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A time-heterogeneous d-vine copula model for unbalanced and
unequally spaced longitudinal data. <em>BIOMTC</em>, <em>79</em>(2),
734–746. (<a href="https://doi.org/10.1111/biom.13652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many longitudinal studies, the number and timing of measurements differ across study subjects. Statistical analysis of such data requires accounting for both the unbalanced study design and the unequal spacing of repeated measurements. This paper proposes a time-heterogeneous D-vine copula model that allows for time adjustment in the dependence structure of unequally spaced and potentially unbalanced longitudinal data. The proposed approach not only offers flexibility over its time-homogeneous counterparts but also allows for parsimonious model specifications at the tree or vine level for a given D-vine structure. It further provides a robust strategy to specify the joint distribution of non-Gaussian longitudinal data. The performance of the time-heterogeneous D-vine copula models are evaluated through simulation studies and by a real data application. Our findings suggest improved predictive performance of the proposed approach over the linear mixed-effects model and time-homogeneous D-vine copula model.},
  archive      = {J_BIOMTC},
  author       = {Md Erfanul Hoque and Elif F. Acar and Mahmoud Torabi},
  doi          = {10.1111/biom.13652},
  journal      = {Biometrics},
  number       = {2},
  pages        = {734-746},
  shortjournal = {Biometrics},
  title        = {A time-heterogeneous D-vine copula model for unbalanced and unequally spaced longitudinal data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional data analysis for longitudinal data with
informative observation times. <em>BIOMTC</em>, <em>79</em>(2), 722–733.
(<a href="https://doi.org/10.1111/biom.13646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In functional data analysis for longitudinal data, the observation process is typically assumed to be noninformative, which is often violated in real applications. Thus, methods that fail to account for the dependence between observation times and longitudinal outcomes may result in biased estimation. For longitudinal data with informative observation times, we find that under a general class of shared random effect models, a commonly used functional data method may lead to inconsistent model estimation while another functional data method results in consistent and even rate-optimal estimation. Indeed, we show that the mean function can be estimated appropriately via penalized splines and that the covariance function can be estimated appropriately via penalized tensor-product splines, both with specific choices of parameters. For the proposed method, theoretical results are provided, and simulation studies and a real data analysis are conducted to demonstrate its performance.},
  archive      = {J_BIOMTC},
  author       = {Caleb Weaver and Luo Xiao and Wenbin Lu},
  doi          = {10.1111/biom.13646},
  journal      = {Biometrics},
  number       = {2},
  pages        = {722-733},
  shortjournal = {Biometrics},
  title        = {Functional data analysis for longitudinal data with informative observation times},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural networks for clustered and longitudinal data using
mixed effects models. <em>BIOMTC</em>, <em>79</em>(2), 711–721. (<a
href="https://doi.org/10.1111/biom.13615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although most statistical methods for the analysis of longitudinal data have focused on retrospective models of association, new advances in mobile health data have presented opportunities for predicting future health status by leveraging an individual&#39;s behavioral history alongside data from similar patients. Methods that incorporate both individual-level and sample-level effects are critical to using these data to its full predictive capacity. Neural networks are powerful tools for prediction, but many assume input observations are independent even when they are clustered or correlated in some way, such as in longitudinal data. Generalized linear mixed models (GLMM) provide a flexible framework for modeling longitudinal data but have poor predictive power particularly when the data are highly nonlinear. We propose a generalized neural network mixed model that replaces the linear fixed effect in a GLMM with the output of a feed-forward neural network. The model simultaneously accounts for the correlation structure and complex nonlinear relationship between input variables and outcomes, and it utilizes the predictive power of neural networks. We apply this approach to predict depression and anxiety levels of schizophrenic patients using longitudinal data collected from passive smartphone sensor data.},
  archive      = {J_BIOMTC},
  author       = {Francesca Mandel and Riddhi Pratim Ghosh and Ian Barnett},
  doi          = {10.1111/biom.13615},
  journal      = {Biometrics},
  number       = {2},
  pages        = {711-721},
  shortjournal = {Biometrics},
  title        = {Neural networks for clustered and longitudinal data using mixed effects models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric additive time-varying coefficients model for
longitudinal data with censored time origin. <em>BIOMTC</em>,
<em>79</em>(2), 695–710. (<a
href="https://doi.org/10.1111/biom.13610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis of longitudinal data often involves modeling treatment effects on clinically relevant longitudinal biomarkers since an initial event (the time origin). In some studies including preventive HIV vaccine efficacy trials, some participants have biomarkers measured starting at the time origin, whereas others have biomarkers measured starting later with the time origin unknown. The semiparametric additive time-varying coefficient model is investigated where the effects of some covariates vary nonparametrically with time while the effects of others remain constant. Weighted profile least squares estimators coupled with kernel smoothing are developed. The method uses the expectation maximization approach to deal with the censored time origin. The Kaplan–Meier estimator and other failure time regression models such as the Cox model can be utilized to estimate the distribution and the conditional distribution of left censored event time related to the censored time origin. Asymptotic properties of the parametric and nonparametric estimators and consistent asymptotic variance estimators are derived. A two-stage estimation procedure for choosing weight is proposed to improve estimation efficiency. Numerical simulations are conducted to examine finite sample properties of the proposed estimators. The simulation results show that the theory and methods work well. The efficiency gain of the two-stage estimation procedure depends on the distribution of the longitudinal error processes. The method is applied to analyze data from the Merck 023/HVTN 502 Step HIV vaccine study.},
  archive      = {J_BIOMTC},
  author       = {Yanqing Sun and Qiong Shou and Peter B. Gilbert and Fei Heng and Xiyuan Qian},
  doi          = {10.1111/biom.13610},
  journal      = {Biometrics},
  number       = {2},
  pages        = {695-710},
  shortjournal = {Biometrics},
  title        = {Semiparametric additive time-varying coefficients model for longitudinal data with censored time origin},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust bayesian variable selection for gene–environment
interactions. <em>BIOMTC</em>, <em>79</em>(2), 684–694. (<a
href="https://doi.org/10.1111/biom.13670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene–environment (G× E) interactions have important implications to elucidate the etiology of complex diseases beyond the main genetic and environmental effects. Outliers and data contamination in disease phenotypes of G× E studies have been commonly encountered, leading to the development of a broad spectrum of robust regularization methods. Nevertheless, within the Bayesian framework, the issue has not been taken care of in existing studies. We develop a fully Bayesian robust variable selection method for G× E interaction studies. The proposed Bayesian method can effectively accommodate heavy-tailed errors and outliers in the response variable while conducting variable selection by accounting for structural sparsity. In particular, for the robust sparse group selection, the spike-and-slab priors have been imposed on both individual and group levels to identify important main and interaction effects robustly. An efficient Gibbs sampler has been developed to facilitate fast computation. Extensive simulation studies, analysis of diabetes data with single-nucleotide polymorphism measurements from the Nurses&#39; Health Study, and The Cancer Genome Atlas melanoma data with gene expression measurements demonstrate the superior performance of the proposed method over multiple competing alternatives.},
  archive      = {J_BIOMTC},
  author       = {Jie Ren and Fei Zhou and Xiaoxi Li and Shuangge Ma and Yu Jiang and Cen Wu},
  doi          = {10.1111/biom.13670},
  journal      = {Biometrics},
  number       = {2},
  pages        = {684-694},
  shortjournal = {Biometrics},
  title        = {Robust bayesian variable selection for gene–environment interactions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian sample size determination using commensurate priors
to leverage preexperimental data. <em>BIOMTC</em>, <em>79</em>(2),
669–683. (<a href="https://doi.org/10.1111/biom.13649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops Bayesian sample size formulae for experiments comparing two groups, where relevant preexperimental information from multiple sources can be incorporated in a robust prior to support both the design and analysis. We use commensurate predictive priors for borrowing of information and further place Gamma mixture priors on the precisions to account for preliminary belief about the pairwise (in)commensurability between parameters that underpin the historical and new experiments. Averaged over the probability space of the new experimental data, appropriate sample sizes are found according to criteria that control certain aspects of the posterior distribution, such as the coverage probability or length of a defined density region. Our Bayesian methodology can be applied to circumstances that compare two normal means, proportions, or event times. When nuisance parameters (such as variance) in the new experiment are unknown, a prior distribution can further be specified based on preexperimental data. Exact solutions are available based on most of the criteria considered for Bayesian sample size determination, while a search procedure is described in cases for which there are no closed-form expressions. We illustrate the application of our sample size formulae in the design of clinical trials, where pretrial information is available to be leveraged. Hypothetical data examples, motivated by a rare-disease trial with an elicited expert prior opinion, and a comprehensive performance evaluation of the proposed methodology are presented.},
  archive      = {J_BIOMTC},
  author       = {Haiyan Zheng and Thomas Jaki and James M.S. Wason},
  doi          = {10.1111/biom.13649},
  journal      = {Biometrics},
  number       = {2},
  pages        = {669-683},
  shortjournal = {Biometrics},
  title        = {Bayesian sample size determination using commensurate priors to leverage preexperimental data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian interaction selection model for multimodal
neuroimaging data analysis. <em>BIOMTC</em>, <em>79</em>(2), 655–668.
(<a href="https://doi.org/10.1111/biom.13648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodality or multiconstruct data arise increasingly in functional neuroimaging studies to characterize brain activity under different cognitive states. Relying on those high-resolution imaging collections, it is of great interest to identify predictive imaging markers and intermodality interactions with respect to behavior outcomes. Currently, most of the existing variable selection models do not consider predictive effects from interactions, and the desired higher-order terms can only be included in the predictive mechanism following a two-step procedure, suffering from potential misspecification. In this paper, we propose a unified Bayesian prior model to simultaneously identify main effect features and intermodality interactions within the same inference platform in the presence of high-dimensional data. To accommodate the brain topological information and correlation between modalities, our prior is designed by compiling the intermediate selection status of sequential partitions in light of the data structure and brain anatomical architecture, so that we can improve posterior inference and enhance biological plausibility. Through extensive simulations, we show the superiority of our approach in main and interaction effects selection, and prediction under multimodality data. Applying the method to the Adolescent Brain Cognitive Development (ABCD) study, we characterize the brain functional underpinnings with respect to general cognitive ability under different memory load conditions.},
  archive      = {J_BIOMTC},
  author       = {Yize Zhao and Ben Wu and Jian Kang},
  doi          = {10.1111/biom.13648},
  journal      = {Biometrics},
  number       = {2},
  pages        = {655-668},
  shortjournal = {Biometrics},
  title        = {Bayesian interaction selection model for multimodal neuroimaging data analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayes optimal informer sets for early-stage drug discovery.
<em>BIOMTC</em>, <em>79</em>(2), 642–654. (<a
href="https://doi.org/10.1111/biom.13637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important experimental design problem in early-stage drug discovery is how to prioritize available compounds for testing when very little is known about the target protein. Informer-based ranking (IBR) methods address the prioritization problem when the compounds have provided bioactivity data on other potentially relevant targets. An IBR method selects an informer set of compounds, and then prioritizes the remaining compounds on the basis of new bioactivity experiments performed with the informer set on the target. We formalize the problem as a two-stage decision problem and introduce the Bayes Optimal Informer SEt (BOISE) method for its solution. BOISE leverages a flexible model of the initial bioactivity data, a relevant loss function, and effective computational schemes to resolve the two-step design problem. We evaluate BOISE and compare it to other IBR strategies in two retrospective studies, one on protein-kinase inhibition and the other on anticancer drug sensitivity. In both empirical settings BOISE exhibits better predictive performance than available methods. It also behaves well with missing data, where methods that use matrix completion show worse predictive performance.},
  archive      = {J_BIOMTC},
  author       = {Peng Yu and Spencer Ericksen and Anthony Gitter and Michael A. Newton},
  doi          = {10.1111/biom.13637},
  journal      = {Biometrics},
  number       = {2},
  pages        = {642-654},
  shortjournal = {Biometrics},
  title        = {Bayes optimal informer sets for early-stage drug discovery},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inference for stationary points in gaussian process
regression models for event-related potentials analysis.
<em>BIOMTC</em>, <em>79</em>(2), 629–641. (<a
href="https://doi.org/10.1111/biom.13621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stationary points embedded in the derivatives are often critical for a model to be interpretable and may be considered as key features of interest in many applications. We propose a semiparametric Bayesian model to efficiently infer the locations of stationary points of a nonparametric function, which also produces an estimate of the function. We use Gaussian processes as a flexible prior for the underlying function and impose derivative constraints to control the function&#39;s shape via conditioning. We develop an inferential strategy that intentionally restricts estimation to the case of at least one stationary point, bypassing possible mis-specifications in the number of stationary points and avoiding the varying dimension problem that often brings in computational complexity. We illustrate the proposed methods using simulations and then apply the method to the estimation of event-related potentials derived from electroencephalography (EEG) signals. We show how the proposed method automatically identifies characteristic components and their latencies at the individual level, which avoids the excessive averaging across subjects that is routinely done in the field to obtain smooth curves. By applying this approach to EEG data collected from younger and older adults during a speech perception task, we are able to demonstrate how the time course of speech perception processes changes with age.},
  archive      = {J_BIOMTC},
  author       = {Cheng-Han Yu and Meng Li and Colin Noe and Simon Fischer-Baum and Marina Vannucci},
  doi          = {10.1111/biom.13621},
  journal      = {Biometrics},
  number       = {2},
  pages        = {629-641},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for stationary points in gaussian process regression models for event-related potentials analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian spatiotemporal modeling on complex-valued fMRI
signals via kernel convolutions. <em>BIOMTC</em>, <em>79</em>(2),
616–628. (<a href="https://doi.org/10.1111/biom.13631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model-based approach that combines Bayesian variable selection tools, a novel spatial kernel convolution structure, and autoregressive processes for detecting a subject&#39;s brain activation at the voxel level in complex-valued functional magnetic resonance imaging (CV-fMRI) data. A computationally efficient Markov chain Monte Carlo algorithm for posterior inference is developed by taking advantage of the dimension reduction of the kernel-based structure. The proposed spatiotemporal model leads to more accurate posterior probability activation maps and less false positives than alternative spatial approaches based on Gaussian process models, and other complex-valued models that do not incorporate spatial and/or temporal structure. This is illustrated in the analysis of simulated data and human task-related CV-fMRI data. In addition, we show that complex-valued approaches dominate magnitude-only approaches and that the kernel structure in our proposed model considerably improves sensitivity rates when detecting activation at the voxel level.},
  archive      = {J_BIOMTC},
  author       = {Cheng-Han Yu and Raquel Prado and Hernando Ombao and Daniel Rowe},
  doi          = {10.1111/biom.13631},
  journal      = {Biometrics},
  number       = {2},
  pages        = {616-628},
  shortjournal = {Biometrics},
  title        = {Bayesian spatiotemporal modeling on complex-valued fMRI signals via kernel convolutions},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel bayesian functional spatial partitioning method with
application to prostate cancer lesion detection using MRI.
<em>BIOMTC</em>, <em>79</em>(2), 604–615. (<a
href="https://doi.org/10.1111/biom.13602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial partitioning methods correct for nonstationarity in spatially related data by partitioning the space into regions of local stationarity. Existing spatial partitioning methods can only estimate linear partitioning boundaries. This is inadequate for detecting an arbitrarily shaped anomalous spatial region within a larger area. We propose a novel Bayesian functional spatial partitioning (BFSP) algorithm, which estimates closed curves that act as partitioning boundaries around anomalous regions of data with a distinct distribution or spatial process. Our method utilizes transitions between a fixed Cartesian and moving polar coordinate system to model the smooth boundary curves using functional estimation tools. Using adaptive Metropolis-Hastings, the BFSP algorithm simultaneously estimates the partitioning boundary and the parameters of the spatial distributions within each region. Through simulation we show that our method is robust to shape of the target zone and region-specific spatial processes. We illustrate our method through the detection of prostate cancer lesions using magnetic resonance imaging.},
  archive      = {J_BIOMTC},
  author       = {Maria Masotti and Lin Zhang and Ethan Leng and Gregory J. Metzger and Joseph S. Koopmeiners},
  doi          = {10.1111/biom.13602},
  journal      = {Biometrics},
  number       = {2},
  pages        = {604-615},
  shortjournal = {Biometrics},
  title        = {A novel bayesian functional spatial partitioning method with application to prostate cancer lesion detection using MRI},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder to “instrumented difference-in-differences.”
<em>BIOMTC</em>, <em>79</em>(2), 601–603. (<a
href="https://doi.org/10.1111/biom.13780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We thank all the discussants for the careful reading and insightful comments. In our rejoinder, we extend the discussion of how the assumptions of instrumented difference-in-differences (iDID) compare to the assumptions of the standard instrumental variable method. We also make additional comments on how iDID is related to the fuzzy DID. We highlight future research directions to enhance the utility of iDID, including extensions to adjust for covariate shift in two-sample iDID design, and generalization of iDID to multiple time points and a multi-valued instrumental variable for DID.},
  archive      = {J_BIOMTC},
  author       = {Ting Ye and Ashkan Ertefaie and James Flory and Sean Hennessy and Dylan S. Small},
  doi          = {10.1111/biom.13780},
  journal      = {Biometrics},
  number       = {2},
  pages        = {601-603},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Instrumented difference-in-differences”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on: Instrumented difference-in-differences, by
ting ye, ashkan ertefaie, james flory, sean hennessy and dylan s. small.
<em>BIOMTC</em>, <em>79</em>(2), 597–600. (<a
href="https://doi.org/10.1111/biom.13785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I discuss the assumptions needed for identification of average treatment effects and local average treatment effects in instrumented difference-in-differences (IDID), and the possible trade-offs between assumptions of standard IV and those needed for the new proposal IDID, in one- and two-sample settings. I also discuss the interpretation of the estimands identified under monotonicity. I conclude by suggesting possible extensions to the estimation method, by outlining a strategy to use data-adaptive estimation of the nuisance parameters, based on recent developments.},
  archive      = {J_BIOMTC},
  author       = {Karla DiazOrdaz},
  doi          = {10.1111/biom.13785},
  journal      = {Biometrics},
  number       = {2},
  pages        = {597-600},
  shortjournal = {Biometrics},
  title        = {Discussion on: Instrumented difference-in-differences, by ting ye, ashkan ertefaie, james flory, sean hennessy and dylan s. small},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumented difference-in-differences” by
ting ye, ashkan ertefaie, james flory, sean hennessy &amp; dylan s.
small. <em>BIOMTC</em>, <em>79</em>(2), 592–596. (<a
href="https://doi.org/10.1111/biom.13784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reinterpret the instrumented difference-in-differences (iDID) under a linear instrumental variables (IV) model. Under the linear IV model, we show why iDID is a clear improvement over two existing methods, difference-in-differences (DID) and a cross-sectional, IV analysis. We also re-express some of the assumptions of iDID using familiar, regression-based identification assumptions. We conclude with a method inspired by the linear IV model that can potentially remedy the weak identification problem in iDID.},
  archive      = {J_BIOMTC},
  author       = {Hyunseung Kang},
  doi          = {10.1111/biom.13784},
  journal      = {Biometrics},
  number       = {2},
  pages        = {592-596},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumented difference-in-differences” by ting ye, ashkan ertefaie, james flory, sean hennessy &amp; dylan s. small},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumented difference-in-differences” by
ye, ertefaie, flory, hennessy, small. <em>BIOMTC</em>, <em>79</em>(2),
587–591. (<a href="https://doi.org/10.1111/biom.13781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ye, Ertefaie, Flory, Hennessy, and Small (YEFHS) proposed a new method, instrumented difference-in-differences, for dealing with unmeasured confounding. In this note, I connect and compare assumptions and identifications in instrumental variable (IV) and difference-in-differences (DID) methods with those in YEFHS, derive new identification results, and discuss different choices when extending such results to adjust for covariates.},
  archive      = {J_BIOMTC},
  author       = {Zhiqiang Tan},
  doi          = {10.1111/biom.13781},
  journal      = {Biometrics},
  number       = {2},
  pages        = {587-591},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumented difference-in-differences” by ye, ertefaie, flory, hennessy, small},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumented difference-in-differences” by
ting ye, ashkan ertefaie, james flory, sean hennessy, and dylan s.
small. <em>BIOMTC</em>, <em>79</em>(2), 582–586. (<a
href="https://doi.org/10.1111/biom.13779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss Ye et al. 2022, which combines instrumental variables methods with difference in differences. First, we compare the paper to other works in the difference in differences literatures and argue that the main contribution lies in the multiply robust estimation approach. Then, we reformulate the causal assumptions in Ye et al. 2022 in the usual theoretical framework of the instrumental variables literature. This clarifies in which sense the difference in differences design can weaken the standard instrumental variable conditions.},
  archive      = {J_BIOMTC},
  author       = {Jad Beyhum and Jean-Pierre Florens and Ingrid Van Keilegom},
  doi          = {10.1111/biom.13779},
  journal      = {Biometrics},
  number       = {2},
  pages        = {582-586},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumented difference-in-differences” by ting ye, ashkan ertefaie, james flory, sean hennessy, and dylan s. small},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Instrumented difference-in-differences. <em>BIOMTC</em>,
<em>79</em>(2), 569–581. (<a
href="https://doi.org/10.1111/biom.13783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmeasured confounding is a key threat to reliable causal inference based on observational studies. Motivated from two powerful natural experiment devices, the instrumental variables and difference-in-differences, we propose a new method called instrumented difference-in-differences that explicitly leverages exogenous randomness in an exposure trend to estimate the average and conditional average treatment effect in the presence of unmeasured confounding. We develop the identification assumptions using the potential outcomes framework. We propose a Wald estimator and a class of multiply robust and efficient semiparametric estimators, with provable consistency and asymptotic normality. In addition, we extend the instrumented difference-in-differences to a two-sample design to facilitate investigations of delayed treatment effect and provide a measure of weak identification. We demonstrate our results in simulated and real datasets.},
  archive      = {J_BIOMTC},
  author       = {Ting Ye and Ashkan Ertefaie and James Flory and Sean Hennessy and Dylan S. Small},
  doi          = {10.1111/biom.13783},
  journal      = {Biometrics},
  number       = {2},
  pages        = {569-581},
  shortjournal = {Biometrics},
  title        = {Instrumented difference-in-differences},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder to discussions on “instrumental variable
estimation of the causal hazard ratio.” <em>BIOMTC</em>, <em>79</em>(2),
564–568. (<a href="https://doi.org/10.1111/biom.13793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we respond to comments on our paper, “Instrumental variable estimation of the causal hazard ratio.”},
  archive      = {J_BIOMTC},
  author       = {Linbo Wang and Eric Tchetgen Tchetgen and Torben Martinussen and Stijn Vansteelandt},
  doi          = {10.1111/biom.13793},
  journal      = {Biometrics},
  number       = {2},
  pages        = {564-568},
  shortjournal = {Biometrics},
  title        = {Rejoinder to discussions on “Instrumental variable estimation of the causal hazard ratio”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumental variable estimation of the
causal hazard ratio” by linbo wang, eric tchetgen tchetgen, torben
martinussen, and stijn vansteelandt. <em>BIOMTC</em>, <em>79</em>(2),
559–563. (<a href="https://doi.org/10.1111/biom.13794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {A. James O&#39;Malley and Pablo Martínez-Camblor and Todd A. MacKenzie},
  doi          = {10.1111/biom.13794},
  journal      = {Biometrics},
  number       = {2},
  pages        = {559-563},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumental variable estimation of the causal hazard ratio” by linbo wang, eric tchetgen tchetgen, torben martinussen, and stijn vansteelandt},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumental variable estimation of the
causal hazard ratio,” by linbo wang, eric tchetgen tchetgen, torben
martinussen, and stijn vansteelandt. <em>BIOMTC</em>, <em>79</em>(2),
554–558. (<a href="https://doi.org/10.1111/biom.13790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study an augmented variant of the estimator proposed by Wang, Tchetgen Tchetgen, Martinussen, and Vansteelandt.},
  archive      = {J_BIOMTC},
  author       = {Benjamin R. Baer and Robert L. Strawderman and Ashkan Ertefaie},
  doi          = {10.1111/biom.13790},
  journal      = {Biometrics},
  number       = {2},
  pages        = {554-558},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumental variable estimation of the causal hazard ratio,” by linbo wang, eric tchetgen tchetgen, torben martinussen, and stijn vansteelandt},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “instrumental variable estimation of the
causal hazard ratio,” by linbo wang, eric tchetgen tchetgen, torben
martinussen, stijn vansteelandt. <em>BIOMTC</em>, <em>79</em>(2),
551–553. (<a href="https://doi.org/10.1111/biom.13791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wang, et al.&#39;s estimator for causal hazard ratios for endogenous treatments makes an important addition to a researcher&#39;s toolkit for analyzing censored duration outcomes. Their method complements existing methods in the semiparametric treatment effects literature and suggests useful avenues for future research.},
  archive      = {J_BIOMTC},
  author       = {Brigham Russell Frandsen},
  doi          = {10.1111/biom.13791},
  journal      = {Biometrics},
  number       = {2},
  pages        = {551-553},
  shortjournal = {Biometrics},
  title        = {Discussion on “Instrumental variable estimation of the causal hazard ratio,” by linbo wang, eric tchetgen tchetgen, torben martinussen, stijn vansteelandt},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Instrumental variable estimation of the causal hazard
ratio. <em>BIOMTC</em>, <em>79</em>(2), 539–550. (<a
href="https://doi.org/10.1111/biom.13792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox&#39;s proportional hazards model is one of the most popular statistical models to evaluate associations of exposure with a censored failure time outcome. When confounding factors are not fully observed, the exposure hazard ratio estimated using a Cox model is subject to unmeasured confounding bias. To address this, we propose a novel approach for the identification and estimation of the causal hazard ratio in the presence of unmeasured confounding factors. Our approach is based on a binary instrumental variable, and an additional no-interaction assumption in a first-stage regression of the treatment on the IV and unmeasured confounders. We propose, to the best of our knowledge, the first consistent estimator of the (population) causal hazard ratio within an instrumental variable framework. A version of our estimator admits a closed-form representation. We derive the asymptotic distribution of our estimator and provide a consistent estimator for its asymptotic variance. Our approach is illustrated via simulation studies and a data application.},
  archive      = {J_BIOMTC},
  author       = {Linbo Wang and Eric Tchetgen Tchetgen and Torben Martinussen and Stijn Vansteelandt},
  doi          = {10.1111/biom.13792},
  journal      = {Biometrics},
  number       = {2},
  pages        = {539-550},
  shortjournal = {Biometrics},
  title        = {Instrumental variable estimation of the causal hazard ratio},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effect: An introduction to research design and causality
by nick huntington-klein (2022). New york. Chapman and hall.
Https://doi.org/10.1201/9781003226055. <em>BIOMTC</em>, <em>79</em>(1),
531–532. (<a href="https://doi.org/10.1111/biom.13835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Hung-Ching Chang and Michael T. Gorczyca},
  doi          = {10.1111/biom.13835},
  journal      = {Biometrics},
  number       = {1},
  pages        = {531-532},
  shortjournal = {Biometrics},
  title        = {The effect: an introduction to research design and causality by nick huntington-klein (2022). new york. chapman and hall. https://doi.org/10.1201/9781003226055},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <em>BIOMTC</em>, <em>79</em>(1), 528–531. (<a
href="https://doi.org/10.1111/biom.13839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Naitee Ting},
  doi          = {10.1111/biom.13839},
  journal      = {Biometrics},
  number       = {1},
  pages        = {528-531},
  shortjournal = {Biometrics},
  title        = {Confidence intervals for discrete data in clinical research by vivek pradhan, ashis k. gangopadhyay, sandeep m. menon, cynthia basu, tathagata banerjee, chapman and hall. 2021. pp. 226. $119.95. (hbk). ISBN: 978-1138048980},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gene expression data analysis: A statistical and machine
learning perspective. Pankaj barah, dhruba kumar bhattacharyya, jugal
kumar kalita (2022). Boca raton, florida and london. CRC press; taylor
and francis. Https://doi.org/10.1201/9780429322655. <em>BIOMTC</em>,
<em>79</em>(1), 526–528. (<a
href="https://doi.org/10.1111/biom.13817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Amrita Chattopadhyay},
  doi          = {10.1111/biom.13817},
  journal      = {Biometrics},
  number       = {1},
  pages        = {526-528},
  shortjournal = {Biometrics},
  title        = {Gene expression data analysis: a statistical and machine learning perspective. pankaj barah, dhruba kumar bhattacharyya, jugal kumar kalita (2022). boca raton, florida and london. CRC press; taylor and francis. https://doi.org/10.1201/9780429322655},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rejoinder to “reader reaction to ‘outcome-adaptive lasso:
Variable selection for causal inference’ by shortreed and ertefaie
(2017).” <em>BIOMTC</em>, <em>79</em>(1), 521–525. (<a
href="https://doi.org/10.1111/biom.13681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Jeremiah Jones and Ashkan Ertefaie and Susan M. Shortreed},
  doi          = {10.1111/biom.13681},
  journal      = {Biometrics},
  number       = {1},
  pages        = {521-525},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Reader reaction to ‘Outcome-adaptive lasso: Variable selection for causal inference’ by shortreed and ertefaie (2017)”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reader reaction to “outcome-adaptive lasso: Variable
selection for causal inference” by shortreed and ertefaie (2017).
<em>BIOMTC</em>, <em>79</em>(1), 514–520. (<a
href="https://doi.org/10.1111/biom.13683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortreed and Ertefaie introduced a clever propensity score variable selection approach for estimating average causal effects, namely, the outcome adaptive lasso (OAL). OAL aims to select desirable covariates, confounders, and predictors of outcome, to build an unbiased and statistically efficient propensity score estimator. Due to its design, a potential limitation of OAL is how it handles the collinearity problem, which is often encountered in high-dimensional data. As seen in Shortreed and Ertefaie, OAL&#39;s performance degraded with increased correlation between covariates. In this note, we propose the generalized OAL (GOAL) that combines the strengths of the adaptively weighted L 1 penalty and the elastic net to better handle the selection of correlated covariates. Two different versions of GOAL, which differ in their procedure (algorithm), are proposed. We compared OAL and GOAL in simulation scenarios that mimic those examined by Shortreed and Ertefaie. Although all approaches performed equivalently with independent covariates, we found that both GOAL versions were more performant than OAL in low and high dimensions with correlated covariates.},
  archive      = {J_BIOMTC},
  author       = {Ismaila Baldé and Yi Archer Yang and Geneviève Lefebvre},
  doi          = {10.1111/biom.13683},
  journal      = {Biometrics},
  number       = {1},
  pages        = {514-520},
  shortjournal = {Biometrics},
  title        = {Reader reaction to “Outcome-adaptive lasso: Variable selection for causal inference” by shortreed and ertefaie (2017)},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference with outcomes truncated by death in
multiarm studies. <em>BIOMTC</em>, <em>79</em>(1), 502–513. (<a
href="https://doi.org/10.1111/biom.13554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to evaluate causal effects when the outcomes of interest suffer from truncation-by-death in many clinical studies; that is, outcomes cannot be observed if patients die before the time of measurement. To address this problem, it is common to consider average treatment effects by principal stratification, for which, the identifiability results and estimation methods with a binary treatment have been established in previous literature. However, in multiarm studies with more than two treatment options, estimation of causal effects becomes more complicated and requires additional techniques. In this article, we consider identification, estimation, and bounds of causal effects with multivalued ordinal treatments and the outcomes subject to truncation-by-death. We define causal parameters of interest in this setting and show that they are identifiable either using some auxiliary variable or based on linear model assumption. We then propose a semiparametric method for estimating the causal parameters and derive their asymptotic results. When the identification conditions are invalid, we derive sharp bounds of the causal effects by use of covariates adjustment. Simulation studies show good performance of the proposed estimator. We use the estimator to analyze the effects of a four-level chronic toxin on fetal developmental outcomes such as birth weight in rats and mice, with data from a developmental toxicity trial conducted by the National Toxicology Program. Data analyses demonstrate that a high dose of the toxin significantly reduces the weights of pups.},
  archive      = {J_BIOMTC},
  author       = {Shanshan Luo and Wei Li and Yangbo He},
  doi          = {10.1111/biom.13554},
  journal      = {Biometrics},
  number       = {1},
  pages        = {502-513},
  shortjournal = {Biometrics},
  title        = {Causal inference with outcomes truncated by death in multiarm studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the association between latent classes and
competing risks outcomes with multiphenotype data. <em>BIOMTC</em>,
<em>79</em>(1), 488–501. (<a
href="https://doi.org/10.1111/biom.13563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent class analysis is an intuitive tool to characterize disease phenotype heterogeneity. With data more frequently collected on multiple phenotypes in chronic disease studies, it is of rising interest to investigate how the latent classes embedded in one phenotype are related to another phenotype. Motivated by a cohort with mild cognitive impairment (MCI) from the Uniform Data Set (UDS), we propose and study a time-dependent structural model to evaluate the association between latent classes and competing risk outcomes that are subject to missing failure types. We develop a two-step estimation procedure which circumvents latent class membership assignment and is rigorously justified in terms of accounting for the uncertainty in classifying latent classes. The new method also properly addresses the realistic complications for competing risks outcomes, including random censoring and missing failure types. The asymptotic properties of the resulting estimator are established. Given that the standard bootstrapping inference is not feasible in the current problem setting, we develop analytical inference procedures, which are easy to implement. Our simulation studies demonstrate the advantages of the proposed method over benchmark approaches. We present an application to the MCI data from UDS, which uncovers a detailed picture of the neuropathological relevance of the baseline MCI subgroups.},
  archive      = {J_BIOMTC},
  author       = {Teng Fei and John Hanfelt and Limin Peng},
  doi          = {10.1111/biom.13563},
  journal      = {Biometrics},
  number       = {1},
  pages        = {488-501},
  shortjournal = {Biometrics},
  title        = {Evaluating the association between latent classes and competing risks outcomes with multiphenotype data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Sensitivity analyses informed by tests for bias in
observational studies. <em>BIOMTC</em>, <em>79</em>(1), 475–487. (<a
href="https://doi.org/10.1111/biom.13558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational study, the treatment received and the outcome exhibited may be associated in the absence of an effect caused by the treatment, even after controlling for observed covariates. Two tactics are common: (i) a test for unmeasured bias may be obtained using a secondary outcome for which the effect is known and (ii) a sensitivity analysis may explore the magnitude of unmeasured bias that would need to be present to explain the observed association as something other than an effect caused by the treatment. Can such a test for unmeasured bias inform the sensitivity analysis? If the test for bias does not discover evidence of unmeasured bias, then ask: Are conclusions therefore insensitive to larger unmeasured biases? Conversely, if the test for bias does find evidence of bias, then ask: What does that imply about sensitivity to biases? This problem is formulated in a new way as a convex quadratically constrained quadratic program and solved on a large scale using interior point methods by a modern solver. That is, a convex quadratic function of N variables is minimized subject to constraints on linear and convex quadratic functions of these variables. The quadratic function that is minimized is a statistic for the primary outcome that is a function of the unknown treatment assignment probabilities. The quadratic function that constrains this minimization is a statistic for subsidiary outcome that is also a function of these same unknown treatment assignment probabilities. In effect, the first statistic is minimized over a confidence set for the unknown treatment assignment probabilities supplied by the unaffected outcome. This process avoids the mistake of interpreting the failure to reject a hypothesis as support for the truth of that hypothesis. The method is illustrated by a study of the effects of light daily alcohol consumption on high-density lipoprotein (HDL) cholesterol levels. In this study, the method quickly optimizes a nonlinear function of N = 800 $N=800$ variables subject to linear and quadratic constraints. In the example, strong evidence of unmeasured bias is found using the subsidiary outcome, but, perhaps surprisingly, this finding makes the primary comparison insensitive to larger biases.},
  archive      = {J_BIOMTC},
  author       = {Paul R. Rosenbaum},
  doi          = {10.1111/biom.13558},
  journal      = {Biometrics},
  number       = {1},
  pages        = {475-487},
  shortjournal = {Biometrics},
  title        = {Sensitivity analyses informed by tests for bias in observational studies},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian multiple index models for environmental mixtures.
<em>BIOMTC</em>, <em>79</em>(1), 462–474. (<a
href="https://doi.org/10.1111/biom.13569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important goal of environmental health research is to assess the risk posed by mixtures of environmental exposures. Two popular classes of models for mixtures analyses are response-surface methods and exposure-index methods. Response-surface methods estimate high-dimensional surfaces and are thus highly flexible but difficult to interpret. In contrast, exposure-index methods decompose coefficients from a linear model into an overall mixture effect and individual index weights; these models yield easily interpretable effect estimates and efficient inferences when model assumptions hold, but, like most parsimonious models, incur bias when these assumptions do not hold. In this paper, we propose a Bayesian multiple index model framework that combines the strengths of each, allowing for non-linear and non-additive relationships between exposure indices and a health outcome, while reducing the dimensionality of the exposure vector and estimating index weights with variable selection. This framework contains response-surface and exposure-index models as special cases, thereby unifying the two analysis strategies. This unification increases the range of models possible for analysing environmental mixtures and health, allowing one to select an appropriate analysis from a spectrum of models varying in flexibility and interpretability. In an analysis of the association between telomere length and 18 organic pollutants in the National Health and Nutrition Examination Survey (NHANES), the proposed approach fits the data as well as more complex response-surface methods and yields more interpretable results.},
  archive      = {J_BIOMTC},
  author       = {Glen McGee and Ander Wilson and Thomas F. Webster and Brent A. Coull},
  doi          = {10.1111/biom.13569},
  journal      = {Biometrics},
  number       = {1},
  pages        = {462-474},
  shortjournal = {Biometrics},
  title        = {Bayesian multiple index models for environmental mixtures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating perinatal critical windows of susceptibility to
environmental mixtures via structured bayesian regression tree pairs.
<em>BIOMTC</em>, <em>79</em>(1), 449–461. (<a
href="https://doi.org/10.1111/biom.13568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maternal exposure to environmental chemicals during pregnancy can alter birth and children&#39;s health outcomes. Research seeks to identify critical windows, time periods when exposures can change future health outcomes, and estimate the exposure–response relationship. Existing statistical approaches focus on estimation of the association between maternal exposure to a single environmental chemical observed at high temporal resolution (e.g., weekly throughout pregnancy) and children&#39;s health outcomes. Extending to multiple chemicals observed at high temporal resolution poses a dimensionality problem and statistical methods are lacking. We propose a regression tree–based model for mixtures of exposures observed at high temporal resolution. The proposed approach uses an additive ensemble of tree pairs that defines structured main effects and interactions between time-resolved predictors and performs variable selection to select out of the model predictors not correlated with the outcome. In simulation, we show that the tree-based approach performs better than existing methods for a single exposure and can accurately estimate critical windows in the exposure–response relation for mixtures. We apply our method to estimate the relationship between five exposures measured weekly throughout pregnancy and birth weight in a Denver, Colorado, birth cohort. We identified critical windows during which fine particulate matter, sulfur dioxide, and temperature are negatively associated with birth weight and an interaction between fine particulate matter and temperature. Software is made available in the R package dlmtree.},
  archive      = {J_BIOMTC},
  author       = {Daniel Mork and Ander Wilson},
  doi          = {10.1111/biom.13568},
  journal      = {Biometrics},
  number       = {1},
  pages        = {449-461},
  shortjournal = {Biometrics},
  title        = {Estimating perinatal critical windows of susceptibility to environmental mixtures via structured bayesian regression tree pairs},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A smoothed corrected score approach for proportional hazards
model with misclassified discretized covariates induced by
error-contaminated continuous time-dependent exposure. <em>BIOMTC</em>,
<em>79</em>(1), 437–448. (<a
href="https://doi.org/10.1111/biom.13595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the proportional hazards model in which the covariates include the discretized categories of a continuous time-dependent exposure variable measured with error. Naively ignoring the measurement error in the analysis may cause biased estimation and erroneous inference. Although various approaches have been proposed to deal with measurement error when the hazard depends linearly on the time-dependent variable, it has not yet been investigated how to correct when the hazard depends on the discretized categories of the time-dependent variable. To fill this gap in the literature, we propose a smoothed corrected score approach based on approximation of the discretized categories after smoothing the indicator function. The consistency and asymptotic normality of the proposed estimator are established. The observation times of the time-dependent variable are allowed to be informative. For comparison, we also extend to this setting two approximate approaches, the regression calibration and the risk-set regression calibration. The methods are assessed by simulation studies and by application to data from an HIV clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Xiao Song and Edward C. Chao and Ching-Yun Wang},
  doi          = {10.1111/biom.13595},
  journal      = {Biometrics},
  number       = {1},
  pages        = {437-448},
  shortjournal = {Biometrics},
  title        = {A smoothed corrected score approach for proportional hazards model with misclassified discretized covariates induced by error-contaminated continuous time-dependent exposure},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An individual level infectious disease model in the presence
of uncertainty from multiple, imperfect diagnostic tests.
<em>BIOMTC</em>, <em>79</em>(1), 426–436. (<a
href="https://doi.org/10.1111/biom.13579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian compartmental infectious disease models yield important inference on disease transmission by appropriately accounting for the dynamics and uncertainty of infection processes. In addition to estimating transition probabilities and reproductive numbers, these statistical models allow researchers to assess the probability of disease risk and quantify the effectiveness of interventions. These infectious disease models rely on data collected from all individuals classified as positive based on various diagnostic tests. In infectious disease testing, however, such procedures produce both false-positives and false-negatives at varying rates depending on the sensitivity and specificity of the diagnostic tests being used. We propose a novel Bayesian spatio-temporal infectious disease modeling framework that accounts for the additional uncertainty in the diagnostic testing and classification process that provides estimates of the important transmission dynamics of interest to researchers. The method is applied to data on the 2006 mumps epidemic in Iowa, in which over 6,000 suspected mumps cases were tested using a buccal or oral swab specimen, a urine specimen, and/or a blood specimen. Although all procedures are believed to have high specificities, the sensitivities can be low and vary depending on the timing of the test as well as the vaccination status of the individual being tested.},
  archive      = {J_BIOMTC},
  author       = {Caitlin Ward and Grant D. Brown and Jacob J. Oleson},
  doi          = {10.1111/biom.13579},
  journal      = {Biometrics},
  number       = {1},
  pages        = {426-436},
  shortjournal = {Biometrics},
  title        = {An individual level infectious disease model in the presence of uncertainty from multiple, imperfect diagnostic tests},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A linear mixed model to estimate COVID-19-induced excess
mortality. <em>BIOMTC</em>, <em>79</em>(1), 417–425. (<a
href="https://doi.org/10.1111/biom.13578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Corona Virus Disease (COVID-19) pandemic has increased mortality in countries worldwide. To evaluate the impact of the pandemic on mortality, the use of excess mortality rather than reported COVID-19 deaths has been suggested. Excess mortality, however, requires estimation of mortality under nonpandemic conditions. Although many methods exist to forecast mortality, they are either complex to apply, require many sources of information, ignore serial correlation, and/or are influenced by historical excess mortality. We propose a linear mixed model that is easy to apply, requires only historical mortality data, allows for serial correlation, and down-weighs the influence of historical excess mortality. Appropriateness of the linear mixed model is evaluated with fit statistics and forecasting accuracy measures for Belgium and the Netherlands. Unlike the commonly used 5-year weekly average, the linear mixed model is forecasting the year-specific mortality, and as a result improves the estimation of excess mortality for Belgium and the Netherlands.},
  archive      = {J_BIOMTC},
  author       = {Johan Verbeeck and Christel Faes and Thomas Neyens and Niel Hens and Geert Verbeke and Patrick Deboosere and Geert Molenberghs},
  doi          = {10.1111/biom.13578},
  journal      = {Biometrics},
  number       = {1},
  pages        = {417-425},
  shortjournal = {Biometrics},
  title        = {A linear mixed model to estimate COVID-19-induced excess mortality},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human disease clinical treatment network for the elderly:
Analysis of the medicare inpatient length of stay and readmission data.
<em>BIOMTC</em>, <em>79</em>(1), 404–416. (<a
href="https://doi.org/10.1111/biom.13549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical treatment outcomes are the quality and cost targets that health-care providers aim to improve. Most existing outcome analysis focuses on a single disease or all diseases combined. Motivated by the success of molecular and phenotypic human disease networks (HDNs), this article develops a clinical treatment network that describes the interconnections among diseases in terms of inpatient length of stay (LOS) and readmission. Here one node represents one disease, and two nodes are linked with an edge if their LOS and number of readmissions are conditionally dependent. This is the very first HDN that jointly analyzes multiple clinical treatment outcomes at the pan-disease level. To accommodate the unique data characteristics, we propose a modeling approach based on two-part generalized linear models and estimation based on penalized integrative analysis. Analysis is conducted on the Medicare inpatient data of 100,000 randomly selected subjects for the period of January 2010 to December 2018. The resulted network has 1008 edges for 106 nodes. We analyze key network properties including connectivity, module/hub, and temporal variation. The findings are biomedically sensible. For example, high connectivity and hub conditions, such as disorders of lipid metabolism and essential hypertension, are identified. There are also findings that are less/not investigated in the literature. Overall, this study can provide additional insight into diseases&#39; properties and their interconnections and assist more efficient disease management and health-care resources allocation.},
  archive      = {J_BIOMTC},
  author       = {Hao Mei and Ruofan Jia and Guanzhong Qiao and Zhenqiu Lin and Shuangge Ma},
  doi          = {10.1111/biom.13549},
  journal      = {Biometrics},
  number       = {1},
  pages        = {404-416},
  shortjournal = {Biometrics},
  title        = {Human disease clinical treatment network for the elderly: Analysis of the medicare inpatient length of stay and readmission data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving efficiency of inference in clinical trials with
external control data. <em>BIOMTC</em>, <em>79</em>(1), 394–403. (<a
href="https://doi.org/10.1111/biom.13583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose we are interested in the effect of a treatment in a clinical trial. The efficiency of inference may be limited due to small sample size. However, external control data are often available from historical studies. Motivated by an application to Helicobacter pylori infection, we show how to borrow strength from such data to improve efficiency of inference in the clinical trial. Under an exchangeability assumption about the potential outcome mean, we show that the semiparametric efficiency bound for estimating the average treatment effect can be reduced by incorporating both the clinical trial data and external controls. We then derive a doubly robust and locally efficient estimator. The improvement in efficiency is prominent especially when the external control data set has a large sample size and small variability. Our method allows for a relaxed overlap assumption, and we illustrate with the case where the clinical trial only contains a treated group. We also develop doubly robust and locally efficient approaches that extrapolate the causal effect in the clinical trial to the external population and the overall population. Our results also offer a meaningful implication for trial design and data collection. We evaluate the finite-sample performance of the proposed estimators via simulation. In the Helicobacter pylori infection application, our approach shows that the combination treatment has potential efficacy advantages over the triple therapy.},
  archive      = {J_BIOMTC},
  author       = {Xinyu Li and Wang Miao and Fang Lu and Xiao-Hua Zhou},
  doi          = {10.1111/biom.13583},
  journal      = {Biometrics},
  number       = {1},
  pages        = {394-403},
  shortjournal = {Biometrics},
  title        = {Improving efficiency of inference in clinical trials with external control data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference in response-adaptive clinical trials when the
enrolled population varies over time. <em>BIOMTC</em>, <em>79</em>(1),
381–393. (<a href="https://doi.org/10.1111/biom.13582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common assumption of data analysis in clinical trials is that the patient population, as well as treatment effects, do not vary during the course of the study. However, when trials enroll patients over several years, this hypothesis may be violated. Ignoring variations of the outcome distributions over time, under the control and experimental treatments, can lead to biased treatment effect estimates and poor control of false positive results. We propose and compare two procedures that account for possible variations of the outcome distributions over time, to correct treatment effect estimates, and to control type-I error rates. The first procedure models trends of patient outcomes with splines. The second leverages conditional inference principles, which have been introduced to analyze randomized trials when patient prognostic profiles are unbalanced across arms. These two procedures are applicable in response-adaptive clinical trials. We illustrate the consequences of trends in the outcome distributions in response-adaptive designs and in platform trials, and investigate the proposed methods in the analysis of a glioblastoma study.},
  archive      = {J_BIOMTC},
  author       = {Massimiliano Russo and Steffen Ventz and Victoria Wang and Lorenzo Trippa},
  doi          = {10.1111/biom.13582},
  journal      = {Biometrics},
  number       = {1},
  pages        = {381-393},
  shortjournal = {Biometrics},
  title        = {Inference in response-adaptive clinical trials when the enrolled population varies over time},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interim monitoring in sequential multiple assignment
randomized trials. <em>BIOMTC</em>, <em>79</em>(1), 368–380. (<a
href="https://doi.org/10.1111/biom.13562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A sequential multiple assignment randomized trial (SMART) facilitates the comparison of multiple adaptive treatment strategies (ATSs) simultaneously. Previous studies have established a framework to test the homogeneity of multiple ATSs by a global Wald test through inverse probability weighting. SMARTs are generally lengthier than classical clinical trials due to the sequential nature of treatment randomization in multiple stages. Thus, it would be beneficial to add interim analyses allowing for an early stop if overwhelming efficacy is observed. We introduce group sequential methods to SMARTs to facilitate interim monitoring based on the multivariate chi-square distribution. Simulation studies demonstrate that the proposed interim monitoring in SMART (IM-SMART) maintains the desired type I error and power with reduced expected sample size compared to the classical SMART. Finally, we illustrate our method by reanalyzing a SMART assessing the effects of cognitive behavioral and physical therapies in patients with knee osteoarthritis and comorbid subsyndromal depressive symptoms.},
  archive      = {J_BIOMTC},
  author       = {Liwen Wu and Junyao Wang and Abdus S. Wahed},
  doi          = {10.1111/biom.13562},
  journal      = {Biometrics},
  number       = {1},
  pages        = {368-380},
  shortjournal = {Biometrics},
  title        = {Interim monitoring in sequential multiple assignment randomized trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accounting for post-randomization variables in
meta-analysis: A joint meta-regression approach. <em>BIOMTC</em>,
<em>79</em>(1), 358–367. (<a
href="https://doi.org/10.1111/biom.13573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-regression is widely used in systematic reviews to investigate sources of heterogeneity and the association of study-level covariates with treatment effectiveness. Existing meta-regression approaches are successful in adjusting for baseline covariates, which include real study-level covariates (e.g., publication year) that are invariant within a study and aggregated baseline covariates (e.g., mean age) that differ for each participant but are measured before randomization within a study. However, these methods have several limitations in adjusting for post-randomization variables. Although post-randomization variables share a handful of similarities with baseline covariates, they differ in several aspects. First, baseline covariates can be aggregated at the study level presumably because they are assumed to be balanced by the randomization, while post-randomization variables are not balanced across arms within a study and are commonly aggregated at the arm level. Second, post-randomization variables may interact dynamically with the primary outcome. Third, unlike baseline covariates, post-randomization variables are themselves often important outcomes under investigation. In light of these differences, we propose a Bayesian joint meta-regression approach adjusting for post-randomization variables. The proposed method simultaneously estimates the treatment effect on the primary outcome and on the post-randomization variables. It takes into consideration both between- and within-study variability in post-randomization variables. Studies with missing data in either the primary outcome or the post-randomization variables are included in the joint model to improve estimation. Our method is evaluated by simulations and a real meta-analysis of major depression disorder treatments.},
  archive      = {J_BIOMTC},
  author       = {Qinshu Lian and Jing Zhang and James S. Hodges and Yong Chen and Haitao Chu},
  doi          = {10.1111/biom.13573},
  journal      = {Biometrics},
  number       = {1},
  pages        = {358-367},
  shortjournal = {Biometrics},
  title        = {Accounting for post-randomization variables in meta-analysis: A joint meta-regression approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Debiased lasso for generalized linear models with a
diverging number of covariates. <em>BIOMTC</em>, <em>79</em>(1),
344–357. (<a href="https://doi.org/10.1111/biom.13587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and drawing inference on the joint associations between single-nucleotide polymorphisms and a disease has sparked interest in genome-wide associations studies. In the motivating Boston Lung Cancer Survival Cohort (BLCSC) data, the presence of a large number of single nucleotide polymorphisms of interest, though smaller than the sample size, challenges inference on their joint associations with the disease outcome. In similar settings, we find that neither the debiased lasso approach (van de Geer et al., 2014), which assumes sparsity on the inverse information matrix, nor the standard maximum likelihood method can yield confidence intervals with satisfactory coverage probabilities for generalized linear models. Under this “large n , diverging p ” scenario, we propose an alternative debiased lasso approach by directly inverting the Hessian matrix without imposing the matrix sparsity assumption, which further reduces bias compared to the original debiased lasso and ensures valid confidence intervals with nominal coverage probabilities. We establish the asymptotic distributions of any linear combinations of the parameter estimates, which lays the theoretical ground for drawing inference. Simulations show that the proposed refined debiased estimating method performs well in removing bias and yields honest confidence interval coverage. We use the proposed method to analyze the aforementioned BLCSC data, a large-scale hospital-based epidemiology cohort study investigating the joint effects of genetic variants on lung cancer risks.},
  archive      = {J_BIOMTC},
  author       = {Lu Xia and Bin Nan and Yi Li},
  doi          = {10.1111/biom.13587},
  journal      = {Biometrics},
  number       = {1},
  pages        = {344-357},
  shortjournal = {Biometrics},
  title        = {Debiased lasso for generalized linear models with a diverging number of covariates},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized case-control sampling under generalized linear
models. <em>BIOMTC</em>, <em>79</em>(1), 332–343. (<a
href="https://doi.org/10.1111/biom.13571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A generalized case-control (GCC) study, like the standard case-control study, leverages outcome-dependent sampling (ODS) to extend to nonbinary responses. We develop a novel, unifying approach for analyzing GCC study data using the recently developed semiparametric extension of the generalized linear model (GLM), which is substantially more robust to model misspecification than existing approaches based on parametric GLMs. For valid estimation and inference, we use a conditional likelihood to account for the biased sampling design. We describe analysis procedures for estimation and inference for the semiparametric GLM under a conditional likelihood, and we discuss problems with estimation and inference under a conditional likelihood when the response distribution is misspecified. We demonstrate the flexibility of our approach over existing ones through extensive simulation studies, and we apply the methodology to an analysis of the Asset and Health Dynamics Among the Oldest Old study, which motives our research. The proposed approach yields a simple yet versatile solution for handling ODS in a wide variety of possible response distributions and sampling schemes encountered in practice.},
  archive      = {J_BIOMTC},
  author       = {Jacob M. Maronge and Ran Tao and Jonathan S. Schildcrout and Paul J. Rathouz},
  doi          = {10.1111/biom.13571},
  journal      = {Biometrics},
  number       = {1},
  pages        = {332-343},
  shortjournal = {Biometrics},
  title        = {Generalized case-control sampling under generalized linear models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modelling publication bias and p-hacking. <em>BIOMTC</em>,
<em>79</em>(1), 319–331. (<a
href="https://doi.org/10.1111/biom.13560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publication bias and p -hacking are two well-known phenomena that strongly affect the scientific literature and cause severe problems in meta-analyses. Due to these phenomena, the assumptions of meta-analyses are seriously violated and the results of the studies cannot be trusted. While publication bias is very often captured well by the weighting function selection model, p -hacking is much harder to model and no definitive solution has been found yet. In this paper, we advocate the selection model approach to model publication bias and propose a mixture model for p -hacking. We derive some properties for these models, and we compare them formally and through simulations. Finally, two real data examples are used to show how the models work in practice.},
  archive      = {J_BIOMTC},
  author       = {Jonas Moss and Riccardo De Bin},
  doi          = {10.1111/biom.13560},
  journal      = {Biometrics},
  number       = {1},
  pages        = {319-331},
  shortjournal = {Biometrics},
  title        = {Modelling publication bias and p-hacking},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectra in low-rank localized layers (SpeLLL) for
interpretable time–frequency analysis. <em>BIOMTC</em>, <em>79</em>(1),
304–318. (<a href="https://doi.org/10.1111/biom.13577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-varying frequency characteristics of many biomedical time series contain important scientific information. However, the high-dimensional nature of the time-varying power spectrum as a surface in time and frequency limits its direct use by applied researchers and clinicians for elucidating complex mechanisms. In this article, we introduce a new approach to time–frequency analysis that decomposes the time-varying power spectrum in to orthogonal rank-one layers in time and frequency to provide a parsimonious representation that illustrates relationships between power at different times and frequencies. The approach can be used in fully nonparametric analyses or in semiparametric analyses that account for exogenous information and time-varying covariates. An estimation procedure is formulated within a penalized reduced-rank regression framework that provides estimates of layers that are interpretable as power localized within time blocks and frequency bands. Empirical properties of the procedure are illustrated in simulation studies and its practical use is demonstrated through an analysis of heart rate variability during sleep.},
  archive      = {J_BIOMTC},
  author       = {Marie Tuft and Martica H. Hall and Robert T. Krafty},
  doi          = {10.1111/biom.13577},
  journal      = {Biometrics},
  number       = {1},
  pages        = {304-318},
  shortjournal = {Biometrics},
  title        = {Spectra in low-rank localized layers (SpeLLL) for interpretable time–frequency analysis},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection in nonlinear function-on-scalar
regression. <em>BIOMTC</em>, <em>79</em>(1), 292–303. (<a
href="https://doi.org/10.1111/biom.13564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new method for variable selection in a nonlinear additive function-on-scalar regression (FOSR) model. Existing methods for variable selection in FOSR have focused on the linear effects of scalar predictors, which can be a restrictive assumption in the presence of multiple continuously measured covariates. We propose a computationally efficient approach for variable selection in existing linear FOSR using functional principal component scores of the functional response and extend this framework to a nonlinear additive function-on-scalar model. The proposed method provides a unified and flexible framework for variable selection in FOSR, allowing nonlinear effects of the covariates. Numerical analysis using simulation study illustrates the advantages of the proposed method over existing variable selection methods in FOSR even when the underlying covariate effects are all linear. The proposed procedure is demonstrated on accelerometer data from the 2003–2004 cohorts of the National Health and Nutrition Examination Survey (NHANES) in understanding the association between diurnal patterns of physical activity and demographic, lifestyle, and health characteristics of the participants.},
  archive      = {J_BIOMTC},
  author       = {Rahul Ghosal and Arnab Maity},
  doi          = {10.1111/biom.13564},
  journal      = {Biometrics},
  number       = {1},
  pages        = {292-303},
  shortjournal = {Biometrics},
  title        = {Variable selection in nonlinear function-on-scalar regression},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent group detection in functional partially linear
regression models. <em>BIOMTC</em>, <em>79</em>(1), 280–291. (<a
href="https://doi.org/10.1111/biom.13557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a functional partially linear regression model with latent group structures to accommodate the heterogeneous relationship between a scalar response and functional covariates. The proposed model is motivated by a salinity tolerance study of barley families, whose main objective is to detect salinity tolerant barley plants. Our model is flexible, allowing for heterogeneous functional coefficients while being efficient by pooling information within a group for estimation. We develop an algorithm in the spirit of the K-means clustering to identify latent groups of the subjects under study. We establish the consistency of the proposed estimator, derive the convergence rate and the asymptotic distribution, and develop inference procedures. We show by simulation studies that the proposed method has higher accuracy for recovering latent groups and for estimating the functional coefficients than existing methods. The analysis of the barley data shows that the proposed method can help identify groups of barley families with different salinity tolerant abilities.},
  archive      = {J_BIOMTC},
  author       = {Wu Wang and Ying Sun and Huixia Judy Wang},
  doi          = {10.1111/biom.13557},
  journal      = {Biometrics},
  number       = {1},
  pages        = {280-291},
  shortjournal = {Biometrics},
  title        = {Latent group detection in functional partially linear regression models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating sample similarities into latent class analysis:
A tree-structured shrinkage approach. <em>BIOMTC</em>, <em>79</em>(1),
264–279. (<a href="https://doi.org/10.1111/biom.13580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with using multivariate binary observations to estimate the probabilities of unobserved classes with scientific meanings. We focus on the setting where additional information about sample similarities is available and represented by a rooted weighted tree. Every leaf in the given tree contains multiple samples. Shorter distances over the tree between the leaves indicate a priori higher similarity in class probability vectors. We propose a novel data integrative extension to classical latent class models with tree-structured shrinkage. The proposed approach enables (1) borrowing of information across leaves, (2) estimating data-driven leaf groups with distinct vectors of class probabilities, and (3) individual-level probabilistic class assignment given the observed multivariate binary measurements. We derive and implement a scalable posterior inference algorithm in a variational Bayes framework. Extensive simulations show more accurate estimation of class probabilities than alternatives that suboptimally use the additional sample similarity information. A zoonotic infectious disease application is used to illustrate the proposed approach. The paper concludes by a brief discussion on model limitations and extensions.},
  archive      = {J_BIOMTC},
  author       = {Mengbing Li and Daniel E. Park and Maliha Aziz and Cindy M. Liu and Lance B. Price and Zhenke Wu},
  doi          = {10.1111/biom.13580},
  journal      = {Biometrics},
  number       = {1},
  pages        = {264-279},
  shortjournal = {Biometrics},
  title        = {Integrating sample similarities into latent class analysis: A tree-structured shrinkage approach},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instrumental variable estimation of complier causal
treatment effect with interval-censored data. <em>BIOMTC</em>,
<em>79</em>(1), 253–263. (<a
href="https://doi.org/10.1111/biom.13565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing causal treatment effect on a time-to-event outcome is of key interest in many scientific investigations. Instrumental variable (IV) is a useful tool to mitigate the impact of endogenous treatment selection to attain unbiased estimation of causal treatment effect. Existing development of IV methodology, however, has not attended to outcomes subject to interval censoring, which are ubiquitously present in studies with intermittent follow-up but are challenging to handle in terms of both theory and computation. In this work, we fill in this important gap by studying a general class of causal semiparametric transformation models with interval-censored data. We propose a nonparametric maximum likelihood estimator of the complier causal treatment effect. Moreover, we design a reliable and computationally stable expectation–maximization (EM) algorithm, which has a tractable objective function in the maximization step via the use of Poisson latent variables. The asymptotic properties of the proposed estimators, including the consistency, asymptotic normality, and semiparametric efficiency, are established with empirical process techniques. We conduct extensive simulation studies and an application to a colorectal cancer screening data set, showing satisfactory finite-sample performance of the proposed method as well as its prominent advantages over naive methods.},
  archive      = {J_BIOMTC},
  author       = {Shuwei Li and Limin Peng},
  doi          = {10.1111/biom.13565},
  journal      = {Biometrics},
  number       = {1},
  pages        = {253-263},
  shortjournal = {Biometrics},
  title        = {Instrumental variable estimation of complier causal treatment effect with interval-censored data},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logistic regression analysis of two-phase studies using
generalized method of moments. <em>BIOMTC</em>, <em>79</em>(1), 241–252.
(<a href="https://doi.org/10.1111/biom.13584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase designs can reduce the cost of epidemiological studies by limiting the ascertainment of expensive covariates or/and exposures to an efficiently selected subset (phase-II) of a larger (phase-I) study. Efficient analysis of the resulting data set combining disparate information from phase-I and phase-II, however, can be complex. Most of the existing methods, including semiparametric maximum-likelihood estimator, require the information in phase-I to be summarized into a fixed number of strata. In this paper, we describe a novel method for the analysis of two-phase studies where information from phase-I is summarized by parameters associated with a reduced logistic regression model of the disease outcome on available covariates. We then setup estimating equations for parameters associated with the desired extended logistic regression model, based on information on the reduced model parameters from phase-I and complete data available at phase-II after accounting for nonrandom sampling design. We use generalized method of moments to solve overly identified estimating equations and develop the resulting asymptotic theory for the proposed estimator. Simulation studies show that the use of reduced parametric models, as opposed to summarizing data into strata, can lead to more efficient utilization of phase-I data. An application of the proposed method is illustrated using the data from the U.S. National Wilms Tumor Study.},
  archive      = {J_BIOMTC},
  author       = {Prosenjit Kundu and Nilanjan Chatterjee},
  doi          = {10.1111/biom.13584},
  journal      = {Biometrics},
  number       = {1},
  pages        = {241-252},
  shortjournal = {Biometrics},
  title        = {Logistic regression analysis of two-phase studies using generalized method of moments},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMIM: A unified framework of survival sensitivity analysis
using multiple imputation and martingale. <em>BIOMTC</em>,
<em>79</em>(1), 230–240. (<a
href="https://doi.org/10.1111/biom.13555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censored survival data are common in clinical trial studies. We propose a unified framework for sensitivity analysis to censoring at random in survival data using multiple imputation and martingale, called SMIM. The proposed framework adopts the δ-adjusted and control-based models, indexed by the sensitivity parameter, entailing censoring at random and a wide collection of censoring not at random assumptions. Also, it targets a broad class of treatment effect estimands defined as functionals of treatment-specific survival functions, taking into account missing data due to censoring. Multiple imputation facilitates the use of simple full-sample estimation; however, the standard Rubin&#39;s combining rule may overestimate the variance for inference in the sensitivity analysis framework. We decompose the multiple imputation estimator into a martingale series based on the sequential construction of the estimator and propose the wild bootstrap inference by resampling the martingale series. The new bootstrap inference has a theoretical guarantee for consistency and is computationally efficient compared to the nonparametric bootstrap counterpart. We evaluate the finite-sample performance of the proposed SMIM through simulation and an application on an HIV clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Shu Yang and Yilong Zhang and Guanghan Frank Liu and Qian Guan},
  doi          = {10.1111/biom.13555},
  journal      = {Biometrics},
  number       = {1},
  pages        = {230-240},
  shortjournal = {Biometrics},
  title        = {SMIM: A unified framework of survival sensitivity analysis using multiple imputation and martingale},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A matching procedure for sequential experiments that
iteratively learns which covariates improve power. <em>BIOMTC</em>,
<em>79</em>(1), 216–229. (<a
href="https://doi.org/10.1111/biom.13561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a dynamic allocation procedure that increases power and efficiency when measuring an average treatment effect in sequential randomized trials exploiting some subjects&#39; previous assessed responses. Subjects arrive sequentially and are either randomized or paired to a previously randomized subject and administered the alternate treatment. The pairing is made via a dynamic matching criterion that iteratively learns which specific covariates are important to the response. We develop estimators for the average treatment effect as well as an exact test. We illustrate our method&#39;s increase in efficiency and power over other allocation procedures in both simulated scenarios and a clinical trial dataset. An R package “ SeqExpMatch ” for use by practitioners is available on CRAN .},
  archive      = {J_BIOMTC},
  author       = {Adam Kapelner and Abba Krieger},
  doi          = {10.1111/biom.13561},
  journal      = {Biometrics},
  number       = {1},
  pages        = {216-229},
  shortjournal = {Biometrics},
  title        = {A matching procedure for sequential experiments that iteratively learns which covariates improve power},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CASANOVA: Permutation inference in factorial survival
designs. <em>BIOMTC</em>, <em>79</em>(1), 203–215. (<a
href="https://doi.org/10.1111/biom.13575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose inference procedures for general factorial designs with time-to-event endpoints. Similar to additive Aalen models, null hypotheses are formulated in terms of cumulative hazards. Deviations are measured in terms of quadratic forms in Nelson–Aalen-type integrals. Different from existing approaches, this allows to work without restrictive model assumptions as proportional hazards. In particular, crossing survival or hazard curves can be detected without a significant loss of power. For a distribution-free application of the method, a permutation strategy is suggested. The resulting procedures&#39; asymptotic validity is proven and small sample performances are analyzed in extensive simulations. The analysis of a data set on asthma illustrates the applicability.},
  archive      = {J_BIOMTC},
  author       = {Marc Ditzhaus and Jon Genuneit and Arnold Janssen and Markus Pauly},
  doi          = {10.1111/biom.13575},
  journal      = {Biometrics},
  number       = {1},
  pages        = {203-215},
  shortjournal = {Biometrics},
  title        = {CASANOVA: Permutation inference in factorial survival designs},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk prediction with imperfect survival outcome information
from electronic health records. <em>BIOMTC</em>, <em>79</em>(1),
190–202. (<a href="https://doi.org/10.1111/biom.13599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Readily available proxies for the time of disease onset such as the time of the first diagnostic code can lead to substantial risk prediction error if performing analyses based on poor proxies. Due to the lack of detailed documentation and labor intensiveness of manual annotation, it is often only feasible to ascertain for a small subset the current status of the disease by a follow-up time rather than the exact time. In this paper, we aim to develop risk prediction models for the onset time efficiently leveraging both a small number of labels on the current status and a large number of unlabeled observations on imperfect proxies. Under a semiparametric transformation model for onset and a highly flexible measurement error model for proxy onset time, we propose the semisupervised risk prediction method by combining information from proxies and limited labels efficiently. From an initially estimator solely based on the labeled subset, we perform a one-step correction with the full data augmenting against a mean zero rank correlation score derived from the proxies. We establish the consistency and asymptotic normality of the proposed semisupervised estimator and provide a resampling procedure for interval estimation. Simulation studies demonstrate that the proposed estimator performs well in a finite sample. We illustrate the proposed estimator by developing a genetic risk prediction model for obesity using data from Mass General Brigham Healthcare Biobank.},
  archive      = {J_BIOMTC},
  author       = {Jue Hou and Stephanie F. Chan and Xuan Wang and Tianxi Cai},
  doi          = {10.1111/biom.13599},
  journal      = {Biometrics},
  number       = {1},
  pages        = {190-202},
  shortjournal = {Biometrics},
  title        = {Risk prediction with imperfect survival outcome information from electronic health records},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Jackknife model averaging for high-dimensional quantile
regression. <em>BIOMTC</em>, <em>79</em>(1), 178–189. (<a
href="https://doi.org/10.1111/biom.13574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a frequentist model averaging method for quantile regression with high-dimensional covariates. Although research on these subjects has proliferated as separate approaches, no study has considered them in conjunction. Our method entails reducing the covariate dimensions through ranking the covariates based on marginal quantile utilities. The second step of our method implements model averaging on the models containing the covariates that survive the screening of the first step. We use a delete-one cross-validation method to select the model weights, and prove that the resultant estimator possesses an optimal asymptotic property uniformly over any compact (0,1) subset of the quantile indices. Our proof, which relies on empirical process theory, is arguably more challenging than proofs of similar results in other contexts owing to the high-dimensional nature of the problem and our relaxation of the conventional assumption of the weights summing to one. Our investigation of finite-sample performance demonstrates that the proposed method exhibits very favorable properties compared to the least absolute shrinkage and selection operator (LASSO) and smoothly clipped absolute deviation (SCAD) penalized regression methods. The method is applied to a microarray gene expression data set.},
  archive      = {J_BIOMTC},
  author       = {Miaomiao Wang and Xinyu Zhang and Alan T. K. Wan and Kang You and Guohua Zou},
  doi          = {10.1111/biom.13574},
  journal      = {Biometrics},
  number       = {1},
  pages        = {178-189},
  shortjournal = {Biometrics},
  title        = {Jackknife model averaging for high-dimensional quantile regression},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated failure time modeling via nonparametric
mixtures. <em>BIOMTC</em>, <em>79</em>(1), 165–177. (<a
href="https://doi.org/10.1111/biom.13556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accelerated failure time (AFT) model assuming a log-linear relationship between failure time and a set of covariates can be either parametric or semiparametric, depending on the distributional assumption for the error term. Both classes of AFT models have been popular in the analysis of censored failure time data. The semiparametric AFT model is more flexible and robust to departures from the distributional assumption than its parametric counterpart. However, the semiparametric AFT model is subject to producing biased results for estimating any quantities involving an intercept. Estimating an intercept requires a separate procedure. Moreover, a consistent estimation of the intercept requires stringent conditions. Thus, essential quantities such as mean failure times might not be reliably estimated using semiparametric AFT models, which can be naturally done in the framework of parametric AFT models. Meanwhile, parametric AFT models can be severely impaired by misspecifications. To overcome this, we propose a new type of the AFT model using a nonparametric Gaussian-scale mixture distribution. We also provide feasible algorithms to estimate the parameters and mixing distribution. The finite sample properties of the proposed estimators are investigated via an extensive stimulation study. The proposed estimators are illustrated using a real dataset.},
  archive      = {J_BIOMTC},
  author       = {Byungtae Seo and Sangwook Kang},
  doi          = {10.1111/biom.13556},
  journal      = {Biometrics},
  number       = {1},
  pages        = {165-177},
  shortjournal = {Biometrics},
  title        = {Accelerated failure time modeling via nonparametric mixtures},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric quantile process regression and
estimation of marginal quantile effects. <em>BIOMTC</em>,
<em>79</em>(1), 151–164. (<a
href="https://doi.org/10.1111/biom.13576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible estimation of multiple conditional quantiles is of interest in numerous applications, such as studying the effect of pregnancy-related factors on low and high birth weight. We propose a Bayesian nonparametric method to simultaneously estimate noncrossing, nonlinear quantile curves. We expand the conditional distribution function of the response in I-spline basis functions where the covariate-dependent coefficients are modeled using neural networks. By leveraging the approximation power of splines and neural networks, our model can approximate any continuous quantile function. Compared to existing models, our model estimates all rather than a finite subset of quantiles, scales well to high dimensions, and accounts for estimation uncertainty. While the model is arbitrarily flexible, interpretable marginal quantile effects are estimated using accumulative local effect plots and variable importance measures. A simulation study shows that our model can better recover quantiles of the response distribution when the data are sparse, and an analysis of birth weight data is presented.},
  archive      = {J_BIOMTC},
  author       = {Steven G. Xu and Brian J. Reich},
  doi          = {10.1111/biom.13576},
  journal      = {Biometrics},
  number       = {1},
  pages        = {151-164},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric quantile process regression and estimation of marginal quantile effects},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noniterative adjustment to regression estimators with
population-based auxiliary information for semiparametric models.
<em>BIOMTC</em>, <em>79</em>(1), 140–150. (<a
href="https://doi.org/10.1111/biom.13585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease registries, surveillance data, and other datasets with extremely large sample sizes become increasingly available in providing population-based information on disease incidence, survival probability, or other important public health characteristics. Such information can be leveraged in studies that collect detailed measurements but with smaller sample sizes. In contrast to recent proposals that formulate additional information as constraints in optimization problems, we develop a general framework to construct simple estimators that update the usual regression estimators with some functionals of data that incorporate the additional information. We consider general settings that incorporate nuisance parameters in the auxiliary information, non- i.i.d . data such as those from case-control studies, and semiparametric models with infinite-dimensional parameters common in survival analysis. Details of several important data and sampling settings are provided with numerical examples.},
  archive      = {J_BIOMTC},
  author       = {Fei Gao and K. C. G. Chan},
  doi          = {10.1111/biom.13585},
  journal      = {Biometrics},
  number       = {1},
  pages        = {140-150},
  shortjournal = {Biometrics},
  title        = {Noniterative adjustment to regression estimators with population-based auxiliary information for semiparametric models},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of separable direct and indirect effects in
continuous time. <em>BIOMTC</em>, <em>79</em>(1), 127–139. (<a
href="https://doi.org/10.1111/biom.13559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research questions involve time-to-event outcomes that can be prevented from occurring due to competing events. In these settings, we must be careful about the causal interpretation of classical statistical estimands. In particular, estimands on the hazard scale, such as ratios of cause-specific or subdistribution hazards, are fundamentally hard to interpret causally. Estimands on the risk scale, such as contrasts of cumulative incidence functions, do have a clear causal interpretation, but they only capture the total effect of the treatment on the event of interest; that is, effects both through and outside of the competing event. To disentangle causal treatment effects on the event of interest and competing events, the separable direct and indirect effects were recently introduced. Here we provide new results on the estimation of direct and indirect separable effects in continuous time. In particular, we derive the nonparametric influence function in continuous time and use it to construct an estimator that has certain robustness properties. We also propose a simple estimator based on semiparametric models for the two cause-specific hazard functions. We describe the asymptotic properties of these estimators and present results from simulation studies, suggesting that the estimators behave satisfactorily in finite samples. Finally, we reanalyze the prostate cancer trial from Stensrud et al. (2020).},
  archive      = {J_BIOMTC},
  author       = {Torben Martinussen and Mats Julius Stensrud},
  doi          = {10.1111/biom.13559},
  journal      = {Biometrics},
  number       = {1},
  pages        = {127-139},
  shortjournal = {Biometrics},
  title        = {Estimation of separable direct and indirect effects in continuous time},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional additive models for optimizing individualized
treatment rules. <em>BIOMTC</em>, <em>79</em>(1), 113–126. (<a
href="https://doi.org/10.1111/biom.13586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel functional additive model is proposed, which is uniquely modified and constrained to model nonlinear interactions between a treatment indicator and a potentially large number of functional and/or scalar pretreatment covariates. The primary motivation for this approach is to optimize individualized treatment rules based on data from a randomized clinical trial. We generalize functional additive regression models by incorporating treatment-specific components into additive effect components. A structural constraint is imposed on the treatment-specific components in order to provide a class of additive models with main effects and interaction effects that are orthogonal to each other. If primary interest is in the interaction between treatment and the covariates, as is generally the case when optimizing individualized treatment rules, we can thereby circumvent the need to estimate the main effects of the covariates, obviating the need to specify their form and thus avoiding the issue of model misspecification. The methods are illustrated with data from a depression clinical trial with electroencephalogram functional data as patients&#39; pretreatment covariates.},
  archive      = {J_BIOMTC},
  author       = {Hyung Park and Eva Petkova and Thaddeus Tarpey and R. Todd Ogden},
  doi          = {10.1111/biom.13586},
  journal      = {Biometrics},
  number       = {1},
  pages        = {113-126},
  shortjournal = {Biometrics},
  title        = {Functional additive models for optimizing individualized treatment rules},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size considerations for stepped wedge designs with
subclusters. <em>BIOMTC</em>, <em>79</em>(1), 98–112. (<a
href="https://doi.org/10.1111/biom.13596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stepped wedge cluster randomized trial (SW-CRT) is an increasingly popular design for evaluating health service delivery or policy interventions. An essential consideration of this design is the need to account for both within-period and between-period correlations in sample size calculations. Especially when embedded in health care delivery systems, many SW-CRTs may have subclusters nested in clusters, within which outcomes are collected longitudinally. However, existing sample size methods that account for between-period correlations have not allowed for multiple levels of clustering. We present computationally efficient sample size procedures that properly differentiate within-period and between-period intracluster correlation coefficients in SW-CRTs in the presence of subclusters. We introduce an extended block exchangeable correlation matrix to characterize the complex dependencies of outcomes within clusters. For Gaussian outcomes, we derive a closed-form sample size expression that depends on the correlation structure only through two eigenvalues of the extended block exchangeable correlation structure. For non-Gaussian outcomes, we present a generic sample size algorithm based on linearization and elucidate simplifications under canonical link functions. For example, we show that the approximate sample size formula under a logistic linear mixed model depends on three eigenvalues of the extended block exchangeable correlation matrix. We provide an extension to accommodate unequal cluster sizes and validate the proposed methods via simulations. Finally, we illustrate our methods in two real SW-CRTs with subclusters.},
  archive      = {J_BIOMTC},
  author       = {Kendra Davis-Plourde and Monica Taljaard and Fan Li},
  doi          = {10.1111/biom.13596},
  journal      = {Biometrics},
  number       = {1},
  pages        = {98-112},
  shortjournal = {Biometrics},
  title        = {Sample size considerations for stepped wedge designs with subclusters},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel statistical test for treatment differences in
clinical trials using a response-adaptive forward-looking gittins index
rule. <em>BIOMTC</em>, <em>79</em>(1), 86–97. (<a
href="https://doi.org/10.1111/biom.13581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most common objective for response-adaptive clinical trials is to seek to ensure that patients within a trial have a high chance of receiving the best treatment available by altering the chance of allocation on the basis of accumulating data. Approaches that yield good patient benefit properties suffer from low power from a frequentist perspective when testing for a treatment difference at the end of the study due to the high imbalance in treatment allocations. In this work we develop an alternative pairwise test for treatment difference on the basis of allocation probabilities of the covariate-adjusted response-adaptive randomization with forward-looking Gittins Index (CARA-FLGI) Rule for binary responses. The performance of the novel test is evaluated in simulations for two-armed studies and then its applications to multiarmed studies are illustrated. The proposed test has markedly improved power over the traditional Fisher exact test when this class of nonmyopic response adaptation is used. We also find that the test&#39;s power is close to the power of a Fisher exact test under equal randomization.},
  archive      = {J_BIOMTC},
  author       = {Helen Yvette Barnett and Sofía S. Villar and Helena Geys and Thomas Jaki},
  doi          = {10.1111/biom.13581},
  journal      = {Biometrics},
  number       = {1},
  pages        = {86-97},
  shortjournal = {Biometrics},
  title        = {A novel statistical test for treatment differences in clinical trials using a response-adaptive forward-looking gittins index rule},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic logistic state space prediction model for clinical
decision making. <em>BIOMTC</em>, <em>79</em>(1), 73–85. (<a
href="https://doi.org/10.1111/biom.13593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction modeling for clinical decision making is of great importance and needed to be updated frequently with the changes of patient population and clinical practice. Existing methods are either done in an ad hoc fashion, such as model recalibration or focus on studying the relationship between predictors and outcome and less so for the purpose of prediction. In this article, we propose a dynamic logistic state space model to continuously update the parameters whenever new information becomes available. The proposed model allows for both time-varying and time-invariant coefficients. The varying coefficients are modeled using smoothing splines to account for their smooth trends over time. The smoothing parameters are objectively chosen by maximum likelihood. The model is updated using batch data accumulated at prespecified time intervals, which allows for better approximation of the underlying binomial density function. In the simulation, we show that the new model has significantly higher prediction accuracy compared to existing methods. We apply the method to predict 1 year survival after lung transplantation using the United Network for Organ Sharing data.},
  archive      = {J_BIOMTC},
  author       = {Jiakun Jiang and Wei Yang and Erin M. Schnellinger and Stephen E. Kimmel and Wensheng Guo},
  doi          = {10.1111/biom.13593},
  journal      = {Biometrics},
  number       = {1},
  pages        = {73-85},
  shortjournal = {Biometrics},
  title        = {Dynamic logistic state space prediction model for clinical decision making},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). On restricted mean time in favor of treatment.
<em>BIOMTC</em>, <em>79</em>(1), 61–72. (<a
href="https://doi.org/10.1111/biom.13570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean time in favor (RMT-IF) of treatment is a nonparametric effect size for complex life history data. It is defined as the net average time the treated spend in a more favorable state than the untreated over a prespecified time window. It generalizes the familiar restricted mean survival time (RMST) from the two-state life–death model to account for intermediate stages in disease progression. The overall estimand can be additively decomposed into stage-wise effects, with the standard RMST as a component. Alternate expressions of the overall and stage-wise estimands as integrals of the marginal survival functions for a sequence of landmark transitioning events allow them to be easily estimated by plug-in Kaplan–Meier estimators. The dynamic profile of the estimated treatment effects as a function of follow-up time can be visualized using a multilayer, cone-shaped “bouquet plot.” Simulation studies under realistic settings show that the RMT-IF meaningfully and accurately quantifies the treatment effect and outperforms traditional tests on time to the first event in statistical efficiency thanks to its fuller utilization of patient data. The new methods are illustrated on a colon cancer trial with relapse and death as outcomes and a cardiovascular trial with recurrent hospitalizations and death as outcomes. The R-package rmt implements the proposed methodology and is publicly available from the Comprehensive R Archive Network (CRAN).},
  archive      = {J_BIOMTC},
  author       = {Lu Mao},
  doi          = {10.1111/biom.13570},
  journal      = {Biometrics},
  number       = {1},
  pages        = {61-72},
  shortjournal = {Biometrics},
  title        = {On restricted mean time in favor of treatment},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic priors to dynamically borrow information from
historical data in clinical trials. <em>BIOMTC</em>, <em>79</em>(1),
49–60. (<a href="https://doi.org/10.1111/biom.13551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Use of historical data and real-world evidence holds great potential to improve the efficiency of clinical trials. One major challenge is to effectively borrow information from historical data while maintaining a reasonable type I error and minimal bias. We propose the elastic prior approach to address this challenge. Unlike existing approaches, this approach proactively controls the behavior of information borrowing and type I errors by incorporating a well-known concept of clinically significant difference through an elastic function, defined as a monotonic function of a congruence measure between historical data and trial data. The elastic function is constructed to satisfy a set of prespecified criteria such that the resulting prior will strongly borrow information when historical and trial data are congruent, but refrain from information borrowing when historical and trial data are incongruent. The elastic prior approach has a desirable property of being information borrowing consistent, that is, asymptotically controls type I error at the nominal value, no matter that historical data are congruent or not to the trial data. Our simulation study that evaluates the finite sample characteristic confirms that, compared to existing methods, the elastic prior has better type I error control and yields competitive or higher power. The proposed approach is applicable to binary, continuous, and survival endpoints.},
  archive      = {J_BIOMTC},
  author       = {Liyun Jiang and Lei Nie and Ying Yuan},
  doi          = {10.1111/biom.13551},
  journal      = {Biometrics},
  number       = {1},
  pages        = {49-60},
  shortjournal = {Biometrics},
  title        = {Elastic priors to dynamically borrow information from historical data in clinical trials},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate adjustment in continuous biomarker assessment.
<em>BIOMTC</em>, <em>79</em>(1), 39–48. (<a
href="https://doi.org/10.1111/biom.13601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous biomarkers are common for disease screening and diagnosis. To reach a dichotomous clinical decision, a threshold would be imposed to distinguish subjects with disease from nondiseased individuals. Among various performance metrics, specificity at a controlled sensitivity level (or vice versa) is often desirable because it directly targets the clinical utility of the intended clinical test. Meanwhile, covariates, such as age, race, as well as sample collection conditions, could impact the biomarker distribution and may also confound the association between biomarker and disease status. Therefore, covariate adjustment is important in such biomarker evaluation. Most existing covariate adjustment methods do not specifically target the desired sensitivity/specificity level, but rather do so for the entire biomarker distribution. As such, they might be more prone to model misspecification. In this paper, we suggest a parsimonious quantile regression model for the diseased population, only locally at the controlled sensitivity level, and assess specificity with covariate-specific control of the sensitivity. Variance estimates are obtained from a sample-based approach and bootstrap. Furthermore, our proposed local model extends readily to a global one for covariate adjustment for the receiver operating characteristic (ROC) curve over the sensitivity continuum. We demonstrate computational efficiency of this proposed method and restore the inherent monotonicity in the estimated covariate-adjusted ROC curve. The asymptotic properties of the proposed estimators are established. Simulation studies show favorable performance of the proposal. Finally, we illustrate our method in biomarker evaluation for aggressive prostate cancer.},
  archive      = {J_BIOMTC},
  author       = {Ziyi Li and Yijian Huang and Dattatraya Patil and Martin G. Sanda},
  doi          = {10.1111/biom.13601},
  journal      = {Biometrics},
  number       = {1},
  pages        = {39-48},
  shortjournal = {Biometrics},
  title        = {Covariate adjustment in continuous biomarker assessment},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder to discussion on “adaptive enrichment designs
with a continuous biomarker.” <em>BIOMTC</em>, <em>79</em>(1), 36–38.
(<a href="https://doi.org/10.1111/biom.13639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Nigel Stallard},
  doi          = {10.1111/biom.13639},
  journal      = {Biometrics},
  number       = {1},
  pages        = {36-38},
  shortjournal = {Biometrics},
  title        = {Rejoinder to discussion on “Adaptive enrichment designs with a continuous biomarker”},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “adaptive enrichment designs with a continuous
biomarker” by nigel stallard. <em>BIOMTC</em>, <em>79</em>(1), 31–35.
(<a href="https://doi.org/10.1111/biom.13641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We congratulate Dr. Nigel Stallard on his stimulating paper on adaptive enrichment designs with a continuous biomarker . Dr. Stallard details a framework for a large and interesting class of enrichment procedures. His work has motivated us to offer some thoughts in response. Dr. Stallard&#39;s strategy is to use the maximum of a test statistic over a set of possible threshold values to define the enriched population to be sampled in a second stage. This reminds us of procedures for identifying a change point, a biomarker value beyond which the effect of treatment is increased. For simplicity we focus our comments on Dr. Stallard&#39;s Rule 1 for selecting the second-stage sampling threshold. Using this rule, we present the likelihood ratio approach for adaptive testing and compare it to Dr. Stallard&#39;s approach for a few scenarios.},
  archive      = {J_BIOMTC},
  author       = {Nancy Flournoy and Sergey Tarima},
  doi          = {10.1111/biom.13641},
  journal      = {Biometrics},
  number       = {1},
  pages        = {31-35},
  shortjournal = {Biometrics},
  title        = {Discussion on “Adaptive enrichment designs with a continuous biomarker” by nigel stallard},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “adaptive enrichment designs with a continuous
biomarker” by n. stallard. <em>BIOMTC</em>, <em>79</em>(1), 26–30. (<a
href="https://doi.org/10.1111/biom.13642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Christopher Jennison},
  doi          = {10.1111/biom.13642},
  journal      = {Biometrics},
  number       = {1},
  pages        = {26-30},
  shortjournal = {Biometrics},
  title        = {Discussion on “Adaptive enrichment designs with a continuous biomarker” by n. stallard},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “adaptive enrichment designs with a continuous
biomarker” by nigel stallard. <em>BIOMTC</em>, <em>79</em>(1), 23–25.
(<a href="https://doi.org/10.1111/biom.13643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {James M. S. Wason},
  doi          = {10.1111/biom.13643},
  journal      = {Biometrics},
  number       = {1},
  pages        = {23-25},
  shortjournal = {Biometrics},
  title        = {Discussion on “Adaptive enrichment designs with a continuous biomarker” by nigel stallard},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion on “adaptive enrichment designs with a continuous
biomarker” by nigel stallard. <em>BIOMTC</em>, <em>79</em>(1), 20–22.
(<a href="https://doi.org/10.1111/biom.13640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Rachael V. Phillips and Mark J. van der Laan},
  doi          = {10.1111/biom.13640},
  journal      = {Biometrics},
  number       = {1},
  pages        = {20-22},
  shortjournal = {Biometrics},
  title        = {Discussion on “Adaptive enrichment designs with a continuous biomarker” by nigel stallard},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive enrichment designs with a continuous biomarker.
<em>BIOMTC</em>, <em>79</em>(1), 9–19. (<a
href="https://doi.org/10.1111/biom.13644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular design for clinical trials assessing targeted therapies is the two-stage adaptive enrichment design with recruitment in stage 2 limited to a biomarker-defined subgroup chosen based on data from stage 1. The data-dependent selection leads to statistical challenges if data from both stages are used to draw inference on treatment effects in the selected subgroup. If subgroups considered are nested, as when defined by a continuous biomarker, treatment effect estimates in different subgroups follow the same distribution as estimates in a group-sequential trial. This result is used to obtain tests controlling the familywise type I error rate (FWER) for six simple subgroup selection rules, one of which also controls the FWER for any selection rule. Two approaches are proposed: one based on multivariate normal distributions suitable if the number of possible subgroups, k , is small, and one based on Brownian motion approximations suitable for large k . The methods, applicable in the wide range of settings with asymptotically normal test statistics, are illustrated using survival data from a breast cancer trial.},
  archive      = {J_BIOMTC},
  author       = {Nigel Stallard},
  doi          = {10.1111/biom.13644},
  journal      = {Biometrics},
  number       = {1},
  pages        = {9-19},
  shortjournal = {Biometrics},
  title        = {Adaptive enrichment designs with a continuous biomarker},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Report of the editors—2022. <em>BIOMTC</em>, <em>79</em>(1),
5–8. (<a href="https://doi.org/10.1111/biom.13834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13834},
  journal      = {Biometrics},
  number       = {1},
  pages        = {5-8},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2022},
  volume       = {79},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
