<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMET_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet---71">BIOMET - 71</h2>
<ul>
<li><details>
<summary>
(2023). Ancestor regression in linear structural equation models.
<em>BIOMET</em>, <em>110</em>(4), 1117–1124. (<a
href="https://doi.org/10.1093/biomet/asad008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method for causal discovery in linear structural equation models. We propose a simple technique based on statistical testing in linear models that can distinguish between ancestors and non-ancestors of any given variable. Naturally, this approach can then be extended to estimating the causal order among all variables. We provide explicit error control for false causal discovery, at least asymptotically. This holds true even under Gaussianity, where other methods fail due to non-identifiable structures. These Type I error guarantees come at the cost of reduced power. Additionally, we provide an asymptotically valid goodness-of-fit p -value for assessing whether multivariate data stem from a linear structural equation model.},
  archive      = {J_BIOMET},
  author       = {Schultheiss, C and Bühlmann, P},
  doi          = {10.1093/biomet/asad008},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1117-1124},
  shortjournal = {Biometrika},
  title        = {Ancestor regression in linear structural equation models},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A subsampling perspective for extending the validity of
state-of-the-art bootstraps in the frequency domain. <em>BIOMET</em>,
<em>110</em>(4), 1099–1115. (<a
href="https://doi.org/10.1093/biomet/asad006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrapping spectral mean statistics has been a notoriously difficult problem over the past 25 years. Many frequency domain bootstraps are valid only for certain time series structures, e.g., linear processes, or for special types of statistics, i.e., ratio statistics, because such bootstraps fail to capture the limiting variance of spectral statistics in general settings. We address this issue with a different form of resampling, namely, subsampling. While not considered previously, subsampling provides consistent variance estimation under much weaker conditions than any existing bootstrap in the frequency domain. Mixing is not used, as is often standard with subsampling. Rather, subsampling can be generally justified under the same conditions needed for original spectral mean statistics to have distributional limits in the first place. This result has impacts for other bootstrap methods. Subsampling then applies to extending the validity of recent state-of-the-art bootstraps in the frequency domain. We nontrivially link subsampling to such bootstraps, which broadens their range, as moment and block assumptions needed for these are cut by more than half. Essentially, state-of-the-art bootstraps then require no more stringent assumptions than those needed for a target limit distribution to exist, which is unusual in the bootstrap world. We also close a gap in the theory of subsampling for time series with distributional approximations, in addition to variance estimation, for frequency domain statistics.},
  archive      = {J_BIOMET},
  author       = {Yu, Haihan and Kaiser, Mark S and Nordman, Daniel J},
  doi          = {10.1093/biomet/asad006},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1099-1115},
  shortjournal = {Biometrika},
  title        = {A subsampling perspective for extending the validity of state-of-the-art bootstraps in the frequency domain},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). √2-estimation for smooth eigenvectors of matrix-valued
functions. <em>BIOMET</em>, <em>110</em>(4), 1077–1098. (<a
href="https://doi.org/10.1093/biomet/asad018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern statistical methods for multivariate time series rely on the eigendecomposition of matrix-valued functions such as time-varying covariance and spectral density matrices. The curse of indeterminacy or misidentification of smooth eigenvector functions has not received much attention. We resolve this important problem and recover smooth trajectories by examining the distance between the eigenvectors of the same matrix-valued function evaluated at two consecutive points. We change the sign of the next eigenvector if its distance with the current one is larger than the square root of 2. In the case of distinct eigenvalues, this simple method delivers smooth eigenvectors. For coalescing eigenvalues, we match the corresponding eigenvectors and apply an additional signing around the coalescing points. We establish consistency and rates of convergence for the proposed smooth eigenvector estimators. Simulation results and applications to real data confirm that our approach is needed to obtain smooth eigenvectors.},
  archive      = {J_BIOMET},
  author       = {Motta, Giovanni and Wu, Wei Biao and Pourahmadi, Mohsen},
  doi          = {10.1093/biomet/asad018},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1077-1098},
  shortjournal = {Biometrika},
  title        = {√2-estimation for smooth eigenvectors of matrix-valued functions},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equivariant estimation of fréchet means. <em>BIOMET</em>,
<em>110</em>(4), 1055–1076. (<a
href="https://doi.org/10.1093/biomet/asad014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fréchet mean generalizes the concept of a mean to a metric space setting. In this work we consider equivariant estimation of Fréchet means for parametric models on metric spaces that are Riemannian manifolds. The geometry and symmetry of such a space are partially encoded by its isometry group of distance-preserving transformations. Estimators that are equivariant under the isometry group take into account the symmetry of the metric space. For some models, there exists an optimal equivariant estimator, which will necessarily perform as well or better than other common equivariant estimators, such as the maximum likelihood estimator or the sample Fréchet mean. We derive the general form of this minimum risk equivariant estimator and in a few cases provide explicit expressions for it. A result for finding the Fréchet mean for distributions with radially decreasing densities is presented and used to find expressions for the minimum risk equivariant estimator. In some models the isometry group is not large enough relative to the parametric family of distributions for there to exist a minimum risk equivariant estimator. In such cases, we introduce an adaptive equivariant estimator that uses the data to select a submodel for which there is a minimum risk equivariant estimator. Simulation results show that the adaptive equivariant estimator performs favourably relative to alternative estimators.},
  archive      = {J_BIOMET},
  author       = {McCormack, A and Hoff, P D},
  doi          = {10.1093/biomet/asad014},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1055-1076},
  shortjournal = {Biometrika},
  title        = {Equivariant estimation of fréchet means},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient estimation under data fusion. <em>BIOMET</em>,
<em>110</em>(4), 1041–1054. (<a
href="https://doi.org/10.1093/biomet/asad007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to make inferences about a smooth, finite-dimensional parameter by fusing together data from multiple sources. Previous works have studied the estimation of a variety of parameters in similar data fusion settings, including estimation of the average treatment effect and average reward under a policy, with the majority of them merging one historical data source with covariates, actions and rewards, and one data source of the same covariates. In this article, we consider the general case where one or more data sources align with each part of the distribution of the target population, such as the conditional distribution of the reward given actions and covariates. We describe potential gains in efficiency that can arise from fusing these data sources together in a single analysis, which we characterize by a reduction in the semiparametric efficiency bound. We also provide a general means of constructing estimators that achieve these bounds. Numerical simulations demonstrate marked improvements in efficiency from using the proposed estimators rather than their natural alternatives. Finally, we illustrate the magnitude of efficiency gains that can be realized in vaccine immunogenicity studies by fusing data from two HIV vaccine trials.},
  archive      = {J_BIOMET},
  author       = {Li, Sijia and Luedtke, Alex},
  doi          = {10.1093/biomet/asad007},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1041-1054},
  shortjournal = {Biometrika},
  title        = {Efficient estimation under data fusion},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust fusion-extraction procedure with summary statistics
in the presence of biased sources. <em>BIOMET</em>, <em>110</em>(4),
1023–1040. (<a href="https://doi.org/10.1093/biomet/asad013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information from multiple data sources is increasingly available. However, some data sources may produce biased estimates due to biased sampling, data corruption or model misspecification. Thus there is a need for robust data combination methods that can be used with biased sources. In this paper, a robust data fusion-extraction method is proposed. Unlike existing methods, the proposed method can be applied in the important case where researchers have no knowledge of which data sources are unbiased. The proposed estimator is easy to compute and employs only summary statistics; hence it can be applied in many different fields, such as meta-analysis, Mendelian randomization and distributed systems. The proposed estimator is consistent, even if many data sources are biased, and is asymptotically equivalent to the oracle estimator that uses only unbiased data. Asymptotic normality of the proposed estimator is also established. In contrast to existing meta-analysis methods, the theoretical properties are guaranteed for our estimator, even if the number of data sources and the dimension of the parameter diverge as the sample size increases. Furthermore, the proposed method provides consistent selection for unbiased data sources with probability approaching 1. Simulation studies demonstrate the efficiency and robustness of the proposed method empirically. The method is applied to a meta-analysis dataset to evaluate surgical treatment for moderate periodontal disease and to a Mendelian randomization dataset to study the risk factors for head and neck cancer.},
  archive      = {J_BIOMET},
  author       = {Wang, Ruoyu and Wang, Qihua and Miao, Wang},
  doi          = {10.1093/biomet/asad013},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1023-1040},
  shortjournal = {Biometrika},
  title        = {A robust fusion-extraction procedure with summary statistics in the presence of biased sources},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric estimation of the intensity function of a
spatial point process on a riemannian manifold. <em>BIOMET</em>,
<em>110</em>(4), 1009–1021. (<a
href="https://doi.org/10.1093/biomet/asad012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with nonparametric estimation of the intensity function of a point process on a Riemannian manifold. It provides a first-order asymptotic analysis of the proposed kernel estimator for Poisson processes, supplemented by empirical work to probe the behaviour in finite samples and under other generative regimes. The investigation highlights the scope for finite-sample improvements by allowing the bandwidth to adapt to local curvature.},
  archive      = {J_BIOMET},
  author       = {Ward, S and Battey, H S and Cohen, E A K},
  doi          = {10.1093/biomet/asad012},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1009-1021},
  shortjournal = {Biometrika},
  title        = {Nonparametric estimation of the intensity function of a spatial point process on a riemannian manifold},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An instrumental variable method for point processes:
Generalized wald estimation based on deconvolution. <em>BIOMET</em>,
<em>110</em>(4), 989–1008. (<a
href="https://doi.org/10.1093/biomet/asad005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point processes are probabilistic tools for modelling event data. While there exists a fast-growing literature on the relationships between point processes, how such relationships connect to causal effects remains unexplored. In the presence of unmeasured confounders, parameters from point process models do not necessarily have causal interpretations. We propose an instrumental variable method for causal inference with point process treatment and outcome. We define causal quantities based on potential outcomes and establish nonparametric identification results with a binary instrumental variable. We extend the traditional Wald estimation to deal with point process treatment and outcome, showing that it should be performed after a Fourier transform of the intention-to-treat effects on the treatment and outcome, and thus takes the form of deconvolution. We refer to this approach as generalized Wald estimation and propose an estimation strategy based on well-established deconvolution methods.},
  archive      = {J_BIOMET},
  author       = {Jiang, Zhichao and Chen, Shizhe and Ding, Peng},
  doi          = {10.1093/biomet/asad005},
  journal      = {Biometrika},
  number       = {4},
  pages        = {989-1008},
  shortjournal = {Biometrika},
  title        = {An instrumental variable method for point processes: Generalized wald estimation based on deconvolution},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proximal mediation analysis. <em>BIOMET</em>,
<em>110</em>(4), 973–987. (<a
href="https://doi.org/10.1093/biomet/asad015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common concern when trying to draw causal inferences from observational data is that the measured covariates are insufficiently rich to account for all sources of confounding. In practice, many of the covariates may only be proxies of the latent confounding mechanism. Recent work has shown that in certain settings where the standard no-unmeasured-confounding assumption fails, proxy variables can be leveraged to identify causal effects. Results currently exist for the total causal effect of an intervention, but little consideration has been given to learning about the direct or indirect pathways of the effect through a mediator variable. In this work, we describe three separate proximal identification results for natural direct and indirect effects in the presence of unmeasured confounding. We then develop a semiparametric framework for inference on natural direct and indirect effects, which leads us to locally efficient, multiply robust estimators.},
  archive      = {J_BIOMET},
  author       = {Dukes, Oliver and Shpitser, Ilya and Tchetgen Tchetgen, Eric J},
  doi          = {10.1093/biomet/asad015},
  journal      = {Biometrika},
  number       = {4},
  pages        = {973-987},
  shortjournal = {Biometrika},
  title        = {Proximal mediation analysis},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric efficient g-estimation with invalid
instrumental variables. <em>BIOMET</em>, <em>110</em>(4), 953–971. (<a
href="https://doi.org/10.1093/biomet/asad011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The instrumental variable method is widely used in the health and social sciences for identification and estimation of causal effects in the presence of potential unmeasured confounding. To improve efficiency, multiple instruments are routinely used, raising concerns about bias due to possible violation of the instrumental variable assumptions. To address such concerns, we introduce a new class of G-estimators that are guaranteed to remain consistent and asymptotically normal for the causal effect of interest provided that a set of at least |$\gamma$| out of |$K$| candidate instruments are valid, for |$\gamma \leqslant K$| set by the analyst ex ante without necessarily knowing the identities of the valid and invalid instruments. We provide formal semiparametric efficiency theory supporting our results. Simulation studies and applications to UK Biobank data demonstrate the superior empirical performance of the proposed estimators compared with competing methods.},
  archive      = {J_BIOMET},
  author       = {Sun, B and Liu, Z and Tchetgen Tchetgen, E J},
  doi          = {10.1093/biomet/asad011},
  journal      = {Biometrika},
  number       = {4},
  pages        = {953-971},
  shortjournal = {Biometrika},
  title        = {Semiparametric efficient G-estimation with invalid instrumental variables},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampling distribution for single-regression granger
causality estimators. <em>BIOMET</em>, <em>110</em>(4), 933–952. (<a
href="https://doi.org/10.1093/biomet/asad009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single-regression Granger–Geweke causality estimator has previously been shown to solve known problems associated with the more conventional likelihood ratio estimator; however, its sampling distribution has remained unknown. We show that, under the null hypothesis of vanishing Granger causality, the single-regression estimator converges to a generalized χ 2 distribution, which is well approximated by a Γ distribution. We show that this holds too for Geweke’s spectral causality averaged over a given frequency band, and derive explicit expressions for the generalized χ 2 and Γ-approximation parameters in both cases. We present a Neyman–Pearson test based on the single-regression estimators, and discuss how it may be deployed in empirical scenarios. We outline how our analysis may be extended to the conditional case, point-frequency spectral Granger causality and the important case of state-space Granger causality.},
  archive      = {J_BIOMET},
  author       = {Gutknecht, A J and Barnett, L},
  doi          = {10.1093/biomet/asad009},
  journal      = {Biometrika},
  number       = {4},
  pages        = {933-952},
  shortjournal = {Biometrika},
  title        = {Sampling distribution for single-regression granger causality estimators},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Targeted optimal treatment regime learning using summary
statistics. <em>BIOMET</em>, <em>110</em>(4), 913–931. (<a
href="https://doi.org/10.1093/biomet/asad020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized decision-making, aiming to derive optimal treatment regimes based on individual characteristics, has recently attracted increasing attention in many fields, such as medicine, social services and economics. Current literature mainly focuses on estimating treatment regimes from a single source population. In real-world applications, the distribution of a target population can be different from that of the source population. Therefore, treatment regimes learned by existing methods may not generalize well to the target popu- lation. Because of privacy concerns and other practical issues, individual-level data from the target population are often not available, which makes treatment regime learning more challenging. We consider the problem of treatment regime estimation when the source and target populations may be heterogeneous, individual-level data are available from the source population and only the summary information of covariates, such as moments, is accessible from the target population. We develop a weighting framework that tailors a treatment regime for a given target population by leveraging the available summary statistics. Specifically, we propose a calibrated augmented inverse probability weighted estimator of the value function for the target population and estimate an optimal treatment regime by maximizing this estimator within a class of prespecified regimes. We show that the proposed calibrated estimator is consistent and asymptotically normal even with flexible semi/nonparametric models for nuisance function approximation, and that the variance of the value estimator can be consistently estimated. We demonstrate the empirical performance of the proposed method using simulation studies and a real application using two datasets on sepsis.},
  archive      = {J_BIOMET},
  author       = {Chu, J and Lu, W and Yang, S},
  doi          = {10.1093/biomet/asad020},
  journal      = {Biometrika},
  number       = {4},
  pages        = {913-931},
  shortjournal = {Biometrika},
  title        = {Targeted optimal treatment regime learning using summary statistics},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soft calibration for selection bias problems under
mixed-effects models. <em>BIOMET</em>, <em>110</em>(4), 897–911. (<a
href="https://doi.org/10.1093/biomet/asad016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration weighting has been widely used to correct selection biases in nonprobability sampling, missing data and causal inference. The main idea is to calibrate the biased sample to the benchmark by adjusting the subject weights. However, hard calibration can produce enormous weights when an exact calibration is enforced on a large set of extraneous covariates. This article proposes a soft calibration scheme, where the outcome and the selection indicator follow mixed-effect models. The scheme imposes an exact calibration on the fixed effects and an approximate calibration on the random effects. On the one hand, our soft calibration has an intrinsic connection with best linear unbiased prediction, which results in a more efficient estimation compared to hard calibration. On the other hand, soft calibration weighting estimation can be envisioned as penalized propensity score weight estimation, with the penalty term motivated by the mixed-effect structure. The asymptotic distribution and a valid variance estimator are derived for soft calibration. We demonstrate the superiority of the proposed estimator over other competitors in simulation studies and using a real-world data application on the effect of BMI screening on childhood obesity.},
  archive      = {J_BIOMET},
  author       = {Gao, Chenyin and Yang, Shu and Kim, Jae Kwang},
  doi          = {10.1093/biomet/asad016},
  journal      = {Biometrika},
  number       = {4},
  pages        = {897-911},
  shortjournal = {Biometrika},
  title        = {Soft calibration for selection bias problems under mixed-effects models},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric counterfactual density estimation.
<em>BIOMET</em>, <em>110</em>(4), 875–896. (<a
href="https://doi.org/10.1093/biomet/asad017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal effects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters: density approximations and the distance between counterfactual densities. We study nonparametric efficiency bounds, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular nontrivial functions of counterfactuals, linking the problems of density and mean estimation. We propose doubly robust-style estimators, and study their rates of convergence, showing that they can be optimally efficient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but we highlight results for L 2 projections on linear models and Kullbach–Leibler projections on exponential families. Finally, we illustrate our method by estimating the density of the CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density effect. Our methods are implemented in the R package npcausal on GitHub.},
  archive      = {J_BIOMET},
  author       = {Kennedy, E H and Balakrishnan, S and Wasserman, L A},
  doi          = {10.1093/biomet/asad017},
  journal      = {Biometrika},
  number       = {4},
  pages        = {875-896},
  shortjournal = {Biometrika},
  title        = {Semiparametric counterfactual density estimation},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Rejoinder: “Statistical inference for streamed longitudinal
data.” <em>BIOMET</em>, <em>110</em>(4), 871–874. (<a
href="https://doi.org/10.1093/biomet/asad051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Luo, Lan and Wang, Jingshen and Hector, Emily C},
  doi          = {10.1093/biomet/asad051},
  journal      = {Biometrika},
  number       = {4},
  pages        = {871-874},
  shortjournal = {Biometrika},
  title        = {Rejoinder: ‘Statistical inference for streamed longitudinal data’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “statistical inference for streamed
longitudinal data.” <em>BIOMET</em>, <em>110</em>(4), 867–869. (<a
href="https://doi.org/10.1093/biomet/asad043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Ning, Yang and Duan, Jingyi},
  doi          = {10.1093/biomet/asad043},
  journal      = {Biometrika},
  number       = {4},
  pages        = {867-869},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Statistical inference for streamed longitudinal data’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “statistical inference for streamed
longitudinal data.” <em>BIOMET</em>, <em>110</em>(4), 863–866. (<a
href="https://doi.org/10.1093/biomet/asad035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Wang, J and Wang, H and Chen, K},
  doi          = {10.1093/biomet/asad035},
  journal      = {Biometrika},
  number       = {4},
  pages        = {863-866},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Statistical inference for streamed longitudinal data’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “statistical inference for streamed
longitudinal data.” <em>BIOMET</em>, <em>110</em>(4), 859–862. (<a
href="https://doi.org/10.1093/biomet/asad034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Song, Peter X-K and Zhou, Ling},
  doi          = {10.1093/biomet/asad034},
  journal      = {Biometrika},
  number       = {4},
  pages        = {859-862},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Statistical inference for streamed longitudinal data’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Statistical inference for streamed longitudinal data.
<em>BIOMET</em>, <em>110</em>(4), 841–858. (<a
href="https://doi.org/10.1093/biomet/asad010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern longitudinal data, for example from wearable devices, may consist of measurements of biological signals on a fixed set of participants at a diverging number of time-points. Traditional statistical methods are not equipped to handle the computational burden of repeatedly analysing the cumulatively growing dataset each time new data are collected. We propose a new estimation and inference framework for dynamic updating of point estimates and their standard errors along sequentially collected datasets with dependence, both within and between the datasets. The key technique is a decomposition of the extended inference function vector of the quadratic inference function constructed over the cumulative longitudinal data into a sum of summary statistics over data batches. We show how this sum can be recursively updated without the need to access the whole dataset, resulting in a computationally efficient streaming procedure with minimal loss of statistical efficiency. We prove consistency and asymptotic normality of our streaming estimator as the number of data batches diverges, even as the number of independent participants remains fixed. Simulations demonstrate the advantages of our approach over traditional statistical methods that assume independence between data batches. Finally, we investigate the relationship between physical activity and several diseases through analysis of accelerometry data from the National Health and Nutrition Examination Survey.},
  archive      = {J_BIOMET},
  author       = {Luo, Lan and Wang, Jingshen and Hector, Emily C},
  doi          = {10.1093/biomet/asad010},
  journal      = {Biometrika},
  number       = {4},
  pages        = {841-858},
  shortjournal = {Biometrika},
  title        = {Statistical inference for streamed longitudinal data},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: “Ancestor regression in linear structural
equation models.” <em>BIOMET</em>, <em>110</em>(3), 839. (<a
href="https://doi.org/10.1093/biomet/asad028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asad028},
  journal      = {Biometrika},
  number       = {3},
  pages        = {839},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Ancestor regression in linear structural equation models’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Median regularity and honest inference. <em>BIOMET</em>,
<em>110</em>(3), 831–838. (<a
href="https://doi.org/10.1093/biomet/asad002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new notion of regularity of an estimator called median regularity. We prove that uniformly valid honest inference for a functional is possible if and only if there exists a median regular estimator of that functional. To the best of our knowledge, such a notion of regularity that is necessary for uniformly valid inference is unavailable in the literature.},
  archive      = {J_BIOMET},
  author       = {Kuchibhotla, Arun Kumar and Balakrishnan, Sivaraman and Wasserman, Larry},
  doi          = {10.1093/biomet/asad002},
  journal      = {Biometrika},
  number       = {3},
  pages        = {831-838},
  shortjournal = {Biometrika},
  title        = {Median regularity and honest inference},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Marginal proportional hazards models for multivariate
interval-censored data. <em>BIOMET</em>, <em>110</em>(3), 815–830. (<a
href="https://doi.org/10.1093/biomet/asac059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate interval-censored data arise when there are multiple types of events or clusters of study subjects, such that the event times are potentially correlated and when each event is only known to occur over a particular time interval. We formulate the effects of potentially time-varying covariates on the multivariate event times through marginal proportional hazards models while leaving the dependence structures of the related event times unspecified. We construct the nonparametric pseudolikelihood under the working assumption that all event times are independent, and we provide a simple and stable EM-type algorithm. The resulting nonparametric maximum pseudolikelihood estimators for the regression parameters are shown to be consistent and asymptotically normal, with a limiting covariance matrix that can be consistently estimated by a sandwich estimator under arbitrary dependence structures for the related event times. We evaluate the performance of the proposed methods through extensive simulation studies and present an application to data from the Atherosclerosis Risk in Communities Study.},
  archive      = {J_BIOMET},
  author       = {Xu, Yangjianchen and Zeng, Donglin and Lin, D Y},
  doi          = {10.1093/biomet/asac059},
  journal      = {Biometrika},
  number       = {3},
  pages        = {815-830},
  shortjournal = {Biometrika},
  title        = {Marginal proportional hazards models for multivariate interval-censored data},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing kronecker product covariance matrices for
high-dimensional matrix-variate data. <em>BIOMET</em>, <em>110</em>(3),
799–814. (<a href="https://doi.org/10.1093/biomet/asac063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kronecker product covariance structure provides an efficient way to model the inter-correlations of matrix-variate data. In this paper, we propose test statistics for the Kronecker product covariance matrix based on linear spectral statistics of renormalized sample covariance matrices. A central limit theorem is proved for the linear spectral statistics, with explicit formulas for the mean and covariance functions, thereby filling a gap in the literature. We then show theoretically that the proposed test statistics have well-controlled size and high power. We further propose a bootstrap resampling algorithm to approximate the limiting distributions of the associated linear spectral statistics. Consistency of the bootstrap procedure is guaranteed under mild conditions. The proposed test procedure is also applicable to the Kronecker product covariance model with additional random noise. In our simulations, the empirical sizes of the proposed test procedure and its bootstrapped version are close to the corresponding theoretical values, while the power converges to |$1$| quickly as the dimension and sample size increase.},
  archive      = {J_BIOMET},
  author       = {Yu, Long and Xie, Jiahui and Zhou, Wang},
  doi          = {10.1093/biomet/asac063},
  journal      = {Biometrika},
  number       = {3},
  pages        = {799-814},
  shortjournal = {Biometrika},
  title        = {Testing kronecker product covariance matrices for high-dimensional matrix-variate data},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional analysis of variance in multivariate linear
regression. <em>BIOMET</em>, <em>110</em>(3), 777–797. (<a
href="https://doi.org/10.1093/biomet/asad001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a systematic theory for high-dimensional analysis of variance in multivariate linear regression, where the dimension and the number of coefficients can both grow with the sample size. We propose a new U -type statistic to test linear hypotheses and establish a high-dimensional Gaussian approximation result under fairly mild moment assumptions. Our general framework and theory can be used to deal with the classical one-way multivariate analysis of variance, and the nonparametric one-way multivariate analysis of variance in high dimensions. To implement the test procedure, we introduce a sample-splitting-based estimator of the second moment of the error covariance and discuss its properties. A simulation study shows that our proposed test outperforms some existing tests in various settings.},
  archive      = {J_BIOMET},
  author       = {Lou, Zhipeng and Zhang, Xianyang and Wu, Wei Biao},
  doi          = {10.1093/biomet/asad001},
  journal      = {Biometrika},
  number       = {3},
  pages        = {777-797},
  shortjournal = {Biometrika},
  title        = {High-dimensional analysis of variance in multivariate linear regression},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Existence of matching priors on compact spaces.
<em>BIOMET</em>, <em>110</em>(3), 763–776. (<a
href="https://doi.org/10.1093/biomet/asac061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A matching prior at level |$1-\alpha$| is a prior such that an associated |$1-\alpha$| credible region is also a |$1-\alpha$| confidence set. We study the existence of matching priors for general families of credible regions. Our main result gives topological conditions under which matching priors for specific families of credible regions exist. Informally, we prove that, on compact parameter spaces, a matching prior exists if the so-called rejection-probability function is jointly continuous when we adopt the Wasserstein metric on priors. In light of this general result, we observe that typical families of credible regions, such as credible balls, highest-posterior density regions, quantiles, etc., fail to meet this topological condition. We show how to design approximate posterior credible balls and highest-posterior density regions that meet these topological conditions, yielding matching priors. Finally, we evaluate a numerical scheme for computing approximately matching priors based on discretization and iteration. The proof of our main theorem uses tools from nonstandard analysis and establishes new results about the nonstandard extension of the Wasserstein metric that may be of independent interest.},
  archive      = {J_BIOMET},
  author       = {Duanmu, Haosui and Roy, Daniel M and Smith, Aaron},
  doi          = {10.1093/biomet/asac061},
  journal      = {Biometrika},
  number       = {3},
  pages        = {763-776},
  shortjournal = {Biometrika},
  title        = {Existence of matching priors on compact spaces},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable elimination, graph reduction and the efficient
g-formula. <em>BIOMET</em>, <em>110</em>(3), 739–761. (<a
href="https://doi.org/10.1093/biomet/asac062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study efficient estimation of an interventional mean associated with a point exposure treatment under a causal graphical model represented by a directed acyclic graph without hidden variables. Under such a model, a subset of the variables may be uninformative, in that failure to measure them neither precludes identification of the interventional mean nor changes the semiparametric variance bound for regular estimators of it. We develop a set of graphical criteria that are sound and complete for eliminating all the uninformative variables, so that the cost of measuring them can be saved without sacrificing estimation efficiency, which could be useful when designing a planned observational or randomized study. Further, we construct a reduced directed acyclic graph on the set of informative variables only. We show that the interventional mean is identified from the marginal law by the g-formula (Robins, 1986) associated with the reduced graph, and the semiparametric variance bounds for estimating the interventional mean under the original and the reduced graphical model agree. The g-formula is an irreducible, efficient identifying formula in the sense that the nonparametric estimator of the formula, under regularity conditions, is asymptotically efficient under the original causal graphical model, and no formula with this property exists that depends only on a strict subset of the variables.},
  archive      = {J_BIOMET},
  author       = {Guo, F Richard and Perković, Emilija and Rotnitzky, Andrea},
  doi          = {10.1093/biomet/asac062},
  journal      = {Biometrika},
  number       = {3},
  pages        = {739-761},
  shortjournal = {Biometrika},
  title        = {Variable elimination, graph reduction and the efficient g-formula},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dependent censoring based on parametric copulas.
<em>BIOMET</em>, <em>110</em>(3), 721–738. (<a
href="https://doi.org/10.1093/biomet/asac067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a survival time |$T$| that is subject to random right censoring, and suppose that |$T$| is stochastically dependent on the censoring time |$C$|⁠ . We are interested in the marginal distribution of |$T$|⁠ . This situation is often encountered in practice. Consider, for example, the case where |$T$| is a patient’s time to death from a certain disease. Then the censoring time |$C$| could be the time until the patient leaves the study or the time until death from another cause. If the reason for leaving the study is related to the health condition of the patient, or if the patient dies from a disease that has similar risk factors to the disease of interest, then |$T$| and |$C$| are likely to be dependent. In this paper we propose a new model that takes such dependence into account. The model is based on a parametric copula for the relationship between |$T$| and |$C$|⁠ , and on parametric marginal distributions for |$T$| and |$C$|⁠ . Unlike most other authors, we do not assume that the parameter defining the copula is known. We give sufficient conditions on these parametric copulas and marginals under which the bivariate distribution of |$(T,C)$| is identified. These sufficient conditions are then checked for a wide range of common copulas and marginals. We also study the estimation of the model, and carry out extensive simulations and analysis on a pancreatic cancer dataset to illustrate the proposed model and estimation procedure.},
  archive      = {J_BIOMET},
  author       = {Czado, C and Van Keilegom, I},
  doi          = {10.1093/biomet/asac067},
  journal      = {Biometrika},
  number       = {3},
  pages        = {721-738},
  shortjournal = {Biometrika},
  title        = {Dependent censoring based on parametric copulas},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral adjustment for spatial confounding.
<em>BIOMET</em>, <em>110</em>(3), 699–719. (<a
href="https://doi.org/10.1093/biomet/asac069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjusting for an unmeasured confounder is generally an intractable problem, but in the spatial setting it may be possible under certain conditions. We derive necessary conditions on the coherence between the exposure and the unmeasured confounder that ensure the effect of exposure is estimable. We specify our model and assumptions in the spectral domain to allow for different degrees of confounding at different spatial resolutions. One assumption that ensures identifiability is that confounding present at global scales dissipates at local scales. We show that this assumption in the spectral domain is equivalent to adjusting for global-scale confounding in the spatial domain by adding a spatially smoothed version of the exposure to the mean of the response variable. Within this general framework, we propose a sequence of confounder adjustment methods that range from parametric adjustments based on the Matérn coherence function to more robust semiparametric methods that use smoothing splines. These ideas are applied to areal and geostatistical data for both simulated and real datasets.},
  archive      = {J_BIOMET},
  author       = {Guan, Yawen and Page, Garritt L and Reich, Brian J and Ventrucci, Massimo and Yang, Shu},
  doi          = {10.1093/biomet/asac069},
  journal      = {Biometrika},
  number       = {3},
  pages        = {699-719},
  shortjournal = {Biometrika},
  title        = {Spectral adjustment for spatial confounding},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Thresholded graphical lasso adjusts for latent variables.
<em>BIOMET</em>, <em>110</em>(3), 681–697. (<a
href="https://doi.org/10.1093/biomet/asac060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural learning of Gaussian graphical models in the presence of latent variables has long been a challenging problem. Chandrasekaran et al. (2012) proposed a convex program for estimating a sparse graph plus a low-rank term that adjusts for latent variables; however, this approach poses challenges from both computational and statistical perspectives. We propose an alternative, simple solution: apply a hard-thresholding operator to existing graph selection methods. Conceptually simple and computationally attractive, the approach of thresholding the graphical lasso is shown to be graph selection consistent in the presence of latent variables under a simpler minimum edge strength condition and at an improved statistical rate. The results are extended to estimators for thresholded neighbourhood selection and constrained |$\ell_{1}$| -minimization for inverse matrix estimation as well. We show that our simple thresholded graph estimators yield stronger empirical results than existing methods for the latent variable graphical model problem, and we apply them to a neuroscience case study on estimating functional neural connections.},
  archive      = {J_BIOMET},
  author       = {Wang, Minjie and Allen, Genevera I},
  doi          = {10.1093/biomet/asac060},
  journal      = {Biometrika},
  number       = {3},
  pages        = {681-697},
  shortjournal = {Biometrika},
  title        = {Thresholded graphical lasso adjusts for latent variables},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Honest calibration assessment for binary outcome
predictions. <em>BIOMET</em>, <em>110</em>(3), 663–680. (<a
href="https://doi.org/10.1093/biomet/asac068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability predictions from binary regressions or machine learning methods ought to be calibrated: if an event is predicted to occur with probability |$x$|⁠ , it should materialize with approximately that frequency, which means that the so-called calibration curve |$p(\cdot)$| should equal the identity, i.e., |$p(x) = x$| for all |$x$| in the unit interval. We propose honest calibration assessment based on novel confidence bands for the calibration curve, which are valid subject to only the natural assumption of isotonicity. Besides testing the classical goodness-of-fit null hypothesis of perfect calibration, our bands facilitate inverted goodness-of-fit tests whose rejection allows for the sought-after conclusion of a sufficiently well-specified model. We show that our bands have a finite-sample coverage guarantee, are narrower than those of existing approaches, and adapt to the local smoothness of the calibration curve |$p$| and the local variance of the binary observations. In an application to modelling predictions of an infant having low birth weight, the bounds give informative insights into model calibration.},
  archive      = {J_BIOMET},
  author       = {Dimitriadis, Timo and Dümbgen, Lutz and Henzi, Alexander and Puke, Marius and Ziegel, Johanna},
  doi          = {10.1093/biomet/asac068},
  journal      = {Biometrika},
  number       = {3},
  pages        = {663-680},
  shortjournal = {Biometrika},
  title        = {Honest calibration assessment for binary outcome predictions},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing time-varying causal effect moderation in the
presence of cluster-level treatment effect heterogeneity and
interference. <em>BIOMET</em>, <em>110</em>(3), 645–662. (<a
href="https://doi.org/10.1093/biomet/asac065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The micro-randomized trial is a sequential randomized experimental design to empirically evaluate the effectiveness of mobile health intervention components that may be delivered at hundreds or thousands of decision points. Micro-randomized trials have motivated a new class of causal estimands, termed causal excursion effects , for which semiparametric inference can be conducted via a weighted, centred least-squares criterion (Boruvka et al., 2018). Causal excursion effects allow health scientists to answer important scientific questions about how intervention effectiveness may change over time or may be moderated by individual characteristics, time-varying context or past responses. Existing definitions and associated methods assume between-subject independence and noninterference. Deviations from these assumptions often occur. In this paper, causal excursion effects are revisited under potential cluster-level treatment effect heterogeneity and interference, where the treatment effect of interest may depend on cluster-level moderators. Utility of the proposed methods is shown by analysing data from a multi-institution cohort of first-year medical residents in the United States.},
  archive      = {J_BIOMET},
  author       = {Shi, J and Wu, Z and Dempsey, W},
  doi          = {10.1093/biomet/asac065},
  journal      = {Biometrika},
  number       = {3},
  pages        = {645-662},
  shortjournal = {Biometrika},
  title        = {Assessing time-varying causal effect moderation in the presence of cluster-level treatment effect heterogeneity and interference},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the statistical role of inexact matching in observational
studies. <em>BIOMET</em>, <em>110</em>(3), 631–644. (<a
href="https://doi.org/10.1093/biomet/asac066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational causal inference, exact covariate matching plays two statistical roles: (i) it effectively controls for bias due to measured confounding; (ii) it justifies assumption-free inference based on randomization tests. In this paper we show that inexact covariate matching does not always play these same roles. We find that inexact matching often leaves behind statistically meaningful bias, and that this bias renders standard randomization tests asymptotically invalid. We therefore recommend additional model-based covariate adjustment after inexact matching. In the framework of local misspecification, we prove that matching makes subsequent parametric analyses less sensitive to model selection or misspecification. We argue that gaining such robustness is the primary statistical role of inexact matching.},
  archive      = {J_BIOMET},
  author       = {Guo, Kevin and Rothenhäusler, Dominik},
  doi          = {10.1093/biomet/asac066},
  journal      = {Biometrika},
  number       = {3},
  pages        = {631-644},
  shortjournal = {Biometrika},
  title        = {On the statistical role of inexact matching in observational studies},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the implied weights of linear regression for causal
inference. <em>BIOMET</em>, <em>110</em>(3), 615–629. (<a
href="https://doi.org/10.1093/biomet/asac058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basic principle in the design of observational studies is to approximate the randomized experiment that would have been conducted under ideal circumstances. At present, linear regression models are commonly used to analyse observational data and estimate causal effects. How do linear regression adjustments in observational studies emulate key features of randomized experiments, such as covariate balance, self-weighted sampling and study representativeness? In this paper, we provide answers to this and related questions by analysing the implied individual-level data weights of various linear regression methods. We derive new closed-form expressions of these implied weights, and examine their properties in both finite and large samples. Among others, in finite samples we characterize the implied target population of linear regression, and in large samples demonstrate the multiply robust properties of regression estimators from the perspective of their implied weights. We show that the implied weights of general regression methods can be equivalently obtained by solving a convex optimization problem. This equivalence allows us to bridge ideas from the regression modelling and causal inference literatures. As a result, we propose novel regression diagnostics for causal inference that are part of the design stage of an observational study. We implement the weights and diagnostics in the new lmw package for R .},
  archive      = {J_BIOMET},
  author       = {Chattopadhyay, Ambarish and Zubizarreta, José R},
  doi          = {10.1093/biomet/asac058},
  journal      = {Biometrika},
  number       = {3},
  pages        = {615-629},
  shortjournal = {Biometrika},
  title        = {On the implied weights of linear regression for causal inference},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Splitting strategies for post-selection inference.
<em>BIOMET</em>, <em>110</em>(3), 597–614. (<a
href="https://doi.org/10.1093/biomet/asac070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of providing valid inference for a selected parameter in a sparse regression setting. It is well known that classical regression tools can be unreliable in this context because of the bias generated in the selection step. Many approaches have been proposed in recent years to ensure inferential validity. In this article we consider a simple alternative to data splitting based on randomizing the response vector, which allows for higher selection and inferential power than the former, and is applicable with an arbitrary selection rule. We perform a theoretical and empirical comparison of the two methods and derive a central limit theorem for the randomization approach. Our investigations show that the gain in power can be substantial.},
  archive      = {J_BIOMET},
  author       = {Rasines, D García and Young, G A},
  doi          = {10.1093/biomet/asac070},
  journal      = {Biometrika},
  number       = {3},
  pages        = {597-614},
  shortjournal = {Biometrika},
  title        = {Splitting strategies for post-selection inference},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal design of the barker proposal and other locally
balanced metropolis–hastings algorithms. <em>BIOMET</em>,
<em>110</em>(3), 579–595. (<a
href="https://doi.org/10.1093/biomet/asac056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the class of first-order locally balanced Metropolis–Hastings algorithms introduced in Livingstone &amp; Zanella (2022). To choose a specific algorithm within the class, the user must select a balancing function |$g:{\mathbb{R}}_+ \to {\mathbb{R}}_+$| satisfying |$g(t) = tg(1/t)$| and a noise distribution for the proposal increment. Popular choices within the class are the Metropolis-adjusted Langevin algorithm and the recently introduced Barker proposal. We first establish a general limiting optimal acceptance rate of 57 |$\%$| and scaling of |$n^{-1/3}$|⁠ , as the dimension |$n$| tends to infinity among all members of the class under mild smoothness assumptions on |$g$| and when the target distribution for the algorithm is of product form. In particular, we obtain an explicit expression for the asymptotic efficiency of an arbitrary algorithm in the class, as measured by expected squared jumping distance. We then consider how to optimize this expression under various constraints. We derive an optimal choice of noise distribution for the Barker proposal, an optimal choice of balancing function under a Gaussian noise distribution, and an optimal choice of first-order locally balanced algorithm among the entire class, which turns out to depend on the specific target distribution. Numerical simulations confirm our theoretical findings, and in particular, show that a bimodal choice of noise distribution in the Barker proposal gives rise to a practical algorithm that is consistently more efficient than the original Gaussian version.},
  archive      = {J_BIOMET},
  author       = {Vogrinc, Jure and Livingstone, Samuel and Zanella, Giacomo},
  doi          = {10.1093/biomet/asac056},
  journal      = {Biometrika},
  number       = {3},
  pages        = {579-595},
  shortjournal = {Biometrika},
  title        = {Optimal design of the barker proposal and other locally balanced Metropolis–Hastings algorithms},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized bayes framework for probabilistic clustering.
<em>BIOMET</em>, <em>110</em>(3), 559–578. (<a
href="https://doi.org/10.1093/biomet/asad004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss-based clustering methods, such as k-means clustering and its variants, are standard tools for finding groups in data. However, the lack of quantification of uncertainty in the estimated clusters is a disadvantage. Model-based clustering based on mixture models provides an alternative approach, but such methods face computational problems and are highly sensitive to the choice of kernel. In this article we propose a generalized Bayes framework that bridges between these paradigms through the use of Gibbs posteriors. In conducting Bayesian updating, the loglikelihood is replaced by a loss function for clustering, leading to a rich family of clustering methods. The Gibbs posterior represents a coherent updating of Bayesian beliefs without needing to specify a likelihood for the data, and can be used for characterizing uncertainty in clustering. We consider losses based on Bregman divergence and pairwise similarities, and develop efficient deterministic algorithms for point estimation along with sampling algorithms for uncertainty quantification. Several existing clustering algorithms, including k-means, can be interpreted as generalized Bayes estimators in our framework, and thus we provide a method of uncertainty quantification for these approaches, allowing, for example, calculation of the probability that a data point is well clustered.},
  archive      = {J_BIOMET},
  author       = {Rigon, Tommaso and Herring, Amy H and Dunson, David B},
  doi          = {10.1093/biomet/asad004},
  journal      = {Biometrika},
  number       = {3},
  pages        = {559-578},
  shortjournal = {Biometrika},
  title        = {A generalized bayes framework for probabilistic clustering},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering consistency with dirichlet process mixtures.
<em>BIOMET</em>, <em>110</em>(2), 551–558. (<a
href="https://doi.org/10.1093/biomet/asac051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dirichlet process mixtures are flexible nonparametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.},
  archive      = {J_BIOMET},
  author       = {Ascolani, F and Lijoi, A and Rebaudo, G and Zanella, G},
  doi          = {10.1093/biomet/asac051},
  journal      = {Biometrika},
  number       = {2},
  pages        = {551-558},
  shortjournal = {Biometrika},
  title        = {Clustering consistency with dirichlet process mixtures},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal row-column designs. <em>BIOMET</em>,
<em>110</em>(2), 537–549. (<a
href="https://doi.org/10.1093/biomet/asac046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Row-column designs have been widely used in experiments involving double confounding. Among them, one that provides unconfounded estimation of all main effects and as many two-factor interactions as possible is preferred, and is called optimal. Most current work focuses on the construction of two-level row-column designs, while the corresponding optimality theory has been largely ignored. Moreover, most constructed designs contain at least one replicate of a full factorial design, which is not flexible as the number of factors increases. In this study, a theoretical framework is built up to evaluate the optimality of row-column designs with prime level. A method for constructing optimal row-column designs with prime level is proposed. Subsequently, optimal full factorial three-level row-column designs are constructed for any parameter combination. Optimal fractional factorial two-level and three-level row-column designs are also constructed for cost saving.},
  archive      = {J_BIOMET},
  author       = {Zhou, Zheng and Zhou, Yongdao},
  doi          = {10.1093/biomet/asac046},
  journal      = {Biometrika},
  number       = {2},
  pages        = {537-549},
  shortjournal = {Biometrika},
  title        = {Optimal row-column designs},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiplicative structural nested mean model for
zero-inflated outcomes. <em>BIOMET</em>, <em>110</em>(2), 519–536. (<a
href="https://doi.org/10.1093/biomet/asac050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-inflated nonnegative outcomes are common in many applications. In this work, motivated by freemium mobile game data, we propose a class of multiplicative structural nested mean models for zero-inflated nonnegative outcomes which flexibly describes the joint effect of a sequence of treatments in the presence of time-varying confounders. The proposed estimator solves a doubly robust estimating equation, where the nuisance functions, namely the propensity score and conditional outcome means given confounders, are estimated parametrically or nonparametrically. To improve the accuracy, we leverage the characteristic of zero-inflated outcomes by estimating the conditional means in two parts, that is, separately modelling the probability of having positive outcomes given confounders, and the mean outcome conditional on its being positive and given the confounders. We show that the proposed estimator is consistent and asymptotically normal as either the sample size or the follow-up time goes to infinity. Moreover, the typical sandwich formula can be used to estimate the variance of treatment effect estimators consistently, without accounting for the variation due to estimating nuisance functions. Simulation studies and an application to a freemium mobile game dataset are presented to demonstrate the empirical performance of the proposed method and support our theoretical findings.},
  archive      = {J_BIOMET},
  author       = {Yu, Miao and Lu, Wenbin and Yang, Shu and Ghosh, Pulak},
  doi          = {10.1093/biomet/asac050},
  journal      = {Biometrika},
  number       = {2},
  pages        = {519-536},
  shortjournal = {Biometrika},
  title        = {A multiplicative structural nested mean model for zero-inflated outcomes},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrapping whittle estimators. <em>BIOMET</em>,
<em>110</em>(2), 499–518. (<a
href="https://doi.org/10.1093/biomet/asac044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting parametric models by optimizing frequency-domain objective functions is an attractive approach of parameter estimation in time series analysis. Whittle estimators are a prominent example in this context. Under weak conditions and the assumption that the true spectral density of the underlying process does not necessarily belong to the parametric class of spectral densities fitted, the distribution of Whittle estimators typically depends on difficult to estimate characteristics of the underlying process. This makes the implementation of asymptotic results for the construction of confidence intervals or for assessing the variability of estimators difficult in practice. In this paper we propose a frequency-domain bootstrap method to estimate the distribution of Whittle estimators that is asymptotically valid under assumptions that not only allow for possible model misspecification, but also for weak dependence conditions that are satisfied by a wide range of stationary stochastic processes. Adaptations of the bootstrap procedure developed to incorporate different modifications of Whittle estimators proposed in the literature, such as, for instance, tapered, debiased or boundary extended Whittle estimators, are also considered. Simulations demonstrate the capabilities of the bootstrap method proposed and its good finite sample performance. A real-life data analysis on sunspots is also presented.},
  archive      = {J_BIOMET},
  author       = {Kreiss, J -P and Paparoditis, E},
  doi          = {10.1093/biomet/asac044},
  journal      = {Biometrika},
  number       = {2},
  pages        = {499-518},
  shortjournal = {Biometrika},
  title        = {Bootstrapping whittle estimators},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample-constrained partial identification with application
to selection bias. <em>BIOMET</em>, <em>110</em>(2), 485–498. (<a
href="https://doi.org/10.1093/biomet/asac042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many partial identification problems can be characterized by the optimal value of a function over a set where both the function and set need to be estimated by empirical data. Despite some progress for convex problems, statistical inference in this general setting remains to be developed. To address this, we derive an asymptotically valid confidence interval for the optimal value through an appropriate relaxation of the estimated set. We then apply this general result to the problem of selection bias in population-based cohort studies. We show that existing sensitivity analyses, which are often conservative and difficult to implement, can be formulated in our framework and made significantly more informative via auxiliary information on the population. We conduct a simulation study to evaluate the finite sample performance of our inference procedure, and conclude with a substantive motivating example on the causal effect of education on income in the highly selected UK Biobank cohort. We demonstrate that our method can produce informative bounds using plausible population-level auxiliary constraints. We implement this method in the |$\texttt{R}$| package |$\texttt{selectioninterval}$|⁠ .},
  archive      = {J_BIOMET},
  author       = {Tudball, Matthew J and Hughes, Rachael A and Tilling, Kate and Bowden, Jack and Zhao, Qingyuan},
  doi          = {10.1093/biomet/asac042},
  journal      = {Biometrika},
  number       = {2},
  pages        = {485-498},
  shortjournal = {Biometrika},
  title        = {Sample-constrained partial identification with application to selection bias},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design-based theory for cluster rerandomization.
<em>BIOMET</em>, <em>110</em>(2), 467–483. (<a
href="https://doi.org/10.1093/biomet/asac045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complete randomization balances covariates on average, but covariate imbalance often exists in finite samples. Rerandomization can ensure covariate balance in the realized experiment by discarding the undesired treatment assignments. Many field experiments in public health and social sciences assign the treatment at the cluster level due to logistical constraints or policy considerations. Moreover, they are frequently combined with re-randomization in the design stage. We define cluster rerandomization as a cluster-randomized experiment compounded with rerandomization to balance covariates at the individual or cluster level. Existing asymptotic theory can only deal with rerandomization with treatments assigned at the individual level, leaving that for cluster rerandomization an open problem. To fill the gap, we provide a design-based theory for cluster rerandomization. Moreover, we compare two cluster rerandomization schemes that use prior information on the importance of the covariates: one based on the weighted Euclidean distance and the other based on the Mahalanobis distance with tiers of covariates. We demonstrate that the former dominates the latter with optimal weights and orthogonalized covariates. Last but not least, we discuss the role of covariate adjustment in the analysis stage, and recommend covariate-adjusted procedures that can be conveniently implemented by least squares with the associated robust standard errors.},
  archive      = {J_BIOMET},
  author       = {Lu, Xin and Liu, Tianle and Liu, Hanzhong and Ding, Peng},
  doi          = {10.1093/biomet/asac045},
  journal      = {Biometrika},
  number       = {2},
  pages        = {467-483},
  shortjournal = {Biometrika},
  title        = {Design-based theory for cluster rerandomization},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating causes of effects by posterior effects of causes.
<em>BIOMET</em>, <em>110</em>(2), 449–465. (<a
href="https://doi.org/10.1093/biomet/asac038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the case with a single causal variable, Dawid et al. (2014) defined the probability of causation, and Pearl (2000) defined the probability of necessity to assess the causes of effects. For a case with multiple causes that could affect each other, this paper defines the posterior total and direct causal effects based on the evidence observed for post-treatment variables, which could be viewed as measurements of causes of effects. Posterior causal effects involve the probabilities of counterfactual variables. Thus, as with the probability of causation, the probability of necessity and direct causal effects, the identifiability of posterior total and direct causal effects requires more assumptions than the identifiability of traditional causal effects conditional on pre-treatment variables. We present assumptions required for the identifiability of posterior causal effects and provide identification equations. Further, when the causal relationships between multiple causes and an endpoint can be depicted by causal networks, we can simplify both the required assumptions and the identification equations of the posterior total and direct causal effects. Finally, using numerical examples, we compare the posterior total and direct causal effects with other measures for evaluating the causes of effects and the population attributable risks.},
  archive      = {J_BIOMET},
  author       = {Lu, Zitong and Geng, Zhi and Li, Wei and Zhu, Shengyu and Jia, Jinzhu},
  doi          = {10.1093/biomet/asac038},
  journal      = {Biometrika},
  number       = {2},
  pages        = {449-465},
  shortjournal = {Biometrika},
  title        = {Evaluating causes of effects by posterior effects of causes},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lasso-adjusted treatment effect estimation under
covariate-adaptive randomization. <em>BIOMET</em>, <em>110</em>(2),
431–447. (<a href="https://doi.org/10.1093/biomet/asac036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating and inferring treatment effects in randomized experiments. In practice, stratified randomization, or more generally, covariate-adaptive randomization, is routinely used in the design stage to balance treatment allocations with respect to a few variables that are most relevant to the outcomes. Then, regression is performed in the analysis stage to adjust the remaining imbalances to yield more efficient treatment effect estimators. Building upon and unifying recent results obtained for ordinary-least-squares adjusted estimators under covariate-adaptive randomization, this paper presents a general theory of regression adjustment that allows for model mis-specification and the presence of a large number of baseline covariates. We exemplify the theory on two lasso-adjusted treatment effect estimators, both of which are optimal in their respective classes. In addition, nonparametric consistent variance estimators are proposed to facilitate valid inferences, which work irrespective of the specific randomization methods used. The robustness and improved efficiency of the proposed estimators are demonstrated through numerical studies.},
  archive      = {J_BIOMET},
  author       = {Liu, Hanzhong and Tu, Fuyi and Ma, Wei},
  doi          = {10.1093/biomet/asac036},
  journal      = {Biometrika},
  number       = {2},
  pages        = {431-447},
  shortjournal = {Biometrika},
  title        = {Lasso-adjusted treatment effect estimation under covariate-adaptive randomization},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel two-sample tests in high dimensions: Interplay
between moment discrepancy and dimension-and-sample orders.
<em>BIOMET</em>, <em>110</em>(2), 411–430. (<a
href="https://doi.org/10.1093/biomet/asac049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the increasing use of kernel-based metrics for high-dimensional and large-scale data, we study the asymptotic behaviour of kernel two-sample tests when the dimension and sample sizes both diverge to infinity. We focus on the maximum mean discrepancy using an isotropic kernel, which includes maximum mean discrepancy with the Gaussian kernel and the Laplace kernel, and the energy distance as special cases. We derive asymptotic expansions of the kernel two-sample statistics, based on which we establish a central limit theorem under both the null hypothesis and the local and fixed alternatives. The new nonnull central limit theorem results allow us to perform asymptotic exact power analysis, which reveals a delicate interplay between the moment discrepancy that can be detected by the kernel two-sample tests and the dimension-and-sample orders. The asymptotic theory is further corroborated through numerical studies.},
  archive      = {J_BIOMET},
  author       = {Yan, Jian and Zhang, Xianyang},
  doi          = {10.1093/biomet/asac049},
  journal      = {Biometrika},
  number       = {2},
  pages        = {411-430},
  shortjournal = {Biometrika},
  title        = {Kernel two-sample tests in high dimensions: Interplay between moment discrepancy and dimension-and-sample orders},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stage optimal dynamic treatment regimes for survival
outcomes with dependent censoring. <em>BIOMET</em>, <em>110</em>(2),
395–410. (<a href="https://doi.org/10.1093/biomet/asac047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a reinforcement learning method for estimating an optimal dynamic treatment regime for survival outcomes with dependent censoring. The estimator allows the failure time to be conditionally independent of censoring and dependent on the treatment decision times, supports a flexible number of treatment arms and treatment stages, and can maximize either the mean survival time or the survival probability at a certain time-point. The estimator is constructed using generalized random survival forests and can have polynomial rates of convergence. Simulations and analysis of the Atherosclerosis Risk in Communities study data suggest that the new estimator brings higher expected outcomes than existing methods in various settings.},
  archive      = {J_BIOMET},
  author       = {Cho, Hunyong and Holloway, Shannon T and Couper, David J and Kosorok, Michael R},
  doi          = {10.1093/biomet/asac047},
  journal      = {Biometrika},
  number       = {2},
  pages        = {395-410},
  shortjournal = {Biometrika},
  title        = {Multi-stage optimal dynamic treatment regimes for survival outcomes with dependent censoring},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional linear regression for discretely observed data:
From ideal to reality. <em>BIOMET</em>, <em>110</em>(2), 381–393. (<a
href="https://doi.org/10.1093/biomet/asac053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite extensive studies on functional linear regression, there exists a fundamental gap in theory between the ideal estimation from fully observed covariate functions and the reality that one can only observe functional covariates discretely with noise. The challenge arises when deriving a sharp perturbation bound for the estimated eigenfunctions in the latter case, which renders existing techniques for functional linear regression not applicable. We use a pooling method to attain the estimated eigenfunctions and propose a sample-splitting strategy to estimate the principal component scores, which facilitates the theoretical treatment for discretely observed data. The slope function is estimated by approximated least squares, and we show that the resulting estimator attains the optimal convergence rates for both estimation and prediction when the number of measurements per subject reaches a certain magnitude of the sample size. This phase transition phenomenon differs from the known results for the pooled mean and covariance estimation, and reveals the elevated difficulty in estimating the regression function. Numerical experiments, using simulated and real data examples, yield favourable results when compared with existing methods.},
  archive      = {J_BIOMET},
  author       = {Zhou, Hang and Yao, Fang and Zhang, Huiming},
  doi          = {10.1093/biomet/asac053},
  journal      = {Biometrika},
  number       = {2},
  pages        = {381-393},
  shortjournal = {Biometrika},
  title        = {Functional linear regression for discretely observed data: From ideal to reality},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Additive models for symmetric positive-definite matrices and
lie groups. <em>BIOMET</em>, <em>110</em>(2), 361–379. (<a
href="https://doi.org/10.1093/biomet/asac055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and investigate an additive regression model for symmetric positive-definite matrix-valued responses and multiple scalar predictors. The model exploits the Abelian group structure inherited from either of the log-Cholesky and log-Euclidean frameworks for symmetric positive-definite matrices and naturally extends to general Abelian Lie groups. The proposed additive model is shown to connect to an additive model on a tangent space. This connection not only entails an efficient algorithm to estimate the component functions, but also allows one to generalize the proposed additive model to general Riemannian manifolds. Optimal asymptotic convergence rates and normality of the estimated component functions are established, and numerical studies show that the proposed model enjoys good numerical performance, and is not subject to the curse of dimensionality when there are multiple predictors. The practical merits of the proposed model are demonstrated through an analysis of brain diffusion tensor imaging data.},
  archive      = {J_BIOMET},
  author       = {Lin, Z and Müller, H -G and Park, B U},
  doi          = {10.1093/biomet/asac055},
  journal      = {Biometrika},
  number       = {2},
  pages        = {361-379},
  shortjournal = {Biometrika},
  title        = {Additive models for symmetric positive-definite matrices and lie groups},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient-based sparse principal component analysis with
extensions to online learning. <em>BIOMET</em>, <em>110</em>(2),
339–360. (<a href="https://doi.org/10.1093/biomet/asac041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse principal component analysis is an important technique for simultaneous dimensionality reduction and variable selection with high-dimensional data. In this work we combine the unique geometric structure of the sparse principal component analysis problem with recent advances in convex optimization to develop novel gradient-based sparse principal component analysis algorithms. These algorithms enjoy the same global convergence guarantee as the original alternating direction method of multipliers, and can be more efficiently implemented with the rich toolbox developed for gradient methods from the deep learning literature. Most notably, these gradient-based algorithms can be combined with stochastic gradient descent methods to produce efficient online sparse principal component analysis algorithms with provable numerical and statistical performance guarantees. The practical performance and usefulness of the new algorithms are demonstrated in various simulation studies. As an application, we show how the scalability and statistical accuracy of our method enable us to find interesting functional gene groups in high-dimensional RNA sequencing data.},
  archive      = {J_BIOMET},
  author       = {Qiu, Yixuan and Lei, Jing and Roeder, Kathryn},
  doi          = {10.1093/biomet/asac041},
  journal      = {Biometrika},
  number       = {2},
  pages        = {339-360},
  shortjournal = {Biometrika},
  title        = {Gradient-based sparse principal component analysis with extensions to online learning},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian universal likelihood ratio testing.
<em>BIOMET</em>, <em>110</em>(2), 319–337. (<a
href="https://doi.org/10.1093/biomet/asac064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical likelihood ratio test based on the asymptotic chi-squared distribution of the log-likelihood is one of the fundamental tools of statistical inference. A recent universal likelihood ratio test approach based on sample splitting provides valid hypothesis tests and confidence sets in any setting for which we can compute the split likelihood ratio statistic, or, more generally, an upper bound on the null maximum likelihood. The universal likelihood ratio test is valid in finite samples and without regularity conditions. This test empowers statisticians to construct tests in settings for which no valid hypothesis test previously existed. For the simple, but fundamental, case of testing the population mean of |$d$| -dimensional Gaussian data with an identity covariance matrix, the classical likelihood ratio test itself applies. Thus, this setting serves as a perfect test bed to compare the classical likelihood ratio test against the universal likelihood ratio test. This work presents the first in-depth exploration of the size, power and relationships between several universal likelihood ratio test variants. We show that a repeated subsampling approach is the best choice in terms of size and power. For large numbers of subsamples, the repeated subsampling set is approximately spherical. We observe reasonable performance even in a high-dimensional setting, where the expected squared radius of the best universal likelihood ratio test’s confidence set is approximately 3/2 times the squared radius of the classical likelihood ratio test’s spherical confidence set. We illustrate the benefits of the universal likelihood ratio test through testing a nonconvex doughnut-shaped null hypothesis, where a universal inference procedure can have higher power than a standard approach.},
  archive      = {J_BIOMET},
  author       = {Dunn, Robin and Ramdas, Aaditya and Balakrishnan, Sivaraman and Wasserman, Larry},
  doi          = {10.1093/biomet/asac064},
  journal      = {Biometrika},
  number       = {2},
  pages        = {319-337},
  shortjournal = {Biometrika},
  title        = {Gaussian universal likelihood ratio testing},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hug and hop: A discrete-time, nonreversible markov chain
monte carlo algorithm. <em>BIOMET</em>, <em>110</em>(2), 301–318. (<a
href="https://doi.org/10.1093/biomet/asac039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the hug and hop Markov chain Monte Carlo algorithm for estimating expectations with respect to an intractable distribution. The algorithm alternates between two kernels, referred to as hug and hop. Hug is a nonreversible kernel that repeatedly applies the bounce mechanism from the recently proposed bouncy particle sampler to produce a proposal point that is far from the current position yet on almost the same contour of the target density, leading to a high acceptance probability. Hug is complemented by hop, which deliberately proposes jumps between contours and has an efficiency that degrades very slowly with increasing dimension. There are many parallels between hug and Hamiltonian Monte Carlo using a leapfrog integrator, including the order of the integration scheme, but hug is also able to make use of local Hessian information without requiring implicit numerical integration steps, and its performance is not terminally affected by unbounded gradients of the log-posterior. We test hug and hop empirically on a variety of toy targets and real statistical models, and find that it can, and often does, outperform Hamiltonian Monte Carlo.},
  archive      = {J_BIOMET},
  author       = {Ludkin, M and Sherlock, C},
  doi          = {10.1093/biomet/asac039},
  journal      = {Biometrika},
  number       = {2},
  pages        = {301-318},
  shortjournal = {Biometrika},
  title        = {Hug and hop: A discrete-time, nonreversible markov chain monte carlo algorithm},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On boosting the power of chatterjee’s rank correlation.
<em>BIOMET</em>, <em>110</em>(2), 283–299. (<a
href="https://doi.org/10.1093/biomet/asac048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ingenious approach of Chatterjee (2021) to estimate a measure of dependence first proposed by Dette et al. (2013) based on simple rank statistics has quickly caught attention. This measure of dependence has the appealing property of being between 0 and 1, and being 0 or 1 if and only if the corresponding pair of random variables is independent or one is a measurable function of the other almost surely. However, more recent studies ( Cao &amp; Bickel 2020 ; Shi et al. 2022b ) showed that independence tests based on Chatterjee’s rank correlation are unfortunately rate inefficient against various local alternatives and they call for variants. We answer this call by proposing an improvement to Chatterjee’s rank correlation that still consistently estimates the same dependence measure, but provably achieves near-parametric efficiency in testing against Gaussian rotation alternatives. This is possible by incorporating many right nearest neighbours in constructing the correlation coefficients. We thus overcome the ‘ only one disadvantage’ of Chatterjee’s rank correlation ( Chatterjee, 2021 , § 7).},
  archive      = {J_BIOMET},
  author       = {Lin, Z and Han, F},
  doi          = {10.1093/biomet/asac048},
  journal      = {Biometrika},
  number       = {2},
  pages        = {283-299},
  shortjournal = {Biometrika},
  title        = {On boosting the power of chatterjee’s rank correlation},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: “Optimal row-column designs.”
<em>BIOMET</em>, <em>110</em>(1), 281. (<a
href="https://doi.org/10.1093/biomet/asad003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asad003},
  journal      = {Biometrika},
  number       = {1},
  pages        = {281},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Optimal row-column designs’},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal minimax random designs for weighted least squares
estimators. <em>BIOMET</em>, <em>110</em>(1), 273–280. (<a
href="https://doi.org/10.1093/biomet/asac016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies an experimental design problem where the values of a predictor variable, denoted by |$x$|⁠ , are to be determined with the goal of estimating a function |$m(x)$|⁠ , which is observed with noise. A linear model is fitted to |$m(x)$|⁠ , but it is not assumed that the model is correctly specified. It follows that the quantity of interest is the best linear approximation of |$m(x)$|⁠ , which is denoted by |$\ell(x)$|⁠ . It is shown that in this framework the ordinary least squares estimator typically leads to an inconsistent estimation of |$\ell(x)$|⁠ , and rather weighted least squares should be considered. An asymptotic minimax criterion is formulated for this estimator, and a design that minimizes the criterion is constructed. An important feature of this problem is that the |$x$| values should be random, rather than fixed. Otherwise, the minimax risk is infinite. It is shown that the optimal random minimax design is different from its deterministic counterpart, which was studied previously, and a simulation study indicates that it generally performs better when |$m(x)$| is a quadratic or a cubic function. Another finding is that, when the variance of the noise goes to infinity, the random and deterministic minimax designs coincide. The results are illustrated for polynomial regression models and a generalization is given in the Supplementary Material .},
  archive      = {J_BIOMET},
  author       = {Azriel, D},
  doi          = {10.1093/biomet/asac016},
  journal      = {Biometrika},
  number       = {1},
  pages        = {273-280},
  shortjournal = {Biometrika},
  title        = {Optimal minimax random designs for weighted least squares estimators},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression of exchangeable relational arrays.
<em>BIOMET</em>, <em>110</em>(1), 265–272. (<a
href="https://doi.org/10.1093/biomet/asac031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational arrays represent measures of association between pairs of actors, often in varied contexts or over time. Trade flows between countries, financial transactions between individuals, contact frequencies between school children in classrooms and dynamic protein-protein interactions are all examples of relational arrays. Elements of a relational array are often modelled as a linear function of observable covariates. Uncertainty estimates for regression coefficient estimators, and ideally the coefficient estimators themselves, must account for dependence between elements of the array, e.g., relations involving the same actor. Existing estimators of standard errors that recognize such relational dependence rely on estimating extremely complex, heterogeneous structure across actors. This paper develops a new class of parsimonious coefficient and standard error estimators for regressions of relational arrays. We leverage an exchangeability assumption to derive standard error estimators that pool information across actors, and are substantially more accurate than existing estimators in a variety of settings. This exchangeability assumption is pervasive in network and array models in the statistics literature, but not previously considered when adjusting for dependence in a regression setting with relational data. We demonstrate improvements in inference theoretically, via a simulation study, and by analysis of a dataset involving international trade.},
  archive      = {J_BIOMET},
  author       = {Marrs, F W and Fosdick, B K and Mccormick, T H},
  doi          = {10.1093/biomet/asac031},
  journal      = {Biometrika},
  number       = {1},
  pages        = {265-272},
  shortjournal = {Biometrika},
  title        = {Regression of exchangeable relational arrays},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A simple and general debiased machine learning theorem with
finite-sample guarantees. <em>BIOMET</em>, <em>110</em>(1), 257–264. (<a
href="https://doi.org/10.1093/biomet/asac033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Debiased machine learning is a meta-algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may seek the confidence interval for a treatment effect estimated with a neural network. We present a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation and semiparametric efficiency by finite-sample arguments. The rate of convergence is |$n^{-1/2}$| for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.},
  archive      = {J_BIOMET},
  author       = {Chernozhukov, V and Newey, W K and Singh, R},
  doi          = {10.1093/biomet/asac033},
  journal      = {Biometrika},
  number       = {1},
  pages        = {257-264},
  shortjournal = {Biometrika},
  title        = {A simple and general debiased machine learning theorem with finite-sample guarantees},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seeded binary segmentation: A general methodology for fast
and optimal changepoint detection. <em>BIOMET</em>, <em>110</em>(1),
249–256. (<a href="https://doi.org/10.1093/biomet/asac052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose seeded binary segmentation for large-scale changepoint detection problems. We construct a deterministic set of background intervals, called seeded intervals, in which single changepoint candidates are searched for. The final selection of changepoints based on these candidates can be done in various ways, adapted to the problem at hand. The method is thus easy to adapt to many changepoint problems, ranging from univariate to high dimensional. Compared to recently popular random background intervals, seeded intervals lead to reproducibility and much faster computations. For the univariate Gaussian change in mean set-up, the methodology is shown to be asymptotically minimax optimal when paired with appropriate selection criteria. We demonstrate near-linear runtimes and competitive finite sample estimation performance. Furthermore, we illustrate the versatility of our method in high-dimensional settings.},
  archive      = {J_BIOMET},
  author       = {Kovács, S and Bühlmann, P and Li, H and Munk, A},
  doi          = {10.1093/biomet/asac052},
  journal      = {Biometrika},
  number       = {1},
  pages        = {249-256},
  shortjournal = {Biometrika},
  title        = {Seeded binary segmentation: A general methodology for fast and optimal changepoint detection},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Separable expansions for covariance estimation via the
partial inner product. <em>BIOMET</em>, <em>110</em>(1), 225–247. (<a
href="https://doi.org/10.1093/biomet/asac035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonparametric estimation of covariance lies at the heart of functional data analysis, whether for curve or surface-valued data. The case of a two-dimensional domain poses both statistical and computational challenges, which are typically alleviated by assuming separability. However, separability is often questionable, sometimes even demonstrably inadequate. We propose a framework for the analysis of covariance operators of random surfaces that generalizes separability while retaining its major advantages. Our approach is based on the expansion of the covariance into a series of separable terms. The expansion is valid for any covariance over a two-dimensional domain. Leveraging the key notion of the partial inner product, we generalize the power iteration method to general Hilbert spaces, and show how the aforementioned expansion can be efficiently constructed in practice at the level of the surface observations. Truncation of the expansion and retention of the leading terms automatically induces a nonparametric estimator of the covariance, whose parsimony is dictated by the truncation level. The resulting estimator can be calculated, stored and manipulated with little computational overhead relative to separability. Consistency and rates of convergence are derived under mild regularity assumptions, illustrating the trade-off between bias and variance regulated by the truncation level. The merits and practical performance of the proposed methodology are demonstrated in a comprehensive simulation study.},
  archive      = {J_BIOMET},
  author       = {Masak, T and Sarkar, S and Panaretos, V M},
  doi          = {10.1093/biomet/asac035},
  journal      = {Biometrika},
  number       = {1},
  pages        = {225-247},
  shortjournal = {Biometrika},
  title        = {Separable expansions for covariance estimation via the partial inner product},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Response best-subset selector for multivariate regression
with high-dimensional response variables. <em>BIOMET</em>,
<em>110</em>(1), 205–223. (<a
href="https://doi.org/10.1093/biomet/asac037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the statistical problem of response-variable selection with high-dimensional response variables and a diverging number of predictor variables with respect to the sample size in the framework of multivariate linear regression. A response best-subset selection model is proposed by introducing a 0-1 selection indicator for each response variable, and then a response best-subset selector is developed by introducing a separation parameter and a novel penalized least-squares function. The proposed procedure can perform response-variable selection and regression-coefficient estimation simultaneously, and the response best-subset selector has the property of model consistency under mild conditions for both fixed and diverging numbers of predictor variables. Also, consistency and asymptotic normality of regression-coefficient estimators are established for cases with a fixed dimension, and it is found that the Bonferroni test is a special response best-subset selector. Finite-sample simulations show that the response best-subset selector has strong advantages over existing competitors in terms of the Matthews correlation coefficient, a criterion that aims to balance accuracies for both true and false response variables. An analysis of real data demonstrates the effectiveness of the response best-subset selector in an application involving the identification of dosage-sensitive genes.},
  archive      = {J_BIOMET},
  author       = {Hu, Jianhua and Huang, Jian and Liu, Xiaoqian and Liu, Xu},
  doi          = {10.1093/biomet/asac037},
  journal      = {Biometrika},
  number       = {1},
  pages        = {205-223},
  shortjournal = {Biometrika},
  title        = {Response best-subset selector for multivariate regression with high-dimensional response variables},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linearized maximum rank correlation estimation.
<em>BIOMET</em>, <em>110</em>(1), 187–203. (<a
href="https://doi.org/10.1093/biomet/asac027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a linearized maximum rank correlation estimator for the single-index model. Unlike the existing maximum rank correlation and other rank-based methods, the proposed estimator has a closed-form expression, making it appealing in theory and computation. The proposed estimator is robust to outliers in the response and its construction does not need knowledge of the unknown link function or the error distribution. Under mild conditions, it is shown to be consistent and asymptotically normal when the predictors satisfy the linearity of the expectation assumption. A more general class of estimators is also studied. Inference procedures based on the plug-in rule or random weighting resampling are employed for variance estimation. The proposed method can be easily modified to accommodate censored data. It can also be extended to deal with high-dimensional data combined with a penalty function. Extensive simulation studies provide strong evidence that the proposed method works well in various practical situations. Its application is illustrated with the Beijing PM 2.5 dataset.},
  archive      = {J_BIOMET},
  author       = {Shen, Guohao and Chen, Kani and Huang, Jian and Lin, Yuanyuan},
  doi          = {10.1093/biomet/asac027},
  journal      = {Biometrika},
  number       = {1},
  pages        = {187-203},
  shortjournal = {Biometrika},
  title        = {Linearized maximum rank correlation estimation},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust differential abundance test in compositional data.
<em>BIOMET</em>, <em>110</em>(1), 169–185. (<a
href="https://doi.org/10.1093/biomet/asac029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential abundance tests for compositional data are essential and fundamental in various biomedical applications, such as single-cell, bulk RNA-seq and microbiome data analysis. However, because of the compositional constraint and the prevalence of zero counts in the data, differential abundance analysis on compositional data remains a complicated and unsolved statistical problem. This article proposes a new differential abundance test, the robust differential abundance test, to address these challenges. Compared with existing methods, the robust differential abundance test is simple and computationally efficient, is robust to prevalent zero counts in compositional datasets, can take the data’s compositional nature into account, and has a theoretical guarantee of controlling false discoveries in a general setting. Furthermore, in the presence of observed covariates, the robust differential abundance test can work with covariate-balancing techniques to remove potential confounding effects and draw reliable conclusions. The proposed test is applied to several numerical examples, and its merits are demonstrated using both simulated and real datasets.},
  archive      = {J_BIOMET},
  author       = {Wang, Shulei},
  doi          = {10.1093/biomet/asac029},
  journal      = {Biometrika},
  number       = {1},
  pages        = {169-185},
  shortjournal = {Biometrika},
  title        = {Robust differential abundance test in compositional data},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimax designs for causal effects in temporal experiments
with treatment habituation. <em>BIOMET</em>, <em>110</em>(1), 155–168.
(<a href="https://doi.org/10.1093/biomet/asac024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many modern settings, such as an online marketplace, randomized experiments need to be executed over multiple time periods. In such temporal experiments, it has been observed that the effects of an intervention on an experimental unit may be large when the unit is first exposed to it, but then it attenuates after repeated exposures. This is typically due to units’ habituation to the intervention, or some other form of learning, such as when users gradually start to ignore repeated mails sent by a promotional campaign. This paper proposes randomized designs for estimating causal effects in temporal experiments when habituation is present. We show that our designs are minimax optimal in a large class of practical designs. Our analysis is based on the randomization framework of causal inference, and imposes no parametric modelling assumptions on the outcomes.},
  archive      = {J_BIOMET},
  author       = {Basse, Guillaume W and Ding, Yi and Toulis, Panos},
  doi          = {10.1093/biomet/asac024},
  journal      = {Biometrika},
  number       = {1},
  pages        = {155-168},
  shortjournal = {Biometrika},
  title        = {Minimax designs for causal effects in temporal experiments with treatment habituation},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spherical clustering in detection of groups of concomitant
extremes. <em>BIOMET</em>, <em>110</em>(1), 135–153. (<a
href="https://doi.org/10.1093/biomet/asac020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing empirical evidence that spherical |$k$| -means clustering performs well at identifying groups of concomitant extremes in high dimensions, thereby leading to sparse models. We provide one of the first theoretical results supporting this approach, but also demonstrate some pitfalls. Furthermore, we show that an alternative cost function may be more appropriate for identifying concomitant extremes, and it results in a novel spherical |$k$| -principal-components clustering algorithm. Our main result establishes a broadly satisfied sufficient condition guaranteeing the success of this method, albeit in a rather basic setting. Finally, we illustrate in simulations that |$k$| -principal components clustering outperforms |$k$| -means clustering in the difficult case of weak asymptotic dependence within the groups.},
  archive      = {J_BIOMET},
  author       = {Fomichov, V and Ivanovs, J},
  doi          = {10.1093/biomet/asac020},
  journal      = {Biometrika},
  number       = {1},
  pages        = {135-153},
  shortjournal = {Biometrika},
  title        = {Spherical clustering in detection of groups of concomitant extremes},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data integration: Exploiting ratios of parameter estimates
from a reduced external model. <em>BIOMET</em>, <em>110</em>(1),
119–134. (<a href="https://doi.org/10.1093/biomet/asac022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the situation of estimating the parameters in a generalized linear prediction model, from an internal dataset, where the outcome variable |$Y$| is binary and there are two sets of covariates, |$X$| and |$Z$|⁠ . We have information from an external study that provides parameter estimates for a generalized linear model of |$Y$| on |$X$|⁠ . We propose a method that makes limited assumptions about the similarity of the distributions in the two study populations. The method involves orthogonalizing the |$Z$| variables and then borrowing information about the ratio of the coefficients from the external model. The method is justified based on a new result relating the parameters in a generalized linear model to the parameters in a generalized linear model with omitted covariates. The method is applicable if the regression coefficients in the |$Y$| given |$X$| model are similar in the two populations, up to an unknown scalar constant. This type of transportability between populations is something that can be checked from the available data. The asymptotic variance of the proposed method is derived. The method is evaluated in a simulation study and shown to gain efficiency compared to simple analysis of the internal dataset, and is robust compared to an alternative method of incorporating external information.},
  archive      = {J_BIOMET},
  author       = {Taylor, Jeremy M G and Choi, Kyuseong and Han, Peisong},
  doi          = {10.1093/biomet/asac022},
  journal      = {Biometrika},
  number       = {1},
  pages        = {119-134},
  shortjournal = {Biometrika},
  title        = {Data integration: Exploiting ratios of parameter estimates from a reduced external model},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instrumental variable estimation of the marginal structural
cox model for time-varying treatments. <em>BIOMET</em>, <em>110</em>(1),
101–118. (<a href="https://doi.org/10.1093/biomet/asab062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robins (1998) introduced marginal structural models, a general class of counterfactual models for the joint effects of time-varying treatments in complex longitudinal studies subject to time-varying confounding. Robins (1998) established the identification of marginal structural model parameters under a sequential randomization assumption, which rules out unmeasured confounding of treatment assignment over time. The marginal structural Cox model is one of the most popular marginal structural models for evaluating the causal effect of time-varying treatments on a censored failure time outcome. In this paper, we establish sufficient conditions for identification of marginal structural Cox model parameters with the aid of a time-varying instrumental variable, in the case where sequential randomization fails to hold due to unmeasured confounding. Our instrumental variable identification condition rules out any interaction between an unmeasured confounder and the instrumental variable in its additive effects on the treatment process, the longitudinal generalization of the identifying condition of Wang &amp; Tchetgen Tchetgen (2018) . We describe a large class of weighted estimating equations that give rise to consistent and asymptotically normal estimators of the marginal structural Cox model, thereby extending the standard inverse probability of treatment weighted estimation of marginal structural models to the instrumental variable setting. Our approach is illustrated via extensive simulation studies and an application to estimating the effect of community antiretroviral therapy coverage on HIV incidence.},
  archive      = {J_BIOMET},
  author       = {Cui, Y and Michael, H and Tanser, F and Tchetgen Tchetgen, E},
  doi          = {10.1093/biomet/asab062},
  journal      = {Biometrika},
  number       = {1},
  pages        = {101-118},
  shortjournal = {Biometrika},
  title        = {Instrumental variable estimation of the marginal structural cox model for time-varying treatments},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing generalized linear models with high-dimensional
nuisance parameters. <em>BIOMET</em>, <em>110</em>(1), 83–99. (<a
href="https://doi.org/10.1093/biomet/asac021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models often have high-dimensional nuisance parameters, as seen in applications such as testing gene-environment interactions or gene-gene interactions. In these scenarios, it is essential to test the significance of a high-dimensional subvector of the model’s coefficients. Although some existing methods can tackle this problem, they often rely on the bootstrap to approximate the asymptotic distribution of the test statistic, and are thus computationally expensive. Here, we propose a computationally efficient test with a closed-form limiting distribution, which allows the parameter being tested to be either sparse or dense. We show that, under certain regularity conditions, the Type-I error of the proposed method is asymptotically correct, and we establish its power under high-dimensional alternatives. Extensive simulations demonstrate the good performance of the proposed test and its robustness when certain sparsity assumptions are violated. We also apply the proposed method to Chinese famine sample data in order to show its performance when testing the significance of gene-environment interactions.},
  archive      = {J_BIOMET},
  author       = {Chen, Jinsong and Li, Quefeng and Chen, Hua Yun},
  doi          = {10.1093/biomet/asac021},
  journal      = {Biometrika},
  number       = {1},
  pages        = {83-99},
  shortjournal = {Biometrika},
  title        = {Testing generalized linear models with high-dimensional nuisance parameters},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On f-modelling-based empirical bayes estimation of
variances. <em>BIOMET</em>, <em>110</em>(1), 69–81. (<a
href="https://doi.org/10.1093/biomet/asac019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of empirical Bayes estimation of multiple variances when provided with sample variances. Assuming an arbitrary prior on the variances, we derive different versions of the Bayes estimators using different loss functions. For one particular loss function, the resulting Bayes estimator relies on the marginal cumulative distribution function of the sample variances only. When replacing it with the empirical distribution function, we obtain an empirical Bayes version called the |$F$| -modelling-based empirical Bayes estimator of variances. We provide theoretical properties of this estimator, and further demonstrate its advantages through extensive simulations and real data analysis.},
  archive      = {J_BIOMET},
  author       = {Kwon, Yeil and Zhao, Zhigen},
  doi          = {10.1093/biomet/asac019},
  journal      = {Biometrika},
  number       = {1},
  pages        = {69-81},
  shortjournal = {Biometrika},
  title        = {On F-modelling-based empirical bayes estimation of variances},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform inference in high-dimensional gaussian graphical
models. <em>BIOMET</em>, <em>110</em>(1), 51–68. (<a
href="https://doi.org/10.1093/biomet/asac030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models have become a popular tool for representing dependencies within large sets of variables and are crucial for representing causal structures. We provide results for uniform inference on high-dimensional graphical models, in which the number of target parameters |$d$| is potentially much larger than the sample size, under approximate sparsity. Our results highlight how graphical models can be estimated and recovered using modern machine learning methods in high-dimensional complex settings. To construct simultaneous confidence regions on many target parameters, it is crucial to have sufficiently fast estimation rates of the nuisance functions. In this context, we establish uniform estimation rates and sparsity guarantees for the square-root lasso estimator in a random design under approximate sparsity conditions. These might be of independent interest for related problems in high dimensions. We also demonstrate in a comprehensive simulation study that our procedure has good small sample properties in comparison to existing methods, and we present two empirical applications.},
  archive      = {J_BIOMET},
  author       = {Klaassen, S and Kueck, J and Spindler, M and Chernozhukov, V},
  doi          = {10.1093/biomet/asac030},
  journal      = {Biometrika},
  number       = {1},
  pages        = {51-68},
  shortjournal = {Biometrika},
  title        = {Uniform inference in high-dimensional gaussian graphical models},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Localized conformal prediction: A generalized inference
framework for conformal prediction. <em>BIOMET</em>, <em>110</em>(1),
33–50. (<a href="https://doi.org/10.1093/biomet/asac040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new inference framework called localized conformal prediction. It generalizes the framework of conformal prediction by offering a single-test-sample adaptive construction that emphasizes a local region around this test sample, and can be combined with different conformal scores. The proposed framework enjoys an assumption-free finite sample marginal coverage guarantee, and it also offers additional local coverage guarantees under suitable assumptions. We demonstrate how to change from conformal prediction to localized conformal prediction using several conformal scores, and we illustrate a potential gain via numerical examples.},
  archive      = {J_BIOMET},
  author       = {Guan, Leying},
  doi          = {10.1093/biomet/asac040},
  journal      = {Biometrika},
  number       = {1},
  pages        = {33-50},
  shortjournal = {Biometrika},
  title        = {Localized conformal prediction: A generalized inference framework for conformal prediction},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subsampling sparse graphons under minimal assumptions.
<em>BIOMET</em>, <em>110</em>(1), 15–32. (<a
href="https://doi.org/10.1093/biomet/asac032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the properties of two subsampling procedures for networks, vertex subsampling and |$p$| -subsampling, under the sparse graphon model. The consistency of network subsampling is demonstrated under the minimal assumptions of weak convergence of the corresponding network statistics and an expected subsample size growing to infinity more slowly than the number of vertices in the network. Furthermore, under appropriate sparsity conditions, we derive limiting distributions for the nonzero eigenvalues of an adjacency matrix under the sparse graphon model. Our weak convergence result implies the consistency of our subsampling procedures for eigenvalues under appropriate conditions.},
  archive      = {J_BIOMET},
  author       = {Lunde, Robert and Sarkar, Purnamrita},
  doi          = {10.1093/biomet/asac032},
  journal      = {Biometrika},
  number       = {1},
  pages        = {15-32},
  shortjournal = {Biometrika},
  title        = {Subsampling sparse graphons under minimal assumptions},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Propensity scores in the design of observational studies for
causal effects. <em>BIOMET</em>, <em>110</em>(1), 1–13. (<a
href="https://doi.org/10.1093/biomet/asac054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of any study, whether experimental or observational, that is intended to estimate the causal effects of a treatment condition relative to a control condition refers to those activities that precede any examination of outcome variables. As defined in our 1983 article ( Rosenbaum &amp; Rubin, 1983 ), the propensity score is the unit-level conditional probability of assignment to treatment versus control given the observed covariates; so the propensity score explicitly does not involve any outcome variables, in contrast to other summaries of variables sometimes used in observational studies. Balancing the distributions of covariates in the treatment and control groups by matching or balancing on the propensity score is therefore an aspect of the design of the observational study. In this invited comment on our 1983 article, we review the situation in the early 1980s and recall some apparent paradoxes that propensity scores helped to resolve. We demonstrate that it is possible to balance an enormous number of low-dimensional summaries of a high-dimensional covariate, even though it is generally impossible to match individuals closely for all the components of a high-dimensional covariate. In a sense, there is only one crucial observed covariate, the propensity score, and there is one crucial unobserved covariate, the principal unobserved covariate . The propensity score and the principal unobserved covariate are equal when treatment assignment is strongly ignorable, that is, unconfounded. Controlling for observed covariates is a prelude to the crucial step from association to causation, the step that addresses potential biases from unmeasured covariates. The design of an observational study also prepares for the step to causation: by selecting comparisons to increase the design sensitivity, by seeking opportunities to detect bias, by seeking mutually supportive evidence affected by different biases, by incorporating quasi-experimental devices such as multiple control groups, and by including the economist’s instruments. All of these considerations reflect the formal development of sensitivity analyses that were largely informal prior to the 1980s.},
  archive      = {J_BIOMET},
  author       = {Rosenbaum, P R and Rubin, D B},
  doi          = {10.1093/biomet/asac054},
  journal      = {Biometrika},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Biometrika},
  title        = {Propensity scores in the design of observational studies for causal effects},
  volume       = {110},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
