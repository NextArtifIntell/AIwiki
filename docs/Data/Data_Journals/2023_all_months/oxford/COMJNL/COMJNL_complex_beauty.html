<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMJNL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comjnl---217">COMJNL - 217</h2>
<ul>
<li><details>
<summary>
(2023). The path-structure connectivity of augmented k-ary n-cubes.
<em>COMJNL</em>, <em>66</em>(12), 3119–3128. (<a
href="https://doi.org/10.1093/comjnl/bxac197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For connected graphs |$G$| and |$H$|⁠ , the |$H$| -structure connectivity |$\kappa (G; H)$| (resp. |$H$| -substructure connectivity |$\kappa ^{s}(G; H)$|⁠ ) of |$G$| is the minimum cardinality of a set of subgraphs |$\mathcal{F}$| of |$G$| such that each is isomorphic to |$H$| (resp. to a connected subgraph of |$H$|⁠ ) so that |$G-\mathcal{F}$| is disconnected or singleton. In this paper, we consider |$P_t$| -structure connectivity and |$P_t$| -substructure connectivity of augmented |$k$| -ary |$n$| -cubes |$AQ_{n,k}$| for |$n\geq 2$|⁠ , |$k\geq 3$| and |$1\leq t\leq 4n-2$|⁠ . We obtain that |$\kappa (AQ_{n,k}; P_t)=\kappa ^s(AQ_{n,k}; P_t)=\frac{4n-2}{t}+1$| for |$t\mid 4n-2$|⁠ , |$t\nmid 2n-1$|⁠ , |$t&gt;6$|⁠ , |$n\geq 3$| and |$k\geq 4$|⁠ ; |$\kappa (AQ_{n,k}; P_t)=\kappa ^s(AQ_{n,k}; P_t)=\lceil \frac{4n-2}{t}\rceil $|⁠ , in other cases.},
  archive      = {J_COMJNL},
  author       = {Ba, Lina and Zhang, Yaxian and Zhang, Heping},
  doi          = {10.1093/comjnl/bxac197},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3119-3128},
  shortjournal = {Comput. J.},
  title        = {The path-structure connectivity of augmented k-ary n-cubes},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chinese RoBERTa distillation for emotion classification.
<em>COMJNL</em>, <em>66</em>(12), 3107–3118. (<a
href="https://doi.org/10.1093/comjnl/bxac153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through knowledge distillation method, a student model can imitate the output of a teacher model to improve its generalization ability without changing the computational complexity. However, in existing knowledge distillation research, the efficiency of knowledge transfer is still not satisfactory, especially from pre-trained language models (PTMs) like Robustly optimized BERT approach (RoBERTa) to another structure student model. To address this issue, this paper proposes a prediction framework (RTLSTM) for Chinese emotion classification based on knowledge distillation. In RTLSTM, a new triple loss strategy is proposed for training a student ‘BiLSTM’, which combines supervised learning, distillation and word vector losses. This strategy enables the student to learn more fully from a teacher model RoBERTa and retains 99\% of the teacher models’ language understanding capability. We carried out emotion classification experiments on five Chinese datasets to compare RTLSTM with baseline models. The experiment results show that RTLSTM outperforms the baseline models belonging to the RNN group in terms of prediction performance under similar numbers of parameters. Moreover, RTLSTM is superior to the PTMs group baseline models through 92\% fewer parameters and 83\% less prediction time under comparable prediction performance.},
  archive      = {J_COMJNL},
  author       = {Liu, Pingshan and Lv, Shuyue},
  doi          = {10.1093/comjnl/bxac153},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3107-3118},
  shortjournal = {Comput. J.},
  title        = {Chinese RoBERTa distillation for emotion classification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strong anonymous communication system based on segment
routing over SDN. <em>COMJNL</em>, <em>66</em>(12), 3092–3106. (<a
href="https://doi.org/10.1093/comjnl/bxac151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networking (SDN) has brought a novel networking paradigm for achieving the goal of anonymous communication. In this paper, we propose a strong anonymous communication scheme based on segment routing (STAR), which does not need all forwarding devices to support OpenFlow protocol and is easy to deploy in SDNs. In STAR, by expanding the packet format of segment routing via Sphinx protocol, the required routing information is encrypted and hidden in the packet header to prevent the enemy from associating with the communication parties. Moreover, to avoid the adversary connecting the communication parties based on load information, the trust controller is employed as an auxiliary node to negotiate the symmetric key between the communication parties for encrypting the packet load. The theoretical analysis shows that, when the adversary compromises multiple intermediate nodes, a low correct linking probability and effective attack resistance are obtained, which proves the weak correlation and stronger anonymity of STAR. Besides, evaluation results confirm that, compared with existing anonymous systems, the proposed STAR can ensure stronger anonymity and higher throughput (83.7\% for that of no anonymity) by just introducing very small communication latency (microseconds) and resource cost. Especially for the large-volume data in large-scale SDNs, the advantages will be more obvious.},
  archive      = {J_COMJNL},
  author       = {Feng, Li and Ni, Xiaoling and Ling, Zhen and Wang, Liangmin},
  doi          = {10.1093/comjnl/bxac151},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3092-3106},
  shortjournal = {Comput. J.},
  title        = {Strong anonymous communication system based on segment routing over SDN},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential-aided preimage attacks on round-reduced keccak.
<em>COMJNL</em>, <em>66</em>(12), 3069–3091. (<a
href="https://doi.org/10.1093/comjnl/bxac150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At FSE 2008, Leurent introduced the preimage attack on MD4 by exploiting differential trails. In this paper, we apply the differential-aided preimage attack to Keccak with the message modification techniques. Instead of directly finding the preimage, we exploit differential characteristics to modify the messages, so that the differences of their hashing values and the changes of given target can be controlled. By adding some constraints, a trail can be used to change one bit at a time and reduce the time complexity by a factor of 2. When the number of rounds increases, we introduce two-stage modification techniques to satisfy part of constraints as well. In order to solve other constraints, we also combine the linear-structure technique and accordingly give a preimage attack on 5-round Keccak[ |$r=1440,c=160,l=80$| ].},
  archive      = {J_COMJNL},
  author       = {Wei, Congming and Dong, Xiaoyang and Meier, Willi and Qin, Lingyue and Fu, Ximing},
  doi          = {10.1093/comjnl/bxac150},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3069-3091},
  shortjournal = {Comput. J.},
  title        = {Differential-aided preimage attacks on round-reduced keccak},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic demirci–selçuk meet-in-the-middle attack on SIMON.
<em>COMJNL</em>, <em>66</em>(12), 3052–3068. (<a
href="https://doi.org/10.1093/comjnl/bxac149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demirci–Selçuk meet-in-the-middle (DS-MITM) attack is an effective method for cryptanalysis. As far as we know, the published automatic results of DS-MITM attack are all for byte-oriented ciphers. In this article, we first propose the automatic analysis method of DS-MITM attack for bit-oriented ciphers based on constraint programming, which is integrated with key-bridging technique. Based on the automatic modeling method, we propose the first result of DS-MITM attack on SIMON, which is a family of lightweight block ciphers proposed by the National Security Agency (NSA) in 2013.},
  archive      = {J_COMJNL},
  author       = {Lv, Yin and Shi, Danping and Guo, Yi and Chen, Qiu and Hu, Lei and Guo, Zihui},
  doi          = {10.1093/comjnl/bxac149},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3052-3068},
  shortjournal = {Comput. J.},
  title        = {Automatic Demirci–Selçuk meet-in-the-middle attack on SIMON},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital image anti-forensic model using exponential chaotic
biogeography-based optimization algorithm. <em>COMJNL</em>,
<em>66</em>(12), 3038–3051. (<a
href="https://doi.org/10.1093/comjnl/bxac148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The innovation in visual imagery has led to massive growth in technologies, wherein digital cameras are obtainable at affordable prices. Thus, the digital images are easily captured and processed due to the internet connectivity. On the other hand, the development of strong image editing software facilitated the forgers to manipulate the accessible images with different tampering operations. Several techniques are devised for detecting the forgeries. Accordingly, this paper devises an anti-forensic model, namely Exponentially Weighted Moving Average-Chaotic Biography Based Optimization (E-CBBO) for joint photographic experts group (JPEG) compression to mitigate the forgeries occurred on the internet while transmitting data. The proposed E-CBBO is designed by integrating the properties of the exponentially weighted moving average (EWMA) with the chaotic biography-based optimization (CBBO). The suggested JPEG anti-forensic model is used to eliminate JPEG compression artifacts through the use of unique deblocking, smoothing with dither and decalibration operations. In addition, the goal is to balance visual quality and forensic undetectability when compressing the JPEG image. The fitness function is developed using the structural similarity index (SSIM), universal image quality index (UIQI) and histogram deviation parameters. With a maximum accuracy of 93.2\%, a minimal MSE of 0.110, a maximum SSIM of 0.932 and a maximum UIQI of 0.890, the suggested E-CBBO beat existing approaches.},
  archive      = {J_COMJNL},
  author       = {Sudhakar, Dr R and Rao, Dr P V Venkateswara},
  doi          = {10.1093/comjnl/bxac148},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3038-3051},
  shortjournal = {Comput. J.},
  title        = {Digital image anti-forensic model using exponential chaotic biogeography-based optimization algorithm},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rainbow tables: How far can CPU go? <em>COMJNL</em>,
<em>66</em>(12), 3029–3037. (<a
href="https://doi.org/10.1093/comjnl/bxac147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainbow tables are techniques commonly used in computer security to invert one-way functions, for instance to crack passwords, when the domain of definition is reasonably sized. This article explores the limit on the problem size that can be treated by rainbow tables when the precomputation and the attack phases are both CPU-driven. We conclude that the bottleneck is no longer the memory as it may have been and the precomputation phase seems to have been underestimated so far. We offer a comparison of what can be done on different environments depending on the needs and available computing power of the users.},
  archive      = {J_COMJNL},
  author       = {Avoine, Gildas and Carpent, Xavier and Leblanc-Albarel, Diane},
  doi          = {10.1093/comjnl/bxac147},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3029-3037},
  shortjournal = {Comput. J.},
  title        = {Rainbow tables: How far can CPU go?},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive MAC protocol based on time-domain interference
alignment for UWANs. <em>COMJNL</em>, <em>66</em>(12), 3015–3028. (<a
href="https://doi.org/10.1093/comjnl/bxac145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial and temporal uncertainty caused by large propagation delays is a fundamental feature of Underwater Acoustic Networks (UWANs), which seriously affects the performance of the UWANs and also brings challenges to the design of MAC protocols. In this paper, we develop an adaptive MAC protocol based on deep reinforcement learning for UWANs, called ARL-MAC protocol, to intelligently allocate time slots for nodes. Firstly, we design a reward mechanism based on the idea of Time-Domain Interference Alignment (TDIA). We determine the reward according to the combination of the node action and the feedback corresponding to the action. Then, we propose a flexible training mechanism to deal with the ever-changing underwater environment, which improves the fairness of time slot allocation. In addition, we introduce the Deep Recurrent Q-Network (DRQN) algorithm to solve the partially observable information issue. Finally, we evaluate the ARL-MAC protocol with the different number of nodes and changing network environment. Simulation results reveal that the ARL-MAC protocol outperforms other MAC protocols for UWANs in terms of throughput, collision rate and service fairness.},
  archive      = {J_COMJNL},
  author       = {Zhao, Nan and Yao, Nianmin and Gao, Zhenguo},
  doi          = {10.1093/comjnl/bxac145},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3015-3028},
  shortjournal = {Comput. J.},
  title        = {An adaptive MAC protocol based on time-domain interference alignment for UWANs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RSCOEWR: Radical-based sentiment classification of online
education website reviews. <em>COMJNL</em>, <em>66</em>(12), 3000–3014.
(<a href="https://doi.org/10.1093/comjnl/bxac144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online education is becoming more and more popular with the development of the Internet. In particular, due to the COVID-19 pandemic, many countries around the world are increasing the popularity of online education, which makes the research on sentiment classification of course reviews of online education websites an important research direction in natural language processing tasks. Traditional sentiment classification models are mostly based on English. Unlike English, Chinese characters are based on pictograms. Radicals of Chinese characters can also express certain semantics, and characters with the same radical often have similar meanings. Therefore, RSCOEWR, a word-level and radical-level based sentiment classification model for course reviews of Chinese online education websites is proposed, which solves the problem of data sparsity of reviews by feature extraction of multiple dimensions. In addition, a deep learning model based on CNN, BILSTM, BIGRU and Attention is constructed to solve the problem of high dimension and assigning the same attention to context of traditional sentiment classification model. Extensive comparative experiment results show that RSCOEWR outperforms the state-of-the-art sentiment classification models, and the experimental results on public Chinese sentiment classification datasets prove the generalization ability of RSCOEWR.},
  archive      = {J_COMJNL},
  author       = {Li, Jie and Sun, GuoYing},
  doi          = {10.1093/comjnl/bxac144},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {3000-3014},
  shortjournal = {Comput. J.},
  title        = {RSCOEWR: Radical-based sentiment classification of online education website reviews},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disjunctive total domination in harary graphs.
<em>COMJNL</em>, <em>66</em>(12), 2990–2999. (<a
href="https://doi.org/10.1093/comjnl/bxac142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let |$ G $| be a graph. A set |$ S $| of vertices in |$ G $| is a disjunctive total dominating set of |$ G $| if every vertex is adjacent to a vertex of |$ S $| or has at least two vertices in |$ S $| at distance two from it. The disjunctive total domination number, |$ \gamma _t^d(G) $|⁠ , is the minimum cardinality of such a set. In this paper, we determine disjunctive total domination number of Harary graph |$ H_{k,n} $| for all |$ k $| and |$ n $|⁠ .},
  archive      = {J_COMJNL},
  author       = {Çiftçi, Canan and Aytaç, Vecḋ},
  doi          = {10.1093/comjnl/bxac142},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2990-2999},
  shortjournal = {Comput. J.},
  title        = {Disjunctive total domination in harary graphs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shorter linkable ring signature based on middle-product
learning with errors problem. <em>COMJNL</em>, <em>66</em>(12),
2974–2989. (<a href="https://doi.org/10.1093/comjnl/bxac141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DualRing is a novel generic construction introduced by Yuen et al. (CRYPTO’21), which can transform a special kind of (Type-T*) canonical identification scheme to a ring signature scheme. Compared with the classical approaches, this method can get a shorter signature. In this paper, we construct a new middle-product learning with errors (MPLWE)-based ring signature scheme by using this framework. Specifically, we propose a new MPLWE-based identification scheme, which is compatible with the DualRing, then we obtain a ring signature scheme by using DualRing framework. We also show how to achieve linkability from this ring signature by using a collision resistant hash function. In the end, we provide available parameter options for our (linkable) ring signature scheme. Under these parameters, the signature size of our linkable ring signature is |$2-40 \times $| shorter (depending on the ring size) than the previous MPLWE-based scheme by Das et al. (Africacrypt’19).},
  archive      = {J_COMJNL},
  author       = {Lin, Hao and Sun, Shi-Feng and Wang, Mingqiang and Liu, Joseph K and Wang, Weijia},
  doi          = {10.1093/comjnl/bxac141},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2974-2989},
  shortjournal = {Comput. J.},
  title        = {Shorter linkable ring signature based on middle-product learning with errors problem},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lattice-based redactable signature scheme using
cryptographic accumulators for trees. <em>COMJNL</em>, <em>66</em>(12),
2961–2973. (<a href="https://doi.org/10.1093/comjnl/bxac140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redactable signatures allow the signature holder to remove admissible data blocks in the signed data while generating valid signatures about different redacted data without communicating with the primary signer. Now, this sort of signature has attracted widespread attention due to its many application scenarios such as electronic medical records, smart grids and XML files. However, there are rarely redactable signature schemes that can resist quantum attacks so far. In the wake of quantum calculation era, it is essential to blossom more quantum-resistant redactable signatures for different data structures. Moreover, it is popular to use accumulators to design redactable signature schemes. Unfortunately, the existing accumulators do not support tree data structures. Therefore, this paper first gives the definition of accumulator schemes for trees, and designs a lattice-based accumulator scheme for trees. Our accumulator scheme features shorter accumulator values and a faster witness generation algorithm than existing lattice trapdoor accumulators. Second, this paper resorts to approximate trapdoors and the preimage sampleable technique, and presents a lattice-based redactable signature scheme for trees using our accumulator scheme. Meanwhile, this scheme fulfills unforgeability, transparency and privacy under adaptive chosen-message attacks. Furthermore, the experiment results show that the redactable signature scheme meets actual revision requirements well.},
  archive      = {J_COMJNL},
  author       = {Zhao, Yong and Yang, Shaojun and Wu, Wei and Huang, Xinyi},
  doi          = {10.1093/comjnl/bxac140},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2961-2973},
  shortjournal = {Comput. J.},
  title        = {A lattice-based redactable signature scheme using cryptographic accumulators for trees},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online/offline attribute-based boolean keyword search for
internet of things. <em>COMJNL</em>, <em>66</em>(12), 2948–2960. (<a
href="https://doi.org/10.1093/comjnl/bxac139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet of Things (IoT) and cloud computing, a large amount of IoT data has been stored and shared in cloud servers. However, sensitive IoT data may be leaked by untrusted cloud servers, which is a key problem hindering the development of IoT-cloud systems. To solve this problem, searchable encryption has been presented, which enables IoT devices to encrypt the collected data before uploading them to the cloud and make retrievals over the encrypted data by keyword queries. In this paper, we will study attribute-based keyword search (ABKS), which relieves us from cumbersome key management and provides a fine-grained access control. However, since the existing ABKS schemes are all constructed by bilinear pairings, which incurs high computational costs, they are not suitable for resource-constrained IoT devices. In this article, we propose an online/offline attribute-based boolean keyword search scheme, which can significantly reduce the online computing costs of IoT devices. In addition, our scheme can support boolean keyword search for data users, which has more flexible keyword expressivity than the existing online/offline ABKS schemes. We simulate our scheme and the experimental results show that it is both efficient and practical for the real IoT-cloud systems.},
  archive      = {J_COMJNL},
  author       = {Yao, Jiahui and Xu, Lingling},
  doi          = {10.1093/comjnl/bxac139},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2948-2960},
  shortjournal = {Comput. J.},
  title        = {Online/Offline attribute-based boolean keyword search for internet of things},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPGA-CPU architecture accelerated regular expression
matching with fast preprocessing. <em>COMJNL</em>, <em>66</em>(12),
2928–2947. (<a href="https://doi.org/10.1093/comjnl/bxac138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular Expression Matching (REM) is the core of Deep Packet Inspection (DPI), which is important for various network security applications. The burgeoning Software Defined Network and Network Function Virtualization technologies make the network evolve more dynamic, which brings serious challenges for DPI engines to achieve high matching performance with fast rule-set update capability. To meet these challenges, this paper proposes a heterogeneous Field Programmable Gate Array (FPGA)-Central Processing Unit (CPU) architecture to accelerate Deterministic Finite Automaton (DFA)-based REM with high preprocessing performance. Firstly, a novel regex decomposition technique is proposed to solve the DFA state explosion problem, which splits each regex into one prefix and several postfixes. Secondly, heterogeneous architecture is presented to collaboratively handle regex matching, in which prefixes are matched in parallel in an FPGA and postfixes are matched in a CPU. To further improve the matching performance, several well-designed DFA compression techniques and regex decomposition optimizations are proposed. Our design has been implemented in a DPI prototype employing a medium-end FPGA. Extensive experiments are conducted to evaluate the performance. Results reveal that our proposed architecture achieves 6.33 Gbps matching throughput on the Snort rule-set (v3.0), which is close to state-of-the-art FPGA NFA-based schemes. However, the rule-set preprocessing time is significantly reduced to &lt;7 minutes, compared with up to several hours of FPGA NFA-based countermeasures.},
  archive      = {J_COMJNL},
  author       = {Zhong, Jincheng and Chen, Shuhui and Han, Biao},
  doi          = {10.1093/comjnl/bxac138},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2928-2947},
  shortjournal = {Comput. J.},
  title        = {FPGA-CPU architecture accelerated regular expression matching with fast preprocessing},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The generalized 3-connectivity of the folded hypercube FQn.
<em>COMJNL</em>, <em>66</em>(12), 2921–2927. (<a
href="https://doi.org/10.1093/comjnl/bxac137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized |$k$| -connectivity of a graph |$G$|⁠ , denoted by |$\kappa _k(G)$|⁠ , is a generalization of the traditional connectivity and can serve for measuring the capability of a network |$G$| to connect any |$k$| vertices in |$G$|⁠ . It is well known that the generalized |$k$| -connectivity is an important indicator for measuring the fault tolerance and reliability of interconnection networks. The |$n$| -dimensional folded hypercube |$FQ_n$|⁠ , which is an important variation of hypercubes, can be obtained from the |$n$| -dimensional hypercube |$Q_n$| by adding an edge between any pair of vertices with complementary addresses. In this paper, we show that |$\kappa _3(FQ_n)=n$| for |$n\ge 2$|⁠ , that is, for any three vertices in |$FQ_n$|⁠ , there exist |$n$| internally disjoint trees connecting them.},
  archive      = {J_COMJNL},
  author       = {Wang, Jing and Li, Fangmin},
  doi          = {10.1093/comjnl/bxac137},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2921-2927},
  shortjournal = {Comput. J.},
  title        = {The generalized 3-connectivity of the folded hypercube FQn},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-based congestion-aware datacenter load balancing with
smart probing. <em>COMJNL</em>, <em>66</em>(12), 2908–2920. (<a
href="https://doi.org/10.1093/comjnl/bxac135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data center networks employ multipath topologies for delivering high bisection bandwidth. Load balancing through these multiple paths reduces latency and improves throughput. Network fabric-based schemes need customized hardware and cannot be implemented on commodity hardware. The existing end-host-based solutions have shown performance better than equal-cost multi-path (ECMP) but they use active probing and uneven congestion signal (e.g. coarse-grained RTT or ECN) that degrades performance especially in asymmetric topologies. Therefore, most of these end-host solutions cannot accurately sense congestion. End-to-end latency can effectively provide accurate path congestion information and smart probing with the help of congestion window can reduce network overhead. In this paper, we propose an edge-based congestion-aware load-balancing solution with smart probing. We combine ECN and end-to-end latency to get more accurate congestion metric and with probabilistic approach we avoid congestion at the best available path. Furthermore, we employ congestion window trend to perform smart probing that will reduce network overhead by performing probing only when required. The proposed scheme is implemented in network simulator ns-3 and flow completion time is taken as the key performance metric. The results show that the proposed scheme not only outperformed ECMP, Clove and LetFlow but also improved the performance over Hermes and CONGA at least 10–30\% by employing (i) an accurate congestion signal that effectively detects congestion and asymmetries, and (ii) smart probing to reduce network overhead.},
  archive      = {J_COMJNL},
  author       = {Bhutta, Areeb Ahmed and Ahmed, Hasnain},
  doi          = {10.1093/comjnl/bxac135},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2908-2920},
  shortjournal = {Comput. J.},
  title        = {Edge-based congestion-aware datacenter load balancing with smart probing},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CR loss: Improving biometric using ClassRoom learning
approach. <em>COMJNL</em>, <em>66</em>(12), 2897–2907. (<a
href="https://doi.org/10.1093/comjnl/bxac134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important factors in deep feature learning is their loss function design which highly influences the network performance. In this paper, we proposed a classroom (CR) learning approach along with arcface loss for contactless palmprint recognition to obtain high-level discriminative features without any extra load made to the network architecture. CR loss allows the network to learn the best possible feature representations for palmprint images. To validate our concept, we performed extensive experimental evaluations on various popular benchmark palmprint databases where our methods outperform the state-of-the-art methods. We also introduced a challenging contactless palmprint database called Harbin Institute of Technology-Network &amp; Information Security Research Center contactless palmprint database version 1.0 (HIT-NIST-V1), as a new contribution to this domain. The result proves that the proposed CR loss consistently outperforms the SOTA methods for all the considered databases and especially for HIT-NIST-V1.},
  archive      = {J_COMJNL},
  author       = {Prasad, Shitala and Chai, Tingting and Li, Jiahui and Zhang, Zhaoxin},
  doi          = {10.1093/comjnl/bxac134},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2897-2907},
  shortjournal = {Comput. J.},
  title        = {CR loss: Improving biometric using ClassRoom learning approach},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dueling DQN-based computational offloading method in
MEC-enabled IIoT network. <em>COMJNL</em>, <em>66</em>(12), 2887–2896.
(<a href="https://doi.org/10.1093/comjnl/bxac133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial Internet of Things (IIoT) is a promising mechanism of Industry 4.0. Mobile edge computing (MEC) is an emerging mechanism that is an enabler for IoT applications, which applies computation offloading mechanism to offload workload from IoT devices to MEC servers to enhance computing quality. This paper addresses the computation offloading problem under a heterogeneously loaded MEC-enabled IIoT network, which specifically applies one-hop offloading for devices and task queue for devices and servers. We first formulate our computation offloading problem as a Markov Decision Process, then design a dueling deep Q-network-based computation offloading method for better optimizing the value function for specific state and advantage function for specific action. Experimental results verify that our scheme reduces more energy consumption (30\%), latency (50\%) and task dropped rate (50\%) of IIoT devices, compared with other popular methods.},
  archive      = {J_COMJNL},
  author       = {Hsu, Ching-Kuo},
  doi          = {10.1093/comjnl/bxac133},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2887-2896},
  shortjournal = {Comput. J.},
  title        = {A dueling DQN-based computational offloading method in MEC-enabled IIoT network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CCESHP: Causal consistency model of edge storage based on
hash ring and partial geo-replication. <em>COMJNL</em>, <em>66</em>(12),
2874–2886. (<a href="https://doi.org/10.1093/comjnl/bxac129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, most of the causal consistency models that rely on cloud storage have problems such as high operation delays and large metadata overhead. To solve these problems, this paper proposes a causal consistency model for edge storage based on hash rings, CCESHP. The proposed model uses two hashes to map the keys and servers on the hash ring for grouping and stores a subset of the complete data set in a replica node located at the edge of the network, thereby realizing a partial geographic replication strategy in the edge storage environment. Operation latency will be reduced since the edge replica is closer to the client. At the same time, it also generates and maintains a combined timestamp to capture causality according to the update type, which can keep the amount of managed metadata in a relatively stable and low state, reduce the overhead of system management metadata, and improve system throughput. The experimental evaluation results under different workloads show that the model has better performance in throughput and operation delay when compared with the existing causal consistency model.},
  archive      = {J_COMJNL},
  author       = {Tian, Junfeng and Jia, Haoyi and Bai, Wenqing},
  doi          = {10.1093/comjnl/bxac129},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2874-2886},
  shortjournal = {Comput. J.},
  title        = {CCESHP: Causal consistency model of edge storage based on hash ring and partial geo-replication},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Threshold homomorphic encryption from provably secure NTRU.
<em>COMJNL</em>, <em>66</em>(12), 2861–2873. (<a
href="https://doi.org/10.1093/comjnl/bxac126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Encryption (HE) supports computation on encrypted data without the need to decrypt, enabling secure outsourcing of computing to an untrusted cloud. Motivated by application scenarios where private information is offered by different data owners, Multi-Key Homomorphic Encryption (MKHE) and Threshold Homomorphic Encryption (ThHE) were proposed. Unlike MKHE, ThHE schemes do not require expensive ciphertext extension procedures and are therefore as efficient as their underlying single-key HE schemes. In this work, we propose a novel NTRU-type ThHE scheme which caters to the computation scenarios with pre-defined participants. In addition to inheriting the simplicity of NTRU scheme, our construction has no expensive relinearization and correspondingly no costly evaluation keys. Controlling noise to make it increase linearly and then using a wide key distribution, our scheme is immune to the subfield lattice attacks and its security follows from the hardness of the standard R-LWE problem. Finally, based on the {0,1}-linear secret sharing and noise flooding techniques, we design a single round distributed threshold decryption protocol, where the decryption is able to be completed even when only given a subset (say |$t$| -out-of- |$k$|⁠ ) of partial decryptions. To the best of our knowledge, our construction is the first NTRU-type ThHE scheme.},
  archive      = {J_COMJNL},
  author       = {Xu, Kexin and Hong Meng Tan, Benjamin and Wang, Li-Ping and Mi Mi Aung, Khin and Wang, Huaxiong},
  doi          = {10.1093/comjnl/bxac126},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {2861-2873},
  shortjournal = {Comput. J.},
  title        = {Threshold homomorphic encryption from provably secure NTRU},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust image hashing combining 3D space contour and vector
angle features. <em>COMJNL</em>, <em>66</em>(11), 2844–2859. (<a
href="https://doi.org/10.1093/comjnl/bxac127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an image hashing scheme combining 3D space contour (TDSC) features with vector angle (VA) features is proposed. The proposed algorithm extracts the 3D contours of the local component variation features of the image and the expression changes of the local component of the image in the form of a 3D VA to improve the performance. First, the gray component of the color image is used to construct a 3D space and the contour change features of the local component of the gray image are extracted using multi-perspectives. Then, the opposite color component and the brightness component Y of the YCbCr color space are extracted from the input image. The angular features of several image components are, respectively, extracted in the 3D space. Finally, the TDSC features are combined with the VA features to obtain image hashing. The simulations demonstrate and validate that the proposed image hashing scheme not only has better classification performance compared with the other image hashing techniques but is also equipped with the performance of tamper localization.},
  archive      = {J_COMJNL},
  author       = {Liu, Shuai and Zhao, Yan},
  doi          = {10.1093/comjnl/bxac127},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2844-2859},
  shortjournal = {Comput. J.},
  title        = {Robust image hashing combining 3D space contour and vector angle features},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Double-lead content search and producer location prediction
scheme for producer mobility in named data networking. <em>COMJNL</em>,
<em>66</em>(11), 2825–2843. (<a
href="https://doi.org/10.1093/comjnl/bxac125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Named Data Network (NDN) has become a popular network architecture because of high resource utilization, strong security and high transmission efficiency. Meanwhile, mobile multimedia communication has become the mainstream with the popularization and application of smart terminals. Most of the research on NDN mobility is focused on consumer mobility without taking producer mobility into account. To solve the delay and high cost carried by producer moving, we propose a Double-Lead content search algorithm based on neighbor and proxy and a location prediction algorithm based on traffic features. We use a neural network model to predict a new location of producers and calculate route before switching, which can save the rerouting latency in advance when predicting accurately. In a few cases of inaccurate predictions, we select different search methods according to the distance of the producer’s movement, to complete the Double-Lead search between the producer and the consumer. Experimental results show that DLPNDN can reduce the delay and traffic overhead well in NDN when the producer moves.},
  archive      = {J_COMJNL},
  author       = {Rui, Lanlan and Dai, Shiyue and Gao, Zhipeng and Qiu, Xuesong and Chen, Xingyu},
  doi          = {10.1093/comjnl/bxac125},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2825-2843},
  shortjournal = {Comput. J.},
  title        = {Double-lead content search and producer location prediction scheme for producer mobility in named data networking},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel construction of certificateless aggregate signature
scheme for healthcare wireless medical sensor networks. <em>COMJNL</em>,
<em>66</em>(11), 2810–2824. (<a
href="https://doi.org/10.1093/comjnl/bxac123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure privacy and security of healthcare wireless medical sensor networks (HWMSNs), several concrete constructions of efficient certificateless aggregate signature (CLAS) scheme without bilinear pairing were proposed in the last few years. However, many previous constructions of CLAS scheme were found to be impractical, which either fail to meet the claimed security or contain design flaws. For example, in some of the previous proposals, any adversary can forge a valid signature on any new message. In this paper, we first demonstrate some security issues and design flaws in the previous proposals of CLAS scheme. As follows, to further address the above deficiencies, a new construction of CLAS scheme with improved security is presented, and the formal security proof is given using Forking Lemma in the random oracle model, assuming that the discrete logarithm problem is hard. Compared with the previous CLAS schemes, our construction has similar computational costs, and it provides better security guarantees. Therefore, compared with the existing solutions, our proposal with strong security and high computational efficiency is more suitable for use in HWMSNs.},
  archive      = {J_COMJNL},
  author       = {Qiao, Zirui and Yang, Qiliang and Zhou, Yanwei and Yang, Bo and Zhang, Mingwu},
  doi          = {10.1093/comjnl/bxac123},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2810-2824},
  shortjournal = {Comput. J.},
  title        = {A novel construction of certificateless aggregate signature scheme for healthcare wireless medical sensor networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical implementation of encoding range top-2 queries.
<em>COMJNL</em>, <em>66</em>(11), 2794–2809. (<a
href="https://doi.org/10.1093/comjnl/bxac122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design a practical variant of an encoding for range Top-2 query ( RT2Q ) and evaluate its performance. Given an array |$A[1,n]$| of |$n$| elements from a total order, the range Top-2 encoding problem is to construct a data structure that answers |${\textsf{RT2Q}}{}$|⁠ , which returns the positions of the first and second largest elements within a given range of |$A$|⁠ , without accessing the array |$A$| at query time. We design the following two implementations: (i) an implementation based on an alternative representation of Davoodi et a l.’s [Phil. Trans. Royal Soc. A, 2016] data structure, which supports queries efficiently. Experimental results show that our implementation is efficient in practice and gives improved time-space trade-offs compared with the indexing data structures (which keep the original array |$A$| as part of the data structure) for range maximum queries. (ii) Another implementation based on Jo et al .’s |${\textsf{RT2Q}}{}$| encoding on |$2 \times n$| array [CPM, 2016], which can be constructed in |$O(n)$| time. We compare our encoding with Gawrychowski and Nicholson’s optimal encoding [ICALP, 2015] and show that in most cases, our encoding shows faster construction time while using a competitive space in practice.},
  archive      = {J_COMJNL},
  author       = {Park, Wooyoung and Jo, Seungbum and Rao Satti, Srinivasa},
  doi          = {10.1093/comjnl/bxac122},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2794-2809},
  shortjournal = {Comput. J.},
  title        = {Practical implementation of encoding range top-2 queries},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Character-based value factorization for MADRL.
<em>COMJNL</em>, <em>66</em>(11), 2782–2793. (<a
href="https://doi.org/10.1093/comjnl/bxac121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Value factorization is a popular method for cooperative multi-agent deep reinforcement learning. In this method, agents generally have the same ability and rely only on individual value function to select actions, which is calculated from total environment reward. It ignores the impact of individual characteristics of heterogeneous agents on actions selection, which leads to the lack of pertinence during training and the increase of difficulty in learning effective policies. In order to stimulate individual awareness of heterogeneous agents and improve their learning efficiency and stability, we propose a novel value factorization method based on Personality Characteristics , PCQMIX, which assigns personality characteristics to each agent and takes them as internal rewards to train agents. As a result, PCQMIX can generate heterogeneous agents with specific personality characteristics suitable for specific scenarios. Experiments show that PCQMIX generates agents with stable personality characteristics and outperforms all baselines in multiple scenarios of the StarCraft II micromanagement task.},
  archive      = {J_COMJNL},
  author       = {Liqin, Xiong and Lei, Cao and Xiliang, Chen and Jun, Lai and Xijian, Luo},
  doi          = {10.1093/comjnl/bxac121},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2782-2793},
  shortjournal = {Comput. J.},
  title        = {Character-based value factorization for MADRL},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New obfuscation scheme for conjunctions. <em>COMJNL</em>,
<em>66</em>(11), 2773–2781. (<a
href="https://doi.org/10.1093/comjnl/bxac120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been renewed interest in conjunction obfuscations. A conjunction, which is called pattern matching with wildcards sometimes, is associated with a pattern |$\mathsf{pat}\in {0,1,*}^n$| where * is a wildcard. It accepts if and only if the input bits are the same as the pattern at all non-wildcard positions. The conjunction obfuscation starts to get noticed because it provides the ability to protect these sensitive patterns while preserving its functionality. It is meaningful when the conjunction obfuscation is applied in the pattern matching, biological recognition, resisting SQL injection attacks and so on. In this work, we propose a new candidate of conjunction obfuscation. It not only retains the simplicity of the intuitive scheme in BKM18, but also adds wildcards to the pattern. Besides, we also propose a conjunction obfuscation with multi-bit output. The second obfuscation has the same size of the obfuscated program as the first obfuscation. Both obfuscations provide the distributional virtual black-box security.},
  archive      = {J_COMJNL},
  author       = {Zhang, Zheng and Zhang, Fangguo},
  doi          = {10.1093/comjnl/bxac120},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2773-2781},
  shortjournal = {Comput. J.},
  title        = {New obfuscation scheme for conjunctions},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive federated learning with non-IID data.
<em>COMJNL</em>, <em>66</em>(11), 2758–2772. (<a
href="https://doi.org/10.1093/comjnl/bxac118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of Internet of things(IoT) devices, it generates an enormous volume of data, and it is a challenge to mine the IoT data value while ensuring security and privacy. Federated learning is a decentralized approach for training data located on edge devices, such as mobile phones and IoT devices, while keeping privacy, efficiency, and security. However, the Non-IID (non-independent and identically distributed) data, always greatly impacts the performance of the global model. In this paper, we propose a FedDynamic algorithm to solve the statistical challenge of federated learning caused by Non-IID. As Non-IID data can lead to significant differences in model parameters between edge devices, we set different weights for different devices during model aggregation to get a high-performance global model. We analyze and exact key indices (local model accuracy, local data quality, and model difference between local models and the global model), which can reflect the quality of the model, and calculate the aggregation weight for edge devices based on the key indices. Furthermore, we dynamically adjust aggregation weight based on accuracy’s variety to solve weight staleness during the training process. Experiments on the MNIST, FMNIST, EMNIST, CINIC-10 and CIFAR-10 datasets show that the FedDynamic algorithm has better accuracy and convergence performance, compared to the FedAvg, FedProx and Scaffold algorithms.},
  archive      = {J_COMJNL},
  author       = {Zeng, Yan and Mu, Yuankai and Yuan, Junfeng and Teng, Siyuan and Zhang, Jilin and Wan, Jian and Ren, Yongjian and Zhang, Yunquan},
  doi          = {10.1093/comjnl/bxac118},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2758-2772},
  shortjournal = {Comput. J.},
  title        = {Adaptive federated learning with non-IID data},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RTIM hashing: Robust and compact video hashing with a
rotation- and translation-invariant model. <em>COMJNL</em>,
<em>66</em>(11), 2741–2757. (<a
href="https://doi.org/10.1093/comjnl/bxac115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video hashing is a popular research topic in the fields of multimedia information and security because its fast matching and low-cost storage characteristics are widely used in many applications (video copy detection, video retrieval, video authentication, etc.). This paper describes a compact video hashing method with a rotation- and translation-invariant model (RTIM). The key contribution of this approach is that it innovatively reconstructs an input video into a 3D RTIM by combining ring partition and a pipeline histogram; this is a first in video hashing and helps make video hashes resistant to rotation and translation. Then, the proposed model is decomposed via Tucker decomposition, and the generated core tensor is used to produce a compact hash. As the core tensor is a compressed version of the original tensor, hash construction with the core tensor makes RTIM hashing compact and achieves desirable discrimination ability. Different from existing video hashing algorithms, RTIM hashing can not only resist many commonly used digital operations, especially video rotation and cyclic frame shifting, but also achieve good discrimination ability. Various experiments demonstrate the effectiveness of our algorithm. Receiver operating characteristic curve comparisons show that compared with the state-of-the-art video hashing algorithms, RTIM hashing is more robust and compact.},
  archive      = {J_COMJNL},
  author       = {Chen, Lv and Ye, Dengpan and Shang, Yueyun},
  doi          = {10.1093/comjnl/bxac115},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2741-2757},
  shortjournal = {Comput. J.},
  title        = {RTIM hashing: Robust and compact video hashing with a rotation- and translation-invariant model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble framework combining family information for android
malware detection. <em>COMJNL</em>, <em>66</em>(11), 2721–2740. (<a
href="https://doi.org/10.1093/comjnl/bxac114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each malware application belongs to a specific malware family, and each family has unique characteristics. However, existing Android malware detection schemes do not pay attention to the use of malware family information. If the family information is exploited well, it could improve the accuracy of malware detection. In this paper, we propose a general E nsemble framework combining F amily I nformation for Android M alware Detector , called EFIMDetector. First, eight categories of features are extracted from Android application packages. Then, we define the malware family with a large sample size as a prosperous family and construct a classifier for each prosperous family as a conspicuousness evaluator for the family characteristics. These conspicuousness evaluators are combined with a general classifier (which can be a base or ensemble classifier in itself), called the final classifier, to form a two-layer ensemble framework. For the samples of prosperous families with conspicuous family characteristics, the conspicuousness evaluators directly provide detection results. For other samples (including the samples of prosperous families with nonconspicuous family characteristics and the samples of nonprosperous families), the final classifier is responsible for detection. Seven common base classifiers and three common ensemble classifiers are used to detect malware in the experiment. The results show that the proposed ensemble framework can effectively improve the detection accuracy of these classifiers.},
  archive      = {J_COMJNL},
  author       = {Li, Yao and Xiong, Zhi and Zhang, Tao and Zhang, Qinkun and Fan, Ming and Xue, Lei},
  doi          = {10.1093/comjnl/bxac114},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2721-2740},
  shortjournal = {Comput. J.},
  title        = {Ensemble framework combining family information for android malware detection},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based internet-of-things for secure transmission
of medical data in rural areas. <em>COMJNL</em>, <em>66</em>(11),
2705–2720. (<a href="https://doi.org/10.1093/comjnl/bxac113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients’ medical data are extremely sensitive information during storage and transfer, and it needs the highest security level. Furthermore, these records must frequently be linked to patient medical data, and then the linked medical data are securely transmitted to the healthcare center. In this study, a Blockchain-Based Traceable Data Sharing method is proposed to securely transfer the medical data. A Paillier homomorphic encryption method is used to prevent data theft or attacks from occurring in the cloud as a result of the transfer of medical data there. It prevents intravenous third parties, which executes arithmetic operations on the ciphertext. Then the encrypted data are stored in the cloud and to remove clone nodes in the gateway, a software-defined networking technology is introduced. Then a Blockchain-Based Traceable Data Sharing is proposed to ensure data privacy and authenticity while maintaining data privacy at the point of data transmission. Data are then encrypted using a new Enhanced Cipher Text-Policy Encryption Attribute-based Encryption (E-CP-ABE). Private blockchain transfers are carried out on the chain, supporting fine-grain access control with flexible access policies and creating a private key in E-CP-ABE. The presented technique is executed in Matlab software of version R2020a. The performance parameters are encryption, and decryption time, mean square error (MSE), peak signal-to-noise ratio (PSNR), sensitivity, respectively. The encryption process function is nearly 8\% superior than the existing methods and the decryption time is 14\% greater than other methods. As a result, this study shows that the research approach outperformed in terms of encryption time and decryption time, as well as PSNR, MSE, and sensitivity. This technique outperforms other state-of-the-art algorithms in terms of imperceptibility and robustness against various attacks. Consequently, this approach is more reliable than previous methods for the transmission of medical data.},
  archive      = {J_COMJNL},
  author       = {Munagala, N V L M Krishna and Rani, A Daisy and Reddy, D V Rama Koti},
  doi          = {10.1093/comjnl/bxac113},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2705-2720},
  shortjournal = {Comput. J.},
  title        = {Blockchain-based internet-of-things for secure transmission of medical data in rural areas},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing deep-learning based side-channel analysis through
simultaneously multi-byte training. <em>COMJNL</em>, <em>66</em>(11),
2674–2704. (<a href="https://doi.org/10.1093/comjnl/bxac112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preparing a large number of physical traces is an important first step in Side-Channel Analysis, especially in Deep-Learning based Side-Channel Analysis (DL-SCA). With sufficient training data and a proper modeling algorithm, the secret key of cryptographic devices can be successfully recovered with a small number of attacking data. However, in reality, it may be impossible or difficult, in some threat models, to collect sufficient data due to various resource constraints. In this case, the performance of DL-SCA will be severely decreased. In this work, we propose an easy-to-implement method to achieve an efficient DL-SCA with a small number of training data in the scenario of software-based cryptographic implementations. Our simultaneously multi-byte training method, which trains the model with side-channel leakage characteristics of different byte intermediate values, significantly enhances the robustness and performance of DL-SCA. The simulated experiment shows that our method achieves more robust profiling. The success rate of recovering a secret AES key can be improved by 250\% with the same collected data. The results of attacking real-world COTS USIM cards are consistent with the ones of simulation-based counterparts. Compared with state-of-the-art data-augmentation techniques, our results show that the proposed method can achieve the same or even better performance without additional generated training data.},
  archive      = {J_COMJNL},
  author       = {Jin, Chengbin and Zhou, Yongbin},
  doi          = {10.1093/comjnl/bxac112},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2674-2704},
  shortjournal = {Comput. J.},
  title        = {Enhancing deep-learning based side-channel analysis through simultaneously multi-byte training},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trapezoidal sketch: A sketch structure for frequency
estimation of data streams. <em>COMJNL</em>, <em>66</em>(11), 2656–2673.
(<a href="https://doi.org/10.1093/comjnl/bxac111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sketch is one of the typical and widely used data structures for estimating the frequencies of items in data streams. However, since the counter sizes in traditional rectangular sketch ( r- sketch) are the same, it is hard to achieve small space usage, high capacity (i.e. the maximum frequency can be recorded) and high estimated accuracy simultaneously. Moreover, when considering the high skewness of data streams, this problem will become even worse. Consequently, we propose the trapezoidal sketch ( t- sketch) in this paper. In the t- sketch, different from the r- sketch, the counter sizes in different layers are different. Therefore, the low space usage and high capacity can be achieved simultaneously in the t- sketch. Moreover, based on the basic t- sketch, we propose the space-saving t- sketch and the capacity-improvement t- sketch and analyze the properties of these two t- sketches. Finally, for improving the estimation accuracy of the t- sketch further, we propose the probabilistic-based estimation error-reducing algorithm. Compared with the CM sketch, CU sketch, C sketch and A sketch, the simulation results show that the performances on space usage, capacity and estimation accuracy are improved successfully by the space-saving t- sketch and the capacity-improvement t- sketch.},
  archive      = {J_COMJNL},
  author       = {Li, Ning and Yuan, Xin and Ortega, Jose-Fernan Martinez and Diaz, Vicente Hernandez},
  doi          = {10.1093/comjnl/bxac111},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2656-2673},
  shortjournal = {Comput. J.},
  title        = {Trapezoidal sketch: A sketch structure for frequency estimation of data streams},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAM: Scrutinizing leading APIs for detecting suspicious call
sequences. <em>COMJNL</em>, <em>66</em>(11), 2638–2655. (<a
href="https://doi.org/10.1093/comjnl/bxac110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of smartphones has given exponential rise to the number of new mobile malware. These malware programs are employing stealthy obfuscations to hide their malicious activities. To perform malicious activities a program must make application programming interface (API) calls. Unlike dynamic, static analysis can find all the API call paths but have some issues: large number of features; higher false positives when features reduced; and lowering false positives increases the detection rate. Certain Android API calls, e.g. android.app.Activity:boolean requestWindowFeature(int) enable malware programs to call other APIs to hide their activities. We call them leading APIs as they can lead to malicious activities. To overcome these issues, we propose new heuristics and feature groupings for building a Leading API-call Map, named LAM. We create LAM from a dominant (leading) API call tree. Dominance is a transitive relation and hence enumerates all the call sequences that a leading API leads to. LAM substantially reduces the number and improves the quality of features for combating obfuscations and detecting suspicious call sequences with few false positives. For the dataset used in this paper, LAM reduced the number of features from 509 607 to 29 977. Using 10-fold cross-validation, LAM achieved an accuracy of 97.9\% with 0.4\% false positives.},
  archive      = {J_COMJNL},
  author       = {Alam, Shahid},
  doi          = {10.1093/comjnl/bxac110},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2638-2655},
  shortjournal = {Comput. J.},
  title        = {LAM: Scrutinizing leading APIs for detecting suspicious call sequences},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple histograms-based reversible data hiding using fast
performance optimization and adaptive pixel distribution.
<em>COMJNL</em>, <em>66</em>(11), 2623–2637. (<a
href="https://doi.org/10.1093/comjnl/bxac109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prediction error-based reversible data hiding, multiple histograms modification (MHM) is well known for high image quality and thus has received wide attention in recent years. However, the computational cost for performance optimization in MHM is too high, which is particularly critical for real-time applications. This manuscript aims to reduce the computational complexity of MHM by presenting two techniques, including fast performance optimization and adaptive pixel distribution. Fast performance optimization provides a two-stage process for optimal bin selection by exploiting the concept of per-bit distortion of data embedding within a prediction error histogram (PEH). In fast performance optimization, the distribution characteristics of the per-bit distortion are investigated to significantly narrow down the solution space of optimal bin selection. The second technique is adaptive pixel distribution, which tries to nonuniformly allocate pixels into multiple PEHs to further reduce the time complexity. Extensive experiments show that the computational complexity of MHM is significantly reduced while well preserving the image quality.},
  archive      = {J_COMJNL},
  author       = {Yuan, Junying and Zheng, Huicheng and Ni, Jiangqun},
  doi          = {10.1093/comjnl/bxac109},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2623-2637},
  shortjournal = {Comput. J.},
  title        = {Multiple histograms-based reversible data hiding using fast performance optimization and adaptive pixel distribution},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ImposeSVD: Incrementing PureSVD for top-n recommendations
for cold-start problems and sparse datasets. <em>COMJNL</em>,
<em>66</em>(11), 2595–2622. (<a
href="https://doi.org/10.1093/comjnl/bxac106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduced two novel collaborative filtering techniques for recommendation systems in cases of various cold-start situations and incomplete datasets. The first model establishes an asymmetric weight matrix between items without using item meta-data and eradicates the disadvantages of neighborhood approaches by automatic determination of threshold values. Our first model, z-scoREC, is also regarded as a pure deep-learning model because it performs like a vanilla auto-encoder in transforming column vectors with z -score normalization similar to batch normalization. With the second model, ImposeSVD, we aimed to enhance the shortcomings of the PureSVD in cases of cold-start and incomplete data by preserving its straightforward implementation and non-parametric form. The ImposeSVD model relies on the z-scoREC, produces synthetic new predictions for the users by decomposing the latent factors from the imposed matrix. We evaluated our method on the well-known datasets and found out that our method was outperforming similar approaches in the specific scenarios including recommendations for cold-start users, strength in cold-start systems, and diversification of long-tail item recommendations in lists. Our z-scoREC model also outperformed familiar neighbor-based approaches when operated as a recommender system and gave a closer appearance to the decomposition methods despite its simple and rigid cost framework.},
  archive      = {J_COMJNL},
  author       = {Yilmazer, Hakan and Özel, Selma Ayşe},
  doi          = {10.1093/comjnl/bxac106},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {2595-2622},
  shortjournal = {Comput. J.},
  title        = {ImposeSVD: Incrementing PureSVD for top-N recommendations for cold-start problems and sparse datasets},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: Intelligent forecast of stock markets to
handle COVID-19 economic crisis by modified generative adversarial
networks. <em>COMJNL</em>, <em>66</em>(10), 2593. (<a
href="https://doi.org/10.1093/comjnl/bxac130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxac130},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2593},
  shortjournal = {Comput. J.},
  title        = {Correction to: Intelligent forecast of stock markets to handle COVID-19 economic crisis by modified generative adversarial networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reversible circuit synthesis method using sub-graphs of
shared functional decision diagrams. <em>COMJNL</em>, <em>66</em>(10),
2574–2592. (<a href="https://doi.org/10.1093/comjnl/bxac107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible circuit synthesis methods based on decision diagrams achieve low quantum costs but do not account for quantum bit (qubit) limits for the application of reversible logic in quantum computing. Here, a synthesis method using sub-graphs of shared functional decision diagrams (SFDDs) is proposed for reducing the number of lines when synthesizing reversible circuits. An SFDD is partitioned into sub-graphs by exploiting the longest dominant-active paths, and the sub-graphs are mapped to reversible gate cascades. To further reduce the number of lines, template root matching is presented for reusing circuit lines. Experimental results indicate that the proposed method achieves the known minimum number of lines in many cases and has good scalability. Although the proposed method increases the quantum cost over a prior method based on functional decision diagrams, it significantly reduces the number of lines in most cases. Compared with the one-pass method using quantum multiple-valued decision diagrams, the proposed method reduces the quantum cost without increasing the number of lines in many cases. When compared with the lookup table-based method using a direct mapping flow, the method reduces the number of lines in a few cases. Thus, the method aids in the physical realization of a quantum circuit.},
  archive      = {J_COMJNL},
  author       = {Bu, Dengli and Deng, Junyi and Tang, Pengjie and Yang, Shuhong},
  doi          = {10.1093/comjnl/bxac107},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2574-2592},
  shortjournal = {Comput. J.},
  title        = {Reversible circuit synthesis method using sub-graphs of shared functional decision diagrams},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptively secure KP-ABE for circuits with fan-in n and
fan-out 1. <em>COMJNL</em>, <em>66</em>(10), 2554–2573. (<a
href="https://doi.org/10.1093/comjnl/bxac105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attribute-based encryption (ABE) scheme is suitable for access control of ciphertext in cloud computing. Kowalczyk and Wee proposed an adaptively secure attribute encryption scheme that supports |$NC^1$| circuits. However, the circuit depth increases because this scheme supports only circuits with fan-in 2, which increases the key length and computational complexity of the key generation and decryption algorithms. To improve efficiency, we designed a secret sharing scheme for circuits with fan-in |$n$|⁠ , and we proposed a key-policy ABE scheme that supports circuits with fan-in |$n$|⁠ . We also designed pebbling rules for secret sharing in circuits with fan-in |$n$|⁠ , improving the compactness of the security reduction. Finally, we proved the adaptive security of the scheme by using a piecewise guessing framework and dual-system encryption. Compared with existing schemes, our scheme reduces the key size, improves the efficiency of the key generation algorithm and the decryption algorithm and provides tighter security reduction.},
  archive      = {J_COMJNL},
  author       = {Sun, Keshuo and Gao, Haiying},
  doi          = {10.1093/comjnl/bxac105},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2554-2573},
  shortjournal = {Comput. J.},
  title        = {Adaptively secure KP-ABE for circuits with fan-in n and fan-out 1},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved lattice-based ring signature with unclaimable
anonymity in the standard model. <em>COMJNL</em>, <em>66</em>(10),
2542–2553. (<a href="https://doi.org/10.1093/comjnl/bxac104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ring signatures enable a user to sign messages on behalf of an arbitrary set of users, called the ring, without revealing exactly which member of that ring actually generated the signature. The signer-anonymity property makes ring signatures have been an active research topic. Recently, Park and Sealfon (PS; CRYPTO’19) presented an important anonymity notion named signer-unclaimability and constructed a lattice-based ring signature scheme with unclaimable anonymity in the standard model; however, it did not consider the unforgeable w.r.t. adversarially chosen-key attack (the public key ring of a signature may contain keys created by an adversary) and the signature size grows quadratically in the size of ring and message. In this work, we propose a new lattice-based ring signature scheme with unclaimable anonymity in the standard model. In particular, our work improves the security and efficiency of PS work, which is unforgeable w.r.t. adversarially chosen-key attack, and the ring signature size grows linearly in the ring size.},
  archive      = {J_COMJNL},
  author       = {Hu, Mingxing and Zhang, Weijiong and Liu, Zhen},
  doi          = {10.1093/comjnl/bxac104},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2542-2553},
  shortjournal = {Comput. J.},
  title        = {An improved lattice-based ring signature with unclaimable anonymity in the standard model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the maximum cliques of the subgraphs induced by binary
constant weight codes in powers of hypercubes. <em>COMJNL</em>,
<em>66</em>(10), 2535–2541. (<a
href="https://doi.org/10.1093/comjnl/bxac103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the maximum independent sets (or maximum cliques) of a given graph is fundamental in graph theory and is also one of the most important in terms of the application of graph theory. Let |$A(n,d,w)$| be the size of the maximum independent set of |$Q_{n}^{(d-1,w)}$|⁠ , which is the induced subgraph of points of weight |$w$| of the |$d-1^{th}$| -power of |$n$| -dimensional hypercubes. In order to further understand and study the dependent set of |$Q_{n}^{(d-1,w)}$|⁠ , we explore its clique number and the structure of the maximum clique. This paper obtains the clique number and the structure of the maximum clique of |$Q_{n}^{(d-1,w)}$| for |$5\leq d\leq 6$|⁠ . Moreover, the characterizations for |$A(n,d,w)=2$| and |$3$| are also given.},
  archive      = {J_COMJNL},
  author       = {Shi, Juanjuan and Kou, Yongfang and Hu, Yulan and Yang, Weihua},
  doi          = {10.1093/comjnl/bxac103},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2535-2541},
  shortjournal = {Comput. J.},
  title        = {On the maximum cliques of the subgraphs induced by binary constant weight codes in powers of hypercubes},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical attacks of round-reduced SIMON based on deep
learning. <em>COMJNL</em>, <em>66</em>(10), 2517–2534. (<a
href="https://doi.org/10.1093/comjnl/bxac102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At CRYPTO’19, Gohr built a bridge between deep learning and cryptanalysis. Based on deep neural networks, he trained neural distinguishers of SPECK32/64. Besides, with the help of neural distinguishers, he attacked 11-round SPECK32/64 using Bayesian optimization. Compared with the traditional attack, its complexity was reduced. Although his work opened a new direction of machine learning aided cryptanalysis, there are still two research gaps that researchers are eager to fill in. (i) Can the attack using neural distinguishers be used to other block ciphers? (ii) Are there effective key recovery attacks on large-size block ciphers adopting neural distinguishers? In this paper, our core target is to propose an effective neural-aided key recovery policy to attack large-size block ciphers. For large-size block ciphers, it costs too much time in pre-computation, especially in wrong key response profile, which is the main reason why there are almost no neural aided attacks on large-size block ciphers. Fortunately, we find that there is a fatal flaw in the wrong key profile. In the some experiments of SIMON32/64 and SIMON48/96, there is a regular of change in response profiles, which implies that we can use partial response instead of the complete response. Based on this, we propose a generic key recovery attack scheme which can attack large-size block ciphers. As an application, we perform a key recovery attack on 13-round SIMON64/128, which is the first practical attack using neural distinguishers to large-size ciphers. In addition, we also attack 13-round SIMON32/64 and SIMON48/96, which also shows that the neural distinguishers can be used to other block ciphers.},
  archive      = {J_COMJNL},
  author       = {Hou, Zezhou and Ren, Jiongjiong and Chen, Shaozhen},
  doi          = {10.1093/comjnl/bxac102},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2517-2534},
  shortjournal = {Comput. J.},
  title        = {Practical attacks of round-reduced SIMON based on deep learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multivariate-based provably secure certificateless
signature scheme with applications to the internet of medical things.
<em>COMJNL</em>, <em>66</em>(10), 2499–2516. (<a
href="https://doi.org/10.1093/comjnl/bxac100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few years, Internet of Medical Things (IoMT) has completely transformed the healthcare industry. It is bringing out the most notable, and unprecedented impacts on human health, and has totally changed the way we look at the healthcare industry. The healthcare sector all around the globe are leapfrogging, and adopting the technology, helping in transforming drastically in a very short span of time. However, as more and more number of medical devices are being connected to IoMT, security issues like ensuring authenticity and integrity of the transmitted data are also on the rise. In view of the context, there is a need of an efficient cryptographic primitive that can address these issues in a viable manner. A signature scheme seems to be the natural choice to mitigate the security concerns. But, traditional signature schemes, both public-key-infrastructure-based and Identity-based, have their own disadvantages, which makes them unsuitable for IoMT networks. Thus, to address the security issues and problems like certificate management and key escrow, herein, we put forward the first multivariate-based certificateless signature scheme, namely, Multivariate Certificateless Signature (Mul-CLS) , which is built on top of the intractability of multivariate-quadratic (MQ) problem. The fact that multivariate public key cryptosystem provides fast, post-quantum safe and efficient primitives makes it a front-runner candidate among the other post-quantum cryptography candidates. Our scheme Mul-CLS provides existential unforgeability against chosen message and chosen identity Super Type I and Super Type II adversary if solving the MQ problem is NP-hard. In addition to that, our proposed Mul-CLS presents itself as a robust and cost-friendly cryptographic building block for building IoMT networks.},
  archive      = {J_COMJNL},
  author       = {Srivastava, Vikas and Debnath, Sumit Kumar},
  doi          = {10.1093/comjnl/bxac100},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2499-2516},
  shortjournal = {Comput. J.},
  title        = {A multivariate-based provably secure certificateless signature scheme with applications to the internet of medical things},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-aided statistical attack for cryptanalysis.
<em>COMJNL</em>, <em>66</em>(10), 2480–2498. (<a
href="https://doi.org/10.1093/comjnl/bxac099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Crypto’19, Gohr proposed the first deep learning-based key recovery attack on 11-round Speck32/64, which opens the direction of neural-aided cryptanalysis. Until now, neural-aided cryptanalysis still faces two problems: (i) the attack complexity estimations rely purely on practical experiments; (ii) it does not work when there are not enough neutral bits. To the best of our knowledge, we are the first to solve these two problems. In this paper, we propose a Neural-Aided Statistical Attack (NASA) that has the following advantages: (i) NASA supports estimating the theoretical complexity. (ii) NASA does not rely on any special properties including neutral bits. Moreover, we propose three methods for reducing the complexity of NASA. One of the methods, which is based on a newly proposed concept named Informative Bit that reveals an important phenomenon, makes NASA applicable to large-size ciphers. We have performed a series of experiments on round reduced Speck32/64, DES, and Speck96/96. These experiments do not only verify the correctness of NASA, but also further highlight the advantage and potential of NASA. Our work arguably raises a new direction for neural-aided cryptanalysis.},
  archive      = {J_COMJNL},
  author       = {Chen, Yi and Shen, Yantian and Yu, Hongbo},
  doi          = {10.1093/comjnl/bxac099},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2480-2498},
  shortjournal = {Comput. J.},
  title        = {Neural-aided statistical attack for cryptanalysis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anti-occlusion target tracking based on joint confidence.
<em>COMJNL</em>, <em>66</em>(10), 2462–2479. (<a
href="https://doi.org/10.1093/comjnl/bxac098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to the challenge of occlusion during tracking, this paper proposes an anti-occlusion tracking based on joint confidence. Under the framework of the kernel correlation filter (KCF) tracking, the dimension of the feature is extended to construct a robust target appearance model, and the size of the target is estimated during the tracking process. We first judge whether occlusion occurs or not by the measurement by combining the maximum of the detection response map with the average peak correlation energy, then design the corresponding anti-interference tracking strategy. If the occlusion does not occur during the tracking process, the KCF tracking is performed, otherwise, re-detection is introduced to locate the target position, and the region corresponding to the re-detection is added to the regulation term of the KCF for context learning. The fusion of the filter template before occlusion and the context model learned during occlusion is used to locate the target and to update the model. Experimental evaluations on the datasets OTB2013, OTB100 and TC128 show that compared with the state-of-the-art algorithms such as KCF, Siamese and other algorithms, our proposed algorithm has stronger robustness and higher tracking accuracy when occlusion occurs.},
  archive      = {J_COMJNL},
  author       = {Zhou, Wei and Ding, Xiaoxue and Xu, Haixia},
  doi          = {10.1093/comjnl/bxac098},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2462-2479},
  shortjournal = {Comput. J.},
  title        = {Anti-occlusion target tracking based on joint confidence},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A clothoid curve-based intersection collision warning scheme
in internet of vehicles. <em>COMJNL</em>, <em>66</em>(10), 2447–2461.
(<a href="https://doi.org/10.1093/comjnl/bxac097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important problems in traffic safety is providing effective collision warnings in intersection areas. In this paper, we propose a Clothoid Curve-based Intersection Collision Warning scheme (CICW) in the Internet of Vehicles. In CICW, we first present a clothoid curve-based vehicle trajectory prediction model. In this model, vehicles can establish the trajectory prediction equations by themselves. Each vehicle solves the equations based on its internal state information, electronic map, GPS data and neighbour vehicles’ state information derived from periodical beacons. The vehicle then predicates the crossing points of the predicted trajectory between itself and the neighbour vehicles. Based on the reference points, it further obtains the earliest possible collision location and then issues a warning. Extensive simulation results show that the performance of the proposed scheme achieves higher collision warning accuracy and a lower error warning ratio compared to existing schemes.},
  archive      = {J_COMJNL},
  author       = {Luo, Xuanhao and Feng, Yong and Wang, Chengdong},
  doi          = {10.1093/comjnl/bxac097},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2447-2461},
  shortjournal = {Comput. J.},
  title        = {A clothoid curve-based intersection collision warning scheme in internet of vehicles},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the batch outsourcing of pairing computations.
<em>COMJNL</em>, <em>66</em>(10), 2437–2446. (<a
href="https://doi.org/10.1093/comjnl/bxac095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairing-based cryptography is utilized in a wide range of devices, such as servers, mobile devices, smart cards and sensors. Pairing computation would be a burden for power and/or computation-restricted devices. Protocols for outsourcing pairing computations from limited devices to more resourceful devices are already proposed. These protocols naturally require verification of the computation and secrecy of the inputs and/or outputs. Similarly, batch pairing outsourcing protocols aim to improve efficiency over multiple runs of the state-of-the-art single pairing delegation protocols. Here, we will cover efficient, privacy preserving, secure batch pairing outsource protocols for each type based on secrecy of inputs and outputs. We propose the first generic outsourcing protocol where inputs and outputs of the pairing function are secret. In addition to this, we give some methods to avoid certain type of attacks, increase efficiency and get rid of pairing arithmetic. The proposed protocols enable limited devices to outsource pairing computations with only elliptic curve arithmetic.},
  archive      = {J_COMJNL},
  author       = {Kalkar, Oznur and Sertkaya, Isa and Tutdere, Seher},
  doi          = {10.1093/comjnl/bxac095},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2437-2446},
  shortjournal = {Comput. J.},
  title        = {On the batch outsourcing of pairing computations},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Glaucoma detection using multiple feature set with recurrent
neural network. <em>COMJNL</em>, <em>66</em>(10), 2426–2436. (<a
href="https://doi.org/10.1093/comjnl/bxac093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most widespread illnesses of blindness is glaucoma. Optic nerve are essentialfor clear vision, but glaucoma effects the optic nerves and results blurred vision. This condition is often exacerbated by abnormally high intra-ocular pressure. Accurate early identification and continuous screening can help to minimize loss of vision. A non-invasive computer-aided diagnosis treatment uses optical fundus images to detect glaucoma in its early stages. This work includes image preprocessing, optic disk (OD) segmentation, feature extraction from the OD and recurrent neural network classification to identify glaucoma. The performance of the proposed system is tested using fundus image datasets such as DRISHTI-GS and Large-Scale Attention-Based Glaucoma (LAG). By this method, glaucoma detection accuracyof 96.1\% is obtained for DRISHTI-GS and 92.73\% for LAG dataset, which is higher thanthe existing state of arts. Proposed procedure can help ophthalmologists diagnose glaucomawith good performance.},
  archive      = {J_COMJNL},
  author       = {Shyla, N S Jeya and Emmanuel, W R Sam},
  doi          = {10.1093/comjnl/bxac093},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2426-2436},
  shortjournal = {Comput. J.},
  title        = {Glaucoma detection using multiple feature set with recurrent neural network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loading cost-aware model caching and request routing in
edge-enabled wireless sensor networks. <em>COMJNL</em>, <em>66</em>(10),
2409–2425. (<a href="https://doi.org/10.1093/comjnl/bxac088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing works on caching in multi-access edge computing focus on service caching and request routing. However, loading cost and execution time influenced by resource sharing have not been well exploited. To fill this gap, we investigate the joint optimization problem over deep neural network (DNN) model caching and DNN request routing with edge collaboration in edge-enabled wireless sensor networks. A problem is formulated, with the objective of maximizing throughput, under constraints of budget, accuracy and latency etc. The proof of NP-hardness for the formulated problem is provided. To solve the problem, an approximation algorithm based on randomized rounding is presented. In addition, the approximation ratio for the presented algorithm is proved to be |$1/(1-\sqrt{4\ln S/\xi^\dagger})$|⁠ , where |$S$| is the number of edge servers and |$\xi^\dagger$| is the objective value from linear relaxation. Extensive experiments demonstrate that the system throughput for the presented algorithm can be improved by 58.8\% on average, compared with that of the baseline algorithm.},
  archive      = {J_COMJNL},
  author       = {Yao, Mianyang and Chen, Long and Wu, Yalan and Wu, Jigang},
  doi          = {10.1093/comjnl/bxac088},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2409-2425},
  shortjournal = {Comput. J.},
  title        = {Loading cost-aware model caching and request routing in edge-enabled wireless sensor networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view spatial–temporal graph neural network for traffic
prediction. <em>COMJNL</em>, <em>66</em>(10), 2393–2408. (<a
href="https://doi.org/10.1093/comjnl/bxac086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial–temporal graph neural network has drawn more and more attention in recent years and is widely used to various real-world applications. However, learning the spatial–temporal graph neural network structure presents unique challenges including: (i) the dynamic spatial correlation; (ii) the dynamic temporal correlation. Even the existing methods take into account the spatial correlation, they still learn the static road network structure information, which cannot reflect the dynamic of road relations. Some of the works has focused on modeling the long-term time series, but the improvements have been limited tightly. To overcome these challenges, we proposed a novel approach called Multi-View Spatial–Temporal Graph Neural Network. Differ from the existing research, we designed a multi-view temporal transformer module to extract dynamic temporal correlation and enhance the expression of medium and long-term temporal features. We propose a multi-view spatial structure and a corresponding multi-view graph convolutional module, which are capable of simultaneously combining the features of static road network structure and dynamic changes. Compared with 11 baselines, our proposed model has achieved significant improvement in the accuracy of prediction.},
  archive      = {J_COMJNL},
  author       = {Li, He and Jin, Duo and Li, XueJiao and Huang, HongJie and Yun, JinPeng and Huang, LongJi},
  doi          = {10.1093/comjnl/bxac086},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2393-2408},
  shortjournal = {Comput. J.},
  title        = {Multi-view Spatial–Temporal graph neural network for traffic prediction},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verifiable conjunctive dynamic searchable symmetric
encryption with forward and backward privacy. <em>COMJNL</em>,
<em>66</em>(10), 2379–2392. (<a
href="https://doi.org/10.1093/comjnl/bxac084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic searchable symmetric encryption (DSSE) with forward and backward privacy makes it possible to perform search on the outsourced encrypted database efficiently while still allowing updates under acceptable leakage. Current forward and backward private DSSE (FB-DSSE) scheme proposed by Zuo et al. cannot support conjunctive keyword query and the cloud server needs to be honest-but-curious. Recent FB-DSSE scheme supporting conjunctive keyword query proposed by Patranabis et al. cannot verify search results. On the other hand, searchable symmetric encryption scheme proposed by Wang et al. that supports conjunctive keyword query and the verification of search results cannot achieve forward and backward privacy. The problem of constructing a verifiable conjunctive FB-DSSE scheme is still open. In this paper, we propose a verifiable conjunctive dynamic searchable symmetric encryption scheme (VCDSSE). VCDSSE is a FB-DSSE scheme that additionally supports the verification of search results and conjunctive keyword query. We revisit homomorphic MAC to enable efficient verification of search results, adopt the technique of oblivious cross-tags to achieve conjunctive keyword query and utilize state chain to ensure forward and backward privacy. The formal security analysis and performance evaluation demonstrate that VCDSSE is secure and practical as compared with Mitra scheme in terms of search time.},
  archive      = {J_COMJNL},
  author       = {Lu, Haitang and Chen, Jie and Ning, Jianting and Zhang, Kai},
  doi          = {10.1093/comjnl/bxac084},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2379-2392},
  shortjournal = {Comput. J.},
  title        = {Verifiable conjunctive dynamic searchable symmetric encryption with forward and backward privacy},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revocable policy-based chameleon hash for blockchain
rewriting. <em>COMJNL</em>, <em>66</em>(10), 2365–2378. (<a
href="https://doi.org/10.1093/comjnl/bxac083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy-based chameleon hash is a useful primitive for blockchain rewriting systems. It allows a user to create a mutable transaction associated with an access policy, whereas a modifier who possesses sufficient rewriting privileges from a trusted authority satisfying the access policy can rewrite the mutable transaction. However, it lacks a revocation mechanism. The modifiers can always rewrite the mutable transactions even if their given rewriting privileges are compromised. In this work, we introduce revocable policy-based chameleon. The property of revocation allows some modifiers’ rewriting privileges to be revoked, regardless of whether their rewriting privileges are compromised or not.},
  archive      = {J_COMJNL},
  author       = {Tian, Yangguang and Miyaji, Atsuko and Matsubara, Koki and Cui, Hui and Li, Nan},
  doi          = {10.1093/comjnl/bxac083},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2365-2378},
  shortjournal = {Comput. J.},
  title        = {Revocable policy-based chameleon hash for blockchain rewriting},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified multi-key fully homomorphic encryption scheme in
the plain model. <em>COMJNL</em>, <em>66</em>(10), 2355–2364. (<a
href="https://doi.org/10.1093/comjnl/bxac082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-key fully homomorphic encryption (MFHE) supports arbitrary meaningful computations on encrypted data under different public keys even without access to the secret key, which is well tailored for the secure multiparty computation scenarios. Based on the Gentry–Sahai–Waters scheme (a single-key FHE in Crypto 2013) with the underlying learning with errors problem, MW16 scheme (Eurocrypt 2016) utilizes the method of ‘linear combination procedure’ (LCP) as a subroutine to construct the auxiliary information for the expanded ciphertexts of MFHE scheme. However, every party shares a common random string (CRS) to be distributed by a trusted setup, which is unpractical. Meanwhile, the noise in the auxiliary information is too much compared with the one in fresh ciphertexts. In this paper, we propose a modified MFHE scheme in the plain model, i.e. without CRS, to enhance the practicability of MFHE. Specifically, every involved party generates his own public key independent on a CRS. Then a potential improvement on the LCP is developed to provide auxiliary information, which largely reduces the noise and leads to a smaller modulus for our MFHE. Furthermore, the feasibility of our proposal is also proved by theoretical performance comparisons.},
  archive      = {J_COMJNL},
  author       = {Xu, Wenju and Wang, Baocang and Qu, Quanbo and Zhou, Tanping and Duan, Pu},
  doi          = {10.1093/comjnl/bxac082},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2355-2364},
  shortjournal = {Comput. J.},
  title        = {Modified multi-key fully homomorphic encryption scheme in the plain model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding performance of a vulnerable heterogeneous edge
data center: A modeling approach. <em>COMJNL</em>, <em>66</em>(10),
2339–2354. (<a href="https://doi.org/10.1093/comjnl/bxac081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) jobs not only require computational resources but also are delay-sensitive and security-sensitive. Edge computing emerges as a promising paradigm to improve the quality of experience for IoT users. Edge computing faces many security threats, perhaps even more than traditional data centers. With a growing amount of data offloaded to Edge Data Centers (EDCs), the EDC performance needs to be considered and evaluated carefully for improving the vulnerable EDC resource utilization while satisfying IoT job requirements. This paper develops an analytical model, which can capture the dynamics of an EDC system with the following features: (i) The system is under heterogeneous workloads; (ii) the system is subject to attacks, which prevent equipment units in the system from providing service and (iii) the jobs in the system are delay-sensitive. Namely, the job processing fails before the processing is completed. Based on the proposed model, we develop formulas for performance and profit metrics and conduct a series of simulation experiments to verify the correctness and accuracy of our model. Finally, through our model, we evaluate the performance of the EDC, and we offer solutions for EDC administrators to maximize profit.},
  archive      = {J_COMJNL},
  author       = {Yang, Runkai and Mišić, Jelena and Mišić, Vojislav B and Liang, Xiao and Zhou, Shenshen and Chang, Xiaolin},
  doi          = {10.1093/comjnl/bxac081},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2339-2354},
  shortjournal = {Comput. J.},
  title        = {Understanding performance of a vulnerable heterogeneous edge data center: A modeling approach},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple blind signature for e-voting and e-cash.
<em>COMJNL</em>, <em>66</em>(10), 2331–2338. (<a
href="https://doi.org/10.1093/comjnl/bxac079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new cryptographic primitive, called multiple blind signature (MBS), which is designed based on the integration of both normal blind signature scheme and dual signature. The major difference between a normal blind signature and an MBS is that using a normal blind signature, only one message, |$m$|⁠ , can be verified, but using an MBS, any subset, |${M}^{\prime }$|⁠ , of multiple messages in a set, |$M$|⁠ , where |${M}^{\prime}{\subseteq} M$|⁠ , can be verified. With this additional property, we will show that MBS is especially suitable for e-voting and e-cash applications. In other words, we classify these processes in two applications into two phases, on-line and off-line phases. One unique property of this design is that most time-consuming computation and interaction can be performed in advance in off-line phase. There is no cost of computation and interaction in the online phase.},
  archive      = {J_COMJNL},
  author       = {Harn, Lein and Hsu, Chingfang and Xia, Zhe and Li, Zixuan},
  doi          = {10.1093/comjnl/bxac079},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {2331-2338},
  shortjournal = {Comput. J.},
  title        = {Multiple blind signature for e-voting and e-cash},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Correction to: Learning disjunctive multiplicity
expressions and disjunctive generalize multiplicity expressions from
both positive and negative examples. <em>COMJNL</em>, <em>66</em>(9),
2329. (<a href="https://doi.org/10.1093/comjnl/bxac117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxac117},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2329},
  shortjournal = {Comput. J.},
  title        = {Correction to: Learning disjunctive multiplicity expressions and disjunctive generalize multiplicity expressions from both positive and negative examples},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Android malware detection in bytecode level using TF-IDF and
XGBoost. <em>COMJNL</em>, <em>66</em>(9), 2317–2328. (<a
href="https://doi.org/10.1093/comjnl/bxac198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android is the dominant operating system in the smartphone market and there exists millions of applications in various application stores. The increase in the number of applications has necessitated the detection of malicious applications in a short time. As opposed to dynamic analysis, it is possible to obtain results in a shorter time in static analysis as there is no need to run the applications. However, obtaining various information from application packages using reverse engineering techniques still requires a substantial amount of processing power. Although some attempts have been made to solve this problem by analyzing binary files without decoding the source code, there is still more work to be done in this area. In this study, we analyzed the applications in bytecode level without decoding the binary source files. We proposed a model using Term Frequency - Inverse Document Frequency (TF-IDF) word representation for feature extraction and Extreme Gradient Boosting (XGBoost) method for classification. The experimental results show that our model classifies a given application package as a malware or benign in 2.75 s with 99.05\% F1-score on a balanced dataset, and in 3.30 s with 99.35\% F1-score on an imbalanced dataset containing obfuscated malwares.},
  archive      = {J_COMJNL},
  author       = {Ozogur, Gokhan and Erturk, Mehmet Ali and Gurkas Aydin, Zeynep and Aydin, Muhammed Ali},
  doi          = {10.1093/comjnl/bxac198},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2317-2328},
  shortjournal = {Comput. J.},
  title        = {Android malware detection in bytecode level using TF-IDF and XGBoost},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-based hybrid query reformulation for biomedical
information retrieval. <em>COMJNL</em>, <em>66</em>(9), 2296–2316. (<a
href="https://doi.org/10.1093/comjnl/bxac078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query reformulation is a well-known technique intended to improve the performance of Information Retrieval Systems. Among the several available techniques, Query Expansion (QE) reformulates the initial query by adding similar terms, drawn from several sources (corpus, knowledge resources), to the query terms in order to retrieve more relevant documents. Most QE methods are based on the relationships between the original query term and candidate terms (new terms) in order to select the most similar expansion terms. In this paper, we suggested a new hybrid query reformulation through QE and term re-weighting techniques. The suggested approach aimed to demonstrate the effectiveness of QE with a semantic selection of candidate terms according to the specificity of original query terms in the improvement of retrieval performance. To this end, we exploited both relationships defined by knowledge resources and the distributed semantics, recently revealed by neural network analysis. For term re-weighting, we proposed a new semantic method based on semantic similarity measure that assigns a weight to each term of the expanded query. The conducted experiments on OHSUMED and TREC 2014 CDS test collections, including long and short queries, yielded significant results that outperformed the baseline and state-of-the-art approaches.},
  archive      = {J_COMJNL},
  author       = {Selmi, Wided and Kammoun, Hager and Amous, Ikram},
  doi          = {10.1093/comjnl/bxac078},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2296-2316},
  shortjournal = {Comput. J.},
  title        = {Semantic-based hybrid query reformulation for biomedical information retrieval},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New strategies to improve differential-linear attacks with
applications to chaskey. <em>COMJNL</em>, <em>66</em>(9), 2279–2295. (<a
href="https://doi.org/10.1093/comjnl/bxac076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential-linear cryptanalysis, as the combination of differential and linear cryptanalysis, is an efficient way to attack many kinds of ciphers. Recently, various refinements to this cryptanalytic technique have been proposed, especially with good effects on ARX ciphers. In the current framework of a differential-linear attack, a cipher ; is often divided into three parts: a differential part ; , a linear part ; and a connective part ; . It is a challenging problem to deal with the connective part when building a differential-linear distinguisher, and for ARX ciphers, estimating the correlation of ; experimentally under given input difference ; and output linear mask ; is the main approach so far. In this paper, we discuss the effects of ; and ; on the correlation of ; for the first time. As a result, we propose a new strategy to find ; and ; to build differential-linear distinguishers with high correlations for ARX ciphers based on algebraic equations derived from their round functions. For the key recovery parts of differential-linear attacks, we also find a new partitioning technique which will reduce the time complexity. Based on our new methods, we improve the differential-linear attack on 7-round Chaskey.},
  archive      = {J_COMJNL},
  author       = {Xu, Yaqi and Wu, Baofeng and Lin, Dongdai},
  doi          = {10.1093/comjnl/bxac076},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2279-2295},
  shortjournal = {Comput. J.},
  title        = {New strategies to improve differential-linear attacks with applications to chaskey},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forward secure public-key authenticated encryption with
conjunctive keyword search. <em>COMJNL</em>, <em>66</em>(9), 2265–2278.
(<a href="https://doi.org/10.1093/comjnl/bxac075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key encryption with keyword search is a promising primitive which enables search over encrypted data in secure data outsourcing services. In traditional construction, the associated keywords may be recovered from a given trapdoor by a malicious server through keyword guessing attacks. Therefore, the notion of public-key authenticated encryption with keyword search (PAEKS) was introduced, where a sender encrypts (and authenticates) the keywords using a receiver’s public key and its secret key. In this paper, we consider the forward security for PAEKS and introduce a new primitive: ; (FS-PAEKS), which captures the information leakage risk from previously issued queries due to the updates on the outsourced data. Technically, we embed a non-interactively agreed key into the cipher-keyword generation algorithm, and bind the cipher-keyword and the trapdoor with a set converted from algorithm-generation time. Finally, we present an efficient FS-PAEKS scheme supporting conjunctive query, and prove its forward security against chosen keyword attacks and keyword guessing attacks. To illustrate practical performance, we implement our FS-PAEKS and related PAEKS schemes based on Enron dataset in real cloud environment.},
  archive      = {J_COMJNL},
  author       = {Jiang, Zhe and Zhang, Kai and Wang, Liangliang and Ning, Jianting},
  doi          = {10.1093/comjnl/bxac075},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2265-2278},
  shortjournal = {Comput. J.},
  title        = {Forward secure public-key authenticated encryption with conjunctive keyword search},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Path-rank-based data chunk scheduling for concurrent
multipath transmission. <em>COMJNL</em>, <em>66</em>(9), 2254–2264. (<a
href="https://doi.org/10.1093/comjnl/bxac074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The device equipped with a multi-homing feature optimally exploits multiple network interfaces in modern communications networks such as the Internet of Things (IoT) and machine-to-machine communication using the concurrent multipath transfer (CMT). This enhances system performance by concurrently scheduling data chunks on multiple network paths. For a while, several scheduling criteria have been developed to optimize performance. However, it has been identified that CMT still suffers from many serious problems, such as spurious retransmission, receiver buffer blocking, improper congestion window (CWND) growth, re-ordering and long round trip time, resulting in poor performance. These problems occur due to the asymmetric nature of path characteristics. Thus, this paper introduces a path rank-based CMT (R-CMT) that schedules data chunks according to the rank of the path. The proposed scheduling method calculates the rank of each network path based on the ratio of successfully received and transmitted chunks. The simulation results indicate that the proposed R-CMT scheduling achieves higher performance in terms of network latency, throughput and CWND growth.},
  archive      = {J_COMJNL},
  author       = {Tomar, Parul and Kumar, Gyanendra and Verma, Lal Pratap},
  doi          = {10.1093/comjnl/bxac074},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2254-2264},
  shortjournal = {Comput. J.},
  title        = {Path-rank-based data chunk scheduling for concurrent multipath transmission},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable misinformation mitigation in social networks using
reverse sampling. <em>COMJNL</em>, <em>66</em>(9), 2230–2253. (<a
href="https://doi.org/10.1093/comjnl/bxac073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider misinformation propagating through a social network and study the problem of its prevention. The goal is to identify a set of ; users that need to be convinced to adopt a limiting campaign so as to minimize the number of people that end up adopting the misinformation. This work presents Reverse Prevention Sampling (; ), an algorithm that provides a scalable solution to the misinformation mitigation problem. Our theoretical analysis shows that ; runs in ; expected time and returns a ; -approximate solution with at least ; probability (where ; is a typically small network parameter and ; is a confidence parameter). The time complexity of ; substantially improves upon the previously best-known algorithms that run in time ; . We experimentally evaluate ; on large datasets and show that it outperforms the state-of-the-art solution by several orders of magnitude in terms of running time. This demonstrates that misinformation mitigation can be made practical while still offering strong theoretical guarantees.},
  archive      = {J_COMJNL},
  author       = {Simpson, Michael and Srinivasan, Venkatesh and Thomo, Alex},
  doi          = {10.1093/comjnl/bxac073},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2230-2253},
  shortjournal = {Comput. J.},
  title        = {Scalable misinformation mitigation in social networks using reverse sampling},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting affine equivalence of boolean functions and
circuit transformation. <em>COMJNL</em>, <em>66</em>(9), 2220–2229. (<a
href="https://doi.org/10.1093/comjnl/bxac072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affine equivalence of Boolean functions has various applications in computer science and modern cryptography, such as circuit design and S-boxes. Existing methods for detecting affine equivalence of Boolean functions work in some cases but not when the truth table of a Boolean function is sparse. To improve previous methods and overcome this limitation, we propose a method by transforming the Boolean function to a function with the property that its function values at the orthonormal basis are all equal to 1 or 0, which narrows down the search space of affine transformations. Our first algorithm has the advantage of getting a smaller search space than previous methods and is especially useful for sparse functions. Specifically, when the Boolean functions are sparse, the search space can be reduced exponentially in average and experiments show the efficiency of our first algorithm. We then present another algorithm to transform one circuit into its equivalent affine circuit by synthesizing a reversible circuit and inserting it in front of the original circuit. To our knowledge, this is the first work to automatically synthesize an affine equivalent circuit for any given circuit and the first to do this by combining reversible circuit and non-reversible circuit.},
  archive      = {J_COMJNL},
  author       = {Zeng, Xiao and Yang, Guowu and Song, Xiaoyu and Perkowski, Marek A and Chen, Gang},
  doi          = {10.1093/comjnl/bxac072},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2220-2229},
  shortjournal = {Comput. J.},
  title        = {Detecting affine equivalence of boolean functions and circuit transformation},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavior analysis-based IoT services for crowd management.
<em>COMJNL</em>, <em>66</em>(9), 2208–2219. (<a
href="https://doi.org/10.1093/comjnl/bxac071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the world population growing exponentially reaching 7.8 billion people in 2020, the issue of crowd management has become more difficult especially when the situation requires social distancing (e.g. due to COVID-19). The Internet of Things (IoT) technology can help in tackling such issues. In this article, we propose a behavior analysis-based IoT services architecture for crowd management. We propose to use a behavior analysis approach based on using generative model as Hidden Markov Model to help crowd managers to make good decisions in invoking IoT services. The proposed approach is based on sectioning video segments captured from surveillance cameras of locations that require crowd management into spatio-temporal flow-blocks for marginalization of arbitrarily dense flow field. Then, each flow-block is classified as normal and abnormal. To demonstrate our approach, we used a real case study where crowd management is required namely, Muslim’s pilgrimage (i.e. Hajj and Umrah), where real dataset is used for experimenting. The results of the experiments we have conducted are promising in real-time performance. Such results are expected to compare favorably to those found in the literature by other researchers.},
  archive      = {J_COMJNL},
  author       = {Noor, Talal H},
  doi          = {10.1093/comjnl/bxac071},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2208-2219},
  shortjournal = {Comput. J.},
  title        = {Behavior analysis-based IoT services for crowd management},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature extraction based deep indexing by deep fuzzy
clustering for image retrieval using jaro winkler distance.
<em>COMJNL</em>, <em>66</em>(9), 2191–2207. (<a
href="https://doi.org/10.1093/comjnl/bxac070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval aims to locate similar image based on given query from large-sized image dataset. In the recent decades, different methods are developed to retrieve similar images, but increasing the retrieval performance is a challenging task. Hence, an effective method named Lion Henry Gas Solubility Optimization-based Deep Fuzzy Clustering (LHGSO-based DFC) is developed to increase the retrieval efficiency of image retrieval. Based on the features extraction, images are indexed efficiently to the corresponding cluster using clustering model. The proposed method retrieves the relevant images with respect to query image based on the similarity. The similarity measure is computed using Jaro Winkler distance, in which the images corresponding to the cluster that have higher similarity measure is retrieved more effectively. The proposed method achieved higher performance in terms of the metrics, like ; -measure, precision and recall with the values of 0.887, 0.891 and 0.882.},
  archive      = {J_COMJNL},
  author       = {Kumar, B Mathan and Ainapure, Bharati S and Singh, Suryabhan Pratap and Vyas, Sumit},
  doi          = {10.1093/comjnl/bxac070},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2191-2207},
  shortjournal = {Comput. J.},
  title        = {Feature extraction based deep indexing by deep fuzzy clustering for image retrieval using jaro winkler distance},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deriving homing sequences for finite state machines with
timeouts. <em>COMJNL</em>, <em>66</em>(9), 2181–2190. (<a
href="https://doi.org/10.1093/comjnl/bxac069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State identification is the well-known problem in the automata theory that is aimed to determining the current or initial state of a system under test and this fact is widely used in the model-based testing of software and hardware systems. When modern systems are modeled, it is necessary to take into account the timed aspects and for this reason classical Finite State Machines (FSM) are extended by clock variables. In this work, we study the homing problem for FSMs with timeouts (TFSM). For this purpose, we introduce the notion of a timed homing sequence (HS) that is different from that for classical FSMs and propose a method for checking the existence and deriving a timed HS if it exists. A proposed method is based on the FSM abstraction of a TFSM, i.e. on a classical FSM that partially describes the behavior of a corresponding TFSM and inherits many of its properties. Since timeouts allow the system to move from state to state without input impact, we define a timed HS as a sequence that sets a TFSM to a stable state where the system can stay infinitely long waiting for an input.},
  archive      = {J_COMJNL},
  author       = {Tvardovskii, Aleksandr and Yevtushenko, Nina},
  doi          = {10.1093/comjnl/bxac069},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2181-2190},
  shortjournal = {Comput. J.},
  title        = {Deriving homing sequences for finite state machines with timeouts},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy inference-based decision support system for disease
diagnosis. <em>COMJNL</em>, <em>66</em>(9), 2169–2180. (<a
href="https://doi.org/10.1093/comjnl/bxac068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease diagnosis is an exciting task due to many associated factors. Inaccuracy in the measurement of a patient’s symptoms and the medical expert’s expertise has some limitations capacity to articulate cause affects the diagnosis process when several connected variables contribute to uncertainty in the diagnosis process. In this case, a decision support system that can assist clinicians in developing a more accurate diagnosis has a lot of potentials. This work aims to deploy a fuzzy inference-based decision support system to diagnose various diseases. Our suggested method distinguishes new cases based on illness symptoms. Distinguishing symptomatic disorders becomes a time-consuming task in most cases. It is critical to design a system that can accurately track symptoms to identify diseases using a fuzzy inference system (FIS). Different coefficients were used to predict and compute the severity of the predicted diseases for each sign of disease. This study aims to differentiate and diagnose COVID-19, typhoid, malaria and pneumonia. The FIS approach was utilized in this study to determine the condition correlating with input symptoms. The FIS method demonstrates that afflictive illness can be diagnosed based on the symptoms. Our decision support system’s findings showed that FIS might be used to identify a variety of ailments. Doctors, patients, medical practitioners and other healthcare professionals could benefit from our suggested decision support system for better diagnosis and treatment.},
  archive      = {J_COMJNL},
  author       = {Alam, Talha Mahboob and Shaukat, Kamran and Khelifi, Adel and Aljuaid, Hanan and Shafqat, Malaika and Ahmed, Usama and Nafees, Sadeem Ahmad and Luo, Suhuai},
  doi          = {10.1093/comjnl/bxac068},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2169-2180},
  shortjournal = {Comput. J.},
  title        = {A fuzzy inference-based decision support system for disease diagnosis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric searchable encryption without false positive and
its applications. <em>COMJNL</em>, <em>66</em>(9), 2155–2168. (<a
href="https://doi.org/10.1093/comjnl/bxac067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a prominent cryptographic tool, geometric searchable encryption (GSE) can be applied in many scenarios, such as location-based services (LBS), social networks and vehicle networks. Unfortunately, most of existing searchable encryption schemes supporting the functionality of geometric range searches suffer from false positives, which will lead people to make a wrong decision and further raise some serious consequences such as financial loss. In addition, some of them are designed under a symmetric system, which is not enough flexible deployed in LBS since in a symmetric system only a private key holder creates ciphertext, whereas in a public-key system anyone who holds a public key can produce ciphertext. In this paper, we intend to design a novel GSE scheme without any false positive under a public-key system supporting arbitrary geometric area searches, which is able to guarantee an accurate query result. Toward this goal, we develop a novel technique in handling the relation between a point and any convex polygon in combination with an inner product encryption, which is able to support arbitrary convex polygon range searches without any false positive. A comprehensive experiment demonstrates that, compared with the known schemes, our scheme possesses a 100\% accuracy as well as an acceptable efficiency in the sense that it can guarantee that all files retrieved by users are exactly matched ones. Finally, we provide two practical examples of our GSE scheme: privacy-preserving friend-nearby notification with a common point of interest and privacy-preserving parking monitor and guiding system.},
  archive      = {J_COMJNL},
  author       = {Chen, Zhenhua and Nie, Jingjing and Li, Zhanli and Ge, Chunpeng and Susilo, Willy},
  doi          = {10.1093/comjnl/bxac067},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2155-2168},
  shortjournal = {Comput. J.},
  title        = {Geometric searchable encryption without false positive and its applications},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Target-specific sentiment analysis method combining
word-masking data enhancement and adversarial learning. <em>COMJNL</em>,
<em>66</em>(9), 2138–2154. (<a
href="https://doi.org/10.1093/comjnl/bxac066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target-specific sentiment analysis is an emerging topic in the field of text mining but current approaches to deriving the polarity of a sentence suffer from two main drawbacks: on one hand, we lack of a large and well-curated corpus, and on the other hand, current solutions based on deep learning are particularly vulnerable to the attack of adversarial samples. A novel target-specific sentiment classification method is proposed. Firstly, the method of masking target entities is applied to replace synonyms and insert words randomly; secondly, the target-specific sentiment classification model of adversarial learning is constructed with six baseline models; finally, we combine data enhancement and adversarial learning to construct target-specific sentiment classification model. Experimental results show that Macro-F1 values are improved by 0.30–2.91, 0.88–2.42 and 0.13–1.94\% compared to the six baseline models by using Laptop14, Restaurant14 and Twitter original datasets, respectively, using Adversarial learning. Using word-masking data enhancement samples and Adversarial learning from Laptop14, Restaurant14 and Twitter shows that Macro-F1 values are improved by 0.9–2.64, 1.59–3.09 and 0.18–1.71\% compared to the six baseline (SC), respectively. Our method can effectively improve the quality of samples, it improves the classification performance and the capability of adversarial samples defense.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Dai, Shanghong and Fiumara, Giacomo and De Meo, Pasquale},
  doi          = {10.1093/comjnl/bxac066},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2138-2154},
  shortjournal = {Comput. J.},
  title        = {Target-specific sentiment analysis method combining word-masking data enhancement and adversarial learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Construction and validation of early software size
estimation models based on ADAF-adjusted ACD metrics. <em>COMJNL</em>,
<em>66</em>(9), 2123–2137. (<a
href="https://doi.org/10.1093/comjnl/bxac065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software size estimation is a vital activity of software project planning and management. Early software size estimation is a challenging task due to the limited information available during the early phases of software development. The goal of this paper is to construct and validate early software size estimation models based on four analysis-to-design adjustment factor (ADAF)-adjusted analysis class diagram metrics (i.e. ADAF-adjusted number of classes, ADAF-adjusted number of attributes, ADAF-adjusted number of methods and ADAF-adjusted number of relationships) using stepwise multiple linear regression and leave-one-out cross-validation. Furthermore, the prediction accuracy of the best-performing proposed model is also compared with the model based on objective class points. The results of this comparison reveal that our proposed method reduces errors significantly (i.e. on average, 16\% reduction in mean absolute residual and 24\% reduction in mean squared error).},
  archive      = {J_COMJNL},
  author       = {Daud, Marriam and Afzal Malik, Ali},
  doi          = {10.1093/comjnl/bxac065},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2123-2137},
  shortjournal = {Comput. J.},
  title        = {Construction and validation of early software size estimation models based on ADAF-adjusted ACD metrics},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The a-average degree edge-connectivity of bijective
connection networks. <em>COMJNL</em>, <em>66</em>(9), 2118–2122. (<a
href="https://doi.org/10.1093/comjnl/bxac064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conditional edge-connectivity is an important parameter to evaluate the reliability and fault tolerance of multi-processor systems. The ; -dimensional bijective connection networks ; contain hypercubes, crossed cubes, Möbius cubes and twisted cubes, etc. The conditional edge-connectivity of a connected graph ; is the minimum cardinality of edge sets, whose deletion disconnects ; and results in each remaining component satisfying property ; . And let ; be the edge set as desired. For a positive integer ; , if ; denotes the property that the average degree of each component of ; is no less than ; , then the conditional edge-connectivity can be called the ; -average degree edge-connectivity ; . In this paper, we determine that the exact value of the ; -average degree edge-connectivity of an ; -dimensional bijective connection network ; is ; for each ; and ; .},
  archive      = {J_COMJNL},
  author       = {Yang, Yayu and Zhang, Mingzu and Meng, Jixiang and Chen, Rongda},
  doi          = {10.1093/comjnl/bxac064},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2118-2122},
  shortjournal = {Comput. J.},
  title        = {The a-average degree edge-connectivity of bijective connection networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coordinate system-based trust-aware web services composition
in edge and cloud environment. <em>COMJNL</em>, <em>66</em>(9),
2102–2117. (<a href="https://doi.org/10.1093/comjnl/bxac063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the next few years, the number of devices connected to the web will increase dramatically. This has encouraged the development of complexes and intelligent service-based applications using new paradigms and technologies such as edge computing and cloud computing. These applications are mainly built by the interaction between heterogonous web services. Answering end-users’ queries demands that these services collect, integrate and communicate sensitive data, which makes the vulnerability of data a serious issue. Thereby, this issue is a critical problem in the automatic services composition while users are frequently unquieted about the security and control of their sensitive data. To address this problem, our main idea is that avoiding disclosure of sensitive information can be ensured by selecting services where their mutual trust is high while preserving a good quality of service (QoS) as possible. A new Heuristic Trust-aware Web Services Composition Approach (CWS_SMA) is proposed. CWS_SMA aims to: (i) compute an optimal trust and QoS-aware web services composition based on the mathematical coordinate system, (ii) improve the composition response time based on a set of cooperative intelligent agents. Eventually, the simulations results show the effectiveness of the proposed solution.},
  archive      = {J_COMJNL},
  author       = {Brahmi, Zaki and Selmi, Afef},
  doi          = {10.1093/comjnl/bxac063},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2102-2117},
  shortjournal = {Comput. J.},
  title        = {Coordinate system-based trust-aware web services composition in edge and cloud environment},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated learning communication-efficiency framework via
corset construction. <em>COMJNL</em>, <em>66</em>(9), 2077–2101. (<a
href="https://doi.org/10.1093/comjnl/bxac062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) can learn a shared global model across multiple client devices without breaching privacy requirements. But an essential challenge is that devices in FL usually have limited network bandwidth, resulting in inefficient communication as an important bottleneck for FL implementation. Current studies try to overcome this shortcoming by compressing the number of model update bits uploaded by every client. But they did not explore the underlying reason why redundant parameters are generated. In this paper, we propose Corset-Based Federated Learning framework (CBFL) —a novel FL communication framework from the perspective of redundancy data. Instead of training full datasets on a regular network model, CBFL trains a much smaller evolution network model on extracted ; , which intrinsically reduces the overall transmission bits and obtains efficient computation while maintaining a desirable model accuracy. In CBFL, a novel Fedcorset construction algorithm at selected clients and a further distributed model evolution scheme to fit the constructed ; are included. The training model size is dynamically adapted to the corset, either removing a fraction of unimportant or adding important connections at each communication iteration. Experimental results show that CBFL transfers about 13\% of communication bits and saves around 56\% computing time while having only 2\% destination in model accuracy.},
  archive      = {J_COMJNL},
  author       = {Li, Kaiju and Wang, Hao},
  doi          = {10.1093/comjnl/bxac062},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {2077-2101},
  shortjournal = {Comput. J.},
  title        = {Federated learning communication-efficiency framework via corset construction},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Adaptive laplacian support vector machine
for semi-supervised learning. <em>COMJNL</em>, <em>66</em>(8), 2075. (<a
href="https://doi.org/10.1093/comjnl/bxac046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxac046},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2075},
  shortjournal = {Comput. J.},
  title        = {Correction to: Adaptive laplacian support vector machine for semi-supervised learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPDPOA: Student psychology dragonfly political optimizer
algorithm-based soil moisture and heat-level prediction for plant health
monitoring in internet of things. <em>COMJNL</em>, <em>66</em>(8),
2059–2074. (<a href="https://doi.org/10.1093/comjnl/bxac096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article devised an effective Student Psychology-based Dragonfly Political Optimizer (SPDPOA) for predicting heat level and soil moisture to monitor plant health in the Internet of Things (IoT). The developed SPDPOA is modeled by integrating the Student Psychology-based Optimization (SPBO) algorithm, Dragonfly Algorithm (DA) and Political optimizer (PO), respectively. The prediction process is done in the base station (BS), which gathers the IoT nodes’ information through optimal Cluster Head (CH) using Deep Recurrent Neural Network (Deep RNN). Moreover, the CH selection and routing process are established using a developed SPDPOA scheme. The data transformation and feature selection processes are done based on Box-Cox transformation and wrapper model, correspondingly, which helps in the selection of best features. Moreover, the developed SPDPOA scheme attained better performance in Packet Delivery Ratio (PDR), energy and testing accuracy of 0.7232, 0.6342 J and 0.9372, respectively.},
  archive      = {J_COMJNL},
  author       = {Muppidi, Satish and Bhamidipati, Kishore and Arumugam, Sajeev Ram},
  doi          = {10.1093/comjnl/bxac096},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2059-2074},
  shortjournal = {Comput. J.},
  title        = {SPDPOA: Student psychology dragonfly political optimizer algorithm-based soil moisture and heat-level prediction for plant health monitoring in internet of things},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimized deep belief network for land cover
classification using synthetic-aperture radar images and landsat images.
<em>COMJNL</em>, <em>66</em>(8), 2043–2058. (<a
href="https://doi.org/10.1093/comjnl/bxac077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper intends to propose an automated deep learning-based land cover classification model of remote sensing images. The model includes (i) pre-processing, (ii) feature extraction and (iii) classification. The captured synthetic-aperture radar (SAR) and Landsat-8 images are initially pre-processed using the Gabor filtering model. Subsequently, from SAR images the gray-level-co-occurrence matrix-based texture characteristics are extracted, and temperature vegetation index-based characteristics, normalized vegetation index-based features, normalized difference index-based features and coloration index features are extracted from Landsat-8 images. Finally, the extracted features are subjected to an optimized deep belief network (DBN), where the weight is fine-tuned by the optimization logic. For this, a new Sunflower adopted Red Deer (SARD) algorithm is introduced in this work that hybrids the concept of Red Deer algorithm and Sunflower optimization. The performance of the proposed classification model is compared over other conventional models concerning different measures. Especially, the accuracy of the presented work (SARD+DBN) for Testcase3 is 5, 7, 6 and 30\% better than existing DA + DBN, JA + DBN, SLnO+DBN and LA + DBN methods, respectively.},
  archive      = {J_COMJNL},
  author       = {Bhatt, Abhishek and Thakur, Vandana},
  doi          = {10.1093/comjnl/bxac077},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2043-2058},
  shortjournal = {Comput. J.},
  title        = {An optimized deep belief network for land cover classification using synthetic-aperture radar images and landsat images},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relationship between component connectivity and component
diagnosability of some regular networks. <em>COMJNL</em>,
<em>66</em>(8), 2033–2042. (<a
href="https://doi.org/10.1093/comjnl/bxac061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a kind of conditional connectivity, component connectivity is an improvement of traditional connectivity, which is conducive to enhance the reliability of the network. To be specific, the ; -component connectivity of a network ; , written as ; , is defined as the minimum number of all node cuts whose removal causes the remaining network to have at least ; components. Component diagnosability, as another measure of network reliability, is usually related to the number of components in the remaining network. The ; -component diagnosability, written as ; , is defined as the maximum number of faulty sets such that at least ; components in the surviving network and all faulty nodes can be diagnosed. This paper mainly explores the relationship between component connectivity and component diagnosability of some regular networks. Once knowing the component connectivity of such a network, with the help of this relationship, we can easily obtain the component diagnosability of the network. Furthermore, we apply this relationship to some famous regular networks to obtain their component diagnosabilities under the PMC model.},
  archive      = {J_COMJNL},
  author       = {Sun, Xueli and Fan, Jianxi and Cheng, Baolei and Zhou, Jingya and Wang, Yan},
  doi          = {10.1093/comjnl/bxac061},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2033-2042},
  shortjournal = {Comput. J.},
  title        = {Relationship between component connectivity and component diagnosability of some regular networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated computer-aided diagnosis of diabetic retinopathy
based on segmentation and classification using k-nearest neighbor
algorithm in retinal images. <em>COMJNL</em>, <em>66</em>(8), 2011–2032.
(<a href="https://doi.org/10.1093/comjnl/bxac059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is an eye oddity where the human retina is afflicted because of the ever-increasing quantity of insulin in the blood. It leads to the loss of sight. Preliminary diagnosis of DR assists to improve to inhibit future injury. Proper DR screening has been recognized as an economical way to accumulate health services. Automated retinal analysis become known as the most significant screening approach for primitive DR diagnosis, which leads to diminishing the workload related to manual screening and also, cost-effective and less time-consuming efforts. In the proposed work, the preprocessing, removal of applicant lesion pixels, and formulation of feature set have been examined which is fully appropriate for the classification task. In preprocessing approach, the framework removes the unwanted pixels, eliminates the optic disc, and extraction of the blood vessels from the retinal images. Morphological operations are applied to extract the boundaries of the blood vessels and then 2D discrete wavelet decomposition is applied to estimate the horizontal, vertical and diagonal coefficients. The candidate lesion pixels i.e. dark and bright DR pixels are detected using an adaptive threshold that uses local statistical, geometrical, and location-based characteristics of the background image. The extracted feature set is processed using a K-nearest neighbor (KNN) classifier with 80\% of training data and 20\% of testing data to diagnose the severity level of the disease. The proposed scheme is evaluated by the DIARETDB1 benchmark dataset with the performance parameters, i.e. 95\% of accuracy, 92.6\% of sensitivity and 87.56\% specificity achieved with less computation time required.},
  archive      = {J_COMJNL},
  author       = {Kaur, Jaspreet and Kaur, Prabhpreet},
  doi          = {10.1093/comjnl/bxac059},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2011-2032},
  shortjournal = {Comput. J.},
  title        = {Automated computer-aided diagnosis of diabetic retinopathy based on segmentation and classification using K-nearest neighbor algorithm in retinal images},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The diagnosability of interconnection networks with missing
edges and broken-down nodes under the PMC and MM* models.
<em>COMJNL</em>, <em>66</em>(8), 2000–2010. (<a
href="https://doi.org/10.1093/comjnl/bxac058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosability is often considered as an important factor for measuring the self-diagnostic ability of network systems. However, classic system-level diagnosis focuses only on processor faults and ignores the objective reality of communication faults. Under real circumstances, missing edges and node failures usually occur simultaneously in multiprocessor systems (called hybrid fault circumstances). Therefore, it is important to study the diagnosability of multiprocessor systems under hybrid fault circumstances. In this paper, we propose several diagnosabilities of interconnection networks with missing edges and faulty nodes. By exploring some important relationships between diagnosability and the minimum degree of a network under hybrid fault circumstances, we present and prove the diagnosability of several classic interconnection networks, including BC (bijective connection) networks, star graphs, folded hypercubes, exchanged hypercubes, exchanged crossed cubes, k-ary n-cubes, bubble-sort star graphs and balanced hypercubes, with missing edges and broken-down nodes under the PMC (Preparata, Metze and Chien) and MM; (Maeng and Malek) models.},
  archive      = {J_COMJNL},
  author       = {Guo, Chen and Liu, Qiuming and Xiao, Zhifang and Peng, Shuo},
  doi          = {10.1093/comjnl/bxac058},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {2000-2010},
  shortjournal = {Comput. J.},
  title        = {The diagnosability of interconnection networks with missing edges and broken-down nodes under the PMC and MM* models},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of adaptive time-weighted dynamic time warping
for time series vegetation classification using satellite images in
solapur district. <em>COMJNL</em>, <em>66</em>(8), 1982–1999. (<a
href="https://doi.org/10.1093/comjnl/bxac057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global seasonal change and continued rapid growth have maximized the need to assess the urban dwellers’ depend on vegetation for their lives, and also in the urban ecosystem resources. The conventional outcomes devoted to cropland mapping, with the help of high-quality remote sensing data’s. This paper is to investigate and develop a new methodology that pertains to time series analysis for classifying the type of vegetation in a farm area of Ujani Dam located in Solapur District, Maharastra. The proposed model develops a novel adaptive time-weighted dynamic time warping (ATWDTW) for the time series analysis using the satellite images. The gathered satellite images from the farm are processed initially and subjected to analysis by ATWDTW. The TWDTW concept is optimally tuned by the new hybrid meta-heuristic algorithm termed moth flame-based bird swarm optimization (MF-BSA) for enhancing the classification performance. Regarding the false omission rate of the proposed MF-BSA-ATWDTW model attains 5.56\% and 29.9\% lower than SVM and K-means respectively. From the analysis, it is possible to get a deep insight into the vegetation to be done in each year, and the comparative analysis proves that the proposed model is further adaptable for experimental use in relating and explaining environmental and ecological time-series information.},
  archive      = {J_COMJNL},
  author       = {Kumawat, Manisha and Khaparde, Arti},
  doi          = {10.1093/comjnl/bxac057},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1982-1999},
  shortjournal = {Comput. J.},
  title        = {Development of adaptive time-weighted dynamic time warping for time series vegetation classification using satellite images in solapur district},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hamiltonian properties of the data center network HSDC with
faulty elements. <em>COMJNL</em>, <em>66</em>(8), 1965–1981. (<a
href="https://doi.org/10.1093/comjnl/bxac055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data center network HSDC is a superior candidate for building large-scale data centers, and strikes a good balance among diameter, bisection width, incremental scalability and other important characteristics in contrast to the state-of-the-art data center network architectures. The Hamiltonian property is an important indicator to measure the reliability of a network. In this paper, we study the Hamiltonian properties of HSDC’s logic graph ; . Firstly, we prove that ; is Hamiltonian-connected for ; . Secondly, we propose an ; algorithm for finding a Hamiltonian path between any two distinct nodes in ; , where ; is the number of nodes in ; . Furthermore, we consider the Hamiltonian properties of ; with faulty elements, and prove that ; is ; -fault-tolerant Hamiltonian-connected and ; -fault-tolerant Hamiltonian for ; .},
  archive      = {J_COMJNL},
  author       = {Dong, Hui and Fan, Jianxi and Cheng, Baolei and Wang, Yan and Xu, Li},
  doi          = {10.1093/comjnl/bxac055},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1965-1981},
  shortjournal = {Comput. J.},
  title        = {Hamiltonian properties of the data center network HSDC with faulty elements},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D flattering amplified neural network-based segmentation of
amygdala and hippocampus. <em>COMJNL</em>, <em>66</em>(8), 1949–1964.
(<a href="https://doi.org/10.1093/comjnl/bxac054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent emergence in deep learning resulted in significant improvement in the segmentation accuracy of sub cortical brain structures like hippocampus and amygdala. The traditional methods of segmentation cannot produce an ideal segmentation result that exhibits issues like redundant computations, inconsistencies, coefficient variations and motion artifacts. Therefore, in this paper, an improved 3D Flatteringly Amplified Neural Network model for biomedical imaging is efficiently proposed, which can make full use of the 3D spatial information of MRI image itself to overcome the inconsistency of segmented images along with equalizing the coefficient variation of tiny region of brain image segmentation. Also while equalizing the coefficient, certain significant minute details are lost due to motion artifacts hence, the robust Amyg-Hippo Seg algorithm has been introducing that extracts the features through deep learning, and achieve high-precision segmentation, it reduced the computational complexity without neglecting minute features. In addition, the Daytona dropout function provides uncertainty information and reduces over-fitting problems. The outcome of the proposed work efficiently segments the most significant regions of hippocampus and amygdala with 97.4\% accuracy.},
  archive      = {J_COMJNL},
  author       = {Smitha, J C and Jane, Ambily and Chandran, Lekshmi},
  doi          = {10.1093/comjnl/bxac054},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1949-1964},
  shortjournal = {Comput. J.},
  title        = {3D flattering amplified neural network-based segmentation of amygdala and hippocampus},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crime type prediction in saudi arabia based on intelligence
gathering. <em>COMJNL</em>, <em>66</em>(8), 1936–1948. (<a
href="https://doi.org/10.1093/comjnl/bxac053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the biggest social problems currently facing major cities around the globe is the high rate of crime. The largest part of the social-economic loss globally is ascribed to criminal activities. Crime also has direct impacts on the nation’s economy, social constructs and country’s global repute. Inadequate policing capital is one of the biggest challenges facing many global economies. As a result, these resources have to be rationed. This implies that some areas will not be covered extensively thus providing favorable environs for perpetrators. To combat crime, more innovative security measures are needed. In this sense, traditional methods are being replaced with modern approaches of machine learning systems that can predict the occurrence of crime. These crime forecasts can be used by legislatures and law enforcers to make effective and informed approaches that can efficiently eradicate criminals and facilitate nation building. This paper seeks to review the literature on the application of machine learning models in crime prediction and to find the influences that have an impact on crimes in Saudi Arabia. The results show that after the four models were trained and tested, the random forest classifier had the highest accuracy of 97.84\%.},
  archive      = {J_COMJNL},
  author       = {Albahli, Saleh and Albattah, Waleed},
  doi          = {10.1093/comjnl/bxac053},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1936-1948},
  shortjournal = {Comput. J.},
  title        = {Crime type prediction in saudi arabia based on intelligence gathering},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resistance distances in simplicial networks.
<em>COMJNL</em>, <em>66</em>(8), 1922–1935. (<a
href="https://doi.org/10.1093/comjnl/bxac052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that in many real networks, such as brain networks and scientific collaboration networks, there exist higher order nonpairwise relations among nodes, i.e. interactions between more than two nodes at a time. This simplicial structure can be described by simplicial complexes and has an important effect on topological and dynamical properties of networks involving such group interactions. In this paper, we study analytically resistance distances in iteratively growing networks with higher order interactions characterized by the simplicial structure that is controlled by a parameter ; . We derive exact formulas for interesting quantities about resistance distances, including Kirchhoff index, additive degree-Kirchhoff index, multiplicative degree-Kirchhoff index, as well as average resistance distance, which have found applications in various areas elsewhere. We show that the average resistance distance tends to a ; -dependent constant, indicating the impact of simplicial organization on the structural robustness measured by average resistance distance.},
  archive      = {J_COMJNL},
  author       = {Zhu, Mingzhe and Xu, Wanyue and Zhang, Zhongzhi and Kan, Haibin and Chen, Guanrong},
  doi          = {10.1093/comjnl/bxac052},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1922-1935},
  shortjournal = {Comput. J.},
  title        = {Resistance distances in simplicial networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The local diagnosability of a class of cayley graphs with
conditional faulty edges under the PMC model. <em>COMJNL</em>,
<em>66</em>(8), 1913–1921. (<a
href="https://doi.org/10.1093/comjnl/bxac051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosability of a multiprocessor system is of great significance in measuring the reliability and faulty tolerance of interconnection networks. In this paper, we firstly study the diagnosability of a class of Cayley graphs ; under the PMC model. We prove that ; keeps the strong local diagnosability property even if it has the set ; of ; faulty edges and ; is maximum number of faulty edges, where ; is the regular degree of ; . Secondly, we study the diagnosability of ; with conditional faulty edges under the PMC model. We prove that ; keeps strong local diagnosability property even if it has the set ; of ; faulty edges, provided that each vertex of ; is incident with at least two fault-free edges, where ; is maximum number of faulty edges. Finally, we prove that ; keeps strong local diagnosability property no matter how many edges are faulty, provided that each vertex of ; is incident with at least four fault-free edges.},
  archive      = {J_COMJNL},
  author       = {Ren, Yunxia and Wang, Shiying},
  doi          = {10.1093/comjnl/bxac051},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1913-1921},
  shortjournal = {Comput. J.},
  title        = {The local diagnosability of a class of cayley graphs with conditional faulty edges under the PMC model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative analysis of overlap community detection
techniques on social media platform. <em>COMJNL</em>, <em>66</em>(8),
1893–1912. (<a href="https://doi.org/10.1093/comjnl/bxac050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structure over social media (SM) is the collaborative group of globally spread users with identical characteristics and ideologies. The collective features of SM are inherent with both the implicit and explicit nature of end-users. This paper presents an analytical and methodological community detection framework to bind passive users’ implicit and explicit nature after scrutinizing graphical data to identify seed nodes and communities. Moreover, this work provides the concept of the unsupervised machine learning approach over the graphical perspective of SM to identify the trade-off between similarity of nodes attributes and density of connections for social theories. Subsequently, this paper evaluates a comprehensive analysis of the benchmark community detection algorithm (CDA) Label Propagation Algorithm (LPA), Clique Percolation Method (CPM), Democratic Estimate of the Modular Organization of a Network (DEMON) and Non-Negative Matrix Factorization (NMF). The evaluation has been carried out over modularity and normalized mutual information of resultant structured community on six real-time SM data set. The performance of benchmark CDAs is significantly increased after incorporating social theories. NMF, DEMON, CPM and LPA gained the highest improvement over Zachary’s Karate Club data sets, i.e. approximate 26.91\%, 21.68\%, 18.79\%, 19.96\%, respectively.},
  archive      = {J_COMJNL},
  author       = {Meena, Pawan and Pawar, Mahesh and Pandey, Anjana},
  doi          = {10.1093/comjnl/bxac050},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1893-1912},
  shortjournal = {Comput. J.},
  title        = {Comparative analysis of overlap community detection techniques on social media platform},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network traffic classification based on a deep learning
approach using NetFlow data. <em>COMJNL</em>, <em>66</em>(8), 1882–1892.
(<a href="https://doi.org/10.1093/comjnl/bxac049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic classification is of fundamental importance to a wide range of network activities, such as security monitoring, accounting, quality of service and forecasting for long-term provisioning purposes. This task has been increasingly implemented using machine learning methods due to the inability of conventional approaches to accommodate the increasing use of encryption. However, the application of machine learning methods to network traffic classification based on sampled NetFlow data is poorly developed despite the fact that NetFlow is a widely extended monitoring solution routinely employed by network operators. This study addresses this issue by proposing a network traffic classification module using NetFlow data in conjunction with a deep neural network. The performance of the proposed classification module is demonstrated by its application to two real-world datasets, and an average classification accuracy of 95\% is obtained for ; 1.4 million test cases. Moreover, the performance of the proposed classifier is demonstrated to be superior to three other state-of-the-art classifiers. Accordingly, the proposed module represents a promising alternative for network traffic classification.},
  archive      = {J_COMJNL},
  author       = {Long, Zhang and Jinsong, Wang},
  doi          = {10.1093/comjnl/bxac049},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1882-1892},
  shortjournal = {Comput. J.},
  title        = {Network traffic classification based on a deep learning approach using NetFlow data},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tactics and techniques classification in cyber threat
intelligence. <em>COMJNL</em>, <em>66</em>(8), 1870–1881. (<a
href="https://doi.org/10.1093/comjnl/bxac048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Completing the classification of tactics and techniques in cyber threat intelligence (CTI) is an important way to obtain tactics, techniques and procedures (TTPs) and portray the behavior of cyber attacks. However, the high level of abstraction of tactics and techniques information and their presence in CTI, usually in the form of natural language text, make it difficult for traditional manual analysis methods and feature engineering-based machine learning methods to complete the classification of tactics and techniques effectively. Meanwhile, flat deep learning methods do not perform well in classifying more fine-grained techniques due to their inability to exploit the hierarchical relationship between tactics and techniques. Therefore, this paper regards the tactics and techniques of TTPs defined in Adversarial Tactics, Techniques and Common Knowledge knowledge base as labels and proposes a Convolutional Neural Network (CNN) model based on hierarchical knowledge migration and attention mechanism for classifying tactics and techniques in CTI, named HM-ACNN (CNN based on hierarchical knowledge migration and attention mechanism). HM-ACNN classifies tactics and techniques into two phases, and the underlying network model for both phases is the Attention-based CNN network. The first step in HM-ACNN is converting the CTI text into a two-dimensional image based on the word embedding model, and then start training the classification of tactics through the CNN structure based on the attention mechanism before the classification of techniques. Secondly, after the tactics classification training is completed, the tactic-to-technique knowledge migration is then completed by transforming the parameters of the CNN layer and the attention layer in the tactics classification process based on the special hierarchical relationship between tactics and techniques. Then, the classification of techniques is finished by fine-tuning. The experimental results show that HM-ACNN performs well in the tactics and techniques classification tasks, and the metric F1 values reach 93.66\% and 86.29\%, which are better than other models such as CNN, Recurrent Neural Network and CRNN (Recurrent Convolutional Neural Networks).},
  archive      = {J_COMJNL},
  author       = {Yu, Zhongkun and Wang, JunFeng and Tang, BinHui and Lu, Li},
  doi          = {10.1093/comjnl/bxac048},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1870-1881},
  shortjournal = {Comput. J.},
  title        = {Tactics and techniques classification in cyber threat intelligence},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urdu named entity recognition system using deep learning
approaches. <em>COMJNL</em>, <em>66</em>(8), 1856–1869. (<a
href="https://doi.org/10.1093/comjnl/bxac047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is a fundamental part of other natural language processing tasks such as information retrieval, question answering systems and machine translation. Progress and success have already been achieved in research on the English NER systems. However, the Urdu NER system is still in its infancy due to the complexity and morphological richness of the Urdu language. Existing Urdu NER systems are highly dependent on manual feature engineering and word embedding to capture similarity. Their performance lags if the words are previously unknown or infrequent. The feature-based models suffer from complicated feature engineering and are often highly reliant on external resources. To overcome these limitations in this study, we present several deep neural approaches that automatically learn features from the data and eliminate manual feature engineering. Our extension involved convolutional neural network to extract character-level features and combine them with word embedding to handle out-of-vocabulary words. The study also presents a tweets dataset in Urdu, annotated manually for five named entity classes. The effectiveness of the deep learning approaches is demonstrated on four benchmarks datasets. The proposed method demonstrates notable progress upon current state-of-the-art NER approaches in Urdu. The results show an improvement of 6.26\% in the F1 score.},
  archive      = {J_COMJNL},
  author       = {Haq, Rafiul and Zhang, Xiaowang and Khan, Wahab and Feng, Zhiyong},
  doi          = {10.1093/comjnl/bxac047},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1856-1869},
  shortjournal = {Comput. J.},
  title        = {Urdu named entity recognition system using deep learning approaches},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An unobtrusive approach to emotion detection in e-learning
systems. <em>COMJNL</em>, <em>66</em>(8), 1840–1855. (<a
href="https://doi.org/10.1093/comjnl/bxac044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have proved that emotions play vital role in a human’s life. They affect our way of living, making decisions and also our way of learning. There are many methods for emotion detection in e-learning. However, each of them comes with its own set of disadvantages discussed in the literature review. In this paper, the attributes that have been identified are purely unobtrusive in nature; attributes that do not interfere with the learner’s activity and less is known to them that their emotions are being monitored. A methodology is presented to detect the emotions of the learner using keystrokes, mouse clicks, forum discussions and the results of assessments. Machine learning models have been trained and tested to predict the learner’s emotions. The logistic regression performed fairly well in comparison to the other algorithms with an accuracy of about 85\% and cross-validation score of 86\%. During this study, interesting patterns are observed in learner’s emotions that are discussed. Future directions include collecting diverse data to understand emotions of learners from various age groups and observing patterns in their emotional changes.},
  archive      = {J_COMJNL},
  author       = {Rasheed, Fareeha and Wahid, Abdul},
  doi          = {10.1093/comjnl/bxac044},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1840-1855},
  shortjournal = {Comput. J.},
  title        = {An unobtrusive approach to emotion detection in E-learning systems},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gambling domain name recognition via certificate and textual
analysis. <em>COMJNL</em>, <em>66</em>(8), 1829–1839. (<a
href="https://doi.org/10.1093/comjnl/bxac043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-line gambling is the key illegal behaviour of public security department in most countries due to the potential threat to cyberspace security and social stability. Hence, the research on gambling domain names (GDN) classification is quite important and in great demand for academia and industry. Till now, there is very little research work on this topic. Most of the GDN training datasets in previous work were chosen from GDN blacklists provided by publicly available data sources, and the authors did not verify the authenticity and accuracy of these datasets, and the classification results are not particularly satisfactory. In this paper, certificated and textual analysis-based classification method CT-GDNC is proposed to get GDN training data set with an accuracy of 0.9776 and significantly improve the classification results of GDN. The exhaustive comparative experiments on 10K GDN obtained via Bert fine-tuning model and 10K benign data collected from Alex Top 1 million list show that the proposed method achieves new baseline result for GDN classification with classification accuracy 0.9936, precision 0.9936, F1 0.9936 and recall 0.9939.},
  archive      = {J_COMJNL},
  author       = {Sun, GuoYing and Ye, Feng and Chai, Tingting and Zhang, Zhaoxin and Tong, Xiaojun and Prasad, Shitala},
  doi          = {10.1093/comjnl/bxac043},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1829-1839},
  shortjournal = {Comput. J.},
  title        = {Gambling domain name recognition via certificate and textual analysis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale emulation network topology partition based on
community detection with the weight of vertex similarity.
<em>COMJNL</em>, <em>66</em>(8), 1817–1828. (<a
href="https://doi.org/10.1093/comjnl/bxac041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of physical resources, if a large-scale emulation network environment composed of millions of vertices and edges is constructed by virtualization technology, the whole network topology should be partitioned into a set of subnets. The topology partition is a work of graph partition. The existing topology partition methods have shortcomings, such as low efficiency and poor practicability, especially for large-scale network topology. The emulation network is a kind of complex network and has the characteristics of community structure. Therefore, we proposed LENTP (large-scale emulation network topology partition) based on the community detection with the weight of the vertex similarity for large-scale topology partition. In the first stage, the tree-structured area compression reduces the topology scales significantly to improve partition efficiency. And then, the improved Louvain algorithm is used to topology partitioning and obtain an initial set of subnets with the minimum number of subnets and remote links. Finally, after repartitioning and merging for the initial subnets, the result of subnets is the final topology partition that reaches the optimization objectives with the conditions of the virtual resources. In the experiment, the method is tested in five groups of network topology with different scales. The results demonstrate that LENTP can partition the network topology over 1 000 000 nodes and significantly improve the running-time efficiency of the network topology partition.},
  archive      = {J_COMJNL},
  author       = {Yan, Jianen and Xu, Haiyan and Li, Ning and Zhang, Zhaoxin},
  doi          = {10.1093/comjnl/bxac041},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1817-1828},
  shortjournal = {Comput. J.},
  title        = {Large-scale emulation network topology partition based on community detection with the weight of vertex similarity},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive visualization of large point clouds using an
autotuning multiresolution out-of-core strategy. <em>COMJNL</em>,
<em>66</em>(7), 1802–1816. (<a
href="https://doi.org/10.1093/comjnl/bxac179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasingly large amount of data acquired into point clouds, from LiDAR (Light Detection and Ranging) sensors and 2D/3D sensors, massive point clouds processing has become a topic with high interest for several fields. Current client-server applications usually use multiresolution out-of-core proposals; nevertheless, the construction of the data structures required is very time-consuming. Furthermore, these multiresolution approaches present problems regarding point density changes between different levels of detail and artifacts due to the rendering of elements entering and leaving the field of view. We present an autotuning multiresolution out-of-core strategy to avoid these problems. Other objectives are reducing loading times while maintaining low memory requirements, high visualization quality and achieving interactive visualization of massive point clouds. This strategy identifies certain parameters, called performance parameters, and defines a set of premises to obtain the goals mentioned above. The optimal parameter values depend on the number of points per cell in the multiresolution structure. We test our proposal in our web-based visualization software designed to work with the structures and storage format used and display massive point clouds achieving interactive visualization of point clouds with more than 27 billion points.},
  archive      = {J_COMJNL},
  author       = {Teijeiro, Diego and Amor, Margarita and Doallo, Ramón and Deibe, David},
  doi          = {10.1093/comjnl/bxac179},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1802-1816},
  shortjournal = {Comput. J.},
  title        = {Interactive visualization of large point clouds using an autotuning multiresolution out-of-core strategy},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel representation and prediction initiative for
underground water by using deep learning technique of remote sensing
images. <em>COMJNL</em>, <em>66</em>(7), 1784–1801. (<a
href="https://doi.org/10.1093/comjnl/bxac101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper intends to introduce a novel groundwater prediction model by inducing the novel hydro indices that are not yet popular in earlier techniques. As per the proposed work, statistical features like mean, median, skewness and kurtosis are estimated. Moreover, the vegetation index includes simple ratio, normalized difference vegetation index, Kauth–Thomas Tasseled cap transformation and infrared index transformation. Furthermore, a novel hydro index is formulated by combining the statistical model function with the vegetation index. Subsequently, the detection process is carried out by ensemble technique, which includes the classifiers like random forest (RF), neural network (NN), support vector machine (SVM) and deep belief network (DBN). The final predicted result is attained from DBN. The performance of the adopted model is computed to the existing models with respect to certain measures. At learning rate 50, the maximum accuracy of the proposed model is 45.65, 34.78, 58.70, 72.83, 18.48 and 23.91\% better than the existing models like SVM, RF, convolutional neural network, K-nearest neighbors, NN and artificial neural network, respectively.},
  archive      = {J_COMJNL},
  author       = {Sureshkumar, Veluguri and Somarajadikshitar, Rajasomashekar and Beeram, B Sarala},
  doi          = {10.1093/comjnl/bxac101},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1784-1801},
  shortjournal = {Comput. J.},
  title        = {A novel representation and prediction initiative for underground water by using deep learning technique of remote sensing images},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social media analysis of customer emotions in pizza
industry. <em>COMJNL</em>, <em>66</em>(7), 1777–1783. (<a
href="https://doi.org/10.1093/comjnl/bxac042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current century, big number of people adopting social media very fastly because social media is very reliable, fast and easy to use. So, many big industries adopting social media also, like Facebook to analyze their customers’ reviews, analyze customers’ emotions and to understand their behavior, which is very helpful in providing facilities to customers. Nowadays bulk of data available is on social media. To increase competitive benefits and assess competitive environment of industries, companies need to analyze social media data as well as customer generated data. To guide large industries in understanding how they may use social media to understand customer emotion and use social media marketing approaches into their marketing strategies to get customer engagement. This paper describes in-depth analysis of pizza industry of United Kingdom to understand their customers’ emotions and get data for analysis from Facebook. We are using January 2018–December 2018 data of five largest pizza chains in United Kingdom: Papa John, Prezzo, Pizza Express, Pizza Hut and Dominos. The results disclose the power of sentimental analysis and social media competitive analysis will put positive impact on business value. At the end of this paper we will provide some recommendations to enhance marketing strategies.},
  archive      = {J_COMJNL},
  author       = {Nasir, Muhammad Umar and Rehmat, Urva and Ahmad, Imran},
  doi          = {10.1093/comjnl/bxac042},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1777-1783},
  shortjournal = {Comput. J.},
  title        = {Social media analysis of customer emotions in pizza industry},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved hill climbing algorithm for graph partitioning.
<em>COMJNL</em>, <em>66</em>(7), 1761–1776. (<a
href="https://doi.org/10.1093/comjnl/bxac039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning is an NP-hard combinatorial optimization problem, and is a fundamental step in distributing workloads on parallel compute systems, circuit placement, and sparse matrix reordering. The proposed heuristic algorithms such as streaming graph partitioning provide solutions to large-scale graph in a reasonable amount of time. However, the ability of breaking out of local minima in existing these methods is very limited as they are simple in reflecting the connectivity between vertices in real graphs with power-law distribution characteristic. As hill climbing algorithm is a local search method, it can be adopted to improve the result of graph partitioning. However, directly adopting the existing hill climbing algorithm to graph partitioning will result in local minima and poor convergence speed during the iterative process. In this paper, we propose an improved hill climbing graph partitioning algorithm based on clustering. Instead of taking a single vertex as a basic unit, the proposed method considers a cluster consisting of a series of vertices as a hill to move during each iteration. The method uses a new metric that considers both balance and edgecuts to look for the most beneficial cluster as the hill. With these improvements, the method provides a strong power to break out of local minima and achieve an adaptive tradeoff between balance and edgecuts. Experimental results on real-world graphs show that the proposed algorithm substantially reduces edgecuts within a controlled imbalance range.},
  archive      = {J_COMJNL},
  author       = {Li, He and Liu, Yanna and Yang, Shuqi and Lin, Yishuai and Yang, Yi and Yoo, Jaesoo},
  doi          = {10.1093/comjnl/bxac039},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1761-1776},
  shortjournal = {Comput. J.},
  title        = {An improved hill climbing algorithm for graph partitioning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel reduced reference image quality assessment based on
formal concept analysis. <em>COMJNL</em>, <em>66</em>(7), 1749–1760. (<a
href="https://doi.org/10.1093/comjnl/bxac038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of image quality provides the confidentiality, safety, quality and transparency of the obtained images. At the same time, there are many existing quality metric approaches that belong to the group of modification alteration procedures (pixel-based metric). These techniques have not been well-matched with perceptual image quality. The Perceptual Human Visual System (HVS) aspect drives our approach; The human visual system (HVS) is the best judge of image quality. This paper presents a new reduced reference image quality metric in the spatial domain that reduces the complexity of the image quality assessment. From the reference and distortion images, we extract four intrinsic features, namely, contrast, entropy, histogram and standard deviation. Then, we build the formal concept analysis matrix for reference and distortion images. Finally, we compare the obtained matrixes to evaluate the image quality. The performance of the proposed technique is assessed using LIVE, TID2013 and CSIQ datasets, and the obtained results are compared in terms of PSNR, SSIM and NCC metrics. Also, a comparison with more recent and relevant approaches is performed to highlight the superior performance of our proposed approach, the experimental results indicated that the proposed approach provides efficient performance among the compression, Gaussian blur, Contrast and add noise distortion types.},
  archive      = {J_COMJNL},
  author       = {AlShaikh, Muath},
  doi          = {10.1093/comjnl/bxac038},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1749-1760},
  shortjournal = {Comput. J.},
  title        = {A novel reduced reference image quality assessment based on formal concept analysis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning disjunctive multiplicity expressions and
disjunctive generalize multiplicity expressions from both positive and
negative examples. <em>COMJNL</em>, <em>66</em>(7), 1733–1748. (<a
href="https://doi.org/10.1093/comjnl/bxac037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of a schema for eXtensible Markup Language (XML) documents has numerous advantages. Unfortunately, many XML documents in practice are not accompanied by a (valid) schema. Therefore, it is essential to devise algorithms to infer schemas from XML documents, where the fundamental task is learning regular expressions. In this paper, we focus on the learning of ; (DMEs), a subclass of regular expressions that are particularly suitable to specify unordered models and have been used as the foundation of the schemas for unordered XML. Previous work for learning DME lacks inference algorithms that support positive and negative examples. Further, presently there has been no algorithm can learn DMEs extended with numeric occurrences. We address these challenges in the present paper and first propose a novel algorithm to learn DMEs from positive and negative examples by using genetic algorithms and parallel techniques. Then we extend DMEs to disjunctive generalized multiplicity expressions (DGMEs), which allow numeric occurrences and develop an algorithm to learn DGMEs from positive and negative examples. Finally, experimental results show that with only positive examples, our algorithm can generate a DME with an acceptable learning time, which can accept all positive examples, and when given both positive and negative examples, we can learn DMEs or DGMEs with high accuracy.},
  archive      = {J_COMJNL},
  author       = {Li, Yeting and Chen, Haiming and Chen, Zixuan},
  doi          = {10.1093/comjnl/bxac037},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1733-1748},
  shortjournal = {Comput. J.},
  title        = {Learning disjunctive multiplicity expressions and disjunctive generalize multiplicity expressions from both positive and negative examples},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Indistinguishable leakage-resilient circuit compiler.
<em>COMJNL</em>, <em>66</em>(7), 1717–1732. (<a
href="https://doi.org/10.1093/comjnl/bxac036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transforming any circuit into a circuit with the same functionality but resilience against well-defined classes of leakage is an essential subject of cryptography. Whereas, according to the relationship between ; and simulated leakage-resilient circuit compiler, the compiling needs a specific mechanism to guarantee that its input is free of any leakage. Unfortunately, it makes it tough to implement this strategy in the context of actual leaks. In this work, we formally propose the indistinguishable bounded leakage-resilient circuit compiler ; to solve the problem mentioned above. Specifically, it specifies that any polynomial-time adversary who derives the bound secret information from the circuit compiler can not distinguish an equivalent compiled circuit ; with the same output. To achieve this goal, first, we utilize the leakage-resilient indistinguishable encoding as its input. Second, it takes advantage of the indistinguishability obfuscation ; program to construct an obfuscated circuit that removes the random coins of the input encoding . Its total running time is the original circuit ; running time plus ; , and the size is independent of the original ; . As this new compiler overcomes the restriction of the simulated-based leakage resilient circuit compiler,it will bring an independent interest result in leakage-resilient cryptography.},
  archive      = {J_COMJNL},
  author       = {Xu, Shiyou and Wang, Jian and Wang, Liangliang},
  doi          = {10.1093/comjnl/bxac036},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1717-1732},
  shortjournal = {Comput. J.},
  title        = {Indistinguishable leakage-resilient circuit compiler},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-graph automata and some applications. <em>COMJNL</em>,
<em>66</em>(7), 1698–1716. (<a
href="https://doi.org/10.1093/comjnl/bxac035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper defines a novel graph constructed on a residuated lattice using a specific definition of the fuzzy graph (called ; -graph). These types of graphs also have applications in libraries, pharmacies and machine facilities. Among these applications, this research presents the ; -graph automaton related to the ; -graph constructed by a minimum zero forcing set. The ; -graph automaton is used to find efficient searching for diseases that have the most similar symptoms. It has been demonstrated that if two ; -graphs are isomorphic, then the related ; -graph automata are isomorphic but not necessarily vice versa. New notions, ; -graph automata, pseudo-isomorphism and self-sufficiency are proved. Besides, by taking advantage of the properties of the residuated lattice, it has been proven that the two ; -graph automata are equivalent if and only if the ; -graph automata contracted of ; -graph automata are equivalent. Ultimately, in light of the above, some related theorems are proved and several examples are provided to illustrate these new notions.},
  archive      = {J_COMJNL},
  author       = {Raisi Sarbizhan, Elham and Mehdi Zahedi, Mohammad and Shamsizadeh, Marzieh},
  doi          = {10.1093/comjnl/bxac035},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1698-1716},
  shortjournal = {Comput. J.},
  title        = {L-graph automata and some applications},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face identification using conditional generative adversarial
network. <em>COMJNL</em>, <em>66</em>(7), 1687–1697. (<a
href="https://doi.org/10.1093/comjnl/bxac034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of research studies that have dealt with face corrupted images to the level of being unrecognizable by a human are based on full image reconstruction. In some real scenarios, a single corrupted image might need to be recognized among a limited number of available clean images. This study deals with face identification from artificially corrupted images with various kinds of noises. The work proposes a face identification conditional generative adversarial network (FI-CGAN) model to identify faces based on the CGAN. The proposed models reconstruct the corrupted image based on available clean images to map the corrupted image to a specific label. The classification is made using the nearest neighbor method with peak signal-to-noise ratio, mean squared error and structural similarity index as metrics. The study used the Specs on Faces dataset and achieved a satisfactory performance for face identification.},
  archive      = {J_COMJNL},
  author       = {Kais Jameel, Samer and Majidpour, Jafar and Al-Talabani, Abdulbasit K and Anwar Qadir, Jihad},
  doi          = {10.1093/comjnl/bxac034},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1687-1697},
  shortjournal = {Comput. J.},
  title        = {Face identification using conditional generative adversarial network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the performance of feature selection methods with
low-sample-size data. <em>COMJNL</em>, <em>66</em>(7), 1664–1686. (<a
href="https://doi.org/10.1093/comjnl/bxac033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection refers to a critical preprocessing of machine learning to remove irrelevant and redundant data. According to feature selection methods, sufficient samples are usually required to select a reliable feature subset, especially considering the presence of outliers. However, sufficient samples cannot always be ensured in several real-world applications (e.g. neuroscience, bioinformatics and psychology). This study proposed a method to improve the performance of feature selection methods with ultra low-sample-size data, which is named feature selection based on data quality and variable training samples (QVT). Given that none of feature selection methods can perform optimally in all scenarios, QVT is primarily characterized by its versatility, because it can be implemented in any feature selection method. Furthermore, compared to the existing methods which tried to extract a stable feature subset for low-sample-size data by increasing the sample size or using more complicated algorithm, QVT tried to get improvement using the original data. An experiment was performed using 20 benchmark datasets, three feature selection methods and three classifiers to verify the feasibility of QVT; the results showed that using features selected by QVT is capable of achieving higher classification accuracy than using the explicit feature selection method, and significant differences exist.},
  archive      = {J_COMJNL},
  author       = {Zheng, Wanwan and Jin, Mingzhe},
  doi          = {10.1093/comjnl/bxac033},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1664-1686},
  shortjournal = {Comput. J.},
  title        = {Improving the performance of feature selection methods with low-sample-size data},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability assessment of combined hardware–software
non-repairable time-critical systems. <em>COMJNL</em>, <em>66</em>(7),
1644–1663. (<a href="https://doi.org/10.1093/comjnl/bxac032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there exist various reliability assessment models, many of these models did not consider the temporal incorrectness, incompatibilities and dependencies between hardware and software. Also, many models did not consider the hardware changes, software changes and hardware changes leading to software changes resulting in system failure. Thus, an approach that considers various aspects like pure software failures, pure hardware failures, software change, hardware change, hardware change leading to software change and incompatibility issues between hardware and software components (in the presence of temporal incorrectness) leading to the failure of non-repairable time-critical heterogeneous software systems has been developed. This approach considers the operation of the hardware–software system, both in normal (non-degraded) and degraded modes. The proposed approach was applied to a time-critical stepper motor system, results were obtained and a case study was performed. The effectiveness of the proposed approach has been analyzed by comparing the results obtained from the proposed approach with the results obtained from the other existing approaches.},
  archive      = {J_COMJNL},
  author       = {Verma, Anjushi and Gayen, Tirthankar},
  doi          = {10.1093/comjnl/bxac032},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1644-1663},
  shortjournal = {Comput. J.},
  title        = {Reliability assessment of combined Hardware–Software non-repairable time-critical systems},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining frequent serial positioning episode rules with
forward and backward search technique from event sequences.
<em>COMJNL</em>, <em>66</em>(7), 1622–1643. (<a
href="https://doi.org/10.1093/comjnl/bxac031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large event sequence can generate episode rules that are patterns which help to identify the possible dependencies existing among event types. Frequent episodes occurring in a simple sequence of events are commonly used for mining the episodes from a sequential database. Mining serial positioning episode rules (MSPER) using a fixed-gap episode occurrence suffers from unsatisfied scalability with complex sequences to test whether an episode occurs in a sequence. Large number of redundant nodes was generated in the MSPER-trie-based data structure. In this paper, forward and backward search algorithm (FBSA) is proposed here to detect minimal occurrences of frequent peak episodes. An extensive correlation of parameter settings and the generating procedure of fixed-gap episodes are carried out. To generate a fixed-gap episode and estimate the variance that decides the parameter selection in event sequences, Spearman’s correlation coefficient is used for verifying the sequence of occurrences of the episodes. MFSPER with FBSA is developed to eliminate the frequent sequence scans and redundant event sets. The MFSPER–FBSA stores the minimal occurrences of frequent peak episodes from the event sequences. The experimental evaluation on benchmark datasets shows that the proposed technique outperforms the existing methods with respect to memory, execution time, recall and precision.},
  archive      = {J_COMJNL},
  author       = {K, Poongodi and Kumar, Dhananjay},
  doi          = {10.1093/comjnl/bxac031},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1622-1643},
  shortjournal = {Comput. J.},
  title        = {Mining frequent serial positioning episode rules with forward and backward search technique from event sequences},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interopera: An efficient cross-chain trading protocol.
<em>COMJNL</em>, <em>66</em>(7), 1609–1621. (<a
href="https://doi.org/10.1093/comjnl/bxac030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of blockchains, blockchain systems are moving on from a stand-alone manner to cross-chain interactions, and achieving interoperability is emerging as one of the essential features of blockchains. Unfortunately, existing mechanisms such as XCLAIM mostly focus on exchanging assets between two blockchains and it is slow and expensive to process each cross-chain trade among more than two blockchains as multiple transactions are required. In this paper, we present Interopera, a decentralized and efficient cross-chain trading protocol among two or more blockchains. Interopera atomically processes each cross-chain trade faster and more cheaply with fewer transactions by a two-phase lock/unlock process. Interopera also achieves efficient cross-chain communication by our presented Partitioned-FlyClient and Tx-FlyClient. Partitioned-FlyClient is based on FlyClient but more efficient with smaller proof size, reducing the storage and bandwidth overheads. Tx-FlyClient maintains efficiency even when cross-chain trades become frequent, instead of other mechanisms only being effective under low cross-chain trades volumes. We also develop a proof-of-concept implementation and the results demonstrate high efficiency of our protocol.},
  archive      = {J_COMJNL},
  author       = {Yin, Lingyuan and Xu, Jing and Zhang, Zhenfeng},
  doi          = {10.1093/comjnl/bxac030},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1609-1621},
  shortjournal = {Comput. J.},
  title        = {Interopera: An efficient cross-chain trading protocol},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel SGD-u-network-based pixel-level road crack
segmentation and classification. <em>COMJNL</em>, <em>66</em>(7),
1595–1608. (<a href="https://doi.org/10.1093/comjnl/bxac029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic road crack detection plays a major role in developing an intelligent transportation system. The traditional approach of ; inspection is expensive and requires more man-power. In-order to solve this problem, a novel approach for automatic road crack segmentation was developed using Stack Generative adversarial network Discriminator-U-Network (SGD-U-Network). We have collected 19 300 crack and non-crack images (MIT-CHN-ORR dataset) from the Outer Ring Road of Chennai, TamilNadu, India. The MIT-CHN-ORR dataset was initially pre-processed using traditional image processing techniques for ground truth image generation. A stage-I and stage-II stack Generative Adversarial Network (GAN) model was introduced for generating high-resolution non-crack images. Then, the extracted features from Stack GAN Discriminator of stage II (SGD2) was concatenated with every level of expansion path in SGD-U-Network for segmenting the crack regions of the input crack images. Also, multi-feature-based classifier was developed using the features extracted from SGD2 and the bottleneck layer of SGD-U-Network. Our proposed model was implemented on MIT-CHN-ORR dataset and also analyzed our model performance using other existing benchmark datasets. The experimental analysis showcased that the proposed method outperformed the other state-of-the-art approaches.},
  archive      = {J_COMJNL},
  author       = {Sekar, Aravindkumar and Perumal, Varalakshmi},
  doi          = {10.1093/comjnl/bxac029},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1595-1608},
  shortjournal = {Comput. J.},
  title        = {A novel SGD-U-network-based pixel-level road crack segmentation and classification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and approximating the visit of a set of sites with
a fleet of UAVs. <em>COMJNL</em>, <em>66</em>(7), 1586–1594. (<a
href="https://doi.org/10.1093/comjnl/bxac028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of flying over an area affected by a natural disaster (e.g. an earthquake) with a fleet of self-piloting unmanned aerial vehicles with cameras or other kinds of sensors on board; the aim is to acquire knowledge of the situation before rescuers start working. We model this situation as a new graph theoretical problem; then, we study its complexity providing an approximation ratio that becomes constant in some special (though practically reasonable) cases; finally, we put in relation the approximability of this new problem and of a well-known one. To the best of our knowledge, no previous work has ever considered all together the constraints we take into account from a theoretical point of view, so this is the first very general graph theoretical model for this problem.},
  archive      = {J_COMJNL},
  author       = {Calamoneri, Tiziana and Tavernelli, Daniele},
  doi          = {10.1093/comjnl/bxac028},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1586-1594},
  shortjournal = {Comput. J.},
  title        = {Modeling and approximating the visit of a set of sites with a fleet of UAVs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improvement of MADRL equilibrium based on pareto
optimization. <em>COMJNL</em>, <em>66</em>(7), 1573–1585. (<a
href="https://doi.org/10.1093/comjnl/bxac027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the incalculability caused by the issue of inconsistent objective functions in multi-agent deep reinforcement learning, the concept of Nash equilibrium is introduced. However, a Marko game may have multiple equilibriums, how to filter out a stable and optimal one is worth studying. Besides solution concept, how to keep the balance between exploration and exploitation is another key issue in reinforcement learning. On basis of the methods, which can converge to Nash equilibrium, this paper makes improvement through Pareto optimization. In order to alleviate the problem of over fitting caused by Pareto optimization and non-convergence caused by strategy change, we use stratified sampling in place of random sampling as assistance. What’s more, our methods are trained through fictitious self-play to make full of self-learning experiences. By analyzing the experiment carried out on MAgent platform, the proposed methods are not only far better than traditional methods, but also reaching or even surpassing the state of art MADRL methods.},
  archive      = {J_COMJNL},
  author       = {Zhao, Zhiruo and Cao, Lei and Chen, Xiliang and Lai, Jun and Zhang, Legui},
  doi          = {10.1093/comjnl/bxac027},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1573-1585},
  shortjournal = {Comput. J.},
  title        = {Improvement of MADRL equilibrium based on pareto optimization},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the axioms of common meadows: Fracterm calculus,
flattening and incompleteness. <em>COMJNL</em>, <em>66</em>(7),
1565–1572. (<a href="https://doi.org/10.1093/comjnl/bxac026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common meadows are arithmetic structures with inverse or division, made total on ; by a flag ; for ease of calculation. We examine some axiomatizations of common meadows to clarify their relationship with commutative rings and serve different theoretical agendas. A common meadow fracterm calculus is a special form of the equational axiomatization of common meadows, originally based on the use of division on the rational numbers. We study axioms that allow the basic process of simplifying complex expressions involving division. A useful axiomatic extension of the common meadow fracterm calculus imposes the requirement that the characteristic of common meadows be zero (using a simple infinite scheme of closed equations). It is known that these axioms are complete for the full equational theory of common cancellation meadows of characteristic ; . Here, we show that these axioms do ; prove all conditional equations which hold in all common cancellation meadows of characteristic ; .},
  archive      = {J_COMJNL},
  author       = {Bergstra, Jan A and Tucker, John V},
  doi          = {10.1093/comjnl/bxac026},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1565-1572},
  shortjournal = {Comput. J.},
  title        = {On the axioms of common meadows: Fracterm calculus, flattening and incompleteness},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Corrigendum to: A multi-objective randomly updated beetle
swarm and multi-verse optimization for brain tumor segmentation and
classification. <em>COMJNL</em>, <em>66</em>(6), 1564. (<a
href="https://doi.org/10.1093/comjnl/bxac018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxac018},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1564},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: A multi-objective randomly updated beetle swarm and multi-verse optimization for brain tumor segmentation and classification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Corrigendum to: IoT data quality issues and potential
solutions: A literature review. <em>COMJNL</em>, <em>66</em>(6), 1563.
(<a href="https://doi.org/10.1093/comjnl/bxac014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxac014},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1563},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: IoT data quality issues and potential solutions: a literature review},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Role of machine learning on key extraction for data privacy
preservation of health care sectors in IoT environment. <em>COMJNL</em>,
<em>66</em>(6), 1549–1562. (<a
href="https://doi.org/10.1093/comjnl/bxad016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy and security in the medical field are major aspects to consider in the current era. This is due to the huge necessity for data among providers, payers and patients, respectively. Recently, more researchers are making their contributions in this field under different aspects. But, there need more enhancements concerning security. In this circumstance, this paper intends to propose a new IoT-dependent health care privacy preservation model with the impact of the machine learning algorithm. Here, the information or data from IoT devices is subjected to the proposed sanitization process via generating the optimal key. In this work, the utility of the machine learning model is the greatest gateway to generating optimal keys as it is already trained with the neural network. Moreover, identifying the optimal key is the greatest crisis, which is done by a new Improved Dragonfly Algorithm algorithm. Thereby, the sanitization process works, and finally, the sanitized data are uploaded to IoT. The data restoration is the inverse process of sanitization, which gives the original data. Finally, the performance of the proposed work is validated over state-of-the-art models in terms of sanitization and restoration analysis.},
  archive      = {J_COMJNL},
  author       = {Kathavate, Pravin Narayan},
  doi          = {10.1093/comjnl/bxad016},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1549-1562},
  shortjournal = {Comput. J.},
  title        = {Role of machine learning on key extraction for data privacy preservation of health care sectors in IoT environment},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of li-ion battery discharge patterns in IoT
devices under random use via machine learning algorithms.
<em>COMJNL</em>, <em>66</em>(6), 1541–1548. (<a
href="https://doi.org/10.1093/comjnl/bxac089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents foreseeing of the Lithium-ion battery discharge models for the Internet of Things (IoT) devices under randomized use patterns. IoT systems run in harmony with the human–machine interface, communication protocols and sharing data so long as uninterrupted data communication is exploited for their devices. Hence, forecasting the battery discharge duration is a very important issue for the regularization of IoT device performances. The well-known discharge duration is generally about the age-related electrochemical phenomena of an electrochemistry for Lithium-ion battery. The discharge changes of the battery were obtained from the input–output dynamics of the random battery use obtained from the randomized battery usage dataset in the NASA Ames prognostics data repository. They were investigated by machine learning methods and their results were estimated for life expectancy regularization of the IoT devices. In order to find the appropriate models of battery usage under randomized patterns, artificial neural network (ANN), Gaussian process and nonlinear regression models are evaluated in terms of battery capacity and internal resistance change as a function of discharged energy. The ; , Adjusted ; , root-mean-square-error (RMSE) and normalized-mean-square-error (NMSE) criteria were used to compare the performances of the obtained models for different settings. According to the results, ANN model, with settings of radial basis function activation function within single hidden-layer, and 20 hidden-layer neurons, shows the best performance in terms of ; and ; metrics. ; is achieved by the ANN model with the settings of single hidden-layer with 10 neurons and hyperbolic-tangent activation function.},
  archive      = {J_COMJNL},
  author       = {Gökçen, Ahmet and Gökçen, Alkım and Şahin, Savaş},
  doi          = {10.1093/comjnl/bxac089},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1541-1548},
  shortjournal = {Comput. J.},
  title        = {Prediction of li-ion battery discharge patterns in IoT devices under random use via machine learning algorithms},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved unsegmented phonocardiogram classification using
nonlinear time scattering features. <em>COMJNL</em>, <em>66</em>(6),
1525–1540. (<a href="https://doi.org/10.1093/comjnl/bxac025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phonocardiogram (PCG) signals highlight the relevant characteristics for the prediction of heart diseases or heart-related disorders. However, it is challenging to classify heart abnormality relying on an unbalanced PCG dataset due to low classification performance. Recently, several studies have attempted to predict heart abnormality based on segmented and unsegmented features extracted using PCG signals. This study aims to develop an automated PCG classification model eliminating any segmentation of the heart sound signal for predicting heart abnormality. So, we have proposed a new approach based on wavelet scattering transform to predict two classes of PCG signals, namely, normal and abnormal. Based on the wavelet scattering transform, five scattering time window features were extracted from each PCG signal. The PhysioNet 2016 PCG database has been used here to evaluate and compare the classification performance based on the k Nearest Neighbors (KNN) classifier. The proposed architecture used a KNN classifier with different distance functions (Euclidean, Cityblock, Chebyshev, Minkowsky, Correlation, Spearman and Cosine) and has been compared with other traditional classifiers (classification tree, linear discriminant analysis, support vector machine and ensemble). The proposed framework using nonlinear wavelet scattering features with a KNN classifier based Cityblock distance function achieved classification performance over the total datasets with accuracy, sensitivity and specificity values of 97.82\%, 95.04\% and 98.72\%, respectively.},
  archive      = {J_COMJNL},
  author       = {Ajitkumar Singh, Sinam and Dinita Devi, Ningthoujam and Majumder, Swanirbhar},
  doi          = {10.1093/comjnl/bxac025},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1525-1540},
  shortjournal = {Comput. J.},
  title        = {An improved unsegmented phonocardiogram classification using nonlinear time scattering features},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible and controllable access policy update for encrypted
data sharing in the cloud. <em>COMJNL</em>, <em>66</em>(6), 1507–1524.
(<a href="https://doi.org/10.1093/comjnl/bxac024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising service paradigm, cloud computing has attracted lots of enterprises and individuals to outsource big data to public cloud. To facilitate secure data using and sharing, ciphertext-policy attribute-based encryption (CP-ABE) is a suitable solution, which can provide fine-grained access control and encryption functionalities simultaneously. However, some serious challenges are still remaining toward achieving flexible and controllable access policy update in CP-ABE, which essentially impede the powerful access control ability of CP-ABE from long-term and large-scale deployment in real systems. In this work, we propose a novel scheme named policy updatable CP-ABE for encrypted data sharing scenes. The proposed scheme features the following achievements: (i) it supports fine-grained update algorithms with no restriction on update time; (ii) the cloud server can effectively verify update ciphertexts, so the integrity of original data would not be compromised intentionally or accidentally during the update and (iii) most operations of encryption and policy update are securely outsourced to cloud servers, leaving extremely low overheads for data owners and users. We formally define its security model and prove it is adaptively secure. Also, we implement the proposed scheme using the Charm framework. The experiment results demonstrate that it is efficient and practical.},
  archive      = {J_COMJNL},
  author       = {Wang, Ti and Zhou, Yongbin and Ma, Hui and Zhang, Rui},
  doi          = {10.1093/comjnl/bxac024},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1507-1524},
  shortjournal = {Comput. J.},
  title        = {Flexible and controllable access policy update for encrypted data sharing in the cloud},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized data flows for the functional scalability of
service-oriented IoT systems. <em>COMJNL</em>, <em>66</em>(6),
1477–1506. (<a href="https://doi.org/10.1093/comjnl/bxac023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Horizontal and vertical scalability have been widely studied in the context of computational resources. However, with the exponential growth in the number of connected objects, functional scalability (in terms of the size of software systems) is rapidly becoming a central challenge for building efficient service-oriented Internet of Things (IoT) systems that generate huge volumes of data continuously. As systems scale up, a centralized approach for moving data between services becomes infeasible because it leads to a single performance bottleneck. A distributed approach avoids such a bottleneck, but it incurs additional network traffic as data streams pass through multiple mediators. Decentralized data exchange is the only solution for realizing totally efficient IoT systems, since it avoids a single performance bottleneck and dramatically minimizes network traffic. In this paper, we present a functionally scalable approach that separates data and control for the realization of decentralized data flows in service-oriented IoT systems. Our approach is evaluated empirically, and the results show that it scales well with the size of IoT systems by substantially reducing both the number of data flows and network traffic in comparison with distributed data flows.},
  archive      = {J_COMJNL},
  author       = {Arellanes, Damian and Lau, Kung-Kiu and Sakellariou, Rizos},
  doi          = {10.1093/comjnl/bxac023},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1477-1506},
  shortjournal = {Comput. J.},
  title        = {Decentralized data flows for the functional scalability of service-oriented IoT systems},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-stabilizing distributed algorithm for the generalized
dominating set problem with safe convergence. <em>COMJNL</em>,
<em>66</em>(6), 1452–1476. (<a
href="https://doi.org/10.1093/comjnl/bxac021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A self-stabilizing distributed algorithm is guaranteed eventually to reach and stay at a legitimate configuration regardless of the initial configuration of a distributed system. In this paper, we propose the generalized dominating set problem, which is a generalization of the dominating set and ; -redundant dominating set problems. In the generalized dominating set we propose in this paper, each node ; is given its set of domination wish sets, and a generalized dominating set is a set of nodes such that each node is contained in the set or has a wish set in which all its members are in the set. We propose a self-stabilizing distributed algorithm for finding a minimal generalized dominating set in an arbitrary network under the unfair distributed daemon. The proposed algorithm converges in ; steps and ; rounds, where ; (resp., ; ) is the number of nodes (resp., edges). Furthermore, it has the safe convergence property with safe convergence time in ; rounds. The space complexity of the proposed algorithm is ; bits per node, where ; is the maximum degree of nodes.},
  archive      = {J_COMJNL},
  author       = {Kobayashi, Hisaki and Sudo, Yuichi and Kakugawa, Hirotsugu and Masuzawa, Toshimitsu},
  doi          = {10.1093/comjnl/bxac021},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1452-1476},
  shortjournal = {Comput. J.},
  title        = {A self-stabilizing distributed algorithm for the generalized dominating set problem with safe convergence},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Template attack assisted linear cryptanalysis on outer
rounds protected DES implementations. <em>COMJNL</em>, <em>66</em>(6),
1434–1451. (<a href="https://doi.org/10.1093/comjnl/bxac020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, when the security of a block cipher implementation is considered, the leakages related to the outer rounds encryptions can be used by side channel attacks (SCA) to recover the secret key. Therefore, the outer rounds of block cipher implementations should be protected. However, in order to lower the implementation price, the inner rounds of block cipher implementations may be unprotected. In light of this, the security of an outer rounds protected DES implementation is considered. In detail, template attack (TA), which is information theoretically the strongest SCA style, can be used to obtain the inner round output. Then, linear cryptanalysis (LC) can be used to recover the secret key. Finally, the optimal key enumeration algorithm can be used to optimize the efficiency of TA assisted LC. We evaluate the efficiency of TA assisted LC in simulated scenarios where a three outer rounds protected DES implementation is targeted. The evaluation results show that when 800 correct samples are available and the number of key enumeration is ; , the efficiency of TA assisted LC can reach 83\% of success rate. Overall, an efficient combination attack style that can be used to accurately evaluate the security of an outer rounds protected DES implementation is proposed.},
  archive      = {J_COMJNL},
  author       = {Zhang, Hailong and Yang, Wei},
  doi          = {10.1093/comjnl/bxac020},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1434-1451},
  shortjournal = {Comput. J.},
  title        = {Template attack assisted linear cryptanalysis on outer rounds protected DES implementations},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new neural distinguisher considering features derived from
multiple ciphertext pairs. <em>COMJNL</em>, <em>66</em>(6), 1419–1433.
(<a href="https://doi.org/10.1093/comjnl/bxac019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-aided cryptanalysis is a challenging topic, in which the neural distinguisher (; ) is a core module. In this paper, we propose a new ; considering multiple ciphertext pairs simultaneously. Besides, multiple ciphertext pairs are constructed from different keys. The motivation is that the distinguishing accuracy can be improved by exploiting features derived from multiple ciphertext pairs. To verify this motivation, we have applied this new ; to five different ciphers. Experiments show that taking multiple ciphertext pairs as input indeed brings accuracy improvement. Then, we prove that our new ; applies to two different neural-aided key recovery attacks. Moreover, the accuracy improvement is helpful for reducing the data complexity of the neural-aided statistic attack. The code is available at ; .},
  archive      = {J_COMJNL},
  author       = {Chen, Yi and Shen, Yantian and Yu, Hongbo and Yuan, Sitong},
  doi          = {10.1093/comjnl/bxac019},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1419-1433},
  shortjournal = {Comput. J.},
  title        = {A new neural distinguisher considering features derived from multiple ciphertext pairs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplified high level parallelism expression on
heterogeneous systems through data partition pattern description.
<em>COMJNL</em>, <em>66</em>(6), 1400–1418. (<a
href="https://doi.org/10.1093/comjnl/bxac017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of heterogeneous systems, the demand for high-level programming methods that ease heterogeneous programming and produce portable applications has become more urgent. This paper proposes DACL, the data associated computing language. DACL introduces data partition patterns to achieve architecture-independent parallelism expression. Meanwhile, DACL provides simplified language extensions, as well as programming features such as serialization of the computing process, parameterization of data attributes and modularity, thus reducing the difficulty of heterogeneous programming and improving programming productivity. The operational semantics show that DACL enables different levels of parallelism degree calculation and retains data access patterns, reserving optimization potential. To support cross-platform execution, the currently implemented source-to-source compilers employ OpenMP and OpenCL as the backend. We reconstructed multiple benchmarks selected from the Parboil and Rodinia benchmark suits with DACL and conducted a comparison test on CPU, GPU and MIC platforms. The code size of each rebuilt benchmark is roughly equivalent to that of the serial code, which is only 13\%–64\% of the benchmark OpenCL code. With the support of the compilation system, the reconstructed code can execute on different processors without modification, yielding a competitive or better performance to that of the manually written benchmark code.},
  archive      = {J_COMJNL},
  author       = {Wu, Shusen and Dong, Xiaoshe and Chen, Heng and Wang, Longxiang and Wang, Qiang and Zhu, Zhengdong},
  doi          = {10.1093/comjnl/bxac017},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1400-1418},
  shortjournal = {Comput. J.},
  title        = {Simplified high level parallelism expression on heterogeneous systems through data partition pattern description},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On grain-like small state stream ciphers against fast
correlation attacks: Cryptanalysis of plantlet, fruit-v2 and fruit-80.
<em>COMJNL</em>, <em>66</em>(6), 1376–1399. (<a
href="https://doi.org/10.1093/comjnl/bxac016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast correlation attack (FCA) is one of the most important cryptanalytic techniques against LFSR-based stream ciphers. In CRYPTO 2018, Todo ; found a new property for the FCA and proposed a novel algorithm which was successfully applied to the Grain family of stream ciphers. Nevertheless, these techniques cannot be directly applied to Grain-like small state stream ciphers with keyed update, such as Plantlet, Fruit-v2 and Fruit80. In this paper, we study the security of Grain-like small state stream ciphers by the FCA. We first observe that the number of required parity-check equations can be reduced when there are multiple different parity-check equations. With exploiting the Skellam distribution, we introduce a sufficient condition to identify the correct LFSR initial state and derive a new relationship between the number and bias of the required parity-check equations. Then, a modified algorithm is presented based on this new relationship, which can recover the LFSR initial state no matter what the round key bits are. Under the condition that the LFSR initial state is known, an algorithm is given against the degraded system and to recover the NFSR state at some time instant, along with the round key bits. As cases study, we apply our cryptanalytic techniques to Plantlet, Fruit-v2 and Fruit-80. As a result, for Plantlet, our attack takes ; time complexity and ; keystream bits to recover the full 80-bit key. Regarding Fruit-v2, ; time complexity and ; keystream bits are needed to determine the secret key. As for Fruit-80, ; time complexity and ; keystream bits are required to recover the secret key. More flexible attacks can be obtained with lower data complexity at the cost of increasing the attack time. Especially, for Fruit-v2, a key recovery attack can be launched with data complexity of ; and time complexity of ; . Moreover, we have implemented our attack methods on a toy version of Fruit-v2. The attack matches the expected complexities predicted by our theoretical analysis quite well, which proves the validity of our cryptanalytic techniques.},
  archive      = {J_COMJNL},
  author       = {Wang, Shichang and Liu, Meicheng and Lin, Dongdai and Ma, Li},
  doi          = {10.1093/comjnl/bxac016},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1376-1399},
  shortjournal = {Comput. J.},
  title        = {On grain-like small state stream ciphers against fast correlation attacks: Cryptanalysis of plantlet, fruit-v2 and fruit-80},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach for CT-based COVID-19 classification and
lesion segmentation based on deep learning. <em>COMJNL</em>,
<em>66</em>(6), 1366–1375. (<a
href="https://doi.org/10.1093/comjnl/bxac015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) pandemic has been a globally dangerous crisis that causes an increasingly high death rate. Applying machine learning to the computed-tomography (CT)-based COVID-19 diagnosis is essential and attracts the attention of the research community. This paper introduces an approach for simultaneously identifying COVID-19 disease and segmenting its manifestations on lung images. The proposed method is an asymmetric U-Net-like model improved with skip connections. The experiment was conducted on a light-weighted feature extractor called CRNet with a feature enhancement technique called atrous spatial pyramid pooling. Classifying between COVID-19 and non-COVID-19 cases recorded the highest mean scores of 97.1, 94.4, and 97.0\% for accuracy, dice similarity coefficient (DSC) and F1 score, respectively. Alternatively, the respective highest mean scores of the classification between COVID-19 and community-acquired pneumonia were 99.89, 99.79, and 99.97\%. The lesion segmentation performance was with the highest mean of 99.6 and 84.7\% for, respectively, accuracy and DSC.},
  archive      = {J_COMJNL},
  author       = {Truong, Hieu Minh and Huynh, Hieu Trung},
  doi          = {10.1093/comjnl/bxac015},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1366-1375},
  shortjournal = {Comput. J.},
  title        = {A novel approach for CT-based COVID-19 classification and lesion segmentation based on deep learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dragonfly political optimizer algorithm-based rider deep
long short-term memory for soil moisture and heat level prediction in
IoT. <em>COMJNL</em>, <em>66</em>(6), 1350–1365. (<a
href="https://doi.org/10.1093/comjnl/bxab215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different computerized technologies to monitor plant health in the Internet of Things (IoT) paradigm gained various benefits but generating accurate result in the soil moisture and heat level prediction is the potential challenge. Thus, an effective Dragonfly Political Optimizer Algorithm-based Rider Deep Long Short-Term Memory (DPOA-based Rider Deep LSTM) is developed for generating better prediction results of soil moisture and heat level. The proposed DPOA is the integration of the Dragonfly Algorithm and Political Optimizer. The proposed system maintains the Base Station (BS) that collects the information from the IoT nodes through Cluster Head. At BS, the data transformation is carried out using Yeo Johnson transformation. The transformed result is transferred to feature selection, which is evaluated by holoentropy, and finally, the prediction process of soil moisture and the heat level is done at BS using the proposed method. The proposed method achieved higher performance in terms of Packet Delivery Ratio, energy, accuracy, sensitivity and specificity with the values of 0.7156, 0.7123, 0.9474, 0.9523 and 0.9254, respectively.},
  archive      = {J_COMJNL},
  author       = {Muppidi, Satish and P G, Om Prakash and B, Kishore},
  doi          = {10.1093/comjnl/bxab215},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1350-1365},
  shortjournal = {Comput. J.},
  title        = {Dragonfly political optimizer algorithm-based rider deep long short-term memory for soil moisture and heat level prediction in IoT},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorrectly generated RSA keys: How i learned to stop
worrying and recover lost plaintexts. <em>COMJNL</em>, <em>66</em>(6),
1342–1349. (<a href="https://doi.org/10.1093/comjnl/bxac199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When generating primes ; and ; for an RSA key, the algorithm specifies that if ; and ; must be relatively prime to the public exponent ; . If this is not done, then the decryption exponent is not well defined. However, what if a software bug allows the generation of public parameters ; and ; of an RSA key with this property and then it is subsequently used for encryption? Though this may seem like a purely academic question, a software bug in a preview release of the Windows 10 operating system makes this question of more than purely theoretical. Without a well defined decryption exponent, plaintexts encrypted to such keys will be undecryptable thus potentially losing user data, a serious software defect. Though the decryption exponent is no longer well defined, it is in fact possible to recover the a small number of potential plaintexts, if the prime factors ; and ; of the public modulus ; are known. This paper presents an analysis of what steps fail in the RSA algorithm and derives a plaintext recovery algorithm. The runtime of this algorithm is ; making it practical to use, and it has been implemented in python.},
  archive      = {J_COMJNL},
  author       = {Shumow, Daniel},
  doi          = {10.1093/comjnl/bxac199},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1342-1349},
  shortjournal = {Comput. J.},
  title        = {Incorrectly generated RSA keys: How i learned to stop worrying and recover lost plaintexts},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Row, row, row your boat: How to not find weak keys in
pilsung. <em>COMJNL</em>, <em>66</em>(6), 1335–1341. (<a
href="https://doi.org/10.1093/comjnl/bxac092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Pilsung cipher is part of the North Korean Red Star operating system, which was leaked to the West in 2014. Initial analysis by Kryptos Logic reported a possibility of a class of weak keys due to the use of pseudo-random diffusion. Following this lead, we analyzed the cipher and identified a small class of such weak keys. We developed techniques for searching for a key that belongs to the class. After spending thousands of CPU hours, we found a supposedly weak key for a slightly weaker version of Pilsung, but the key did not behave as we expected. On further investigation we found out a crucial misunderstanding in a critical part of the cipher and that no such class of weak keys exists in Pilsung. Thus, this paper makes two main contributions to the art of cryptanalysis. First, it identifies and shows how to investigate a potential weakness in randomizing diffusion, which although does not exist in Pilsung, may affect future designs. Second, it highlights the need for early verification of results in order to identify errors before expending significant resources.},
  archive      = {J_COMJNL},
  author       = {Chuengsatiansup, Chitchanok and Ronen, Eyal and Rose, Gregory G and Yarom, Yuval},
  doi          = {10.1093/comjnl/bxac092},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1335-1341},
  shortjournal = {Comput. J.},
  title        = {Row, row, row your boat: How to not find weak keys in pilsung},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The need for being explicit: Failed attempts to construct
implicit certificates from lattices. <em>COMJNL</em>, <em>66</em>(6),
1320–1334. (<a href="https://doi.org/10.1093/comjnl/bxac132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global efforts such as the National Institute of Standards and Technology (NIST)’s post-quantum standardization center on cryptographic primitives like public-key encryption and signature schemes that are secure even in the presence of quantum adversaries. In addition, one must also consider efficient certificate management as new technologies like the Internet of Things and 5G wireless networks rely on them. For example, the IEEE Standard for vehicle-to-vehicle communication depends on implicit certificates. However, the only efficient construction available is over elliptic curves, and hence not quantum-secure. This paper investigates approaches to construct implicit certificate schemes from lattices, employing the NIST Round 3 signature schemes Dilithium and Falcon. We consider emulation of the existing implicit certificate scheme and proceed to more innovative techniques like combining the two schemes or pairing them with encryption. Unfortunately, we encounter problems with each design, due to recurring causes like conflicting secret key and signature sizes, unique sampler requirements and the rigidity of the parameter sets. By explaining each of these issues, this paper will hopefully spark ideas for more successful constructions.},
  archive      = {J_COMJNL},
  author       = {Bindel, Nina and McCarthy, Sarah},
  doi          = {10.1093/comjnl/bxac132},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1320-1334},
  shortjournal = {Comput. J.},
  title        = {The need for being explicit: Failed attempts to construct implicit certificates from lattices},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How not to design an efficient FHE-friendly block cipher:
seljuk. <em>COMJNL</em>, <em>66</em>(6), 1312–1319. (<a
href="https://doi.org/10.1093/comjnl/bxac146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase in the practical applications of secure computation protocols, increasingly more research is focused on the efficiency of the symmetric-key primitives underlying them. Whereas traditional block ciphers have evolved to be efficient with respect to certain performance metrics, secure computation protocols call for a different efficiency metric: ; . Arithmetic complexity is viewed through the number and layout of nonlinear operations in the circuit implemented by the protocol. Symmetric-key algorithms that are optimized for this metric are said to be algebraic ciphers. It has been shown that recently proposed algebraic ciphers are greatly efficient in ZK and MPC protocols. However, there has not been many algebraic ciphers proposed targeting Fully Homomorphic Encryption (FHE). In this paper, we evaluate the behavior of ; when implemented as a circuit in an FHE protocol. To this end, we present a state-of-the-art comparison of ; and Vision implemented using HElib. Counterintuitively, ; does not deliver a better performance than AES in this setting. Then, by attempting to improve a bottleneck of the FHE implementation evaluating Vision we present a new cipher: ; . Despite the improvement with respect to ; , ; does not deliver the expected performance.},
  archive      = {J_COMJNL},
  author       = {Ashur, Tomer and Mahzoun, Mohammad and Toprakhisar, Dilara},
  doi          = {10.1093/comjnl/bxac146},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1312-1319},
  shortjournal = {Comput. J.},
  title        = {How not to design an efficient FHE-friendly block cipher: Seljuk},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on failed approaches and insightful losses in
cryptology — foreword. <em>COMJNL</em>, <em>66</em>(6), 1311. (<a
href="https://doi.org/10.1093/comjnl/bxac191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Ashur, Tomer and Mitchell, Chris J},
  doi          = {10.1093/comjnl/bxac191},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {1311},
  shortjournal = {Comput. J.},
  title        = {Special issue on failed approaches and insightful losses in cryptology — foreword},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CWSOGG: Catching web shell obfuscation based on genetic
algorithm and generative adversarial network. <em>COMJNL</em>,
<em>66</em>(5), 1295–1309. (<a
href="https://doi.org/10.1093/comjnl/bxac040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A web shell is a backdoor used by hackers to control Web servers and perform privilege escalation, and thus it is crucial to detect web shells effectively. However, the detection of obfuscated web shells has always been a challenge. Inspired by adversarial training methods in the field of computer vision, this paper proposes a generative adversarial network (GAN)-based web shell detection model training framework. Since there has been no method that can generate obfuscated web shells effectively, a generator based on the genetic algorithm, which combines and optimizes the pre-set obfuscation methods, is used to obtain new obfuscation combinations and generate obfuscated samples. The whole proposed framework is named the CWSOGG. When training the detection model, the generator generates web shells that can bypass the discriminator, and the discriminator catches the features of obfuscated samples. Through the adversarial training of the discriminator and generator, the detection model improves its ability to detect obfuscated web shells. To verify the proposed framework is flexible to different models, the discriminator based on four main neural networks has been implemented. Meanwhile, to build complete feature extraction models, both statistical and semantic features are extracted. Due to the lack of web shell data, a clean dataset containing 4,375 web shells is constructed and used to evaluate the CWSOGG. The results have shown that the detection accuracy of each model increases by 86.71\% on the generated obfuscated web shells on average and by 7.50\% on the simulated real-world obfuscated web shells on average.},
  archive      = {J_COMJNL},
  author       = {Pang, Bo and Liang, Gang and Yang, Jin and Chen, Yijing and Wang, Xinyi and He, Wenbo},
  doi          = {10.1093/comjnl/bxac040},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1295-1309},
  shortjournal = {Comput. J.},
  title        = {CWSOGG: Catching web shell obfuscation based on genetic algorithm and generative adversarial network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved self attention mechanism based on optimized
BERT-BiLSTM model for accurate polarity prediction. <em>COMJNL</em>,
<em>66</em>(5), 1279–1294. (<a
href="https://doi.org/10.1093/comjnl/bxac013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarity prediction is the field of study that discovers people’s opinions, feelings, assessments, perspectives and feelings about associations and their attributes as communicated in written text. It is one of the most active research areas in the field of text mining. Nowadays online reviews play an important role by giving a helping hand to the customers to know about other customer’s opinions about the product they are going to purchase. This also guides the organizations and government sectors to increase their quality of product and services. Pre-trained BERT (Bidirectional Encoder Representations from Transformers) is used for word embedding in this model. The fine-tuned BERT is used for better word representation which in turn improves the sentimental analysis classification accuracy. Bidirectional Long Short-Term Memory classifier is utilized for polarity prediction. To enhance the performance of Bidirectional Long Short-Term Memory, the weight parameters of Bi-directional LSTM are optimally selected by using APSO algorithm. Improved self-attention mechanism is added with BiLSTM for focusing on significant words in the context. For performance analysis, four bench mark datasets are used for experiments.},
  archive      = {J_COMJNL},
  author       = {Shobana, J and Murali, M},
  doi          = {10.1093/comjnl/bxac013},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1279-1294},
  shortjournal = {Comput. J.},
  title        = {An improved self attention mechanism based on optimized BERT-BiLSTM model for accurate polarity prediction},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear attacks on SNOW 3G and SNOW-v using automatic search.
<em>COMJNL</em>, <em>66</em>(5), 1268–1278. (<a
href="https://doi.org/10.1093/comjnl/bxac012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a linear attack model of SNOW 3G and SNOW-V based on automatic search technology is proposed. We first describe the linear approximation of Finite State Machine transformation, which allows a wider range of automatic search, then model it with the automatic search technology based on SAT/SMT program. Adopting this generic method, we seek out a binary linear approximation of SNOW 3G with correlation of ; which has been verified by test. Treating this binary approximation as a mask of an 8-bit distribution in a fixed field, we provide a method to obtain the 8-bit distribution. The binary approximation is used in a fast correlation attack with expected time and memory complexity ; , given ; key stream words. For the full version of SNOW-V, considering the linear relationship between Linear Feedback Shift Register parts at three successive moments, we search out a distinguisher with correlation of ; , which results in a distinguishing attack with an expected complexity of ; .},
  archive      = {J_COMJNL},
  author       = {Shi, Zhen and Jin, Chenhui},
  doi          = {10.1093/comjnl/bxac012},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1268-1278},
  shortjournal = {Comput. J.},
  title        = {Linear attacks on SNOW 3G and SNOW-V using automatic search},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On 2-interval pairwise compatibility properties of two
classes of grid graphs. <em>COMJNL</em>, <em>66</em>(5), 1256–1267. (<a
href="https://doi.org/10.1093/comjnl/bxac011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph ; is called a pairwise compatibility graph (PCG) if it admits a tuple ; of an edge-weighted tree ; of non-negative edge weights with leaf set ; , two non-negative real numbers ; such that each vertex ; represents a leaf ; and ; has an edge ; if and only if the distance between the two leaves ; and ; in the tree ; lies within interval ; . It has been proven that not all graphs are PCGs. A graph ; is called a ; -interval PCG if there exists an edge-weighted tree ; and ; mutually exclusive intervals of non-negative real numbers such that there is an edge between two vertices in ; if and only if the distance between their corresponding leaves in ; lies within any of the ; intervals. It is known that every graph ; is a ; -interval PCG for ; , where ; is the set of edges of ; . It is thus interesting to know the smallest value of ; for which ; is a ; -interval PCG. In this paper, we show that grid graphs and a subclass of ; D grid graphs are ; -interval PCGs.},
  archive      = {J_COMJNL},
  author       = {Papan, Bishal Basak and Pranto, Protik Bose and Rahman, Md Saidur},
  doi          = {10.1093/comjnl/bxac011},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1256-1267},
  shortjournal = {Comput. J.},
  title        = {On 2-interval pairwise compatibility properties of two classes of grid graphs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust image hashing with saliency map and sparse model.
<em>COMJNL</em>, <em>66</em>(5), 1241–1255. (<a
href="https://doi.org/10.1093/comjnl/bxac010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hashing is an effective technology for extensive image applications, such as retrieval, authentication and copy detection. This paper designs a new image hashing scheme based on saliency map and sparse model. The major contributions are twofold. The first contribution is the construction of a weighted image representation by combining a visual attention model called Itti model and the matrix of color vector angle (CVA). Since the Itti model can efficiently detect saliency map and CVA fully captures color information of image, they contribute to a visually robust and discriminative image representation. The second contribution is the hash extraction from the weighted image representation via sparse model. A classical sparse model called robust principal component analysis is exploited to decompose the weighted image representation into a low-rank component and a sparse component. As the low-rank component can describe intrinsic structure of image, hash calculation with low-rank component can achieve good discrimination. The efficiencies of the proposed scheme are validated by extensive experiments with open databases. The results demonstrate that the proposed scheme is superior to some state-of-the-art schemes in terms of classification performance between robustness and discrimination.},
  archive      = {J_COMJNL},
  author       = {Yu, Mengzhu and Tang, Zhenjun and Li, Zhixin and Liang, Xiaoping and Zhang, Xianquan},
  doi          = {10.1093/comjnl/bxac010},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1241-1255},
  shortjournal = {Comput. J.},
  title        = {Robust image hashing with saliency map and sparse model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel hybrid segmentation approach for decision support: A
case study in banking. <em>COMJNL</em>, <em>66</em>(5), 1228–1240. (<a
href="https://doi.org/10.1093/comjnl/bxac009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving decision-making has become of paramount importance for gaining a competitive edge across organizations. Segmentation/clustering is a key enabler for enhancing decision-making. The RFM (Recency, Frequency and Monetary) is a scoring model, which consists of three parameters that is widely used for the same purpose, especially for customer-centric organizations. However, the current RFM still has some drawbacks that diminish the effectiveness of decision-making and thus, research is still needed in this area. In an attempt to fill this gap, a novel data-driven weighted model is proposed. It introduces a new parameter; ‘Adoption’, to be added to the existing RFM parameters. The research contribution is extended to hybrid the proposed model with a clustering technique to segment data into distinct and meaningful groups or clusters. The hybrid model was applied on real bank data as a case study to enhance the usage of its digital channels. The results showed that bank customers were grouped into four distinct clusters. The bank stakeholders were able to identify the characteristics of customers and develop their relevant strategies accordingly. The developed model was evaluated by measuring its stability and robustness from both scientific and business perspectives.},
  archive      = {J_COMJNL},
  author       = {Mosa, Mona and Agami, Nedaa and Elkhayat, Ghada and Kholief, Mohamed},
  doi          = {10.1093/comjnl/bxac009},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1228-1240},
  shortjournal = {Comput. J.},
  title        = {A novel hybrid segmentation approach for decision support: A case study in banking},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DEV-ETA: An interpretable detection framework for encrypted
malicious traffic. <em>COMJNL</em>, <em>66</em>(5), 1213–1227. (<a
href="https://doi.org/10.1093/comjnl/bxac008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic encrypted technology enables Internet users to protect their data secrecy, but it also brings a challenge to malicious package detection. To tackle this issue, researchers have investigated into encrypted traffic analysis (ETA) in recent years. Existing works, however, only focus on the accuracy of malicious flow identification. Using ETA as a technical black box, they pay little attention to the internal details and explanation of models. In this paper, we, for the first time, introduce interpretable machine learning into ETA. We aim to provide a reasonable explanation for detection results, so as to enable one to understand and further trust network security analysts. We develop a complete analysis framework, named DEV-ETA (detection, explanation and verification of ETA). DEV-ETA applies post hoc interpretation methods to explain the detection results and verify the explanation using the joint distribution of support features on the dataset. We run thorough experiments to explain the detection result using three popular explanation approaches, namely SHAP, LIME and MSS, and we verify the explanation via the feature distribution plot. The experimental results show that our design can interpret the detection result of ETA model instead of just simply treating the model as a black box.},
  archive      = {J_COMJNL},
  author       = {Yang, Luming and Fu, Shaojing and Wang, Yongjun and Liang, Kaitai and Mo, Fan and Liu, Bo},
  doi          = {10.1093/comjnl/bxac008},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1213-1227},
  shortjournal = {Comput. J.},
  title        = {DEV-ETA: An interpretable detection framework for encrypted malicious traffic},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New meet-in-the-middle attacks on FOX block cipher.
<em>COMJNL</em>, <em>66</em>(5), 1195–1212. (<a
href="https://doi.org/10.1093/comjnl/bxac007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FOX block cipher was designed with a Lai–Massey scheme, in which the round function uses the Substitution-Permutation-Substitution structure. A meet-in-the-middle (MITM) attack is one of the most important issues for the security of the block cipher, which consists of a precomputation phase for constructing a distinguisher and an online phase for key recovery. This paper studies the MITM attacks against FOX. The first MITM distinguishers of 5-round FOX64, 7-round FOX64-256 and 5-round FOX128 are presented when using the differential enumeration technique with truncated differential characteristics. Then, based on these distinguishers, the attacks for key recovery on 7-round FOX64, 11-round FOX64-256 and 7-round FOX128 are presented with the state-test and state-search techniques. It is shown that the attack on 11-round FOX64-256 is proposed for the first time; attacks on 7-round FOX64 and 7-round FOX128 can be improved with lower time and memory complexities compared with the currently known attacks.},
  archive      = {J_COMJNL},
  author       = {Dong, Xiaoli and Wei, Yongzhuang and Gao, Wen and Chen, Jie},
  doi          = {10.1093/comjnl/bxac007},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1195-1212},
  shortjournal = {Comput. J.},
  title        = {New meet-in-the-middle attacks on FOX block cipher},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Component fault diagnosis and fault tolerance of alternating
group graphs. <em>COMJNL</em>, <em>66</em>(5), 1184–1194. (<a
href="https://doi.org/10.1093/comjnl/bxac006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability of a multiprocessor system becomes an important issue for parallel computing. Component diagnosability and component connectivity of a graph play crucial roles in assessing the vulnerability of an interconnection network, which are two significant indicators for the reliability and fault tolerance of a multiprocessor system. Until now, only a little knowledge of results have been known on ; -component diagnosability and ; -component connectivity. In this paper, we first propose the ; -component diagnosability of ; -dimensional alternating group graph ; under PMC model. And then we promote our research on ; by a fairly good construction for general ; -component connectivity of ; , where ; . The theoretical analysis and simulation show that the general ; -component connectivity of ; is larger than those of ; , ; and ; .},
  archive      = {J_COMJNL},
  author       = {Huang, Yanze and Lin, Limei and Cheng, Eddie and Xu, Li},
  doi          = {10.1093/comjnl/bxac006},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1184-1194},
  shortjournal = {Comput. J.},
  title        = {Component fault diagnosis and fault tolerance of alternating group graphs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aspect-based sentiment analysis using interaction matrix and
global attention neural network. <em>COMJNL</em>, <em>66</em>(5),
1167–1183. (<a href="https://doi.org/10.1093/comjnl/bxac005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis aims to identify the sentiment polarity of aspects in a given sentence. Although existing neural network models show promising results, they cannot meet the expectations in the case of a single network structure and limited dataset. When an aspect term composes more than one word, many models use the coarse-grained attention mechanism but lead to the unsatisfactory results. Besides, the relative distance between words in a sentence is always out of consideration. In this paper, we propose a model based on the interaction matrix and global attention mechanism to improve the ability of aspect-based sentiment analysis. First of all, the relative distance features of words in a sentence are initialized to enrich word embedding. Second, classic neural networks are applied to extract the essential features of word embedding in a sentence, such as long short-term memory and convolutional neural network. Third, an interaction matrix and global attention mechanism are combined to calculate weighted scores and measure relationships between aspect terms and context words. Finally, sentiment polarity is represented through a softmax layer. Experimental results on restaurant, laptop and twitter datasets show that the performance of the proposed model is superior to other methods.},
  archive      = {J_COMJNL},
  author       = {Wang, Xiaodi and Pan, Xiaoge and Yang, Tian and Xie, Jianhua and Tang, Mingwei},
  doi          = {10.1093/comjnl/bxac005},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1167-1183},
  shortjournal = {Comput. J.},
  title        = {Aspect-based sentiment analysis using interaction matrix and global attention neural network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual closeness, matching number and chromatic number.
<em>COMJNL</em>, <em>66</em>(5), 1156–1166. (<a
href="https://doi.org/10.1093/comjnl/bxac004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual closeness is a novel graph-based network vulnerability parameter. In this model, links are perfectly reliable and the nodes fail independently of each other. We characterize those graphs with maximum residual closeness and those connected graphs with minimum residual closeness when matching number (chromatic number, respectively) is fixed.},
  archive      = {J_COMJNL},
  author       = {Wang, Yanna and Zhou, Bo},
  doi          = {10.1093/comjnl/bxac004},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1156-1166},
  shortjournal = {Comput. J.},
  title        = {Residual closeness, matching number and chromatic number},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A transformer based encodings for detection of semantically
equivalent questions in cQA. <em>COMJNL</em>, <em>66</em>(5), 1139–1155.
(<a href="https://doi.org/10.1093/comjnl/bxac003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probability of redundancy in questions has significantly increased due to the increasing influx of users on different cQA forums such as Quora, Stack overflow, etc. Because of this redundancy, the responses are scattered through various variations of the same question that results in unsatisfactory search results to a specific question. To address this issue, this work proposes the model for discovering the semantic similarity among the cQA questions. We followed two approaches (i) Feature-based: the question embedding is created using four forms of word embeddings and an ensemble of all four. Then Siamese LSTM (sLSTM) is used to find the semantic similarity among the questions. (ii) Fine-tuning: we fine-tuned BERT model on STS and SNLI data, which employs Siamese network architectures to generate semantically meaningful sentence embeddings. Then sBERT is used to assess the similarity between the questions. Experiments were carried out on Quora (QQP) and Stack Exchange cQA dataset with training sets of different sizes and word vectors of different dimensionalities. The model shows significant improvement over the state-of-the-artwork on sentence similarity tasks.},
  archive      = {J_COMJNL},
  author       = {Kumar, Shobhan and Chauhan, Arun},
  doi          = {10.1093/comjnl/bxac003},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1139-1155},
  shortjournal = {Comput. J.},
  title        = {A transformer based encodings for detection of semantically equivalent questions in cQA},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). F2SO: An energy efficient cluster based routing protocol
using fuzzy firebug swarm optimization algorithm in WSN.
<em>COMJNL</em>, <em>66</em>(5), 1126–1138. (<a
href="https://doi.org/10.1093/comjnl/bxac002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important design concern associated with wireless sensor networks is energy efficiency that mostly deals with routing and clustering techniques. The major aim of this study is to provide energy efficient cluster routing protocols for wireless sensor network. Initially, we used a Honey Badger Algorithm for cluster heads selection. Honey Badger Algorithm is used to choose the distance to the base station, residual energy, distance to its neighbors, node degree node and centrality with the optimal cluster head among all sensors. The routing between base stations and cluster heads is performed through Fuzzy Firebug Swarm Optimization algorithm. Hence, the fuzzy decision module automatically adjusts the search behavior of firebug swarm optimization. Fuzzy rules are used with the node degree, residual energy and distance to the base station to address the firebug swarm optimization algorithm’s constraints during routing. Finally, cluster maintenance is done which is the major step to eliminate the failure nodes. This process assists to improve the network lifetime during data transmission by eliminating the faulty nodes and establish an efficient path between the source node and the base station. Wide experimentation is accomplished to assess the efficiency of the proposed protocol using different evaluation measures. The proposed methodology offers improved end-to-end delay, increased packet delivery ratio, higher throughput, low packet drop ratio and minimizes energy consumption when compared with the existing wireless sensor network routing protocols.},
  archive      = {J_COMJNL},
  author       = {Suresh, K and Sreeja Mole, S S and Joseph Selva Kumar, A},
  doi          = {10.1093/comjnl/bxac002},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1126-1138},
  shortjournal = {Comput. J.},
  title        = {F2SO: An energy efficient cluster based routing protocol using fuzzy firebug swarm optimization algorithm in WSN},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-party signing for ISO/IEC digital signature standards.
<em>COMJNL</em>, <em>66</em>(5), 1111–1125. (<a
href="https://doi.org/10.1093/comjnl/bxac001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-party signing can be used to provide a high level of key protection especially in the blockchain systems where the safety of money relies on the safety of the signing key. With a two-party signing protocol, the signing key is distributed among two devices, thus the funds are safe as long as one device remains uncorrupted. In this paper, we study the two-party signing protocols for all ISO/IEC signature standards. The mechanisms based on elliptic curve discrete logarithm in ISO/IEC can be divided into three types: Schnorr-type, Elliptic Curve Digital Signature Algorithm (ECDSA)-type and SM2-type. There have already been efficient two-party protocols based on Schnorr signature scheme which can be easily extended into all Schnorr-type standards. However, it is particularly hard to construct efficient distributed SM2-type and ECDSA-type protocols due to their nonlinear signing equations. In this paper, we present the first secure and efficient two-party protocol over SM2-type signature standard. We prove its security in the generic group model. We then construct a more efficient two-party ECDSA protocol that is secure in the generic group model and outperforms all previous works.},
  archive      = {J_COMJNL},
  author       = {Tang, Guofeng and Zhang, Zhenfeng},
  doi          = {10.1093/comjnl/bxac001},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1111-1125},
  shortjournal = {Comput. J.},
  title        = {Two-party signing for ISO/IEC digital signature standards},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum attacks on 1K-AES and PRINCE. <em>COMJNL</em>,
<em>66</em>(5), 1102–1110. (<a
href="https://doi.org/10.1093/comjnl/bxab216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By introducing the BHT algorithm into the slide attack on 1K-AES and the related-key attack on PRINCE, we present the corresponding quantum attacks in this paper. In the proposed quantum attacks, we generalize the BHT algorithm to the situation where the number of marked items is unknown ahead of time. Moreover, we give an implementation scheme of classifier oracle based on Quantum Phase Estimation algorithm in presented quantum attacks. The complexity analysis shows that the query complexity, time complexity and memory complexity of the presented quantum attacks are all ; when the success probability is about ; , where ; is the block size. Compared with the corresponding classical attacks, the proposed quantum attacks can achieve subquadratic speed-up under the same success probability no matter on query complexity, time complexity or memory complexity. Furthermore, the query complexity of the proposed quantum slide attack on 1K-AES is less than Grover search on 1K-AES by a factor of ; When compared with the Grover search on PRINCE, the query complexity of the presented quantum attack on PRINCE is reduced from ; to ; When compared with the combination of Grover and Simon’s algorithms on PRINCE, the query complexity of our quantum attack on PRINCE is reduced from ; to ; Besides, the proposed quantum slide attack on 1K-AES indicates that the quantum slide attack could also be applied on Substitution-Permutation Network construction, apart from the iterated Even-Mansour cipher and Feistel constructions.},
  archive      = {J_COMJNL},
  author       = {Cai, Bin-Bin and Wu, Yusen and Dong, Jing and Qin, Su-Juan and Gao, Fei and Wen, Qiao-Yan},
  doi          = {10.1093/comjnl/bxab216},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1102-1110},
  shortjournal = {Comput. J.},
  title        = {Quantum attacks on 1K-AES and PRINCE},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An educational data mining system for predicting and
enhancing tertiary students’ programming skill. <em>COMJNL</em>,
<em>66</em>(5), 1083–1101. (<a
href="https://doi.org/10.1093/comjnl/bxab214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational Data Mining (EDM) has become a promising research field for improving the quality of students and the education system. Although EDM dates back to several years, there is still lack of works for measuring and enhancing the computer programming skills of tertiary students. As such, we, in this paper, propose an EDM system for evaluating and improving tertiary students’ programming skills. The proposed EDM system comprises two key modules for (i) classification process and (ii) learning process,. The classification module predicts the current status of a student and the learning process module helps generate respective suggestions and feedback to enhance the student’s quality. In particular, for the classification module, we prepare a real dataset related to this task and evaluate the dataset to investigate six key Machine Learning (ML) algorithms, Support Vector Machine (SVM), decision tree, artificial neural network, Random Forest (RF), ; -nearest neighbor and naive Bayes classifier, using accuracy-related performance measure metrics and goodness of the fit. The experimental results manifest that RF and SVM can predict the students more accurately than the other models. In addition, critical factors analysis is accomplished to identify the critical features toward achieving high classification accuracy. At last, we design an improvement mechanism in the learning process module that helps the students enhance their programming skills.},
  archive      = {J_COMJNL},
  author       = {Marjan, Md Abu and Uddin, Md Palash and Ibn Afjal, Masud},
  doi          = {10.1093/comjnl/bxab214},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1083-1101},
  shortjournal = {Comput. J.},
  title        = {An educational data mining system for predicting and enhancing tertiary students’ programming skill},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new method to find all the high-probability word-oriented
truncated differentials: Application to midori, SKINNY and CRAFT.
<em>COMJNL</em>, <em>66</em>(5), 1069–1082. (<a
href="https://doi.org/10.1093/comjnl/bxab213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new method to find high-probability truncated differentials using matrix muliplication. For Markov cipher with similar round function, suppose that the transition probability matrix of round function is ; , then ; contains all the differential probabilities of an ; -round block cipher. To reduce the matrix dimension, we consider the word-oriented truncated differential and the truncated transition probability matrix ; . Regardless of the effect of the ; -box, we focus on whether there is a non-zero difference on one cell instead of the value of the difference. In this case, the matrix dimension reduces significantly and we can calculate ; using a workstation. Then all the ; -round truncated differential probabilities can be found from ; . And the probability in ; is the probability of the whole truncated differential hull but not a single or several truncated differential characteristics. Besides, we make a more accurate probability estimation of the truncated differential of lightweight block cipher. Combined with the truncated differential hull, we found some longer truncated differential distinguishers. And as ; stores all the truncated differential probabilities, we can also find all the impossible truncated differentials.},
  archive      = {J_COMJNL},
  author       = {Guo, Hao and Zhang, Zhiyu and Yang, Qianqian and Hu, Lei and Luo, Yiyuan},
  doi          = {10.1093/comjnl/bxab213},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1069-1082},
  shortjournal = {Comput. J.},
  title        = {A new method to find all the high-probability word-oriented truncated differentials: Application to midori, SKINNY and CRAFT},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Laser beam welded aluminum-titanium dissimilar sheet metals:
Neural network based strength and hardness prediction model.
<em>COMJNL</em>, <em>66</em>(5), 1053–1068. (<a
href="https://doi.org/10.1093/comjnl/bxab211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘Laser Beam Welding (LBW) is a welding technique used to join pieces of metal or thermoplastics with the aid of laser’. The beam offers a concerted heat source, which enabled higher, deeper welds and narrower welding rates. The procedure is commonly exploited in higher volume appliances using mechanization. It is dependent on penetration or keyhole mode welding. This paper intends to design a novel prediction model on LBW using the Optimized Neural Network (NN) framework. The input to the optimized NN is the welding properties like ‘Laser power, welding speed, offset, shielding gas, flow/pressure, focal distance and frequency (where power, speed and offset gets varied)’ that directly predict the hardness and tensile strength of welds since the NN is already trained with the provided data. In order to make the prediction model more accurate, this paper aims to train the NN using a new improved Trial Integer-based Whale Optimization Algorithm (TI-WOA) via updating the weight. Finally, the betterment of the suggested scheme is validated with respect to error analysis. Accordingly, from the analysis, it is observed that the proposed methods are 50\%, 13.33\%, 6.67\% and 4\% better than ANN-BP, RBF, ANN-GA and NN-WOA models, respectively, at 70th learning percentage.},
  archive      = {J_COMJNL},
  author       = {Chandran, Sudhin and Rajesh, R and Anand, M Dev},
  doi          = {10.1093/comjnl/bxab211},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {1053-1068},
  shortjournal = {Comput. J.},
  title        = {Laser beam welded aluminum-titanium dissimilar sheet metals: Neural network based strength and hardness prediction model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance evaluation of FPGA-based LSTM neural networks
for pulse signal detection on real-time radar warning receivers.
<em>COMJNL</em>, <em>66</em>(4), 1040–1052. (<a
href="https://doi.org/10.1093/comjnl/bxac167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar warning receivers are real-time systems used to detect emitted signals by the enemy targets. The conventional method of detecting the signal is to determine the noise floor and differentiate the signals above the noise floor by setting a threshold value. The common methodology for detecting signals in noisy environment is Constant False Alarm Rate (CFAR) detection. In CFAR methodology, threshold level is determined for a specified probability of false alarm. CFAR dictates the signal power to be detected is higher than the noise floor, i.e. signal-to-noise ratio (SNR) should be positive. To detect radar signals for negative SNR values machine learning techniques can be used. It is possible to detect radar signals for negative SNR values by Long Short-Term Memory (LSTM) Artificial Neural Network (ANN). In this study, we evaluated whether LSTM ANN can replace the CFAR algorithm for signal detection in real-time radar receiver systems. We implemented a Field Programmable Gate Array (FPGA) based LSTM ANN architecture, where pulse signal detection could be performed with 94\% success rate at -5 dB SNR level. To the best of our knowledge our study is the first where LSTM ANN is implemented on FPGA for radar warning receiver signal detection.},
  archive      = {J_COMJNL},
  author       = {Tekincan, Erdoğan Berkay and Erçelebi Ayyıldız, Tülin and Ayyıldız, Nizam},
  doi          = {10.1093/comjnl/bxac167},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1040-1052},
  shortjournal = {Comput. J.},
  title        = {Performance evaluation of FPGA-based LSTM neural networks for pulse signal detection on real-time radar warning receivers},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis performance of image processing technique its
application by decision support systems on covid-19 disease prediction
using convolution neural network. <em>COMJNL</em>, <em>66</em>(4),
1030–1039. (<a href="https://doi.org/10.1093/comjnl/bxac154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Covid-19 pandemic has been identified as a key issue for human society, in recent times. The presence of the infection on any human is identified according to different symptoms like cough, fever, headache, breathless and so on. However, most of the symptoms are shared by various other diseases, which makes it challenging for the medical practitioners to identify the infection. To aid the medical practitioners, there are a number of approaches designed which use different features like blood report, lung and cardiac features to detect the disease. The method captures the lung image using magnetic resonance imaging scan device and records the cardiac features. Using the image, the lung features are extracted and from the cardiac graph, the cardiac features are extracted. Similarly, from the blood samples, the features are extracted. By extracting such features from the person, the method estimates different weight measures to predict the disease. Different methods estimate the similarity of the samples in different ways to classify the input sample. However, the image processing techniques are used for different problems in medical domain; the same has been used in the detection of the disease. Also, the presence of Covid-19 is detected using different set of features by various approaches.},
  archive      = {J_COMJNL},
  author       = {Ravishankar, K and Jothikumar, C},
  doi          = {10.1093/comjnl/bxac154},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1030-1039},
  shortjournal = {Comput. J.},
  title        = {Analysis performance of image processing technique its application by decision support systems on covid-19 disease prediction using convolution neural network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cryptanalysis on reduced-round 3D and saturnin.
<em>COMJNL</em>, <em>66</em>(4), 1017–1029. (<a
href="https://doi.org/10.1093/comjnl/bxac116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D is an Advanced Encryption Standard (AES)-like cipher employed 3D structure proposed in 2008. The main innovation of 3D is the multi-dimensional state, generalizing the design of Rijndael and allowing block sizes beyond the 256-bit boundary. Saturnin, a lightweight block cipher has been selected as a second-round candidate in the National Institute of Standards and Technology standardization for lightweight cryptography. It also employs a 3D structure and provides high security against quantum and classic attacks. The exchange-equivalence attacks proposed by Bardeh and Rønjom consider how quadruples of plaintexts confirm distinguishable properties for AES. It is similar to the principle of yoyo attack, but it can find a longer number of rounds of distinguisher. In this paper, we investigate the exchange-equivalence attack on 3D and yoyo attack on Saturnin. Our new results turn out to be the first secret-key chosen plaintext distinguisher for 10-round 3D. The complexity of the distinguisher is about ; in terms of data, memory and computational complexity. For Saturnin, we propose the first six-super-round impossible differential yoyo attack, which is suitable for the two-S-layer version. Compared with the previous impossible differential attacks in the design report of Saturnin, the attacks presented here are the best in terms of the complexity under the chosen-plaintext scenario.},
  archive      = {J_COMJNL},
  author       = {Zhang, Li and Wu, Wenling and Zheng, YaFei and Wang, Bolin},
  doi          = {10.1093/comjnl/bxac116},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {1017-1029},
  shortjournal = {Comput. J.},
  title        = {Cryptanalysis on reduced-round 3D and saturnin},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal symmetric ratcheting for secure communication.
<em>COMJNL</em>, <em>66</em>(4), 987–1016. (<a
href="https://doi.org/10.1093/comjnl/bxab209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate state exposure threats to long-lived instant messaging sessions, ratcheting was introduced, which is used in practice in protocols like Signal. However, existing ratcheting protocols generally come with a high cost. Recently, Caforio et al. proposed pragmatic constructions, which compose a weakly secure ‘light’ protocol and a strongly secure ‘heavy’ protocol, in order to achieve so-called ratcheting on-demand. The light protocol they proposed has still a high complexity.; In this paper, we propose the lightest possible protocol we could imagine, which essentially encrypts and then hashes the secret key. We prove it secure in the standard model by introducing a new security notion, which relates symmetric encryption with key updates by hashing. Our protocol composes well with the generic transformation techniques by Caforio et al. to offer high security and performance at the same time. In a second step, we propose another protocol based on a newly defined integrated primitive, extending standard one-time authenticated encryption with an additional output block used as a secret key for the next message. We instantiate this primitive firstly from any authenticated encryption with associated data, and then we propose an efficient instantiation using advanced encryption standard (AES) encryption to update the key and AES-Galois/Counter mode of operation to encrypt and decrypt messages.},
  archive      = {J_COMJNL},
  author       = {Yan, Hailun and Vaudenay, Serge and Collins, Daniel and Caforio, Andrea},
  doi          = {10.1093/comjnl/bxab209},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {987-1016},
  shortjournal = {Comput. J.},
  title        = {Optimal symmetric ratcheting for secure communication},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-data cryptanalysis on SKINNY block cipher.
<em>COMJNL</em>, <em>66</em>(4), 970–986. (<a
href="https://doi.org/10.1093/comjnl/bxab208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At CRYPTO 2021, Dong et al. proposed an automatic method of Meet-in-the-Middle (MITM) key-recovery attacks. In this paper, we further extend it to a new automatic model which can be used to find low-data complexity attacks. With the help of the automatic model, we propose MITM attacks against reduced-round versions of all the six members of the ; family with extremely low-data complexity. More precisely, we present MITM attacks against 19-round ; -; -; , 15-round ; -; -; , 11-round ; -; -; with three, two, one plaintext-ciphertext pairs, separately. In addition, we can attack two more rounds and three more rounds with no more than ; and ; data complexity, respectively.},
  archive      = {J_COMJNL},
  author       = {Hua, Jialiang and Liu, Tai and Cui, Yulong and Qin, Lingyue and Dong, Xiaoyang and Cui, Huiyong},
  doi          = {10.1093/comjnl/bxab208},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {970-986},
  shortjournal = {Comput. J.},
  title        = {Low-data cryptanalysis on SKINNY block cipher},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact of COVID-19 on the human personality: An analysis
based on document modeling using machine learning tools.
<em>COMJNL</em>, <em>66</em>(4), 963–969. (<a
href="https://doi.org/10.1093/comjnl/bxab207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease of 2019 (COVID-19) has affected the globe terribly. The rapid spread of this virus and the precautionary measures to prevent it have impacted the lives of all human beings around the world in all dimensions. The anxieties over the virus along with the social restrictions have challenged the mental health and might have acute psychological consequences. In this study, our aim is to analyze whether COVID-19 has done any significant changes to very well-known five-factor personality traits of all the humans all over the world from social media text, such as Twitter. We first train and validate five machine learning models on the benchmark essays dataset and then those models are tested on the preprocessed Twitter dataset, consisting of pre_covid and post_covid tweets. The novelty of this study is to analyze and establish the fact that in this short period of time, COVID-19 cannot make very significant changes in the human personality all over the world. We have compared the performances of five machine learning models and what we have found is that the result provided by one model is also justified by the other models.},
  archive      = {J_COMJNL},
  author       = {Acharya, Amitabha and Aryan, Aman and Saha, Sujay and Ghosh, Anupam},
  doi          = {10.1093/comjnl/bxab207},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {963-969},
  shortjournal = {Comput. J.},
  title        = {Impact of COVID-19 on the human personality: An analysis based on document modeling using machine learning tools},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Customer segmentation using k-means clustering and the
hybrid particle swarm optimization algorithm. <em>COMJNL</em>,
<em>66</em>(4), 941–962. (<a
href="https://doi.org/10.1093/comjnl/bxab206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a competitive market, it is of great significance to divide customer groups to develop customer-centered personalized products. In this paper, we propose a customer segmentation method based on the K-means algorithm and the improved particle swarm optimization (PSO) algorithm. As the PSO algorithm easily falls into local extremum, the improved hybrid particle swarm optimization (IHPSO) algorithm is proposed to improve optimization accuracy. The full factorial design is used to determine the optimal parameter combination; the roulette operator is used to select excellent particles; then, the selected particles are crossed according to their adaptive crossover probabilities; when the population falls into a local optimum, the particles are mutated according to their adaptive mutation probabilities. Aimed at the K-means’ sensitivity to selecting the initial cluster centers, IHPSO is used to optimize the cluster centers (IHPSO-KM). We compare IHPSO with the PSO, LDWPSO, GA, GA-PSO and ALPSO algorithms on nine benchmark functions. We also conduct comparative experiments to compare IHPSO-KM with several conventional and state-of-the-art approaches on five UCI datasets. All results show that the two proposed methods outperform existing models. Finally, IHPSO-KM is applied in customer segmentation. The experimental results also prove the rationality and applicability of IHPSO-KM for customer segmentation.},
  archive      = {J_COMJNL},
  author       = {Li, Yue and Qi, Jianfang and Chu, Xiaoquan and Mu, Weisong},
  doi          = {10.1093/comjnl/bxab206},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {941-962},
  shortjournal = {Comput. J.},
  title        = {Customer segmentation using K-means clustering and the hybrid particle swarm optimization algorithm},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identity-based encryption with continuous leakage-resilient
CCA security from static complexity assumption. <em>COMJNL</em>,
<em>66</em>(4), 924–940. (<a
href="https://doi.org/10.1093/comjnl/bxab205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although a large number of provably secure cryptographic primitives have been proposed in the literature, many of these schemes might be broken in practice because of various leakage attacks. Therefore, the leakage resilience should be considered in designing these primitives. However, in identity-based cryptography, most of the existing leakage-resilient identity-based encryption (IBE) schemes suffer some limitations: they either resist the leakage attacks in the selective identity security model or achieve the chosen-ciphertext attack (CCA) security based on a non-static assumption. In this paper, an IBE scheme with adaptive leakage-resilient CCA security is proposed, and its security is rigorously proved in the random oracle model under a classic static complexity assumption, e.g. decisional bilinear Diffie–Hellman assumption. In our construction, all elements of ciphertext are randomly distributed in the adversary’s view. Hence, the adversary cannot obtain any useful information of the user’s private key from the given ciphertexts. Moreover, a unique property of our construction is that the leakage parameter is independent of the plaintext space, which contributes a better leakage rate.},
  archive      = {J_COMJNL},
  author       = {Zhou, Yanwei and Wang, Zhaolong and Qiao, Zirui and Wang, Ying and Yang, Bo and Mu, Yi and Zhang, Mingwu},
  doi          = {10.1093/comjnl/bxab205},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {924-940},
  shortjournal = {Comput. J.},
  title        = {Identity-based encryption with continuous leakage-resilient CCA security from static complexity assumption},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the longest common cartesian substring problem.
<em>COMJNL</em>, <em>66</em>(4), 907–923. (<a
href="https://doi.org/10.1093/comjnl/bxab204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Cartesian tree is associated with a string of numbers and is structured as a heap from which the original string can be recovered. Although Cartesian trees have been introduced 40 years ago, the Cartesian tree matching problem appeared very recently. It consists in finding all substrings of given text, which have the same Cartesian tree as that of a given pattern. In this paper, we address the problem of computing the longest common Cartesian substrings of two strings and present three methods for such problem. Our first method is based on a classical suffix tree construction and solves the problem in randomized linear time and linear space, although the space overhead is quite prohibitive in the case of large strings. Our second solution is based on classical dynamic programming, and our third solution is based on a constructive approach. Both of them run in quadratic worst case time but are more space economical in practice. From our experimental results, it turns out that our second solution runs faster than the standard suffix tree solution for short strings, whereas our third solution is more suitable for large strings, when storing a full suffix tree becomes prohibitive.},
  archive      = {J_COMJNL},
  author       = {Faro, Simone and Lecroq, Thierry and Park, Kunsoo and Scafiti, Stefano},
  doi          = {10.1093/comjnl/bxab204},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {907-923},
  shortjournal = {Comput. J.},
  title        = {On the longest common cartesian substring problem},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human visual system guided reversible data hiding based on
multiple histograms modification. <em>COMJNL</em>, <em>66</em>(4),
888–906. (<a href="https://doi.org/10.1093/comjnl/bxab203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a human visual system (HVS) guided reversible data hiding method based on multiple histograms modification. The proposed method utilizes the texture features of an image to adaptively modify the pixels, for a lower HVS quality distortion. The HVS quality is taken as the optimization objective and a new expansion-bin-selection strategy is given to solve the optimization. For the same HVS quality distortion, the proposed method can embed more secret bits and give higher priority for the modifications in texture regions. Besides, a better optimization rule is proposed to accelerate the speed and then consider more available solutions. In this way, a trade-off between the embedding performance and time complexity can be achieved. Experimental results show that the proposed method can achieve a better HVS quality than the conventional methods.},
  archive      = {J_COMJNL},
  author       = {Zhang, Cheng and Ou, Bo and Li, Xiaolong and Xiong, Jianqin},
  doi          = {10.1093/comjnl/bxab203},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {888-906},
  shortjournal = {Comput. J.},
  title        = {Human visual system guided reversible data hiding based on multiple histograms modification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Taylor sun flower optimization-based compressive sensing for
image compression and recovery. <em>COMJNL</em>, <em>66</em>(4),
873–887. (<a href="https://doi.org/10.1093/comjnl/bxab202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most prominent challenges in compressive sensing are seeking the domain where an image is represented sparsely and hence be faithfully recovered to obtain high-quality results. This paper introduces an approach for image compression and recovery. The proposed approach involves two phases: the initial step is the compression phase, and the second step is the recovery phase. Initially, the medical image is subjected to the compression module wherein the self-similarity and the 3-dimensional (3D) transform are adapted for compressing the image. Then, in the recovery phase, the compressive sensing recovery is performed based on structural similarity index measure (SSIM)-based collaborative sparsity measure (S-CoSM), and the novel optimization algorithm, named Taylor-based Sunflower optimization (Taylor-SFO) algorithm. An effective S-CoSM measure is designed by modifying the CoSM using the SSIM metric. The proposed Taylor-SFO will be designed by integrating the Taylor series with the sunflower optimization (SFO) algorithm. The performance of the proposed Taylor-SFO approach is evaluated for matrices SSIM of 0.9412 and peak signal to noise ratio of 57.57 dB.},
  archive      = {J_COMJNL},
  author       = {R, Sekar and G, Ravi},
  doi          = {10.1093/comjnl/bxab202},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {873-887},
  shortjournal = {Comput. J.},
  title        = {Taylor sun flower optimization-based compressive sensing for image compression and recovery},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Some scheduling problems with job rejection and a learning
effect. <em>COMJNL</em>, <em>66</em>(4), 866–872. (<a
href="https://doi.org/10.1093/comjnl/bxab201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examined single and parallel machine scheduling problems with a learning effect and job rejection simultaneously. In real life, job processing times decrease when there is a learning effect. In some cases, producers cannot process all the jobs and pay the penalty cost for these jobs that they do not process. In our study, learning effect and job rejection are considered at the same time. We examined four different objective functions. Our objectives for single-machine scheduling problems are makespan and rejection cost minimization, total completion time and rejection cost minimization and total absolute deviation of completion times (TADC) and rejection cost minimization. Our objective for parallel machines is makespan and rejection cost minimization. The problems are solved by mathematical models, and four different algorithms are proposed for the problems. From these algorithms, the same results are obtained with single-machine makespan and rejection cost minimization, parallel machine makespan and rejection cost minimization and total completion time and rejection cost minimization. The accuracy for these models is obtained as 100\%. The proposed algorithm for TADC and rejection cost minimization yielded close-to-optimal results. Mathematical model and algorithm results for 10 jobs, 20 jobs and 30 jobs are compared and the results are presented. The obtained solutions are obtained in polynomial time.},
  archive      = {J_COMJNL},
  author       = {Toksari, M Duran and Atalay, Berrin},
  doi          = {10.1093/comjnl/bxab201},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {866-872},
  shortjournal = {Comput. J.},
  title        = {Some scheduling problems with job rejection and a learning effect},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient traceable attribute-based signature with
update-free revocation for blockchain. <em>COMJNL</em>, <em>66</em>(4),
842–865. (<a href="https://doi.org/10.1093/comjnl/bxab199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based signature (ABS) allows signers with a set of attributes to sign messages anonymously using a specific signing policy. However, previous schemes suffer from some efficiency issues which are not widely applied on the blockchain. In this paper, we investigate ABS regarding its features and efficiency in the blockchain setting and provide our solution correspondingly. To solve the revocation problem of ABS in a more efficient manner, we introduce the update-free revocation function. Instead of the passive attribute expiration approaches, we take the active method to ensure that no parameter updates are required by users after the execution of the revocation function. In terms of efficiency, we first address the problem that the signer has to provide proof for all attributes in the predicate for privacy, which is one of the efficiency bottlenecks for ABS. By taking advantage of the blockchain architecture, we propose a new solution which can achieve the constant signature size and verification cost, while the signing cost can be greatly reduced. The corresponding security levels are satisfied according to their strict criteria. A generic construction as well as an instantiation are provided which is provably secure in the standard model satisfying the newly defined formal security definitions. Finally, a purer primitive is discussed.},
  archive      = {J_COMJNL},
  author       = {Zhang, Jixin and Chen, Jiageng},
  doi          = {10.1093/comjnl/bxab199},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {842-865},
  shortjournal = {Comput. J.},
  title        = {Efficient traceable attribute-based signature with update-free revocation for blockchain},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster connectivity and super cluster connectivity of
DQcube. <em>COMJNL</em>, <em>66</em>(4), 826–841. (<a
href="https://doi.org/10.1093/comjnl/bxab198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Zhou, Qianru and Zhou, Shuming and Liu, Xiaoqing and Yu, Zhengqin},
  doi          = {10.1093/comjnl/bxab198},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {826-841},
  shortjournal = {Comput. J.},
  title        = {Cluster connectivity and super cluster connectivity of DQcube},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment classification of tourist’s opinion on tourist
places of interest in south india using tweet reviews. <em>COMJNL</em>,
<em>66</em>(4), 815–825. (<a
href="https://doi.org/10.1093/comjnl/bxab197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergent technology has been increasingly incorporated with the tourism industry. In the digital world, social media plays an important role in identifying the most visited tourist places. More than 80\% of people commonly used the social media platform called ‘Twitter’. By using tweet reviews, tourists around the world know about the feelings, suggestions and opinions of most visited tourist places in the selected region. This paper recommends an approach for the development of trip planning in South India based on tourist’s preferences. For the research, we use Twitter Application Programming Interface to collect information about the most visited tourist places in South India by using tweets reviews and store it as South India Tourism Tweet reviews database. This paper mainly focuses on analyzing Twitter reviews, which are very helpful to locate the most visited tourist place. In the tweets where reviews are mostly unstructured and heterogeneous, which are then classified into positive tweets, negative tweets and neutral tweets by using the preprocessing technique and machine learning algorithm called support vector machine. Performance measures are calculated by using the classification results. Finally, the results are plotted by using the classification results and performance measures as a wordcloud. This can be very useful for tourists to select the most visited tourist locations. The proposed system can satisfy individual tourist needs.},
  archive      = {J_COMJNL},
  author       = {Bharathi, G and Anandharaj, G},
  doi          = {10.1093/comjnl/bxab197},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {815-825},
  shortjournal = {Comput. J.},
  title        = {Sentiment classification of tourist’s opinion on tourist places of interest in south india using tweet reviews},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sasa: A SimulAtor of self-stabilizing algorithms.
<em>COMJNL</em>, <em>66</em>(4), 796–814. (<a
href="https://doi.org/10.1093/comjnl/bxab196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present ; , an open-source SimulAtor of Self-stabilizing Algorithms. Self-stabilization defines the ability of a distributed algorithm to recover after transient failures. ; is implemented as a faithful representation of the atomic-state model (also called the locally shared memory model with composite atomicity). This model is the most commonly used one in the self-stabilizing area to prove both the correct operation of self-stabilizing algorithms and complexity bounds on them. ; encompasses all features necessary to debug, test and analyze self-stabilizing algorithms. All these facilities are programmable to enable users to accommodate to their particular needs. For example, asynchrony is modeled by programmable stochastic daemons playing the role of input sequence generators. Properties of algorithms can be checked using formal test oracles. The ; distribution also provides several facilities to easily achieve (batch-mode) simulation campaigns. We show that the lightweight design of ; allows to efficiently perform huge such campaigns. Following a modular approach, we have aimed at relying as much as possible the design of ; on existing tools, including ; , ; and several tools developed in the ; of the VERIMAG laboratory.},
  archive      = {J_COMJNL},
  author       = {Altisen, Karine and Devismes, Stéphane and Jahier, Erwan},
  doi          = {10.1093/comjnl/bxab196},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {796-814},
  shortjournal = {Comput. J.},
  title        = {Sasa: A SimulAtor of self-stabilizing algorithms},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pulmonary lung nodule detection from computed tomography
images using two-stage convolutional neural network. <em>COMJNL</em>,
<em>66</em>(4), 785–795. (<a
href="https://doi.org/10.1093/comjnl/bxab191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is one of the leading causes of cancer-related death in people all over the world. Lung cancer screening is a crucial part of the diagnosis of cancer. The initial sign of lung cancer is the pulmonary nodules that can be detected based on the computed tomography (CT) scan images. In some cases, the nodules are not obvious and may take a trained eye and a considerable amount of time to detect. The automatic detection of the nodules can save considerable time and money, thus opening prescreening accessibility, ultimately saving lives. Hence, a two-stage convolutional neural network (CNN) model is proposed to segment and detect pulmonary lung nodules from CT scan images. In the first stage, we have used U-net for nodule segmentation, and in the second stage, a CNN model is designed to reduce the false positives (FPs) generated in the previous stage, which enhances the overall efficiency and correctness of the system. The proposed method’s performance is evaluated on the publicly available Lung Image Database Consortium and Image Database Resource Initiative dataset. The proposed two-stage CNN model achieved an accuracy of 84.4\%, where the FP per true positive is reduced from 11.1 to 0.97. The proposed model shows its superiority over the existing methods for nodule detection.},
  archive      = {J_COMJNL},
  author       = {Jain, Sweta and Choudhari, Pruthviraj and Gour, Mahesh},
  doi          = {10.1093/comjnl/bxab191},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {785-795},
  shortjournal = {Comput. J.},
  title        = {Pulmonary lung nodule detection from computed tomography images using two-stage convolutional neural network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Declarative programming with intensional sets in java using
JSetL. <em>COMJNL</em>, <em>66</em>(3), 763–784. (<a
href="https://doi.org/10.1093/comjnl/bxab195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensional sets are sets given by a property rather than by enumerating their elements. In a previous work, we have proposed a decision procedure for a first-order logic language which provides restricted intensional sets (RISs), i.e. a sub-class of intensional sets that are guaranteed to denote finite—though unbounded—sets. In this paper, we show how RIS can be exploited as a convenient programming tool also in a conventional setting, namely the imperative O-O language Java. We do this by considering a Java library, called JSetL, that integrates the notions of logical variable, (set) unification and constraints that are typical of constraint logic programming languages into the Java language. We show how JSetL is naturally extended to accommodate for RIS and RIS constraints and how this extension can be exploited; on the one hand, to support a more declarative style of programming and, on the other hand, to effectively enhance the expressive power of the constraint language provided by the library.},
  archive      = {J_COMJNL},
  author       = {Cristiá, Maximiliano and Fois, Andrea and Rossi, Gianfranco},
  doi          = {10.1093/comjnl/bxab195},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {763-784},
  shortjournal = {Comput. J.},
  title        = {Declarative programming with intensional sets in java using JSetL},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Particle rider mutual information and dendritic-squirrel
search algorithm with artificial immune classifier for brain tumor
classification. <em>COMJNL</em>, <em>66</em>(3), 743–762. (<a
href="https://doi.org/10.1093/comjnl/bxab194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Images (MRI) is an imperative imaging modality employed in the medical diagnosis tool for detecting brain tumors. However, the major obstacle in MR images classification is the semantic gap between low-level visual information obtained by MRI machines and high-level information alleged by the clinician. Hence, this research article introduces a novel technique, namely Dendritic-Squirrel Search Algorithm-based Artificial immune classifier (Dendritic-SSA-AIC) using MRI for brain tumor classification. Initially the pre-processing is performed followed by segmentation is devised using sparse fuzzy-c-means (Sparse FCM) is employed for segmentation to extract statistical and texture features. Furthermore, the Particle Rider mutual information (PRMI) is employed for feature selection, which is devised by integrating Particle swarm optimization, Rider optimization algorithm and mutual information. AIC is employed to classify the brain tumor, in which the Dendritic-SSA algorithm designed by combining dendritic cell algorithm and Squirrel search algorithm (SSA). The proposed PRMI-Dendritic-SSA-AIC provides superior performance with maximal accuracy of 97.789\%, sensitivity of 97.577\% and specificity of 98\%.},
  archive      = {J_COMJNL},
  author       = {Chakre, Rahul Ramesh and Patil, Dipak V},
  doi          = {10.1093/comjnl/bxab194},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {743-762},
  shortjournal = {Comput. J.},
  title        = {Particle rider mutual information and dendritic-squirrel search algorithm with artificial immune classifier for brain tumor classification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Caviar-sunflower optimization algorithm-based deep learning
classifier for multi-document summarization. <em>COMJNL</em>,
<em>66</em>(3), 727–742. (<a
href="https://doi.org/10.1093/comjnl/bxab193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multi-document summarization model using an optimization algorithm named CAVIAR Sun Flower Optimization (CAV-SFO). In this method, two classifiers, namely: Generative Adversarial Network (GAN) classifier and Deep Recurrent Neural Network (Deep RNN), are utilized to generate a score for summarizing multi-documents. Initially, the simHash method is applied for removing the duplicate/real duplicate contents from sentences. Then, the result is given to the proposed CAV-SFO based GAN classifier to determine the score for individual sentences. The CAV-SFO is newly designed by incorporating CAVIAR with Sun Flower Optimization Algorithm (SFO). On the other hand, the pre-processing step is done for duplicate-removed sentences from input multi-document based on stop word removal and stemming. Afterward, text-based features are extracted from pre-processed documents, and then CAV-SFO based Deep RNN is introduced for generating a score; thereby, the internal model parameters are optimally tuned. Finally, the score generated by CAV-SFO based GAN and CAV-SFO based Deep RNN is hybridized, and the final score is obtained using a multi-document compression ratio. The proposed TaylorALO-based GAN showed improved results with maximal precision of 0.989, maximal recall of 0.986, maximal F-Measure of 0.823, maximal Rouge-Precision of 0.930, and maximal Rouge-recall of 0.870.},
  archive      = {J_COMJNL},
  author       = {J, Sheela and B, Janet},
  doi          = {10.1093/comjnl/bxab193},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {727-742},
  shortjournal = {Comput. J.},
  title        = {Caviar-sunflower optimization algorithm-based deep learning classifier for multi-document summarization},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defending against data poisoning attacks: From distributed
learning to federated learning. <em>COMJNL</em>, <em>66</em>(3),
711–726. (<a href="https://doi.org/10.1093/comjnl/bxab192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), a variant of distributed learning (DL), supports the training of a shared model without accessing private data from different sources. Despite its benefits with regard to privacy preservation, FL’s distributed nature and privacy constraints make it vulnerable to data poisoning attacks. Existing defenses, primarily designed for DL, are typically not well adapted to FL. In this paper, we study such attacks and defenses. In doing so, we start from the perspective of DL and then give consideration to a real-world FL scenario, with the aim being to explore the requisites of a desirable defense in FL. Our study shows that (i) the batch size used in each training round affects the effectiveness of defenses in DL, (ii) the defenses investigated are somewhat effective and moderately influenced by batch size in FL settings and (iii) the non-IID data makes it more difficult to defend against data poisoning attacks in FL. Based on the findings, we discuss the key challenges and possible directions in defending against such attacks in FL. In addition, we propose detect and suppress the potential outliers(DSPO), a defense against data poisoning attacks in FL scenarios. Our results show that DSPO outperforms other defenses in several cases.},
  archive      = {J_COMJNL},
  author       = {Tian, Yuchen and Zhang, Weizhe and Simpson, Andrew and Liu, Yang and Jiang, Zoe Lin},
  doi          = {10.1093/comjnl/bxab192},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {711-726},
  shortjournal = {Comput. J.},
  title        = {Defending against data poisoning attacks: From distributed learning to federated learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new approach for resource recommendation in the fog-based
IoT using a hybrid algorithm. <em>COMJNL</em>, <em>66</em>(3), 692–710.
(<a href="https://doi.org/10.1093/comjnl/bxab189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) is an architecture of connected physical objects; these objects can communicate with each other and transmit and receive data. Also, fog-based IoT is a distributed platform that provides reliable access to virtualized resources based on various technologies such as high-performance computing and service-oriented design. A fog recommender system is an intelligent engine that suggests suitable services for fog users with less answer time and more accuracy. With the rapid growth of files and information sharing, fog recommender systems’ importance is also increased. Besides, the resource management problem appears challenging in fog-based IoT because of the fog’s unpredictable and highly variable environment. However, many current methods suffer from the low accuracy of fog recommendations. Due to this problem’s Non-deterministic Polynomial-time (NP)-hard nature, a new approach is presented for resource recommendation in the fog-based IoT using a hybrid optimization algorithm. To simulate the suggested method, the CloudSim simulation environment is used. The experimental results show that the accuracy is optimized by about 1–8\% compared with the Cooperative Filtering method utilizing Smoothing and Fusing and Artificial Bee Colony algorithm. The outcomes of the present paper are notable for scholars, and they supply insights into subsequent study domains in this field.},
  archive      = {J_COMJNL},
  author       = {Xu, Zhiwang and Qin, Huibin and Yang, Shengying and Arefzadeh, Seyedeh Maryam},
  doi          = {10.1093/comjnl/bxab189},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {692-710},
  shortjournal = {Comput. J.},
  title        = {A new approach for resource recommendation in the fog-based IoT using a hybrid algorithm},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient parameter server placement for distributed deep
learning in edge computing. <em>COMJNL</em>, <em>66</em>(3), 678–691.
(<a href="https://doi.org/10.1093/comjnl/bxab188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter servers (PSs) placement is one of the most important factors for global model training on distributed deep learning. This paper formulates a novel problem for placement strategy of PSs in the dynamic available storage capacity, with the objective of minimizing the training time of the distributed deep learning under the constraints of storage capacity and the number of local PSs. Then, we provide the proof for the NP-hardness of the proposed problem. The whole training epochs are divided into two parts, i.e. the first epoch and the other epochs. For the first epoch, an approximation algorithm and a rounding algorithm are proposed in this paper, to solve the proposed problem. For the other epochs, an adjustment algorithm is proposed, by continuously adjusting the decisions for placement strategy of PSs to decrease the training time of the global model. Simulation results show that the proposed approximation algorithm and rounding algorithm perform better than existing works for all cases, in terms of the training time of global model. Meanwhile, the training time of global model for the proposed approximation algorithm is very close to that for optimal solution generated by the brute-force approach for all cases. Besides, the integrated algorithm outperforms the existing works when the available storage capacity varies during the training.},
  archive      = {J_COMJNL},
  author       = {Wu, Yalan and Yan, Jiaquan and Chen, Long and Wu, Jigang and Li, Yidong},
  doi          = {10.1093/comjnl/bxab188},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {678-691},
  shortjournal = {Comput. J.},
  title        = {Efficient parameter server placement for distributed deep learning in edge computing},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid strategy improved whale optimization algorithm for
web service composition. <em>COMJNL</em>, <em>66</em>(3), 662–677. (<a
href="https://doi.org/10.1093/comjnl/bxab187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the number of web services on the Internet, various service providers provide many similar services with the same function but different quality of service (QoS) attributes. It is a key problem to be solved urgently to select the service composition quickly, meeting the users’ QoS requirements from many candidate services. Optimization of web service composition is an NP-hard issue and intelligent optimization algorithms have become the mainstream method to solve this complex problem. This paper proposed a hybrid strategy improved whale optimization algorithm, which is based on the concepts of chaos initialization, nonlinear convergence factor and mutation. By maintaining a balance between exploration and exploitation, the problem of slow or early convergence is overcome to a certain extent. To evaluate its performance more accurately, the proposed algorithm was first tested on a set of standard benchmarks. After, simulations were performed using the real quality of web service dataset. Experimental results show that the proposed algorithm is better than the original version and other meta-heuristic algorithms on average, as well as verifies the feasibility and stability of web service composition optimization.},
  archive      = {J_COMJNL},
  author       = {Ju, Chuanxiang and Ding, Hangqi and Hu, Benjia},
  doi          = {10.1093/comjnl/bxab187},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {662-677},
  shortjournal = {Comput. J.},
  title        = {A hybrid strategy improved whale optimization algorithm for web service composition},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reusable group fuzzy extractor and group-shared bitcoin
wallet. <em>COMJNL</em>, <em>66</em>(3), 643–661. (<a
href="https://doi.org/10.1093/comjnl/bxab185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel cryptographic primitive named reusable group fuzzy extractor (RGFE) allowing any member of a group to extract and reproduce random strings from a fuzzy and non-uniform source of high entropy (called fingerprint). Any group member can anonymously generate a random string for the group using his fingerprint and can be traced when needed, whereas other members can reproduce the string using their own fingerprints. Moreover, a fingerprint can be repeatedly used to generate multiple random strings. Basing on RGFE, we present group-shared Bitcoin wallet, which can be used by a group of users to receive or spend coins via biometrics in a traceable way.},
  archive      = {J_COMJNL},
  author       = {Ma, Jie and Qi, Bin and Lv, Kewei},
  doi          = {10.1093/comjnl/bxab185},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {643-661},
  shortjournal = {Comput. J.},
  title        = {Reusable group fuzzy extractor and group-shared bitcoin wallet},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PBFL: Communication-efficient federated learning via
parameter predicting. <em>COMJNL</em>, <em>66</em>(3), 626–642. (<a
href="https://doi.org/10.1093/comjnl/bxab184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging privacy-preserving technology for machine learning, which enables end devices to cooperatively train a global model without uploading their local sensitive data. Because of limited network bandwidth and considerable communication overhead, communication efficiency has become an essential bottleneck for FL. Existing solutions attempt to improve this situation by reducing communication rounds while usually come with more computation resource consumption or model accuracy deterioration. In this paper, we propose a parameter Prediction-Based DL (PBFL). In which an extended Kalman filter-based prediction algorithm, a practical prediction error threshold setting mechanism and an effective global model updating strategy are included. Instead of collecting all updates from participants, PBFL takes advantage of predicting values to aggregate the model, which substantially reduces required communication rounds while guaranteeing model accuracy. Inspired by the idea of prediction, each participant checks whether its prediction value is out of the tolerance threshold limits and only uploads local updates that have an inaccurate prediction value. In this way, no additional local computational resources are required. Experimental results on both multilayer perceptrons and convolutional neural networks show that PBFL outperforms the state-of-the-art methods and improves the communication efficiency by &gt;66\% with 1\% higher model accuracy.},
  archive      = {J_COMJNL},
  author       = {Li, Kaiju and Xiao, Chunhua},
  doi          = {10.1093/comjnl/bxab184},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {626-642},
  shortjournal = {Comput. J.},
  title        = {PBFL: Communication-efficient federated learning via parameter predicting},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT data quality issues and potential solutions: A
literature review. <em>COMJNL</em>, <em>66</em>(3), 615–625. (<a
href="https://doi.org/10.1093/comjnl/bxab183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things (IoT), data gathered from dozens of devices are the base for creating business value and developing new products and services. If data are of poor quality, decisions are likely to be non-sense. Data quality is crucial to gain business value of the IoT initiatives. This paper presents a systematic literature review regarding IoT data quality from 2000 to 2020. We analyzed 58 articles to identify IoT data quality dimensions and issues and their categorizations. According to this analysis, we offer a classification of IoT data characterizations using the focus group method and clarify the link between dimensions and issues in each category. Manifesting a link between dimensions and issues in each category is incumbent, while this critical affair in extant categorizations is ignored. We also examine data security as an important data quality issue and suggest potential solutions to overcome IoT’s security issues. The finding of this study proposes a new research discipline for additional examination for researchers and practitioners in determining data quality in the context of IoT.},
  archive      = {J_COMJNL},
  author       = {Mansouri, Taha and Sadeghi Moghadam, Mohammad Reza and Monshizadeh, Fatemeh and Zareravasan, Ahad},
  doi          = {10.1093/comjnl/bxab183},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {615-625},
  shortjournal = {Comput. J.},
  title        = {IoT data quality issues and potential solutions: A literature review},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified framework to discover permutation generation
algorithms. <em>COMJNL</em>, <em>66</em>(3), 603–614. (<a
href="https://doi.org/10.1093/comjnl/bxab181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two simple, intuitive and general algorithmic frameworks that can be used to design a wide variety of permutation generation algorithms. The frameworks can be used to produce 19 existing permutation algorithms, including the well-known algorithms of Heap, Wells, Langdon, Zaks, Tompkins and Lipski. We use the frameworks to design two new sorting-based permutation generation algorithms, one of which is optimal.},
  archive      = {J_COMJNL},
  author       = {Ganapathi, Pramod and Chowdhury, Rezaul},
  doi          = {10.1093/comjnl/bxab181},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {603-614},
  shortjournal = {Comput. J.},
  title        = {A unified framework to discover permutation generation algorithms},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An incremental hierarchical clustering based system for
record linkage in e-commerce domain. <em>COMJNL</em>, <em>66</em>(3),
581–602. (<a href="https://doi.org/10.1093/comjnl/bxab179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel record linkage system for E-commerce products is presented. Our system aims to cluster the same products that are crawled from different E-commerce websites into the same cluster. The proposed system achieves a very high success rate by combining both semi-supervised and unsupervised approaches. Unlike the previously proposed systems in the literature, neither a training set nor structured corpora are necessary. The core of the system is based on Hierarchical Agglomerative Clustering (HAC); however, the HAC algorithm is modified to be dynamic such that it can efficiently cluster a stream of incoming new data. Since the proposed system does not depend on any prior data, it can cluster new products. The system uses bag-of-words representation of the product titles, employs a single distance metric, exploits multiple domain-based attributes and does not depend on the characteristics of the natural language used in the product records. To our knowledge, there is no commonly used tool or technique to measure the quality of a clustering task. Therefore in this study, we use ELKI (Environment for Developing KDD-Applications Supported by Index-Structures), an open-source data mining software, for performance measurement of the clustering methods; and show how to use ELKI for this purpose. To evaluate our system, we collect our own dataset and make it publicly available to researchers who study E-commerce product clustering. Our proposed system achieves 96.25\% F-Measure according to our experimental analysis. The other state-of-the-art clustering systems obtain the best 89.12\% F-Measure.},
  archive      = {J_COMJNL},
  author       = {Gözükara, Furkan and Özel, Selma Ayşe},
  doi          = {10.1093/comjnl/bxab179},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {581-602},
  shortjournal = {Comput. J.},
  title        = {An incremental hierarchical clustering based system for record linkage in E-commerce domain},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepSTF: A deep spatial–temporal forecast model of taxi
flow. <em>COMJNL</em>, <em>66</em>(3), 565–580. (<a
href="https://doi.org/10.1093/comjnl/bxab178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taxi flow forecast is significant for planning transportation and allocating basic transportation resources. The flow forecast in the urban adjacent area is different from the fixed-point flow forecast. Their data are more complex and diverse, which make them more challenging to forecast. This paper introduces a deep spatial–temporal forecast (DeepSTF) model for the flow forecasting of urban adjacent area, which divides the urban into grids and makes it have a graph structure. The model builds a spatial–temporal calculation block, which uses graph convolutional network to extract spatial correlation feature and uses two-layer temporal convolutional networks to extract time-dependent feature. Based on the theory of dilation convolution and causal convolution, the model overcomes the under-fitting phenomenon of other models when calculating with rapidly changing data. In order to improve the accuracy of prediction, we take weather as an implicit factor and let it participate in the feature calculation process. A comparison experiment is set between our model and the seven existing traffic flow forecast models. The experimental results prove that the model has better the capabilities of long-term traffic prediction and performs well in various evaluation indicators.},
  archive      = {J_COMJNL},
  author       = {Lv, Zhiqiang and Li, Jianbo and Dong, Chuanhao and Xu, Zhihao},
  doi          = {10.1093/comjnl/bxab178},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {565-580},
  shortjournal = {Comput. J.},
  title        = {DeepSTF: A deep Spatial–Temporal forecast model of taxi flow},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pattern matching model for recognition of stone inscription
characters. <em>COMJNL</em>, <em>66</em>(3), 554–564. (<a
href="https://doi.org/10.1093/comjnl/bxab177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As there are countless significant works done for handwritten character recognition, very meager effort has been reported for inscription characters especially for Tamil stone inscriptions. The real challenge faced in handling stone inscription is dataset collection and foreground and background discrimination. Till present days, the archeological department follows traditional way of capturing, preserving and deciphering stone inscriptions which is manual, more time consuming and need expert assistance. Hence digitized recognition is essential and efficient pattern matching algorithm is needed to be developed to deal with variations in shape and size of complex structured characters present in Tamil stone inscriptions. In this paper, an automated character recognition by pattern matching approach is developed, where character features were extracted by using pattern matching algorithm that helps achieving good recognition rate. Recognition of ancient Tamil stone inscriptions characters and finding their corresponding contemporary Tamil character is done by Image-based Character Pattern Identification (ICPI) system. Modified Speeded Up Robust Feature with Bag of Grapheme (MSURF-BoG) algorithm is implemented to detect the strongest key points from the input character with different orientations. These key point features were created for training the image as a model called Bag of Grapheme (BoG) with code word creation. Hence unsupervised key point features were extracted and pattern matching is performed. 11; century Tamil stone inscriptions were taken as samples which has 7 vowels and 17 consonants, totally 24 characters were used. Here samples with different orientation from each 24 character were used for training the system. The proposed system is evaluated by recognition accuracy which is reported for character wise at the maximum of 96\%.},
  archive      = {J_COMJNL},
  author       = {Devi, K Durga and Maheswari, P Uma and Polasi, Phani Kumar and Preetha, R and Vidhyalakshmi, M},
  doi          = {10.1093/comjnl/bxab177},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {554-564},
  shortjournal = {Comput. J.},
  title        = {Pattern matching model for recognition of stone inscription characters},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel generation method for diverse privacy image based on
machine learning. <em>COMJNL</em>, <em>66</em>(3), 540–553. (<a
href="https://doi.org/10.1093/comjnl/bxab176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have been extensively applied in various fields, and face recognition is one of the most important applications. Artificial intelligence has reached or even surpassed human capabilities in many fields. However, while artificial intelligence application provides convenience to the human lives, it also leads to the risk of privacy leaking. At present, the privacy protection technology for human faces has received extensive attention. Research goals of face privacy protection technology mainly include providing face anonymization and data availability protection. Existing methods usually have insufficient anonymity and they are not easy to control the degree of image distortion, which makes it difficult to achieve the purpose of privacy protection. Moreover, they do not explicitly perform diversity preservation of attributes such as emotions, expressions and ethnicities, so they cannot perform data analysis tasks on non-identity attributes. This paper proposes a diverse privacy face image generation algorithm based on machine learning, called DIVFGEN. This algorithm comprehensively considers image distortion, identity mapping distance loss and emotion classification loss; transforms the privacy protection target into the problem of generating adversarial examples based on the recognition model; and uses an adaptive optimization algorithm to generate anonymity and diversity of privacy images. The experimental results show that on the Cohn-Kanade+ dataset, our algorithm can reduce the probability of facial recognition by the neural network when it accurately classifies sentiment, from 98.6\% to 4.8\%.},
  archive      = {J_COMJNL},
  author       = {Niu, Weina and Luo, Yuheng and Ding, Kangyi and Zhang, Xiaosong and Wang, Yanping and Li, Beibei},
  doi          = {10.1093/comjnl/bxab176},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {540-553},
  shortjournal = {Comput. J.},
  title        = {A novel generation method for diverse privacy image based on machine learning},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-class liver cancer diseases classification using CT
images. <em>COMJNL</em>, <em>66</em>(3), 525–539. (<a
href="https://doi.org/10.1093/comjnl/bxab162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver cancer is the fourth common cancer in the world and the third leading reason of cancer mortality. The conventional methods for detecting liver cancer are blood tests, biopsy and image tests. In this paper, we propose an automated computer-aided diagnosis technique for the classification of multi-class liver cancer i.e. primary, hepatocellular carcinoma, and secondary, metastases using computed tomography (CT) images. The proposed algorithm is a two-step process: enhancement of CT images using contrast limited adaptive histogram equalization algorithm and extraction of features for the detection and the classification of the different classes of the tumor. The overall achieved accuracy, sensitivity and specificity with the proposed method for the classification of multi-class tumors are 97\%, 94.3\% and 100\% with experiment 1 and 84\% all of them with experiment 2, respectively. By automatic feature selection scheme accuracy is deviated maximum by 10.5\% from the overall and the ratio features accuracy decreases linearly by 5.5\% with 20 to 5 selected features. The proposed methodology can help to assist radiologists in liver cancer diagnosis.},
  archive      = {J_COMJNL},
  author       = {Krishan, A and Mittal, D},
  doi          = {10.1093/comjnl/bxab162},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {525-539},
  shortjournal = {Comput. J.},
  title        = {Multi-class liver cancer diseases classification using CT images},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Erratum to: A hybrid machine learning approach for
performance modeling of cloud-based big data applications.
<em>COMJNL</em>, <em>66</em>(2), 524. (<a
href="https://doi.org/10.1093/comjnl/bxab180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Ataie, Ehsan and Evangelinou, Athanasia and Gianniti, Eugenio and Ardagna, Danilo},
  doi          = {10.1093/comjnl/bxab180},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {524},
  shortjournal = {Comput. J.},
  title        = {Erratum to: A hybrid machine learning approach for performance modeling of cloud-based big data applications},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to: A self-tallying electronic voting based on
blockchain. <em>COMJNL</em>, <em>66</em>(2), 523. (<a
href="https://doi.org/10.1093/comjnl/bxab175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Zeng, Gongxian and He, Meiqi and Yiu, Siu Ming and Huang, Zhengan},
  doi          = {10.1093/comjnl/bxab175},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {523},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: A self-tallying electronic voting based on blockchain},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lung lobe segmentation and feature extraction-based
hierarchical attention network for COVID-19 prediction from chest x-ray
images. <em>COMJNL</em>, <em>66</em>(2), 508–522. (<a
href="https://doi.org/10.1093/comjnl/bxac136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is a rising respiratory sickness. It causes harsh pneumonia and is considered to cover higher collisions in the healthcare domain. The diagnosis at an early stage is more complex to get accurate treatment for reducing the stress in the clinical sector. Chest X-ray scan is the standard imaging diagnosis test employed for pneumonia disease. Automatic detection of COVID-19 helps to control the community outbreak but tracing this viral infection through X-ray results in a challenging task in the medical community. To automatically detect the viral disease in order to reduce the mortality rate, an effective COVID-19 detection method is modelled in this research by the proposed manta-ray multi-verse optimization-based hierarchical attention network (MRMVO-based HAN) classifier. Accordingly, the MRMVO is the incorporation of manta-ray foraging optimization and multi-verse optimizer. Based on the segmented lung lobes, the features are acquired from segmented regions in such a way that the process of COVID-19 detection mechanism is carried out with the features acquired from interested lobe regions. The proposed method has good performance with the measures, such as accuracy, true positive rate and true negative rate with the values of 93.367, 89.921 and 95.071\%.},
  archive      = {J_COMJNL},
  author       = {Christina Magneta, S and Sundar, C and Thanabal, M S},
  doi          = {10.1093/comjnl/bxac136},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {508-522},
  shortjournal = {Comput. J.},
  title        = {Lung lobe segmentation and feature extraction-based hierarchical attention network for COVID-19 prediction from chest X-ray images},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-point and triple-point queries visibility constrained
minimum link paths in simple polygons. <em>COMJNL</em>, <em>66</em>(2),
496–507. (<a href="https://doi.org/10.1093/comjnl/bxab200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the query version of constrained minimum link paths between two points inside a simple polygon ; with ; vertices such that there is at least one point on the path, visible from a query point. The method is based on partitioning ; into a number of faces of equal link distance from a point. This partitioning is essentially a link-based shortest path map. Initially, we solve this problem for two given points ; , ; and a query point ; . Then, the proposed solution is extended to a general case for three arbitrary query points: ; , ; and ; . In the former case, we propose an algorithm with ; preprocessing time. For the latter case, we develop an algorithm with ; preprocessing time. The link distance of a ; -; path between ; , ; and the path are provided in time ; and ; , respectively, for the above two cases, where ; is the number of links.},
  archive      = {J_COMJNL},
  author       = {Reza Zarrabi, Mohammad and Moghaddam Charkari, Nasrollah},
  doi          = {10.1093/comjnl/bxab200},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {496-507},
  shortjournal = {Comput. J.},
  title        = {Single-point and triple-point queries visibility constrained minimum link paths in simple polygons},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical attacks on reduced-round 3D and saturnin.
<em>COMJNL</em>, <em>66</em>(2), 479–495. (<a
href="https://doi.org/10.1093/comjnl/bxab174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D, an advanced encryption standard-like cipher employed three-dimensional structure, was proposed in 2008. Its recommended number of rounds is 22. Although the longest key recovery attack can currently reach 13 rounds, the complexity of existing attacks for &gt;6 rounds seems to exceed the practically feasible complexity. Thus, a practical attack for 7-round 3D has yet to be developed. Recently, a lightweight block cipher called Saturnin has been selected as a second-round candidate in the National Institute of Standards and Technology standardization for lightweight cryptography. Saturnin also employs a three-dimensional structure and provides high security against quantum and classic attacks. In this paper, we investigate the yoyo attack on these two ciphers. Combined with the meet-in-the-middle technique, we apply the yoyo trick to 7-round 3D and recover the whole 512-bit secret key with ; plaintexts and adaptively chosen ciphertexts and ; complexity of full encryptions. To our best knowledge, it is the first practical key recovery attack for 7-round 3D to date. For Saturnin, we found a minor typo in its design report. The designers intended to make a super round containing two S-layers, but one was inadvertently omitted in the algorithm description. We propose a 5-super-round key recovery attack, which is suitable for both one-S-layer version and two-S-layer version. Since the round function of Saturnin has better diffusion, which leads that the meet-in-the-middle technique cannot be applied to this cipher directly. For the one-S-layer version, we address this problem by proposing a new technique called ; . This technique will fail on the other version, which proves the necessity of containing two S-layers in one-super-round. Finally, our attack requires ; plaintext pairs and adaptively chosen ciphertext pairs and ; one-round encryptions.},
  archive      = {J_COMJNL},
  author       = {Hou, Tao and Cui, Ting and Zhang, Jiyan},
  doi          = {10.1093/comjnl/bxab174},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {479-495},
  shortjournal = {Comput. J.},
  title        = {Practical attacks on reduced-round 3D and saturnin},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart multimedia compressor—intelligent algorithms for text
and image compression. <em>COMJNL</em>, <em>66</em>(2), 463–478. (<a
href="https://doi.org/10.1093/comjnl/bxab173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of text compression algorithms have been proposed in the past decades, which have been very effective and usually operate on conventional character/word-based approaches. A novel data compression perspective of data mining is explored in this research and the paper focuses on novel frequent sequence/pattern mining approach to text compression. This work attempts to make use of longer-range correlations between words in languages for achieving better text compression. We propose a novel and efficient method by making the compression of any word-level text in a universal manner for corpora across domains referred as Universal Huffman Tree-based encoding. The major contribution of this work is in terms of avoidance of code table communication to the decoder. Simulation results over benchmark datasets indicate that Universal Huffman encoding employing frequent sequence mining (achieves [20\%, 89\%] improvement in compression in reduced time. The paper also contributes a usable interface for Data compression that employs the proposed frequent sequent mining-based data compression algorithm. The interface supports features such as feedback, consistency, usability, navigation, visual appeal, performance and accessibility on par with existing compression softwares. The work results in an intelligent data compression software employing knowledge engineering perspective.},
  archive      = {J_COMJNL},
  author       = {Oswald, C and Sivaselvan, B},
  doi          = {10.1093/comjnl/bxab173},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {463-478},
  shortjournal = {Comput. J.},
  title        = {Smart multimedia Compressor—Intelligent algorithms for text and image compression},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability evaluation of clustered faults for regular
networks under the probabilistic diagnosis model. <em>COMJNL</em>,
<em>66</em>(2), 441–462. (<a
href="https://doi.org/10.1093/comjnl/bxab172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scale of the system expands, processor failures are inevitable. Fault diagnosis has great significance in analyzing the reliability of multiprocessing systems. Probabilistic fault diagnosis is a method that attempts to diagnose nodes correctly with high probability. In this paper, we extend the threshold ; to threshold ; for regular networks based on probabilistic diagnosis algorithm and determine the status of a cluster of nodes by analyzing the local performance. Moreover, we evaluate the global performance based on the Poisson distribution and the Binomial distribution and show that the achievement in terms of correctness demonstrates a good performance. Finally, we employ the probabilistic diagnosis scheme to explore some well-known networks, including complete cubic networks, dual cubes and hierarchical hypercubes as well.},
  archive      = {J_COMJNL},
  author       = {Li, Xiao-Yan and Zhang, Yufang and Liu, Ximeng and Wang, Xiangke and Cheng, Hongju},
  doi          = {10.1093/comjnl/bxab172},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {441-462},
  shortjournal = {Comput. J.},
  title        = {Reliability evaluation of clustered faults for regular networks under the probabilistic diagnosis model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An advanced EEG motion artifacts eradication algorithm.
<em>COMJNL</em>, <em>66</em>(2), 429–440. (<a
href="https://doi.org/10.1093/comjnl/bxab170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalography (EEG) signal is corrupted with some non-cerebral activities due to patient movement during signal measurement. These non-cerebral activities are termed as artifacts, which may diminish the superiority of acquired EEG signal statistics. The state of the art artifact elimination approaches applied canonical correlation analysis (CCA) for confiscating EEG motion artifacts accompanied by ensemble empirical mode decomposition (EEMD). An improved cascaded approach based on Gaussian elimination CCA (GECCA) and EEMD is applied to suppress EEG artifacts effectively. However, in a highly noisy environment, a novel addition of median filter before the GECCA algorithm is suggested for improving the accuracy of onslaught the EEG signal. The median filter is opted due to its edge preserving nature and speed. This proposed approach is appraised using efficacy grounds for instance Del signal to noise ratio, Lambda (λ), root mean square error and receiver operating characteristic (ROC) parameters and verified contrary to presently obtainable EEG artifacts exclusion methods. The primary concern is to improve the efficacy and precision of the proposed artifact elimination technique. The elapsed time is also calculated to evaluate the computation efficiency. Results show that the proposed algorithm is appropriate to be used as an addition to existing algorithms in use.},
  archive      = {J_COMJNL},
  author       = {Shukla, Piyush Kumar and Roy, Vandana and Shukla, Prashant Kumar and Chaturvedi, Anoop Kumar and Saxena, Aumreesh Kumar and Maheshwari, Manish and Pal, Parashu Ram},
  doi          = {10.1093/comjnl/bxab170},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {429-440},
  shortjournal = {Comput. J.},
  title        = {An advanced EEG motion artifacts eradication algorithm},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density-based dynamically self-parameterized clustering for
material inspection. <em>COMJNL</em>, <em>66</em>(2), 416–428. (<a
href="https://doi.org/10.1093/comjnl/bxab169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ultrasonic based nondestructive testing (NDT) is the common technique used to perform material testing using ultrasonic signals. These signals are difficult to interpret and the examiner has to focus on every sampling signal to observe the changes in characteristics of signals. The core target points of the proposed work are used to identify the size and position of the defects as fast as possible. For that, an unsupervised machine learning approach is proposed to analyze defects such as shrinkage, porosity, crack, discontinuity, lack of fusion, lack of penetration and overlap. This would be helpful in the domain of material science and knowledge mining to study the structural integrity of metals during the manufacturing process and can be applied in automobile industries to increase the quality of manufactured parts. The proposed work incorporates a novel Density-Based Dynamically Self-Parameterized Clustering for Material Inspection (DBDSPCMI) method to effectively predict and identify the defect size and its position. The proposed method proves to be effective with an accuracy of 97.04\% in measuring defect size and 95\% in identifying defect position.},
  archive      = {J_COMJNL},
  author       = {Radha, P and Selvakumar, N and Raja Sekar, J and Johnsonselva, J V},
  doi          = {10.1093/comjnl/bxab169},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {416-428},
  shortjournal = {Comput. J.},
  title        = {Density-based dynamically self-parameterized clustering for material inspection},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Potential of machine learning based support vector
regression for solar radiation prediction. <em>COMJNL</em>,
<em>66</em>(2), 399–415. (<a
href="https://doi.org/10.1093/comjnl/bxab168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurements of the solar radiation quantities profoundly affect on the energy output ratios. A decline in solar radiation measurements in many countries, which is due to reasons high cost, difficulty of measurement that necessitated developing different methods to estimate the proportion of solar radiation. Many empirical models have been developed using special variables and coefficients, such as ; . The development of machine-learning algorithms makes these algorithms as a possible application instead of the empirical models to decrease the error rate and obtaining better results. In this paper, radial basis function is applied as the kernel function of support vector regression (SVR) method to calculate the amount of monthly average daily of the global solar radiation in four sites in Egypt. Five variables used as input (sunshine duration, air temperature, relative humidity, solar declination angle and extraterrestrial solar radiation). The experimental results have a good estimation in all locations according to root mean square error, however, this study proved that SVR models can be as an efficient machine-learning technique with a higher accuracy.},
  archive      = {J_COMJNL},
  author       = {Mohamed, Zahraa E and Saleh, Hussein H},
  doi          = {10.1093/comjnl/bxab168},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {399-415},
  shortjournal = {Comput. J.},
  title        = {Potential of machine learning based support vector regression for solar radiation prediction},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault-tolerant strongly hamiltonian laceability and
hyper-hamiltonian laceability of cayley graphs generated by
transposition trees. <em>COMJNL</em>, <em>66</em>(2), 384–398. (<a
href="https://doi.org/10.1093/comjnl/bxab167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bipartite graph is Hamiltonian laceable if any two of its vertices in different partite sets are connected by a Hamiltonian path. A Hamiltonian laceable graph ; is called strongly Hamiltonian laceable if any two of its vertices in the same partite set are connected by a path of length ; . A Hamiltonian laceable graph ; (with two partite sets ; ) is called hyper-Hamiltonian laceable, if for any vertex ; for ; , there is a Hamiltonian path of ; between any two vertices in ; . In this paper, we focus on the edge-fault-tolerant strongly Hamiltonian laceability and hyper-Hamiltonian laceability on the class of Cayley graphs generated by transposition trees, which are a generalization of star graph and bubble-sort graph. For every ; -dimensional Cayley graph generated by a transposition tree ; , we show that ; is strongly Hamiltonian laceable for any ; with ; , which generalizes results in [ ; , ; ], and show that ; is hyper-Hamiltonian laceable for any ; with ; .},
  archive      = {J_COMJNL},
  author       = {Xue, Shudan and Deng, Qingying and Li, Pingshan},
  doi          = {10.1093/comjnl/bxab167},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {384-398},
  shortjournal = {Comput. J.},
  title        = {Fault-tolerant strongly hamiltonian laceability and hyper-hamiltonian laceability of cayley graphs generated by transposition trees},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting urban anomalies using factor analysis and one
class support vector machine. <em>COMJNL</em>, <em>66</em>(2), 373–383.
(<a href="https://doi.org/10.1093/comjnl/bxab166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of anomalies in spatiotemporal traffic data is not only critical for intelligent transportation systems and public safety but also very challenging. Anomalies in traffic data often exhibit complex forms in two aspects, (i) spatiotemporal complexity (i.e. we need to associate individual locations and time intervals formulating a panoramic view of an anomaly) and (ii) multi-source complexity (i.e. we need an algorithm that can model the anomaly degree of the multiple data sources of different densities, distributions and scales). To tackle these challenges, we proposed a three-step method that uses factor analysis to extract features, then uses the goodness-of-fit test to obtain the anomaly score of a single data point and then uses one class support vector machine to synthesize the anomaly score. Finally, we conduct extensive experiments on real-world trip data include taxi and bike data. And these extensive experiments demonstrate the effectiveness of our proposed approach.},
  archive      = {J_COMJNL},
  author       = {Lu, Cong and Huang, Jianbin and Huang, Longji},
  doi          = {10.1093/comjnl/bxab166},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {373-383},
  shortjournal = {Comput. J.},
  title        = {Detecting urban anomalies using factor analysis and one class support vector machine},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Authenticated tree-based r-LWE group key exchange.
<em>COMJNL</em>, <em>66</em>(2), 360–372. (<a
href="https://doi.org/10.1093/comjnl/bxab165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first constant round, multicast, authenticated tree-based R-LWE group key exchange protocol with logarithmic communication and memory complexity. Our protocol achieves post-quantum security through a reduction to a Diffie–Hellman-like analogue to the decisional R-LWE problem. We also present a sequential version with constant memory complexity but a logarithmic number of rounds and communication complexity.},
  archive      = {J_COMJNL},
  author       = {Hougaard, Hector and Miyaji, Atsuko},
  doi          = {10.1093/comjnl/bxab165},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {360-372},
  shortjournal = {Comput. J.},
  title        = {Authenticated tree-based R-LWE group key exchange},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rare correlated coherent association rule mining with
CLS-MMS. <em>COMJNL</em>, <em>66</em>(2), 342–359. (<a
href="https://doi.org/10.1093/comjnl/bxab164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of coherent association rules based on propositional logic is an important area of association rule mining. Users may get a large number of itemsets for low minsup and lose valuable itemsets for high minsup. Mining without minsup may cause itemset explosions that contain spurious itemsets with low correlations and take a long time to mine. For mining coherence rules, existing approaches consider only the frequent itemsets, ignoring rare itemsets. Moreover, all items in the database are regarded equally important, which is not practical in real-world applications. By using the confidence-lift specified multiple minimum supports combined with propositional logic, we propose an efficient approach called rare correlated coherent association rule mining that addresses all of the problems stated above. We define and incorporate termination bound of support (; ) and termination bound of dissociation (; ) for early pruning of the candidate itemsets. In the proposed approach, support thresholds are automatically applied to the itemsets and coherent association rules are derived from the frequent and rare itemsets with high correlation and confidence. Experimental results obtained from real-life datasets show the effectiveness of the proposed approach in terms of itemsets and rule generation, correlation, confidence, runtime and scalability.},
  archive      = {J_COMJNL},
  author       = {Datta, Subrata and Mali, Kalyani and Ghosh, Udit and Bose, Subrata and Das, Sourav and Ghosh, Sourav},
  doi          = {10.1093/comjnl/bxab164},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {342-359},
  shortjournal = {Comput. J.},
  title        = {Rare correlated coherent association rule mining with CLS-MMS},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessment and monitoring of salinity of soils after tsunami
in nagapattinam area using fuzzy logic-based classification.
<em>COMJNL</em>, <em>66</em>(2), 333–341. (<a
href="https://doi.org/10.1093/comjnl/bxab163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research attempts to know the salinity level and how much agricultural land was affected by the 2004 Indian Ocean tsunami in the study area. Nagapattinam province of Tamilnadu, which was strongly hit by the 2004 Indian Ocean tsunami, is selected as the demonstration site. IKONOS and QuickBird image for the periods 2003 (pre-tsunami, IKONOS), 2004 (immediate tsunami, IKONOS) and 2006 (post-tsunami, QuickBird) have been used in this study. The purpose of this research is to detect the effects of the tsunami in the study area. Multispectral images were obtained in order to detect the salt-affected soils and compare the data with images after the tsunami. The near-infrared reflectance spectra contain significant information related to soil components. The development of remote sensing techniques can support the monitoring and efficient mapping of salinity level in soil for environmental assessment. In this research, we have implemented the techniques in four levels: image preprocessing, extracting land region, feature extraction and image classification. Image preprocessing is the first step in the image processing chain and is usually necessary prior to image classification and analysis. In the second level, land regions are extracted from the study area using the Soil-Adjusted Vegetation Indices. In the feature extraction level, we have extracted 15 salinity features from the image data and performed the feature selection method to select the best five features of soil. In the classification stage, the salt-affected land region is classified into three classes (C1, highly saline; C2, saline; and C3, non-saline) using the fuzzy logic method. In the immediate tsunami and after-tsunami data, 95.15- and 121.4-hectare area, respectively, were detected as a salt-affected area.},
  archive      = {J_COMJNL},
  author       = {Jiji, W and Merlin, G and Rajesh, A},
  doi          = {10.1093/comjnl/bxab163},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {333-341},
  shortjournal = {Comput. J.},
  title        = {Assessment and monitoring of salinity of soils after tsunami in nagapattinam area using fuzzy logic-based classification},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving bitcoin transaction propagation efficiency through
local clique network. <em>COMJNL</em>, <em>66</em>(2), 318–332. (<a
href="https://doi.org/10.1093/comjnl/bxab186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is a popular decentralized cryptocurrency, and the Bitcoin network is essentially an unstructured peer-to-peer (P2P) network that can synchronize distributed database of replicated ledgers through message broadcasting. In the Bitcoin network, the average clustering coefficient of nodes is very high, resulting in low message propagation efficiency. In addition, average node degree in the Bitcoin network is also considerably large, causing high message redundancy when nodes use the gossip protocol to broadcast messages. These may affect message propagation speed, hindering Bitcoin from being applied to scenarios of high transactional throughputs. To illustrate, we have collected single-hop propagation data of transactions of 366 blocks from Bitcoin Core. The analysis results show that transaction verification and network delay are two major causes of low transaction propagation efficiency. In this paper, we propose a novel P2P network structure, called ; (LCN), for message broadcasting in the Bitcoin network. Specifically, to reduce transaction validation latency and message redundancy, in LCN local nodes (logically) form cliques, and only a few nodes in a clique broadcast messages to the other cliques, instead of each node sending messages to its neighboring nodes. We have conducted extensive experiments, and the results show that message redundancy is low in LCN, and message propagation speed increases significantly. Meanwhile, LCN exhibits excellent robustness when average node degree remains high in the Bitcoin network.},
  archive      = {J_COMJNL},
  author       = {Yan, Kailun and Zhang, Jilian and Wu, Yongdong},
  doi          = {10.1093/comjnl/bxab186},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {318-332},
  shortjournal = {Comput. J.},
  title        = {Improving bitcoin transaction propagation efficiency through local clique network},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-simulation and formal verification of co-operative drone
control with logic-based specifications. <em>COMJNL</em>,
<em>66</em>(2), 295–317. (<a
href="https://doi.org/10.1093/comjnl/bxab161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) co-operative systems are complex cyber-physical systems that integrate a high-level control algorithm with pre-existing closed implementations of lower-level vehicle kinematics. In model-driven development, simulation is one of the techniques that are usually applied, together with testing, in the analysis of system behaviours. This work proposes a method and tools to validate the design of UAV co-operative systems based on co-simulation and formal verification. The method uses the Prototype Verification System, an interactive theorem prover based on a higher-order logic language, and the Functional Mock-up Interface, a widely accepted standard for co-simulation. In this paper, results on the co-simulation and proofs of safety requirements of a representative co-ordination algorithm are shown and discussed in a scenario where quadcopters are deployed and perform space-coverage operations.},
  archive      = {J_COMJNL},
  author       = {Bernardeschi, Cinzia and Domenici, Andrea and Fagiolini, Adriano and Palmieri, Maurizio},
  doi          = {10.1093/comjnl/bxab161},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {295-317},
  shortjournal = {Comput. J.},
  title        = {Co-simulation and formal verification of co-operative drone control with logic-based specifications},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fractional sailfish optimizer with deep convolution neural
network for compressive sensing based magnetic resonance image
reconstruction. <em>COMJNL</em>, <em>66</em>(2), 280–294. (<a
href="https://doi.org/10.1093/comjnl/bxab160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance image (MRI) is extensively adapted in clinical diagnosis due to its improved representation of changes with soft tissue. The current innovations in compressive sensing (CS) showed that it is viable to precisely reconstruct MRI with K-space data with reduced scanning duration and it takes more time for computation and it is complex to capture the fine details. The CS is capable to minimize the imaging time and it uses compressibility for reconstructing the images. In this paper, a novel deep-learning method is devised for reconstructing images using a set of MRI. Here, MRI acquisition and MRIsub sampling is performed for reducing the size of images to perform improved analysis. Here, a convolution layer is utilized for imitating the process of compressed sampling that can learn the sampling matrix to prevent complex designs. In addition, a convolution layer is utilized to execute the initial reconstruction. In addition, Deep Convolutional Neural Network (Deep CNN) is adapted for image reconstruction. The training of Deep CNN is performed using proposed Fractional Sailfish Optimizer (FSO) algorithm. The proposed FSO is designed by incorporating fractional concepts in Sail fish optimization for tuning the optimal weights of Deep CNN. The proposed FSO-based CSNet outperformed other methods with minimal MSE of 1.877 and maximal PSNR of 45.396 dB.},
  archive      = {J_COMJNL},
  author       = {Kumar, Penta Anil and Gunasundari, R and Aarthi, R},
  doi          = {10.1093/comjnl/bxab160},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {280-294},
  shortjournal = {Comput. J.},
  title        = {Fractional sailfish optimizer with deep convolution neural network for compressive sensing based magnetic resonance image reconstruction},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The key-dependent capacity in multidimensional linear
cryptanalysis. <em>COMJNL</em>, <em>66</em>(2), 269–279. (<a
href="https://doi.org/10.1093/comjnl/bxab159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity is an important parameter in multidimensional linear attack. In this paper, we firstly explore the distribution of the key-dependent capacity. Based on the magnitude of the correlation contributions, we divide the linear approximations subspace into two sets: one set consists of the strong linear approximations, and the other set consists of the weak linear approximations. We construct two statistics using the linear approximations in the two sets, respectively. Under reasonable assumptions, both of the two statistics follow Gamma distribution. Thus, the capacity is the sum of two statistics that follow Gamma distribution. Secondly, the accuracy of the model is verified by experiments on SMALLPRESENT[4]. Our experimental results show that this model can estimate the variance of the key-dependent capacity more accurately. Thus, we obtain more precise knowledge of the data complexity of the multidimensional linear attack. We derive the upper bound of the data complexity for multidimensional linear attack. Finally, based on our theoretical results, we explore the data complexity of Cho’s multidimensional linear attack on PRESENT. Our results are the smallest data complexity for the same round attack so far.},
  archive      = {J_COMJNL},
  author       = {Cao, Wenqin and Zhang, Wentao and Zhao, Xuefeng},
  doi          = {10.1093/comjnl/bxab159},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {269-279},
  shortjournal = {Comput. J.},
  title        = {The key-dependent capacity in multidimensional linear cryptanalysis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to: Congestion free transient plane(CFTP) using
bandwidth sharing during link failures in SDN. <em>COMJNL</em>,
<em>66</em>(1), 267. (<a
href="https://doi.org/10.1093/comjnl/bxaa024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Vanamoorthy, Muthumanikandan and Chinnaiah, Valliyammai},
  doi          = {10.1093/comjnl/bxaa024},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {267},
  shortjournal = {Comput. J.},
  title        = {Corrigendum to: Congestion free transient Plane(CFTP) using bandwidth sharing during link failures in SDN},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Squirrel search deer hunting-based deep recurrent neural
network for survival prediction using PAN-cancer gene expression data.
<em>COMJNL</em>, <em>66</em>(1), 245–266. (<a
href="https://doi.org/10.1093/comjnl/bxab158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper devises a novel technique, namely Squirrel Search Deer Hunting-based deep recurrent neural network (SSDH-based DRNN) for cancer-survival rate prediction using gene expression (GE) data. Initially, the input GE data are transformed using the polynomial kernel data transformation. Then entropy-based Bayesian fuzzy clustering is employed for gene selection. Then, the selected features are strengthened through survival indicators based on time series data features, like simple moving average (SMA) and rate of change. Finally, the survival rate prediction is performed using a deep recurrent neural network (DRNN), in which the training is carried out with squirrel search deer hunting (SSDH). The proposed SSDH algorithm is devised by combining Squirrel Search Algorithm (SSA) and deer hunting optimization algorithm (DHOA). The performance of the proposed methodology is analyzed using Pan-Cancer (PANCAN) dataset with a prediction error of 4.05\%, RMSE of 7.58, the accuracy of 90.98\%, precision of 90.80\%, recall of 92.03\% and F1-score of 91.41\%. The devised method with higher prediction accuracy and the lower prediction error is employed for the cancer survival prediction of the patients for the cancer prognosis. Besides, it will be helpful for the clinical management of cancer patients.},
  archive      = {J_COMJNL},
  author       = {Majji, Ramachandro and Rajeswari, R and Vidyadhari, Ch and Cristin, R},
  doi          = {10.1093/comjnl/bxab158},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {245-266},
  shortjournal = {Comput. J.},
  title        = {Squirrel search deer hunting-based deep recurrent neural network for survival prediction using PAN-cancer gene expression data},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective resource allocation and load balancing in
hierarchical HetNets: Toward QoS-aware multi-access edge computing.
<em>COMJNL</em>, <em>66</em>(1), 229–244. (<a
href="https://doi.org/10.1093/comjnl/bxab157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a key feature of next-generation heterogeneous networks aimed at providing a variety of services for different applications by performing related processing tasks closer to the user equipment. In this research, we investigated on connection management approaches in multi-access edge computing systems. This paper presents joint radio resource allocation and MEC optimization in a multi-layer NOMA HetNet in order to maximize system’s energy efficiency. The continues carrier allocation and handoff decision variables, in addition to the interference incorporated in the goal function, modifies the primary optimization problem to a mixed integer nonlinear programming. Network selection is done statically based on the Analytic Hierarchy Process, and station selection is done dynamically based on the Data Envelope Analysis method. Also, an effective feedback mechanism has been designed in collaboration with the server resource manager to solve a global optimization problem in order to load balancing and meet the users quality of service constraints simultaneously. To reduce the computational complexity and to achieve a locally optimal solution, we applied variable relaxation and majorization minimization approach in which offloading decision and multi-part Markov noise analysis have been developed to model users’ preferences without the need for explicit information from the users. Based on the simulations, the proposed approach not only results in a significant increase of system’s energy efficiency but also enhances the system throughput in multiple-source scenarios.},
  archive      = {J_COMJNL},
  author       = {Bonab, Mohammad Jalilvand Aghdam and Kandovan, Ramin Shaghaghi},
  doi          = {10.1093/comjnl/bxab157},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {229-244},
  shortjournal = {Comput. J.},
  title        = {Effective resource allocation and load balancing in hierarchical HetNets: Toward QoS-aware multi-access edge computing},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid fault diagnosis capability analysis of highly
connected graphs. <em>COMJNL</em>, <em>66</em>(1), 221–228. (<a
href="https://doi.org/10.1093/comjnl/bxab156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zhu ; introduced the ; -edge tolerable diagnosability to measure the fault diagnosis capability of a multiprocessor system with faulty links. This kind of diagnosability is a generalization of the concept of traditional diagnosability. A graph is called a maximally connected graph if its minimum degree equals its vertex connectivity. It is well-known that many irregular networks are maximally connected graphs and the ; -edge tolerable diagnosabilities of these networks are unknown, which is our motivation for research. In this paper, we obtain the lower bound of the ; -edge tolerable diagnosability of a class of ; -connected graphs and establish the ; -edge tolerable diagnosability of a class of maximally connected graphs under the PMC model and the MM; model, which extend some results in (Hakimi, S.L. and Amin, A.T. (1974) Characterization of connection assignment of diagnosable systems. ; , (Chang, C.P., Lai, P.L., Tan, J.J.M. and Hsu, L.H. (2004) Diagnosability of t-connected networks and product networks under the comparison diagnosis model. ; , 53, 1582–1590) and (Lian, G., Zhou, S., Hsieh, S.Y., Liu, J., Chen, G. and Wang, Y. (2019) Performance evaluation on hybrid fault diagnosability of regular networks. ; , 796, 147–153).},
  archive      = {J_COMJNL},
  author       = {Wei, Yulong and Li, Rong-hua and Yang, Weihua},
  doi          = {10.1093/comjnl/bxab156},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {221-228},
  shortjournal = {Comput. J.},
  title        = {Hybrid fault diagnosis capability analysis of highly connected graphs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Construction of lightweight authenticated joint arithmetic
computation for 5G IoT networks. <em>COMJNL</em>, <em>66</em>(1),
208–220. (<a href="https://doi.org/10.1093/comjnl/bxab155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next generation of Internet of Things (IoT) networks and mobile communications (5G IoT networks) has the particularity of being heterogeneous, therefore, it has very strong ability to compute, store, etc. Group-oriented applications demonstrate its potential ability in 5G IoT networks. One of the main challenges for secure group-oriented applications (SGA) in 5G IoT networks is how to secure communication and computation among these heterogeneous devices. Conventional protocols are not suitable for SGA in 5G IoT networks since multiparty joint computation in this environment requires lightweight communication and computation overhead. Furthermore, the primary task of SGA is to securely transmit various types of jointly computing data. Hence, membership authentication and secure multiparty joint arithmetic computation become two fundamental security services in SGA for 5G IoT networks. The membership authentication allows communication entities to authenticate their communication partners and the multiparty joint computations allow a secret output to be shared among all communication entities. The multiparty joint computation result can be used to protect exchange information in the communication or be used as a result that all users jointly compute by using their secret inputs. A novel construction of computation/communications-efficient membership authenticated joint arithmetic computation is proposed in this paper for 5G IoT networks, which not only integrates the function of membership authentication and joint arithmetic computation but also realizes both computation and communication efficiency on each group member side. Our protocol is secure against inside attackers and outside attackers, and also meets all the described security goals. Meanwhile, in this construction the privacy of tokens can be well protected so tokens can be reused multiple times. This proposal is noninteractive and can be easily extended to joint arithmetic computation with any number of inputs. Hence, our design has more attraction for lightweight membership authenticated joint arithmetic computation in 5G IoT networks.},
  archive      = {J_COMJNL},
  author       = {Hsu, Chingfang and Harn, Lein and Xia, Zhe and Cui, Jianqun and Chen, Jingxue},
  doi          = {10.1093/comjnl/bxab155},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {208-220},
  shortjournal = {Comput. J.},
  title        = {Construction of lightweight authenticated joint arithmetic computation for 5G IoT networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-key fully homomorphic encryption from additive
homomorphism. <em>COMJNL</em>, <em>66</em>(1), 197–207. (<a
href="https://doi.org/10.1093/comjnl/bxab154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption (FHE) allows direct computations over the encrypted data without access to the decryption. Hence multi-key FHE is well suitable for secure multiparty computation. Recently, Brakerski ; (TCC 2019 and EUROCRYPT 2020) utilized additively homomorphic encryption to construct FHE schemes with different properties. Motivated by their work, we are attempting to construct multi-key FHE schemes via additively homomorphic encryption. In this paper, we propose a general framework of constructing multi-key FHE, combining the additively homomorphic encryption with specific multiparty computation protocols constructed from encryption switching protocol. Concretely, every involved party encrypts his plaintexts with an additively homomorphic encryption under his own public key. Then the ciphertexts are evaluated by suitable multiparty computation protocols performed by two cooperative servers without collusion. Furthermore, an instantiation with an ElGamal variant scheme is presented. Performance comparisons show that our multi-key FHE from additively homomorphic encryption is more efficient and practical.},
  archive      = {J_COMJNL},
  author       = {Xu, Wenju and Wang, Baocang and Hu, Yupu and Duan, Pu and Zhang, Benyu and Liu, Momeng},
  doi          = {10.1093/comjnl/bxab154},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {197-207},
  shortjournal = {Comput. J.},
  title        = {Multi-key fully homomorphic encryption from additive homomorphism},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-keyword ranked searchable encryption with the wildcard
keyword for data sharing in cloud computing. <em>COMJNL</em>,
<em>66</em>(1), 184–196. (<a
href="https://doi.org/10.1093/comjnl/bxab153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-keyword ranked searchable encryption (MRSE) supports multi-keyword contained in one query and returns the top-k search results related to the query keyword set. It realized effective search on encrypted data. Most previous works about MRSE can only make the complete keyword search and rank on the server-side. However, with more practice, users may not be able to express some keywords completely when searching. Server-side ranking increases the possibilities of the server inferring some keywords queried, leading to the leakage of the user’s sensitive information. In this paper, we propose a new MRSE system named ‘multi-keyword ranked searchable encryption with the wildcard keyword (MRSW)’. It allows the query keyword set to contain a wildcard keyword by using Bloom filter (BF). Using hierarchical clustering algorithm, a clustering Bloom filter tree (CBF-Tree) is constructed, which improves the efficiency of wildcard search. By constructing a modified inverted index (MII) table on the basis of the term frequency-inverse document frequency (TF-IDF) rule, the ranking function of MRSW is performed by the user. MRSW is proved secure under adaptive chosen-keyword attack (CKA2) model, and experiments on a real data set from the web of science indicate that MRSW is efficient and practical.},
  archive      = {J_COMJNL},
  author       = {Liu, Jinlu and Zhao, Bo and Qin, Jing and Zhang, Xi and Ma, Jixin},
  doi          = {10.1093/comjnl/bxab153},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {184-196},
  shortjournal = {Comput. J.},
  title        = {Multi-keyword ranked searchable encryption with the wildcard keyword for data sharing in cloud computing},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A verifiable symmetric searchable encryption scheme based on
the AVL tree. <em>COMJNL</em>, <em>66</em>(1), 174–183. (<a
href="https://doi.org/10.1093/comjnl/bxab152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable symmetric searchable encryption is a keyword search technology that supports verification of search results. Many schemes improve search performance by dividing each keyword label into segments and storing them in a Trie-tree at the expense of high storage. And the index will degenerate into a linear linked list when all keyword labels have the same prefix except for the last segment. But it will greatly affect the search efficiency. In this paper, we propose a verifiable symmetric searchable encryption scheme based on the AVL Tree (abbreviated as VSSE-AVL), which uses complete keyword labels to build the index. Compared with the Trie-tree index, VSSE-AVL not only balances storage and search performance, but also avoids degradation. To verify the correctness and completeness of empty search results, we store path information in each leaf node and node with only one child node. Considering the substitution attack, we bind the file identifier and the file so that the client will find out once the server returns inconsistent search results. Rigorous security analysis shows VSSE-AVL satisfies privacy and verifiability. Compared with the verifiable SSE-2 with the same security, the experimental evaluation shows that our proposed scheme performs better on storage, search and verification.},
  archive      = {J_COMJNL},
  author       = {Wang, Qing and Zhang, Xi and Qin, Jing and Ma, Jixin and Huang, Xinyi},
  doi          = {10.1093/comjnl/bxab152},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {174-183},
  shortjournal = {Comput. J.},
  title        = {A verifiable symmetric searchable encryption scheme based on the AVL tree},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing binary matrices with good implementation
properties for low-latency block ciphers based on lai-massey structure.
<em>COMJNL</em>, <em>66</em>(1), 160–173. (<a
href="https://doi.org/10.1093/comjnl/bxab151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion layers are crucial components for lightweight cryptographic schemes. Optimal binary matrices are widely used diffusion layers that can be easier to achieve the best security/performance trade-off. However, most of the constructions of binary matrices are concentrated in smaller dimensions. Besides, to maximize the number of branches, the performance is often neglected. In this paper, we investigate the diffusion of the Lai-Massey (L-M) structures and propose a series of binary diffusion layers with the best possible branch number and efficient software/hardware implementations as well for feasible parameters (up to 64). Firstly, we prove the lower bound of the circuit depth of a binary matrix with a fixed branch number. Then, we construct binary matrices by L-M structure with cyclic shift as round functions because of taking account of the improvement of software performance and demonstrate that this construction can not get the diffusion layers with branch number &gt;4. Then, we get some 4 ; 4 and 6 ; 6 optimal binary matrices with branch number 4 by one-round L-M structure. Note that the depth of these results is optimal, i. e. they achieve the lowest hardware costs without loss of software efficiency. Secondly, we construct diffusion layers by extended L-M structures to obtain binary matrices with large sizes. We give a list of software/hardware friendly optimal binary matrices with large dimensions, especially for dimensions 48 and 64. In particular, some of the solutions are Maximum Distance Binary Linear matrices. Finally, we also present diffusion layers constructed by the extended generalized L-M structure to improve their applicabilities on other platforms.},
  archive      = {J_COMJNL},
  author       = {Li, Xiaodan and Wu, Wenling},
  doi          = {10.1093/comjnl/bxab151},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {160-173},
  shortjournal = {Comput. J.},
  title        = {Constructing binary matrices with good implementation properties for low-latency block ciphers based on lai-massey structure},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT analytics-inspired real-time monitoring for early
prediction of COVID-19 symptoms. <em>COMJNL</em>, <em>66</em>(1),
144–159. (<a href="https://doi.org/10.1093/comjnl/bxab150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this pandemic, providing a quality environment is considered one of the essential objectives of smart wellbeing observation. Therefore, the prediction of irregular events has become a fundamental requirement of assistive or clinical consideration. By concentrating on this need, a dew computation-inspired irregular physical event determination solution is presented to determine the symptoms of COVID-19 by analyzing the physical activities of the individuals at their initial stage. The benefit of the proposed solution is enhanced by forwarding the predicted outcomes and their occurrence within a speculated time on a private cloud database to decide the health seriousness. Furthermore, a dynamic decisive-module is introduced to notify medical specialists about the current wellbeing status of the individuals under monitoring. The real-time prediction efficiency of the proposed solution is determined by implementing and calculating the outcomes on both the dew and cloud platforms. The calculated outcomes exhibit the improved viability of the dew platform over the cloud platform by increasing the prediction speed of 46.27\% for 40 and 45.54\% for 30 frames per second. Moreover, the event prediction performance is justified over the state-of-the-art monitoring arrangements by achieving accuracy (92.88\%), specificity (90.87\%), sensitivity (88.26\%) and F1-measure (89.53\%) with the least decision-making delay.},
  archive      = {J_COMJNL},
  author       = {Manocha, Ankush and Kumar, Gulshan and Bhatia, Munish},
  doi          = {10.1093/comjnl/bxab150},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {144-159},
  shortjournal = {Comput. J.},
  title        = {IoT analytics-inspired real-time monitoring for early prediction of COVID-19 symptoms},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to adaptive resource allocation for energy
saving in reconfigurable heterogeneous networks. <em>COMJNL</em>,
<em>66</em>(1), 128–143. (<a
href="https://doi.org/10.1093/comjnl/bxab149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense deployment of small cell networks is a key feature of next generation mobile networks aimed at providing the necessary capacity increase. Wireless heterogeneous networks are created by combining several radio access technologies, each with its own potentials, capabilities and limitations. In these networks, providing real-time services with quality assurance is essential. For effective use of radio resources, the Radio Resource Management method was introduced which its performance and efficiency is better than the control of independent radio resources in any radio access technology. In this paper, we introduced a novel approach to select the most effective radio access technologies by taking into account some performance parameters like the type of service, users’ distribution pattern and the cost of the services. It also optimizes the handover relations between macrolayer and small cells. The proposed approach is a self-optimizing model can be employed to control resources and improve performance indices associated with mobile networks without human interference by only relying on network intelligence. In order to maximize the network performance, we applied the dynamic backhauling technique to analyze the uplink signaling data which increased the validity level of the decision-making process. Based on the extracted semantic information, the network decision-making engine is able to adjust the network parameters and efficiently allocate the resources. The numerical results exhibit considerable power saving for different traffic models in addition to reduce the rate of vertical handovers. The results also show increase the network throughput by up to 30\%.},
  archive      = {J_COMJNL},
  author       = {Nabipour, Mohammad and Momen, Amir Reza},
  doi          = {10.1093/comjnl/bxab149},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {128-143},
  shortjournal = {Comput. J.},
  title        = {A novel approach to adaptive resource allocation for energy saving in reconfigurable heterogeneous networks},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information-theoretic channel for multi-exposure image
fusion. <em>COMJNL</em>, <em>66</em>(1), 114–127. (<a
href="https://doi.org/10.1093/comjnl/bxab148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-exposure image fusion has emerged as an increasingly important and interesting research topic in information fusion. It aims at producing an image with high quality by fusing a set of differently exposed images. In this article, we present a pixel-level method for multi-exposure image fusion based on an information-theoretic approach. In our scheme, an information channel between two source images is used to compute the Rényi entropy associated with each pixel in one image with respect to the other image and hence to produce the weight maps for the source images. Since direct weight-averaging of the source images introduce unpleasing artifacts, we employ Laplacian multi-scale fusion. Based on this pyramid scheme, images at every scale are fused by weight maps, and a final fused image is inversely reconstructed. Multi-exposure image fusion with the proposed method is easy to construct and implement and can deliver, in less than a second for a set of three input images of size 512; 340, competitive and compelling results versus state-of-art methods through visual comparison and objective evaluation.},
  archive      = {J_COMJNL},
  author       = {Hao, Qiaohong and Zhao, Qi and Sbert, Mateu and Feng, Qinghe and Ancuti, Cosmin and Feixas, Miquel and Vila, Marius and Zhang, Jiawan},
  doi          = {10.1093/comjnl/bxab148},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {114-127},
  shortjournal = {Comput. J.},
  title        = {Information-theoretic channel for multi-exposure image fusion},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chronological sailfish optimizer for preserving privacy in
cloud based on khatri-rao product. <em>COMJNL</em>, <em>66</em>(1),
101–113. (<a href="https://doi.org/10.1093/comjnl/bxab147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The innovative trends of cloud computing acquired the interest of several individuals or enterprises that started outsourcing data to the cloud servers. Recently, numerous techniques are introduced for facilitating privacy protection on untrusted cloud platforms. However, the classical privacy-preserving techniques failed to prevent leakage and incur huge information loss. This paper introduces the efficient technique, named the chronological sailfish optimizer (CSFO) algorithm for privacy preservation in cloud computing. The proposed CSFO is devised by integrating the chronological concept in SailFish optimizer. The input data are fed to a privacy-preservation process wherein hamming weight-based RSA and Khatri-Rao products are utilized for data privacy. Here, the hamming weighted-based RSA is determined by combining the sha256 algorithm with the hamming weight with Rivest–Shamir–Adleman (HRSA) system. Hence, an optimization-driven algorithm is utilized to evaluate optimal matrix generation to handle both the utility and the sensitive information. Here, the fitness function is newly devised considering, realism, privacy and fitness. The experimentation is performed using four datasets, like Pathway Interaction Database, Hungarian, Cleveland and Switzerland. The proposed CSFO provided superior performance with maximal privacy of 0.2173, maximal realism 0.9456 and maximal fitness of 0.5416.},
  archive      = {J_COMJNL},
  author       = {Kalpana, Parsi},
  doi          = {10.1093/comjnl/bxab147},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {101-113},
  shortjournal = {Comput. J.},
  title        = {Chronological sailfish optimizer for preserving privacy in cloud based on khatri-rao product},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational model for prediction of malignant mesothelioma
diagnosis. <em>COMJNL</em>, <em>66</em>(1), 86–100. (<a
href="https://doi.org/10.1093/comjnl/bxab146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesothelioma is an aggressive lung cancer, harms the linings of the lungs. It is one of the deadliest cancers diagnosed in those exposed to fibrous silicate minerals (asbestos). Millions of people face severe consequences as they are diagnosed at late stages. This study presents a comparison of several machine learning approaches with distinct feature sets and addresses the issue of class imbalance. The dataset used in this study is available publicly on the University of California Irvine (UCI) machine learning repository. This study used the resampling technique, synthetic minority oversampling technique ; , and adaptive synthetic sampling ; to handle the class imbalance. Most of the machine learning strategies performed well with the resampling technique. The best accuracy using the resampling strategy was achieved by artificial neural networks (ANN). The highest accuracy was recorded on the feature set selected by principal component analysis (PCA) is 96\%. Overall, ensemble techniques performed well. The proposed stacking-based classifier achieved the highest accuracy (89\%) on data balanced using SMOTE and ADASYN.},
  archive      = {J_COMJNL},
  author       = {Gupta, Surbhi and Gupta, Manoj Kumar},
  doi          = {10.1093/comjnl/bxab146},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {86-100},
  shortjournal = {Comput. J.},
  title        = {Computational model for prediction of malignant mesothelioma diagnosis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prognosis and prediction of breast cancer using machine
learning and ensemble-based training model. <em>COMJNL</em>,
<em>66</em>(1), 70–85. (<a
href="https://doi.org/10.1093/comjnl/bxab145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increase in occurrence of human diseases all over the world. Among those, Breast Cancer has increased with an alarming rate in the past decade and this trend of increase would continue to grow. Now, there is a need for efficient text analytics and feature extraction tools to assist classifying, sharing and retrieving the information on human diseases in general and Breast Cancer in particular. In light of above, the present study has been undertaken with the objective to provide a comparative analysis of different classifiers on Breast Cancer dataset, and to propose a new ensemble training method of Machine Learning Classification. Here, machine learning models (such as K-Nearest Neighbour, Logistic Regression, Decision Tree, Random Forest, Gradient Boost, Support Vector Machine) and deep learning classifiers (such as Multi-Layer Feed Forward Neural Network, Recurrent Neural Network and Long Short Term Memory) have been applied on Breast Cancer dataset. An Ensemble Learning model for Prediction is proposed to classify the results among different classifiers. Finally, the Voting Ensemble is implemented to find out the optimal classifier for prediction of Breast Cancer. The results have been computed using the evaluation parameters such as Accuracy, Precision, Recall and Specificity. The confusion matrix drawn on the basis of evaluation parameters provides more emphasis on predicted and actual instances. Performance Evaluation for various machine learning models is computed. Results of this investigation concludes that Voting Ensemble outperforms other machine learning models. The prediction using Voting Ensemble resulted in an accuracy rate of 97.9 per cent, precision of 96.77 per cent and recall of 100 per cent.},
  archive      = {J_COMJNL},
  author       = {Gupta, Niharika and Kaushik, Baij Nath},
  doi          = {10.1093/comjnl/bxab145},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {70-85},
  shortjournal = {Comput. J.},
  title        = {Prognosis and prediction of breast cancer using machine learning and ensemble-based training model},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face identification system based on synthesizing realistic
image using edge-aided GANs. <em>COMJNL</em>, <em>66</em>(1), 61–69. (<a
href="https://doi.org/10.1093/comjnl/bxab144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presently, facial image recognition via a thermal camera is a critical phase in numerous fields. Systems using thermal facial images suffer from numerous problems in face identification. In this paper, a model Edge-Aided Generative Adversarial Network (EA-GAN) is introduced to overcome the difficulties of thermal face identification by synthesizing a visible faces image from the thermal version. To enhance the performance of the Conditional Generative Adversarial Network (CGAN) model for the create realistic face images, the edge information extracted from the thermal image has been used as input, thus lead to improving overall the system&#39;s achievement. Moreover, a new model is presented in the present work for face identification by integrating two Convolutional Neural Networks (CNN) to achieve high and rapid accuracy rates. Based on the experiments on the Carl dataset for faces, it is indicated that EA-GAN can synthesize visually comfortable and identity-preserving faces; thus, better performance is achieved in comparison with the state-of-the-art approaches for thermal facial identification.},
  archive      = {J_COMJNL},
  author       = {Majidpour, Jafar and Kais Jameel, Samer and Anwar Qadir, Jihad},
  doi          = {10.1093/comjnl/bxab144},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {61-69},
  shortjournal = {Comput. J.},
  title        = {Face identification system based on synthesizing realistic image using edge-aided GANs},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analysis of computational resources of event-driven
streaming data flow for internet of things: A case study.
<em>COMJNL</em>, <em>66</em>(1), 47–60. (<a
href="https://doi.org/10.1093/comjnl/bxab143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information and communication technologies backbone of a smart city is an Internet of Things (IoT) application that combines technologies such as low power IoT networks, device management, analytics or event stream processing. Hence, designing an efficient IoT architecture for real-time IoT applications brings technical challenges that include the integration of application network protocols and data processing. In this context, the system scalability of two architectures has been analysed: the first architecture, named as POST architecture, integrates the hyper text transfer protocol with an Extract-Transform-Load technique, and is used as baseline; the second architecture, named as MQTT-CEP, is based on a publish-subscribe protocol, i.e. message queue telemetry transport, and a complex event processor engine. In this analysis, SAVIA, a smart city citizen security application, has been deployed following both architectural approaches. Results show that the design of the network protocol and the data analytic layer impacts highly in the Quality of Service experimented by the final IoT users. The experiments show that the integrated MQTT-CEP architecture scales properly, keeps energy consumption limited and thereby, promotes the development of a distributed IoT architecture based on constraint resources. The drawback is an increase in latency, mainly caused by the loosely coupled communication pattern of MQTT, but within reasonable levels which stabilize with increasing workloads.},
  archive      = {J_COMJNL},
  author       = {Tenorio-Trigoso, Alonso and Castillo-Cara, Manuel and Mondragón-Ruiz, Giovanny and Carrión, Carmen and Caminero, Blanca},
  doi          = {10.1093/comjnl/bxab143},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {47-60},
  shortjournal = {Comput. J.},
  title        = {An analysis of computational resources of event-driven streaming data flow for internet of things: A case study},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Youtube trending videos: Boosting machine learning results
using exploratory data analysis. <em>COMJNL</em>, <em>66</em>(1), 35–46.
(<a href="https://doi.org/10.1093/comjnl/bxab142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of social media and the Internet, bulks of data are generated every day in different fields. However, filtering out useful information, which can be turned into knowledge, requires significant effort. This can be achieved using various Big Data Technologies, statistics, data mining and a variety of machine learning algorithms that corporates and businesses are using to improve their performance every day for a long time now. In this paper, we analyze YouTube trending video data that analyses the target that grabs the user’s attention over a relatively short time. We present our analysis by measuring, mining, analyzing and comparing key aspects of time-series YouTube data with respect to its view and audience response statistics from 40 000 trending YouTube videos collected over 205 days. We have performed an exploratory data analysis (EDA) on all of it&#39;s aspect to get data insights and used statistics to find similarities between them to understand viewing pattern of different video categories. We also compare and observe the variation of activity over time with the nature of the event that affects the quality of our analysis. We make the viewership pattern clear belonging to each category separately by direct analysis. The results and findings can help us predict the category of the video that could probably be trending next in the upcoming days and help individuals get boosted views and performance when they upload video as per the predictions and analysis.},
  archive      = {J_COMJNL},
  author       = {Khanam, Sana and Tanweer, Safdar and Khalid, Syed Sibtain},
  doi          = {10.1093/comjnl/bxab142},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {35-46},
  shortjournal = {Comput. J.},
  title        = {Youtube trending videos: Boosting machine learning results using exploratory data analysis},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimal rare pattern-based outlier detection approach for
uncertain data streams under monotonic constraints. <em>COMJNL</em>,
<em>66</em>(1), 16–34. (<a
href="https://doi.org/10.1093/comjnl/bxab139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing association-based outlier detection approaches were proposed to seek for potential outliers from huge full set of uncertain data streams (; ), but could not effectively process the small scale of ; that satisfies preset constraints; thus, they were time consuming. To solve this problem, this paper proposes a novel minimal rare pattern-based outlier detection approach, namely Constrained Minimal Rare Pattern-based Outlier Detection (CMRP-OD), to discover outliers from small sets of ; that satisfy the user-preset succinct or convertible monotonic constraints. First, two concepts of ‘maximal probability’ and ‘support cap’ are proposed to compress the scale of extensible patterns, and then the matrix is designed to store the information of each valid pattern to reduce the scanning times of ; , thus decreasing the time consumption. Second, more factors that can influence the determination of outlier are considered in the design of deviation indices, thus increasing the detection accuracy. Extensive experiments show that compared with the state-of-the-art approaches, CMRP-OD approach has at least 10\% improvement on detection accuracy, and its time cost is also almost reduced half.},
  archive      = {J_COMJNL},
  author       = {Cai, Saihua and Chen, Jinfu and Chen, Haibo and Zhang, Chi and Li, Qian and Shi, Dengzhou and Lin, Wei},
  doi          = {10.1093/comjnl/bxab139},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {16-34},
  shortjournal = {Comput. J.},
  title        = {Minimal rare pattern-based outlier detection approach for uncertain data streams under monotonic constraints},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mean error rate weighted online boosting method.
<em>COMJNL</em>, <em>66</em>(1), 1–15. (<a
href="https://doi.org/10.1093/comjnl/bxab138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting is a generally known technique to convert a group of weak learners into a powerful ensemble. To reach this desired objective successfully, the modules are trained with distinct data samples and the hypotheses are combined in order to achieve an optimal prediction. To make use of boosting technique in online condition is a new approach. It motivates to meet the requirements due to its success in offline conditions. This work presents new online boosting method. We make use of mean error rate of individual base learners to achieve effective weight distribution of the instances to closely match the behavior of OzaBoost. Experimental results show that, in most of the situations, the proposed method achieves better accuracies, outperforming the other state-of-art methods.},
  archive      = {J_COMJNL},
  author       = {Honnikoll, Nagaraj and Baidari, Ishwar},
  doi          = {10.1093/comjnl/bxab138},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Comput. J.},
  title        = {Mean error rate weighted online boosting method},
  volume       = {66},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
