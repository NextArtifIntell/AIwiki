<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOSTAT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biostat---64">BIOSTAT - 64</h2>
<ul>
<li><details>
<summary>
(2023). Improved efficiency for cross-arm comparisons via platform
designs. <em>BIOSTAT</em>, <em>24</em>(4), 1106–1124. (<a
href="https://doi.org/10.1093/biostatistics/kxac030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though platform trials have been touted for their flexibility and streamlined use of trial resources, their statistical efficiency is not well understood. We fill this gap by establishing their greater efficiency for comparing the relative efficacy of multiple interventions over using several separate, 2-arm trials, where the relative efficacy of an arbitrary pair of interventions is evaluated by contrasting their relative risks as compared to control. In theoretical and numerical studies, we demonstrate that the inference of such a contrast using data from a platform trial enjoys identical or better precision than using data from separate trials, even when the former enrolls substantially fewer participants. This benefit is attributed to the sharing of controls among interventions under contemporaneous randomization. We further provide a novel procedure for establishing the noninferiority of a given intervention relative to the most efficacious of the other interventions under evaluation, where this procedure is adaptive in the sense that it need not be a priori known which of these other interventions is most efficacious. Our numerical studies show that this testing procedure can attain substantially better power when the data arise from a platform trial rather than multiple separate trials. Our results are illustrated using data from two monoclonal antibody trials for the prevention of HIV.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Tzu-Jung and Luedtke, Alex and THE AMP INVESTIGATOR GROUP},
  doi          = {10.1093/biostatistics/kxac030},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1106-1124},
  shortjournal = {Biostatistics},
  title        = {Improved efficiency for cross-arm comparisons via platform designs},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible approach for predictive biomarker discovery.
<em>BIOSTAT</em>, <em>24</em>(4), 1085–1105. (<a
href="https://doi.org/10.1093/biostatistics/kxac029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An endeavor central to precision medicine is predictive biomarker discovery; they define patient subpopulations which stand to benefit most, or least, from a given treatment. The identification of these biomarkers is often the byproduct of the related but fundamentally different task of treatment rule estimation. Using treatment rule estimation methods to identify predictive biomarkers in clinical trials where the number of covariates exceeds the number of participants often results in high false discovery rates. The higher than expected number of false positives translates to wasted resources when conducting follow-up experiments for drug target identification and diagnostic assay development. Patient outcomes are in turn negatively affected. We propose a variable importance parameter for directly assessing the importance of potentially predictive biomarkers and develop a flexible nonparametric inference procedure for this estimand. We prove that our estimator is double robust and asymptotically linear under loose conditions in the data-generating process, permitting valid inference about the importance metric. The statistical guarantees of the method are verified in a thorough simulation study representative of randomized control trials with moderate and high-dimensional covariate vectors. Our procedure is then used to discover predictive biomarkers from among the tumor gene expression data of metastatic renal cell carcinoma patients enrolled in recently completed clinical trials. We find that our approach more readily discerns predictive from nonpredictive biomarkers than procedures whose primary purpose is treatment rule estimation. An open-source software implementation of the methodology, the uniCATE R package, is briefly introduced.},
  archive      = {J_BIOSTAT},
  author       = {Boileau, Philippe and Qi, Nina Ting and van der Laan, Mark J and Dudoit, Sandrine and Leng, Ning},
  doi          = {10.1093/biostatistics/kxac029},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1085-1105},
  shortjournal = {Biostatistics},
  title        = {A flexible approach for predictive biomarker discovery},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained groupwise additive index models.
<em>BIOSTAT</em>, <em>24</em>(4), 1066–1084. (<a
href="https://doi.org/10.1093/biostatistics/kxac023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In environmental epidemiology, there is wide interest in creating and using comprehensive indices that can summarize information from different environmental exposures while retaining strong predictive power on a target health outcome. In this context, the present article proposes a model called the constrained groupwise additive index model (CGAIM) to create easy-to-interpret indices predictive of a response variable, from a potentially large list of variables. The CGAIM considers groups of predictors that naturally belong together to yield meaningful indices. It also allows the addition of linear constraints on both the index weights and the form of their relationship with the response variable to represent prior assumptions or operational requirements. We propose an efficient algorithm to estimate the CGAIM, along with index selection and inference procedures. A simulation study shows that the proposed algorithm has good estimation performances, with low bias and variance and is applicable in complex situations with many correlated predictors. It also demonstrates important sensitivity and specificity in index selection, but non-negligible coverage error on constructed confidence intervals. The CGAIM is then illustrated in the construction of heat indices in a health warning system context. We believe the CGAIM could become useful in a wide variety of situations, such as warning systems establishment, and multipollutant or exposome studies.},
  archive      = {J_BIOSTAT},
  author       = {Masselot, Pierre and Chebana, Fateh and Campagna, Céline and Lavigne, Éric and Ouarda, Taha B M J and Gosselin, Pierre},
  doi          = {10.1093/biostatistics/kxac023},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1066-1084},
  shortjournal = {Biostatistics},
  title        = {Constrained groupwise additive index models},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale analysis of count data through topic alignment.
<em>BIOSTAT</em>, <em>24</em>(4), 1045–1065. (<a
href="https://doi.org/10.1093/biostatistics/kxac018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modeling is a popular method used to describe biological count data. With topic models, the user must specify the number of topics |$K$|⁠ . Since there is no definitive way to choose |$K$| and since a true value might not exist, we develop a method, which we call topic alignment , to study the relationships across models with different |$K$|⁠ . In addition, we present three diagnostics based on the alignment. These techniques can show how many topics are consistently present across different models, if a topic is only transiently present, or if a topic splits into more topics when |$K$| increases. This strategy gives more insight into the process of generating the data than choosing a single value of |$K$| would. We design a visual representation of these cross-model relationships, show the effectiveness of these tools for interpreting the topics on simulated and real data, and release an accompanying R package, alto},
  archive      = {J_BIOSTAT},
  author       = {Fukuyama, Julia and Sankaran, Kris and Symul, Laura},
  doi          = {10.1093/biostatistics/kxac018},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1045-1065},
  shortjournal = {Biostatistics},
  title        = {Multiscale analysis of count data through topic alignment},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated splitting into batches for observational
biomedical studies with sequential processing. <em>BIOSTAT</em>,
<em>24</em>(4), 1031–1044. (<a
href="https://doi.org/10.1093/biostatistics/kxac014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimental design usually focuses on the setting where treatments and/or other aspects of interest can be manipulated. However, in observational biomedical studies with sequential processing, the set of available samples is often fixed, and the problem is thus rather the ordering and allocation of samples to batches such that comparisons between different treatments can be made with similar precision. In certain situations, this allocation can be done by hand, but this rapidly becomes impractical with more challenging cohort setups. Here, we present a fast and intuitive algorithm to generate balanced allocations of samples to batches for any single-variable model where the treatment variable is nominal. This greatly simplifies the grouping of samples into batches, makes the process reproducible, and provides a marked improvement over completely random allocations. The general challenges of allocation and why good solutions can be hard to find are also discussed, as well as potential extensions to multivariable settings.},
  archive      = {J_BIOSTAT},
  author       = {Burger, Bram and Vaudel, Marc and Barsnes, Harald},
  doi          = {10.1093/biostatistics/kxac014},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1031-1044},
  shortjournal = {Biostatistics},
  title        = {Automated splitting into batches for observational biomedical studies with sequential processing},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-direct effects in settings with two mediators.
<em>BIOSTAT</em>, <em>24</em>(4), 1017–1030. (<a
href="https://doi.org/10.1093/biostatistics/kxac037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When multiple mediators are present, there are additional effects that may be of interest beyond the well-known natural (NDE) and controlled direct effects (CDE). These effects cross the type of control on the mediators, setting one to a constant level and one to its natural level, which differs across subjects. We introduce five such estimands for the cross-CDE and -NDE when two mediators are measured. We consider both the scenario where one mediator is influenced by the other, referred to as sequential mediators, and the scenario where the mediators do not influence each other. Such estimands may be of interest in immunology, as we discuss in relation to measured immunological responses to SARS-CoV-2 vaccination. We provide identifying expressions for the estimands in observational settings where there is no residual confounding, and where intervention, outcome, and mediators are of arbitrary type. We further provide tight symbolic bounds for the estimands in randomized settings where there may be residual confounding of the outcome and mediator relationship and all measured variables are binary.},
  archive      = {J_BIOSTAT},
  author       = {Gabriel, Erin E and Sjölander, Arvid and Follmann, Dean and Sachs, Michael C},
  doi          = {10.1093/biostatistics/kxac037},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1017-1030},
  shortjournal = {Biostatistics},
  title        = {Cross-direct effects in settings with two mediators},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian sample size determination in basket trials
borrowing information between subsets. <em>BIOSTAT</em>, <em>24</em>(4),
1000–1016. (<a
href="https://doi.org/10.1093/biostatistics/kxac033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basket trials are increasingly used for the simultaneous evaluation of a new treatment in various patient subgroups under one overarching protocol. We propose a Bayesian approach to sample size determination in basket trials that permit borrowing of information between commensurate subsets. Specifically, we consider a randomized basket trial design where patients are randomly assigned to the new treatment or control within each trial subset (“subtrial” for short). Closed-form sample size formulae are derived to ensure that each subtrial has a specified chance of correctly deciding whether the new treatment is superior to or not better than the control by some clinically relevant difference. Given prespecified levels of pairwise (in)commensurability, the subtrial sample sizes are solved simultaneously. The proposed Bayesian approach resembles the frequentist formulation of the problem in yielding comparable sample sizes for circumstances of no borrowing. When borrowing is enabled between commensurate subtrials, a considerably smaller trial sample size is required compared to the widely implemented approach of no borrowing. We illustrate the use of our sample size formulae with two examples based on real basket trials. A comprehensive simulation study further shows that the proposed methodology can maintain the true positive and false positive rates at desired levels.},
  archive      = {J_BIOSTAT},
  author       = {Zheng, Haiyan and Grayling, Michael J and Mozgunov, Pavel and Jaki, Thomas and Wason, James M S},
  doi          = {10.1093/biostatistics/kxac033},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1000-1016},
  shortjournal = {Biostatistics},
  title        = {Bayesian sample size determination in basket trials borrowing information between subsets},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly robust evaluation of high-dimensional surrogate
markers. <em>BIOSTAT</em>, <em>24</em>(4), 985–999. (<a
href="https://doi.org/10.1093/biostatistics/kxac020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When evaluating the effectiveness of a treatment, policy, or intervention, the desired measure of efficacy may be expensive to collect, not routinely available, or may take a long time to occur. In these cases, it is sometimes possible to identify a surrogate outcome that can more easily, quickly, or cheaply capture the effect of interest. Theory and methods for evaluating the strength of surrogate markers have been well studied in the context of a single surrogate marker measured in the course of a randomized clinical study. However, methods are lacking for quantifying the utility of surrogate markers when the dimension of the surrogate grows. We propose a robust and efficient method for evaluating a set of surrogate markers that may be high-dimensional. Our method does not require treatment to be randomized and may be used in observational studies. Our approach draws on a connection between quantifying the utility of a surrogate marker and the most fundamental tools of causal inference—namely, methods for robust estimation of the average treatment effect. This connection facilitates the use of modern methods for estimating treatment effects, using machine learning to estimate nuisance functions and relaxing the dependence on model specification. We demonstrate that our proposed approach performs well, demonstrate connections between our approach and certain mediation effects, and illustrate it by evaluating whether gene expression can be used as a surrogate for immune activation in an Ebola study.},
  archive      = {J_BIOSTAT},
  author       = {Agniel, Denis and Hejblum, Boris P and Thiébaut, Rodolphe and Parast, Layla},
  doi          = {10.1093/biostatistics/kxac020},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {985-999},
  shortjournal = {Biostatistics},
  title        = {Doubly robust evaluation of high-dimensional surrogate markers},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reformulating provider profiling by grouping providers
treating similar patients prior to evaluating performance.
<em>BIOSTAT</em>, <em>24</em>(4), 962–984. (<a
href="https://doi.org/10.1093/biostatistics/kxac019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard approaches to comparing health providers’ performance rely on hierarchical logistic regression models that adjust for patient characteristics at admission. Estimates from these models may be misleading when providers treat different patient populations and the models are misspecified. To address this limitation, we propose a novel profiling approach that identifies groups of providers treating similar populations of patients and then evaluates providers’ performance within each group. The groups of providers are identified using a Bayesian multilevel finite mixture of general location models. To compare the performance of our proposed profiling approach to standard methods, we use patient-level data from 119 skilled nursing facilities in Massachusetts. We use simulated and observed outcome data to explore the performance of these profiling methods in different settings. In simulations, our proposed method classifies providers to groups with similar patients’ admission characteristics. In addition, in the presence of limited overlap in patient characteristics across providers and misspecifications of the outcome model, the provider-level estimates obtained using our approach identified providers that under- and overperformed compared to the standard regression-based approaches more accurately.},
  archive      = {J_BIOSTAT},
  author       = {Silva, Gabriella C and Gutman, Roee},
  doi          = {10.1093/biostatistics/kxac019},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {962-984},
  shortjournal = {Biostatistics},
  title        = {Reformulating provider profiling by grouping providers treating similar patients prior to evaluating performance},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alleviating spatial confounding in frailty models.
<em>BIOSTAT</em>, <em>24</em>(4), 945–961. (<a
href="https://doi.org/10.1093/biostatistics/kxac028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The confounding between fixed effects and (spatial) random effects in a regression setup is termed spatial confounding. This topic continues to gain attention and has been studied extensively in recent years, given that failure to account for this may lead to a suboptimal inference. To mitigate this, a variety of projection-based approaches under the class of restricted spatial models are available in the context of generalized linear mixed models. However, these projection approaches cannot be directly extended to the spatial survival context via frailty models due to dimension incompatibility between the fixed and spatial random effects. In this work, we introduce a two-step approach to handle this, which involves (i) projecting the design matrix to the dimension of the spatial effect (via dimension reduction) and (ii) assuring that the random effect is orthogonal to this new design matrix (confounding alleviation). Under a fully Bayesian paradigm, we conduct fast estimation and inference using integrated nested Laplace approximation. Both simulation studies and application to a motivating data evaluating respiratory cancer survival in the US state of California reveal the advantages of our proposal in terms of model performance and confounding alleviation, compared to alternatives.},
  archive      = {J_BIOSTAT},
  author       = {Azevedo, Douglas R M and Prates, Marcos O and Bandyopadhyay, Dipankar},
  doi          = {10.1093/biostatistics/kxac028},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {945-961},
  shortjournal = {Biostatistics},
  title        = {Alleviating spatial confounding in frailty models},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial difference boundary detection for multiple outcomes
using bayesian disease mapping. <em>BIOSTAT</em>, <em>24</em>(4),
922–944. (<a
href="https://doi.org/10.1093/biostatistics/kxac013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regional aggregates of health outcomes over delineated administrative units (e.g., states, counties, and zip codes), or areal units, are widely used by epidemiologists to map mortality or incidence rates and capture geographic variation. To capture health disparities over regions, we seek “difference boundaries” that separate neighboring regions with significantly different spatial effects. Matters are more challenging with multiple outcomes over each unit, where we capture dependence among diseases as well as across the areal units. Here, we address multivariate difference boundary detection for correlated diseases. We formulate the problem in terms of Bayesian pairwise multiple comparisons and seek the posterior probabilities of neighboring spatial effects being different. To achieve this, we endow the spatial random effects with a discrete probability law using a class of multivariate areally referenced Dirichlet process models that accommodate spatial and interdisease dependence. We evaluate our method through simulation studies and detect difference boundaries for multiple cancers using data from the Surveillance, Epidemiology, and End Results Program of the National Cancer Institute.},
  archive      = {J_BIOSTAT},
  author       = {Gao, Leiwen and Banerjee, Sudipto and Ritz, Beate},
  doi          = {10.1093/biostatistics/kxac013},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {922-944},
  shortjournal = {Biostatistics},
  title        = {Spatial difference boundary detection for multiple outcomes using bayesian disease mapping},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reassessing pharmacogenomic cell sensitivity with multilevel
statistical models. <em>BIOSTAT</em>, <em>24</em>(4), 901–921. (<a
href="https://doi.org/10.1093/biostatistics/kxac010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pharmacogenomic experiments allow for the systematic testing of drugs, at varying dosage concentrations, to study how genomic markers correlate with cell sensitivity to treatment. The first step in the analysis is to quantify the response of cell lines to variable dosage concentrations of the drugs being tested. The signal to noise in these measurements can be low due to biological and experimental variability. However, the increasing availability of pharmacogenomic studies provides replicated data sets that can be leveraged to gain power. To do this, we formulate a hierarchical mixture model to estimate the drug-specific mixture distributions for estimating cell sensitivity and for assessing drug effect type as either broad or targeted effect. We use this formulation to propose a unified approach that can yield posterior probability of a cell being susceptible to a drug conditional on being a targeted effect or relative effect sizes conditioned on the cell being broad. We demonstrate the usefulness of our approach via case studies. First, we assess pairwise agreements for cell lines/drugs within the intersection of two data sets and confirm the moderate pairwise agreement between many publicly available pharmacogenomic data sets. We then present an analysis that identifies sensitivity to the drug crizotinib for cells harboring EML4-ALK or NPM1-ALK gene fusions, as well as significantly down-regulated cell-matrix pathways associated with crizotinib sensitivity.},
  archive      = {J_BIOSTAT},
  author       = {Ploenzke, Matt and Irizarry, Rafael},
  doi          = {10.1093/biostatistics/kxac010},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {901-921},
  shortjournal = {Biostatistics},
  title        = {Reassessing pharmacogenomic cell sensitivity with multilevel statistical models},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian group testing with dilution effects.
<em>BIOSTAT</em>, <em>24</em>(4), 885–900. (<a
href="https://doi.org/10.1093/biostatistics/kxac004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian framework for group testing under dilution effects has been developed, using lattice-based models. This work has particular relevance given the pressing public health need to enhance testing capacity for coronavirus disease 2019 and future pandemics, and the need for wide-scale and repeated testing for surveillance under constantly varying conditions. The proposed Bayesian approach allows for dilution effects in group testing and for general test response distributions beyond just binary outcomes. It is shown that even under strong dilution effects, an intuitive group testing selection rule that relies on the model order structure, referred to as the Bayesian halving algorithm, has attractive optimal convergence properties. Analogous look-ahead rules that can reduce the number of stages in classification by selecting several pooled tests at a time are proposed and evaluated as well. Group testing is demonstrated to provide great savings over individual testing in the number of tests needed, even for moderately high prevalence levels. However, there is a trade-off with higher number of testing stages, and increased variability. A web-based calculator is introduced to assist in weighing these factors and to guide decisions on when and how to pool under various conditions. High-performance distributed computing methods have also been implemented for considering larger pool sizes, when savings from group testing can be even more dramatic.},
  archive      = {J_BIOSTAT},
  author       = {Tatsuoka, Curtis and Chen, Weicong and Lu, Xiaoyi},
  doi          = {10.1093/biostatistics/kxac004},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {885-900},
  shortjournal = {Biostatistics},
  title        = {Bayesian group testing with dilution effects},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian design of clinical trials using joint models for
recurrent and terminating events. <em>BIOSTAT</em>, <em>24</em>(4),
866–884. (<a
href="https://doi.org/10.1093/biostatistics/kxac025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for recurrent event and terminating event data are increasingly used for the analysis of clinical trials. However, few methods have been proposed for designing clinical trials using these models. In this article, we develop a Bayesian clinical trial design methodology focused on evaluating the effect of an investigational product (IP) on both recurrent event and terminating event processes considered as multiple primary endpoints, using a multifrailty joint model. Dependence between the recurrent and terminating event processes is accounted for using a shared frailty. Inferences for the multiple primary outcomes are based on posterior model probabilities corresponding to mutually exclusive hypotheses regarding the benefit of IP with respect to the recurrent and terminating event processes. We propose an approach for sample size determination to ensure the trial design has a high power and a well-controlled type I error rate, with both operating characteristics defined from a Bayesian perspective. We also consider a generalization of the proposed parametric model that uses a nonparametric mixture of Dirichlet processes to model the frailty distributions and compare its performance to the proposed approach. We demonstrate the methodology by designing a colorectal cancer clinical trial with a goal of demonstrating that the IP causes a favorable effect on at least one of the two outcomes but no harm on either.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Jiawei and Psioda, Matthew A and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxac025},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {866-884},
  shortjournal = {Biostatistics},
  title        = {Bayesian design of clinical trials using joint models for recurrent and terminating events},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A controlled effects approach to assessing immune correlates
of protection. <em>BIOSTAT</em>, <em>24</em>(4), 850–865. (<a
href="https://doi.org/10.1093/biostatistics/kxac024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An immune correlate of risk (CoR) is an immunologic biomarker in vaccine recipients associated with an infectious disease clinical endpoint. An immune correlate of protection (CoP) is a CoR that can be used to reliably predict vaccine efficacy (VE) against the clinical endpoint and hence is accepted as a surrogate endpoint that can be used for accelerated approval or guide use of vaccines. In randomized, placebo-controlled trials, CoR analysis is limited by not assessing a causal vaccine effect. To address this limitation, we construct the controlled risk curve of a biomarker, which provides the causal risk of an endpoint if all participants are assigned vaccine and the biomarker is set to different levels. Furthermore, we propose a causal CoP analysis based on controlled effects, where for the important special case that the biomarker is constant in the placebo arm, we study the controlled vaccine efficacy curve that contrasts the controlled risk curve with placebo arm risk. We provide identification conditions and formulae that account for right censoring of the clinical endpoint and two-phase sampling of the biomarker, and consider G-computation estimation and inference under a semiparametric model such as the Cox model. We add modular approaches to sensitivity analysis that quantify robustness of CoP evidence to unmeasured confounding. We provide an application to two phase 3 trials of a dengue vaccine indicating that controlled risk of dengue strongly varies with 50 |$\%$| neutralizing antibody titer. Our work introduces controlled effects causal mediation analysis to immune CoP evaluation.},
  archive      = {J_BIOSTAT},
  author       = {Gilbert, Peter B and Fong, Youyi and Kenny, Avi and Carone, Marco},
  doi          = {10.1093/biostatistics/kxac024},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {850-865},
  shortjournal = {Biostatistics},
  title        = {A controlled effects approach to assessing immune correlates of protection},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing three-level cluster randomized trials to assess
treatment effect heterogeneity. <em>BIOSTAT</em>, <em>24</em>(4),
833–849. (<a
href="https://doi.org/10.1093/biostatistics/kxac026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials often exhibit a three-level structure with participants nested in subclusters such as health care providers, and subclusters nested in clusters such as clinics. While the average treatment effect has been the primary focus in planning three-level randomized trials, interest is growing in understanding whether the treatment effect varies among prespecified patient subpopulations, such as those defined by demographics or baseline clinical characteristics. In this article, we derive novel analytical design formulas based on the asymptotic covariance matrix for powering confirmatory analyses of treatment effect heterogeneity in three-level trials, that are broadly applicable to the evaluation of cluster-level, subcluster-level, and participant-level effect modifiers and to designs where randomization can be carried out at any level. We characterize a nested exchangeable correlation structure for both the effect modifier and the outcome conditional on the effect modifier, and generate new insights from a study design perspective for conducting analyses of treatment effect heterogeneity based on a linear mixed analysis of covariance model. A simulation study is conducted to validate our new methods and two real-world trial examples are used for illustrations.},
  archive      = {J_BIOSTAT},
  author       = {Li, Fan and Chen, Xinyuan and Tian, Zizhong and Esserman, Denise and Heagerty, Patrick J and Wang, Rui},
  doi          = {10.1093/biostatistics/kxac026},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {833-849},
  shortjournal = {Biostatistics},
  title        = {Designing three-level cluster randomized trials to assess treatment effect heterogeneity},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible parametric accelerated failure time model and the
extension to time-dependent acceleration factors. <em>BIOSTAT</em>,
<em>24</em>(3), 811–831. (<a
href="https://doi.org/10.1093/biostatistics/kxac009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated failure time (AFT) models are used widely in medical research, though to a much lesser extent than proportional hazards models. In an AFT model, the effect of covariates act to accelerate or decelerate the time to event of interest, that is, shorten or extend the time to event. Commonly used parametric AFT models are limited in the underlying shapes that they can capture. In this article, we propose a general parametric AFT model, and in particular concentrate on using restricted cubic splines to model the baseline to provide substantial flexibility. We then extend the model to accommodate time-dependent acceleration factors. Delayed entry is also allowed, and hence, time-dependent covariates. We evaluate the proposed model through simulation, showing substantial improvements compared to standard parametric AFT models. We also show analytically and through simulations that the AFT models are collapsible, suggesting that this model class will be well suited to causal inference. We illustrate the methods with a data set of patients with breast cancer. Finally, we provide highly efficient, user-friendly Stata, and R software packages.},
  archive      = {J_BIOSTAT},
  author       = {Crowther, Michael J and Royston, Patrick and Clements, Mark},
  doi          = {10.1093/biostatistics/kxac009},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {811-831},
  shortjournal = {Biostatistics},
  title        = {A flexible parametric accelerated failure time model and the extension to time-dependent acceleration factors},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric marginal regression for clustered competing
risks data with missing cause of failure. <em>BIOSTAT</em>,
<em>24</em>(3), 795–810. (<a
href="https://doi.org/10.1093/biostatistics/kxac012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered competing risks data are commonly encountered in multicenter studies. The analysis of such data is often complicated due to informative cluster size (ICS), a situation where the outcomes under study are associated with the size of the cluster. In addition, the cause of failure is frequently incompletely observed in real-world settings. To the best of our knowledge, there is no methodology for population-averaged analysis with clustered competing risks data with an ICS and missing causes of failure. To address this problem, we consider the semiparametric marginal proportional cause-specific hazards model and propose a maximum partial pseudolikelihood estimator under a missing at random assumption. To make the latter assumption more plausible in practice, we allow for auxiliary variables that may be related to the probability of missingness. The proposed method does not impose assumptions regarding the within-cluster dependence and allows for ICS. The asymptotic properties of the proposed estimators for both regression coefficients and infinite-dimensional parameters, such as the marginal cumulative incidence functions, are rigorously established. Simulation studies show that the proposed method performs well and that methods that ignore the within-cluster dependence and the ICS lead to invalid inferences. The proposed method is applied to competing risks data from a large multicenter HIV study in sub-Saharan Africa where a significant portion of causes of failure is missing.},
  archive      = {J_BIOSTAT},
  author       = {Zhou, Wenxian and Bakoyannis, Giorgos and Zhang, Ying and Yiannoutsos, Constantin T},
  doi          = {10.1093/biostatistics/kxac012},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {795-810},
  shortjournal = {Biostatistics},
  title        = {Semiparametric marginal regression for clustered competing risks data with missing cause of failure},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed cox proportional hazards regression using
summary-level information. <em>BIOSTAT</em>, <em>24</em>(3), 776–794.
(<a href="https://doi.org/10.1093/biostatistics/kxac006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual-level data sharing across multiple sites can be infeasible due to privacy and logistical concerns. This article proposes a general distributed methodology to fit Cox proportional hazards models without sharing individual-level data in multi-site studies. We make inferences on the log hazard ratios based on an approximated partial likelihood score function that uses only summary-level statistics. This approach can be applied to both stratified and unstratified models, accommodate both discrete and continuous exposure variables, and permit the adjustment of multiple covariates. In particular, the fitting of stratified Cox models can be carried out with only one file transfer of summary-level information. We derive the asymptotic properties of the proposed estimators and compare the proposed estimators with the maximum partial likelihood estimators using pooled individual-level data and meta-analysis methods through simulation studies. We apply the proposed method to a real-world data set to examine the effect of sleeve gastrectomy versus Roux-en-Y gastric bypass on the time to first postoperative readmission.},
  archive      = {J_BIOSTAT},
  author       = {Li, Dongdong and Lu, Wenbin and Shu, Di and Toh, Sengwee and Wang, Rui},
  doi          = {10.1093/biostatistics/kxac006},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {776-794},
  shortjournal = {Biostatistics},
  title        = {Distributed cox proportional hazards regression using summary-level information},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semisupervised calibration of risk with noisy event times
(SCORNET) using electronic health record data. <em>BIOSTAT</em>,
<em>24</em>(3), 760–775. (<a
href="https://doi.org/10.1093/biostatistics/kxac003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging large-scale electronic health record (EHR) data to estimate survival curves for clinical events can enable more powerful risk estimation and comparative effectiveness research. However, use of EHR data is hindered by a lack of direct event time observations. Occurrence times of relevant diagnostic codes or target disease mentions in clinical notes are at best a good approximation of the true disease onset time. On the other hand, extracting precise information on the exact event time requires laborious manual chart review and is sometimes altogether infeasible due to a lack of detailed documentation. Current status labels—binary indicators of phenotype status during follow-up—are significantly more efficient and feasible to compile, enabling more precise survival curve estimation given limited resources. Existing survival analysis methods using current status labels focus almost entirely on supervised estimation, and naive incorporation of unlabeled data into these methods may lead to biased estimates. In this article, we propose Semisupervised Calibration of Risk with Noisy Event Times (SCORNET), which yields a consistent and efficient survival function estimator by leveraging a small set of current status labels and a large set of informative features. In addition to providing theoretical justification of SCORNET, we demonstrate in both simulation and real-world EHR settings that SCORNET achieves efficiency akin to the parametric Weibull regression model, while also exhibiting semi-nonparametric flexibility and relatively low empirical bias in a variety of generative settings.},
  archive      = {J_BIOSTAT},
  author       = {Ahuja, Yuri and Liang, Liang and Zhou, Doudou and Huang, Sicong and Cai, Tianxi},
  doi          = {10.1093/biostatistics/kxac003},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {760-775},
  shortjournal = {Biostatistics},
  title        = {Semisupervised calibration of risk with noisy event times (SCORNET) using electronic health record data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian MultiLayer record linkage procedure to analyze
post-acute care recovery of patients with traumatic brain injury.
<em>BIOSTAT</em>, <em>24</em>(3), 743–759. (<a
href="https://doi.org/10.1093/biostatistics/kxac016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding associations between injury severity and postacute care recovery for patients with traumatic brain injury (TBI) is crucial to improving care. Estimating these associations requires information on patients’ injury, demographics, and healthcare utilization, which are dispersed across multiple data sets. Because of privacy regulations, unique identifiers are not available to link records across these data sets. Record linkage methods identify records that represent the same patient across data sets in the absence of unique identifiers. With a large number of records, these methods may result in many false links. Health providers are a natural grouping scheme for patients, because only records that receive care from the same provider can represent the same patient. In some cases, providers are defined within each data set, but they are not uniquely identified across data sets. We propose a Bayesian record linkage procedure that simultaneously links providers and patients. The procedure improves the accuracy of the estimated links compared to current methods. We use this procedure to merge a trauma registry with Medicare claims to estimate the association between TBI patients’ injury severity and postacute care recovery.},
  archive      = {J_BIOSTAT},
  author       = {Shan, Mingyang and Thomas, Kali S and Gutman, Roee},
  doi          = {10.1093/biostatistics/kxac016},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {743-759},
  shortjournal = {Biostatistics},
  title        = {A bayesian MultiLayer record linkage procedure to analyze post-acute care recovery of patients with traumatic brain injury},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending prediction models for use in a new target
population with failure time outcomes. <em>BIOSTAT</em>, <em>24</em>(3),
728–742. (<a
href="https://doi.org/10.1093/biostatistics/kxac011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models are often built and evaluated using data from a population that differs from the target population where model-derived predictions are intended to be used in. In this article, we present methods for evaluating model performance in the target population when some observations are right censored. The methods assume that outcome and covariate data are available from a source population used for model development and covariates, but no outcome data, are available from the target population. We evaluate the finite sample performance of the proposed estimators using simulations and apply the methods to transport a prediction model built using data from a lung cancer screening trial to a nationally representative population of participants eligible for lung cancer screening.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon A},
  doi          = {10.1093/biostatistics/kxac011},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {728-742},
  shortjournal = {Biostatistics},
  title        = {Extending prediction models for use in a new target population with failure time outcomes},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric bayesian inference for optimal dynamic
treatment regimes via dynamic marginal structural models.
<em>BIOSTAT</em>, <em>24</em>(3), 708–727. (<a
href="https://doi.org/10.1093/biostatistics/kxac007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considerable statistical work done on dynamic treatment regimes (DTRs) is in the frequentist paradigm, but Bayesian methods may have much to offer in this setting as they allow for the appropriate representation and propagation of uncertainty, including at the individual level. In this work, we extend the use of recently developed Bayesian methods for Marginal Structural Models to arrive at inference of DTRs. We do this (i) by linking the observational world with a world in which all patients are randomized to a DTR, thereby allowing for causal inference and then (ii) by maximizing a posterior predictive utility, where the posterior distribution has been obtained from nonparametric prior assumptions on the observational world data-generating process. Our approach relies on Bayesian semiparametric inference, where inference about a finite-dimensional parameter is made all while working within an infinite-dimensional space of distributions. We further study Bayesian inference of DTRs in the double robust setting by using posterior predictive inference and the nonparametric Bayesian bootstrap. The proposed methods allow for uncertainty quantification at the individual level, thereby enabling personalized decision-making. We examine the performance of these methods via simulation and demonstrate their utility by exploring whether to adapt HIV therapy to a measure of patients’ liver health, in order to minimize liver scarring.},
  archive      = {J_BIOSTAT},
  author       = {Rodriguez Duque, Daniel and Stephens, David A and Moodie, Erica E M and Klein, Marina B},
  doi          = {10.1093/biostatistics/kxac007},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {708-727},
  shortjournal = {Biostatistics},
  title        = {Semiparametric bayesian inference for optimal dynamic treatment regimes via dynamic marginal structural models},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric causal mediation analysis for stochastic
interventional (in)direct effects. <em>BIOSTAT</em>, <em>24</em>(3),
686–707. (<a
href="https://doi.org/10.1093/biostatistics/kxac002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis has historically been limited in two important ways: (i) a focus has traditionally been placed on binary exposures and static interventions and (ii) direct and indirect effect decompositions have been pursued that are only identifiable in the absence of intermediate confounders affected by exposure. We present a theoretical study of an (in)direct effect decomposition of the population intervention effect, defined by stochastic interventions jointly applied to the exposure and mediators. In contrast to existing proposals, our causal effects can be evaluated regardless of whether an exposure is categorical or continuous and remain well-defined even in the presence of intermediate confounders affected by exposure. Our (in)direct effects are identifiable without a restrictive assumption on cross-world counterfactual independencies, allowing for substantive conclusions drawn from them to be validated in randomized controlled trials. Beyond the novel effects introduced, we provide a careful study of nonparametric efficiency theory relevant for the construction of flexible, multiply robust estimators of our (in)direct effects, while avoiding undue restrictions induced by assuming parametric models of nuisance parameter functionals. To complement our nonparametric estimation strategy, we introduce inferential techniques for constructing confidence intervals and hypothesis tests, and discuss open-source software, the |$\texttt{medshift}$| |$\texttt{R}$| package, implementing the proposed methodology. Application of our (in)direct effects and their nonparametric estimators is illustrated using data from a comparative effectiveness trial examining the direct and indirect effects of pharmacological therapeutics on relapse to opioid use disorder.},
  archive      = {J_BIOSTAT},
  author       = {Hejazi, Nima S and Rudolph, Kara E and Van Der Laan, Mark J and Díaz, Iván},
  doi          = {10.1093/biostatistics/kxac002},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {686-707},
  shortjournal = {Biostatistics},
  title        = {Nonparametric causal mediation analysis for stochastic interventional (in)direct effects},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible bayesian framework for individualized inference
via adaptive borrowing. <em>BIOSTAT</em>, <em>24</em>(3), 669–685. (<a
href="https://doi.org/10.1093/biostatistics/kxab051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion in high-resolution data capture technologies in health has increased interest in making inferences about individual-level parameters. While technology may provide substantial data on a single individual, how best to use multisource population data to improve individualized inference remains an open research question. One possible approach, the multisource exchangeability model (MEM), is a Bayesian method for integrating data from supplementary sources into the analysis of a primary source. MEM was originally developed to improve inference for a single study by asymmetrically borrowing information from a set of similar previous studies and was further developed to apply a more computationally intensive symmetric borrowing in the context of basket trial; however, even for asymmetric borrowing, its computational burden grows exponentially with the number of supplementary sources, making it unsuitable for applications where hundreds or thousands of supplementary sources (i.e., individuals) could contribute to inference on a given individual. In this article, we propose the data-driven MEM (dMEM), a two-stage approach that includes both source selection and clustering to enable the inclusion of an arbitrary number of sources to contribute to individualized inference in a computationally tractable and data-efficient way. We illustrate the application of dMEM to individual-level human behavior and mental well-being data collected via smartphones, where our approach increases individual-level estimation precision by 84% compared with a standard no-borrowing method and outperforms recently proposed competing methods in 80% of individuals.},
  archive      = {J_BIOSTAT},
  author       = {Ji, Ziyu and Wolfson, Julian},
  doi          = {10.1093/biostatistics/kxab051},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {669-685},
  shortjournal = {Biostatistics},
  title        = {A flexible bayesian framework for individualized inference via adaptive borrowing},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized decomposition using residuals (PeDecURe) for
feature extraction in the presence of nuisance variables.
<em>BIOSTAT</em>, <em>24</em>(3), 653–668. (<a
href="https://doi.org/10.1093/biostatistics/kxac031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging data are an increasingly important part of etiological studies of neurological and psychiatric disorders. However, mitigating the influence of nuisance variables, including confounders, remains a challenge in image analysis. In studies of Alzheimer’s disease, for example, an imbalance in disease rates by age and sex may make it difficult to distinguish between structural patterns in the brain (as measured by neuroimaging scans) attributable to disease progression and those characteristic of typical human aging or sex differences. Concerningly, when not properly accounted for, nuisance variables pose threats to the generalizability and interpretability of findings from these studies. Motivated by this critical issue, in this work, we examine the impact of nuisance variables on feature extraction methods and propose Penalized Decomposition Using Residuals (PeDecURe), a new method for obtaining nuisance variable-adjusted features. PeDecURe estimates primary directions of variation which maximize covariance between partially residualized imaging features and a variable of interest (e.g., Alzheimer’s diagnosis) while simultaneously mitigating the influence of nuisance variation through a penalty on the covariance between partially residualized imaging features and those variables. Using features derived using PeDecURe’s first direction of variation, we train a highly accurate and generalizable predictive model, as evidenced by its robustness in testing samples with different underlying nuisance variable distributions. We compare PeDecURe to commonly used decomposition methods (principal component analysis (PCA) and partial least squares) as well as a confounder-adjusted variation of PCA. We find that features derived from PeDecURe offer greater accuracy and generalizability and lower correlations with nuisance variables compared with the other methods. While PeDecURe is primarily motivated by challenges that arise in the analysis of neuroimaging data, it is broadly applicable to data sets with highly correlated features, where novel methods to handle nuisance variables are warranted.},
  archive      = {J_BIOSTAT},
  author       = {Weinstein, Sarah M and Davatzikos, Christos and Doshi, Jimit and Linn, Kristin A and Shinohara, Russell T and For the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1093/biostatistics/kxac031},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {653-668},
  shortjournal = {Biostatistics},
  title        = {Penalized decomposition using residuals (PeDecURe) for feature extraction in the presence of nuisance variables},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overcoming the impacts of two-step batch effect correction
on gene expression estimation and inference. <em>BIOSTAT</em>,
<em>24</em>(3), 635–652. (<a
href="https://doi.org/10.1093/biostatistics/kxab039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonignorable technical variation is commonly observed across data from multiple experimental runs, platforms, or studies. These so-called batch effects can lead to difficulty in merging data from multiple sources, as they can severely bias the outcome of the analysis. Many groups have developed approaches for removing batch effects from data, usually by accommodating batch variables into the analysis (one-step correction) or by preprocessing the data prior to the formal or final analysis (two-step correction). One-step correction is often desirable due it its simplicity, but its flexibility is limited and it can be difficult to include batch variables uniformly when an analysis has multiple stages. Two-step correction allows for richer models of batch mean and variance. However, prior investigation has indicated that two-step correction can lead to incorrect statistical inference in downstream analysis. Generally speaking, two-step approaches introduce a correlation structure in the corrected data, which, if ignored, may lead to either exaggerated or diminished significance in downstream applications such as differential expression analysis. Here, we provide more intuitive and more formal evaluations of the impacts of two-step batch correction compared to existing literature. We demonstrate that the undesired impacts of two-step correction (exaggerated or diminished significance) depend on both the nature of the study design and the batch effects. We also provide strategies for overcoming these negative impacts in downstream analyses using the estimated correlation matrix of the corrected data. We compare the results of our proposed workflow with the results from other published one-step and two-step methods and show that our methods lead to more consistent false discovery controls and power of detection across a variety of batch effect scenarios. Software for our method is available through GitHub ( https://github.com/jtleek/sva-devel ) and will be available in future versions of the |$\texttt{sva}$| R package in the Bioconductor project ( https://bioconductor.org/packages/release/bioc/html/sva.html ).},
  archive      = {J_BIOSTAT},
  author       = {Li, Tenglong and Zhang, Yuqing and Patil, Prasad and Johnson, W Evan},
  doi          = {10.1093/biostatistics/kxab039},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {635-652},
  shortjournal = {Biostatistics},
  title        = {Overcoming the impacts of two-step batch effect correction on gene expression estimation and inference},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing chromatin relocalization in 3D using the patient
rule induction method. <em>BIOSTAT</em>, <em>24</em>(3), 618–634. (<a
href="https://doi.org/10.1093/biostatistics/kxab033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) genome architecture is critical for numerous cellular processes, including transcription, while certain conformation-driven structural alterations are frequently oncogenic. Inferring 3D chromatin configurations has been advanced by the emergence of chromatin conformation capture assays, notably Hi-C, and attendant 3D reconstruction algorithms. These have enhanced understanding of chromatin spatial organization and afforded numerous downstream biological insights. Until recently, comparisons of 3D reconstructions between conditions and/or cell types were limited to prescribed structural features. However, multiMDS , a pioneering approach developed by Rieber and Mahony (2019). that performs joint reconstruction and alignment, enables quantification of all locus-specific differences between paired Hi-C data sets. By subsequently mapping these differences to the linear (1D) genome the identification of relocalization regions is facilitated through the use of peak calling in conjunction with continuous wavelet transformation. Here, we seek to refine this approach by performing the search for significant relocalization regions in terms of the 3D structures themselves, thereby retaining the benefits of 3D reconstruction and avoiding limitations associated with the 1D perspective. The search for (extreme) relocalization regions is conducted using the patient rule induction method (PRIM). Considerations surrounding orienting structures with respect to compartmental and principal component axes are discussed, as are approaches to inference and reconstruction accuracy assessment. The illustration makes recourse to comparisons between four different cell types.},
  archive      = {J_BIOSTAT},
  author       = {Segal, Mark R},
  doi          = {10.1093/biostatistics/kxab033},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {618-634},
  shortjournal = {Biostatistics},
  title        = {Assessing chromatin relocalization in 3D using the patient rule induction method},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vaccine efficacy at a point in time. <em>BIOSTAT</em>,
<em>24</em>(3), 603–617. (<a
href="https://doi.org/10.1093/biostatistics/kxac008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vaccine trials are generally designed to assess efficacy on clinical disease. The vaccine effect on infection, while important both as a proxy for transmission and to describe a vaccine’s entire effects, requires frequent (e.g., twice a week) longitudinal sampling to capture all infections. Such sampling may not always be feasible. A logistically easy approach is to collect a sample to test for infection at a regularly scheduled visit. Such point or cross-sectional sampling does not permit estimation of classic vaccine efficacy on infection, as long duration infections are sampled with higher probability. Building on work by Rinta-Kokko and others (2009) and Lipsitch and Kahn (2021) , we evaluate proxies of the vaccine effect on transmission at a point in time; the vaccine efficacy on prevalent infection and on prevalent viral load, VE |$_{\rm PI}$| and VE |$_{\rm PVL}$|⁠ , respectively. Longer infections with higher viral loads should have more transmission potential and prevalent vaccine efficacy naturally captures this aspect. We demonstrate how these parameters obtain from an underlying proportional hazards model for infection and allow for waning efficacy on infection, duration, and viral load. We estimate these parameters based on regression models with either repeated cross-sectional sampling or frequent longitudinal sampling. We evaluate the methods by simulation and analyze a phase III vaccine trial with polymerase chain reaction (PCR) cross-sectional sampling for subclinical infection.},
  archive      = {J_BIOSTAT},
  author       = {Follmann, Dean A and Fay, Michael P},
  doi          = {10.1093/biostatistics/kxac008},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {603-617},
  shortjournal = {Biostatistics},
  title        = {Vaccine efficacy at a point in time},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-phase stratified sampling and analysis for predicting
binary outcomes. <em>BIOSTAT</em>, <em>24</em>(3), 585–602. (<a
href="https://doi.org/10.1093/biostatistics/kxab044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-phase study design is a cost-efficient sampling strategy when certain data elements are expensive and, thus, can only be collected on a sub-sample of subjects. To date guidance on how best to allocate resources within the design has assumed that primary interest lies in estimating association parameters. When primary interest lies in the development and evaluation of a risk prediction tool, however, such guidance may, in fact, be detrimental. To resolve this, we propose a novel strategy for resource allocation based on oversampling cases and subjects who have more extreme risk estimates according to a preliminary model developed using fully observed predictors. Key to the proposed strategy is that it focuses on enhancing efficiency regarding estimation of measures of predictive accuracy, rather than on efficiency regarding association parameters which is the standard paradigm. Towards valid estimation and inference for accuracy measures using the resultant data, we extend an existing semiparametric maximum likelihood ethod for estimating odds ratio association parameters to accommodate the biased sampling scheme and data incompleteness. Motivated by our sampling design, we additionally propose a general post-stratification scheme for analyzing general two-phase data for estimating predictive accuracy measures. Through theoretical calculations and simulation studies, we show that the proposed sampling strategy and post-stratification scheme achieve the promised efficiency improvement. Finally, we apply the proposed methods to develop and evaluate a preliminary model for predicting the risk of hospital readmission after cardiac surgery using data from the Pennsylvania Health Care Cost Containment Council.},
  archive      = {J_BIOSTAT},
  author       = {Cao, Yaqi and Haneuse, Sebastien and Zheng, Yingye and Chen, Jinbo},
  doi          = {10.1093/biostatistics/kxab044},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {585-602},
  shortjournal = {Biostatistics},
  title        = {Two-phase stratified sampling and analysis for predicting binary outcomes},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate bayesian spatio-temporal p-spline models to
analyze crimes against women. <em>BIOSTAT</em>, <em>24</em>(3), 562–584.
(<a href="https://doi.org/10.1093/biostatistics/kxab042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Univariate spatio-temporal models for areal count data have received great attention in recent years for estimating risks. However, models for studying multivariate responses are less commonly used mainly due to the computational burden. In this article, multivariate spatio-temporal P-spline models are proposed to study different forms of violence against women. Modeling distinct crimes jointly improves the precision of estimates over univariate models and allows to compute correlations among them. The correlation between the spatial and the temporal patterns may suggest connections among the different crimes that will certainly benefit a thorough comprehension of this problem that affects millions of women around the world. The models are fitted using integrated nested Laplace approximations and are used to analyze four distinct crimes against women at district level in the Indian state of Maharashtra during the period 2001–2013.},
  archive      = {J_BIOSTAT},
  author       = {Vicente, Gonzalo and Goicoa, Tomás and Ugarte, María Dolores},
  doi          = {10.1093/biostatistics/kxab042},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {562-584},
  shortjournal = {Biostatistics},
  title        = {Multivariate bayesian spatio-temporal P-spline models to analyze crimes against women},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributional data analysis via quantile functions and its
application to modeling digital biomarkers of gait in alzheimer’s
disease. <em>BIOSTAT</em>, <em>24</em>(3), 539–561. (<a
href="https://doi.org/10.1093/biostatistics/kxab041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of continuous health monitoring with wearable devices, users now generate their unique streams of continuous data such as minute-level step counts or heartbeats. Summarizing these streams via scalar summaries often ignores the distributional nature of wearable data and almost unavoidably leads to the loss of critical information. We propose to capture the distributional nature of wearable data via user-specific quantile functions (QF) and use these QFs as predictors in scalar-on-quantile-function-regression (SOQFR). As an alternative approach, we also propose to represent QFs via user-specific L-moments, robust rank-based analogs of traditional moments, and use L-moments as predictors in SOQFR (SOQFR-L). These two approaches provide two mutually consistent interpretations: in terms of quantile levels by SOQFR and in terms of L-moments by SOQFR-L. We also demonstrate how to deal with multi-modal distributional data via Joint and Individual Variation Explained using L-moments. The proposed methods are illustrated in a study of association of digital gait biomarkers with cognitive function in Alzheimers disease. Our analysis shows that the proposed methods demonstrate higher predictive performance and attain much stronger associations with clinical cognitive scales compared to simple distributional summaries.},
  archive      = {J_BIOSTAT},
  author       = {Ghosal, Rahul and Varma, Vijay R and Volfson, Dmitri and Hillel, Inbar and Urbanek, Jacek and Hausdorff, Jeffrey M and Watts, Amber and Zipunnikov, Vadim},
  doi          = {10.1093/biostatistics/kxab041},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {539-561},
  shortjournal = {Biostatistics},
  title        = {Distributional data analysis via quantile functions and its application to modeling digital biomarkers of gait in alzheimer’s disease},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly robust nonparametric instrumental variable estimators
for survival outcomes. <em>BIOSTAT</em>, <em>24</em>(2), 518–537. (<a
href="https://doi.org/10.1093/biostatistics/kxab036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable (IV) methods allow us the opportunity to address unmeasured confounding in causal inference. However, most IV methods are only applicable to discrete or continuous outcomes with very few IV methods for censored survival outcomes. In this article, we propose nonparametric estimators for the local average treatment effect on survival probabilities under both covariate-dependent and outcome-dependent censoring. We provide an efficient influence function-based estimator and a simple estimation procedure when the IV is either binary or continuous. The proposed estimators possess double-robustness properties and can easily incorporate nonparametric estimation using machine learning tools. In simulation studies, we demonstrate the flexibility and double robustness of our proposed estimators under various plausible scenarios. We apply our method to the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial for estimating the causal effect of screening on survival probabilities and investigate the causal contrasts between the two interventions under different censoring assumptions.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Youjin and Kennedy, Edward H and Mitra, Nandita},
  doi          = {10.1093/biostatistics/kxab036},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {518-537},
  shortjournal = {Biostatistics},
  title        = {Doubly robust nonparametric instrumental variable estimators for survival outcomes},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage TMLE to reduce bias and improve efficiency in
cluster randomized trials. <em>BIOSTAT</em>, <em>24</em>(2), 502–517.
(<a href="https://doi.org/10.1093/biostatistics/kxab043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) randomly assign an intervention to groups of individuals (e.g., clinics or communities) and measure outcomes on individuals in those groups. While offering many advantages, this experimental design introduces challenges that are only partially addressed by existing analytic approaches. First, outcomes are often missing for some individuals within clusters. Failing to appropriately adjust for differential outcome measurement can result in biased estimates and inference. Second, CRTs often randomize limited numbers of clusters, resulting in chance imbalances on baseline outcome predictors between arms. Failing to adaptively adjust for these imbalances and other predictive covariates can result in efficiency losses. To address these methodological gaps, we propose and evaluate a novel two-stage targeted minimum loss-based estimator to adjust for baseline covariates in a manner that optimizes precision, after controlling for baseline and postbaseline causes of missing outcomes. Finite sample simulations illustrate that our approach can nearly eliminate bias due to differential outcome measurement, while existing CRT estimators yield misleading results and inferences. Application to real data from the SEARCH community randomized trial demonstrates the gains in efficiency afforded through adaptive adjustment for baseline covariates, after controlling for missingness on individual-level outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Balzer, Laura B and van der Laan, Mark and Ayieko, James and Kamya, Moses and Chamie, Gabriel and Schwab, Joshua and Havlir, Diane V and Petersen, Maya L},
  doi          = {10.1093/biostatistics/kxab043},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {502-517},
  shortjournal = {Biostatistics},
  title        = {Two-stage TMLE to reduce bias and improve efficiency in cluster randomized trials},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying uncertainty in spikes estimated from calcium
imaging data. <em>BIOSTAT</em>, <em>24</em>(2), 481–501. (<a
href="https://doi.org/10.1093/biostatistics/kxab034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a number of methods have been proposed to estimate the times at which a neuron spikes on the basis of calcium imaging data. However, quantifying the uncertainty associated with these estimated spikes remains an open problem. We consider a simple and well-studied model for calcium imaging data, which states that calcium decays exponentially in the absence of a spike, and instantaneously increases when a spike occurs. We wish to test the null hypothesis that the neuron did not spike—i.e., that there was no increase in calcium—at a particular timepoint at which a spike was estimated. In this setting, classical hypothesis tests lead to inflated Type I error, because the spike was estimated on the same data used for testing. To overcome this problem, we propose a selective inference approach. We describe an efficient algorithm to compute finite-sample |$p$| -values that control selective Type I error, and confidence intervals with correct selective coverage, for spikes estimated using a recent proposal from the literature. We apply our proposal in simulation and on calcium imaging data from the |$\texttt{spikefinder}$| challenge.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Yiqun T and Jewell, Sean W and Witten, Daniela M},
  doi          = {10.1093/biostatistics/kxab034},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {481-501},
  shortjournal = {Biostatistics},
  title        = {Quantifying uncertainty in spikes estimated from calcium imaging data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation for the bivariate quantile varying coefficient
model with application to diffusion tensor imaging data analysis.
<em>BIOSTAT</em>, <em>24</em>(2), 465–480. (<a
href="https://doi.org/10.1093/biostatistics/kxab031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite interest in the joint modeling of multiple functional responses such as diffusion properties in neuroimaging, robust statistical methods appropriate for this task are lacking. To address this need, we propose a varying coefficient quantile regression model able to handle bivariate functional responses. Our work supports innovative insights into biomedical data by modeling the joint distribution of functional variables over their domains and across clinical covariates. We propose an estimation procedure based on the alternating direction method of multipliers and propagation separation algorithms to estimate varying coefficients using a B-spline basis and an |$L_2$| smoothness penalty that encourages interpretability. A simulation study and an application to a real-world neurodevelopmental data set demonstrates the performance of our model and the insights provided by modeling functional fractional anisotropy and mean diffusivity jointly and their association with gestational age and sex.},
  archive      = {J_BIOSTAT},
  author       = {Pietrosanu, Matthew and Shu, Haoxu and Jiang, Bei and Kong, Linglong and Heo, Giseon and He, Qianchuan and Gilmore, John and Zhu, Hongtu},
  doi          = {10.1093/biostatistics/kxab031},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {465-480},
  shortjournal = {Biostatistics},
  title        = {Estimation for the bivariate quantile varying coefficient model with application to diffusion tensor imaging data analysis},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated causal-predictive machine learning models for
tropical cyclone epidemiology. <em>BIOSTAT</em>, <em>24</em>(2),
449–464. (<a
href="https://doi.org/10.1093/biostatistics/kxab047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic preparedness reduces the adverse health impacts of hurricanes and tropical storms, referred to collectively as tropical cyclones (TCs), but its protective impact could be enhanced by a more comprehensive and rigorous characterization of TC epidemiology. To generate the insights and tools necessary for high-precision TC preparedness, we introduce a machine learning approach that standardizes estimation of historic TC health impacts, discovers common patterns and sources of heterogeneity in those health impacts, and enables identification of communities at highest health risk for future TCs. The model integrates (i) a causal inference component to quantify the immediate health impacts of recent historic TCs at high spatial resolution and (ii) a predictive component that captures how TC meteorological features and socioeconomic/demographic characteristics of impacted communities are associated with health impacts. We apply it to a rich data platform containing detailed historic TC exposure information and records of all-cause mortality and cardiovascular- and respiratory-related hospitalization among Medicare recipients. We report a high degree of heterogeneity in the acute health impacts of historic TCs, both within and across TCs, and, on average, substantial TC-attributable increases in respiratory hospitalizations. TC-sustained windspeeds are found to be the primary driver of mortality and respiratory risks.},
  archive      = {J_BIOSTAT},
  author       = {Nethery, Rachel C and Katz-Christy, Nina and Kioumourtzoglou, Marianthi-Anna and Parks, Robbie M and Schumacher, Andrea and Anderson, G Brooke},
  doi          = {10.1093/biostatistics/kxab047},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {449-464},
  shortjournal = {Biostatistics},
  title        = {Integrated causal-predictive machine learning models for tropical cyclone epidemiology},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Historical controls in clinical trials: A note on linking
pocock’s model with the robust mixture priors. <em>BIOSTAT</em>,
<em>24</em>(2), 443–448. (<a
href="https://doi.org/10.1093/biostatistics/kxab040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several Bayesian methods have been proposed to borrow information dynamically from historical controls in clinical trials. In this note, we identify key features of the relationship between the first method proposed, the bias–variance method, which is strongly related to the commensurate prior approach, and a more recent and widely used approach called robust mixture priors (RMP). We focus on the two key terms that need to be chosen to define the RMP, namely |$w$|⁠ , the prior probability that the new trial differs systematically from the historical trial, and |$s_v^2$|⁠ , the variance of the vague component of the RMP. The relationship with Pocock’s prior reveals that different combinations of these two terms can express similar prior beliefs about the amount of information provided by the historical data. This demonstrates the value of fixing |$s_v^2$|⁠ , e.g., so the vague component is “worth one subject.” Prior belief about the relevance of the historical data is then driven by a single value, the prespecified weight |$w$|⁠ .},
  archive      = {J_BIOSTAT},
  author       = {Callegaro, Andrea and Galwey, Nicholas and Abellan, Juan J},
  doi          = {10.1093/biostatistics/kxab040},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {443-448},
  shortjournal = {Biostatistics},
  title        = {Historical controls in clinical trials: A note on linking pocock’s model with the robust mixture priors},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian finite mixture of regression analysis for cancer
based on histopathological imaging–environment interactions.
<em>BIOSTAT</em>, <em>24</em>(2), 425–442. (<a
href="https://doi.org/10.1093/biostatistics/kxab038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a heterogeneous disease. Finite mixture of regression (FMR)—as an important heterogeneity analysis technique when an outcome variable is present—has been extensively employed in cancer research, revealing important differences in the associations between a cancer outcome/phenotype and covariates. Cancer FMR analysis has been based on clinical, demographic, and omics variables. A relatively recent and alternative source of data comes from histopathological images. Histopathological images have been long used for cancer diagnosis and staging. Recently, it has been shown that high-dimensional histopathological image features, which are extracted using automated digital image processing pipelines, are effective for modeling cancer outcomes/phenotypes. Histopathological imaging–environment interaction analysis has been further developed to expand the scope of cancer modeling and histopathological imaging-based analysis. Motivated by the significance of cancer FMR analysis and a still strong demand for more effective methods, in this article, we take the natural next step and conduct cancer FMR analysis based on models that incorporate low-dimensional clinical/demographic/environmental variables, high-dimensional imaging features, as well as their interactions. Complementary to many of the existing studies, we develop a Bayesian approach for accommodating high dimensionality, screening out noises, identifying signals, and respecting the “main effects, interactions” variable selection hierarchy. An effective computational algorithm is developed, and simulation shows advantageous performance of the proposed approach. The analysis of The Cancer Genome Atlas data on lung squamous cell cancer leads to interesting findings different from the alternative approaches.},
  archive      = {J_BIOSTAT},
  author       = {Im, Yunju and Huang, Yuan and Tan, Aixin and Ma, Shuangge},
  doi          = {10.1093/biostatistics/kxab038},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {425-442},
  shortjournal = {Biostatistics},
  title        = {Bayesian finite mixture of regression analysis for cancer based on histopathological imaging–environment interactions},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A meta-inference framework to integrate multiple external
models into a current study. <em>BIOSTAT</em>, <em>24</em>(2), 406–424.
(<a href="https://doi.org/10.1093/biostatistics/kxab017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is becoming increasingly common for researchers to consider incorporating external information from large studies to improve the accuracy of statistical inference instead of relying on a modestly sized data set collected internally. With some new predictors only available internally, we aim to build improved regression models based on individual-level data from an “internal” study while incorporating summary-level information from “external” models. We propose a meta-analysis framework along with two weighted estimators as the composite of empirical Bayes estimators, which combines the estimates from different external models. The proposed framework is flexible and robust in the ways that (i) it is capable of incorporating external models that use a slightly different set of covariates; (ii) it is able to identify the most relevant external information and diminish the influence of information that is less compatible with the internal data; and (iii) it nicely balances the bias-variance trade-off while preserving the most efficiency gain. The proposed estimators are more efficient than the naïve analysis of the internal data and other naïve combinations of external estimators.},
  archive      = {J_BIOSTAT},
  author       = {Gu, Tian and Taylor, Jeremy M G and Mukherjee, Bhramar},
  doi          = {10.1093/biostatistics/kxab017},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {406-424},
  shortjournal = {Biostatistics},
  title        = {A meta-inference framework to integrate multiple external models into a current study},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ACTOR: A latent dirichlet model to compare expressed isoform
proportions to a reference panel. <em>BIOSTAT</em>, <em>24</em>(2),
388–405. (<a
href="https://doi.org/10.1093/biostatistics/kxab013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relative proportion of RNA isoforms expressed for a given gene has been associated with disease states in cancer, retinal diseases, and neurological disorders. Examination of relative isoform proportions can help determine biological mechanisms, but such analyses often require a per-gene investigation of splicing patterns. Leveraging large public data sets produced by genomic consortia as a reference, one can compare splicing patterns in a data set of interest with those of a reference panel in which samples are divided into distinct groups, such as tissue of origin, or disease status. We propose A latent Dirichlet model to Compare expressed isoform proportions TO a Reference panel (ACTOR), a latent Dirichlet model with Dirichlet Multinomial observations to compare expressed isoform proportions in a data set to an independent reference panel. We use a variational Bayes procedure to estimate posterior distributions for the group membership of one or more samples. Using the Genotype-Tissue Expression project as a reference data set, we evaluate ACTOR on simulated and real RNA-seq data sets to determine tissue-type classifications of genes. ACTOR is publicly available as an R package at https://github.com/mccabes292/actor .},
  archive      = {J_BIOSTAT},
  author       = {McCabe, Sean D and Nobel, Andrew B and Love, Michael I},
  doi          = {10.1093/biostatistics/kxab013},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {388-405},
  shortjournal = {Biostatistics},
  title        = {ACTOR: A latent dirichlet model to compare expressed isoform proportions to a reference panel},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian semiparametric approach for inference on the
population partly conditional mean from longitudinal data with dropout.
<em>BIOSTAT</em>, <em>24</em>(2), 372–387. (<a
href="https://doi.org/10.1093/biostatistics/kxab012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of memory trajectories using longitudinal data often result in highly nonrepresentative samples due to selective study enrollment and attrition. An additional bias comes from practice effects that result in improved or maintained performance due to familiarity with test content or context. These challenges may bias study findings and severely distort the ability to generalize to the target population. In this study, we propose an approach for estimating the finite population mean of a longitudinal outcome conditioning on being alive at a specific time point. We develop a flexible Bayesian semiparametric predictive estimator for population inference when longitudinal auxiliary information is known for the target population. We evaluate the sensitivity of the results to untestable assumptions and further compare our approach to other methods used for population inference in a simulation study. The proposed approach is motivated by 15-year longitudinal data from the Betula longitudinal cohort study. We apply our approach to estimate lifespan trajectories in episodic memory, with the aim to generalize findings to a target population.},
  archive      = {J_BIOSTAT},
  author       = {Josefsson, Maria and Daniels, Michael J and Pudas, Sara},
  doi          = {10.1093/biostatistics/kxab012},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {372-387},
  shortjournal = {Biostatistics},
  title        = {A bayesian semiparametric approach for inference on the population partly conditional mean from longitudinal data with dropout},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting the onset of breast cancer using mammogram
imaging data with irregular boundary. <em>BIOSTAT</em>, <em>24</em>(2),
358–371. (<a
href="https://doi.org/10.1093/biostatistics/kxab032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With mammography being the primary breast cancer screening strategy, it is essential to make full use of the mammogram imaging data to better identify women who are at higher and lower than average risk. Our primary goal in this study is to extract mammogram-based features that augment the well-established breast cancer risk factors to improve prediction accuracy. In this article, we propose a supervised functional principal component analysis (sFPCA) over triangulations method for extracting features that are ordered by the magnitude of association with the failure time outcome. The proposed method accommodates the irregular boundary issue posed by the breast area within the mammogram imaging data with flexible bivariate splines over triangulations. We also provide an eigenvalue decomposition algorithm that is computationally efficient. Compared to the conventional unsupervised FPCA method, the proposed method results in a lower Brier Score and higher area under the ROC curve (AUC) in simulation studies. We apply our method to data from the Joanne Knight Breast Health Cohort at Siteman Cancer Center. Our approach not only obtains the best prediction performance comparing to unsupervised FPCA and benchmark models but also reveals important risk patterns within the mammogram images. This demonstrates the importance of utilizing additional supervised image-based features to clarify breast cancer risk.},
  archive      = {J_BIOSTAT},
  author       = {Jiang, Shu and Cao, Jiguo and Colditz, Graham A and Rosner, Bernard},
  doi          = {10.1093/biostatistics/kxab032},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {358-371},
  shortjournal = {Biostatistics},
  title        = {Predicting the onset of breast cancer using mammogram imaging data with irregular boundary},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prognosis of cancer survivors: Estimation based on
differential equations. <em>BIOSTAT</em>, <em>24</em>(2), 345–357. (<a
href="https://doi.org/10.1093/biostatistics/kxab009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for estimating several prognosis parameters for cancer survivors. The method utilizes the fact that these parameters solve differential equations driven by cumulative hazards. By expressing the parameters as solutions to differential equations, we develop generic estimators that are easy to implement with standard statistical software. We explicitly describe the estimators for prognosis parameters that are often employed in practice, but also for parameters that, to our knowledge, have not been used to evaluate prognosis. We then apply these parameters to assess the prognosis of five common cancers in Norway.},
  archive      = {J_BIOSTAT},
  author       = {Ryalen, Pål C and Møller, Bjørn and Laache, Christoffer H and Stensrud, Mats J and Røysland, Kjetil},
  doi          = {10.1093/biostatistics/kxab009},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {345-357},
  shortjournal = {Biostatistics},
  title        = {Prognosis of cancer survivors: Estimation based on differential equations},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing a predictive signature for two trial endpoints
using the cross-validated risk scores method. <em>BIOSTAT</em>,
<em>24</em>(2), 327–344. (<a
href="https://doi.org/10.1093/biostatistics/kxaa055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing cross-validated risk scores (CVRS) design has been proposed for developing and testing the efficacy of a treatment in a high-efficacy patient group (the sensitive group) using high-dimensional data (such as genetic data). The design is based on computing a risk score for each patient and dividing them into clusters using a nonparametric clustering procedure. In some settings, it is desirable to consider the tradeoff between two outcomes, such as efficacy and toxicity, or cost and effectiveness. With this motivation, we extend the CVRS design (CVRS2) to consider two outcomes. The design employs bivariate risk scores that are divided into clusters. We assess the properties of the CVRS2 using simulated data and illustrate its application on a randomized psychiatry trial. We show that CVRS2 is able to reliably identify the sensitive group (the group for which the new treatment provides benefit on both outcomes) in the simulated data. We apply the CVRS2 design to a psychology clinical trial that had offender status and substance use status as two outcomes and collected a large number of baseline covariates. The CVRS2 design yields a significant treatment effect for both outcomes, while the CVRS approach identified a significant effect for the offender status only after prefiltering the covariates.},
  archive      = {J_BIOSTAT},
  author       = {Cherlin, Svetlana and Wason, James M S},
  doi          = {10.1093/biostatistics/kxaa055},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {327-344},
  shortjournal = {Biostatistics},
  title        = {Developing a predictive signature for two trial endpoints using the cross-validated risk scores method},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalizability score for aggregate causal effect.
<em>BIOSTAT</em>, <em>24</em>(2), 309–326. (<a
href="https://doi.org/10.1093/biostatistics/kxab029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists frequently generalize population level causal quantities such as average treatment effect from a source population to a target population. When the causal effects are heterogeneous, differences in subject characteristics between the source and target populations may make such a generalization difficult and unreliable. Reweighting or regression can be used to adjust for such differences when generalizing. However, these methods typically suffer from large variance if there is limited covariate distribution overlap between the two populations. We propose a generalizability score to address this issue. The score can be used as a yardstick to select target subpopulations for generalization. A simplified version of the score avoids using any outcome information and thus can prevent deliberate biases associated with inadvertent access to such information. Both simulation studies and real data analysis demonstrate convincing results for such selection.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Rui and Chen, Guanhua and Yu, Menggang},
  doi          = {10.1093/biostatistics/kxab029},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {309-326},
  shortjournal = {Biostatistics},
  title        = {A generalizability score for aggregate causal effect},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection for support vector regression using a
genetic algorithm. <em>BIOSTAT</em>, <em>24</em>(2), 295–308. (<a
href="https://doi.org/10.1093/biostatistics/kxab022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector regression (SVR) is particularly beneficial when the outcome and predictors are nonlinearly related. However, when many covariates are available, the method’s flexibility can lead to overfitting and an overall loss in predictive accuracy. To overcome this drawback, we develop a feature selection method for SVR based on a genetic algorithm that iteratively searches across potential subsets of covariates to find those that yield the best performance according to a user-defined fitness function. We evaluate the performance of our feature selection method for SVR, comparing it to alternate methods including LASSO and random forest, in a simulation study. We find that our method yields higher predictive accuracy than SVR without feature selection. Our method outperforms LASSO when the relationship between covariates and outcome is nonlinear. Random forest performs equivalently to our method in some scenarios, but more poorly when covariates are correlated. We apply our method to predict donor kidney function 1 year after transplant using data from the United Network for Organ Sharing national registry.},
  archive      = {J_BIOSTAT},
  author       = {McKearnan, Shannon B and Vock, David M and Marai, G Elisabeta and Canahuate, Guadalupe and Fuller, Clifton D and Wolfson, Julian},
  doi          = {10.1093/biostatistics/kxab022},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {295-308},
  shortjournal = {Biostatistics},
  title        = {Feature selection for support vector regression using a genetic algorithm},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian adaptive model selection design for optimal
biological dose finding in phase i/II clinical trials. <em>BIOSTAT</em>,
<em>24</em>(2), 277–294. (<a
href="https://doi.org/10.1093/biostatistics/kxab028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of the optimal dose presents a major challenge in drug development with molecularly targeted agents, immunotherapy, as well as chimeric antigen receptor T-cell treatments. By casting dose finding as a Bayesian model selection problem, we propose an adaptive design by simultaneously incorporating the toxicity and efficacy outcomes to select the optimal biological dose (OBD) in phase I/II clinical trials. Without imposing any parametric assumption or shape constraint on the underlying dose–response curves, we specify curve-free models for both the toxicity and efficacy endpoints to determine the OBD. By integrating the observed data across all dose levels, the proposed design is coherent in dose assignment and thus greatly enhances efficiency and accuracy in pinning down the right dose. Not only does our design possess a completely new yet flexible dose-finding framework, but it also has satisfactory and robust performance as demonstrated by extensive simulation studies. In addition, we show that our design enjoys desirable coherence properties, while most of existing phase I/II designs do not. We further extend the design to accommodate late-onset outcomes which are common in immunotherapy. The proposed design is exemplified with a phase I/II clinical trial in chronic lymphocytic leukemia.},
  archive      = {J_BIOSTAT},
  author       = {Lin, Ruitao and Yin, Guosheng and Shi, Haolun},
  doi          = {10.1093/biostatistics/kxab028},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {277-294},
  shortjournal = {Biostatistics},
  title        = {Bayesian adaptive model selection design for optimal biological dose finding in phase I/II clinical trials},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian multiregional clinical trials using model
averaging. <em>BIOSTAT</em>, <em>24</em>(2), 262–276. (<a
href="https://doi.org/10.1093/biostatistics/kxab027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiregional clinical trials (MRCTs) provide the benefit of more rapidly introducing drugs to the global market; however, small regional sample sizes can lead to poor estimation quality of region-specific effects when using current statistical methods. With the publication of the International Conference for Harmonisation E17 guideline in 2017, the MRCT design is recognized as a viable strategy that can be accepted by regional regulatory authorities, necessitating new statistical methods that improve the quality of region-specific inference. In this article, we develop a novel methodology for estimating region-specific and global treatment effects for MRCTs using Bayesian model averaging. This approach can be used for trials that compare two treatment groups with respect to a continuous outcome, and it allows for the incorporation of patient characteristics through the inclusion of covariates. We propose an approach that uses posterior model probabilities to quantify evidence in favor of consistency of treatment effects across all regions, and this metric can be used by regulatory authorities for drug approval. We show through simulations that the proposed modeling approach results in lower MSE than a fixed-effects linear regression model and better control of type I error rates than a Bayesian hierarchical model.},
  archive      = {J_BIOSTAT},
  author       = {Bean, Nathan W and Ibrahim, Joseph G and Psioda, Matthew A},
  doi          = {10.1093/biostatistics/kxab027},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {262-276},
  shortjournal = {Biostatistics},
  title        = {Bayesian multiregional clinical trials using model averaging},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical modeling of longitudinal medical cost
trajectory: Renal cell cancer care cost analyses. <em>BIOSTAT</em>,
<em>24</em>(2), 244–261. (<a
href="https://doi.org/10.1093/biostatistics/kxab024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the current cost of cancer care is important to health policy makers. An indispensable step in cost projection is to estimate cost trajectories from an incident cohort of cancer patients using longitudinal medical cost data, accounting for terminal events such as death, and right censoring due to loss of follow-up. Since the cost of cancer care and survival are correlated, a scientifically meaningful quantity for inference in this context is the mean cost trajectory conditional on survival. We propose a two-stage semiparametric approach to estimate the longitudinal cost trajectories from a joint model of longitudinal medical costs and survival. The longitudinal cost trajectories corresponding to various survival times form a bivariate surface in a triangular area. The cost trajectories are estimated using the tensor products of discretized measurement time and survival, as well as effective ridge penalties for data in 2D arrays. The proposed approach balances the practical considerations of model flexibility, statistical efficiency, and computational tractability. We used the proposed method to estimate the cost trajectories of renal cell cancer patients using the Surveillance, Epidemiology, and End Results-Medicare linked database.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Shikun and Shen, Yu and Shih, Ya-Chen Tina and Xu, Ying and Li, Liang},
  doi          = {10.1093/biostatistics/kxab024},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {244-261},
  shortjournal = {Biostatistics},
  title        = {Statistical modeling of longitudinal medical cost trajectory: Renal cell cancer care cost analyses},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable principal component analysis for multilevel
multivariate functional data. <em>BIOSTAT</em>, <em>24</em>(2), 227–243.
(<a href="https://doi.org/10.1093/biostatistics/kxab018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies collect functional data from multiple subjects that have both multilevel and multivariate structures. An example of such data comes from popular neuroscience experiments where participants’ brain activity is recorded using modalities such as electroencephalography and summarized as power within multiple time-varying frequency bands within multiple electrodes, or brain regions. Summarizing the joint variation across multiple frequency bands for both whole-brain variability between subjects, as well as location–variation within subjects, can help to explain neural reactions to stimuli. This article introduces a novel approach to conducting interpretable principal components analysis on multilevel multivariate functional data that decomposes total variation into subject-level and replicate-within-subject-level (i.e., electrode-level) variation and provides interpretable components that can be both sparse among variates (e.g., frequency bands) and have localized support over time within each frequency band. Smoothness is achieved through a roughness penalty, while sparsity and localization of components are achieved by solving an innovative rank-one based convex optimization problem with block Frobenius and matrix |$L_1$| -norm-based penalties. The method is used to analyze data from a study to better understand reactions to emotional information in individuals with histories of trauma and the symptom of dissociation, revealing new neurophysiological insights into how subject- and electrode-level brain activity are associated with these phenomena. Supplementary materials for this article are available online.},
  archive      = {J_BIOSTAT},
  author       = {Zhang, Jun and Siegle, Greg J and Sun, Tao and D’andrea, Wendy and Krafty, Robert T},
  doi          = {10.1093/biostatistics/kxab018},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {227-243},
  shortjournal = {Biostatistics},
  title        = {Interpretable principal component analysis for multilevel multivariate functional data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian nonparametric model for classification of
longitudinal profiles. <em>BIOSTAT</em>, <em>24</em>(1), 209–225. (<a
href="https://doi.org/10.1093/biostatistics/kxab026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across several medical fields, developing an approach for disease classification is an important challenge. The usual procedure is to fit a model for the longitudinal response in the healthy population, a different model for the longitudinal response in the diseased population, and then apply Bayes’ theorem to obtain disease probabilities given the responses. Unfortunately, when substantial heterogeneity exists within each population, this type of Bayes classification may perform poorly. In this article, we develop a new approach by fitting a Bayesian nonparametric model for the joint outcome of disease status and longitudinal response, and then we perform classification through the clustering induced by the Dirichlet process. This approach is highly flexible and allows for multiple subpopulations of healthy, diseased, and possibly mixed membership. In addition, we introduce an Markov chain Monte Carlo sampling scheme that facilitates the assessment of the inference and prediction capabilities of our model. Finally, we demonstrate the method by predicting pregnancy outcomes using longitudinal profiles on the human chorionic gonadotropin beta subunit hormone levels in a sample of Chilean women being treated with assisted reproductive therapy.},
  archive      = {J_BIOSTAT},
  author       = {Gaskins, Jeremy T and Fuentes, Claudio and De La Cruz, Rolando},
  doi          = {10.1093/biostatistics/kxab026},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {209-225},
  shortjournal = {Biostatistics},
  title        = {A bayesian nonparametric model for classification of longitudinal profiles},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smaller p-values in genomics studies using distilled
auxiliary information. <em>BIOSTAT</em>, <em>24</em>(1), 193–208. (<a
href="https://doi.org/10.1093/biostatistics/kxaa053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical research institutions have generated massive amounts of biological data by genetically profiling hundreds of cancer cell lines. In parallel, academic biology labs have conducted genetic screens on small numbers of cancer cell lines under custom experimental conditions. In order to share information between these two approaches to scientific discovery, this article proposes a ”frequentist assisted by Bayes” (FAB) procedure for hypothesis testing that allows auxiliary information from massive genomics datasets to increase the power of hypothesis tests in specialized studies. The exchange of information takes place through a novel probability model for multimodal genomics data, which distills auxiliary information pertaining to cancer cell lines and genes across a wide variety of experimental contexts. If the relevance of the auxiliary information to a given study is high, then the resulting FAB tests can be more powerful than the corresponding classical tests. If the relevance is low, then the FAB tests yield as many discoveries as the classical tests. Simulations and practical investigations demonstrate that the FAB testing procedure can increase the number of effects discovered in genomics studies while still maintaining strict control of type I error and false discovery rate.},
  archive      = {J_BIOSTAT},
  author       = {Bryan, Jordan G and Hoff, Peter D},
  doi          = {10.1093/biostatistics/kxaa053},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {193-208},
  shortjournal = {Biostatistics},
  title        = {Smaller p-values in genomics studies using distilled auxiliary information},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast approximate inference for multivariate longitudinal
data. <em>BIOSTAT</em>, <em>24</em>(1), 177–192. (<a
href="https://doi.org/10.1093/biostatistics/kxab021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting information on multiple longitudinal outcomes is increasingly common in many clinical settings. In many cases, it is desirable to model these outcomes jointly. However, in large data sets, with many outcomes, computational burden often prevents the simultaneous modeling of multiple outcomes within a single model. We develop a mean field variational Bayes algorithm, to jointly model multiple Gaussian, Poisson, or binary longitudinal markers within a multivariate generalized linear mixed model. Through simulation studies and clinical applications (in the fields of sight threatening diabetic retinopathy and primary biliary cirrhosis), we demonstrate substantial computational savings of our approximate approach when compared to a standard Markov Chain Monte Carlo, while maintaining good levels of accuracy of model parameters.},
  archive      = {J_BIOSTAT},
  author       = {Hughes, David M and García-Fiñana, Marta and Wand, Matt P},
  doi          = {10.1093/biostatistics/kxab021},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {177-192},
  shortjournal = {Biostatistics},
  title        = {Fast approximate inference for multivariate longitudinal data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accounting for technical noise in bayesian graphical models
of single-cell RNA-sequencing data. <em>BIOSTAT</em>, <em>24</em>(1),
161–176. (<a
href="https://doi.org/10.1093/biostatistics/kxab011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-cell RNA-sequencing (scRNAseq) data contain a high level of noise, especially in the form of zero-inflation, that is, the presence of an excessively large number of zeros. This is largely due to dropout events and amplification biases that occur in the preparation stage of single-cell experiments. Recent scRNAseq experiments have been augmented with unique molecular identifiers (UMI) and External RNA Control Consortium (ERCC) molecules which can be used to account for zero-inflation. However, most of the current methods on graphical models are developed under the assumption of the multivariate Gaussian distribution or its variants, and thus they are not able to adequately account for an excessively large number of zeros in scRNAseq data. In this article, we propose a single-cell latent graphical model (scLGM)—a Bayesian hierarchical model for estimating the conditional dependency network among genes using scRNAseq data. Taking advantage of UMI and ERCC data, scLGM explicitly models the two sources of zero-inflation. Our simulation study and real data analysis demonstrate that the proposed approach outperforms several existing methods.},
  archive      = {J_BIOSTAT},
  author       = {Oh, Jihwan and Chang, Changgee and Long, Qi},
  doi          = {10.1093/biostatistics/kxab011},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {161-176},
  shortjournal = {Biostatistics},
  title        = {Accounting for technical noise in bayesian graphical models of single-cell RNA-sequencing data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shifting-corrected regularized regression for 1H NMR
metabolomics identification and quantification. <em>BIOSTAT</em>,
<em>24</em>(1), 140–160. (<a
href="https://doi.org/10.1093/biostatistics/kxac015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of identifying and quantifying metabolites in complex mixtures plays a critical role in metabolomics studies to obtain an informative interpretation of underlying biological processes. Manual approaches are time-consuming and heavily reliant on the knowledge and assessment of nuclear magnetic resonance (NMR) experts. We propose a shifting-corrected regularized regression method, which identifies and quantifies metabolites in a mixture automatically. A detailed algorithm is also proposed to implement the proposed method. Using a novel weight function, the proposed method is able to detect and correct peak shifting errors caused by fluctuations in experimental procedures. Simulation studies show that the proposed method performs better with regard to the identification and quantification of metabolites in a complex mixture. We also demonstrate real data applications of our method using experimental and biological NMR mixtures.},
  archive      = {J_BIOSTAT},
  author       = {Vu, Thao and Xu, Yuhang and Qiu, Yumou and Powers, Robert},
  doi          = {10.1093/biostatistics/kxac015},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {140-160},
  shortjournal = {Biostatistics},
  title        = {Shifting-corrected regularized regression for 1H NMR metabolomics identification and quantification},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian integrative analysis and prediction with
application to atherosclerosis cardiovascular disease. <em>BIOSTAT</em>,
<em>24</em>(1), 124–139. (<a
href="https://doi.org/10.1093/biostatistics/kxab016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of associating data from multiple sources and predicting an outcome simultaneously is an important one in modern biomedical research. It has potential to identify multidimensional array of variables predictive of a clinical outcome and to enhance our understanding of the pathobiology of complex diseases. Incorporating functional knowledge in association and prediction models can reveal pathways contributing to disease risk. We propose Bayesian hierarchical integrative analysis models that associate multiple omics data, predict a clinical outcome, allow for prior functional information, and can accommodate clinical covariates. The models, motivated by available data and the need for exploring other risk factors of atherosclerotic cardiovascular disease (ASCVD), are used for integrative analysis of clinical, demographic, and genomics data to identify genetic variants, genes, and gene pathways likely contributing to 10-year ASCVD risk in healthy adults. Our findings revealed several genetic variants, genes, and gene pathways that are highly associated with ASCVD risk, with some already implicated in cardiovascular disease (CVD) risk. Extensive simulations demonstrate the merit of joint association and prediction models over two-stage methods: association followed by prediction.},
  archive      = {J_BIOSTAT},
  author       = {Chekouo, Thierry and Safo, Sandra E},
  doi          = {10.1093/biostatistics/kxab016},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {124-139},
  shortjournal = {Biostatistics},
  title        = {Bayesian integrative analysis and prediction with application to atherosclerosis cardiovascular disease},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint frailty modeling of time-to-event data to elicit the
evolution pathway of events: A generalized linear mixed model approach.
<em>BIOSTAT</em>, <em>24</em>(1), 108–123. (<a
href="https://doi.org/10.1093/biostatistics/kxab037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimorbidity constitutes a serious challenge on the healthcare systems in the world, due to its association with poorer health-related outcomes, more complex clinical management, increases in health service utilization and costs, but a decrease in productivity. However, to date, most evidence on multimorbidity is derived from cross-sectional studies that have limited capacity to understand the pathway of multimorbid conditions. In this article, we present an innovative perspective on analyzing longitudinal data within a statistical framework of survival analysis of time-to-event recurrent data. The proposed methodology is based on a joint frailty modeling approach with multivariate random effects to account for the heterogeneous risk of failure and the presence of informative censoring due to a terminal event. We develop a generalized linear mixed model method for the efficient estimation of parameters. We demonstrate the capacity of our approach using a real cancer registry data set on the multimorbidity of melanoma patients and document the relative performance of the proposed joint frailty model to the natural competitor of a standard frailty model via extensive simulation studies. Our new approach is timely to advance evidence-based knowledge to address increasingly complex needs related to multimorbidity and develop interventions that are most effective and viable to better help a large number of individuals with multiple conditions.},
  archive      = {J_BIOSTAT},
  author       = {Ng, Shu Kay and Tawiah, Richard and Mclachlan, Geoffrey J and Gopalan, Vinod},
  doi          = {10.1093/biostatistics/kxab037},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {108-123},
  shortjournal = {Biostatistics},
  title        = {Joint frailty modeling of time-to-event data to elicit the evolution pathway of events: A generalized linear mixed model approach},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tailored bayes: A risk modeling framework under unequal
misclassification costs. <em>BIOSTAT</em>, <em>24</em>(1), 85–107. (<a
href="https://doi.org/10.1093/biostatistics/kxab023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk prediction models are a crucial tool in healthcare. Risk prediction models with a binary outcome (i.e., binary classification models) are often constructed using methodology which assumes the costs of different classification errors are equal. In many healthcare applications, this assumption is not valid, and the differences between misclassification costs can be quite large. For instance, in a diagnostic setting, the cost of misdiagnosing a person with a life-threatening disease as healthy may be larger than the cost of misdiagnosing a healthy person as a patient. In this article, we present Tailored Bayes (TB), a novel Bayesian inference framework which “tailors” model fitting to optimize predictive performance with respect to unbalanced misclassification costs. We use simulation studies to showcase when TB is expected to outperform standard Bayesian methods in the context of logistic regression. We then apply TB to three real-world applications, a cardiac surgery, a breast cancer prognostication task, and a breast cancer tumor classification task and demonstrate the improvement in predictive performance over standard methods.},
  archive      = {J_BIOSTAT},
  author       = {Karapanagiotis, Solon and Benedetto, Umberto and Mukherjee, Sach and Kirk, Paul D W and Newcombe, Paul J},
  doi          = {10.1093/biostatistics/kxab023},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {85-107},
  shortjournal = {Biostatistics},
  title        = {Tailored bayes: A risk modeling framework under unequal misclassification costs},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse negative binomial mixture model for clustering
RNA-seq count data. <em>BIOSTAT</em>, <em>24</em>(1), 68–84. (<a
href="https://doi.org/10.1093/biostatistics/kxab025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering with variable selection is a challenging yet critical task for modern small-n-large-p data. Existing methods based on sparse Gaussian mixture models or sparse |$K$| -means provide solutions to continuous data. With the prevalence of RNA-seq technology and lack of count data modeling for clustering, the current practice is to normalize count expression data into continuous measures and apply existing models with a Gaussian assumption. In this article, we develop a negative binomial mixture model with lasso or fused lasso gene regularization to cluster samples (small |$n$|⁠ ) with high-dimensional gene features (large |$p$|⁠ ). A modified EM algorithm and Bayesian information criterion are used for inference and determining tuning parameters. The method is compared with existing methods using extensive simulations and two real transcriptomic applications in rat brain and breast cancer studies. The result shows the superior performance of the proposed count data model in clustering accuracy, feature selection, and biological interpretation in pathways.},
  archive      = {J_BIOSTAT},
  author       = {Li, Yujia and Rahman, Tanbin and Ma, Tianzhou and Tang, Lu and Tseng, George C},
  doi          = {10.1093/biostatistics/kxab025},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {68-84},
  shortjournal = {Biostatistics},
  title        = {A sparse negative binomial mixture model for clustering RNA-seq count data},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-index models with functional connectivity network
predictors. <em>BIOSTAT</em>, <em>24</em>(1), 52–67. (<a
href="https://doi.org/10.1093/biostatistics/kxab015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity is defined as the undirected association between two or more functional magnetic resonance imaging (fMRI) time series. Increasingly, subject-level functional connectivity data have been used to predict and classify clinical outcomes and subject attributes. We propose a single-index model wherein response variables and sparse functional connectivity network valued predictors are linked by an unspecified smooth function in order to accommodate potentially nonlinear relationships. We exploit the network structure of functional connectivity by imposing meaningful sparsity constraints, which lead not only to the identification of association of interactions between regions with the response but also the assessment of whether or not the functional connectivity associated with a brain region is related to the response variable. We demonstrate the effectiveness of the proposed model in simulation studies and in an application to a resting-state fMRI data set from the Human Connectome Project to model fluid intelligence and sex and to identify predictive links between brain regions.},
  archive      = {J_BIOSTAT},
  author       = {Weaver, Caleb and Xiao, Luo and Lindquist, Martin A},
  doi          = {10.1093/biostatistics/kxab015},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {52-67},
  shortjournal = {Biostatistics},
  title        = {Single-index models with functional connectivity network predictors},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric regression analysis of bivariate censored
events in a family study of alzheimer’s disease. <em>BIOSTAT</em>,
<em>24</em>(1), 32–51. (<a
href="https://doi.org/10.1093/biostatistics/kxab014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing disease comorbidity patterns in families represents the first step in gene mapping for diseases and is central to the practice of precision medicine. One way to evaluate the relative contributions of genetic risk factor and environmental determinants of a complex trait (e.g., Alzheimer’s disease [AD]) and its comorbidities (e.g., cardiovascular diseases [CVD]) is through familial studies, where an initial cohort of subjects are recruited, genotyped for specific loci, and interviewed to provide extensive disease history in family members. Because of the retrospective nature of obtaining disease phenotypes in family members, the exact time of disease onset may not be available such that current status data or interval-censored data are observed. All existing methods for analyzing these family study data assume single event subject to right-censoring so are not applicable. In this article, we propose a semiparametric regression model for the family history data that assumes a family-specific random effect and individual random effects to account for the dependence due to shared environmental exposures and unobserved genetic relatedness, respectively. To incorporate multiple events, we jointly model the onset of the primary disease of interest and a secondary disease outcome that is subject to interval-censoring. We propose nonparametric maximum likelihood estimation and develop a stable Expectation-Maximization (EM) algorithm for computation. We establish the asymptotic properties of the resulting estimators and examine the performance of the proposed methods through simulation studies. Our application to a real world study reveals that the main contribution of comorbidity between AD and CVD is due to genetic factors instead of environmental factors.},
  archive      = {J_BIOSTAT},
  author       = {Gao, Fei and Zeng, Donglin and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxab014},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {32-51},
  shortjournal = {Biostatistics},
  title        = {Semiparametric regression analysis of bivariate censored events in a family study of alzheimer’s disease},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian multivariate probability of success using
historical data with type i error rate control. <em>BIOSTAT</em>,
<em>24</em>(1), 17–31. (<a
href="https://doi.org/10.1093/biostatistics/kxab050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, it is common to have multiple clinical outcomes (e.g., coprimary endpoints or a primary and multiple secondary endpoints). It is often desirable to establish efficacy in at least one of multiple clinical outcomes, which leads to a multiplicity problem. In the frequentist paradigm, the most popular methods to correct for multiplicity are typically conservative. Moreover, despite guidance from regulators, it is difficult to determine the sample size of a future study with multiple clinical outcomes. In this article, we introduce a Bayesian methodology for multiple testing that asymptotically guarantees type I error control. Using a seemingly unrelated regression model, correlations between outcomes are specifically modeled, which enables inference on the joint posterior distribution of the treatment effects. Simulation results suggest that the proposed Bayesian approach is more powerful than the method of Holm (1979) , which is commonly utilized in practice as a more powerful alternative to the ubiquitous Bonferroni correction. We further develop multivariate probability of success, a Bayesian method to robustly determine sample size in the presence of multiple outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Alt, Ethan M and Psioda, Matthew A and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxab050},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {17-31},
  shortjournal = {Biostatistics},
  title        = {Bayesian multivariate probability of success using historical data with type i error rate control},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing discrete latent structures: Choose LDs over PCs.
<em>BIOSTAT</em>, <em>24</em>(1), 1–16. (<a
href="https://doi.org/10.1093/biostatistics/kxab030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional biological data collection across heterogeneous groups of samples has become increasingly common, creating high demand for dimensionality reduction techniques that capture underlying structure of the data. Discovering low-dimensional embeddings that describe the separation of any underlying discrete latent structure in data is an important motivation for applying these techniques since these latent classes can represent important sources of unwanted variability, such as batch effects, or interesting sources of signal such as unknown cell types. The features that define this discrete latent structure are often hard to identify in high-dimensional data. Principal component analysis (PCA) is one of the most widely used methods as an unsupervised step for dimensionality reduction. This reduction technique finds linear transformations of the data which explain total variance. When the goal is detecting discrete structure, PCA is applied with the assumption that classes will be separated in directions of maximum variance. However, PCA will fail to accurately find discrete latent structure if this assumption does not hold. Visualization techniques, such as t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP), attempt to mitigate these problems with PCA by creating a low-dimensional space where similar objects are modeled by nearby points in the low-dimensional embedding and dissimilar objects are modeled by distant points with high probability. However, since t-SNE and UMAP are computationally expensive, often a PCA reduction is done before applying them which makes it sensitive to PCAs downfalls. Also, tSNE is limited to only two or three dimensions as a visualization tool, which may not be adequate for retaining discriminatory information. The linear transformations of PCA are preferable to non-linear transformations provided by methods like t-SNE and UMAP for interpretable feature weights. Here, we propose iterative discriminant analysis (iDA), a dimensionality reduction technique designed to mitigate these limitations. iDA produces an embedding that carries discriminatory information which optimally separates latent clusters using linear transformations that permit post hoc analysis to determine features that define these latent structures.},
  archive      = {J_BIOSTAT},
  author       = {Alexander, Theresa A and Irizarry, Rafael A and Bravo, Héctor Corrada},
  doi          = {10.1093/biostatistics/kxab030},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Biostatistics},
  title        = {Capturing discrete latent structures: Choose LDs over PCs},
  volume       = {24},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
