<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JAIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair---94">JAIR - 94</h2>
<ul>
<li><details>
<summary>
(2023). Exploiting cultural biases via homoglyphs in text-to-image
synthesis. <em>JAIR</em>, <em>78</em>, 1017–1068. (<a
href="https://doi.org/10.1613/jair.1.15388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models for text-to-image synthesis, such as DALL-E 2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in the textual description, common models reflect cultural biases in their generated images. We analyze this behavior both qualitatively and quantitatively and identify a model’s text encoder as the root cause of the phenomenon. Such behavior can be interpreted as a model feature, offering users a simple way to customize the image generation and reflect their own cultural background. Yet, malicious users or service providers may also try to intentionally bias the image generation. One goal might be to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.},
  archive      = {J_JAIR},
  author       = {Lukas Struppek and Dom Hintersdorf and Felix Friedrich and Manuel br and Patrick Schramowski and Kristian Kersting},
  doi          = {10.1613/jair.1.15388},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1017-1068},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Exploiting cultural biases via homoglyphs in text-to-image synthesis},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general model for aggregating annotations across simple,
complex, and multi-object annotation tasks. <em>JAIR</em>, <em>78</em>,
901–973. (<a href="https://doi.org/10.1613/jair.1.14388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human annotations are vital to supervised learning, yet annotators often disagree on the correct label, especially as annotation tasks increase in complexity. A common strategy to improve label quality is to ask multiple annotators to label the same item and then aggregate their labels. To date, many aggregation models have been proposed for simple categorical or numerical annotation tasks, but far less work has considered more complex annotation tasks, such as those involving open-ended, multivariate, or structured responses. Similarly, while a variety of bespoke models have been proposed for specific tasks, our work is the first we are aware of to introduce aggregation methods that generalize across many, diverse complex tasks, including sequence labeling, translation, syntactic parsing, ranking, bounding boxes, and keypoints. This generality is achieved by applying readily available task-specific distance functions, then devising a task-agnostic method to model these distances between labels, rather than the labels themselves. This article presents a unified treatment of our prior work on complex annotation modeling and extends that work with investigation of three new research questions. First, how do complex annotation task and dataset properties impact aggregation accuracy? Second, how should a task owner navigate the many modeling choices in order to maximize aggregation accuracy? Finally, what tests and diagnoses can verify that aggregation models are specified correctly for the given data? To understand how various factors impact accuracy and to inform model selection, we conduct large-scale simulation studies and broad experiments on real, complex datasets. Regarding testing, we introduce the concept of unit tests for aggregation models and present a suite of such tests to ensure that a given model is not mis-specified and exhibits expected behavior. Beyond investigating these research questions above, we discuss the foundational concept and nature of annotation complexity, present a new aggregation model as a conceptual bridge between traditional models and our own, and contribute a new general semisupervised learning method for complex label aggregation that outperforms prior work.},
  archive      = {J_JAIR},
  author       = {Alexander Braylan and Madalyn Marabella and Omar Alonso and Matthew Lease},
  doi          = {10.1613/jair.1.14388},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {901-973},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A general model for aggregating annotations across simple, complex, and multi-object annotation tasks},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On expected value strong controllability. <em>JAIR</em>,
<em>78</em>, 849–900. (<a
href="https://doi.org/10.1613/jair.1.14561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Probabilistic Simple Temporal Network with Uncertainty (PSTNU) is a variant of the Simple Temporal Network with Uncertainty (STNU) in which known probability distributions govern the timing of uncontrollable timepoints. Previous approaches to solving PSTNUs focus mininizing risk, that is, the probability of violating constraints. These approaches are not applicable in over-constrained controllability problems, when it is certain that all constraints can’t be satisfied. We introduce the Weighted Probabilistic Simple Temporal Network with Uncertainty (WPSTNU), which extends the PSTNU by attaching a fixed value to the satisfaction of temporal constraints, and allows the schedule to violate some constraints in order to maximize the expected value of satisfying others. We study the problem of Expected Value Strong Controllability (EvSC) of WPSTNUs, which seeks a fixed-time schedule maximizing the expected value of satisfied constraints. We solve the EvSC problem using a mixed integer linear program (MILP) that bounds below the probability of satisfying constraints involving uncontrollable timepoints. While solving MILPs generally takes exponential time, we demonstrate our formulation’s effective performance using scheduling problems derived from the HEATlab and MIT ROVERS data sets. We then show how to use this MILP to reschedule during execution, after time has passed and uncertainty is reduced. We describe different fixed-period rescheduling approaches, including time-based and event-based, and report on the most successful strategies compared to the expected value of the fixed schedule produced by the MILP. All of our methods are evaluated on problems with both symmetric and asymmetric (skewed) probability distributions. We show that periodically rescheduling improves the expected value when compared to the fixed schedule, and describe how the benchmark and skewness impact the schedule value improvement. The resulting analysis shows that solving EvSC problems on WPSTNUs is a viable alternative to solving over-constrained controllability problems.},
  archive      = {J_JAIR},
  author       = {Niklas T. Lauffer and William B. Lassiter and Jeremy D. Frank},
  doi          = {10.1613/jair.1.14561},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {849-900},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On expected value strong controllability},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphmax for text generation. <em>JAIR</em>, <em>78</em>,
823–848. (<a href="https://doi.org/10.1613/jair.1.15280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information, we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plugged into any large pre-trained LM for text generation and machine translation. Through extensive experiments, we demonstrate that the new GTV-based regularization can improve performances in various natural language processing (NLP) tasks in comparison with existing methods. Moreover, through human experiments, we observe that participants can easily distinguish the text generated by graphmax or softmax.},
  archive      = {J_JAIR},
  author       = {Bin Liu and Guosheng Yin},
  doi          = {10.1613/jair.1.15280},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {823-848},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Graphmax for text generation},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of learning criteria going beyond the usual risk.
<em>JAIR</em>, <em>78</em>, 781–821. (<a
href="https://doi.org/10.1613/jair.1.15000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtually all machine learning tasks are characterized using some form of loss function, and “good performance” is typically stated in terms of a sufficiently small average loss, taken over the random draw of test data. While optimizing for performance on average is intuitive, convenient to analyze in theory, and easy to implement in practice, such a choice brings about trade-offs. In this work, we survey and introduce a wide variety of non-traditional criteria used to design and evaluate machine learning algorithms, place the classical paradigm within the proper historical context, and propose a view of learning problems which emphasizes the question of “what makes for a desirable loss distribution?” in place of tacit use of the expected loss.},
  archive      = {J_JAIR},
  author       = {Matthew J. Holland and Kazuki Tanabe},
  doi          = {10.1613/jair.1.15000},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {781-821},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey of learning criteria going beyond the usual risk},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing group fairness in machine learning via
utilities. <em>JAIR</em>, <em>78</em>, 747–780. (<a
href="https://doi.org/10.1613/jair.1.14238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group fairness definitions such as Demographic Parity and Equal Opportunity make assumptions about the underlying decision-problem that restrict them to classification problems. Prior work has translated these definitions to other machine learning environments, such as unsupervised learning and reinforcement learning, by implementing their closest mathematical equivalent. As a result, there are numerous bespoke interpretations of these definitions. This work aims to unify the shared aspects of each of these bespoke definitions, and to this end we provide a group fairness framework that generalizes beyond just classification problems. We leverage two fairness principles that enable this generalization. First, our framework measures outcomes in terms of utilities, rather than predictions, and does so for both the decision-maker and the individual. Second, our framework can consider counterfactual outcomes, rather than just observed outcomes, thus preventing loopholes where fairness criteria are satisfied through self-fulfilling prophecies. We provide concrete examples of how our utility fairness framework avoids these assumptions and thus naturally integrates with classification, clustering, and reinforcement learning fairness problems. We also show that many of the bespoke interpretations of Demographic Parity and Equal Opportunity fit nicely as special cases of our framework.},
  archive      = {J_JAIR},
  author       = {Jack Blandin and Ian A. Kash},
  doi          = {10.1613/jair.1.14238},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {747-780},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Generalizing group fairness in machine learning via utilities},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently explaining CSPs with unsatisfiable subset
optimization. <em>JAIR</em>, <em>78</em>, 709–746. (<a
href="https://doi.org/10.1613/jair.1.14260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build on a recently proposed method for stepwise explaining the solutions to Constraint Satisfaction Problems (CSPs) in a human understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified by a cost function. Explanation generation algorithms rely on extracting Minimal Unsatisfiable Subsets (MUSs) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not guarantee subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and address the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). To this end, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for reusing relevant information across multiple algorithm calls; and (3) methods for exploiting domain-specific information to speed up the generation of explanation sequences. We have experimentally validated our algorithms on a large number of CSP problems. We found that our algorithms outperform the MUS approach in terms of explanation quality and computational time (on average up to 56\% faster than a standard MUS approach).},
  archive      = {J_JAIR},
  author       = {Emilio Gamba and Bart Bogaerts and Tias Guns},
  doi          = {10.1613/jair.1.14260},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {709-746},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficiently explaining CSPs with unsatisfiable subset optimization},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the parallel parameterized complexity of MaxSAT variants.
<em>JAIR</em>, <em>78</em>, 673–707. (<a
href="https://doi.org/10.1613/jair.1.14748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the maximum satisfiability problem (max-sat) we are given a propositional formula in conjunctive normal form and have to find an assignment that satisfies as many clauses as possible. We study the parallel parameterized complexity of various versions of max-sat and provide the first constant-time algorithms parameterized either by the solution size or by the allowed excess relative to some guarantee. For the dual parameterized version where the parameter is the number of clauses we are allowed to leave unsatisfied, we present the first parallel algorithm for max-2sat (known as almost-2sat). The difficulty in solving almost-2sat in parallel comes from the fact that the iterative compression method, originally developed to prove that the problem is fixed-parameter tractable at all, is inherently sequential. We observe that a graph flow whose value is a parameter can be computed in parallel and develop a parallel algorithm for the vertex cover problem parameterized above the size of a given matching. Finally, we study the parallel complexity of max-sat parameterized by the vertex cover number, the treedepth, the feedback vertex set number, and the treewidth of the input’s incidence graph. While max-sat is fixedparameter tractable for all of these parameters, we show that they allow different degrees of possible parallelization. For all four we develop dedicated parallel algorithms that are constructive, meaning that they output an optimal assignment – in contrast to results that can be obtained by parallel meta-theorems, which often only solve the decision version.},
  archive      = {J_JAIR},
  author       = {Max Bannach and Malte Skambath and Till Tantau},
  doi          = {10.1613/jair.1.14748},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {673–707},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the parallel parameterized complexity of MaxSAT variants},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Actor prioritized experience replay. <em>JAIR</em>,
<em>78</em>, 639–672. (<a
href="https://doi.org/10.1613/jair.1.14819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A widely-studied deep reinforcement learning (RL) technique known as Prioritized Experience Replay (PER) allows agents to learn from transitions sampled with non-uniform probability proportional to their temporal-difference (TD) error. Although it has been shown that PER is one of the most crucial components for the overall performance of deep RL methods in discrete action domains, many empirical studies indicate that it considerably underperforms off-policy actor-critic algorithms. We theoretically show that actor networks cannot be effectively trained with transitions that have large TD errors. As a result, the approximate policy gradient computed under the Q-network diverges from the actual gradient computed under the optimal Q-function. Motivated by this, we introduce a novel experience replay sampling framework for actor-critic methods, which also regards issues with stability and recent findings behind the poor empirical performance of PER. The introduced algorithm suggests a new branch of improvements to PER and schedules effective and efficient training for both actor and critic networks. An extensive set of experiments verifies our theoretical findings, showing that our method outperforms competing approaches and achieves state-of-the-art results over the standard off-policy actor-critic algorithms.},
  archive      = {J_JAIR},
  author       = {Baturay Saglam and Furkan B. Mutlu and Dogan C. Cicek and Suleyman S. Kozat},
  doi          = {10.1613/jair.1.14819},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {639-672},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Actor prioritized experience replay},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximisation of admissible multi-objective heuristics.
<em>JAIR</em>, <em>78</em>, 619–639. (<a
href="https://doi.org/10.1613/jair.1.14861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-objective (MO) heuristic search, solution costs, as well as heuristic values, are sets of multi-dimensional cost vectors, representing possible non-dominated trade-offs between objectives. The maximum of two or more such vector sets, which is an important operation in creating informative admissible MO heuristics, can be defined in several ways: Geißer et al. recently proposed two MO maximum operators, the component-wise maximum (comax) and the anti-dominance maximum (admax), which represent different trade-offs between informativeness and computational cost. We show that the anti-dominance maximum is not admissibility-preserving, and propose an alternative, the “select one” maximum (somax). We also show that the comax operator is the greatest admissibility-preserving MO maximum, and briefly investigate its efficient implementation. The conclusion of our experimental results is that somax achieves a trade-off similar to that intended with admax – cheaper to compute but less informed – also when compared to an improved comax implementation.},
  archive      = {J_JAIR},
  author       = {Patrik Haslum and Ryan Xiao Wang},
  doi          = {10.1613/jair.1.14861},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {619-639},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Maximisation of admissible multi-objective heuristics},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable neural-probabilistic answer set programming.
<em>JAIR</em>, <em>78</em>, 579–617. (<a
href="https://doi.org/10.1613/jair.1.15027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of combining the robustness of neural networks and the expressiveness of symbolic methods has rekindled the interest in Neuro-Symbolic AI. Deep Probabilistic Programming Languages (DPPLs) have been developed for probabilistic logic programming to be carried out via the probability estimations of deep neural networks (DNNs). However, recent SOTA DPPL approaches allow only for limited conditional probabilistic queries and do not offer the power of true joint probability estimation. In our work, we propose an easy integration of tractable probabilistic inference within a DPPL. To this end, we introduce SLASH, a novel DPPL that consists of Neural-Probabilistic Predicates (NPPs) and a logic program, united via answer set programming (ASP). NPPs are a novel design principle allowing for combining all deep model types and combinations thereof to be represented as a single probabilistic predicate. In this context, we introduce a novel +/− notation for answering various types of probabilistic queries by adjusting the atom notations of a predicate. To scale well, we show how to prune the stochastically insignificant parts of the (ground) program, speeding up reasoning without sacrificing the predictive performance. We evaluate SLASH on various tasks, including the benchmark task of MNIST addition and Visual Question Answering (VQA).},
  archive      = {J_JAIR},
  author       = {Arseny Skryagin and Daniel Ochs and Devendra Singh Dhami and Kristian Kersting},
  doi          = {10.1613/jair.1.15027},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {579-617},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Scalable neural-probabilistic answer set programming},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maintenance of plan libraries for case-based planning:
Offline and online policies. <em>JAIR</em>, <em>78</em>, 527–577. (<a
href="https://doi.org/10.1613/jair.1.14797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-based planning is an approach to planning where previous planning experience provides guidance to solving new problems. Such a guidance can be extremely useful, or even necessary, when the new problem is very hard to solve, or the stored previous experience is highly valuable, because, e.g., it was provided or validated by human experts, and the system should try to reuse it as much as possible. To do so, a case-based planning system stores in a library previous planning experience in the form of already encountered problems and their solutions. The quality of such a plan library critically influences the performance of the planner, and therefore it needs to be carefully designed and created. For this reason, it is also important to update the library during the lifetime of the system, as the type of problems being addressed may evolve or differ from the ones the library was originally designed for. Moreover, like in general case-based reasoning, the library needs to be maintained at a manageable size, otherwise the computational cost of querying it grows excessively, making the entire approach ineffective. In this paper, we formally define the problem of maintaining a library of cases, discuss which criteria should drive the maintenance, study the computational complexity of the maintenance problem, and propose offline techniques to reduce an oversized library that optimize different criteria. Moreover, we introduce a complementary online approach that attempts to limit the growth of the library, and we consider the combination of offline and online techniques to ensure the best performance of the case-based planner. Finally, we experimentally show the practical effectiveness of the offline and online methods for reducing the library.},
  archive      = {J_JAIR},
  author       = {Alfonso Emilio Gerevini and Alessandro Saetti and Ivan Serina and Andrea Loreggia and Luca Putelli and Anna Roubickova},
  doi          = {10.1613/jair.1.14797},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {527-577},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Maintenance of plan libraries for case-based planning: Offline and online policies},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotics of k-fold cross validation. <em>JAIR</em>,
<em>78</em>, 491–526. (<a
href="https://doi.org/10.1613/jair.1.13974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the asymptotic distribution of the K-fold cross validation error in an i.i.d. setting. As the number of observations n goes to infinity while keeping the number of folds K fixed, the K-fold cross validation error is √ n-consistent for the expected out-of-sample error and has an asymptotically normal distribution. A consistent estimate of the asymptotic variance is derived and used to construct asymptotically valid confidence intervals for the expected out-of-sample error. A hypothesis test is developed for comparing two estimators’ expected out-of-sample errors and a subsampling procedure is used to obtain critical values. Monte Carlo simulations demonstrate the asymptotic validity of our confidence intervals for the expected out-of-sample error and investigate the size and power properties of our test. In our empirical application, we use our estimator selection test to compare the out-of-sample predictive performance of OLS, Neural Networks, and Random Forests for predicting the sale price of a domain name in a GoDaddy expiry auction.},
  archive      = {J_JAIR},
  author       = {Jessie Li},
  doi          = {10.1613/jair.1.13974},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {491-526},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Asymptotics of K-fold cross validation},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnosing AI explanation methods with folk concepts of
behavior. <em>JAIR</em>, <em>78</em>, 459–489. (<a
href="https://doi.org/10.1613/jair.1.14053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a formalism for the conditions of a successful explanation of AI. We consider “success” to depend not only on what information the explanation contains, but also on what information the human explainee understands from it. Theory of mind literature discusses the folk concepts that humans use to understand and generalize behavior. We posit that folk concepts of behavior provide us with a “language” that humans understand behavior with. We use these folk concepts as a framework of social attribution by the human explainee—the information constructs that humans are likely to comprehend from explanations—by introducing a blueprint for an explanatory narrative (Figure 1) that explains AI behavior with these constructs. We then demonstrate that many XAI methods today can be mapped to folk concepts of behavior in a qualitative evaluation. This allows us to uncover their failure modes that prevent current methods from explaining successfully—i.e., the information constructs that are missing for any given XAI method, and whose inclusion can decrease the likelihood of misunderstanding AI behavior.},
  archive      = {J_JAIR},
  author       = {Alon Jacovi and Jasmijn Bastings and Sebastian Gehrmann and Yoav Goldberg and Katja Filippova},
  doi          = {10.1613/jair.1.14053},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {459-489},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Diagnosing AI explanation methods with folk concepts of behavior},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to tell easy from hard: Complexities of conjunctive
query entailment in extensions of ALC. <em>JAIR</em>, <em>78</em>,
385–458. (<a href="https://doi.org/10.1613/jair.1.14482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is commonly known that the conjunctive query entailment problem for certain extensions of (the well-known ontology language) ALC is computationally harder than their knowledge base satisfiability problem while for others the complexities coincide, both under the standard and the finite-model semantics. We expose a uniform principle behind this divide by identifying a wide class of (finitely) locally-forward description logics, for which we prove that (finite) query entailment problem can be solved by a reduction to exponentially many calls of the (finite) knowledge base satisfiability problem. Consequently, our algorithm yields tight ExpTime upper bounds for locally-forward logics with ExpTime-complete knowledge base satisfiability problem, including logics between ALC and µALCHbregQ (and more), as well as ALCSCC with global cardinality constraints, for which the complexity of querying remained open. Moreover, to make our technique applicable in future research, we provide easy-to-check sufficient conditions for a logic to be locally-forward based several versions of the on model-theoretic notion of unravellings. Together with existing results, this provides a nearly complete classification of the “benign” vs. “malign” primitive modelling features extending ALC, missing out only the Self operator. We then show a rather counter-intuitive result, namely that the conjunctive entailment problem for ALCSelf is exponentially harder than for ALC. This places the seemingly innocuous Self operator among the “malign” modelling features, like inverses, transitivity or nominals.},
  archive      = {J_JAIR},
  author       = {Bartosz Bednarczyk and Sebastian Rudolph},
  doi          = {10.1613/jair.1.14482},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {385–458},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {How to tell easy from hard: Complexities of conjunctive query entailment in extensions of ALC},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-crossing anonymous MAPF for tethered robots.
<em>JAIR</em>, <em>78</em>, 357–384. (<a
href="https://doi.org/10.1613/jair.1.14351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the anonymous multi-agent path finding (MAPF) problem for a team of tethered robots. The goal is to find a set of non-crossing paths such that the makespan is minimal. A difficulty comes from the fact that a safety distance must be maintained between two robots when they pass through the same subpath, to avoid collisions and cable entanglements. Hence, robots must be synchronized and waiting times must be added when computing the makespan. We show that bounds can be efficiently computed by solving linear assignment problems. We introduce a variable neighborhood search method to improve upper bounds, and a Constraint Programming model to compute optimal solutions. We experimentally evaluate our approach on three different kinds of instances.},
  archive      = {J_JAIR},
  author       = {Xiao Peng and Olivier Simonin and Christine Solnon},
  doi          = {10.1613/jair.1.14351},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {357-384},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Non-crossing anonymous MAPF for tethered robots},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on deep graph representation learning
methods. <em>JAIR</em>, <em>78</em>, 287–356. (<a
href="https://doi.org/10.1613/jair.1.14768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a lot of activity in graph representation learning in recent years. Graph representation learning aims to produce graph representation vectors to represent the structure and characteristics of huge graphs precisely. This is crucial since the effectiveness of the graph representation vectors will influence how well they perform in subsequent tasks like anomaly detection, connection prediction, and node classification. Recently, there has been an increase in the use of other deep-learning breakthroughs for data-based graph problems. Graph-based learning environments have a taxonomy of approaches, and this study reviews all their learning settings. The learning problem is theoretically and empirically explored. This study briefly introduces and summarizes the Graph Neural Architecture Search (G-NAS), outlines several Graph Neural Networks’ drawbacks, and suggests some strategies to mitigate these challenges. Lastly, the study discusses several potential future study avenues yet to be explored.},
  archive      = {J_JAIR},
  author       = {Ijeoma Amuche Chikwendu and Xiaoling Zhang and Isaac Osei Agyemang and Isaac Adjei-Mensah and Ukwuoma Chiagoziem Chima and Chukwuebuka Joseph Ejiyi},
  doi          = {10.1613/jair.1.14768},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {287-356},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A comprehensive survey on deep graph representation learning methods},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Select and augment: Enhanced dense retrieval knowledge graph
augmentation. <em>JAIR</em>, <em>78</em>, 269–285. (<a
href="https://doi.org/10.1613/jair.1.14365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, this framework leverages a retriever model to selectively identify richer or highly relevant text descriptions to use in augmenting entities. Furthermore, the framework treats the number of descriptions to use in augmentation process as a parameter, which allows the flexibility of enumerating across several numbers before identifying an appropriate number. Experiment results for Link Prediction demonstrate a 5.5\% and 3.5\% percentage increase in the Mean Reciprocal Rank (MRR) and Hits@10 scores respectively, in comparison to text-enhanced knowledge graph augmentation methods using traditional CNNs.},
  archive      = {J_JAIR},
  author       = {Micheal Abaho and Yousef H. Alfaifi},
  doi          = {10.1613/jair.1.14365},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {269-285},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Select and augment: Enhanced dense retrieval knowledge graph augmentation},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedding ontologies in the description logic ALC by
axis-aligned cones. <em>JAIR</em>, <em>78</em>, 217–267. (<a
href="https://doi.org/10.1613/jair.1.13939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with knowledge graph embedding with background knowledge, taking the formal perspective of logics. In knowledge graph embedding, knowledge— expressed as a set of triples of the form (a R b) (“a is R-related to b”)—is embedded into a real-valued vector space. The embedding helps exploiting geometrical regularities of the space in order to tackle typical inductive tasks of machine learning such as link prediction. Recent embedding approaches also consider incorporating background knowledge, in which the intended meanings of the symbols a, R, b are further constrained via axioms of a theory. Of particular interest are theories expressed in a formal language with a neat semantics and a good balance between expressivity and feasibility. In that case, the knowledge graph together with the background can be considered to be an ontology. This paper develops a cone-based theory for embedding in order to advance the expressivity of the ontology: it works (at least) with ontologies expressed in the description logic ALC, which comprises restricted existential and universal quantifiers, as well as concept negation and concept disjunction. In order to align the classical Tarskian Style semantics for ALC with the sub-symbolic representation of triples, we use the notion of a geometric model of an ALC ontology and show, as one of our main results, that an ALC ontology is satisfiable in the classical sense iff it is satisfiable by a geometric model based on cones. The geometric model, if treated as a partial model, can even be chosen to be faithful, i.e., to reflect all and only the knowledge captured by the ontology. We introduce the class of axis-aligned cones and show that modulo simple geometric operations any distributive logic (such as ALC) interpreted over cones employs this class of cones. Cones are also attractive from a machine learning perspective on knowledge graph embeddings since they give rise to applying conic optimization techniques.},
  archive      = {J_JAIR},
  author       = {Özgür Lütfü Özcep and Mena Leemhuis and Diedrich Wolter},
  doi          = {10.1613/jair.1.13939},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {217-267},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Embedding ontologies in the description logic ALC by axis-aligned cones},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Amortized variational inference: A systematic review.
<em>JAIR</em>, <em>78</em>, 167–215. (<a
href="https://doi.org/10.1613/jair.1.14258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core principle of Variational Inference (VI) is to convert the statistical inference problem of computing complex posterior probability densities into a tractable optimization problem. This property enables VI to be faster than several sampling-based techniques. However, the traditional VI algorithm is not scalable to large data sets and is unable to readily infer out-of-bounds data points without re-running the optimization process. Recent developments in the field, like stochastic-, black box-, and amortized-VI, have helped address these issues. Generative modeling tasks nowadays widely make use of amortized VI for its efficiency and scalability, as it utilizes a parameterized function to learn the approximate posterior density parameters. In this paper, we review the mathematical foundations of various VI techniques to form the basis for understanding amortized VI. Additionally, we provide an overview of the recent trends that address several issues of amortized VI, such as the amortization gap, generalization issues, inconsistent representation learning, and posterior collapse. Finally, we analyze alternate divergence measures that improve VI optimization.},
  archive      = {J_JAIR},
  author       = {Ankush Ganguly and Sanjana Jain and Ukrit Watchareeruetai},
  doi          = {10.1613/jair.1.14258},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {167-215},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Amortized variational inference: A systematic review},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering what matters: Optimal approximation for
clustering with outliers. <em>JAIR</em>, <em>78</em>, 143–166. (<a
href="https://doi.org/10.1613/jair.1.14883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering with outliers is one of the most fundamental problems in Computer Science. Given a set X of n points and two numbers k, m, the clustering with outliers aims to exclude m points from X and partition the remaining points into k clusters that minimizes a certain cost function. In this paper, we give a general approach for solving clustering with outliers, which results in a fixed-parameter tractable (FPT) algorithm in k and m—i.e., an algorithm with running time of the form f(k, m) · nO(1) for some function f—that almost matches the approximation ratio for its outlier-free counterpart. As a corollary, we obtain FPT approximation algorithms with optimal approximation ratios for k-Median and k-Means with outliers in general and Euclidean metrics. We also exhibit more applications of our approach to other variants of the problem that impose additional constraints on the clustering, such as fairness or matroid constraints.},
  archive      = {J_JAIR},
  author       = {Akanksha Agrawal and Tanmay Inamdar and Saket Saurabh and Jie Xue},
  doi          = {10.1613/jair.1.14883},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {143-166},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Clustering what matters: Optimal approximation for clustering with outliers},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence-oriented diagnosis of discrete-event systems.
<em>JAIR</em>, <em>78</em>, 69–141. (<a
href="https://doi.org/10.1613/jair.1.14630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based diagnosis has always been conceived as set-oriented, meaning that a candidate is a set of faults, or faulty components, that explains a collection of observations. This perspective applies equally to both static and dynamical systems. Diagnosis of discrete-event systems (DESs) is no exception: a candidate is traditionally a set of faults, or faulty events, occurring in a trajectory of the DES that conforms with a given sequence of observations. As such, a candidate does not embed any temporal relationship among faults, nor does it account for multiple occurrences of the same fault. To improve diagnostic explanation and support decision making, a sequence-oriented perspective to diagnosis of DESs is presented, where a candidate is a sequence of faults occurring in a trajectory of the DES, called a fault sequence. Since a fault sequence is possibly unbounded, as the same fault may occur an unlimited number of times in the trajectory, the set of (output) candidates may be unbounded also, which contrasts with set-oriented diagnosis, where the set of candidates is bounded by the powerset of the domain of faults. Still, a possibly unbounded set of fault sequences is shown to be a regular language, which can be defined by a regular expression over the domain of faults, a property that makes sequence-oriented diagnosis feasible in practice. The task of monitoring-based diagnosis is considered, where a new candidate set is generated at the occurrence of each observation. The approach is based on three different techniques: .1/ blind diagnosis, with no compiled knowledge, .2/ greedy diagnosis, with total knowledge compilation, and .3/ lazy diagnosis, with partial knowledge compilation. By knowledge we mean a data structure slightly similar to a classical DES diagnoser, which can be generated (compiled) either entirely offline (greedy diagnosis) or incrementally online (lazy diagnosis). Experimental evidence suggests that, among these techniques, only lazy diagnosis may be viable in non-trivial application domains.},
  archive      = {J_JAIR},
  author       = {Gianfranco Lamperti and Stefano Trerotola and Marina Zanella and Xiangfu Zhao},
  doi          = {10.1613/jair.1.14630},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {69-141},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Sequence-oriented diagnosis of discrete-event systems},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A benchmark study on knowledge graphs enrichment and pruning
methods in the presence of noisy relationships. <em>JAIR</em>,
<em>78</em>, 37–68. (<a
href="https://doi.org/10.1613/jair.1.14494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, knowledge graphs (KGs), as a form of structured human intelligence, have attracted considerable research attention from academia and industry. In this very active field of study, a widely explored problem is that of link prediction, the task of predicting whether two nodes should be connected, based on node attributes and local or global graph connectivity properties. The state of the art in this area is represented by techniques based on graph embeddings. However, KGs, especially those acquired using automated or partly automated techniques, are often riddled with noise, e.g., wrong relationships, which makes the problem of link deletion as important as that of link prediction. In this paper, we address three main research questions. The first is about the true effectiveness of different knowledge graph embedding models under the presence of an increasing number of wrong links. The second is to asses if methods that can predict unknown relationships effectively, work equally well in recognizing incorrect relations. The third is to verify if there are systems robust enough to maintain primacy in all experimental conditions. To answer these research questions, we performed a systematic benchmark study in which the experimental setting includes ten state-of-the-art models, three common KG datasets with different structural properties and three downstream tasks: the widely explored tasks of link prediction and triple classification, and the less popular task of link deletion. Comparative studies often yield contradictory results, where the same systems score better or worse depending on the experimental context. In our work, in order to facilitate the discovery of clear performance patterns and their interpretation, we select and/or aggregate performance data to highlight each specific comparison dimension: dataset complexity, type of task, category of models, and robustness against noise.},
  archive      = {J_JAIR},
  author       = {Stefano Faralli and Andrea Lenzi and Paola Velardi},
  doi          = {10.1613/jair.1.14494},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {37-68},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A benchmark study on knowledge graphs enrichment and pruning methods in the presence of noisy relationships},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting functional constraints in automatic dominance
breaking for constraint optimization. <em>JAIR</em>, <em>78</em>, 1–35.
(<a href="https://doi.org/10.1613/jair.1.14714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominance breaking is a powerful technique in improving the solving efficiency of Constraint Optimization Problems (COPs) by removing provably suboptimal solutions with additional constraints. While dominance breaking is effective in a range of practical problems, it is usually problem specific and requires human insights into problem structures to come up with correct dominance breaking constraints. Recently, a framework is proposed to generate nogood constraints automatically for dominance breaking, which formulates nogood generation as solving auxiliary Constraint Satisfaction Problems (CSPs). However, the framework uses a pattern matching approach to synthesize the auxiliary generation CSPs from the specific forms of objectives and constraints in target COPs, and is only applicable to a limited class of COPs. This paper proposes a novel rewriting system to derive constraints for the auxiliary generation CSPs automatically from COPs with nested function calls, significantly generalizing the original framework. In particular, the rewriting system exploits functional constraints flattened from nested functions in a high-level modeling language. To generate more effective dominance breaking nogoods and derive more relaxed constraints in generation CSPs, we further characterize how to extend the system with rewriting rules exploiting function properties, such as monotonicity, commutativity, and associativity, for specific functional constraints. Experimentation shows significant runtime speedup using the dominance breaking nogoods generated by our proposed method. Studying patterns of generated nogoods also demonstrates that our proposal can reveal dominance relations in the literature and discover new dominance relations on problems with ineffective or no known dominance breaking constraints.},
  archive      = {J_JAIR},
  author       = {Jimmy H.M. Lee and Allen Z. Zhong},
  doi          = {10.1613/jair.1.14714},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-35},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Exploiting functional constraints in automatic dominance breaking for constraint optimization},
  volume       = {78},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of social dynamic agents and long-tailed learning
challenges: A survey. <em>JAIR</em>, <em>77</em>, 1697–1772. (<a
href="https://doi.org/10.1613/jair.1.14749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous robots that can perform common tasks like driving, surveillance, and chores have the biggest potential for impact due to frequency of usage, and the biggest potential for risk due to direct interaction with humans. These tasks take place in openended environments where humans socially interact and pursue their goals in complex and diverse ways. To operate in such environments, such systems must predict this behaviour, especially when the behavior is unexpected and potentially dangerous. Therefore, we summarize trends in various types of tasks, modeling methods, datasets, and social interaction modules aimed at predicting the future location of dynamic, socially interactive agents. Furthermore, we describe long-tailed learning techniques from classification and regression problems that can be applied to prediction problems. To our knowledge this is the first work that reviews social interaction modeling within prediction, and long-tailed learning techniques within regression and prediction.},
  archive      = {J_JAIR},
  author       = {Divya Thuremella and Lars Kunze},
  doi          = {10.1613/jair.1.14749},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1697-1772},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Prediction of social dynamic agents and long-tailed learning challenges: A survey},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatically finding the right probabilities in bayesian
networks. <em>JAIR</em>, <em>77</em>, 1637–1696. (<a
href="https://doi.org/10.1613/jair.1.14044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents alternative techniques for inference on classical Bayesian networks in which all probabilities are fixed, and for synthesis problems when conditional probability tables (CPTs) in such networks contain symbolic parameters rather than concrete probabilities. The key idea is to exploit probabilistic model checking as well as its recent extension to parameter synthesis techniques for parametric Markov chains. To enable this, the Bayesian networks are transformed into Markov chains and their objectives are mapped onto probabilistic temporal logic formulas. For exact inference, we compare probabilistic model checking to weighted model counting on various Bayesian network benchmarks. We contrast symbolic model checking using multi-terminal binary (aka: algebraic) decision diagrams to symbolic inference using proba- bilistic sentential decision diagrams, symbolic data structures that are tailored to Bayesian networks. For the parametric setting, we describe how our techniques can be used for various synthesis problems such as computing sensitivity functions (and values), simple and difference parameter tuning and ratio parameter tuning. Our parameter synthesis techniques are applicable to arbitrarily many, possibly dependent, parameters that may occur in multiple CPTs. This lifts restrictions, e.g., on the number of parametrized CPTs, or on parameter dependencies between several CPTs, that exist in the literature. Experiments on several benchmarks show that our parameter synthesis techniques can treat parameter synthesis for Bayesian networks (with hundreds of unknown parameters) that are out of reach for existing techniques.},
  archive      = {J_JAIR},
  author       = {Bahare Salmani and Joost-Pieter Katoen},
  doi          = {10.1613/jair.1.14044},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1637-1696},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Automatically finding the right probabilities in bayesian networks},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimality guarantees for particle belief approximation of
POMDPs. <em>JAIR</em>, <em>77</em>, 1591–1636. (<a
href="https://doi.org/10.1613/jair.1.14525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, this is implemented by using the particle filter belief transition model as the generative model for the MDP solver. While this requires access to the observation density model from the POMDP, it only increases the transition sampling complexity of the MDP solver by a factor of O(C), where C is the number of particles. Thus, when combined with sparse sampling MDP algorithms, this approach can yield algorithms for POMDPs that have no direct theoretical dependence on the size of the state and observation spaces. In addition to our theoretical contribution, we perform five numerical experiments on benchmark POMDPs to demonstrate that a simple MDP algorithm adapted using PB-MDP approximation, Sparse-PFT, achieves performance competitive with other leading continuous observation POMDP solvers.},
  archive      = {J_JAIR},
  author       = {Michael H. Lim and Tyler J. Becker and Mykel J. Kochenderfer and Claire J. Tomlin and Zachary N. Sunberg},
  doi          = {10.1613/jair.1.14525},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1591-1636},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimality guarantees for particle belief approximation of POMDPs},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Certified dominance and symmetry breaking for combinatorial
optimisation. <em>JAIR</em>, <em>77</em>, 1539–1589. (<a
href="https://doi.org/10.1613/jair.1.14296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification method for optimisation problems in which symmetry and dominance breaking is easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes cardinality and parity (XOR) reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems.},
  archive      = {J_JAIR},
  author       = {Bart Bogaerts and Stephan Gocht and Ciaran McCreesh and Jakob Nordström},
  doi          = {10.1613/jair.1.14296},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1539–1589},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Certified dominance and symmetry breaking for combinatorial optimisation},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved peel-and-bound: Methods for generating dual bounds
with multivalued decision diagrams. <em>JAIR</em>, <em>77</em>,
1489–1538. (<a href="https://doi.org/10.1613/jair.1.14607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision diagrams are an increasingly important tool in cutting-edge solvers for discrete optimization. However, the field of decision diagrams is relatively new, and is still incorporating the library of techniques that conventional solvers have had decades to build. We drew inspiration from the warm-start technique used in conventional solvers to address one of the major challenges faced by decision diagram based methods. Decision diagrams become more useful the wider they are allowed to be, but also become more costly to generate, especially with large numbers of variables. In the original version of this paper, we presented a method of peeling off a sub-graph of previously constructed diagrams and using it as the initial diagram for subsequent iterations that we call peel-and-bound. We tested the method on the sequence ordering problem, and our results indicate that our peel-and-bound scheme generates stronger bounds than a branch-and-bound scheme using the same propagators, and at significantly less computational cost. In this extended version of the paper, we also propose new methods for using relaxed decision diagrams to improve the solutions found using restricted decision diagrams, discuss the heuristic decisions involved with the parallelization of peel-and-bound, and discuss how peel-and-bound can be hyper-optimized for sequencing problems. Furthermore, we test the new methods on the sequence ordering problem and the traveling salesman problem with time-windows (TSPTW), and include an updated and generalized implementation of the algorithm capable of handling any discrete optimization problem. The new results show that peel-and-bound outperforms ddo (a decision diagram based branch-and-bound solver) on the TSPTW. We also close 15 open benchmark instances of the TSPTW.},
  archive      = {J_JAIR},
  author       = {Isaac Rudich and Quentin Cappart and Louis-Martin Rousseau},
  doi          = {10.1613/jair.1.14607},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1489-1538},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improved peel-and-bound: Methods for generating dual bounds with multivalued decision diagrams},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classes of hard formulas for QBF resolution. <em>JAIR</em>,
<em>77</em>, 1455–1487. (<a
href="https://doi.org/10.1613/jair.1.14710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, we know only a few handcrafted quantified Boolean formulas (QBFs) that are hard for central QBF resolution systems such as Q-Res and QU-Res, and only one specific QBF family to separate Q-Res and QU-Res. Here we provide a general method to construct hard formulas for Q-Res and QU-Res. The construction uses simple propositional formulas (e.g. minimally unsatisfiable formulas) in combination with easy QBF gadgets (Σb2 formulas without constant winning strategies). This leads to a host of new hard formulas, including new classes of hard random QBFs. We further present generic constructions for formulas separating Q-Res and QU-Res, and for separating Q-Res and LD-Q-Res.},
  archive      = {J_JAIR},
  author       = {Agnes Schleitzer and Olaf Beyersdorff},
  doi          = {10.1613/jair.1.14710},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1455-1487},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Classes of hard formulas for QBF resolution},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Qualitative reasoning about 2D cardinal directions using
answer set programming. <em>JAIR</em>, <em>77</em>, 1371–1453. (<a
href="https://doi.org/10.1613/jair.1.14345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a formal framework (called NCDC-ASP) for representing and reasoning about cardinal directions between extended spatial objects on a plane, using Answer Set Programming (ASP). NCDC-ASP preserves the meaning of cardinal directional relations as in Cardinal Directional Calculus (CDC), and provides solutions to all consistency checking problems in CDC under various conditions (i.e., for a complete/incomplete set of basic/disjunctive CDC constraints over connected/disconnected spatial objects). In particular, NCDC-ASP models a discretized version of the consistency checking problem in ASP, over a finite grid (rather than a plane), where we provide new lower bounds on the grid size to guarantee that it correctly characterizes solutions for the consistency checking in CDC. In addition, NCDC-ASP has the following two novelties important for applications. NCDC-ASP introduces default CDC constraints to represent and reason about background or commonsense knowledge that involves default qualitative directional relations (e.g., &quot;the ice cream truck is by default to the north of the playground&quot; or &quot;the keyboard is normally placed in front of the monitor&quot;). NCDC-ASP introduces inferred CDC constraints to allow inference of missing CDC relations and to provide them as explanations. We illustrate the uses and usefulness of NCDC-ASP with interesting scenarios from the real-world. We design and develop a variety of benchmark instances, and comprehensively evaluate NCDC-ASP from the perspectives of computational efficiency.},
  archive      = {J_JAIR},
  author       = {Yusuf Izmirlioglu and Esra Erdem},
  doi          = {10.1613/jair.1.14345},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1371-1453},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Qualitative reasoning about 2D cardinal directions using answer set programming},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic controllability of temporal plans in uncertain and
partially observable environments. <em>JAIR</em>, <em>77</em>,
1311–1369. (<a href="https://doi.org/10.1613/jair.1.13065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formalism of Simple Temporal Networks (STNs) provides methods for evaluating the feasibility of temporal plans. The basic formalism deals with the consistency of quantitative temporal requirements on scheduled events. This implicitly assumes a single agent has full control over the timing of events. The extension of Simple Temporal Networks with Uncertainty (STNU) introduces uncertainty into the timing of some events. Two main approaches to the feasibility of STNUs involve (1) where a single schedule works irrespective of the duration outcomes, called Strong Controllability, and (2) whether a strategy exists to schedule future events based on the outcomes of past events, called Dynamic Controllability. Case (1) essentially assumes the timing of uncertain events cannot be observed by the agent while case (2) assumes full observability. The formalism of Partially Observable Simple Temporal Networks with Uncertainty (POSTNU) provides an intermediate stance between these two extremes, where a known subset of the uncertain events can be observed when they occur. A sound and complete polynomial algorithm to determining the Dynamic Controllability of POSTNUs has not previously been known; we present one in this paper. This answers an open problem that has been posed in the literature. The approach we take factors the problem into Strong Controllability micro-problems in an overall Dynamic Controllability macro-problem framework. It generalizes the notion of labeled distance graph from STNUs. The generalized labels are expressed as max/min expressions involving the observables. The paper introduces sound generalized reduction rules that act on the generalized labels. These incorporate tightenings based on observability that preserve dynamic viable strategies. It is shown that if the generalized reduction rules reach quiescence without exposing an inconsistency, then the POSTNU is Dynamically Controllable (DC). The paper also presents algorithms that apply the reduction rules in an organized way and reach quiescence in a polynomial number of steps if the POSTNU is Dynamically Controllable. Remarkably, the generalized perspective leads to a simpler and more uniform framework that applies also to the STNU special case. It helps illuminate the previous methods inasmuch as the max/min label representation is more semantically clear than the ad-hoc upper/lower case labels previously used.},
  archive      = {J_JAIR},
  author       = {Arthur Bit-Monnot and Paul Morris},
  doi          = {10.1613/jair.1.13065},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1311-1369},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Dynamic controllability of temporal plans in uncertain and partially observable environments},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Program synthesis with best-first bottom-up search.
<em>JAIR</em>, <em>77</em>, 1275–1310. (<a
href="https://doi.org/10.1613/jair.1.14394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art cost-guided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical results on string manipulation and bit-vector tasks show that Bee Search can outperform existing cost-guided BUS approaches when employing more complex domain-specific languages (DSLs); Bee Search and previous approaches perform equally well with simpler DSLs. Furthermore, our new cost function with Bee Search outperforms previous cost functions on string manipulation tasks.},
  archive      = {J_JAIR},
  author       = {Saqib Ameen and Levi H.S. Lelis},
  doi          = {10.1613/jair.1.14394},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1275-1310},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Program synthesis with best-first bottom-up search},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complexity of computing the shapley value in partition
function form games. <em>JAIR</em>, <em>77</em>, 1237–1274. (<a
href="https://doi.org/10.1613/jair.1.14648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the complexity of computing the Shapley value in partition function form games. We focus on two representations based on marginal contribution nets (embedded MC-nets and weighted MC-nets) and five extensions of the Shapley value. Our results show that while weighted MC-nets are more concise than embedded MC-nets, they have slightly worse computational properties when it comes to computing the Shapley value: two out of five extensions can be computed in polynomial time for embedded MC-nets and only one for weighted MC-nets.},
  archive      = {J_JAIR},
  author       = {Oskar Skibski},
  doi          = {10.1613/jair.1.14648},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1237-1274},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Complexity of computing the shapley value in partition function form games},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical decompositions and termination analysis for
generalized planning. <em>JAIR</em>, <em>77</em>, 1203–1236. (<a
href="https://doi.org/10.1613/jair.1.14185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a new conceptual framework along with proof techniques and algorithmic processes for assessing termination and goal-reachability related properties of generalized plans. We build upon classic results from graph theory to decompose generalized plans into smaller components that are then used to derive hierarchical termination arguments. These methods can be used to determine the utility of a given generalized plan, as well as to guide the synthesis and learning processes for generalized plans. We present theoretical as well as empirical results illustrating the scope of this new approach. Our analysis shows that this approach significantly extends the class of generalized plans that can be assessed automatically, thereby reducing barriers in the synthesis and learning of reliable generalized plans.},
  archive      = {J_JAIR},
  author       = {Siddharth Srivastava},
  doi          = {10.1613/jair.1.14185},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1203-1236},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Hierarchical decompositions and termination analysis for generalized planning},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to DP-fy ML: A practical guide to machine learning with
differential privacy. <em>JAIR</em>, <em>77</em>, 1113–1201. (<a
href="https://doi.org/10.1613/jair.1.14649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) models are ubiquitous in real-world applications and are a constant focus of research. Modern ML models have become more complex, deeper, and harder to reason about. At the same time, the community has started to realize the importance of protecting the privacy of the training data that goes into these models. Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners, particularly with respect to the challenging task of hyperparameter tuning. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are “safe” to use with DP. In this survey paper, we attempt to create a self-contained guide that gives an in-depth overview of the field of DP ML. We aim to assemble information about achieving the best possible DP ML model with rigorous privacy guarantees. Our target audience is both researchers and practitioners. Researchers interested in DP for ML will benefit from a clear overview of current advances and areas for improvement. We also include theory-focused sections that highlight important topics such as privacy accounting and convergence. For a practitioner, this survey provides a background in DP theory and a clear step-by-step guide for choosing an appropriate privacy definition and approach, implementing DP training, potentially updating the model architecture, and tuning hyperparameters. For both researchers and practitioners, consistently and fully reporting privacy guarantees is critical, so we propose a set of specific best practices for stating guarantees. With sufficient computation and a sufficiently large training set or supplemental nonprivate data, both good accuracy (that is, almost as good as a non-private model) and good privacy can often be achievable. And even when computation and dataset size are limited, there are advantages to training with even a weak (but still finite) formal DP guarantee. Hence, we hope this work will facilitate more widespread deployments of DP ML models.},
  archive      = {J_JAIR},
  author       = {Natalia Ponomareva and Hussein Hazimeh and Alex Kurakin and Zheng Xu and Carson Denison and H. Brendan McMahan and Sergei Vassilvitskii and Steve Chien and Abhradeep Guha Thakurta},
  doi          = {10.1613/jair.1.14649},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1113-1201},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {How to DP-fy ML: A practical guide to machine learning with differential privacy},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mimicking behaviors in separated domains. <em>JAIR</em>,
<em>77</em>, 1087–1112. (<a
href="https://doi.org/10.1613/jair.1.14591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devising a strategy to make a system mimic behaviors from another system is a problem that naturally arises in many areas of Computer Science. In this work, we interpret this problem in the context of intelligent agents, from the perspective of ltlf , a formalism commonly used in AI for expressing finite-trace properties. Our model consists of two separated dynamic domains, DA and DB, and an LTLf specification that formalizes the notion of mimicking by mapping properties on behaviors (traces) of DA into properties on behaviors of DB. The goal is to synthesize a strategy that step-by-step maps every behavior of DA into a behavior of DB so that the specification is met. We consider several forms of mapping specifications, ranging from simple ones to full LTLf , and for each, we study synthesis algorithms and computational properties.},
  archive      = {J_JAIR},
  author       = {Giuseppe De Giacomo and Dror Fried and Fabio Patrizi and Shufang Zhu},
  doi          = {10.1613/jair.1.14591},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1087-1112},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Mimicking behaviors in separated domains},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A model to support collective reasoning: Formalization,
analysis and computational assessment. <em>JAIR</em>, <em>77</em>,
1021–1086. (<a href="https://doi.org/10.1613/jair.1.14409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new model to represent human debates and methods to obtain collective conclusions from them. This model overcomes two drawbacks of existing approaches. First, our model does not assume that participants agree on the structure of the debate. It does this by allowing participants to express their opinion about all aspects of the debate. Second, our model does not assume that participants’ opinions are rational, an assumption that significantly limits current approaches. Instead, we define a weaker notion of rationality that characterises coherent opinions, and we consider different scenarios based on the coherence of individual opinions and the level of consensus. We provide a formal analysis of different opinion aggregation functions that compute a collective decision based on the individual opinions and the debate structure. In particular, we demonstrate that aggregated opinions can be coherent even if there is a lack of consensus and individual opinions are not coherent. We conclude with an empirical evaluation demonstrating that collective opinions can be computed efficiently for real-sized debates.},
  archive      = {J_JAIR},
  author       = {Jordi Ganzer and Natalia Criado and Maite Lopez-Sanchez and Simon Parsons and Juan A. Rodriguez-Aguilar},
  doi          = {10.1613/jair.1.14409},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1021-1086},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A model to support collective reasoning: Formalization, analysis and computational assessment},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information lattice learning. <em>JAIR</em>, <em>77</em>,
971–1019. (<a href="https://doi.org/10.1613/jair.1.14277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Information Lattice Learning (ILL) as a general framework to learn rules of a signal (e.g., an image or a probability distribution). In our definition, a rule is a coarsened signal used to help us gain one interpretable insight about the original signal. To make full sense of what might govern the signal’s intrinsic structure, we seek multiple disentangled rules arranged in a hierarchy, called a lattice. Compared to representation/rule-learning models optimized for a specific task (e.g., classification), ILL focuses on explainability: it is designed to mimic human experiential learning and discover rules akin to those humans can distill and comprehend. This paper details the math and algorithms of ILL, and illustrates how it addresses the fundamental question “what makes X an X” by creating rule-based explanations designed to help humans understand. Our focus is on explaining X rather than (re)generating it. We present applications in knowledge discovery, using ILL to distill music theory from scores and chemical laws from molecules and further revealing connections between them. We show ILL’s efficacy and interpretability on benchmarks and assessments, as well as a demonstration of ILL-enhanced classifiers achieving human-level digit recognition using only one or a few MNIST training examples (1–10 per class).},
  archive      = {J_JAIR},
  author       = {Haizi Yu and James A. Evans and Lav R. Varshney},
  doi          = {10.1613/jair.1.14277},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {971-1019},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Information lattice learning},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAlign: A graph neural attention framework for aligning
structurally heterogeneous networks. <em>JAIR</em>, <em>77</em>,
949–969. (<a href="https://doi.org/10.1613/jair.1.14427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment techniques that map the same entities across multiple networks assume that the mapping nodes in two different networks have similar attributes and neighborhood proximity. However, real-world networks often violate such assumptions, having diverse attributes and structural properties. Node mapping across such structurally heterogeneous networks remains a challenge. Although capturing the nodes’ entire neighborhood (in low-dimensional embeddings) may help deal with these characteristic differences, the issue of over-smoothing in the representations that come from higherorder learning still remains a major problem. To address the above concerns, we propose SAlign: a supervised graph neural attention framework for aligning structurally heterogeneous networks that learns the correlation of structural properties of mapping nodes using a set of labeled (mapped) anchor nodes. SAlign incorporates nodes’ graphlet information with a novel structure-aware cross-network attention mechanism that transfers the required higher-order structure information across networks. The information exchanged across networks helps in enhancing the expressivity of the graph neural network, thereby handling any potential over-smoothing problem. Extensive experiments on three real datasets demonstrate that SAlign consistently outperforms the state-of-the-art network alignment methods by at least 1.3-8\% in terms of accuracy score. The code is available at https://github.com/shruti400/SAlign for reproducibility.},
  archive      = {J_JAIR},
  author       = {Shruti Saxena and Joydeep Chandra},
  doi          = {10.1613/jair.1.14427},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {949-969},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {SAlign: A graph neural attention framework for aligning structurally heterogeneous networks},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equivalence in argumentation frameworks with a claim-centric
view: Classical results with novel ingredients. <em>JAIR</em>,
<em>77</em>, 891–948. (<a
href="https://doi.org/10.1613/jair.1.14625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common feature of non-monotonic logics is that the classical notion of equivalence does not preserve the intended meaning in light of additional information. Consequently, the term strong equivalence was coined in the literature and thoroughly investigated. In the present paper, the knowledge representation formalism under consideration is claimaugmented argumentation frameworks (CAFs) which provide a formal basis to analyze conclusion-oriented problems in argumentation by adapting a claim-focused perspective. CAFs extend Dung AFs by associating a claim to each argument representing its conclusion. In this paper, we investigate both ordinary and strong equivalence in CAFs. Thereby, we take the fact into account that one might either be interested in the actual arguments or their claims only. The former point of view naturally yields an extension of strong equivalence for AFs to the claim-based setting while the latter gives rise to a novel equivalence notion which is genuine for CAFs. We tailor, examine and compare these notions and obtain a comprehensive study of this matter for CAFs. We conclude by investigating the computational complexity of naturally arising decision problems.},
  archive      = {J_JAIR},
  author       = {Ringo Baumann and Anna Rapberger and Markus Ulbricht},
  doi          = {10.1613/jair.1.14625},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {891-948},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Equivalence in argumentation frameworks with a claim-centric view: Classical results with novel ingredients},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDP playground: An analysis and debug testbed for
reinforcement learning. <em>JAIR</em>, <em>77</em>, 821–890. (<a
href="https://doi.org/10.1613/jair.1.14314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MDP Playground, a testbed for Reinforcement Learning (RL) agents with dimensions of hardness that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in toy and complex RL environments. We consider and allow control over a wide variety of dimensions, including delayed rewards, sequence lengths, reward density, stochasticity, image representations, irrelevant features, time unit, action range and more. We define a parameterised collection of fast-to-run toy environments in OpenAI Gym by varying these dimensions and propose to use these to understand agents better. We then show how to design experiments using MDP Playground to gain insights on the toy environments. We also provide wrappers that can inject many of these dimensions into any Gym environment. We experiment with these wrappers on Atari and Mujoco to allow for understanding the effects of these dimensions on environments that are more complex than the toy environments. We also compare the effect of the dimensions on the toy and complex environments. Finally, we show how to use MDP Playground to debug agents, to study the interaction of multiple dimensions and describe further use-cases.},
  archive      = {J_JAIR},
  author       = {Raghu Rajan and Jessica Lizeth Borja Diaz and Suresh Guttikonda and Fabio Ferreira and André Biedenkapp and Jan Ole von Hartz and Frank Hutter},
  doi          = {10.1613/jair.1.14314},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {821-890},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {MDP playground: An analysis and debug testbed for reinforcement learning},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Your college dorm and dormmates: Fair resource sharing with
externalities. <em>JAIR</em>, <em>77</em>, 793–820. (<a
href="https://doi.org/10.1613/jair.1.14863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a fair resource sharing problem, where a set of resources are to be shared among a group of agents. Each agent demands one resource and each resource can serve a limited number of agents. An agent cares about what resource they get as well as the externalities imposed by their mates, who share the same resource with them. Clearly, the strong notion of envy-freeness, where no agent envies another for their resource or mates, cannot always be achieved and we show that even deciding the existence of such a strongly envy-free assignment is an intractable problem. Hence, a more interesting question is whether (and in what situations) a relaxed notion of envy-freeness, the Pareto envyfreeness, can be achieved. Under this relaxed notion, an agent envies another only when they envy both the resource and the mates of the other agent. In particular, we are interested in a dorm assignment problem, where students are to be assigned to dorms with the same capacity and they have dichotomous preference over their dormmates. We show that when the capacity of each dorm is 2, a Pareto envy-free assignment always exists and we present a polynomial-time algorithm to compute such an assignment. Nevertheless, the result breaks immediately when the capacity increases to 3, in which case even Pareto envyfreeness cannot be guaranteed. In addition to the existential results, we also investigate the utility guarantees of (Pareto) envy-free assignments in our model.},
  archive      = {J_JAIR},
  author       = {Jiarui Gan and Bo Li and Yingkai Li},
  doi          = {10.1613/jair.1.14863},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {793-820},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Your college dorm and dormmates: Fair resource sharing with externalities},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The jiminy advisor: Moral agreements among stakeholders
based on norms and argumentation. <em>JAIR</em>, <em>77</em>, 737–792.
(<a href="https://doi.org/10.1613/jair.1.14368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and interacts with end users. All of these actors are stakeholders affected by the behavior of the autonomous system. We address the challenge of how the ethical views of such stakeholders can be integrated in the behavior of an autonomous system. We propose an ethical recommendation component called Jiminy which uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. A Jiminy represents the ethical views of each stakeholder by using normative systems, and has three ways of resolving moral dilemmas that involve the opinions of the stakeholders. First, the Jiminy considers how the arguments of the stakeholders relate to one another, which may already resolve the dilemma. Secondly, the Jiminy combines the normative systems of the stakeholders such that the combined expertise of the stakeholders may resolve the dilemma. Thirdly, and only if these two other methods have failed, the Jiminy uses context-sensitive rules to decide which of the stakeholders take preference over the others. At the abstract level, these three methods are characterized by adding arguments, adding attacks between arguments, and revising attacks between arguments. We show how a Jiminy can be used not only for ethical reasoning and collaborative decision-making, but also to provide explanations about ethical behavior.},
  archive      = {J_JAIR},
  author       = {Beishui Liao and Pere Pardo and Marija Slavkovik and Leendert van der Torre},
  doi          = {10.1613/jair.1.14368},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {737-792},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The jiminy advisor: Moral agreements among stakeholders based on norms and argumentation},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). First-order context-specific likelihood weighting in hybrid
probabilistic logic programs. <em>JAIR</em>, <em>77</em>, 683–735. (<a
href="https://doi.org/10.1613/jair.1.13657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical relational AI and probabilistic logic programming have so far mostly focused on discrete probabilistic models. The reasons for this is that one needs to provide constructs to succinctly model the independencies in such models, and also provide efficient inference. Three types of independencies are important to represent and exploit for scalable inference in hybrid models: conditional independencies elegantly modeled in Bayesian networks, context-specific independencies naturally represented by logical rules, and independencies amongst attributes of related objects in relational models succinctly expressed by combining rules. This paper introduces a hybrid probabilistic logic programming language, DC#, which integrates distributional clauses&#39; syntax and semantics principles of Bayesian logic programs. It represents the three types of independencies qualitatively. More importantly, we also introduce the scalable inference algorithm FO-CS-LW for DC#. FO-CS-LW is a first-order extension of the context-specific likelihood weighting algorithm (CS-LW), a novel sampling method that exploits conditional independencies and context-specific independencies in ground models. The FO-CS-LW algorithm upgrades CS-LW with unification and combining rules to the first-order case.},
  archive      = {J_JAIR},
  author       = {Nitesh Kumar and Ondřej Kuželka and Luc De Raedt},
  doi          = {10.1613/jair.1.13657},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {683-735},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {First-order context-specific likelihood weighting in hybrid probabilistic logic programs},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlexiBO: A decoupled cost-aware multi-objective optimization
approach for deep neural networks. <em>JAIR</em>, <em>77</em>, 645–682.
(<a href="https://doi.org/10.1613/jair.1.14139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of machine learning systems often requires trading off different objectives, for example, prediction error and energy consumption for deep neural networks (DNNs). Typically, no single design performs well in all objectives; therefore, finding Pareto-optimal designs is of interest. The search for Pareto-optimal designs involves evaluating designs in an iterative process, and the measurements are used to evaluate an acquisition function that guides the search process. However, measuring different objectives incurs different costs. For example, the cost of measuring the prediction error of DNNs is orders of magnitude higher than that of measuring the energy consumption of a pre-trained DNN as it requires re-training the DNN. Current state-of-the-art methods do not consider this difference in objective evaluation cost, potentially incurring expensive evaluations of objective functions in the optimization process. In this paper, we develop a novel decoupled and cost-aware multi-objective optimization algorithm, which we call Flexible Multi-Objective Bayesian Optimization (FlexiBO) to address this issue. For evaluating each design, FlexiBO selects the objective with higher relative gain by weighting the improvement of the hypervolume of the Pareto region with the measurement cost of each objective. This strategy, therefore, balances the expense of collecting new information with the knowledge gained through objective evaluations, preventing FlexiBO from performing expensive measurements for little to no gain. We evaluate FlexiBO on seven state-of-the-art DNNs for image recognition, natural language processing (NLP), and speech-to-text translation. Our results indicate that, given the same total experimental budget, FlexiBO discovers designs with 4.8\% to 12.4\% lower hypervolume error than the best method in state-of-the-art multi-objective optimization.},
  archive      = {J_JAIR},
  author       = {Md Shahriar Iqbal and Jianhai Su and Lars Kotthoff and Pooyan Jamshidi},
  doi          = {10.1613/jair.1.14139},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {645-682},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {FlexiBO: A decoupled cost-aware multi-objective optimization approach for deep neural networks},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On dynamics in structured argumentation formalisms.
<em>JAIR</em>, <em>77</em>, 563–643. (<a
href="https://doi.org/10.1613/jair.1.14481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a contribution to the research on dynamics in assumption-based argumentation (ABA). We investigate situations where a given knowledge base undergoes certain changes. We show that two frequently investigated problems, namely enforcement of a given target atom and deciding strong equivalence of two given ABA frameworks, are intractable in general. Notably, these problems are both tractable for abstract argumentation frameworks (AFs) which admit a close correspondence to ABA by constructing semanticspreserving instances. Inspired by this observation, we search for tractable fragments for ABA frameworks by means of the instantiated AFs. We argue that the usual instantiation procedure is not suitable for the investigation of dynamic scenarios since too much information is lost when constructing the abstract framework. We thus consider an extension of AFs, called cvAFs, equipping arguments with conclusions and vulnerabilities in order to better anticipate their role after the underlying knowledge base is extended. We investigate enforcement and strong equivalence for cvAFs and present syntactic conditions to decide them. We show that the correspondence between cvAFs and ABA frameworks is close enough to capture dynamics in ABA. This yields the desired tractable fragment. We furthermore discuss consequences for the corresponding problems for logic programs.},
  archive      = {J_JAIR},
  author       = {Anna Rapberger and Markus Ulbricht},
  doi          = {10.1613/jair.1.14481},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {563-643},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On dynamics in structured argumentation formalisms},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A markov framework for learning and reasoning about
strategies in professional soccer. <em>JAIR</em>, <em>77</em>, 517–562.
(<a href="https://doi.org/10.1613/jair.1.13934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategy-optimization is a fundamental element of dynamic and complex team sports such as soccer, American football, and basketball. As the amount of data that is collected from matches in these sports has increased, so has the demand for data-driven decisionmaking support. If alternative strategies need to be balanced, a data-driven approach can uncover insights that are not available from qualitative analysis. This could tremendously aid teams in their match preparations. In this work, we propose a novel Markov modelbased framework for soccer that allows reasoning about the specific strategies teams use in order to gain insights into the efficiency of each strategy. The framework consists of two components: (1) a learning component, which entails modeling a team’s offensive behavior by learning a Markov decision process (MDP) from event data that is collected from the team’s matches, and (2) a reasoning component, which involves a novel application of probabilistic model checking to reason about the efficacy of the learned strategies of each team. In this paper, we provide an overview of this framework and illustrate it on several use cases using real-world event data from three leagues. Our results show that the framework can be used to reason about the shot decision-making of teams and to optimise the defensive strategies used when playing against a particular team. The general ideas presented in this framework can easily be extended to other sports.},
  archive      = {J_JAIR},
  author       = {Maaike Van Roy and Pieter Robberechts and Wen-Chi Yang and Luc De Raedt and Jesse Davis},
  doi          = {10.1613/jair.1.13934},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {517-562},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A markov framework for learning and reasoning about strategies in professional soccer},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stackelberg security games with contagious attacks on a
network: Reallocation to the rescue. <em>JAIR</em>, <em>77</em>,
487–515. (<a href="https://doi.org/10.1613/jair.1.14563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the classic network security games, the defender distributes defending resources to the nodes of the network, and the attacker attacks a node, with the objective of maximizing the damage caused. In this paper, we consider the network defending problem against contagious attacks, e.g., the attack at a node u spreads to the neighbors of u and can cause damage at multiple nodes. Existing works that study shared resources assume that the resource allocated to a node can be shared or duplicated between neighboring nodes. However, in the real world, sharing resource naturally leads to a decrease in defending power of the source node, especially when defending against contagious attacks. Therefore, we study the model in which resources allocated to a node can only be transferred to its neighboring nodes, which we refer to as a reallocation process. We show that the problem of computing optimal defending strategy is NP-hard even for some very special cases. For positive results, we give a mixed integer linear program formulation for the problem and a bi-criteria approximation algorithm. Our experimental results demonstrate that the allocation and reallocation strategies our algorithm computes perform well in terms of minimizing the damage due to contagious attacks.},
  archive      = {J_JAIR},
  author       = {Rufan Bai and Haoxing Lin and Xinyu Yang and Xiaowei Wu and Minming Li and Weijia Jia},
  doi          = {10.1613/jair.1.14563},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {487-515},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Stackelberg security games with contagious attacks on a network: Reallocation to the rescue},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The complexity of matching games: A survey. <em>JAIR</em>,
<em>77</em>, 459–485. (<a
href="https://doi.org/10.1613/jair.1.14281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching games naturally generalize assignment games, a well-known class of cooperative games. Interest in matching games has grown recently due to some breakthrough results and new applications. This state-of-the-art survey provides an overview of matching games and extensions, such as b-matching games and partitioned matching games; the latter originating from the emerging area of international kidney exchange. In this survey we focus on computational complexity aspects of various game-theoretical solution concepts, such as the core, nucleolus and Shapley value, when the input is restricted to a matching game or one of its variants.},
  archive      = {J_JAIR},
  author       = {Marton Benedek and Peter Biro and Matthew Johnson and Daniel Paulusma and Xin Ye},
  doi          = {10.1613/jair.1.14281},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {459-485},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The complexity of matching games: A survey},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards green automated machine learning: Status quo and
future directions. <em>JAIR</em>, <em>77</em>, 427–457. (<a
href="https://doi.org/10.1613/jair.1.14340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution — a machine learning pipeline — tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticized for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large-scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool w.r.t. their “greenness”, i.e., sustainability, are summarized. Finally, we elaborate on how to be transparent about the environmental footprint and what kind of research incentives could direct the community in a more sustainable AutoML research direction. As part of this, we propose a sustainability checklist to be attached to every AutoML paper featuring all core aspects of Green AutoML.},
  archive      = {J_JAIR},
  author       = {Tanja Tornede and Alexander Tornede and Jonas Hanselle and Felix Mohr and Marcel Wever and Eyke Hüllermeier},
  doi          = {10.1613/jair.1.14340},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {427-457},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards green automated machine learning: Status quo and future directions},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contract scheduling with predictions. <em>JAIR</em>,
<em>77</em>, 395–426. (<a
href="https://doi.org/10.1613/jair.1.14117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contract scheduling is a general technique that allows the design of systems with interruptible capabilities, given an algorithm that is not necessarily interruptible. Previous work on this topic has assumed that the interruption is a worst-case deadline that is unknown to the scheduler. In this work, we study new settings in which the scheduler has access to some imperfect prediction in regards to the interruption. In the first setting, which is inspired by recent advances in learning-enhanced algorithms, the prediction describes the time that the interruption occurs. The second setting introduces a new model in which predictions are elicited as responses to a number of binary queries. For both settings, we investigate trade-offs between the robustness (i.e., the worst-case performance of the schedule if the prediction is generated adversarially) and the consistency (i.e., the performance assuming that the prediction is error-free). We also establish results on the performance of the schedules as a function of the prediction error.},
  archive      = {J_JAIR},
  author       = {Spyros Angelopoulos and Shahin Kamali},
  doi          = {10.1613/jair.1.14117},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {395-426},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Contract scheduling with predictions},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Your prompt is my command: On assessing the human-centred
generality of multimodal models. <em>JAIR</em>, <em>77</em>, 377–394.
(<a href="https://doi.org/10.1613/jair.1.14157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even with obvious deficiencies, large prompt-commanded multimodal models are proving to be flexible cognitive tools representing an unprecedented generality. But the directness, diversity, and degree of user interaction create a distinctive “human-centred generality” (HCG), rather than a fully autonomous one. HCG implies that —for a specific user— a system is only as general as it is effective for the user’s relevant tasks and their prevalent ways of prompting. A human-centred evaluation of general-purpose AI systems therefore needs to reflect the personal nature of interaction, tasks and cognition. We argue that the best way to understand these systems is as highly-coupled cognitive extenders, and to analyse the bidirectional cognitive adaptations between them and humans. In this paper, we give a formulation of HCG, as well as a high-level overview of the elements and trade-offs involved in the prompting process. We end the paper by outlining some essential research questions and suggestions for improving evaluation practices, which we envision as characteristic for the evaluation of general artificial intelligence in the future. This paper appears in the AI &amp;amp; Society track.},
  archive      = {J_JAIR},
  author       = {Wout Schellaert and Fernando Martínez-Plumed and Karina Vold and John Burden and Pablo A. M. Casares and Bao Sheng Loe and Roi Reichart and Sean Ó hÉigeartaigh and Anna Korhonen and José Hernández-Orallo},
  doi          = {10.1613/jair.1.14157},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {377-394},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Your prompt is my command: On assessing the human-centred generality of multimodal models},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient multi-goal reinforcement learning via value
consistency prioritization. <em>JAIR</em>, <em>77</em>, 355–376. (<a
href="https://doi.org/10.1613/jair.1.14398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-conditioned reinforcement learning (RL) with sparse rewards remains a challenging problem in deep RL. Hindsight Experience Replay (HER) has been demonstrated to be an effective solution, where HER replaces desired goals in failed experiences with practically achieved states. Existing approaches mainly focus on either exploration or exploitation to improve the performance of HER. From a joint perspective, exploiting specific past experiences can also implicitly drive exploration. Therefore, we concentrate on prioritizing both original and relabeled samples for efficient goal-conditioned RL. To achieve this, we propose a novel value consistency prioritization (VCP) method, where the priority of samples is determined by the consistency of ensemble Q-values. This distinguishes the VCP method with most existing prioritization approaches which prioritizes samples based on the uncertainty of ensemble Q-values. Through extensive experiments, we demonstrate that VCP achieves significantly higher sample efficiency than existing algorithms on a range of challenging goal-conditioned manipulation tasks. We also visualize how VCP prioritizes good experiences to enhance policy learning.},
  archive      = {J_JAIR},
  author       = {Jiawei Xu and Shuxing Li and Rui Yang and Chun Yuan and Lei Han},
  doi          = {10.1613/jair.1.14398},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {355-376},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient multi-goal reinforcement learning via value consistency prioritization},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On centralized critics in multi-agent reinforcement
learning. <em>JAIR</em>, <em>77</em>, 295–354. (<a
href="https://doi.org/10.1613/jair.1.14386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralized Training for Decentralized Execution, where agents are trained offline in a centralized fashion and execute online in a decentralized manner, has become a popular approach in Multi-Agent Reinforcement Learning (MARL). In particular, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, using a centralized critic in this context has yet to be sufficiently analyzed theoretically or empirically. In this paper, we therefore formally analyze centralized and decentralized critic approaches, and analyze the effect of using state-based critics in partially observable environments. We derive theories contrary to the common intuition: critic centralization is not strictly beneficial, and using state values can be harmful. We further prove that, in particular, state-based critics can introduce unexpected bias and variance compared to history-based critics. Finally, we demonstrate how the theory applies in practice by comparing different forms of critics on a wide range of common multi-agent benchmarks. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.},
  archive      = {J_JAIR},
  author       = {Xueguang Lyu and Andrea Baisero and Yuchen Xiao and Brett Daley and Christopher Amato},
  doi          = {10.1613/jair.1.14386},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {295-354},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On centralized critics in multi-agent reinforcement learning},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiring reasoning frameworks in AI and their computational
complexity. <em>JAIR</em>, <em>77</em>, 207–293. (<a
href="https://doi.org/10.1613/jair.1.13970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important problems in AI, among them #SAT, parameter learning and probabilistic inference go beyond the classical satisfiability problem. Here, instead of finding a solution we are interested in a quantity associated with the set of solutions, such as the number of solutions, the optimal solution or the probability that a query holds in a solution. To model such quantitative problems in a uniform manner, a number of frameworks, e.g. Algebraic Model Counting and Semiring-based Constraint Satisfaction Problems, employ what we call the semiring paradigm. In the latter the abstract algebraic structure of the semiring serves as a means of parameterizing the problem definition, thus allowing for different modes of quantitative computations by choosing different semirings. While efficiently solvable cases have been widely studied, a systematic study of the computational complexity of such problems depending on the semiring parameter is missing. In this work, we characterize the latter by NP(R), a novel generalization of NP over semiring R, and obtain NP(R)-completeness results for a selection of semiring frameworks. To obtain more tangible insights into the hardness of NP(R), we link it to well-known complexity classes from the literature. Interestingly, we manage to connect the computational hardness to properties of the semiring. Using this insight, we see that, on the one hand, NP(R) is always at least as hard as NP or ModpP depending on the semiring R and in general unlikely to be in FPSPACEpoly. On the other hand, for broad subclasses of semirings relevant in practice we can employ reductions to NP, ModpP and #P. These results show that in many cases solutions are only mildly harder to compute than functions in NP, ModpP and #P, give us new insights into how problems that involve counting on semirings can be approached, and provide a means of assessing whether an algorithm is appropriate for a given class of problems.},
  archive      = {J_JAIR},
  author       = {Thomas Eiter and Rafael Kiesel},
  doi          = {10.1613/jair.1.13970},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {207-293},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Semiring reasoning frameworks in AI and their computational complexity},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational modelling of quantifier use: Corpus, models,
and evaluation. <em>JAIR</em>, <em>77</em>, 167–206. (<a
href="https://doi.org/10.1613/jair.1.13899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prominent strand of work in formal semantics investigates the ways in which human languages quantify the elements of a set, as when we say All A are B, Few A are B, and so on. Building on a growing body of empirical studies that shed light on the meaning and the use of quantifiers, we extend this line of work by computationally modelling how human speakers textually describe complex scenes in which quantitative relations play an important role. To this end, we conduct a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions. The experiments result in a corpus, called QTUNA, made up of short texts that contain a large variety of quantified expressions. We analyse QTUNA, summarise our findings, and explain how we design computational models of human quantifier use accordingly. Finally, we evaluate these models in accordance with QTUNA.},
  archive      = {J_JAIR},
  author       = {Guanyi Chen and Kees van Deemter},
  doi          = {10.1613/jair.1.13899},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {167-206},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Computational modelling of quantifier use: Corpus, models, and evaluation},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Repairing the cracked foundation: A survey of obstacles in
evaluation practices for generated text. <em>JAIR</em>, <em>77</em>,
103–166. (<a href="https://doi.org/10.1613/jair.1.13715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
  archive      = {J_JAIR},
  author       = {Sebastian Gehrmann and Elizabeth Clark and Thibault Sellam},
  doi          = {10.1613/jair.1.13715},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {103-166},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting action impact regularity and exogenous state
variables for offline reinforcement learning. <em>JAIR</em>,
<em>77</em>, 71–101. (<a
href="https://doi.org/10.1613/jair.1.14580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning—learning a policy from a batch of data—is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world environments where the regularity holds.},
  archive      = {J_JAIR},
  author       = {Vincent Liu and James R. Wright and Martha White},
  doi          = {10.1613/jair.1.14580},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {71-101},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Exploiting action impact regularity and exogenous state variables for offline reinforcement learning},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlexiBERT: Are current transformer architectures too
homogeneous and rigid? <em>JAIR</em>, <em>77</em>, 39–70. (<a
href="https://doi.org/10.1613/jair.1.13942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of a plethora of language models makes the problem of selecting the best one for a custom task challenging. Most state-of-the-art methods leverage transformer-based models (e.g., BERT) or their variants. However, training such models and exploring their hyperparameter space is computationally expensive. Prior work proposes several neural architecture search (NAS) methods that employ performance predictors (e.g., surrogate models) to address this issue; however, such works limit analysis to homogeneous models that use fixed dimensionality throughout the network. This leads to sub-optimal architectures. To address this limitation, we propose a suite of heterogeneous and flexible models, namely FlexiBERT, that have varied encoder layers with a diverse set of possible operations and different hidden dimensions. For better-posed surrogate modeling in this expanded design space, we propose a new graph-similarity-based embedding scheme. We also propose a novel NAS policy, called BOSHNAS, that leverages this new scheme, Bayesian modeling, and second-order optimization, to quickly train and use a neural surrogate model to converge to the optimal architecture. A comprehensive set of experiments shows that the proposed policy, when applied to the FlexiBERT design space, pushes the performance frontier upwards compared to traditional models. FlexiBERT-Mini, one of our proposed models, has 3\% fewer parameters than BERT-Mini and achieves 8.9\% higher GLUE score. A FlexiBERT model with equivalent performance as the best homogeneous model has 2.6× smaller size. FlexiBERT-Large, another proposed model, attains state-of-the-art results, outperforming the baseline models by at least 5.7\% on the GLUE benchmark.},
  archive      = {J_JAIR},
  author       = {Shikhar Tuli and Bhishma Dedhia and Shreshth Tuli and Niraj K. Jha},
  doi          = {10.1613/jair.1.13942},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {39-70},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {FlexiBERT: Are current transformer architectures too homogeneous and rigid?},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object-agnostic affordance categorization via unsupervised
learning of graph embeddings. <em>JAIR</em>, <em>77</em>, 1–38. (<a
href="https://doi.org/10.1613/jair.1.13253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects’ availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that our method learns to create object affordance clusters with a high V-measure even in cluttered scenes. The proposed approach handles object occlusions by capturing effectively possible interactions and without imposing any object or scene constraints.},
  archive      = {J_JAIR},
  author       = {Alexia Toumpa and Anthony G. Cohn},
  doi          = {10.1613/jair.1.13253},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-38},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Object-agnostic affordance categorization via unsupervised learning of graph embeddings},
  volume       = {77},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QNLP in practice: Running compositional models of meaning on
a quantum computer. <em>JAIR</em>, <em>76</em>, 1305–1342. (<a
href="https://doi.org/10.1613/jair.1.14329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh, and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a “bag-of-words” model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected. We demonstrate that all models converge smoothly both in simulations and when run on quantum hardware, and that the results are the expected ones based on the nature of the tasks and the datasets used. Another important goal of this paper is to describe in a way accessible to AI and NLP researchers the main principles, process and challenges of experiments on quantum hardware. Our aim in doing this is to take the first small steps in this unexplored research territory and pave the way for practical Quantum Natural Language Processing.},
  archive      = {J_JAIR},
  author       = {Robin Lorenz and Anna Pearson and Konstantinos Meichanetzidis and Dimitri Kartsaklis and Bob Coecke},
  doi          = {10.1613/jair.1.14329},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1305-1342},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {QNLP in practice: Running compositional models of meaning on a quantum computer},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FactGen: Faithful text generation by factuality-aware
pre-training and contrastive ranking fine-tuning. <em>JAIR</em>,
<em>76</em>, 1281–1303. (<a
href="https://doi.org/10.1613/jair.1.14267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional text generation is supposed to generate a fluent and coherent target text that is faithful to the source text. Although pre-trained models have achieved promising results, they still suffer from the crucial factuality problem. To deal with this issue, we propose a factuality-aware pretraining-finetuning framework named FactGen, which fully considers factuality during two training stages. Specifically, at the pre-training stage, we utilize a natural language inference model to construct target texts that are entailed by the source texts, resulting in a more factually consistent pre-training objective. Then, during the fine-tuning stage, we further introduce a contrastive ranking loss to encourage the model to generate factually consistent text with higher probability. Extensive experiments on three conditional text generation tasks demonstrate the effectiveness and generality of our training framework.},
  archive      = {J_JAIR},
  author       = {ZhiBin Lan and Wei Li and Jinsong Su and Xinyan Xiao and Jiachen Liu and Wenhao Wu and Yajuan Lyu},
  doi          = {10.1613/jair.1.14267},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1281-1303},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {FactGen: Faithful text generation by factuality-aware pre-training and contrastive ranking fine-tuning},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness in forecasting of observations of linear dynamical
systems. <em>JAIR</em>, <em>76</em>, 1247–1280. (<a
href="https://doi.org/10.1613/jair.1.14050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. This behaviour can often be modelled as observations of an unknown dynamical system with an unobserved state. When the training data for the subgroups are not controlled carefully, however, under-representation bias arises. To counter under-representation bias, we introduce two natural notions of fairness in timeseries forecasting problems: subgroup fairness and instantaneous fairness. These notion extend predictive parity to the learning of dynamical systems. We also show globally convergent methods for the fairness-constrained learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. We also show that by exploiting sparsity in the convexifications, we can reduce the run time of our methods considerably. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data set demonstrate the efficacy of our methods.},
  archive      = {J_JAIR},
  author       = {Quan Zhou and Jakub Mareček and Robert Shorten},
  doi          = {10.1613/jair.1.14050},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1247-1280},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Fairness in forecasting of observations of linear dynamical systems},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair and efficient allocation of scarce resources based on
predicted outcomes: Implications for homeless service delivery.
<em>JAIR</em>, <em>76</em>, 1219–1245. (<a
href="https://doi.org/10.1613/jair.1.12847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence, machine learning, and algorithmic techniques in general, provide two crucial abilities with the potential to improve decision-making in the context of allocation of scarce societal resources. They have the ability to flexibly and accurately model treatment response at the individual level, potentially allowing us to better match available resources to individuals. In addition, they have the ability to reason simultaneously about the effects of matching sets of scarce resources to populations of individuals. In this work, we leverage these abilities to study algorithmic allocation of scarce societal resources in the context of homelessness. In communities throughout the United States, there is constant demand for an array of homeless services intended to address different levels of need. Allocations of housing services must match households to appropriate services that continuously fluctuate in availability, while inefficiencies in allocation could “waste” scarce resources as households will remain in-need and re-enter the homeless system, increasing the overall demand for homeless services. This complex allocation problem introduces novel technical and ethical challenges. Using administrative data from a regional homeless system, we formulate the problem of “optimal” allocation of resources given data on households with need for homeless services. The optimization problem aims to allocate available resources such that predicted probabilities of household re-entry are minimized. The key element of this work is its use of a counterfactual prediction approach that predicts household probabilities of re-entry into homeless services if assigned to each service. Through these counterfactual predictions, we find that this approach has the potential to improve the efficiency of the homeless system by reducing re-entry, and, therefore, system-wide demand. However, efficiency comes with trade-offs - a significant fraction of households are assigned to services that increase probability of re-entry. To address this issue as well as the inherent fairness considerations present in any context where there are insufficient resources to meet demand, we discuss the efficiency, equity, and fairness issues that arise in our work and consider potential implications for homeless policies.},
  archive      = {J_JAIR},
  author       = {Amanda R. Kube and Sanmay Das and Patrick J. Fowler},
  doi          = {10.1613/jair.1.12847},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1219-1245},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Fair and efficient allocation of scarce resources based on predicted outcomes: Implications for homeless service delivery},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of environmental features that impact deep
reinforcement learning in sparse-reward domains. <em>JAIR</em>,
<em>76</em>, 1181–1218. (<a
href="https://doi.org/10.1613/jair.1.14390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning has achieved impressive results in recent years; yet, it is still severely troubled by environments showcasing sparse rewards. On top of that, not all sparse-reward environments are created equal, i.e., they can differ in the presence or absence of various features, with many of them having a great impact on learning. In light of this, the present work puts together a literature compilation of such environmental features, covering particularly those that have been taken advantage of and those that continue to pose a challenge. We expect this effort to provide guidance to researchers for assessing the generality of their new proposals and to call their attention to issues that remain unresolved when dealing with sparse rewards.},
  archive      = {J_JAIR},
  author       = {Jim Martin Catacora Ocana and Roberto Capobianco and Daniele Nardi},
  doi          = {10.1613/jair.1.14390},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1181-1218},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An overview of environmental features that impact deep reinforcement learning in sparse-reward domains},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring fairness under unawareness of sensitive
attributes: A quantification-based approach. <em>JAIR</em>, <em>76</em>,
1117–1180. (<a href="https://doi.org/10.1613/jair.1.14033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concerned with directly providing group-level prevalence estimates (rather than individual-level class labels). We show that quantification approaches are particularly suited to tackle the fairness-under-unawareness problem, as they are robust to inevitable distribution shifts while at the same time decoupling the (desirable) objective of measuring group fairness from the (undesirable) side effect of allowing the inference of sensitive attributes of individuals. More in detail, we show that fairness under unawareness can be cast as a quantification problem and solved with proven methods from the quantification literature. We show that these methods outperform previous approaches to measure demographic parity in five experimental protocols, corresponding to important challenges that complicate the estimation of classifier fairness under unawareness.},
  archive      = {J_JAIR},
  author       = {Alessandro Fabris and Andrea Esuli and Alejandro Moreo and Fabrizio Sebastiani},
  doi          = {10.1613/jair.1.14033},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1117-1180},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Measuring fairness under unawareness of sensitive attributes: A quantification-based approach},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coopetition against an amazon. <em>JAIR</em>, <em>76</em>,
1077–1116. (<a href="https://doi.org/10.1613/jair.1.14074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes cooperative data-sharing between competitors vying to predict a consumer&#39;s tastes. We design optimal data-sharing schemes both for when they compete only with each other, and for when they additionally compete with an Amazon – a company with more, better data. We show that simple schemes – threshold rules that probabilistically induce either full data-sharing between competitors, or the full transfer of data from one competitor to another – are either optimal or approximately optimal, depending on properties of the information structure. We also provide conditions under which firms share more data when they face stronger outside competition, and describe situations in which this conclusion is reversed.},
  archive      = {J_JAIR},
  author       = {Ronen Gradwohl and Moshe Tennenholtz},
  doi          = {10.1613/jair.1.14074},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1077-1116},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Coopetition against an amazon},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Negative human rights as a basis for long-term AI safety and
regulation. <em>JAIR</em>, <em>76</em>, 1043–1075. (<a
href="https://doi.org/10.1613/jair.1.14020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems. This article appears in the AI &amp;amp; Society track.},
  archive      = {J_JAIR},
  author       = {Ondrej Bajgar and Jan Horenovsky},
  doi          = {10.1613/jair.1.14020},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1043-1075},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Negative human rights as a basis for long-term AI safety and regulation},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized gradient-quantization based matrix
factorization for fast privacy-preserving point-of-interest
recommendation. <em>JAIR</em>, <em>76</em>, 1019–1041. (<a
href="https://doi.org/10.1613/jair.1.14414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapidly growing of location-based social networks, point-of-interest (POI) recommendation has been attracting tremendous attentions. Previous works for POI recommendation usually use matrix factorization (MF)-based methods, which achieve promising performance. However, existing MF-based methods suffer from two critical limitations: (1) Privacy issues: all users’ sensitive data are collected to the centralized server which may leak on either the server side or during transmission. (2) Poor resource utilization and training efficiency: training on centralized server with potentially huge low-rank matrices is computational inefficient. In this paper, we propose a novel decentralized gradient-quantization based matrix factorization (DGMF) framework to address the above limitations in POI recommendation. Compared with the centralized MF methods which store all sensitive data and low-rank matrices during model training, DGMF treats each user’s device (e.g., phone) as an independent learner and keeps the sensitive data on each user’s end. Furthermore, a privacy-preserving and communication-efficient mechanism with gradient-quantization technique is presented to train the proposed model, which aims to handle the privacy problem and reduces the communication cost in the decentralized setting. Theoretical guarantees of the proposed algorithm and experimental studies on real-world datasets demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_JAIR},
  author       = {Xuebin Zhou and Zhibin Hu and Jin Huang and Jian Chen},
  doi          = {10.1613/jair.1.14414},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1019-1041},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Decentralized gradient-quantization based matrix factorization for fast privacy-preserving point-of-interest recommendation},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal and efficient auctions for the gradual procurement
of strategic service provider agents. <em>JAIR</em>, <em>76</em>,
959–1018. (<a href="https://doi.org/10.1613/jair.1.14126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an outsourcing problem where a software agent procures multiple services  from providers with uncertain reliabilities to complete a computational task before a  strict deadline. The service consumer’s goal is to design an outsourcing strategy (defining  which services to procure and when) so as to maximize a specific objective function. This  objective function can be different based on the consumer’s nature; a socially-focused consumer  often aims to maximize social welfare, while a self-interested consumer often aims  to maximize its own utility. However, in both cases, the objective function depends on  the providers’ execution costs, which are privately held by the self-interested providers and  hence may be misreported to influence the consumer’s decisions. For such settings, we  develop a unified approach to design truthful procurement auctions that can be used by  both socially-focused and, separately, self-interested consumers. This approach benefits  from our proposed weighted threshold payment scheme which pays the provably minimum  amount to make an auction with a monotone outsourcing strategy incentive compatible.  This payment scheme can handle contingent outsourcing plans, where additional procurement  happens gradually over time and only if the success probability of the already hired  providers drops below a time-dependent threshold. Using a weighted threshold payment  scheme, we design two procurement auctions that maximize, as well as two low-complexity  heuristic-based auctions that approximately maximize, the consumer’s expected utility and  expected social welfare, respectively. We demonstrate the effectiveness and strength of our  proposed auctions through both game-theoretical and empirical analysis.},
  archive      = {J_JAIR},
  author       = {Farzaneh Farhadi and Maria Chli and Nicholas R. Jennings},
  doi          = {10.1613/jair.1.14126},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {959-1018},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimal and efficient auctions for the gradual procurement of strategic service provider agents},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair influence maximization in large-scale social networks
based on attribute-aware reverse influence sampling. <em>JAIR</em>,
<em>76</em>, 925–957. (<a
href="https://doi.org/10.1613/jair.1.14450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is the problem of finding a set of seed nodes in the network that maximizes the influence spread, which has become an important topic in social network analysis. Conventional influence maximization algorithms cause “unfair&quot; influence spread among different groups in the population, which could lead to severe bias in public opinion dissemination and viral marketing. To address this issue, we formulate the fair influence maximization problem concerning the trade-off between influence maximization and group fairness. For the purpose of solving the fair influence maximization problem in large-scale social networks efficiently, we propose a novel attribute-based reverse influence sampling (ABRIS) framework. This framework intends to estimate influence in specific groups with guarantee through an attribute-based hypergraph so that we can select seed nodes strategically. Therefore, under the ABRIS framework, we design two different node selection algorithms, ABRIS-G and ABRIS-T. ABRIS-G selects nodes in a greedy scheduling way. ABRIS-T adopts a two-phase node selection method. These algorithms run efficiently and achieve a good trade-off between influence maximization and group fairness. Extensive experiments on six real-world social networks show that our algorithms significantly outperform the state-of-the-art approaches. This article appears in the AI &amp;amp; Society track.},
  archive      = {J_JAIR},
  author       = {Mingkai Lin and Lintan Sun and Rui Yang and Xusheng Liu and Yajuan Wang and Ding Li and Wenzhong Li and Sanglu Lu},
  doi          = {10.1613/jair.1.14450},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {925-957},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Fair influence maximization in large-scale social networks based on attribute-aware reverse influence sampling},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining ℰℒ⊥ bases with adaptable role depth. <em>JAIR</em>,
<em>76</em>, 883–924. (<a
href="https://doi.org/10.1613/jair.1.13777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Formal Concept Analysis, a base for a finite structure is a set of implications that characterizes all valid implications of the structure. This notion can be adapted to the context of Description Logic, where the base consists of a set of concept inclusions instead of implications. In this setting, concept expressions can be arbitrarily large. Thus, it is not clear whether a finite base exists and, if so, how large concept expressions may need to be. We first revisit results in the literature for mining ℰℒ⊥ bases from finite interpretations. Those mainly focus on finding a finite base or on fixing the role depth but potentially losing some of the valid concept inclusions with higher role depth. We then present a new strategy for mining ℰℒ⊥ bases which is adaptable in the sense that it can bound the role depth of concepts depending on the local structure of the interpretation. Our strategy guarantees to capture all ℰℒ⊥ concept inclusions holding in the interpretation, not only the ones up to a fixed role depth. We also consider the case of confident ℰℒ⊥ bases, which requires that some proportion of the domain of the interpretation satisfies the base, instead of the whole domain. This case is useful to cope with noisy data.},
  archive      = {J_JAIR},
  author       = {Ricardo Guimarães and Ana Ozaki and Cosimo Persia and Baris Sertkaya},
  doi          = {10.1613/jair.1.13777},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {883-924},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Mining ℰℒ⊥ bases with adaptable role depth},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing the implicit model selection tradeoff.
<em>JAIR</em>, <em>76</em>, 829–881. (<a
href="https://doi.org/10.1613/jair.1.13764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent rise of machine learning (ML) has been leveraged by practitioners and researchers to provide new solutions to an ever growing number of business problems. As with other ML applications, these solutions rely on model selection, which is typically achieved by evaluating certain metrics on models separately and selecting the model whose evaluations (i.e., accuracy-related loss and/or certain interpretability measures) are optimal. However, empirical evidence suggests that, in practice, multiple models often attain competitive results. Therefore, while models’ overall performance could be similar, they could operate quite differently. This results in an implicit tradeoff in models’ performance throughout the feature space which resolving requires new model selection tools. This paper explores methods for comparing predictive models in an interpretable manner to uncover the tradeoff and help resolve it. To this end, we propose various methods that synthesize ideas from supervised learning, unsupervised learning, dimensionality reduction, and visualization to demonstrate how they can be used to inform model developers about the model selection process. Using various datasets and a simple Python interface, we demonstrate how practitioners and researchers could benefit from applying these approaches to better understand the broader impact of their model selection choices.},
  archive      = {J_JAIR},
  author       = {Zezhen He and Yaron Shaposhnik},
  doi          = {10.1613/jair.1.13764},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {829-881},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Visualizing the implicit model selection tradeoff},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reviewer assignment problem: A systematic review of the
literature. <em>JAIR</em>, <em>76</em>, 761–827. (<a
href="https://doi.org/10.1613/jair.1.14318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate reviewer assignment significantly impacts the quality of proposal evaluation, as accurate and fair reviews are contingent on their assignment to relevant reviewers. The crucial task of assigning reviewers to submitted proposals is the starting point of the review process and is also known as the reviewer assignment problem (RAP). Due to the obvious restrictions of manual assignment, journal editors, conference organizers, and grant managers demand automatic reviewer assignment approaches. Many studies have proposed assignment solutions in response to the demand for automated procedures since 1992. The primary objective of this survey paper is to provide scholars and practitioners with a comprehensive overview of available research on the RAP. To achieve this goal, this article presents an in-depth systematic review of 103 publications in the field of reviewer assignment published in the past three decades and available in the Web of Science, Scopus, ScienceDirect, Google Scholar, and Semantic Scholar databases. This review paper classified and discussed the RAP approaches into two broad categories and numerous subcategories based on their underlying techniques. Furthermore, potential future research directions for each category are presented. This survey shows that the research on the RAP is becoming more significant and that more effort is required to develop new approaches and a framework.},
  archive      = {J_JAIR},
  author       = {Meltem Aksoy and Seda Yanik and Mehmet Fatih Amasyali},
  doi          = {10.1613/jair.1.14318},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {761-827},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Reviewer assignment problem: A systematic review of the literature},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the complexity of finding set repairs for data-graphs.
<em>JAIR</em>, <em>76</em>, 721–759. (<a
href="https://doi.org/10.1613/jair.1.13994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the deeply interconnected world we live in, pieces of information link domains all around us. As graph databases embrace effectively relationships among data and allow processing and querying these connections efficiently, they are rapidly becoming a popular platform for storage that supports a wide range of domains and applications. As in the relational case, it is expected that data preserves a set of integrity constraints that define the semantic structure of the world it represents. When a database does not satisfy its integrity constraints, a possible approach is to search for a ‘similar’ database that does satisfy the constraints, also known as a repair. In this work, we study the problem of computing subset and superset repairs for graph databases with data values using a notion of consistency based on having a set of Reg-GXPath expressions as integrity constraints. We show that for positive fragments of Reg-GXPath these problems admit a polynomial-time algorithm, while the full expressive power of the language renders them intractable.},
  archive      = {J_JAIR},
  author       = {Sergio Abriola and María Vanina Martínez and Nina Pardal and Santiago Cifuentes and Edwin Pin Baque},
  doi          = {10.1613/jair.1.13994},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {721-759},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the complexity of finding set repairs for data-graphs},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the evaluation of (meta-)solver approaches.
<em>JAIR</em>, <em>76</em>, 705–719. (<a
href="https://doi.org/10.1613/jair.1.14102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-solver approaches exploit many individual solvers to potentially build a better solver. To assess the performance of meta-solvers, one can adopt the metrics typically used for individual solvers (e.g., runtime or solution quality) or employ more specific evaluation metrics (e.g., by measuring how close the meta-solver gets to its virtual best performance). In this paper, based on some recently published works, we provide an overview of different performance metrics for evaluating (meta-)solvers by exposing their strengths and weaknesses.},
  archive      = {J_JAIR},
  author       = {Roberto Amadini and Maurizio Gabbrielli and Tong Liu and Jacopo Mauro},
  doi          = {10.1613/jair.1.14102},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {705-719},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the evaluation of (Meta-)solver approaches},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deciding FO-rewritability of regular languages and
ontology-mediated queries in linear temporal logic. <em>JAIR</em>,
<em>76</em>, 645–703. (<a
href="https://doi.org/10.1613/jair.1.14061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our concern is the problem of determining the data complexity of answering an ontology-mediated query (OMQ) formulated in linear temporal logic LTL over (Z,&amp;lt;) and deciding whether it is rewritable to an FO(&amp;lt;)-query, possibly with some extra predicates. First, we observe that, in line with the circuit complexity and FO-definability of regular languages, OMQ answering in AC0, ACC0 and NC1 coincides with FO(&amp;lt;,≡)-rewritability using unary predicates x ≡ 0 (mod n), FO(&amp;lt;,MOD)-rewritability, and FO(RPR)-rewritability using relational primitive recursion, respectively. We prove that, similarly to known PSᴘᴀᴄᴇ-completeness of recognising FO(&amp;lt;)-definability of regular languages, deciding FO(&amp;lt;,≡)- and FO(&amp;lt;,MOD)-definability is also PSᴘᴀᴄᴇ-complete (unless ACC0 = NC1). We then use this result to show that deciding FO(&amp;lt;)-, FO(&amp;lt;,≡)- and FO(&amp;lt;,MOD)-rewritability of LTL OMQs is ExᴘSᴘᴀᴄᴇ-complete, and that these problems become PSᴘᴀᴄᴇ-complete for OMQs with a linear Horn ontology and an atomic query, and also a positive query in the cases of FO(&amp;lt;)- and FO(&amp;lt;,≡)-rewritability. Further, we consider FO(&amp;lt;)-rewritability of OMQs with a binary-clause ontology and identify OMQ classes, for which deciding it is PSᴘᴀᴄᴇ-, Π2p- and coNP-complete.},
  archive      = {J_JAIR},
  author       = {Agi Kurucz and Vladislav Ryzhikov and Yury Savateev and Michael Zakharyaschev},
  doi          = {10.1613/jair.1.14061},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {645-703},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Deciding FO-rewritability of regular languages and ontology-mediated queries in linear temporal logic},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Liability regimes in the age of AI: A use-case driven
analysis of the burden of proof. <em>JAIR</em>, <em>76</em>, 613–644.
(<a href="https://doi.org/10.1613/jair.1.14565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New emerging technologies powered by Artificial Intelligence (AI) have the potential to disruptively transform our societies for the better. In particular, data-driven learning approaches (i.e., Machine Learning (ML)) have been a true revolution in the advancement of multiple technologies in various application domains. But at the same time there is growing concern about certain intrinsic characteristics of these methodologies that carry potential risks to both safety and fundamental rights. Although there are mechanisms in the adoption process to minimize these risks (e.g., safety regulations), these do not exclude the possibility of harm occurring, and if this happens, victims should be able to seek compensation. Liability regimes will therefore play a key role in ensuring basic protection for victims using or interacting with these systems. However, the same characteristics that make AI systems inherently risky, such as lack of causality, opacity, unpredictability or their self and continuous learning capabilities, may lead to considerable difficulties when it comes to proving causation. This paper presents three case studies, as well as the methodology to reach them, that illustrate these difficulties. Specifically, we address the cases of cleaning robots, delivery drones and robots in education. The outcome of the proposed analysis suggests the need to revise liability regimes to alleviate the burden of proof on victims in cases involving AI technologies. This article appears in the AI &amp;amp; Society track.},
  archive      = {J_JAIR},
  author       = {David Fernández Llorca and Vicky Charisi and Ronan Hamon and Ignacio Sánchez and Emilia Gómez},
  doi          = {10.1613/jair.1.14565},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {613-644},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Liability regimes in the age of AI: A use-case driven analysis of the burden of proof},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On fair division under heterogeneous matroid constraints.
<em>JAIR</em>, <em>76</em>, 567–611. (<a
href="https://doi.org/10.1613/jair.1.13779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study fair allocation of indivisible goods among additive agents with feasibility constraints. In these settings, every agent is restricted to get a bundle among a specified set of feasible bundles. Such scenarios have been of great interest to the AI community due to their applicability to real-world problems. Following some impossibility results, we restrict attention to matroid feasibility constraints that capture natural scenarios, such as the allocation of shifts to medical doctors and the allocation of conference papers to referees. We focus on the common fairness notion of envy-freeness up to one good (EF1). Previous algorithms for finding EF1 allocations are either restricted to agents with identical feasibility constraints or allow free disposal of items. An open problem is the existence of EF1 complete allocations among agents who differ both in their valuations and in their feasibility constraints. In this work, we make progress on this problem by providing positive and negative results for several matroid and valuation types. Among other results, we devise polynomial-time algorithms for finding EF1 allocations in the following settings: (i) n agents with heterogeneous (non-identical) binary valuations and partition matroids with heterogeneous capacities; (ii) two agents with heterogeneous additive valuations and partition matroids with heterogeneous capacities; and (iii) three agents with heterogeneous binary valuations and identical base-orderable matroid constraints.},
  archive      = {J_JAIR},
  author       = {Amitay Dror and Michal Feldman and Erel Segal-Halevi},
  doi          = {10.1613/jair.1.13779},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {567–611},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On fair division under heterogeneous matroid constraints},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A logic of east and west. <em>JAIR</em>, <em>76</em>,
527–565. (<a href="https://doi.org/10.1613/jair.1.14113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a logic of east and west (LEW ) for points in 1D Euclidean space. It formalises primitive direction relations: east (E), west (W) and indeterminate east/west (Iew). It has a parameter τ ∈ N&amp;gt;1, which is referred to as the level of indeterminacy in directions. For every τ ∈ N&amp;gt;1, we provide a sound and complete axiomatisation of LEW , and prove that its satisfiability problem is NP-complete. In addition, we show that the finite axiomatisability of LEW depends on τ : if τ = 2 or τ = 3, then there exists a finite sound and complete axiomatisation; if τ &amp;gt; 3, then the logic is not finitely axiomatisable. LEW can be easily extended to higher-dimensional Euclidean spaces. Extending LEW to 2D Euclidean space makes it suitable for reasoning about not perfectly aligned representations of the same spatial objects in different datasets, for example, in crowd-sourced digital maps.},
  archive      = {J_JAIR},
  author       = {Heshan Du and Natasha Alechina and Amin Farjudian and Brian Logan and Can Zhou and Anthony G. Cohn},
  doi          = {10.1613/jair.1.14113},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {527-565},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A logic of east and west},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introduction to the special track on artificial intelligence
and COVID-19. <em>JAIR</em>, <em>76</em>, 523–525. (<a
href="https://doi.org/10.1613/jair.1.14552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human race is facing one of the most meaningful public health emergencies in the modern era caused by the COVID-19 pandemic. This pandemic introduced various challenges, from lock-downs with significant economic costs to fundamentally altering the way of life for many people around the world. The battle to understand and control the virus is still at its early stages yet meaningful insights have already been made. The uncertainty of why some patients are infected and experience severe symptoms, while others are infected but asymptomatic, and others are not infected at all, makes managing this pandemic very challenging. Furthermore, the development of treatments and vaccines relies on knowledge generated from an ever evolving and expanding information space. Given the availability of digital data in the modern era, artificial intelligence (AI) is a meaningful tool for addressing the various challenges introduced by this unexpected pandemic. Some of the challenges include: outbreak prediction, risk modeling including infection and symptom development, testing strategy optimization, drug development, treatment repurposing, vaccine development, and others.},
  archive      = {J_JAIR},
  author       = {Martin Michalowski and Robert Moskovitch and Nitesh V. Chawla},
  doi          = {10.1613/jair.1.14552},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {523-525},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Introduction to the special track on artificial intelligence and COVID-19},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite materialisability of datalog programs with metric
temporal operators. <em>JAIR</em>, <em>76</em>, 471–521. (<a
href="https://doi.org/10.1613/jair.1.14040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DatalogMTL is an extension of Datalog with metric temporal operators that has recently found applications in stream reasoning and temporal ontology-based data access. In contrast to plain Datalog, where materialisation (a.k.a. forward chaining) naturally terminates in finitely many steps, reaching a fixpoint in DatalogMTL may require infinitely many rounds of rule applications. As a result, existing reasoning systems resort to other approaches, such as constructing large Büchi automata, whose implementations turn out to be highly inefficient in practice. In this paper, we propose and study finitely materialisable DatalogMTL programs, for which forward chaining reasoning is guaranteed to terminate. We consider a data-dependent notion of finite materialisability of a program, where termination is guaranteed for a given dataset, as well as a data-independent notion, where termination is guaranteed regardless of the dataset. We show that, for bounded programs (a natural DatalogMTL fragment for which reasoning is as hard as in the full language), checking data-dependent finite materialisability is ExpSpace-complete in combined complexity and PSpace-complete in data complexity; furthermore, we propose a practical materialisation-based decision procedure that works in doubly exponential time. We show that checking data-independent finite materialisability for bounded progams is computationally easier, namely ExpTime-complete; moreover, we propose sufficient conditions for data-indenpendent finite materialisability that can be efficiently checked. We provide also the complexity landscape of fact entailment for different classes of finitely materialisable programs; surprisingly, we could identify a large class of finitely materialisable programs, called MTL-acyclic programs, for which fact entailment has exactly the same data and combined complexity as in plain Datalog, which makes this fragment especially well suited for big-scale applications.},
  archive      = {J_JAIR},
  author       = {Przemysław Wałęga and Michał Zawidzki and Bernardo Cuenca Grau},
  doi          = {10.1613/jair.1.14040},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {471–521},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Finite materialisability of datalog programs with metric temporal operators},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating random SAT instances: Multiple solutions could be
predefined and deeply hidden. <em>JAIR</em>, <em>76</em>, 435–470. (<a
href="https://doi.org/10.1613/jair.1.13909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of SAT instances is an important issue in computer science, and it is useful for researchers to verify the effectiveness of SAT solvers. Addressing this issue could inspire researchers to propose new search strategies. SAT problems exist in various real-world applications, some of which have more than one solution. However, although several algorithms for generating random SAT instances have been proposed, few can be used to generate hard instances that have multiple predefined solutions. In this paper, we propose the KHidden-M algorithm to generate SAT instances with multiple predefined solutions that could be hard to solve by the local search strategy when the number of predefined solutions is small enough and the Hamming distance between them is not less than half of the solution length. Specifically, first, we generate an SAT instance that is satisfied by all of the predefined solutions. Next, if the generated SAT instance does not satisfy the hardness condition, then a strategy will be conducted to adjust clauses through multiple iterations to improve the hardness of the whole instance. We propose three strategies to generate the SAT instance in the first part. The first strategy is called the random strategy, which randomly generates clauses that are satisfied by all of the predefined solutions. The other two strategies are called the estimating strategy and greedy strategy, and using them, we attempt to generate an instance that directly satisfies or is closer to the hardness condition for the local search strategy. We employ two SAT solvers (i.e., WalkSAT and Kissat) to investigate the hardness of the SAT instances generated by our algorithm in the experiments. The experimental results show the effectiveness of the random, estimating and greedy strategies. Compared to the state-of-the-art algorithm for generating SAT instances with predefined solutions, namely, M-hidden, our algorithm could be more effective in generating hard SAT instances.},
  archive      = {J_JAIR},
  author       = {Dongdong Zhao and Lei Liao and Wenjian Luo and Jianwen Xiang and Hao Jiang and Xiaoyi Hu},
  doi          = {10.1613/jair.1.13909},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {435-470},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Generating random SAT instances: Multiple solutions could be predefined and deeply hidden},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed bayesian: A continuous distributed constraint
optimization problem solver. <em>JAIR</em>, <em>76</em>, 393–433. (<a
href="https://doi.org/10.1613/jair.1.14151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the novel Distributed Bayesian (D-Bay) algorithm is presented for solving multi-agent problems within the Continuous Distributed Constraint Optimization Problem (C-DCOP) framework. This framework extends the classical DCOP framework towards utility functions with continuous domains. D-Bay solves a C-DCOP by utilizing Bayesian optimization for the adaptive sampling of variables. We theoretically show that D-Bay converges to the global optimum of the C-DCOP for Lipschitz continuous utility functions. The performance of the algorithm is evaluated empirically based on the sample efficiency. The proposed algorithm is compared to state-of-the-art DCOP and C-DCOP solvers. The algorithm generates better solutions while requiring fewer samples.},
  archive      = {J_JAIR},
  author       = {Jeroen Fransman and Joris Sijs and Henry Dol and Erik Theunissen and Bart De Schutter},
  doi          = {10.1613/jair.1.14151},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {393-433},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Distributed bayesian: A continuous distributed constraint optimization problem solver},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust control for dynamical systems with non-gaussian noise
via formal abstractions. <em>JAIR</em>, <em>76</em>, 341–391. (<a
href="https://doi.org/10.1613/jair.1.14253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllers for dynamical systems that operate in safety-critical settings must account for stochastic disturbances. Such disturbances are often modeled as process noise in a dynamical system, and common assumptions are that the underlying distributions are known and/or Gaussian. In practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. We present a novel controller synthesis method that does not rely on any explicit representation of the noise distributions. In particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target, while also avoiding unsafe regions of the state space. First, we abstract the continuous control system into a finite-state model that captures noise by probabilistic transitions between discrete states. As a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (PAC) bounds on these transition probabilities, based on a finite number of samples of the noise. We capture these bounds in the transition probability intervals of a so-called interval Markov decision process (iMDP). This iMDP is, with a user-specified confidence probability, robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. We use state-of-the-art verification techniques to provide guarantees on the iMDP and compute a controller for which these guarantees carry over to the original control system. In addition, we develop a tailored computational scheme that reduces the complexity of the synthesis of these guarantees on the iMDP. Benchmarks on realistic control systems show the practical applicability of our method, even when the iMDP has hundreds of millions of transitions.},
  archive      = {J_JAIR},
  author       = {Thom Badings and Licio Romao and Alessandro Abate and David Parker and Hasan A. Poonawala and Marielle Stoelinga and Nils Jansen},
  doi          = {10.1613/jair.1.14253},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {341-391},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Robust control for dynamical systems with non-gaussian noise via formal abstractions},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Favoring eagerness for remaining items: Designing efficient,
fair, and strategyproof mechanisms. <em>JAIR</em>, <em>76</em>, 287–339.
(<a href="https://doi.org/10.1613/jair.1.13878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the assignment problem, the goal is to assign indivisible items to agents who have ordinal preferences, efficiently and fairly, in a strategyproof manner. In practice, first-choice maximality, i.e., assigning a maximal number of agents their top items, is often identified as an important efficiency criterion and measure of agents&#39; satisfaction. In this paper, we propose a natural and intuitive efficiency property, favoring-eagerness-for-remaining-items (FERI), which requires that each item is allocated to an agent who ranks it highest among remaining items, thereby implying first-choice maximality. Using FERI as a heuristic, we design mechanisms that satisfy ex-post or ex-ante variants of FERI together with combinations of other desirable properties of efficiency (Pareto-efficiency), fairness (strong equal treatment of equals and sd-weak-envy-freeness), and strategyproofness (sd-weak-strategyproofness). We also explore the limits of FERI mechanisms in providing stronger efficiency, fairness, or strategyproofness guarantees through impossibility results.},
  archive      = {J_JAIR},
  author       = {Xiaoxi Guo and Sujoy Sikdar and Lirong Xia and Yongzhi Cao and Hanpin Wang},
  doi          = {10.1613/jair.1.13878},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {287-339},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Favoring eagerness for remaining items: Designing efficient, fair, and strategyproof mechanisms},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing tseitin-formulas with short regular
resolution refutations. <em>JAIR</em>, <em>76</em>, 265–286. (<a
href="https://doi.org/10.1613/jair.1.13521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tseitin-formulas are systems of parity constraints whose structure is described by a graph. These formulas have been studied extensively in proof complexity as hard instances in many proof systems. In this paper, we prove that a class of unsatisfiable Tseitin-formulas of bounded degree has regular resolution refutations of polynomial length if and only if the treewidth of all underlying graphs G for that class is in O(log |V (G)|). It follows that unsatisfiable Tseitin-formulas with polynomial length of regular resolution refutations are completely determined by the treewidth of the underlying graphs when these graphs have bounded degree. To prove this, we show that any regular resolution refutation of an unsatisfiable Tseitin-formula with graph G of bounded degree has length 2Ω(tw(G))/|V (G)|, thus essentially matching the known 2O(tw(G))poly(|V (G)|) upper bound. Our proof first connects the length of regular resolution refutations of unsatisfiable Tseitin-formulas to the size of representations of satisfiable Tseitin-formulas in decomposable negation normal form (DNNF). Then we prove that for every graph G of bounded degree, every DNNF-representation of every satisfiable Tseitin-formula with graph G must have size 2Ω(tw(G)) which yields our lower bound for regular resolution.},
  archive      = {J_JAIR},
  author       = {Alexis de Colnet and Stefan Mengel},
  doi          = {10.1613/jair.1.13521},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {265-286},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Characterizing tseitin-formulas with short regular resolution refutations},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of zero-shot generalisation in deep reinforcement
learning. <em>JAIR</em>, <em>76</em>, 201–264. (<a
href="https://doi.org/10.1613/jair.1.14174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We rely on a unifying formalism and terminology for discussing different ZSG problems, building upon previous works. We go on to categorise existing benchmarks for ZSG, as well as current methods for tackling these problems. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG, and we recommend building benchmarks in underexplored problem settings such as offline RL ZSG and reward-function variation.},
  archive      = {J_JAIR},
  author       = {Robert Kirk and Amy Zhang and Edward Grefenstette and Tim Rocktäschel},
  doi          = {10.1613/jair.1.14174},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {201-264},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey of zero-shot generalisation in deep reinforcement learning},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Viewpoint: Artificial intelligence accidents waiting to
happen? <em>JAIR</em>, <em>76</em>, 193–199. (<a
href="https://doi.org/10.1613/jair.1.14263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is at a crucial point in its development: stable enough to be used in production systems, and increasingly pervasive in our lives. What does that mean for its safety? In his book Normal Accidents, the sociologist Charles Perrow proposed a framework to analyze new technologies and the risks they entail. He showed that major accidents are nearly unavoidable in complex systems with tightly coupled components if they are run long enough. In this essay, we apply and extend Perrow’s framework to AI to assess its potential risks. Today’s AI systems are already highly complex, and their complexity is steadily increasing. As they become more ubiquitous, different algorithms will interact directly, leading to tightly coupled systems whose capacity to cause harm we will be unable to predict. We argue that under the current paradigm, Perrow’s normal accidents apply to AI systems and it is only a matter of time before one occurs. This article appears in the AI &amp;amp; Society track.},
  archive      = {J_JAIR},
  author       = {Federico Bianchi and Amanda Cercas Curry and Dirk Hovy},
  doi          = {10.1613/jair.1.14263},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {193-199},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Viewpoint: Artificial intelligence accidents waiting to happen?},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on understanding and representing privacy
requirements in the internet-of-things. <em>JAIR</em>, <em>76</em>,
163–192. (<a href="https://doi.org/10.1613/jair.1.14000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People are interacting with online systems all the time. In order to use the services being provided, they give consent for their data to be collected. This approach requires too much human effort and is impractical for systems like Internet-of-Things (IoT) where human-device interactions can be large. Ideally, privacy assistants can help humans make privacy decisions while working in collaboration with them. In our work, we focus on the identification and representation of privacy requirements in IoT to help privacy assistants better understand their environment. In recent years, more focus has been on the technical aspects of privacy. However, the dynamic nature of privacy also requires a representation of social aspects (e.g., social trust). In this survey paper, we review the privacy requirements represented in existing IoT ontologies. We discuss how to extend these ontologies with new requirements to better capture privacy, and we introduce case studies to demonstrate the applicability of the novel requirements.},
  archive      = {J_JAIR},
  author       = {Gideon Ogunniye and Nadin Kokciyan},
  doi          = {10.1613/jair.1.14000},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {163-192},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey on understanding and representing privacy requirements in the internet-of-things},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A practical approach to discretised PDDL+ problems by
translation to numeric planning. <em>JAIR</em>, <em>76</em>, 115–162.
(<a href="https://doi.org/10.1613/jair.1.13904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PDDL+ models are advanced models of hybrid systems and the resulting problems are notoriously difficult for planning engines to cope with. An additional limiting factor for the exploitation of PDDL+ approaches in real-world applications is the restricted number of domain-independent planning engines that can reason upon those models. With the aim of deepening the understanding of PDDL+ models, in this work, we study a novel mapping between a time discretisation of pddl+ and numeric planning as for PDDL2.1 (level 2). The proposed mapping not only clarifies the relationship between these two formalisms but also enables the use of a wider pool of engines, thus fostering the use of hybrid planning in real-world applications. Our experimental analysis shows the usefulness of the proposed translation and demonstrates the potential of the approach for improving the solvability of complex PDDL+ instances.},
  archive      = {J_JAIR},
  author       = {Francesco Percassi and Enrico Scala and Mauro Vallati},
  doi          = {10.1613/jair.1.13904},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {115-162},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A practical approach to discretised PDDL+ problems by translation to numeric planning},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific heuristics in answer set programming: A
declarative non-monotonic approach. <em>JAIR</em>, <em>76</em>, 59–114.
(<a href="https://doi.org/10.1613/jair.1.14091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific heuristics are an essential technique for solving combinatorial problems efficiently. Current approaches to integrate domain-specific heuristics with Answer Set Programming (ASP) are unsatisfactory when dealing with heuristics that are specified non-monotonically on the basis of partial assignments. Such heuristics frequently occur in practice, for example, when picking an item that has not yet been placed in bin packing. Therefore, we present novel syntax and semantics for declarative specifications of domain-specific heuristics in ASP. Our approach supports heuristic statements that depend on the partial assignment maintained during solving, which has not been possible before. We provide an implementation in Alpha that makes Alpha the first lazy-grounding ASP system to support declaratively specified domain-specific heuristics. Two practical example domains are used to demonstrate the benefits of our proposal. Additionally, we use our approach to implement informed search with A*, which is tackled within ASP for the first time. A* is applied to two further search problems. The experiments confirm that combining lazy-grounding ASP solving and our novel heuristics can be vital for solving industrial-size problems.},
  archive      = {J_JAIR},
  author       = {Richard Comploi-Taupe and Gerhard Friedrich and Konstantin Schekotihin and Antonius Weinzierl},
  doi          = {10.1613/jair.1.14091},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {59-114},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Domain-specific heuristics in answer set programming: A declarative non-monotonic approach},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifted reasoning for combinatorial counting. <em>JAIR</em>,
<em>76</em>, 1–58. (<a
href="https://doi.org/10.1613/jair.1.14062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorics math problems are often used as a benchmark to test human cognitive and logical problem-solving skills. These problems are concerned with counting the number of solutions that exist in a specific scenario that is sketched in natural language. Humans are adept at solving such problems as they can identify commonly occurring structures in the questions for which a closed-form formula exists for computing the answer. These formulas exploit the exchangeability of objects and symmetries to avoid a brute-force enumeration of all possible solutions. Unfortunately, current AI approaches are still unable to solve combinatorial problems in this way. This paper aims to fill this gap by developing novel AI techniques for representing and solving such problems. It makes the following five contributions. First, we identify a class of combinatorics math problems which traditional lifted counting techniques fail to model or solve efficiently. Second, we propose a novel declarative language for this class of problems. Third, we propose novel lifted solving algorithms bridging probabilistic inference techniques and constraint programming. Fourth, we implement them in a lifted solver that solves efficiently the class of problems under investigation. Finally, we evaluate our contributions on a real-world combinatorics math problems dataset and synthetic benchmarks.},
  archive      = {J_JAIR},
  author       = {Pietro Totis and Jesse Davis and Luc de Raedt and Angelika Kimmig},
  doi          = {10.1613/jair.1.14062},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-58},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Lifted reasoning for combinatorial counting},
  volume       = {76},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
