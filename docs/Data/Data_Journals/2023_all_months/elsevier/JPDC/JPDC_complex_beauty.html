<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JPDC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jpdc---125">JPDC - 125</h2>
<ul>
<li><details>
<summary>
(2023). Scheduling coflows for minimizing the total weighted
completion time in heterogeneous parallel networks. <em>JPDC</em>,
<em>182</em>, 104752. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coflow is a network abstraction used to represent communication patterns in data centers . The coflow scheduling problem encountered in large data centers is a challenging NP NP -hard problem. Many previous studies on coflow scheduling mainly focus on the single-core model. However, with the growth of data centers, this single-core model is no longer sufficient. This paper addresses the coflow scheduling problem within heterogeneous parallel networks, which feature an architecture consisting of multiple network cores running in parallel. In this paper, two polynomial-time approximation algorithms are developed for the flow-level scheduling problem and the coflow-level scheduling problem in heterogeneous parallel networks, respectively. For the flow-level scheduling problem, the proposed algorithm achieves an approximation ratio of O ( log ⁡ m / log ⁡ log ⁡ m ) O(log⁡m/log⁡log⁡m) when all coflows are released at arbitrary times, where m represents the number of network cores. On the other hand, in the coflow-level scheduling problem, the proposed algorithm achieves an approximation ratio of O ( m ( log ⁡ m / log ⁡ log ⁡ m ) 2 ) O(m(log⁡m/log⁡log⁡m)2) when all coflows are released at arbitrary times. Moreover, we propose a heuristic algorithm for the flow-level scheduling problem. Simulation results using synthetic traffic traces validate the performance of our algorithms and show improvements over the previous algorithm.},
  archive      = {J_JPDC},
  author       = {Chi-Yeh Chen},
  doi          = {10.1016/j.jpdc.2023.104752},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104752},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scheduling coflows for minimizing the total weighted completion time in heterogeneous parallel networks},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RATS: A regulatory anonymous transaction system based on
blockchain. <em>JPDC</em>, <em>182</em>, 104751. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of digital currency such as Bitcoin , the digital currency transaction system with blockchain as the key underlying technology is booming, but the traditional transaction system has the risk of revealing sensitive information such as transaction content and identity. In order to enhance the security, many transactions system with privacy preserving have been proposed, such as Zerocoin, Monero , etc. As there are a decentralized credit entity and strong anonymity in these transaction systems, the authorities can effectively audit and control the participants and transactions in the digital currency system, thus making digital currency a tool for illegal transactions. Recognizing the importance of privacy preserving and regulatory, in this paper, we propose a new regulatory anonymous transaction system based on blockchain—RATS, which can not only protect the privacy of transactions on the blockchain , but also regulate illegal transactions. Firstly, we introduce regulators into the system, and at the same time, we propose a regulatory mechanism that the regulator is allowed to trace the identity of the user without affecting the transaction operation of the system when suspicious transactions are found. In particularly, during the normal transaction, we protect the privacy of users&#39; transaction contents and addresses without affecting the anonymity of the original system. We formalize the system model and security model of RATS. Moreover, the security of the proposed scheme is strictly proved and analyzed.},
  archive      = {J_JPDC},
  author       = {Ming Luo and Jie Zhou and Ping Yang},
  doi          = {10.1016/j.jpdc.2023.104751},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104751},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {RATS: A regulatory anonymous transaction system based on blockchain},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A massively parallel implicit 3D unstructured grid solver
for computing turbulent flows on latest distributed memory computational
architectures. <em>JPDC</em>, <em>182</em>, 104750. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An implicit unstructured grid density-based solver– PRAVAHA based on a parallel variant of the lower upper symmetric Gauss-Seidel (LUSGS) method is developed to compute large-scale engineering problems. A four-layered parallel algorithm is designed to efficiently compute three-dimensional turbulent flows on massively parallel modern multiple instruction multiple data-stream (MIMD) computational hardware. This data parallel approach achieves multiple layers of parallelism including continuity of flow solution, transfer of solution gradients, and calculation of drag/lift/solution residuals, right up to the innermost implicit LUSGS solver sub-routine, which is relatively less explored in the literature. Domain decomposition is performed using the METIS software based on multi-level graph partitioning algorithms. Non-blocking message-passing interface library functions are used to manage inter-processor communication through explicit message passing, efficiently. Super-linear scalability of the parallel solver is established on the current state-of-the-art supercomputing facility, the 838 teraflops PARAM seva on up to 6144 cores. Linear or even super-linear speedup on problems of significant size is observed even on ad-hoc parallel computing platforms like workstations and multi-node clusters, for turbulent flow simulations.},
  archive      = {J_JPDC},
  author       = {M.R. Nived and Vinayak Eswaran},
  doi          = {10.1016/j.jpdc.2023.104750},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104750},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A massively parallel implicit 3D unstructured grid solver for computing turbulent flows on latest distributed memory computational architectures},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). K-CSqu: Ensuring connected k-coverage using cusp squares of
square tessellation. <em>JPDC</em>, <em>182</em>, 104749. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In planar wireless sensor networks (PWSNs), the most essential functionalities of the sensor nodes are both sensing and communication, which are evaluated using two fundamental concepts, namely coverage and connectivity, respectively. However, coverage alone is not sufficient for the correct operation of PWSNs. Additionally, it is important that network connectivity be ensured, where all the sensors are connected to one another, so that every pair of sensors can communicate with each other. To account for both coverage and connectivity, this paper aims at solving the problem of connected k -coverage in PWSNs, where every point in a field of interest is covered by at least k sensors ( k &gt; 1 k&amp;gt;1 ) at the same time, while all the sensors are mutually connected directly or indirectly. In order to solve this problem, we initially tessellate the entire field into adjacent and non-intersecting square tiles. Then, we construct a cusp-square inside each square tile of the tessellation for sensor placement. Based on this cusp-squared square tile, we compute the minimum planar sensor density for k -coverage in PWSNs. Also, we establish a relationship between the sensing and communication radii of the sensors to guarantee network connectivity in PWSNs. Finally, we validate our theoretical analysis using simulation results.},
  archive      = {J_JPDC},
  author       = {Kalyan Nakka and Habib M. Ammari},
  doi          = {10.1016/j.jpdc.2023.104749},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104749},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {K-CSqu: Ensuring connected k-coverage using cusp squares of square tessellation},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Atomic appends in asynchronous byzantine distributed
ledgers. <em>JPDC</em>, <em>182</em>, 104748. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Distributed Ledger Object (DLO) is a concurrent object that maintains a totally ordered sequence of records. In this work we formalize a linearizable Byzantine-tolerant Distributed Ledger Object (BDLO), which is a linearizable DLO where clients and servers processes may deviate arbitrarily from their intended behavior (i.e. they may be Byzantine). The proposed formal definition is accompanied by algorithms that implement BDLOs on top of an underlying Byzantine Atomic Broadcast service. Then we develop a suite of algorithms, based on the previous BDLO implementations, that solve the Atomic Appends problem in the presence of asynchrony, Byzantine clients and Byzantine servers. This problem occurs when clients have a composite record (set of basic records) to append to different BDLOs, in such a way that either each basic record is appended to its BDLO (and this must occur in good circumstances), or no basic record is appended. Distributed algorithms are presented, which solve the Atomic Appends problem when the clients (involved in the Atomic Appends ) and the servers (which maintain the BDLOs) may be Byzantine. Finally we provide proof of concept implementations and an experimental evaluation of the presented algorithms.},
  archive      = {J_JPDC},
  author       = {Vicent Cholvi and Antonio Fernández Anta and Chryssis Georgiou and Nicolas Nicolaou and Michel Raynal and Antonio Russo},
  doi          = {10.1016/j.jpdc.2023.104748},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104748},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Atomic appends in asynchronous byzantine distributed ledgers},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RASM: Resource-aware service migration in edge computing
based on deep reinforcement learning. <em>JPDC</em>, <em>182</em>,
104745. (<a href="https://doi.org/10.1016/j.jpdc.2023.104745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) paradigm allows devices to offload their intensive service tasks that require high Quality of Experience (QoE). Devices mobility forces services to migrate between MECs to maintain QoE in terms of delay. The decision on when to migrate a service requires a cost and QoE tradeoff, and destination MEC selection needs to be done upon latency and resource availability constraints to minimize migrations. To this end, we propose a novel Resource-Aware Service Migration (RASM) mechanism using Deep Q-Network (DQN) to make migration decisions by achieving tradeoff between the QoE in terms of delay and migration cost. Moreover, DQN learns the best policy for maximizing QoE by selecting the migration destination based on the MECs proximity to the device and estimated resource availability at the servers using queuing model . Results show faster convergence to optimal policy , reduced average end-to-end service delay by 27\%, and smaller service rejection rate by 24\% comparing to the state-of-the-art.},
  archive      = {J_JPDC},
  author       = {Lusungu Josh Mwasinga and Duc-Tai Le and Syed M. Raza and Rajesh Challa and Moonseong Kim and Hyunseung Choo},
  doi          = {10.1016/j.jpdc.2023.104745},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104745},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {RASM: Resource-aware service migration in edge computing based on deep reinforcement learning},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncovering i/o demands on HPC platforms: Peeking under the
hood of santos dumont. <em>JPDC</em>, <em>182</em>, 104744. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-Performance Computing (HPC) platforms are required to solve the most diverse large-scale scientific problems in various research areas, such as biology, chemistry, physics, and health sciences. Researchers use a multitude of scientific softwares, which have different requirements. These include input and output operations, which directly impact performance due to the existing difference in processing and data access speeds. Thus, supercomputers must efficiently handle mixed workload when storing data from the applications. Understanding the set of applications and their performance running in a supercomputer is paramount to understanding the storage system&#39;s usage, pinpointing possible bottlenecks, and guiding optimization techniques. This research proposes a methodology and visualization tool to evaluate a supercomputer&#39;s data storage infrastructure&#39;s performance, taking into account the diverse workload and demands of the system over a long period of operation. As a study case, we focus on the Santos Dumont supercomputer, identifying inefficient usage, problematic performance factors, and providing guidelines on how to tackle those issues.},
  archive      = {J_JPDC},
  author       = {André Ramos Carneiro and Jean Luca Bez and Carla Osthoff and Lucas Mello Schnorr and Philippe O.A. Navaux},
  doi          = {10.1016/j.jpdc.2023.104744},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104744},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Uncovering I/O demands on HPC platforms: Peeking under the hood of santos dumont},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel deep learning-based code clone detection model.
<em>JPDC</em>, <em>181</em>, 104747. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code clone detection is a crucial task in software development and maintenance. While deep learning-based methods have been proposed to tackle this problem, most of them neglect the time and memory consumption issues which can be significant when working with limited computational resources. Given the inability of recurrent neural networks to train in a parallel manner, this paper presents a parallel code clone detection model based on temporal convolutional networks . The proposed method splits the corresponding abstract syntax tree into a set of code statement sequences, utilizes a temporal convolutional neural network to generate representations containing complexity features found in the source code , and finally measures the distance between these representations. The proposed method is evaluated on a real-world dataset for code clone detection, and the experimental results demonstrate that it performs comparably to state-of-the-art methods while requiring significantly less time and memory costs.},
  archive      = {J_JPDC},
  author       = {Xiangping Zhang and Jianxun Liu and Min Shi},
  doi          = {10.1016/j.jpdc.2023.104747},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104747},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel deep learning-based code clone detection model},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed precision support in HPC applications: What about
reliability? <em>JPDC</em>, <em>181</em>, 104746. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In their quest for exascale and beyond, High-Performance Computing (HPC) systems continue becoming ever larger and more complex. Application developers, on the other hand, leverage novel methods to improve the efficiency of their own codes: a recent trend is the use of floating-point mixed precision, or the careful interlocking of single- and double-precision arithmetic, as a tool to improve performance as well as reduce network and memory boundedness . However, while it is known that modern HPC systems suffer hardware faults at daily rates, the impact of reduced precision on application reliability is yet to be explored. In this work we aim to fill this gap: first, we propose a qualitative survey to identify the branches of HPC where mixed precision is most popular. Second, we show the results of instruction-level fault injection experiments on a variety of representative HPC workloads, comparing vulnerability to Silent Data Errors (SDEs) under different numerical configurations. Our experiments indicate that use of single and mixed precision leads to comparatively more frequent and more severe SDEs, with concerning implications regarding their use on extreme-scale, fault-prone HPC platforms.},
  archive      = {J_JPDC},
  author       = {Alessio Netti and Yang Peng and Patrik Omland and Michael Paulitsch and Jorge Parra and Gustavo Espinosa and Udit Agarwal and Abraham Chan and Karthik Pattabiraman},
  doi          = {10.1016/j.jpdc.2023.104746},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104746},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Mixed precision support in HPC applications: What about reliability?},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). As easy as ABC: Optimal (a)ccountable (b)yzantine
(c)onsensus is easy! <em>JPDC</em>, <em>181</em>, 104743. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a non-synchronous system with n processes, no t0 -resilient (deterministic or probabilistic) Byzantine consensus protocol can prevent a disagreement among correct processes if the number of faulty processes is ≥n−2t0 . Therefore, the community defined the accountable Byzantine consensus problem: the problem of (i) solving Byzantine consensus whenever possible (e.g., when the number of faulty processes does not exceed t0 ), and (ii) allowing correct processes to obtain proofs of culpability of n−2t0 faulty processes whenever a disagreement occurs. This paper presents ABC , a simple yet efficient transformation of any non-synchronous t0 -resilient (deterministic or probabilistic) Byzantine consensus protocol into its accountable counterpart. In the common case (up to t0 faults), ABC introduces an additive overhead of two communication rounds and O(n2) exchanged bits. Whenever they disagree, correct processes detect culprits by exchanging O(n3) messages, which we prove optimal. Lastly, ABC is not limited to Byzantine consensus: ABC provides accountability for other essential distributed problems (e.g., reliable and consistent broadcast).},
  archive      = {J_JPDC},
  author       = {Pierre Civit and Seth Gilbert and Vincent Gramoli and Rachid Guerraoui and Jovan Komatovic},
  doi          = {10.1016/j.jpdc.2023.104743},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104743},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {As easy as ABC: Optimal (A)ccountable (B)yzantine (C)onsensus is easy!},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A task processing efficiency improvement scheme based on
cloud-edge architecture in computationally intensive scenarios.
<em>JPDC</em>, <em>181</em>, 104742. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Cloud-Edge architecture combines low latency and high-performance computing, it has a wider range of application scenarios. Considering that, how to reasonably allocate tasks to the edge cloud or the center cloud and how to select appropriate virtual machines (VMs) for tasks are two key aspects affecting the efficiency of Cloud-Edge architecture. For the former, the ARDT algorithm is proposed, which dynamically adjusts the optimal threshold by predicting the number of real-time end devices. For the latter, the DBMR algorithm is designed based on the Max-Fit algorithm, in which a structure of the blocking table is put forward to reduce the time complexity of the original algorithm from O ( n 2 ) O(n2) to O ( n ( n − m ) ) O(n(n−m)) . Experimental results show that the ARDT-DBMR algorithm optimizes the average service time and task failure rate by 30\%-40\% compared with the baseline algorithms and other advanced algorithms and significantly ameliorates the problem of abnormal fluctuations in VM utilization in computationally intensive scenarios .},
  archive      = {J_JPDC},
  author       = {Jiahui Feng and Jingze Qi and Yuanning Liu and Liyan Dong and Zhen Liu},
  doi          = {10.1016/j.jpdc.2023.104742},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104742},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A task processing efficiency improvement scheme based on cloud-edge architecture in computationally intensive scenarios},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IAP-SpTV: An input-aware adaptive pipeline SpTV via GCN on
CPU-GPU. <em>JPDC</em>, <em>181</em>, 104741. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse tensor-times-vector (SpTV) is the core computation of tensor decomposition. Optimizing the computational performance of SpTV on CPU-GPU becomes a challenge due to the complexity of the non-zero element sparse distribution of the tensor. To solve this problem, we propose IAP-SpTV, an input-aware adaptive pipeline SpTV via Graph Convolutional Network (GCN) on CPU-GPU. We first design the hybrid tensor format (HTF) and explore the challenges of the HTF-based Pipeline SpTV algorithm. Second, we construct Slice-GCN to overcome the challenge of selecting a suitable format for each slice of HTF. Third, we construct an IAP-SpTV performance model for pipelining to achieve the maximum overlap between transfer and computation time during pipelining. Finally, we conduct experiments on two CPU-GPU platforms of different architectures to verify the correctness, effectiveness, and portability of IAP-SpTV. Overall, IAP-SpTV provides a significant performance improvement of about 24.85\% to 58.42\% compared to the state-of-the-art method.},
  archive      = {J_JPDC},
  author       = {Haotian Wang and Wangdong Yang and Rong Hu and Renqiu Ouyang and Kenli Li and Keqin Li},
  doi          = {10.1016/j.jpdc.2023.104741},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104741},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {IAP-SpTV: An input-aware adaptive pipeline SpTV via GCN on CPU-GPU},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HyperTP: A unified approach for live hypervisor replacement
in datacenters. <em>JPDC</em>, <em>181</em>, 104733. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintenance of virtualized datacenters is often needed for the purposes of introducing new features, fixing bugs or mitigating security problems. However, current maintenance methods are either highly disruptive to the operation of VMs, utilize large amounts of computing resources or require high development efforts. We build HyperTP , a generic framework which combines in a unified way two approaches: in-place server micro-reboot-based hypervisor transplant (noted InPlaceTP ) and live VM migration-based hypervisor transplant (noted MigrationTP ). HyperTP hinges on a VM state hierarchy for organizing different types of hypervisor memory states in terms of their relation to VM execution, and an Unified Intermediate State Representation that abstracts VM-relevant memory states between multiple different hypervisors. We describe our implementations of both approaches, including technical details of our UISR design and the transplant process. Our evaluation results show that HyperTP delivers satisfactory performance: (1) MigrationTP changes a VM&#39;s underlying hypervisor while taking the same time and impacting virtual machines (VMs) with the same performance degradation as normal live migration; and (2) InPlaceTP imposes minimal VM downtime , even under increasing number of VMs and memory sizes. Finally, we discuss how the combination of InPlaceTP and MigrationTP can be used to address the challenges of upgrading a hypervisor cluster, and to mitigate known unpatched hypervisor vulnerabilities.},
  archive      = {J_JPDC},
  author       = {Tu Dinh Ngoc and Boris Teabe and Alain Tchana and Gilles Muller and Daniel Hagimont},
  doi          = {10.1016/j.jpdc.2023.104733},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104733},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HyperTP: A unified approach for live hypervisor replacement in datacenters},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault-tolerant unicast using conditional local safe model in
the data center network BCube. <em>JPDC</em>, <em>181</em>, 104732. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential infrastructure to support cloud computing services , data center networks (DCNs) are primarily used for transmitting and storing data. BCube is a large-scale DCN architecture that can effectively handle the massive data generated by network end devices due to the explosive growth of the Internet. However, as server failures in DCNs become more frequent, ensuring reliable data communication in BCube is critical. In this paper, we first establish a conditional local safe model in BCube, which divides the fault-free nodes in sub-BCube by adding some restrictions, and effectively avoids communication obstacles that may arise from faulty nodes. Then, based on the proposed model, a fault-tolerant unicast path algorithm is designed, which can construct a reliable data transmission path in the local safe sub-BCube of BCube and can tolerate more faulty nodes than existing algorithms. Finally, we conduct simulation experiments to verify that our algorithm can construct the shortest path with a high probability and achieve almost complete success in data transmission when the number of faulty nodes does not exceed half of the total nodes in BCube.},
  archive      = {J_JPDC},
  author       = {Hui Dong and Mengjie Lv and Huaqun Wang and Weibei Fan},
  doi          = {10.1016/j.jpdc.2023.104732},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104732},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fault-tolerant unicast using conditional local safe model in the data center network BCube},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel algorithm for constructing multiple independent
spanning trees in bubble-sort networks. <em>JPDC</em>, <em>181</em>,
104731. (<a href="https://doi.org/10.1016/j.jpdc.2023.104731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of multiple independent spanning trees (ISTs) for data broadcasting in networks provides a number of advantages, including the increase of fault-tolerance and secure message distribution. Thus, the designs of multiple ISTs on several classes of networks have been widely investigated. Kao et al. (2019) [18] proposed an algorithm to construct independent spanning trees in bubble-sort networks. The algorithm is executed in a recursive function and thus is hard to parallelize. In this paper, we focus on the problem of constructing ISTs in bubble-sort networks B n Bn and present a non-recursive algorithm. Our approach can be fully parallelized, i.e., every vertex can determine its parent in each spanning tree in constant time. This solves the open problem from the paper by Kao et al. Furthermore, we show that the total time complexity O ( n ⋅ n ! ) O(n⋅n!) of our algorithm is asymptotically optimal, where n is the dimension of B n Bn and n ! is the number of vertices of the network.},
  archive      = {J_JPDC},
  author       = {Shih-Shun Kao and Ralf Klasing and Ling-Ju Hung and Chia-Wei Lee and Sun-Yuan Hsieh},
  doi          = {10.1016/j.jpdc.2023.104731},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104731},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel algorithm for constructing multiple independent spanning trees in bubble-sort networks},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed approximate minimal steiner trees with millions
of seed vertices on billion-edge graphs. <em>JPDC</em>, <em>181</em>,
104717. (<a href="https://doi.org/10.1016/j.jpdc.2023.104717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a parallel 2-approximation Steiner minimal tree algorithm and its MPI-based distributed implementation. In place of expensive distance computations between all pairs of seed vertices, the solution we employ exploits a cheaper Voronoi cell computation. Our design leverages asynchronous processing and message prioritization to accelerate convergence of distance computations, and harnesses vertex and edge centric processing to offer fast time-to-solution. We demonstrate scalability and performance using real-world graphs with up to 128 billion edges and 512 compute nodes, and show the ability to find Steiner trees with up to one million seed vertices. Using 12 data instances, we present comparison with the state-of-the-art exact solver, SCIP-Jack, and two sequential 2-approximate algorithms. We empirically show that, on average, the total distance of the Steiner tree identified by our solution is 1.1290 times greater than the Steiner minimal tree – well within the theoretical approximation bound of 2.},
  archive      = {J_JPDC},
  author       = {Tahsin Reza and Trevor Steil and Geoffrey Sanders and Roger Pearce},
  doi          = {10.1016/j.jpdc.2023.104717},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104717},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed approximate minimal steiner trees with millions of seed vertices on billion-edge graphs},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Why blockchain needs graph: A survey on studies, scenarios,
and solutions. <em>JPDC</em>, <em>180</em>, 104730. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of blockchain platforms and their applications in industry and academia keeps rising. The multifarious requirements stimulate another technique, graph data and algorithms, to join the blockchains; thus, studies, scenarios, and solutions about graph-related blockchains have emerged. This paper aims to see whether the state-of-the-art studies satisfy the applications through a comprehensive survey on graph-related blockchains. To answer why a blockchain needs graphs in general, we analyze literature about blockchain and graph, as well as use cases on the application-oriented and graph-related scenarios collected from practical blockchain projects. The paper summarizes three graph-related blockchain studies: graph algorithms for blockchains, graph data in blockchains, and graph applications on blockchains. Based on these summarization, it figures out the gaps between the studies and the applications, that is, few of studies natively integrate graph computing into a blockchain. Here, the “graph integration” means processing on-chain graph data, which contains blockchain information, in a real-time, distributed, and consensual manner. We propose the prospect of the Graph-integrated Blockchain Platform (GiBP for short), and explain why a GiBP is inevitable, the challenges of a GiBP, and the GiBPs&#39; main functions and features people expect for future research.},
  archive      = {J_JPDC},
  author       = {Jie Song and Pengyi Zhang and Qiang Qu and Yongjie Bai and Yu Gu and Ge Yu},
  doi          = {10.1016/j.jpdc.2023.104730},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104730},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Why blockchain needs graph: A survey on studies, scenarios, and solutions},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient GPU-based method to compute high-order zernike
moments. <em>JPDC</em>, <em>180</em>, 104729. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of Zernike moments has been extensive in various fields, including image processing and pattern recognition, owing to their desirable characteristics. However, the application of Zernike moments is hindered by two significant obstacles: computational efficiency and accuracy. These issues become particularly noticeable when computing high-order moments. This study presents a novel GPU-based method for efficiently computing high-order Zernike moments by leveraging the computational power of the Single Instruction Multiple Data (SIMD) architecture. The experimental results demonstrate that the proposed method can compute Zernike moments up to order 500 within 0.5 seconds for an image of size 512 × 512 512×512 . To achieve greater accuracy in Zernike moments computation, a k × k k×k sub-region scheme was incorporated into our approach. The results show that the PSNR value of the Lena image reconstructed from 500-order Zernike moments computed using the 9 × 9 9×9 scheme can reach 39.20 dB.},
  archive      = {J_JPDC},
  author       = {Zhuohao Jia and Simon Liao},
  doi          = {10.1016/j.jpdc.2023.104729},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104729},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient GPU-based method to compute high-order zernike moments},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On distributed data aggregation and the precision of
approximate histograms. <em>JPDC</em>, <em>180</em>, 104722. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider selected aspects of distributed data aggregation in wireless sensor networks (WSNs), including random sampling, averaging, and the construction of histograms. We propose a novel algorithm for distributed random sampling based on the extrema propagation technique. We also consider the problem of construction of approximate histograms in WSNs and estimation of the average of sensory data and bin counts using these histograms. We present the results of theoretical analysis of the precision of estimators calculated from approximate histograms and propose a modification of the original method which allows for construction of approximate equi-depth histograms based on a random sample from data. The theoretical and experimental results show that the estimates calculated from approximate equi-depth histograms are more precise than those obtained using equi-width histograms. We also present an idea of an algorithm for a distributed variant of the Count Distinct problem, which uses the extrema propagation technique.},
  archive      = {J_JPDC},
  author       = {Karol Gotfryd and Jacek Cichoń},
  doi          = {10.1016/j.jpdc.2023.104722},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104722},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On distributed data aggregation and the precision of approximate histograms},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PRI: PCH-based privacy-preserving with reusability and
interoperability for enhancing blockchain scalability. <em>JPDC</em>,
<em>180</em>, 104721. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain systems, one of the most popular distributed systems, are well-applied in various scenarios, e.g., logistics and finance. However, traditional blockchain systems suffer from scalability issues. To tackle this issue, Payment Channel Hubs (PCHs) are proposed. Recent efforts, such as A2L (SP&#39;21) and Teechain (SOSP&#39;19), enhance the privacy, reusability , and interoperability properties of PCHs. Nevertheless, these solutions have intrinsic limitations: they rely on trusted hardware or suffer from the deposit lock-in problem. Furthermore, the functionalities of some of these solutions are restricted to fixed-amount payments and do not support multi-party participation. These aforementioned problems limit their capabilities to alleviate blockchain scalability issues. In this paper, we propose PRI, a novel PCH solution that simultaneously guarantees transaction P rivacy (i.e., relationship unlinkability and value confidentiality), deposit R eusability, and blockchain I nteroperability, which can mitigate the aforementioned problems. PRI is constructed by several new building blocks , including (1) an atomic deposit protocol that enforces user and hub to deposit equivalent assets in a shared address for building a fair payment channel; (2) a privacy-preserving deposit certification scheme that leverages the Pointcheval and Sanders signature and non-interactive zero-knowledge proof to resolve the deposit lock-in issue in maintaining payment channels; (3) a range proof which ensures the legality and confidentiality of transaction values. We conduct extensive experimental evaluations of PRI, demonstrating that it improves the state-of-the-art approaches in terms of performance.},
  archive      = {J_JPDC},
  author       = {Yuxian Li and Jian Weng and Wei Wu and Ming Li and Yingjiu Li and Haoxin Tu and Yongdong Wu and Robert H. Deng},
  doi          = {10.1016/j.jpdc.2023.104721},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104721},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PRI: PCH-based privacy-preserving with reusability and interoperability for enhancing blockchain scalability},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing performance and energy across problem sizes
through a search space exploration and machine learning. <em>JPDC</em>,
<em>180</em>, 104720. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HPC systems expose configuration options to assist optimization. Configurations such as parallelism , thread and data mapping, or prefetching have been explored but with a limited optimization objective (e.g., performance) and a fixed problem size. Unfortunately, efficient strategies in one scenario may poorly generalize when applied in new contexts. We investigate the impact of configuration options and different problem sizes over performance and energy. Well-adapted NUMA-related options and cache-prefetchers provide significantly more gains for energy (5.9×) than performance (1.85×). Moreover, reusing optimization strategies from performance to energy only provides 40\% of the gains found when natively optimizing for energy, while transferring strategies across problem sizes limits to 70\% of the original gains. We fill this gap with ML : simple decision trees predict the best configuration for a target size using only information collected on another size. Our models achieve 88\% of the native gains when cross-predicting across performance and energy, and 85\% across problem sizes.},
  archive      = {J_JPDC},
  author       = {Lana Scravaglieri and Mihail Popov and Laércio Lima Pilla and Amina Guermouche and Olivier Aumage and Emmanuelle Saillard},
  doi          = {10.1016/j.jpdc.2023.104720},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104720},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimizing performance and energy across problem sizes through a search space exploration and machine learning},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PHY: A performance-driven hybrid communication compression
method for distributed training. <em>JPDC</em>, <em>180</em>, 104719.
(<a href="https://doi.org/10.1016/j.jpdc.2023.104719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed training is needed to shorten the training time of deep neural networks . However, the communication overhead often hurts performance efficiency, especially in a distributed computing environment with limited network bandwidth . Hence, gradient compression techniques have been proposed to reduce communication time. But, compression also has the risk of causing lower model accuracy and longer training time due to compression loss and compression time. As a result, compression may not consistently achieve desired results, and there are limited discussions on when and which compression should be used. To address this problem, we propose a performance-driven hybrid compression solution. We make three main contributions. (1) We describe a hybrid compression strategy that chooses the compression method for individual model gradients. (2) We build an offline performance estimator and an online loss monitor to ensure the compression decision can minimize training time without sacrificing mode accuracy. (3) Our implementation can be imported to existing deep learning frameworks and applicable to a wide range of compression methods. Up to 3.6x training performance speedup was observed compared to other state-of-the-art methods.},
  archive      = {J_JPDC},
  author       = {Chen-Chun Chen and Yu-Min Chou and Jerry Chou},
  doi          = {10.1016/j.jpdc.2023.104719},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104719},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PHY: A performance-driven hybrid communication compression method for distributed training},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information-theoretic and algorithmic aspects of parallel
and distributed reconstruction from pooled data. <em>JPDC</em>,
<em>180</em>, 104718. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pooled data problem the goal is to efficiently reconstruct a binary signal from additive measurements. Given a signal σ∈{0,1}n , we can query multiple entries at once and get the total number of non-zero entries in the query as a result. We assume that queries are time-consuming and therefore focus on the setting where all queries are executed in parallel. First, we propose and analyze a simple and efficient greedy reconstruction algorithm . Secondly, we derive a sharp information-theoretic threshold for the minimum number of queries required to reconstruct σ with high probability. Finally, we consider two noise models: In the noisy channel model, the result for each entry of the signal flips with a certain probability. In the noisy query model, each query result is subject to random Gaussian noise . We pin down the range of error probabilities and distributions for which our algorithm reconstructs the exact initial states with high probability. Our theoretical findings are complemented by simulations where we compare our simple algorithm with approximate message passing (AMP) that is conjectured to be optimal in a number of related problems.},
  archive      = {J_JPDC},
  author       = {Oliver Gebhard and Max Hahn-Klimroth and Dominik Kaaser and Philipp Loick},
  doi          = {10.1016/j.jpdc.2023.104718},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104718},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Information-theoretic and algorithmic aspects of parallel and distributed reconstruction from pooled data},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual visibility by fat robots with slim omnidirectional
camera. <em>JPDC</em>, <em>180</em>, 104716. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the existing literature of the Mutual Visibility problem for autonomous robot swarms, the adopted visibility models have some idealistic assumptions that are not consistent with practical sensing device implementations. This paper investigates the problem in the more realistic visibility model called opaque fat robots with slim omnidirectional camera . The robots are modeled as unit disks, each having an omnidirectional camera represented as a disk of smaller size. The region obstructed by a single robot in this model is a truncated infinite cone. This makes the visibility model significantly challenging compared to the previous ones. We assume that the robots have compasses that allow agreement in the direction and orientation of both axes of their local coordinate systems . The robots are equipped with visible lights which serve as a medium of communication and also as a form of memory. We present a distributed algorithm for the Mutual Visibility problem which is provably correct in the semi-synchronous setting. Our algorithm also provides a solution for Leader Election which we use as a subroutine in our main algorithm. Although Leader Election is trivial with two axis agreement in the full visibility model, it is challenging in our case and is of independent interest.},
  archive      = {J_JPDC},
  author       = {Kaustav Bose and Abhinav Chakraborty and Krishnendu Mukhopadhyaya},
  doi          = {10.1016/j.jpdc.2023.104716},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104716},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Mutual visibility by fat robots with slim omnidirectional camera},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tile low-rank approximations of non-gaussian space and
space-time tukey g-and-h random field likelihoods and predictions on
large-scale systems. <em>JPDC</em>, <em>180</em>, 104715. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale statistical modeling has become necessary with the vast flood of geospace data coming from various sources. In space statistics, the Maximum Likelihood Estimation (MLE) is widely considered for modeling geospace data by estimating a set of statistical parameters related to a predefined covariance function . This covariance function describes the correlation between a set of geospace locations where the main goal is to model given data samples and impute missing data. Climate/weather modeling is a prevalent application for the MLE operation where data interpolation and forecasting are highly required. In the literature, the Gaussian random field is often used to describe geospace data as one of the most popular models for MLE. However, real-life datasets are often skewed and/or have extreme values, and non-Gaussian random field models are more appropriate for capturing such features. In this work, we provide an exact and approximate parallel implementation of the well-known Tukey g -and- h (TGH) non-Gaussian random field in the context of climate/weather applications. The proposed implementation alleviates the computation complexity of the log-likelihood function, which requires O ( n 2 ) O(n2) storage and O ( n 3 ) O(n3) operations, where N is the number of geospace locations, M is the number of time slots, and n = N × M n=N×M . Based on tile low-rank (TLR) approximations, our implementation of the TGH model can tackle large-scale problems. Furthermore, we rely on task-based programming models and dynamic runtime systems to provide fast execution for the MLE operation in space and space-time cases. We assess the performance and accuracy of the proposed implementations using synthetic space and space-time datasets up to 800 K . We also consider a 12-month precipitation dataset in Germany to demonstrate the advantage of using non-Gaussian over Gaussian random field models . We evaluate the prediction accuracy of the TGH model on the precipitation dataset using the Probability Integral Transformation (PIT) tool showing that the TGH model outperforms the Gaussian modeling in the real dataset. Moreover, our performance assessment indicates that TLR computations allow solving larger matrix sizes while preserving the required accuracy for prediction. The TLR-based approximation shows a speedup up to 7.29 X and 2.96 X over the exact solution.},
  archive      = {J_JPDC},
  author       = {Sagnik Mondal and Sameh Abdulah and Hatem Ltaief and Ying Sun and Marc G. Genton and David E. Keyes},
  doi          = {10.1016/j.jpdc.2023.104715},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104715},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Tile low-rank approximations of non-gaussian space and space-time tukey g-and-h random field likelihoods and predictions on large-scale systems},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the correctness of highly available systems in the
presence of failures. <em>JPDC</em>, <em>180</em>, 104707. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we formally study the guarantees provided by highly available, eventually consistent replicated systems in an environment in which machine failures and network splits are possible. Our analysis accounts for possible replica recovery after a crash, and clients that are (1) stateless or stateful, (2) sticky (always connect to a concrete set of replicas) or mobile, and (3) which can timeout before receiving a response to the sent request. We show why the approaches to prove protocol correctness prevalent in the literature, which do not take into account replica or network crashes, may lead to incorrect conclusions regarding the guarantees offered by the protocol. We adapt the existing formal correctness criteria, such as basic eventual consistency , to the considered environment by defining the family of failure-aware consistency guarantees. We formally identify a set of undesired phenomena (in particular phantom operations ) observed by the clients, which, as we prove, are unavoidable in highly available systems in which unrecoverable replica crashes are possible. We also introduce context preservation , a new client-side requirement for eventually consistent systems that expose concurrency to the client, i.e., allow clients to use, e.g., multi-value registers or observed-remove sets. Context preservation is incomparable with classic session guarantees.},
  archive      = {J_JPDC},
  author       = {Maciej Kokociński and Tadeusz Kobus and Paweł T. Wojciechowski},
  doi          = {10.1016/j.jpdc.2023.04.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104707},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the correctness of highly available systems in the presence of failures},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSMV: A highly scalable multi-versioned software
transactional memory for GPUs. <em>JPDC</em>, <em>180</em>, 104701. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces CSMV (Client Server Multiversioned), a multi-versioned Software TM (STM) for GPUs that adopts an innovative client-server design. By decoupling the execution of transactions from their commit process, CSMV provides two main benefits: (i) it enables the use of fast on chip memory to access the global metadata used to synchronize transaction (ii) it allows for implementing highly efficient collaborative commit procedures, tailored to take full advantage of the architectural characteristics of GPUs . Via an extensive experimental study, we show that CSMV achieves up to 3 orders of magnitude speed-ups with respect to state of the art STMs for GPUs and that it can accelerate by up to 20× irregular applications running on state of the art STMs for CPUs.},
  archive      = {J_JPDC},
  author       = {Diogo Nunes and Daniel Castro and Paolo Romano},
  doi          = {10.1016/j.jpdc.2023.04.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104701},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CSMV: A highly scalable multi-versioned software transactional memory for GPUs},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure and efficient three-factor authentication protocol
for IoT environments. <em>JPDC</em>, <em>179</em>, 104714. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is an information carrier based on the Internet and traditional telecommunications network , which enables all ordinary physical objects that can be independently addressed to form an interconnected network. User authentication protocol is an essential technology for security and privacy in the IoT environment. This paper analyzes the security of Mirsaraei et al.&#39;s three-factor authentication scheme for IoT environments (Mirsaraei et al., 2022 [31] ), and finds that the scheme cannot provide users with untraceability, perfect forward secrecy or the resistance of key compromise impersonation attack. The article improves Mirsaraei et al.&#39;s scheme and proposes a three-factor authentication protocol with perfect forward secrecy using elliptic curve cryptosystem , which retains the general process of Mirsaraei et al.&#39;s scheme. The formal security analysis of the proposed protocol is carried out by ROR (Real-or-Random) model, and the formal security verification of the proposed protocol is implemented by Proverif tool. The cryptoanalysis results demonstrate that the proposed protocol makes up for the shortcomings of Mirsaraei et al.&#39;s scheme in security and can resist more malicious attacks as opposed to recent schemes. Moreover, the performance analysis using MIRACL (Multiprecision Integer and Rational Arithmetic C/C++ Library) shows that, the proposed protocol has great advantages over analogical three-factor authentication schemes in terms of computational overhead and communication overhead .},
  archive      = {J_JPDC},
  author       = {Yi Li},
  doi          = {10.1016/j.jpdc.2023.104714},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104714},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A secure and efficient three-factor authentication protocol for IoT environments},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective switchless inter-FPGA memory networks.
<em>JPDC</em>, <em>179</em>, 104713. (<a
href="https://doi.org/10.1016/j.jpdc.2023.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGA network bandwidth increases to 400 Gbps or higher by high-density optical integration, e.g., Co-Packaged Optics (CPO) and onboard Si-photonics transceivers . This study presents its lightweight switchless network architecture by exploiting an indirect path, consisting of up to k one-hop paths for a diameter- k network topology . It integrates interconnection network and memory channels into a lightweight memory-to-memory inter-FPGA network. It has a uni-directional network topology for connecting the largest number of FPGAs. A newly introduced indirect flow control enables lossless uni-directional networks. We present multi-path point-to-point communications, multi-port message-combine collective communications, and efficient WDM (Wavelength Division Multiplexing) cabling method for the uni-directional networks. We implement and evaluate the uni-directional switchless network on an FPGA cluster consisting of six custom Stratix10 MX2100 FPGA cards connected with a 16-wavelength Arrayed Waveguide Grating (AWG) router. Simulation results show that the multi-port message-combine collective communication achieves 7× faster than conventional communication with 272 FPGAs. Effective WDM cabling using 16 wavelengths decreases the number of cables by 88\% compared with parallel optical cabling for 272 FPGAs.},
  archive      = {J_JPDC},
  author       = {Truong Thao Nguyen and Kien Trung Pham and Hiroshi Yamaguchi and Yutaka Urino and Michihiro Koibuchi},
  doi          = {10.1016/j.jpdc.2023.05.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104713},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Effective switchless inter-FPGA memory networks},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring edge TPU for network intrusion detection in IoT.
<em>JPDC</em>, <em>179</em>, 104712. (<a
href="https://doi.org/10.1016/j.jpdc.2023.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores Google&#39;s Edge TPU for implementing a practical network intrusion detection system (NIDS) at the edge of IoT , based on a deep learning approach. While a significant number of related works explore machine learning-based NIDS for the IoT edge, they generally lack considering the issue of the required computational and energy resources. The focus of this paper is the exploration of deep learning-based NIDS at the edge of IoT, and in particular, the computational and energy efficiency. In particular, the paper studies Google&#39;s Edge TPU as a hardware platform and considers the following three key metrics: computation (inference) time, energy efficiency and traffic classification performance. Various scaled model sizes of two major deep neural network architectures are used to investigate these three metrics. The performance of the Edge TPU-based implementation is compared with that of an energy-efficient embedded CPU (quad-core ARM Cortex-A53).},
  archive      = {J_JPDC},
  author       = {Seyedehfaezeh Hosseininoorbin and Siamak Layeghy and Mohanad Sarhan and Raja Jurdak and Marius Portmann},
  doi          = {10.1016/j.jpdc.2023.05.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104712},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exploring edge TPU for network intrusion detection in IoT},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the performance of classical linear algebra
iterative methods via hybrid parallelism. <em>JPDC</em>, <em>179</em>,
104711. (<a href="https://doi.org/10.1016/j.jpdc.2023.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose fork-join and task-based hybrid implementations of four classical linear algebra iterative methods (Jacobi, Gauss–Seidel, conjugate gradient and biconjugate gradient stabilized) on CPUs as well as variations of them. This class of algorithms, that are ubiquitous in computational frameworks, are duly documented and the corresponding source code is made publicly available for reproducibility. Both weak and strong scalability benchmarks are conducted to statistically analyse their relative efficiencies. The weak scalability results assert the superiority of a task-based hybrid parallelisation over MPI-only and fork-join hybrid implementations. Indeed, the task-based model is able to achieve speedups of up to 25\% larger than its MPI-only counterpart depending on the numerical method and the computational resources used. For strong scalability scenarios, hybrid methods based on tasks remain more efficient with moderate computational resources where data locality does not play an important role. Fork-join hybridisation often yields mixed results and hence does not seem to bring a competitive advantage over a much simpler MPI approach.},
  archive      = {J_JPDC},
  author       = {Pedro J. Martinez-Ferrer and Tufan Arslan and Vicenç Beltran},
  doi          = {10.1016/j.jpdc.2023.04.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104711},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving the performance of classical linear algebra iterative methods via hybrid parallelism},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RD-FCA: A resilient distributed framework for formal concept
analysis. <em>JPDC</em>, <em>179</em>, 104710. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal Concept Analysis (FCA) is a data analysis technique with applications in data mining, artificial intelligence , software engineering , etc. The algorithms for FCA are computationally expensive, and their recursion tree is highly irregular and dynamic in nature. Several distributed FCA algorithms have been proposed to exploit parallelism within and across machines. However, none of the distributed approaches are able to recover from failures in the system. We propose RD-FCA, the first resilient distributed framework for FCA that uses novel load-balancing strategy, handles fail-stop failures and provides at-least-once semantics for concept discovery. Our asynchronous snapshot mechanism with incremental updates reduces the snapshot overhead and minimizes recalculation of concepts during recovery. RD-FCA also supports dynamic addition of workers to accelerate performance. Compared to MapReduce based approaches, RD-FCA performs an order of magnitude faster. We show through extensive evaluation that RD-FCA recovers efficiently from single, multiple, independent and cascading failures .},
  archive      = {J_JPDC},
  author       = {Abhigyan Khaund and Abhishek Mukesh Sharma and Abhishek Tiwari and Shashwat Garg and Sriram Kailasam},
  doi          = {10.1016/j.jpdc.2023.04.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104710},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {RD-FCA: A resilient distributed framework for formal concept analysis},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed non-negative RESCAL with automatic model
selection for exascale data. <em>JPDC</em>, <em>179</em>, 104709. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the boom in the development of computer hardware and software, social media, IoT platforms, and communications, there has been exponential growth in the volume of data produced worldwide. Among these data, relational datasets are growing in popularity as they provide unique insights regarding the evolution of communities and their interactions. Relational datasets are naturally non-negative, sparse, and extra-large. Relational data usually contain triples (subject, relation, object) and are represented as graphs/multigraphs, called knowledge graphs, which need to be embedded into a low-dimensional dense vector space. Among various embedding models, RESCAL allows the learning of relational data to extract the posterior distributions over the latent variables and to make predictions of missing relations. However, RESCAL is computationally demanding and requires a fast and distributed implementation to analyze extra-large real-world datasets. Here we introduce a distributed non-negative RESCAL algorithm for heterogeneous CPU/GPU architectures with automatic selection of the number of latent communities (model selection), called pyDRESCALk. We demonstrate the correctness of pyDRESCALk with real-world and large synthetic tensors and the efficacy showing near-linear scaling that concurs with the theoretical complexities. Finally, pyDRESCALk determines the number of latent communities in an 11-terabyte dense and 9-exabyte sparse synthetic tensor.},
  archive      = {J_JPDC},
  author       = {Manish Bhattarai and Namita kharat and Ismael Boureima and Erik Skau and Benjamin Nebgen and Hristo Djidjev and Sanjay Rajopadhye and James P. Smith and Boian Alexandrov},
  doi          = {10.1016/j.jpdc.2023.04.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104709},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed non-negative RESCAL with automatic model selection for exascale data},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supporting efficient overlapping of host-device operations
for heterogeneous programming with CtrlEvents. <em>JPDC</em>,
<em>179</em>, 104708. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous systems with several kinds of devices, such as multi-core CPUs, GPUs , FPGAs , among others, are now commonplace. Exploiting all these devices with device-oriented programming models, such as CUDA or OpenCL , requires expertise and knowledge about the underlying hardware to tailor the application to each specific device, thus degrading performance portability . Higher-level proposals simplify the programming of these devices, but their current implementations do not have an efficient support to solve problems that include frequent bursts of computation and communication, or input/output operations. In this work we present CtrlEvents, a new heterogeneous runtime solution which automatically overlaps computation and communication whenever possible, simplifying and improving the efficiency of data-dependency analysis and the coordination of both device computations and host tasks that include generic I/O operations. Our solution outperforms other state-of-the-art implementations for most situations, presenting a good balance between portability, programmability and efficiency.},
  archive      = {J_JPDC},
  author       = {Yuri Torres and Francisco J. Andújar and Arturo Gonzalez-Escribano and Diego R. Llanos},
  doi          = {10.1016/j.jpdc.2023.04.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104708},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Supporting efficient overlapping of host-device operations for heterogeneous programming with CtrlEvents},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed message passing algorithm for computing
perfect demand matching. <em>JPDC</em>, <em>179</em>, 104706. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the perfect demand matching problem ( PDM PDM ) which combines aspects of the knapsack problem along with the b -matching problem. It is a generalization of the maximum weight matching problem which has been fundamental in the development of theory of computer science and operations research. This problem is NP-hard and there exists a constant ϵ &gt; 0 ϵ&amp;gt;0 such that the problem admits no 1 + ϵ 1+ϵ -approximation algorithm, unless P=NP. Here, we investigate the performance of a distributed message passing algorithm called Max-sum belief propagation for computing the problem of finding the optimal perfect demand matching. As the main result, we demonstrate the rigorous theoretical analysis of the Max-sum BP algorithm for PDM PDM , and establish that within pseudo-polynomial-time, our algorithm could converge to the optimal solution of PDM PDM , provided that the optimal solution of its LP relaxation is unique and integral. Different from the techniques used in previous literature, our analysis is based on primal-dual complementary slackness conditions , and thus the number of iterations of the algorithm is independent of the structure of the given graph. Moreover, to the best of our knowledge, this is one of a very few instances where BP algorithm is proved correct for NP-hard problems.},
  archive      = {J_JPDC},
  author       = {Guowei Dai and Yannan Chen and Yaping Mao and Dachuan Xu and Xiaoyan Zhang and Zan-Bo Zhang},
  doi          = {10.1016/j.jpdc.2023.04.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104706},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A distributed message passing algorithm for computing perfect demand matching},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating distributed machine learning with model
compression and graph partition. <em>JPDC</em>, <em>179</em>, 104705.
(<a href="https://doi.org/10.1016/j.jpdc.2023.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of data and parameter sizes of machine learning models makes it necessary to improve the efficiency of distributed training. It is observed that the communication cost usually is the bottleneck of distributed training systems. In this paper, we focus on the parameter server framework which is a widely deployed distributed learning framework. The frequent parameter pull, push, and synchronization among multiple machines leads to a huge communication volume. We aim to reduce the communication cost for the parameter server framework. Compressing the training model and optimizing the data and parameter allocation are two existing approaches to reducing communication costs. We jointly consider these two approaches and propose to optimize the data and parameter allocation after compression. Different from previous allocation schemes, the data sparsity property may no longer hold after compression. It brings additional opportunities and challenges for the allocation problem. We also consider the allocation problem for both linear and deep neural network (DNN) models. Fixed and dynamic partition algorithms are proposed accordingly. Experiments on real-world datasets show that our joint compression and partition scheme can efficiently reduce communication overhead for linear and DNN models.},
  archive      = {J_JPDC},
  author       = {Yubin Duan and Jie Wu},
  doi          = {10.1016/j.jpdc.2023.04.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104705},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerating distributed machine learning with model compression and graph partition},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soundness-preserving composition of synchronously and
asynchronously interacting workflow net components. <em>JPDC</em>,
<em>179</em>, 104704. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a compositional approach to constructing correct formal models of information systems from correct models of interacting components. Component behavior is represented using workflow nets — a class of Petri nets . Interactions among components are encoded in an additional interface net. The proposed approach is used to model and compose synchronously and asynchronously interacting workflow nets. Using Petri net morphisms and their properties, we prove that the composition of interacting workflow nets preserves the correctness of components and of an interface.},
  archive      = {J_JPDC},
  author       = {Luca Bernardinello and Irina Lomazova and Roman Nesterov and Lucia Pomello},
  doi          = {10.1016/j.jpdc.2023.04.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104704},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Soundness-preserving composition of synchronously and asynchronously interacting workflow net components},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time and ACO-based offloading algorithm in edge
computing. <em>JPDC</em>, <em>179</em>, 104703. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasingly widespread use of networks and end devices, more and more data and computations must be processed. With processing constrained by the limited resources of the end device, edge computing plays an important role. Edge computing offloads computation to surrounding edge nodes with corresponding computing capabilities so that the end device can get a response within a reasonable latency to meet the user&#39;s needs. Since these edge nodes are composed of multiple heterogeneous computing units , any system&#39;s task-offloading strategy must necessarily affect the system&#39;s load balance and execution time. This study proposes a real-time, two-stage ant colony algorithm (RTACO) with the following goals: 1) the algorithm requires low latency; 2) the algorithm minimizes the makespan of all tasks; 3) the algorithm optimizes the system load and reduces the burden of the task-offloading algorithm, thereby providing a stable and high-performance edge computing system. Experiments show that RTACO requires low execution time, and can still effectively achieve good results even when the system has limited resources.},
  archive      = {J_JPDC},
  author       = {Yung-Ting Chuang and Yuan-Tsang Hung},
  doi          = {10.1016/j.jpdc.2023.04.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104703},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A real-time and ACO-based offloading algorithm in edge computing},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An arbitrable outsourcing data audit scheme supporting
credit reward and punishment and multi-user sharing. <em>JPDC</em>,
<em>178</em>, 100–111. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet of Things , cloud storage allows users to outsource their data without retaining a local copy. However, a serious problem caused by this pattern is that the data owner loses control of the data. In this case, to ensure outsourced storage data integrity, researchers proposed Provable Data Possession . However, most systems cannot deal with disagreements between entities throughout the process. Due to this, we propose an auditing system for outsourced data . By using undeniable blockchain information, it solves disputes between entities and avoids accounting delays by third-party auditors. A credit chain based on the public blockchain is designed. A novel dynamic credit calculation algorithm is adopted in the credit chain to realize an effective reward and punishment mechanism. Security proofs are provided in detail for the proposed scheme. Evaluation experiments show that compared with existing similar schemes, the scheme achieves arbitrable audit and has a lower computational cost.},
  archive      = {J_JPDC},
  author       = {Junfeng Tian and Qian Yang},
  doi          = {10.1016/j.jpdc.2023.04.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {100-111},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An arbitrable outsourcing data audit scheme supporting credit reward and punishment and multi-user sharing},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive anomaly-based DDoS attack detection method in
cloud computing environments using a third party auditor. <em>JPDC</em>,
<em>178</em>, 82–99. (<a
href="https://doi.org/10.1016/j.jpdc.2023.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing environments are indispensable components of the majority of information technology organizations and users&#39; lives. Despite multiple benefits of cloud computing environments, cloud users (CUs) as well as cloud service providers (CSPs) may experience unpleasant conditions by detrimental results of distributed denial of service (DDoS) attacks such as unavailability of cloud services or lengthy response times of the services. In this paper, we provide a threshold anomaly-based DDoS attack detection method to protect cloud environments against DDoS attack. Our proposed method is introduced to reduce DDoS attack consequences in CSPs. Our suggested method includes three newly defined components: 1. A third party auditor (TPA) which acquires direct interaction with each datacenter of the CSP, 2. A zone delimiter (ZD) which encapsulates the sensitive internal specifications of a CSP from TPA, and 3. A protocol which is defined to coordinate TPA, ZD, and CSPs for DDoS attack detection via TPA. We analyze our proposed method by determining and conducting a simulation strategy for an intrusion detection system in CSPs. Results illustrate that interactive communication between TPA and datacenters of CSPs improves the user experience of CUs in the time of DDoS attacks by reducing excessive attack filtering stages. Moreover, by using an intrusion detection system (IDS), we investigate efficiency of the proposed method to recover CSPs from DDoS attacks. We further indicate the efficiency of our proposed method by providing accuracy and qualitative comparisons with other existing methods.},
  archive      = {J_JPDC},
  author       = {Sasha Mahdavi Hezavehi and Rouhollah Rahmani},
  doi          = {10.1016/j.jpdc.2023.04.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {82-99},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Interactive anomaly-based DDoS attack detection method in cloud computing environments using a third party auditor},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating batched sparse iterative solvers for the
collision operator in fusion plasma simulations on GPUs. <em>JPDC</em>,
<em>178</em>, 69–81. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batched linear solvers, which solve many small related but independent problems, are increasingly important for highly parallel processors such as graphics processing units (GPUs). GPUs need a substantial amount of work to keep them operating efficiently and it is not an option to solve smaller problems one-by-one. Because of the small size of each problem, the task of implementing a parallel partitioning scheme and mapping the problem to hardware is not trivial. In recent history, significant attention has been given to batched dense linear algebra . However, there is also an interest in utilizing sparse iterative solvers in a batched form. An example use case is found in a gyrokinetic Particle-In-Cell (PIC) code used for modeling magnetically confined fusion plasma devices. The collision operator has been identified as a bottleneck, and a proxy app has been created for facilitating optimizations and porting to GPUs. The current collision kernel linear solver does not run on the GPU—a major bottleneck. As these matrices are sparse and well-conditioned, batched iterative sparse solvers are an attractive option. A batched sparse iterative solver capability has recently been developed in the Ginkgo library. In this paper, we describe how Ginkgo &#39;s batched solver technology can integrate into the XGC collision kernel and accelerate the simulation process. Comparisons for the solve times on NVIDIA V100 and A100 GPUs and AMD MI100 GPUs with one dual-socket Intel Xeon Skylake CPU node with 40 cores are presented for matrices from the collision kernel of XGC . Further, the speedups observed for the overall collision kernel are presented in comparison to different modern CPUs on multiple supercomputer systems. The results suggest that Ginkgo &#39;s batched sparse iterative solvers are well suited for efficient utilization of the GPU for this problem, and the performance portability of Ginkgo in conjunction with Kokkos (used within XGC as the heterogeneous programming model) allows seamless execution on exascale-oriented heterogeneous architectures .},
  archive      = {J_JPDC},
  author       = {Aditya Kashi and Pratik Nayak and Dhruva Kulkarni and Aaron Scheinberg and Paul Lin and Hartwig Anzt},
  doi          = {10.1016/j.jpdc.2023.03.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {69-81},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Integrating batched sparse iterative solvers for the collision operator in fusion plasma simulations on GPUs},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High performance HITA based binary edward curve crypto
processor for FPGA platforms. <em>JPDC</em>, <em>178</em>, 56–68. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an embedded and resource-constrained environment, Elliptic Curve Cryptography (ECC) has been noted as an efficient and suitable methodology for achieving information security via public-key cryptography. However, the drawback of ECC is its lack of unifiedness in point operation that makes it prone to side-channel attack. Also, ECC does not satisfy the completeness property due to which the addition formula is not defined for all the pairs of input points. Edward curve, with its unified addition law and completeness property, proved to be the answer to aforementioned flaws. High throughput while maintaining low resource is a key issue for elliptic curve cryptography (ECC) hardware implementations in many applications. This paper presents the implementation of a Binary Edward curve Crypto processor over G F ( 2 233 ) GF(2233) for FPGA platforms. The architecture is modified to perform scalar multiplication in a parallel manner using two hybrid Karatsuba field multipliers. Field inversion being one of the most tedious operations while reconversion, is also performed in a parallel manner using an efficient Hex Itoh-Tsujii inversion algorithm. The hardware resources are shared for performing point operations and inversion. Exploiting parallelism in point and inversion operations has resulted in reduction of the clock cycles consumed and the resultant architecture is more efficient in terms of throughput over area. The design takes 0.038 ms on Xilinx Virtex-4 and 0.031 ms on Virtex-7 FPGA platforms to perform a 233-bit point multiplication operation. It takes 73.57\%, 13.71\%, 14.76\% and 48.76\% more efficient than existing scalar multiplication with BEC. This proposed scalable, side-channel attack resilient design outperforms the existing techniques with respect to throughput over area.},
  archive      = {J_JPDC},
  author       = {M. Kalaiarasi and V.R. Venkatasubramani and M.S.K. Manikandan and S. Rajaram},
  doi          = {10.1016/j.jpdc.2023.03.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {56-68},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {High performance HITA based binary edward curve crypto processor for FPGA platforms},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reputation based novel trust management framework with
enhanced availability for cloud. <em>JPDC</em>, <em>178</em>, 43–55. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To select a cloud service provider (CSP) out of immense number of relevant CSPs available, cloud customer (CC) trusts on the reviews provided by the CCs who have used the services provided by CSPs. However, relying on these feedbacks may be hazardous if the review(s) have been deliberately posted with wicked motive. To mitigate the impact of malicious feedbacks, a framework, namely highly available credible cloud trust management framework (HA2CTMF) has been proposed with the intent of revamping the reputation and availability parameters of QoS by evaluating the trust of cloud service providers (CSPs) based on the credibility of feedbacks furnished by CC. HA2CTMF detects collusion and Sybil attacks on the labeled CloudArmor dataset and increases availability of the trust service. Collusion attack has been detected by comparing review count; service count; &amp; mean of reviews with their respective threshold values. Further, Sybil attack has been uncovered with the help of these aspects: deviation from mean of reviews; pattern detection; &amp; comparison of polarity value from both lower &amp; upper threshold values. Additionally, enhanced availability has been ensured by working on disaster management and diminishing repercussions of single point of failure . The results have been accessed on the following parameters: precision; recall; f1-score &amp; availability percentage on the dataset comprising 8774 data points. This framework has exhibited precision of 85.69\%, recall of 89.45\%, f1-score of 87.53\% and availability of 96.71\% compared to CloudArmor framework values on the same parameters as: 44.47\%; 62.21\%; 51.85\% &amp; 70.43\% respectively. As per the results attained, we conclude that proposed HA2CTMF has demonstrated better outcome.},
  archive      = {J_JPDC},
  author       = {Harsh Taneja and Supreet Kaur},
  doi          = {10.1016/j.jpdc.2023.03.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {43-55},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reputation based novel trust management framework with enhanced availability for cloud},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on resource allocation technology in highly trusted
environment of edge computing. <em>JPDC</em>, <em>178</em>, 29–42. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing can use many edge devices to provide users with real-time computing and storage functions. With the development of the Internet of Things (IOT), edge computing is becoming more and more prevalent currently. However, the consequent challenges in search efficiency, reliability requirements and resource allocation appear followed. Therefore, this article focuses on resource allocation and security performance issues. A lightweight trust evaluation mechanism was constructed and time-varying trust coefficients were introduced as incentives to address the problem of distrust between user terminals and edge server entities in multi-cell and multi-user scenarios. This enables the user terminal to immediately distinguish malicious servers. Considering the limited and dynamic changes of computing resources, the problem of complete migration of multi-user tasks was transformed into an issue of computing resource distribution to reduce the total system energy consumption. As a Markov game model, a system was developed to address the problems of centralized single-agent algorithms, including the explosion of action space and difficulty in convergence with increasing the number of users. Besides, a resource allocation algorithm was proposed based on a trust model and multi-agents that follows a centralized training and distributed implementation architecture. The simulated consequences indicated that the proposed algorithm resists malicious attacks , and can quickly make reasonable task migration decisions based on different system states, thereby efficiently decreasing the consumption of the total system energy , and providing a better user experience .},
  archive      = {J_JPDC},
  author       = {Yang Zhang and Kaige Zhu and Xuan Zhao and Quancheng Zhao and Zhenjiang Zhang and Ali Kashif Bashir},
  doi          = {10.1016/j.jpdc.2023.03.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {29-42},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Research on resource allocation technology in highly trusted environment of edge computing},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel computing architecture based on cellular automata
for hydraulic analysis of water distribution networks. <em>JPDC</em>,
<em>178</em>, 11–28. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water distribution networks (WDNs) are one of the largest infrastructures in society. Various methods for formulation and hydraulic analysis of water distribution networks , including numerical and non-numerical methods, have been previously proposed. Due to the complexity, the nonlinearity of the hydraulic equations of water distribution networks, and the need for multiple executions and uncertainties in parameters, solving the hydraulic model of water distribution networks has high time complexity. In this paper, a parallel computational architecture based on the concept of cellular automata is proposed to accelerate the numerical solution of the steady-state water distribution network model. Taylor series is proposed to solve hydraulic equations. The presented architecture was implemented as a parallel hardware platform on a field-programmable gate array. The performance of the proposed method was compared with EPANET software for networks with different complexities and topologies. The results show that the proposed parallel algorithm can accelerate the hydraulic analysis of regular water distribution networks up to 700 times and 250 times for small and large networks, respectively.},
  archive      = {J_JPDC},
  author       = {Ali Suvizi and Azim Farghadan and Morteza Saheb Zamani},
  doi          = {10.1016/j.jpdc.2023.03.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {11-28},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel computing architecture based on cellular automata for hydraulic analysis of water distribution networks},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-phase heuristic algorithm for power-aware offline
scheduling in IaaS clouds. <em>JPDC</em>, <em>178</em>, 1–10. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper aims at mitigating hot-spots during Offline Scheduling in IaaS (Infrastructure-as-a-Service) cloud systems . Unlike previous studies, the research focuses on identifying and resolving hot-spots not at servers, but at server racks. A two-phase algorithm for performing power-aware offline scheduling is proposed. The first phase aims at identifying and mitigating hot-spots at racks, while the second phase performs VM consolidation, i.e. minimization of the number of occupied servers while maintaining a feasible VM mapping and low migration costs. The proposed algorithm takes into account the dynamic nature of VM&#39;s resource consumption: it does not only resolve detected hot-spots, but also tries to avoid hot-spots in a reasonable future time period. The algorithm was tested with the data from a real IaaS cloud with different sets of algorithm&#39;s parameters. Experimental evaluation showed that the statistical estimates of the future VM&#39;s resource consumption provide the most reliable mapping, which is a result of minimization of the number of new hot-spot occurrences.},
  archive      = {J_JPDC},
  author       = {A. Ignatov and I. Maslova and M. Posypkin and W. Yang and J. Wu},
  doi          = {10.1016/j.jpdc.2023.03.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-10},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A two-phase heuristic algorithm for power-aware offline scheduling in IaaS clouds},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel schemes for embedding hamiltonian paths and cycles in
balanced hypercubes with exponential faulty edges. <em>JPDC</em>,
<em>177</em>, 182–191. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The balanced hypercube B H n BHn plays an essential role in large-scale parallel and distributed computing systems. With the increasing probability of edge faults in large-scale networks and the widespread applications of Hamiltonian paths and cycles, it is especially essential to study the fault tolerance of networks in the presence of Hamiltonian paths and cycles. However, existing researches on edge faults ignore that it is almost impossible for all faulty edges to be concentrated in a certain dimension. Thus, the fault tolerance performance of interconnection networks is severely underestimated. This paper focuses on three measures, t -partition-edge fault-tolerant Hamiltonian, t -partition-edge fault-tolerant Hamiltonian laceable, and t -partition-edge fault-tolerant strongly Hamiltonian laceable, and utilizes these measures to explore the existence of Hamiltonian paths and cycles in balanced hypercubes with exponentially faulty edges. We show that the B H n BHn is 2 n − 1 2n−1 -partition-edge fault-tolerant Hamiltonian laceable, 2 n − 1 2n−1 -partition-edge fault-tolerant Hamiltonian, and ( 2 n − 1 − 1 ) (2n−1−1) -partition-edge fault-tolerant strongly Hamiltonian laceable for n ≥ 2 n≥2 . Comparison results show the partitioned fault model can provide the exponential fault tolerance as the value of the dimension n grows.},
  archive      = {J_JPDC},
  author       = {Xiao-Yan Li and Kun Zhao and Hongbin Zhuang and Xiaohua Jia},
  doi          = {10.1016/j.jpdc.2023.03.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {182-191},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Novel schemes for embedding hamiltonian paths and cycles in balanced hypercubes with exponential faulty edges},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy allocation and task scheduling in edge devices based
on forecast solar energy with meteorological information. <em>JPDC</em>,
<em>177</em>, 171–181. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offloading tasks from edge devices to the cloud is an important method to enhance the performance of the edge device. With the help of EH (Energy Harvesting) technology, the edge device can use the collected green energy to support its operations. Most offloading scheduling methods use as much green energy as the edge device collected. Unlike prior research, we consider the long-term benefits of energy. In this paper, we forecast the solar energy supply with meteorological methods which are based on the weather forecast data. Then, we use quadratic programming to allocate energy based on the forecast energy to maximize energy efficiency. Finally, we use NSGA (Non-dominated Sorting Genetic Algorithms) to offload tasks in the edge device. Simulations show that our proposed method not only minimizes the execution time and the energy consumption of clouds, but also enhances the QoE of users.},
  archive      = {J_JPDC},
  author       = {Yongsheng Hao and Qi Wang and Tinghuai Ma and Jinglin Du and Jie Cao},
  doi          = {10.1016/j.jpdc.2023.03.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {171-181},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy allocation and task scheduling in edge devices based on forecast solar energy with meteorological information},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-directional sobel operator kernel on GPUs.
<em>JPDC</em>, <em>177</em>, 160–170. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sobel is one of the most popular edge detection operators used in image processing . To date, most users utilize the two-directional 3×3 Sobel operator as detectors because of its low computational cost and reasonable performance. Simultaneously, many studies have been conducted on using large multi-directional Sobel operators to satisfy their needs considering the high stability, but at an expense of speed. This paper proposes a fast graphics processing unit (GPU) kernel for the four-directional 5×5 Sobel operator . To improve kernel performance, we implement the kernel based on warp-level primitives, which can significantly reduce the number of memory accesses. In addition, we introduce the prefetching mechanism and operator transformation into the kernel to significantly reduce the computational complexity and data transmission latency. Compared with the OpenCV-GPU library, our kernel shows high performances of 6.7x speedup on a Jetson AGX Xavier GPU and 13x on a GTX 1650Ti GPU.},
  archive      = {J_JPDC},
  author       = {Qiong Chang and Xiang Li and Yun Li and Jun Miyazaki},
  doi          = {10.1016/j.jpdc.2023.03.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {160-170},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multi-directional sobel operator kernel on GPUs},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fargraph+: Excavating the parallelism of graph processing
workload on RDMA-based far memory system. <em>JPDC</em>, <em>177</em>,
144–159. (<a href="https://doi.org/10.1016/j.jpdc.2023.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregated architecture brings new opportunities to memory-consuming applications like graph processing. It allows one to outspread memory access pressure from local to far memory, providing an attractive alternative to disk-based processing. Although existing works on general-purpose far memory platforms show great potentials for application expansion, it is unclear how graph processing applications could benefit from disaggregated architecture, and how different optimization methods influence the overall performance. In this paper, we take the first step to analyze the impact of graph processing workload on disaggregated architecture by extending the GridGraph framework on top of the RDMA-based far memory system. We propose Fargraph+, a system with parallel graph data offloading and far memory coordination strategy for enhancing efficiency of graph processing workload on RDMA-based far memory architecture. Specifically, Fargraph+ reduces the overall data movement through a well-crafted, graph-aware data segment offloading mechanism. In addition, we use optimal data segment splitting and asynchronous data buffering to achieve graph iteration-friendly far memory access. We further configure efficient parallelism-oriented control to accelerate performance of multi-threading processing on graph iterations while improving memory efficiency of far memory access by utilizing RDMA queue features. We show that Fargraph+ achieves near-oracle performance for typical in-local-memory graph processing systems. Fargraph+ shows up to 11.2× speedup compared to Fastswap, the state-of-the-art, general-purpose far memory platform.},
  archive      = {J_JPDC},
  author       = {Jing Wang and Chao Li and Yibo Liu and Taolei Wang and Junyi Mei and Lu Zhang and Pengyu Wang and Minyi Guo},
  doi          = {10.1016/j.jpdc.2023.02.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {144-159},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fargraph+: Excavating the parallelism of graph processing workload on RDMA-based far memory system},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). GraphCS: Graph-based client selection for heterogeneity in
federated learning. <em>JPDC</em>, <em>177</em>, 131–143. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning coordinates many mobile devices to train an artificial intelligence model while preserving data privacy collaboratively. Mobile devices are usually equipped with totally different hardware configurations , leading to various training capabilities. At the same time, the distribution of the local training data is highly heterogeneous across different clients. Randomly selecting the clients to participate in the training process results in poor model performance and low system efficiency. In this paper, we propose GraphCS, a graph-based client selection framework for heterogeneity in Federated Learning . GraphCS first measures the distribution coupling across the clients via the model gradients. After that, it divides the clients into different groups according to the diversity of the local datasets. At the same time, it well estimates the runtime training capability of each client by jointly considering the hardware configuration and resource contention caused by the concurrently running apps. With the distribution coupling information and runtime training capability, GraphCS selects the best clients in order to well balance the model accuracy and overall training progress. We evaluate the performance of GraphCS with mobile devices with different hardware configurations on various datasets. The experiment results show that our approach improves model accuracy up to 45.69\%. Meanwhile, it reduces communication and computation overhead 87.35\% and 89.48\% at best, respectively. Furthermore, GraphCS accelerates the overall training process up to 35×.},
  archive      = {J_JPDC},
  author       = {Tao Chang and Li Li and MeiHan Wu and Wei Yu and Xiaodong Wang and ChengZhong Xu},
  doi          = {10.1016/j.jpdc.2023.03.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {131-143},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GraphCS: Graph-based client selection for heterogeneity in federated learning},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HOTD: A holistic cross-layer time-delay attack detection
framework for unmanned aerial vehicle networks. <em>JPDC</em>,
<em>177</em>, 117–130. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unmanned aerial vehicle (UAV) networks have been widely used in military and civilian scenarios; however, they suffer various attacks. Time-delay attacks maliciously delay the transmission of packets without tampering with the contents or significantly affecting the transmission pattern, making detection difficult. In this paper, a holistic cross-layer time-delay attack detection framework (HOTD) is proposed for UAV networks. A holistic selection of the delay-related features available at all layers is performed, before adopting supervised learning to build a consistency model between these features and the corresponding forwarding delay to calculate the degree of consistency of each node. Finally, the clustering method is used to distinguish malicious from benign nodes according to their degree of consistency. Experimental results show that the performance of HOTD is superior to that of state-of-the-art detection methods, and it achieves a detection accuracy higher than 85\% with less than 2.5\% additional overhead.},
  archive      = {J_JPDC},
  author       = {Wenbin Zhai and Shanshan Sun and Liang Liu and Youwei Ding and Wanying Lu},
  doi          = {10.1016/j.jpdc.2023.03.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {117-130},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HOTD: A holistic cross-layer time-delay attack detection framework for unmanned aerial vehicle networks},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards elastic in situ analysis for high-performance
computing simulations. <em>JPDC</em>, <em>177</em>, 106–116. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In situ analysis and visualization have grown increasingly popular for enabling direct access to data from high-performance computing (HPC) simulations. As a simulation progresses and interesting physical phenomena emerge, however, the data produced may become increasingly complex, and users may need to dynamically change the type and scale of in situ analysis tasks being carried out and consequently adapt the amount of resources allocated to such tasks. To date, none of the production in situ analysis frameworks offer such an elasticity feature, and for good reason: the assumption that the number of processes could vary during run time would force developers to rethink software and algorithms at every level of the in situ analysis stack. In this paper we present Colza, a data staging service with elastic in situ visualization capabilities. We demonstrate the use of Colza with the Deep Water Impact and the AMR-Wind simulations, coupling them with the ParaView Catalyst and Ascent in situ libraries, and show that Colza enables dynamic rescaling of these widely-used frameworks with no interruption to the simulation or staging service. We highlight the challenges of enabling such elasticity, which requires overcoming these frameworks&#39; reliance on MPI, using distinct engineering approaches, namely dependency injection and dependency overload. To the best of our knowledge, this work is the first to enable elastic in situ visualization capabilities for HPC applications on top of existing production analysis tools.},
  archive      = {J_JPDC},
  author       = {Matthieu Dorier and Zhe Wang and Srinivasan Ramesh and Utkarsh Ayachit and Shane Snyder and Rob Ross and Manish Parashar},
  doi          = {10.1016/j.jpdc.2023.02.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {106-116},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards elastic in situ analysis for high-performance computing simulations},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TurBO: A cost-efficient configuration-based auto-tuning
approach for cluster-based big data frameworks. <em>JPDC</em>,
<em>177</em>, 89–105. (<a
href="https://doi.org/10.1016/j.jpdc.2023.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data processing frameworks such as Spark usually provide a large number of performance-related configuration parameters , how to auto-tune these parameters for a better performance has been a hot issue in academia as well as industry for years. Through delicately tradeoff between exploration and exploitation, Bayesian Optimization (BO) is currently the most appealing algorithm to achieve configuration auto-tuning. However, considering the tuning cost constraint in practice, there are three critical limitations preventing conventional BO-based approaches from being directly applied into auto-tuning cluster-based big data frameworks. In this paper, we propose a cost-efficient configuration auto-tuning approach named TurBO for big data frameworks based on two enhancements of vanilla BO:1) To reduce the essential iteration times , TurBO integrates a well-designed adaptive pseudo point mechanism with BO; 2) To avoid the time-consuming practical evaluation of sub-optimal configurations as possible, TurBO leverages the proposed CASampling method to intelligently tackle with these sub-optimal configurations based on ensemble learning with historical tuning experiences. To evaluate the performance of TurBO, we conducted a series of experiments on a local Spark cluster with 9 different HiBench benchmark applications . Overall, compared with 3 representative BO-based baseline approaches OpenTuner, Bliss and ResTune, TurBO is able to speedup the tuning procedures respectively by 2.24×, 2.29× and 1.97× on average. Besides, TurBO can always achieve a positive cumulative performance gain under the simulated dynamic workload scenario, which means TurBO is indeed appropriate for workload changes of big data applications .},
  archive      = {J_JPDC},
  author       = {Hui Dou and Lei Zhang and Yiwen Zhang and Pengfei Chen and Zibin Zheng},
  doi          = {10.1016/j.jpdc.2023.03.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {89-105},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {TurBO: A cost-efficient configuration-based auto-tuning approach for cluster-based big data frameworks},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PARMA-CC: A family of parallel multiphase approximate
cluster combining algorithms. <em>JPDC</em>, <em>177</em>, 68–88. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a common task in data analysis applications. Despite the extensive literature, the continuously increasing volumes of data produced by sensors (e.g., rates of several MB/s by 3D scanners such as LIDAR sensors), and the time-sensitivity of the applications leveraging the clustering outcomes (e.g., detecting critical situations such as detecting boundary crossing from a robot arm that could injure human beings) demand for efficient data clustering algorithms that can effectively utilize the increasing computational capacities of modern hardware. To that end, we leverage approximation and parallelization , where the former is to scale down the amount of data, and the latter is to scale up the computation. Regarding parallelization, we explore a design space for synchronization and workload distribution among the threads. As we study different parts of the design space, we propose representative Parallel Multiphase Approximate Cluster Combining, abbreviated as PARMA-CC, algorithms. We show that PARMA-CC algorithms yield equivalent clustering outcomes despite their different approaches. Furthermore, we show that certain PARMA-CC algorithms can achieve higher efficiency with respect to certain properties of the data to be clustered. Generally speaking, in PARMA-CC algorithms, parallel threads compute summaries associated with clusters of data (sub)sets. As the threads concurrently combine the summaries, they construct a comprehensive summary of the sets of clusters. By approximating a cluster with its respective geometrical summaries, PARMA-CC algorithms scale well with increasing data volumes, and, by computing and efficiently combining the summaries in parallel, they enable latency improvements. PARMA-CC algorithms utilize special data structures that enable parallelism through in-place data processing. As we show in our analysis and evaluation, PARMA-CC algorithms can complement and outperform well-established methods, with significantly better timeliness especially when utilizing multiple threads, while still providing highly accurate results in a variety of data sets, even with skewed data distributions, which cause the traditional approaches to exhibit their worst-case behaviour.},
  archive      = {J_JPDC},
  author       = {Amir Keramatian and Vincenzo Gulisano and Marina Papatriantafilou and Philippas Tsigas},
  doi          = {10.1016/j.jpdc.2023.02.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {68-88},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PARMA-CC: A family of parallel multiphase approximate cluster combining algorithms},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating matrix-centric graph processing on GPUs through
bit-level optimizations. <em>JPDC</em>, <em>177</em>, 53–67. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though it is well known that binary values are common in graph applications (e.g., adjacency matrix), how to leverage the phenomenon for efficiency has not yet been adequately explored. This paper presents a systematic study on how to unlock the potential of the bit-level optimizations of graph computations that involve binary values. It proposes a two-level representation named Bit-Block Compressed Sparse Row (B2SR) and presents a series of optimizations to the graph operations on B2SR by the intrinsics of modern GPUs . It additionally introduces Deep Reinforcement Learning (DRL) as an efficient way to best configure the bit-level optimizations on the fly. The DQN-based adaptive tile size selector with dedicated model training can reach 68\% prediction accuracy. Evaluations on the NVIDIA Pascal and Volta GPUs show that the optimizations bring up to 40 and 6555 for essential GraphBLAS kernels SpMV and SpGEMM, respectively, accelerating GraphBLAS-based BFS by up to 433 , SSSP, PR, and CC 35 , and TC 52 .},
  archive      = {J_JPDC},
  author       = {Jou-An Chen and Hsin-Hsuan Sung and Xipeng Shen and Nathan Tallent and Kevin Barker and Ang Li},
  doi          = {10.1016/j.jpdc.2023.02.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-67},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerating matrix-centric graph processing on GPUs through bit-level optimizations},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy enforcement in traditional non-SDN networks.
<em>JPDC</em>, <em>177</em>, 39–52. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Middleboxes are widely used in modern networks for a variety of network functions in cybersecurity, performance enhancement, and monitoring. Middlebox policy enforcement is however complex and tedious with unreliable manual re-configuration of legacy routers. The existing solution on automated policy enforcement relies on software-defined networking and does not apply to the traditional non-SDN networks, which remain popular today in enterprise deployment and core networks. This paper proposes a new architecture based entirely on software-defined middleboxes (instead of using software-defined switches in the prior art) to enable dependable and automated policy enforcement in non-SDN networks whose routers forward packets based on traditional routing protocols that are not policy-sensitive. We present a hot-potato enforcement strategy, which is then enhanced with two optimizations for load-balanced policy enforcement among software-defined middleboxes. Next, we propose two additional optimizations that minimize total traffic and aggregate end-to-end delays subject to link capacity constraints. Further enhancements are made to relieve middlebox processing overhead, avoid packet fragmentation due to policy enforcement, recover from failures, and mitigate delay for time-sensitive applications. We evaluate the proposed architecture on a real-life campus network topology and two simulated topologies to demonstrate the superior performance of our load-balanced enforcement strategies.},
  archive      = {J_JPDC},
  author       = {Olufemi Odegbile and Chaoyi Ma and Shigang Chen and Yuanda Wang},
  doi          = {10.1016/j.jpdc.2023.02.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {39-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Policy enforcement in traditional non-SDN networks},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient deterministic MapReduce algorithms for
parallelizable problems. <em>JPDC</em>, <em>177</em>, 28–38. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MapReduce framework has firmly established itself as one of the most widely used parallel computing platforms for processing big data on tera- and peta-byte scale. Approaching it from a theoretical standpoint has proved to be notoriously difficult, however. In continuation of Goodrich et al.&#39;s early efforts, explicitly espousing the goal of putting the MapReduce framework on footing equal to that of long-established models such as the PRAM, we investigate the obvious complexity question of how the computational power of MapReduce algorithms compares to that of combinational Boolean circuits commonly used for parallel computations. Relying on the standard MapReduce model introduced by Karloff et al. a decade ago, we develop an intricate simulation technique to show that any problem in NC NC (i.e., a problem solved by a logspace-uniform family of Boolean circuits of polynomial size and a depth polylogarithmic in the input size) can be solved by a MapReduce computation in O ( T ( n ) / log ⁡ n ) O(T(n)/log⁡n) rounds, where n is the input size and T ( n ) T(n) is the depth of the witnessing circuit family. Thus, we are able to closely relate the standard, uniform N C NC hierarchy modeling parallel computations to the deterministic MapReduce hierarchy D M R C DMRC by proving that N C i + 1 ⊆ D M R C i NCi+1⊆DMRCi for all i ∈ N i∈N . Besides the theoretical significance, this result has important applied aspects as well. In particular, we show for all problems in N C 1 NC1 —many practically relevant ones, such as integer multiplication and division, the parity function, and recognizing balanced strings of parentheses being among these—how to solve them in a constant number of deterministic MapReduce rounds.},
  archive      = {J_JPDC},
  author       = {Fabian Frei and Koichi Wada},
  doi          = {10.1016/j.jpdc.2023.02.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {28-38},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient deterministic MapReduce algorithms for parallelizable problems},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anthropomorphic diagnosis of runtime hidden behaviors in
OpenMP multi-threaded applications. <em>JPDC</em>, <em>177</em>, 17–27.
(<a href="https://doi.org/10.1016/j.jpdc.2023.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme-scale computing involves hundreds of millions of threads with multi-level parallelism running on large-scale hierarchical and heterogeneous hardware. Some OpenMP multi-threaded applications increasingly suffer from runtime hidden behaviors owning to shared resource contention as well as software- and hardware-related problems. Such hidden behaviors can result in failure and inefficiencies and are among the main challenges in system resiliency. To minimize the impact of hidden behaviors, one must quickly and accurately detect and diagnose the hidden behaviors that cause the failures. However, it is difficult to identify hidden behaviors in the dynamic and noisy data collected by OpenMP multi-threaded monitoring infrastructures. This paper presents an anthropomorphic diagnosis framework for hidden behaviors of OpenMP multi-threaded applications. In the framework, we first design injected heartbeat functions for OpenMP multi-threaded applications. Then, we leverage the heartbeat sequences to extract features of hidden behaviors. Finally, we develop a feature learning-based algorithm using heartbeat analysis, namely HSA, to diagnose hidden behaviors. To evaluate our framework, the NAS Parallel NPB benchmark, EPCC OpenMP micro-benchmark suite, and Jacobi benchmark are used to test the performance of our proposed framework. The experimental results demonstrate that our framework successfully identifies 90.3\% of the injected hidden behaviors of OpenMP multi-threaded applications while acquiring low overhead.},
  archive      = {J_JPDC},
  author       = {Weidong Wang and Dian Li and Wangda Luo and Yujian Kang and Liqiang Wang},
  doi          = {10.1016/j.jpdc.2023.02.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-27},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Anthropomorphic diagnosis of runtime hidden behaviors in OpenMP multi-threaded applications},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affinity-aware resource provisioning for long-running
applications in shared clusters. <em>JPDC</em>, <em>177</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource provisioning plays a pivotal role in determining the right amount of infrastructure resource to run applications and reduce the monetary cost. A significant portion of production clusters is now dedicated to long-running applications (LRAs), which are typically in the form of microservices and executed in the order of hours or even months. It is therefore practically important to plan ahead the placement of LRAs in a shared cluster for the minimized number of compute nodes required by them. Existing works on LRA scheduling are often application-agnostic, without particularly addressing the constraining requirements imposed by LRAs, such as co-location affinity constraints and time-varying resource requirements. In this paper, we present an affinity-aware resource provisioning approach for deploying large-scale LRAs in a shared cluster subject to multiple constraints, with the objective of minimizing the number of compute nodes in use. We investigate a broad range of solution algorithms which fall into three main categories: Application-Centric, Node-Centric, and Multi-Node approaches, and tune them for typical large-scale real-world scenarios. Experimental studies driven by the Alibaba Tianchi dataset show that our algorithms can achieve competitive scheduling effectiveness and running time, as compared with the heuristics used by the latest work including Medea and Lra Sched. Best results are obtained by the Application-Centric algorithms, if the algorithm&#39;s running time is of primary concern, and by Multi-Node algorithms, if the solution quality is of primary concern.},
  archive      = {J_JPDC},
  author       = {Clément Mommessin and Renyu Yang and Natalia V. Shakhlevich and Xiaoyang Sun and Satish Kumar and Junqing Xiao and Jie Xu},
  doi          = {10.1016/j.jpdc.2023.02.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Affinity-aware resource provisioning for long-running applications in shared clusters},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ParaLiNGAM: Parallel causal structure learning for linear
non-gaussian acyclic models. <em>JPDC</em>, <em>176</em>, 114–127. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key objectives in many fields in machine learning is to discover causal relationships among a set of variables from observational data. In linear non-Gaussian acyclic models (LiNGAM), it can be shown that the true underlying causal structure can be identified uniquely from merely observational data. The DirectLiNGAM algorithm is a well-known solution to learn the true causal structure in a high dimensional setting. DirectLiNGAM algorithm executes in a sequence of iterations and it performs a set of comparisons between pairs of variables in each iteration. Unfortunately, the runtime of this algorithm grows significantly as the number of variables increases. In this paper, we propose a parallel algorithm, called ParaLiNGAM, to learn casual structures based on DirectLiNGAM algorithm. We propose a threshold mechanism that can reduce the number of comparisons remarkably compared with the sequential solution. Moreover, in order to further reduce runtime, we employ a messaging mechanism between workers. We also present an implementation of ParaLiNGAM on GPU, considering hardware constraints. Experimental results on synthetic and real data show that our proposed solution outperforms DirectLiNGAM by a factor up to 4788X, and by a median of 2344X.},
  archive      = {J_JPDC},
  author       = {Amirhossein Shahbazinia and Saber Salehkaleybar and Matin Hashemi},
  doi          = {10.1016/j.jpdc.2023.01.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {114-127},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ParaLiNGAM: Parallel causal structure learning for linear non-gaussian acyclic models},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leaderless consensus. <em>JPDC</em>, <em>176</em>, 95–113.
(<a href="https://doi.org/10.1016/j.jpdc.2023.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic synchronous consensus algorithms are leaderless in that processes exchange proposals, choose the maximum value and decide when they see the same choice across a couple of rounds. Indulgent consensus algorithms are typically leader-based . Although they tolerate unexpected delays and find practical applications in blockchains running over an open network like the Internet, their performance is highly dependent on the activity of a single participant. This paper asks whether, under eventual synchrony, it is possible to deterministically solve consensus without a leader. The fact that the weakest failure detector to solve consensus is one that also eventually elects a leader seems to indicate that the answer to the question is negative. We prove in this paper that the answer is actually positive. We first give a precise definition of the very notion of a leaderless algorithm. Then we present three indulgent leaderless consensus algorithms, each we believe interesting in its own right: (i) for shared memory, (ii) for message passing with omission failures and (iii) for message passing with Byzantine failures . Finally, we implement a Byzantine fault tolerant (BFT) state machine replication (SMR), that is leaderless. Our empirical results demonstrate that it is faster and more robust than HotStuff, the recent BFT SMR algorithm used in the Facebook Libra blockchain when deployed in a wide area network.},
  archive      = {J_JPDC},
  author       = {Karolos Antoniadis and Julien Benhaim and Antoine Desjardins and Poroma Elias and Vincent Gramoli and Rachid Guerraoui and Gauthier Voron and Igor Zablotchi},
  doi          = {10.1016/j.jpdc.2023.01.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {95-113},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Leaderless consensus},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent DRL for joint completion delay and energy
consumption with queuing theory in MEC-based IIoT. <em>JPDC</em>,
<em>176</em>, 80–94. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Industrial Internet of Things (IIoT), there exist numerous sensor devices with weak computing power and small energy storage. To meet the real-time and big data computing requirements of industrial production, EIIoT (Edge computing-based IIoT) that combines mobile edge computing with IIoT has emerged. It is necessary to offload computing tasks to nearby edge servers for data storage and processing in EIIoT, thus inevitably causing the edge servers to overload. To this end, we propose a jointly constrained optimization model of delay and energy consumption based on queuing theory ; this model can effectively solve the task offloading problem in EIIoT. Subsequently, to satisfy the unique offloading requirements of EIIoT, we improve the MAPPO (multi agent proximal policy optimization) algorithm structure to form a lightweight optimal task offloading algorithm called Multi-Agent Deep Reinforcement Learning based on Queuing theory (MAQDRL), which is more suitable for EIIoT. In the algorithm, we systematically integrate queuing theory and use Multi-Agent Deep Reinforcement Learning (MADRL) to obtain the optimal offloading strategy in dynamic and random multiuser offloading environments. We also improve the structure of neural networks of MADRL by analyzing the structural characteristics of the input data. As a result, the algorithm that we proposed exhibits good convergence and exceptional performance in terms of the task arrival rate, bandwidth, energy consumption, latency and other indicators. The simulation results indicate that compared with other classical algorithms, MAQDRL is effective for solving the EIIoT offloading problem.},
  archive      = {J_JPDC},
  author       = {Guowen Wu and Zhiqi Xu and Hong Zhang and Shigen Shen and Shui Yu},
  doi          = {10.1016/j.jpdc.2023.02.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-94},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multi-agent DRL for joint completion delay and energy consumption with queuing theory in MEC-based IIoT},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial big data architecture: From data warehouses and data
lakes to the LakeHouse. <em>JPDC</em>, <em>176</em>, 70–79. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of systems supporting spatial data has experienced great enthusiasm in the past, due to the richness of this type of data and their semantics, which can be used in the decision-making process in various fields. Thus, the problem of integrating spatial data into existing databases and information systems has been addressed by creating spatial extensions to relational tables or by creating spatial data warehouses , while arranging data structures and query languages by making them more spatially-aware. With the advent of Big Data, these conventional storage and spatial representation structures are becoming increasingly outdated, and required a new organization of spatial data. Approaches based on distributed storage and data lakes have been proposed, to integrate the complexity of spatial data, with operational and analytical systems which unfortunately quickly showed their limits. Recently the concept of lakehouse was introduced in order to integrate, among other things, the notion of reliability and ACID properties to the volume of data to be managed. This new data architecture is a combination of governed and reliable Data Warehouses and flexible, scalable and cost-effective Data Lakes. In this paper, we present how traditional approaches of spatial data management in the context of spatial big data have quickly shown their limits. We present a literature overview of these approaches, and how they led to the Data LakeHouse. We detail how the Lakehouse paradigm can be used and extended for managing spatial big data, by giving the different components and best practices for building a spatial data LakeHouse architecture optimized for the storage and computing over spatial big data.},
  archive      = {J_JPDC},
  author       = {Soukaina Ait Errami and Hicham Hajji and Kenza Ait El Kadi and Hassan Badir},
  doi          = {10.1016/j.jpdc.2023.02.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Spatial big data architecture: From data warehouses and data lakes to the LakeHouse},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware fully-adaptive resource provisioning in
collaborative CPU-FPGA cloud environments. <em>JPDC</em>, <em>176</em>,
55–69. (<a href="https://doi.org/10.1016/j.jpdc.2023.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud warehouses have been exploiting multi-tenancy in CPU-FPGA collaborative environments, so clients can share the same infrastructure, achieving scalability and maximizing resource utilization. Therefore, the distribution of tasks across CPU and FPGA must be well-balanced so performance and energy are optimized in a highly variant workload scenario. In this paper, we take a step further and, in contrast to existing approaches, exploit DVFS (Dynamic Voltage and Frequency Scaling) on the CPU, together with an intelligent CPU-FPGA resource provisioning mechanism, to further improve energy. For that, we propose EASER, an end user-transparent framework that employs multiple strategies and dynamically selects the most appropriate one to optimize resource provisioning and DVFS according to the warehouse needs, workload properties, and target architecture. Our synergistic DVFS optimization brings up to 22\% additional energy gains over our dynamic provisioning alone. Compared to fixed single strategies with DVFS, EASER brings, on average, 71\% of energy gains.},
  archive      = {J_JPDC},
  author       = {Michael Guilherme Jordan and Guilherme Korol and Tiago Knorst and Mateus Beck Rutzig and Antonio Carlos Schneider Beck},
  doi          = {10.1016/j.jpdc.2023.02.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {55-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-aware fully-adaptive resource provisioning in collaborative CPU-FPGA cloud environments},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving cloud/edge sustainability through artificial
intelligence: A systematic review. <em>JPDC</em>, <em>176</em>, 41–54.
(<a href="https://doi.org/10.1016/j.jpdc.2023.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the increase in the use of services in cloud, fog, edge, and IoT ecosystems has been very notable. On the one hand, environmental sustainability is affected by this type of ecosystem since it can produce a large amount of energy consumption which translates into CO2 emissions into the atmosphere. On the other hand, due to the COVID-19 pandemic, the use of these ecosystems has increased considerably. Thus, it is necessary to apply policies and techniques to maximize sustainability within these ecosystems. Some of these policies and techniques are those based on artificial intelligence . However, the current processing of these policies and techniques can also consume a lot of resources. From this perspective, this article aims to clarify whether the sustainability of cloud/fog/edge/IoT ecosystems is improved by the application of artificial intelligence. To do this, a systematic literature review is developed in this paper. In addition, a set of classifications of the analyzed works is proposed based on the different aspects related to these ecosystems, their sustainability, and the applicability of artificial intelligence to improve them.},
  archive      = {J_JPDC},
  author       = {Belen Bermejo and Carlos Juiz},
  doi          = {10.1016/j.jpdc.2023.02.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {41-54},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving cloud/edge sustainability through artificial intelligence: A systematic review},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally solvable tasks and the limitations of valency
arguments. <em>JPDC</em>, <em>176</em>, 28–40. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An elegant strategy for proving impossibility results in distributed computing was introduced in the celebrated FLP consensus impossibility proof. This strategy is local in nature as at each stage, one configuration of a hypothetical protocol for consensus is considered, together with future decisions of possible extensions. This proof strategy has been used in numerous situations related to consensus, leading one to wonder why it has not been used in impossibility results of two other well-known tasks: set agreement and renaming . This paper provides an explanation of why impossibility proofs of these tasks have been of a global nature. It shows that a protocol can always solve such tasks locally, in the following sense. Given a configuration and all its future decisions, if a single successor configuration is selected, then the protocol can reveal all decisions in this branch of executions, satisfying the task specification. This result is shown for both set agreement and renaming, providing evidence that there are no local impossibility proofs for these tasks.},
  archive      = {J_JPDC},
  author       = {Hagit Attiya and Armando Castañeda and Sergio Rajsbaum},
  doi          = {10.1016/j.jpdc.2023.02.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {28-40},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Locally solvable tasks and the limitations of valency arguments},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VM performance-aware virtual machine migration method based
on ant colony optimization in cloud environment. <em>JPDC</em>,
<em>176</em>, 17–27. (<a
href="https://doi.org/10.1016/j.jpdc.2023.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many virtual machine (VM) allocation methods have been proposed to reduce the number of physical machines (PMs), improve resource utilization for cloud service providers . If VMs are migrated on the same PM, then there will be substantial resource competition among these VMs, which results in VM performance reduction. Many VM migration (VMM) methods neglect VM performance reduction. Although some performance-aware VM allocation methods were proposed, the performance optimization objective of these methods mainly aimed at guaranteeing service level agreement or reducing VM downtime during migration, and they did not scientifically analyze how the VM performance degrades. Therefore, how to minimize the VM performance reduction for users when migrating VMs remains a major challenge. This paper proposes a VM Performance-Aware VMM method (PAVMM) for both users and cloud service providers . To maximize VM performance for users, it utilizes the VM performance model, which was built in our previous works, to predict the VM performance after migrating VMs. It then establishes an optimization objective of maximizing VM performance for users. Meanwhile, minimizing the number of active PMs and the total migration cost are regarded as additional optimization objectives for cloud service providers. Therefore, we formulate VMM as a multi-objective optimization problem , which tries to maximize VM performance for users and minimize the number of active PMs and the total migration cost for cloud service providers simultaneously. Then an ant colony optimization (ACO)-based algorithm is proposed to solve the NP-hard VMM problem. Lastly, the experiments are conducted to evaluate PAVMM, and the results verify its efficiency.},
  archive      = {J_JPDC},
  author       = {Hui Zhao and Nanzhi Feng and Jianhua Li and Guobin Zhang and Jing Wang and Quan Wang and Bo Wan},
  doi          = {10.1016/j.jpdc.2023.02.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-27},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {VM performance-aware virtual machine migration method based on ant colony optimization in cloud environment},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware mapping and scheduling strategies for real-time
workflows under reliability constraints. <em>JPDC</em>, <em>176</em>,
1–16. (<a href="https://doi.org/10.1016/j.jpdc.2023.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on energy minimization for the mapping and scheduling of real-time workflows under reliability constraints. Workflow instances are input periodically to the system. Each instance is composed of several tasks and must complete execution before the arrival of the next instance, and with a prescribed reliability threshold . While the shape of the dependence graph is identical for each instance, task execution times are stochastic and vary from one instance to the next. The reliability threshold is met by executing several replicas for each task. The target platform consists of identical processors equipped with Dynamic Voltage and Frequency Scaling (DVFS) capabilities. A different frequency can be assigned to each task replica to save energy, but it may have negative effect on the deadline and reliability target. This difficult tri-criteria mapping and scheduling problem (energy, deadline, reliability) has been studied only recently for workflows with arbitrary dependence constraints. We investigate new mapping and scheduling strategies based upon layers in the task graph. These strategies better balance replicas across processors, thereby decreasing the time overlap between different replicas of a given task, and saving energy . We compare these strategies with two state-of-the-art approaches and a reference baseline on a variety of benchmark workflows. Our best heuristics achieve an average energy gain of 60\% over the competitors and of 82\% over the baseline.},
  archive      = {J_JPDC},
  author       = {Zhiwei Wu and Li Han and Jing Liu and Yves Robert and Frédéric Vivien},
  doi          = {10.1016/j.jpdc.2023.02.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-aware mapping and scheduling strategies for real-time workflows under reliability constraints},
  volume       = {176},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance modeling on DaVinci AI core. <em>JPDC</em>,
<em>175</em>, 134–149. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive use of Deep Neural Networks (DNNs) encourages people to design domain-specific hardware called Artificial Intelligence (AI) processors. The novel hardware makes optimizations challenging without a proper performance model that reveals working details and performance implications. This paper presents a performance model, Verrocchio, for Huawei DaVinci AI Core, which predicts the execution time of real-world DaVinci kernels. We propose specially-crafted micro-benchmarks to identify contention source, runtime behaviors, and bandwidth sharing, which significantly determine performance. Since DaVinci Core adopts a binary semaphore mechanism for synchronization, Verrocchio views each instruction as a discrete event and manages its execution time based on the programming logic. For evaluation, Verrocchio achieves average error rates of 2.62\% and 2.30\% in sample kernels for single-core and double-core execution. We demonstrate an optimizing process of matrix multiplications with Verrocchio, achieving speedups of 1.70× for operators and 1.53× for applications and error rates of 5.06\% and 5.25\%.},
  archive      = {J_JPDC},
  author       = {Yifeng Tang and Cho-li Wang},
  doi          = {10.1016/j.jpdc.2023.01.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {134-149},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance modeling on DaVinci AI core},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing energy consumption using heterogeneous voltage
frequency scaling of data-parallel applications for multicore systems.
<em>JPDC</em>, <em>175</em>, 121–133. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the exploitation of heterogeneous DVFS (dynamic voltage frequency scaling) control for improving the energy efficiency of data-parallel applications on ccNUMA shared-memory systems. We propose to adjust the clock frequency individually for the appropriately selected groups of cores, taking into account the diversified costs of parallel computation. This paper aims to evaluate the proposed approach using two different data-parallel applications: solving the 3D diffusion problem , and MPDATA fluid dynamics application. As a result, we observe the energy-savings gains of up to 20 percentage points over the traditional homogeneous frequency scaling approach on the server with two 18-core Intel Xeon Gold 6240. Additionally, we confirm the effectiveness of our strategy using two 64-core AMD EPYC 7773X. This paper also introduces two pruning algorithms that help select the optimal heterogeneous DVFS setups taking into account the energy or performance profile of studied applications. Finally, the cost and efficiency of developed algorithms are verified and compared experimentally against the brute-force search.},
  archive      = {J_JPDC},
  author       = {Pawel Bratek and Lukasz Szustak and Roman Wyrzykowski and Tomasz Olas},
  doi          = {10.1016/j.jpdc.2023.01.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {121-133},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reducing energy consumption using heterogeneous voltage frequency scaling of data-parallel applications for multicore systems},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring job running path to predict runtime on multiple
production supercomputers. <em>JPDC</em>, <em>175</em>, 109–120. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are massive jobs submitted in the supercomputer , and the job management system is typically deployed to schedule these jobs and allocate compute resources. FCFS (First Come First Serve) is a popular scheduling policy and the job&#39;s priority is determined based on the arrival time. However, under the FCFS strategy, if existing idle resources cannot meet the requirement of the head job in the waiting queue, they cannot be allocated to other jobs, which suffers from resource waste. To optimize the resource utilization, the backfilling method is proposed, which allocates the reserved idle compute nodes to a small-size and short-running non-head job, on the premise of not delaying the original head job. Obtaining the job&#39;s runtime in advance is necessary for backfilling and the traditional method relies on the user&#39;s estimation. Unfortunately, the estimated runtime provided by users is generally overestimated. Many studies extract features from historical job logs and adopt machine learning to predict the runtime. However, traditional features are insufficient to characterize the job. In this paper, we collect job logs from two supercomputers and present a novel runtime prediction framework called PREP. PREP explores the job&#39;s running path as a new feature, which implies plentiful information about the job&#39;s properties, such as the user, the project, the scale of data sets, and the parameters used. As there is a strong correlation between the job&#39;s runtime and its running path, we group jobs with similar paths into a cluster and train a runtime prediction model for each cluster respectively. Extensive evaluations demonstrate that introducing the new feature can achieve higher prediction accuracy (88.5\% and 82.3\% in two production supercomputers respectively), and our framework has a more desirable prediction performance than other popular strategies like Last-2 and IRPA. In addition, the predicted runtime is inserted into the real job trace of a slurm simulator to verify the advantages of PREP.},
  archive      = {J_JPDC},
  author       = {Wenxiang Yang and Xiangke Liao and Dezun Dong and Jie Yu},
  doi          = {10.1016/j.jpdc.2023.01.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {109-120},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exploring job running path to predict runtime on multiple production supercomputers},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource optimizing federated learning for use with IoT: A
systematic review. <em>JPDC</em>, <em>175</em>, 92–108. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Federated Learning (FL) has been explored as a new paradigm that preserves both data privacy and end-users knowledge while reducing latency during model training. While often applied between a central server and the edge devices, FL deals with a wide range of end-user applications and devices. Given the Internet of Things (IoT) current popularity, its relevance and its penetration in several new application domains, this work examines the new challenges faced when combining IoT with the classic FL model. The limited resources of IoT edge devices require careful adaptation of the way how FL should be structured in this scenario. In addition, since FL is a distributed paradigm that shares deep learning artifacts through a network, there are also communication issues reminiscent to IoT networks that need special consideration. Thus, it is necessary to optimize the use of both processing and communication resources when considering the use of IoT edge devices as part of a FL. This paper systematically reviews current advances and steps taken towards dealing with resource optimization when using FL as part of IoT scenarios. We examine the published works over the last ten years and discuss the main goals, scenarios, and developed solutions to solve the problems encountered. We also present the main metrics used to quantify the effectiveness and to evaluate the performance of existing IoT based architectures with FL support.},
  archive      = {J_JPDC},
  author       = {Leylane Graziele Ferreira da Silva and Djamel F.H. Sadok and Patricia Takako Endo},
  doi          = {10.1016/j.jpdc.2023.01.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {92-108},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Resource optimizing federated learning for use with IoT: A systematic review},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extreme flow decomposition for multi-source multicast with
intra-session network coding. <em>JPDC</em>, <em>175</em>, 80–91. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network coding (NC), when combined with multipath routing , enables a linear programming (LP) formulation for a multi-source multicast with intra-session network coding (MISNC) problem. However, it is still hard to solve using conventional methods due to the enormous scale of variables or constraints. In this paper, we try to solve this problem in terms of throughput maximization from an algorithmic perspective. We propose a novel formulation based on the extreme flow decomposition technique, which facilitates the design and analysis of approximation and online algorithms . For the offline scenario, we develop a fully polynomial time approximation scheme (FPTAS) which can find a ( 1 + ω ) (1+ω) -approximation solution for any specified ω &gt; 0 ω&amp;gt;0 . For the online scenario, we develop an online primal-dual algorithm which proves O ( 1 ) O(1) -competitive and violates link capacities by a factor of O ( log ⁡ m ) O(log⁡m) , where m is the link number. The proposed algorithms share an elegant primal-dual form and thereby have inherent advantages of simplicity, efficiency, and scalability. To better understand the proposed approach, we devise delicate numerical examples on an extended butterfly network to validate the effects of algorithmic parameters and make an interesting comparison between the offline and online cases. We also perform large-scale simulations on real networks to validate the effectiveness of the proposed FPTAS and online algorithm.},
  archive      = {J_JPDC},
  author       = {Jianwei Zhang},
  doi          = {10.1016/j.jpdc.2023.01.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-91},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Extreme flow decomposition for multi-source multicast with intra-session network coding},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor slicing and optimization for multicore NPUs.
<em>JPDC</em>, <em>175</em>, 66–79. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although code generation for Convolution Neural Network (CNN) models has been extensively studied, performing efficient data slicing and parallelization for highly-constrained Multicore Neural Processor Units (NPUs) is still a challenging problem. Given the size of convolutions&#39; input/output tensors and the small footprint of NPU on-chip memories, minimizing memory transactions while maximizing parallelism and MAC utilization are central to any effective solution. This paper proposes a TensorFlow XLA/LLVM compiler optimization pass for Multicore NPUs, called Tensor Slicing Optimization (TSO), which: (a) maximizes convolution parallelism and memory usage across NPU cores; and (b) reduces data transfers between host and NPU on-chip memories by using DRAM memory burst time estimates to guide tensor slicing. To evaluate the proposed approach, a set of experiments was performed using the NeuroMorphic Processor (NMP), a multicore NPU containing 32 RISC-V cores extended with novel CNN instructions. Experimental results show that TSO is capable of identifying the best tensor slicing that minimizes execution time for a set of CNN models. Speed-ups of up to 21.7\% result when comparing the TSO burst-based technique to a no-burst data slicing approach. To validate the generality of the TSO approach, the algorithm was also ported to the Glow Machine Learning framework. The performance of the models were measured on both Glow and TensorFlow XLA/LLVM compilers, revealing similar results.},
  archive      = {J_JPDC},
  author       = {Rafael Sousa and Marcio Pereira and Yongin Kwon and Taeho Kim and Namsoon Jung and Chang Soo Kim and Michael Frank and Guido Araujo},
  doi          = {10.1016/j.jpdc.2022.12.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {66-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Tensor slicing and optimization for multicore NPUs},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Programming parallel dense matrix factorizations and
inversion for new-generation NUMA architectures. <em>JPDC</em>,
<em>175</em>, 51–65. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a methodology to address the programmability issues derived from the emergence of new-generation shared-memory NUMA architectures. For this purpose, we employ dense matrix factorizations and matrix inversion (DMFI) as a use case, and we target two modern architectures (AMD Rome and Huawei Kunpeng 920) that exhibit configurable NUMA topologies. Our methodology pursues performance portability across different NUMA configurations by proposing multi-domain implementations for DMFI plus a hybrid task- and loop-level parallelization that configures multi-threaded executions to fix core-to-data binding, exploiting locality at the expense of minor code modifications. In addition, we introduce a generalization of the multi-domain implementations for DMFI that offers support for virtually any NUMA topology in present and future architectures. Our experimentation on the two target architectures for three representative dense linear algebra operations validates the proposal, reveals insights on the necessity of adapting both the codes and their execution to improve data access locality, and reports performance across architectures and inter- and intra-socket NUMA configurations competitive with state-of-the-art message-passing implementations, maintaining the ease of development usually associated with shared-memory programming.},
  archive      = {J_JPDC},
  author       = {Sandra Catalán and Francisco D. Igual and José R. Herrero and Rafael Rodríguez-Sánchez and Enrique S. Quintana-Ortí},
  doi          = {10.1016/j.jpdc.2023.01.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {51-65},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Programming parallel dense matrix factorizations and inversion for new-generation NUMA architectures},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). PAGroup: Privacy-aware grouping framework for
high-performance federated learning. <em>JPDC</em>, <em>175</em>, 37–50.
(<a href="https://doi.org/10.1016/j.jpdc.2022.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning is designed for multiple mobile devices to collaboratively train an artificial intelligence model while preserving data privacy. Instead of collecting the raw training data from mobile devices to the cloud, Federated Learning coordinates a group of devices to train a shared model in a distributed manner with the training data located on the devices. However, unbalanced data distribution and heterogeneous hardware configurations across different devices badly hurts the performance of collaborative model and severely impacts the overall training progress. Thus, a framework that can well balance the model accuracy and the training progress is urgently required. In this paper, we propose PAGroup, a privacy-aware grouping framework for high-performance Federated Learning. PAGroup intelligently divides the participating clients into different groups through carefully analyzing the privacy requirement of the training data and the solid social relationship of the participating clients. After that, PAGroup conducts data shaping and capability-ware training in order to improve the model performance while accelerating the overall training process. We evaluate the performance of PAGroup with both simulation and hardware testbed . The evaluation results show that PAGroup improves model accuracy up to 21\%. Meanwhile, it decreases 81\% communication overhead , 29\% computation cost and 84\% wall-clock time at best comparing with the baselines.},
  archive      = {J_JPDC},
  author       = {Tao Chang and Li Li and MeiHan Wu and Wei Yu and Xiaodong Wang and ChengZhong Xu},
  doi          = {10.1016/j.jpdc.2022.12.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {37-50},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PAGroup: Privacy-aware grouping framework for high-performance federated learning},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EdgeDecAp: An auction-based decentralized algorithm for
optimizing application placement in edge computing. <em>JPDC</em>,
<em>175</em>, 22–36. (<a
href="https://doi.org/10.1016/j.jpdc.2023.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing , application components can be placed over a range of computational devices from cloud data centers to nodes at the network edge. Application placement can have significant impact on important metrics like latency and resource utilization. Thus, application placement is an important optimization problem . In edge computing, the characteristics of both the infrastructure and the application may change over time, which may require the dynamic re-optimization of the application placement. Most algorithms suggested so far for the dynamic re-optimization of edge application placement are centralized, i.e., they rely on one entity collecting information from the whole infrastructure and making decisions centrally. However, centralized approaches suffer from limited scalability and are vulnerable to failures. In this paper, we present a decentralized approach for the dynamic re-optimization of edge application placement. We adopt an algorithm of Malek et al. for distributed systems and modify it to make it applicable to edge computing. In this approach, each node makes decisions autonomously, using auctions for coordination. Our empirical results demonstrate that the proposed algorithm is very effective in optimizing edge application placement. In an edge system with 637 edge nodes and 563 end devices, our algorithm achieves 54\% higher reduction of application latency than a previous decentralized algorithm.},
  archive      = {J_JPDC},
  author       = {Sven Smolka and Leon Wißenberg and Zoltán Ádám Mann},
  doi          = {10.1016/j.jpdc.2023.01.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {22-36},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {EdgeDecAp: An auction-based decentralized algorithm for optimizing application placement in edge computing},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly-based intrusion detection system in the internet of
things using a convolutional neural network and multi-objective enhanced
capuchin search algorithm. <em>JPDC</em>, <em>175</em>, 1–21. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the growth and pervasiveness of Internet of Things (IoT) devices have led to increased attacks by hackers and attackers. On the other hand, using IoT infrastructure in various fields has increased the number of node security breaches , attacks, and anomalies. Therefore, detecting anomalies in IoT devices is vital to reduce attacks and strengthen security. Over the past few years, various research has been conducted in anomaly-based intrusion detection using machine learning and deep learning methods . The biggest challenge in machine learning methods is the inability to extract new features. To do this, researchers use deep learning methods to extract new features that lead to increased accuracy in intrusion detection . There are important unsolved challenges in research, including determining important features in detecting malicious attacks , extracting features from raw network traffic data using deep networks, and insufficient accuracy in detecting attacks against IoT devices. Convolutional neural networks are considered a powerful and reliable method in this field due to the ability to automatically extract features from data and perform faster calculations. This study has designed and implemented the IoT features extraction convolutional neural network called IoTFECNN with hybrid layers for better anomaly detection in the IoT. Moreover, a binary multi-objective enhanced Capuchin Search Algorithm (CSA) called BMECapSA is developed for efficient feature selection. The combination of the IoTFECNN and BMECapSA methods has led to the introduction of a new hybrid method called CNN-BMECapSA-RF. Finally, the proposed method is implemented and tested on two data sets, NSL-KDD and TON-IoT. The results of various experiments exhibit that the proposed method has better results regarding classification criteria compared to existing deep learning and machine learning-based anomaly detection systems. The proposed method has reached 99.99\% and 99.85\% accuracy by identifying 27\% and 44\% of the effective features on the TON-IoT and NSL-KDD datasets, respectively.},
  archive      = {J_JPDC},
  author       = {Hossein Asgharzadeh and Ali Ghaffari and Mohammad Masdari and Farhad Soleimanian Gharehchopogh},
  doi          = {10.1016/j.jpdc.2022.12.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-21},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Anomaly-based intrusion detection system in the internet of things using a convolutional neural network and multi-objective enhanced capuchin search algorithm},
  volume       = {175},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel global edge switching for the uniform sampling of
simple graphs with prescribed degrees. <em>JPDC</em>, <em>174</em>,
118–129. (<a href="https://doi.org/10.1016/j.jpdc.2022.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uniform sampling of simple graphs matching a prescribed degree sequence is an important tool in network science, e.g. to construct graph generators or null-models. Here, the Edge Switching Markov Chain (ES-MC) is a common choice. Given an arbitrary simple graph with the required degree sequence, ES-MC carries out a large number of small changes, called edge switches, to eventually obtain a uniform sample. In practice, reasonably short runs efficiently yield approximate uniform samples. In this work, we study the problem of executing edge switches in parallel. We discuss parallelizations of ES-MC, but find that this approach suffers from complex dependencies between edge switches. For this reason, we propose the Global Edge Switching Markov Chain (G-ES-MC), an ES-MC variant with simpler dependencies. We show that G-ES-MC converges to the uniform distribution and design shared-memory parallel algorithms for ES-MC and G-ES-MC. In an empirical evaluation, we provide evidence that G-ES-MC requires not more switches than ES-MC (and often fewer), and demonstrate the efficiency and scalability of our parallel G-ES-MC implementation.},
  archive      = {J_JPDC},
  author       = {Daniel Allendorf and Ulrich Meyer and Manuel Penschuck and Hung Tran},
  doi          = {10.1016/j.jpdc.2022.12.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {118-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel global edge switching for the uniform sampling of simple graphs with prescribed degrees},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). List and shelf schedules for independent parallel tasks to
minimize the energy consumption with discrete or continuous speeds.
<em>JPDC</em>, <em>174</em>, 100–117. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling independent tasks on a parallel platform is a widely-studied problem, in particular when the goal is to minimize the total execution time , or makespan ( P | | C max P||Cmax problem in Graham&#39;s notations). Also, many applications do not consist of sequential tasks, but rather parallel tasks, either rigid, with a fixed degree of parallelism , or moldable, with a variable degree of parallelism (i.e., for which we can decide at the execution on how many processors they are executed). Furthermore, since the energy consumption of data centers is a growing concern, both from an environmental and economical point of view, minimizing the energy consumption of a schedule is a main challenge to be addressed. One can then decide, for each task, on how many processors it is executed, and at which speed the processors are operated, with the goal to minimize the total energy consumption . We further focus on co-schedules, where tasks are partitioned into shelves, and we prove that the problem of minimizing the energy consumption remains NP-complete when static energy is consumed during the whole duration of the application. We are however able to provide an optimal algorithm for the schedule within one shelf, i.e., for a set of tasks that start at the same time. Several approximation results are derived, both with discrete and continuous speed models, and extensive simulations are performed to show the performance of the proposed algorithms.},
  archive      = {J_JPDC},
  author       = {Anne Benoit and Louis-Claude Canon and Redouane Elghazi and Pierre-Cyrille Héam},
  doi          = {10.1016/j.jpdc.2022.12.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {100-117},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {List and shelf schedules for independent parallel tasks to minimize the energy consumption with discrete or continuous speeds},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective cloud energy optimizer algorithm for
federated environments. <em>JPDC</em>, <em>174</em>, 81–99. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy is a considerable portion of the cost of a cloud data center . In addition to energy, carbon emission tax imposes another cost on data centers, which have different policies in different cities around the world. Most of the cloud data centers are geographically distributed, which can have a huge advantage in reducing such costs. In addition, in recent years, cloud federation, in which multiple cloud providers share their IT infrastructures voluntarily, could play a crucial role in minimizing the energy consumption of cloud data centers. On the other hand, improper VM placement results frequent VM migration, constant turning off/on physical machines, service quality degradation and energy consumption increase. To tackle these weaknesses, we propose a multi-objective algorithm, Called OUR-ACS, to minimize data centers&#39; energy consumption, carbon emission, and the total expenses (energy cost + carbon tax) in a federated environment considering both initial VM placement and VM consolidation approaches. We evaluate the efficiency of our proposed algorithm by using the ClousSim Plus simulator toolkit using real-world datasets. Compared to the competing algorithms, our simulation results indicate that our proposed algorithm could reduce the total energy consumption , carbon emission, and the total costs of multiple cloud data centers by an average of 37.6\%, 41\%, and 25\%, respectively, in a federated environment .},
  archive      = {J_JPDC},
  author       = {Ehsan Khodayarseresht and Alireza Shameli-Sendi},
  doi          = {10.1016/j.jpdc.2022.12.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {81-99},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A multi-objective cloud energy optimizer algorithm for federated environments},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A federated collaborative recommendation model for
privacy-preserving distributed recommender applications based on
microservice framework. <em>JPDC</em>, <em>174</em>, 70–80. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide adoption of IoT technologies has accelerated the accumulation of big data. Recommender systems (RS) is one of the most effective methods to extract user interested items from the huge volume of big data. However, implementing a recommender system over the distributed error-prone IoT devices faces two challenges. On the one hand, the distributed IoT devices may randomly fail to deliver its local data due to hardware malfunction, which may cause unavailability of the recommender system. Moreover, collecting the raw data from the distributed IoT devices may cause data privacy leakage issue, since the privacy data of user-item interaction records may be abused by vicious parties. In view of these challenges, we propose a federated collaborative recommendation model based on microservice framework in this paper to implement privacy preserving distributed recommendation applications. Firstly, we utilize the federated learning framework to train the collaborative recommendation model, where the raw data on each distributed device is kept locally and only item related model parameters are exposed to train the federated recommendation model. Moreover, we adopt the microservice framework to encapsulate different functions of the federated recommender model. Each distributed device can participate in the federated training process via service registration and service discovery function of the microservice framework. Furthermore, we enhance the typical Neural Collaborative Filtering model with the proposed FedNeuMF model by fusing auxiliary user profiles and item attributes to improve the recommendation accuracy. Finally, we conduct a set of experiments on three real-world datasets to check the performance of our proposal.},
  archive      = {J_JPDC},
  author       = {Wenmin Lin and Hui Leng and Ruihan Dou and Lianyong Qi and Zhigeng Pan and Md. Arafatur Rahman},
  doi          = {10.1016/j.jpdc.2022.12.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-80},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A federated collaborative recommendation model for privacy-preserving distributed recommender applications based on microservice framework},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal placement of applications in the fog environment: A
systematic literature review. <em>JPDC</em>, <em>174</em>, 46–69. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fog-computing paradigm complements cloud computing to support the deployment and execution of latency-sensitive applications at the network edge by offering enhanced computational power. Optimal placement of such applications over a fog network comprising geographically distributed, heterogeneous, and resource-constrained fog nodes is a core challenge in fog-computing paradigm research. This study systematically reviews existing research on optimal fog application placement over the cloud-to-thing continuum. Surveyed articles are analyzed in four aspects: i) layers of the cloud-to-thing continuum considered for placing an application; ii) application characteristics that are considered in making placement decisions; iii) application placement mechanism; iv) tools and technology for placing an application. This review also categorizes the research problems associated with fog application placement. Finally, based on this review, we suggest directions for future adaptive fog-application placement research.},
  archive      = {J_JPDC},
  author       = {Mohammad Mainul Islam and Fahimeh Ramezani and Hai Yan Lu and Mohsen Naderpour},
  doi          = {10.1016/j.jpdc.2022.12.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {46-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimal placement of applications in the fog environment: A systematic literature review},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LOCATOR: Low-power ORB accelerator for autonomous cars.
<em>JPDC</em>, <em>174</em>, 32–45. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization And Mapping (SLAM) is crucial for autonomous navigation . ORB-SLAM is a state-of-the-art Visual SLAM system based on cameras used for self-driving cars. In this paper, we propose a high-performance, energy-efficient, and functionally accurate hardware accelerator for ORB-SLAM, focusing on its most time-consuming stage: Oriented FAST and Rotated BRIEF (ORB) feature extraction. The Rotated BRIEF (rBRIEF) descriptor generation is the main bottleneck in ORB computation, as it exhibits highly irregular access patterns to local on-chip memories causing a high-performance penalty due to bank conflicts. We introduce a technique to find an optimal static pattern to perform parallel accesses to banks based on a genetic algorithm . Furthermore, we propose the combination of an rBRIEF pixel duplication cache, selective ports replication, and pipelining to reduce latency without compromising cost. The accelerator achieves a reduction in energy consumption of 14597× and 9609×, with respect to high-end CPU and GPU platforms, respectively.},
  archive      = {J_JPDC},
  author       = {Raúl Taranco and José-Maria Arnau and Antonio González},
  doi          = {10.1016/j.jpdc.2022.12.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-45},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {LOCATOR: Low-power ORB accelerator for autonomous cars},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based data prefetching in CPU-GPU unified
virtual memory. <em>JPDC</em>, <em>174</em>, 19–31. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unified Virtual Memory (UVM) relieves the developers from the onus of maintaining complex data structures and explicit data migration by enabling on-demand data movement between CPU memory and GPU memory. However, on-demand paging soon becomes a performance bottleneck of UVM due to the high latency caused by page table walks and data migration over interconnect. Prefetching is considered a promising solution to this problem given its ability to leverage the locality of program memory access patterns . However, existing locality-based prefetching schemes can not handle all the situations. An ideal prefetcher should not only look at narrow regions of the requested address space but also capture global context to deliver a good prediction of the memory access pattern. This paper proposes a novel framework for page prefetching for UVM through deep learning . We first show that a powerful Transformer learning model can provide high accuracy for UVM page prefetching. We then perform analysis to interpret this Transformer model and derive several insights that allow us to design a simpler model to match the unconstrained model&#39;s accuracy with orders of magnitude lower cost. We use a pattern-based method to make the UVM page preditor general to different GPU workloads. We evaluate this framework on a set of 11 memory-intensive benchmarks from popular benchmark suites. Our solution outperforms the state-of-the-art (SOTA) UVM framework, improving the performance by 10.89\%, improving the device memory page hit rate by 16.98\% (89.02\% vs. 76.10\% for prior art), and reducing the CPU-GPU interconnect traffic by 11.05\%. According to our proposed unified metric, which combines the accuracy, coverage, and page hit rate, our solution is approaching the ideal prefetching scheme more than the SOTA design (0.90 vs. 0.85, with the perfect prefetcher of 1.0).},
  archive      = {J_JPDC},
  author       = {Xinjian Long and Xiangyang Gong and Bo Zhang and Huiyang Zhou},
  doi          = {10.1016/j.jpdc.2022.12.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {19-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Deep learning based data prefetching in CPU-GPU unified virtual memory},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving concurrency and memory usage in distributed
operating systems for lightweight manycores via cooperative time-sharing
lightweight tasks. <em>JPDC</em>, <em>174</em>, 2–18. (<a
href="https://doi.org/10.1016/j.jpdc.2022.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight manycore processors arise to reconcile performance, energy efficiency, and scalability requirements on a single chip . Operating Systems (OSes) for these processors feature a distributed design, where isolated OS instances cooperate to mitigate programmability and portability issues coming from their architectural intricacies. Currently, OS services often resort to traditional execution flow abstractions (processes or threads) to implement small, periodic, or asynchronous functionalities. Although these abstractions considerably simplify the system design, they have a non-negotiable impact on the limited on-chip memories. Due to the memory restrictions, we argue that OS-level abstractions can be reshaped to reduce the OS memory footprint without introducing considerable overhead. In this context, we propose a complementary OS-level execution engine that supports cooperative time-sharing lightweight tasks that share a unique execution stack and features task synchronization via control flow and dependency graphs. This solution is orthogonal to the underlying execution support and provides numerous OS-level execution flows with reduced memory consumption. We implemented our engine in a distributed OS and executed experiments on a lightweight manycore. Our results show that it has the following advantages when compared to the classical thread abstraction: (i) it provides 63.2× more execution flows per MB of memory; (ii) it features less overhead to manage execution flows and system calls; (iii) it improves core utilization; and (iv) it exhibits competitive results on real-world applications.},
  archive      = {J_JPDC},
  author       = {João Vicente Souto and Márcio Castro},
  doi          = {10.1016/j.jpdc.2022.12.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {2-18},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving concurrency and memory usage in distributed operating systems for lightweight manycores via cooperative time-sharing lightweight tasks},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “improvement of recommendation algorithm
based on collaborative deep learning and its parallelization on spark”
[j. Parallel distrib. Comput. 148 (2021) 58–68]. <em>JPDC</em>,
<em>174</em>, 1. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Fan Yang and Huaqiong Wang and Jianjing Fu},
  doi          = {10.1016/j.jpdc.2022.11.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Corrigendum to “Improvement of recommendation algorithm based on collaborative deep learning and its parallelization on spark” [J. parallel distrib. comput. 148 (2021) 58–68]},
  volume       = {174},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analytical performance estimation during code generation on
modern GPUs. <em>JPDC</em>, <em>173</em>, 152–167. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. We propose an alternative to time-intensive autotuning , scenario-specific performance models, or black-box machine learning to select the best-performing configuration. This paper identifies the relevant performance-defining mechanisms for memory-intensive GPU applications through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient code candidates with high accuracy. We examine the changes of the A100 GPU architecture compared to the predecessor V100 and address the challenges of how to model the data transfer volumes through the new memory hierarchy. We show how our method can be coupled to the “pystencils” stencil code generator , which is used to generate kernels for a range-four 3D-25pt stencil and a complex two-phase fluid solver based on the Lattice Boltzmann Method . For both, it delivers a ranking that can be used to select the best-performing candidate. The method is not limited to stencil kernels but can be integrated into any code generator that can generate the required address expressions.},
  archive      = {J_JPDC},
  author       = {Dominik Ernst and Markus Holzer and Georg Hager and Matthias Knorr and Gerhard Wellein},
  doi          = {10.1016/j.jpdc.2022.11.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {152-167},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Analytical performance estimation during code generation on modern GPUs},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SECDA-TFLite: A toolkit for efficient development of
FPGA-based DNN accelerators for edge inference. <em>JPDC</em>,
<em>173</em>, 140–151. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose SECDA-TFLite, a new open source toolkit for developing DNN hardware accelerators integrated within the TFLite framework. The toolkit leverages the principles of SECDA , a hardware/software co-design methodology, to reduce the design time of optimized DNN inference accelerators on edge devices with FPGAs . With SECDA-TFLite, we reduce the initial setup costs associated with integrating a new accelerator design within a target DNN framework, allowing developers to focus on the design. SECDA-TFLite also includes modules for cost-effective SystemC simulation, profiling, and AXI-based data communication. As a case study, we use SECDA-TFLite to develop and evaluate three accelerator designs across seven common CNN models and two BERT-based models against an ARM A9 CPU-only baseline, achieving an average performance speedup across models of up to 3.4× for the CNN models and of up to 2.5× for the BERT-based models. Our code is available at https://github.com/gicLAB/SECDA-TFLite .},
  archive      = {J_JPDC},
  author       = {Jude Haris and Perry Gibson and José Cano and Nicolas Bohm Agostini and David Kaeli},
  doi          = {10.1016/j.jpdc.2022.11.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {140-151},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SECDA-TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the impact of mode transition on phased transactional
memory performance. <em>JPDC</em>, <em>173</em>, 126–139. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several transactional memory implementations that employ state-of-the-art software and hardware techniques to deliver performance have been investigated in the last decade. Phased-based Transactional Memory (PhTM) systems run transactions in phases, such that all transactions in a phase execute in the same (hardware/software) mode. In PhTM, a runtime monitors the execution and decides when to change all transactions to another execution mode. Identifying the right moment to perform a mode transition is a central problem to achieve performance in PhTM systems. This article analyzes PhTM and provides a characterization of mode transitions and their impact on performance. We consider three PhTM implementations: (i) PhTM*, the first phased-based TM designed; (ii) Commit Throughput Measurement (CTM), a general-purpose runtime; and (iii) GoTM, a Graph-oriented runtime. We conduct a performance analysis to identify the drawbacks and benefits of each PhTM implementation with respect to their associated parameters. Results with speedups of up to 10× over the sequential baseline for CTM show that this mechanism generally shows better performance for a diverse set of applications.},
  archive      = {J_JPDC},
  author       = {Catalina Munoz Morales and Bruno Honorio and Joao P.L. de Carvalho and Alexandro Baldassin and Guido Araujo},
  doi          = {10.1016/j.jpdc.2022.11.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {126-139},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the impact of mode transition on phased transactional memory performance},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel algorithm for approximating the silhouette using
a ball tree. <em>JPDC</em>, <em>173</em>, 115–125. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is widely used in many scientific fields. The contribution of enumerating the value of the silhouette is twofold: firstly it can help choosing a suitable cluster count and secondly it can be used to evaluate the quality of a clustering. Enumerating the silhouette exactly is an extremely time-consuming task, especially in big data applications ; it is therefore common to approximate its value. This article presents an efficient shared-memory parallel algorithm for approximating the silhouette, which uses a ball tree. The process of initialising the ball tree and enumerating the silhouette are fully parallelised using the OpenMP API. The results of our experiments show that the proposed parallel algorithm substantially increases the speed of computing the silhouette whilst retaining necessary precision for real-world applications.},
  archive      = {J_JPDC},
  author       = {Ivan Šimeček and Claudio Kozický},
  doi          = {10.1016/j.jpdc.2022.11.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {115-125},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel algorithm for approximating the silhouette using a ball tree},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault tolerance analysis for hamming graphs with large-scale
faulty links based on k-component edge-connectivity. <em>JPDC</em>,
<em>173</em>, 107–114. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The L -ary n -dimensional hamming graph K L n KLn is one of the most attractive interconnection networks for parallel processing and computing systems. Analysis of the link fault tolerance of topology structure can provide the theoretical basis for the design and optimization of the interconnection networks . The k -component edge-connectivity c λ k ( G ) cλk(G) of an interconnection network G , is the minimum number of set of faulty links, such that these malfunctions disconnect the network G with at least k connected subnetworks . It gives a more precise quantitative analysis of indicators of the robustness of a parallel distributed system in the event of faulty links. Let t = ⌊ n 2 ⌋ t=⌊n2⌋ if L is even, and t = ⌈ n 2 ⌉ t=⌈n2⌉ if L is odd. In this paper, we prove that the ( k + 1 ) (k+1) -component edge-connectivity of L -ary n -dimensional hamming graphs is c λ k + 1 ( K L n ) = ( L − 1 ) n k − e x k ( K L n ) 2 cλk+1(KLn)=(L−1)nk−exk(KLn)2 for k ≤ L t k≤Lt , n ≥ 7 n≥7 , where e x k ( K L n ) exk(KLn) represents the maximum degree sum of the subgraph induced by all k processors of K L n KLn . As the exact values of ( k + 1 ) (k+1) -component edge-connectivity of K L n KLn oscillate greatly, an efficient O ( log ⁡ N ) O(log⁡N) algorithm is designed to determine the exact values of c λ k + 1 ( K L n ) cλk+1(KLn) for each k ≤ L t k≤Lt and N = L n N=Ln . Our result improves those of Xu et al. [33] and Liu et al. [27] .},
  archive      = {J_JPDC},
  author       = {Yayu Yang and Mingzu Zhang and Jixiang Meng},
  doi          = {10.1016/j.jpdc.2022.11.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {107-114},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fault tolerance analysis for hamming graphs with large-scale faulty links based on k-component edge-connectivity},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speculative inter-thread store-to-load forwarding in SMT
architectures. <em>JPDC</em>, <em>173</em>, 94–106. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications running on out-of-order cores have benefited for decades of store-to-load forwarding which accelerates communication of store values to loads of the same thread. Despite threads running on a simultaneous multithreading (SMT) core could also access the load queues (LQ) and store queues (SQ) / store buffers (SB) of other threads to allow inter-thread store-to-load forwarding, we have skipped exploiting it because if we allow communication of different SMT threads via their LQs and SQs/SBs, write atomicity may be violated with respect to the outside world beyond the acceptable model of read-own-write-early multiple-copy atomicity (rMCA). In our prior work, we leveraged this idea to propose inter-thread store-to-load forwarding (ITSLF). ITLSF accelerates synchronization and communication of threads running in a simultaneous multi-threading processor by allowing stores in the store-queue of a thread to forward data to loads of another thread running in the same core without violating rMCA. In this work, we extend the original ITSLF mechanism to allow inter-thread forwarding from speculative stores (Spec-ITSLF). Spec-ITSLF allows forwarding store values to other threads earlier, which further accelerates synchronization. Spec-ITSLF outperforms a baseline SMT core by 15\%, which is 2\% better on average (and up to 5\% for the TATP workload) than the original ITSLF mechanism. More importantly, Spec-ITSLF is on par with the original ITSLF mechanism regarding storage overhead but does not need to keep track of the speculative state of stores, which was an important source of overhead and complexity in the original mechanism.},
  archive      = {J_JPDC},
  author       = {Josué Feliu and Alberto Ros and Manuel E. Acacio and Stefanos Kaxiras},
  doi          = {10.1016/j.jpdc.2022.11.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {94-106},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Speculative inter-thread store-to-load forwarding in SMT architectures},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JMDC: A joint model and data compression system for deep
neural networks collaborative computing in edge-cloud networks.
<em>JPDC</em>, <em>173</em>, 83–93. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have shown exceptional promise in providing Artificial Intelligence (AI) to many computer vision applications . Nevertheless, complex models, intensive computations, and resource constraints hinder the use of DNNs in edge computing scenarios. Existing studies have already focused on edge-cloud collaborative inference, where a DNN model is decoupled at an intermediate layer, and the two parts are sequentially executed at the edge device and the cloud server, respectively. In this work, we examine the status quo approaches of DNN execution, and find that it still takes a lot of time on edge device computation and edge-cloud data transmission. Using this insight, we propose a new edge-cloud DNN collaborative computing framework, JMDC, based on Joint Model and Data Compression . In JMDC, we adopt the attention mechanism to select important model channels for efficient inference computation , and important regions of the intermediate output for transferring to the cloud. We further use the quantization technique to reduce actual bits needed to be transferred. Depending on the specific application requirements on latency or accuracy, JMDC can adaptively determine the optimal partition point and compression strategy under different resource conditions. By extensive experiments based on the Raspberry Pi 4B device and the CIFAR10 dataset, we demonstrate the effectiveness of JMDC in enabling on-demand low-latency DNN inference, and its superiority over other baseline schemes.},
  archive      = {J_JPDC},
  author       = {Yi Ding and Weiwei Fang and Mengran Liu and Meng Wang and Yusong Cheng and Naixue Xiong},
  doi          = {10.1016/j.jpdc.2022.11.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-93},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {JMDC: A joint model and data compression system for deep neural networks collaborative computing in edge-cloud networks},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PTTS: Power-aware tensor cores using two-sided sparsity.
<em>JPDC</em>, <em>173</em>, 70–82. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural networks (DNNs) have become the compelling solution for a broad range of applications such as automatic translation, advertisement recommendation, and speech recognition. Matrix multiplication is the fundamental operation used in various classes of DNNs. Introduction of tensor cores (TCs) in NVIDIA GPGPUs particularly aims at acceleration of neural networks . A TC is a specialized unit dedicated to compute matrix-multiply-and-accumulate (MMA) operations. State-of-the-art DNNs are known to be power-hungry and compute-intensive due to increasing depth of networks, i.e. multiple layers with massive number of neurons. This causes excessive energy in TCs. Sparse neural networks have emerged as an effective solution to address massive amount of computations in DNNs. While sparsity is widely used for acceleration of DNNs, only a handful of studies focused on energy aspect of sparse DNNs and energy-efficient architectures for TCs are scarce. We exploit sparsity in DNNs and propose power gating multipliers with sparse activations or weights. We show that sparsity occurs in TCs for short intervals and conventional power gating techniques cannot fully exploit these idle intervals mainly due to overhead of power gating. To mitigate the overhead of power gating, we propose power-aware tensor cores using two-sided sparsity (PTTS) which monitors inputs of multipliers and turns them off only if inputs remain sparse for long intervals. We also propose an architectural technique that shuffles multiplications to pack idle intervals and increase opportunities for power gating. We introduce a low-cost and implementation-efficient sparse input operand interconnect to change order of effectual multiplications. Our proposed techniques combined are able to save energy by 68\% in Tensor Cores with negligible impact on performance while maintaining accuracy.},
  archive      = {J_JPDC},
  author       = {Ehsan Atoofian},
  doi          = {10.1016/j.jpdc.2022.11.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PTTS: Power-aware tensor cores using two-sided sparsity},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel computing in finance for estimating risk-neutral
densities through option prices. <em>JPDC</em>, <em>173</em>, 61–69. (<a
href="https://doi.org/10.1016/j.jpdc.2022.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Option pricing is one of the most active Financial Economics research fields. Black-Scholes-Merton option pricing theory states that risk-neutral density is lognormal . However, markets&#39; pieces of evidence do not support that assumption. More realistic assumptions impose substantial computational burdens to calculate option pricing functions. Risk-neutral density is a pivotal element to price derivative assets, which can be estimated through nonparametric kernel methods . A significant computational challenge exists for determining optimal kernel bandwidths , addressed in this study through a parallel computing algorithm performed using Graphical Processing Units. The paper proposes a tailor-made Cross-Validation criterion function used to define optimal bandwidths. The selection of optimal bandwidths is crucial for nonparametric estimation and is also the most computationally intensive. We tested the developed algorithms through two data sets related to intraday data for VIX and S&amp;P500 indexes.},
  archive      = {J_JPDC},
  author       = {Ana M. Monteiro and António A.F. Santos},
  doi          = {10.1016/j.jpdc.2022.11.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {61-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel computing in finance for estimating risk-neutral densities through option prices},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general approach for supporting nonblocking data
structures on distributed-memory systems. <em>JPDC</em>, <em>173</em>,
48–60. (<a href="https://doi.org/10.1016/j.jpdc.2022.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonblocking data structures are an essential part of many parallel applications in that they can help to improve fault tolerance and performance. Although there are scores of nonblocking data structures , such as stacks, queues, double-ended queues (deques), lists, widely used in practice, most of them are designed to be used on shared-memory machines only, and cannot be used in a distributed-memory setting. Several recent studies focus on the development of novel tailor-made nonblocking distributed data structures and omit the potential for adapting a great wealth of existing nonblocking shared-memory ones for the distributed-memory case. Hence, we propose a general approach for bridging the gap between most existing nonblocking data structures and distributed-memory machines in this work. Several challenges, such as safe memory reclamation and solving the ABA problem, must be overcome. To address these issues, we present a global memory management scheme. The scheme takes advantage of hazard pointers which are widely used to tackle the problems in shared-memory environments. To demonstrate our general approach, we take stacks as a typical example of nonblocking data structures. This work also provides a survey of well-known nonblocking stack algorithms along with our analysis and evaluation in distributed-memory environments. Moreover, this paper depicts how to improve performance of a stack algorithm by making use of node locality. Besides, a cost model based on worst cases is devised to help gain a better understanding into experimental results of nonblocking distributed data structures , along with our analysis of the influence of two popular lock-free programming patterns on performance.},
  archive      = {J_JPDC},
  author       = {Thanh-Dang Diep and Phuong Hoai Ha and Karl Fürlinger},
  doi          = {10.1016/j.jpdc.2022.11.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {48-60},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A general approach for supporting nonblocking data structures on distributed-memory systems},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of the analytical performance models for GPUs and
extracting the underlying pipeline model. <em>JPDC</em>, <em>173</em>,
32–47. (<a href="https://doi.org/10.1016/j.jpdc.2022.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an in-depth study of the analytical models for the performance estimation of GPUs . We show that the models&#39; analytical equations can be derived from a pipeline analogy that models each GPU subsystem as an abstract pipeline. We call this the Pipeline model. All the equations are reformulated based on generic pipeline characteristics, namely throughput and latency. Our analysis shows equivalences between models and reveals substantial problems with some of the equations. Rather than relying on equations, the Pipeline model is then used to simulate the behavior of kernel executions based on the same hardware parameters as the analytical models. The simplicity of the model and relying on simulation mean that this approach needs less assumptions, is more comprehensive and is more flexible. More performance aspects can be taken into consideration. The different models are compared and evaluated empirically with 14 kernels of the Rodinia benchmark suite with varying occupancy. The Pipeline model gives an average MAPE of 24, while the average MAPE values of the other models lie between 27 and 136.},
  archive      = {J_JPDC},
  author       = {Jan Lemeire and Jan G. Cornelis and Elias Konstantinidis},
  doi          = {10.1016/j.jpdc.2022.11.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-47},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Analysis of the analytical performance models for GPUs and extracting the underlying pipeline model},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling zero knowledge proof by accelerating zk-SNARK
kernels on GPU. <em>JPDC</em>, <em>173</em>, 20–31. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a recent cryptography protocol, Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) allows one party to prove that it possesses certain information without revealing information to these untrusted proof provers. This mechanism has the ability to provide the function for constructing and verifying information integrity without privacy leaking. However, the computation kernels of zk-SNARK consume too much computing power and produce a significant performance bottleneck with the growing data volume and security requirement. In this paper, we take advantage of Graphic Processing Unit (GPU) to enhance zk-SNARK efficiency by accelerating the most time-consuming computation kernels: modular multiplication and Number-Theoretic Transform (NTT)/Inverse Number-Theoretic Transform (INTT) in Elliptic Curve Cryptography (ECC) pairing with two major improvements: (1) Adopting interval limbs multiply-add quaternary operation to directly accelerate ECC pairing by making full advantage of information entropy within the limited hardware bit width; (2) Data layout and shuffle methods in GPU global memory and shared memory for data space consistency maintenance accelerating NTT/INTT which indirectly works on ECC pairing. To the best of our knowledge, our work would be the first exploration to accelerate these improvements on GPU. The measured results show that our methods are able to accelerate modular multiplication and NTT/INNT by 1.22×  and 4.67×  times respectively compared with the previous GPU implementation. With these accelerated kernels, we are able to achieve 3.14×  speedup for Groth16, which is the most efficient zk-SNARK implementation working on BLS12-381 ECC field. With the bottleneck tackled, our work will expand the deployment scenarios of zk-SNARK in Zero Knowledge Proof (ZKP).},
  archive      = {J_JPDC},
  author       = {Ning Ni and Yongxin Zhu},
  doi          = {10.1016/j.jpdc.2022.10.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {20-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Enabling zero knowledge proof by accelerating zk-SNARK kernels on GPU},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of DPDK-based applications through
tracing. <em>JPDC</em>, <em>173</em>, 1–19. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Data Plane Development Kit (DPDK) is an efficient framework developed based on the kernel bypass approach to accelerate packet processing in userspace. This framework includes many libraries and Poll Mode Drivers (PMDs) that are optimized for maximum network performance. Several studies have demonstrated that porting existing data plane applications to DPDK can considerably increase their performance. Unfortunately, kernel bypassing makes traditional network monitoring and debugging tools obsolete. Therefore, considering the complexity of networking software, detecting unexpected behaviors, and diagnosing its root causes became a tedious task. The literature reports a few tools intended for debugging DPDK application bugs, but they are mostly ineffective against performance bugs. In this paper, we propose a tracing-based performance analysis framework that is dedicated to DPDK applications. The framework provides many analyses enabling the practitioner to gain insights into the application internals and diagnose performance bugs. We use an efficient approach to collect tracing data from DPDK libraries and drivers, with a very low overhead (&lt;0.2\%). Our framework correlates the data collected to build a data model that illustrates the states of the monitored application&#39; components. We leverage this data model to calculate adapted performance metrics and display them in time-synchronized graphical views . The numerous use cases that we present demonstrate the ability of our tool to identify and diagnose efficiently various performance bugs.},
  archive      = {J_JPDC},
  author       = {Adel Belkhiri and Martin Pepin and Mike Bly and Michel Dagenais},
  doi          = {10.1016/j.jpdc.2022.10.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-19},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance analysis of DPDK-based applications through tracing},
  volume       = {173},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A low-power WNoC transceiver with a novel energy
consumption management scheme for dependable IoT systems. <em>JPDC</em>,
<em>172</em>, 144–158. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless network-on-chip architectures (WNoCs), by combining wired and wireless modules and links, provide fast and efficient communication infrastructures for complex on-chip systems such as various IoT and intelligent systems. The most challenging step of designing high-performance and energy-efficient WNoCs is the structure of transceivers . In fact, transceivers and wired routers are the most power-hungry elements in WNoCs; therefore, any cut-down of power consumption in transceivers and routers significantly reduce the power consumption of WNoCs. In this paper, a low-power transceiver for WNoC is designed and then, a high-performance and configurable energy consumption management scheme for the transceiver is proposed. In the designed transceiver, various strategies are employed at both circuit-level and also at architectural-level for reducing static and dynamic power consumption. The proposed energy management scheme controls the activity and tunes the voltage level of the power supply of each module, in a distributed and dynamic manner. The evaluation results demonstrate that a WNoC utilizing the proposed scheme provides lower energy consumption , less latency, and higher reliability and higher network throughput compared to both of relevant single-chip and multi-chip WNoC architectures, at the cost of negligible impact on silicon area.},
  archive      = {J_JPDC},
  author       = {Fahimeh Yazdanpanah},
  doi          = {10.1016/j.jpdc.2022.10.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {144-158},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A low-power WNoC transceiver with a novel energy consumption management scheme for dependable IoT systems},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel branch-and-bound algorithm with history-based
domination and its application to the sequential ordering problem.
<em>JPDC</em>, <em>172</em>, 131–143. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe the first parallel Branch-and-Bound (B&amp;B) algorithm with a history-based domination technique. Although history-based domination substantially speeds up a B&amp;B search, it makes parallelization much more challenging. Our algorithm is the first parallel exact algorithm for the Sequential Ordering Problem using a pure B&amp;B approach. To effectively explore the solution space, we have developed three novel parallelization techniques: thread restart, parallel history domination, and history-table memory management . The proposed algorithm was experimentally evaluated using the SOPLIB and TSPLIB benchmarks on multi-core processors. Using 32 threads with a time limit of one hour, the algorithm gives geometric-mean speedups of 72x and 20x on the medium-difficulty SOPLIB and TSPLIB instances, respectively. On the hard instances, it solves 12 instances that the sequential algorithm does not solve, with geometric-mean speedups of 16x on SOPLIB and 32x on TSPLIB. Super-linear speedups up to 366x are seen on 16 instances.},
  archive      = {J_JPDC},
  author       = {Taspon Gonggiatgul and Ghassan Shobaki and Pınar Muyan-Özçelik},
  doi          = {10.1016/j.jpdc.2022.10.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {131-143},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel branch-and-bound algorithm with history-based domination and its application to the sequential ordering problem},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A two-level network-on-chip architecture with multicast
support. <em>JPDC</em>, <em>172</em>, 114–130. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is essential for implementing processing systems of edge computing , internet of things (IoT) and wireless multimedia sensor networks (WMSN) to use low-power parallel and distributed architectures with high speed and low power consumption . Most of the artificial intelligence and machine learning applications need to be executed on efficient distributed architectures with multicast support . In this paper, TLAM, a high-performance and low-cost NoC architecture is proposed. TLAM stands for Two-Level network-on-chip Architecture with Multicast support and includes a hybrid path/tree based multicast routing algorithm and a specific two-level mesh topology. The routing algorithm of TLAM is basically working according to the Hamiltonian paths. In the proposed architecture, the topology is partitioned and the two-level links provide an efficient infrastructure for low-cost and low-latency transmission of multicast and broadcast messages between partitions. In TLAM, in addition to multicasting routing algorithm, hardware components for handling multicasting packets are designed in order to achieve performance improvements. The goal is to improve the efficiency and performance, especially latency, while handling both unicast and multicast packets . Experimental evaluations including network-level and router-level analysis with different configurations under various traffic patterns were performed. The evaluations in terms of latency, throughput, area and power consumption , indicate that TLAM provides higher performance, especially for dense multicasting and broadcasting, in comparison with the existing architectures.},
  archive      = {J_JPDC},
  author       = {Fahimeh Yazdanpanah},
  doi          = {10.1016/j.jpdc.2022.10.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {114-130},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A two-level network-on-chip architecture with multicast support},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GVMP: A multi-objective joint VM and vGPU placement
heuristic for API remoting-based GPU virtualization and disaggregation
in cloud data centers. <em>JPDC</em>, <em>172</em>, 97–113. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diverse needs of customers drive cloud providers to incorporate more GPU-enabled services. It is known that users barely utilize GPUs. Hence, GPU virtualization techniques are employed to enable GPU sharing and increase resource utilization. Among GPU virtualization techniques, API remoting leverages disaggregation for GPU provisioning, allowing GPUs to be accessed remotely by non-resident VMs. It enables a VM to access any available GPU in the data center which, in turn, provides more flexibility to reach a better placement. It also enables users to choose VM and vGPU instances separately based on computational needs as GPU-enabled VM instances with fixed configurations may not proportionally utilize allocated resources. However, an inefficient VM or vGPU placement may result in poor performance and increase in energy consumption and rejection ratio. There is also a network communication overhead for remotely allocated vGPUs. The main challenge is how to place VMs and vGPUs such that rejection ratio is decreased and the negative performance impacts caused by GPU sharing and remote access are kept at a minimum. In this work, we formally define the joint problem of VM and vGPU placement based on API remoting as a multi-objective ILP model and introduce gVMP as a heuristic to solve it. The proposed method is compared against state-of-the-art heuristics and commercial solutions that are based on API remoting and GRID technologies for the placement of GPU-enabled VMs. We study the effectiveness of the proposed gVMP algorithm for low and high arrival rates in slow and fast networks. Our results show that under different scenarios and in comparison to state-of-the-art policies, our heuristic improves average request duration, efficiency, and SLA by up to 40\%, 25\% and 29\%, respectively.},
  archive      = {J_JPDC},
  author       = {Ahmad Siavashi and Mahmoud Momtazpour},
  doi          = {10.1016/j.jpdc.2022.10.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {97-113},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GVMP: A multi-objective joint VM and vGPU placement heuristic for API remoting-based GPU virtualization and disaggregation in cloud data centers},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent energy-efficient scheduling with ant colony
techniques for heterogeneous edge computing. <em>JPDC</em>,
<em>172</em>, 84–96. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficiency is a significant issue in heterogeneous edge computing systems for a large number of latency-sensitive applications. This article presents an efficient technique to minimize energy overhead of time-constrained applications modeled by DAGs in heterogeneous edge computing . The technique is divided into three stages. First, we design a new method to compute task priority and propose the ant-colony based energy-aware scheduling algorithm to get a preliminary scheduling result. Second, taking the slack time between tasks and their deadlines into consideration, we propose the downward proportionally reclaiming slack algorithm to further cut down energy overhead by the DVFS technique. Third, taking the slack time between tasks into consideration, we propose the upward and downward proportionally reclaiming slack algorithm to cut down energy overhead by the DVFS technique again. Simulated results indicate that the presented technique is highly efficient in reducing energy overhead compared with state-of-the-art techniques using benchmarks of distinct characteristics.},
  archive      = {J_JPDC},
  author       = {Jing Liu and Pei Yang and Cen Chen},
  doi          = {10.1016/j.jpdc.2022.10.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {84-96},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intelligent energy-efficient scheduling with ant colony techniques for heterogeneous edge computing},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A blockchain-orchestrated deep learning approach for secure
data transmission in IoT-enabled healthcare system. <em>JPDC</em>,
<em>172</em>, 69–83. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of the Internet of Things (IoT) with traditional healthcare systems has improved quality of healthcare services . However, the wearable devices and sensors used in Healthcare System (HS) continuously monitor and transmit data to the nearby devices or servers using an unsecured open channel. This connectivity between IoT devices and servers improves operational efficiency, but it also gives a lot of room for attackers to launch various cyber-attacks that can put patients under critical surveillance in jeopardy. In this article, a Blockchain-orchestrated Deep learning approach for Secure Data Transmission in IoT-enabled healthcare system hereafter referred to as “BDSDT” is designed. Specifically, first a novel scalable blockchain architecture is proposed to ensure data integrity and secure data transmission by leveraging Zero Knowledge Proof (ZKP) mechanism. Then, BDSDT integrates with the off-chain storage InterPlanetary File System (IPFS) to address difficulties with data storage costs and with an Ethereum smart contract to address data security issues. The authenticated data is further used to design a deep learning architecture to detect intrusion in HS network. The latter combines Deep Sparse AutoEncoder (DSAE) with Bidirectional Long Short-Term Memory (BiLSTM) to design an effective intrusion detection system . Experiments on two public data sources (CICIDS-2017 and ToN-IoT) reveal that the proposed BDSDT outperformed state-of-the-arts in both non-blockchain and blockchain settings and have obtained accuracy close to 99\% using both datasets.},
  archive      = {J_JPDC},
  author       = {Prabhat Kumar and Randhir Kumar and Govind P. Gupta and Rakesh Tripathi and Alireza Jolfaei and A.K.M. Najmul Islam},
  doi          = {10.1016/j.jpdc.2022.10.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {69-83},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A blockchain-orchestrated deep learning approach for secure data transmission in IoT-enabled healthcare system},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LOSC: A locality-optimized subgraph construction scheme for
out-of-core graph processing. <em>JPDC</em>, <em>172</em>, 51–68. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data applications increasingly rely on the analysis of large graphs. In recent years, a number of out-of-core graph processing systems have been proposed to process graphs with billions of edges on just one commodity computer, by efficiently using the secondary storage (e.g., hard disk, SSD). Unfortunately, these graph processing systems continue to suffer from poor performance, despite of many solutions proposed to address the disk I/O bottleneck problem, a commonly recognized root cause. However, our experimental results show that another root cause of the poor performance is the subgraph construction phase of graph processing, which induces a large number of random memory accesses that substantially weaken cache access locality and thus greatly degrade the performance. In this paper, we propose an efficient out-of-core graph processing system, LOSC, to substantially reduce the overheads of subgraph construction. LOSC proposes a locality-optimized subgraph construction scheme that significantly improves the in-memory data access locality of the subgraph construction phase. Furthermore, LOSC adopts a compact edge storage format and a lightweight replication of vertices to reduce I/O traffic and improve computation efficiency. Extensive evaluation results show that LOSC is respectively 9.4× and 5.1× faster than GraphChi and GridGraph, two representative out-of-core systems. In addition, LOSC outperforms other state-of-art out-of-core graph processing systems including FlashGraph, GraphZ, G-Store and NXGraph. For example, LOSC can be up to 6.9× faster than FlashGraph.},
  archive      = {J_JPDC},
  author       = {Xianghao Xu and Fang Wang and Hong Jiang and Yongli Cheng and Yu Hua and Dan Feng and Yongxuan Zhang},
  doi          = {10.1016/j.jpdc.2022.10.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {51-68},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {LOSC: A locality-optimized subgraph construction scheme for out-of-core graph processing},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Malware detection using image representation of malware data
and transfer learning. <em>JPDC</em>, <em>172</em>, 32–50. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increased proliferation of internet-enabled mobile devices and large internet use, cybercrime incidents have grown exponentially, often leading to huge financial losses. Most cybercrimes are launched through malware attacks, phishing attacks , denial/distributed denial of attacks, looting people&#39;s money, stealing credential information for unauthorized use, personal identity thefts, etc. Timely detection of malware can avoid such damage. However, it requires an efficient and effective approach to detecting such attacks. This study attempts to devise a malware detection approach using transfer learning and machine learning algorithms . A hybrid approach has been adopted where pre-trained models VVG-16 and ResNet-50 extract hybrid feature sets from the data to be used with the machine learning algorithms . In doing so, this study contrives the Bi-model architecture where the same models are combined sequentially in the stacked form to obtain higher performance as the output of the first model is used to train the second model. With the Bi-model structure, 100\% accuracy is obtained for a 25 classes problem. Performance comparison with state-of-the-art models and T-test proves the superior performance of the proposed approach.},
  archive      = {J_JPDC},
  author       = {Furqan Rustam and Imran Ashraf and Anca Delia Jurcut and Ali Kashif Bashir and Yousaf Bin Zikria},
  doi          = {10.1016/j.jpdc.2022.10.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-50},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Malware detection using image representation of malware data and transfer learning},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-cooperative game algorithms for computation offloading
in mobile edge computing environments. <em>JPDC</em>, <em>172</em>,
18–31. (<a href="https://doi.org/10.1016/j.jpdc.2022.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) has become a promising technology for 5G networks . Computation offloading is an essential issue of MEC, which enables mobile User Equipment (UE) to enjoy rich wireless resources and huge computing power anywhere. This paper considers the Quality-of-Experience (QoE) of UEs in 5G MEC systems and presents a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm for computation offloading of MEC applications. We establish an MEC computation offloading model by considering the QoE requirements of UEs, and discuss the communication overheads , computation cost, and energy consumption models to minimize the energy consumption and time delay of each UE. Considering that there are multiple UEs who want to offload their computation tasks to a resource-constrained MEC server, and each UE is selfish and competitive, we formulate the problem of computation offloading decision as a non-cooperative game model. We prove the existence of a Nash Equilibrium (NE) solution for the proposed game model. In addition, we propose an algorithm that jointly optimizes energy consumption and time delay under QoE preferences to achieve optimal offloading benefits for each UE. Moreover, we respectively propose a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm to efficiently find the NE solution. Extensive simulation experiments are conducted to verify the effectiveness of the proposed MEC computation offloading model and the QCOG-DG and QCOG-SG algorithms. Simulation results show that the proposed QCOG-DG algorithm can efficiently find the NE solutions in the MEC scenarios with UEs of different sizes.},
  archive      = {J_JPDC},
  author       = {Jianguo Chen and Qingying Deng and Xulei Yang},
  doi          = {10.1016/j.jpdc.2022.10.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {18-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Non-cooperative game algorithms for computation offloading in mobile edge computing environments},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agent coalitions for load balancing in cloud data centers.
<em>JPDC</em>, <em>172</em>, 1–17. (<a
href="https://doi.org/10.1016/j.jpdc.2022.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The workload of Cloud data centers is constantly fluctuating causing imbalances across physical hosts that may lead to violations of service-level agreements. To mitigate workload imbalances, this work proposes a concurrent agent-based problem-solving technique supported by cooperative game theory capable of balancing workloads by means of live migration of virtual machines (VMs). Nearby agents managing physical hosts are partitioned into coalitions in which agents play coalitional games to progressively balance separate sections of a data center while considering the coalition&#39;s benefit of migrating a VM as well as its associated network overhead. Simulation results show that, in general, the proposed coalition-based load balancing mechanism outperformed a load balancing mechanism based on a hill-climbing algorithm used by a top data center vendor when considering altogether (i) the standard deviation of resource usage, (ii) the number of migrations, and (iii) the number of switch hops per migration.},
  archive      = {J_JPDC},
  author       = {J. Octavio Gutierrez-Garcia and Joel Antonio Trejo-Sánchez and Daniel Fajardo-Delgado},
  doi          = {10.1016/j.jpdc.2022.10.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-17},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Agent coalitions for load balancing in cloud data centers},
  volume       = {172},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on distributed intelligence at the edge for
the future internet of things. <em>JPDC</em>, <em>171</em>, 157–162. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Andrzej Goscinski ( Guest Editor ) and Flavia C. Delicato (Guest Editor) and Giancarlo Fortino (Guest Editor) and Anna Kobusińska (Guest Editor) and Gautam Srivastava (Guest Editor)},
  doi          = {10.1016/j.jpdc.2022.09.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {157-162},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Special issue on distributed intelligence at the edge for the future internet of things},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed three-way formal concept analysis for large
formal contexts. <em>JPDC</em>, <em>171</em>, 141–156. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way concept analysis (3WCA) is a framework based on Formal concept analysis and three-way decisions is used in the field of knowledge discovery to solve uncertainties in many domains like machine learning , data mining and software engineering . The 3WCA requires both the formal context and its complement context for generating concepts and constructing the concept lattice . In three-way concept analysis, data are analyzed using two types of concept lattices constructed using classical formal concept analysis (FCA) algorithms: object-induced (OE) concept lattices and attribute-induced (AE) concept lattices. The existing formal concept analysis algorithms focus on the sequential generation of OE and AE concepts rather than finding them in parallel and cannot process large datasets efficiently. The main contribution of this paper is that we propose a novel parallel algorithm for concept generation and construction of the three way concept lattice for knowledge discovery and representation in large datasets. Aiming to construct an efficient algorithm for 3WCA, this paper primarily discusses the existing algorithms for concept generation. Further we develop an efficient algorithm for OE and AE concept generation and lattice construction. Extensive experiments are conducted on various datasets to evaluate the efficiency of the proposed algorithm. Both the experimental and statistical results demonstrate the efficacy of the algorithm on larger datasets. Also the proposed algorithm can significantly decrease the time required for OE/AE concept generation and lattice construction compared to the existing classical FCA algorithms.},
  archive      = {J_JPDC},
  author       = {Raghavendra Kumar Chunduri and Aswani Kumar Cherukuri},
  doi          = {10.1016/j.jpdc.2022.09.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {141-156},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed three-way formal concept analysis for large formal contexts},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secured distributed algorithms without hardness assumptions.
<em>JPDC</em>, <em>171</em>, 130–140. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study algorithms in the LOCAL LOCAL model that produce secured output. Specifically, each vertex computes its part in the output, the entire output is correct, but each vertex cannot discover the output of other vertices, with a certain probability. As the extensive research in the distributed algorithms field yielded efficient decentralized algorithms, the discussion about the security of distributed algorithms was somewhat neglected. Nevertheless, many protocols and algorithms were devised in the research area of secure multi-party computation problem. However, the focus in those protocols was to work for every function f at the expense of increasing the round complexity, or the necessity of several computational assumptions. We present a novel approach, which identifies and develops those algorithms that are inherently secure (which means they do not require any further constructions). This approach yields efficient secure algorithms for various labeling and decomposition problems without requiring any hardness assumption, but only a private randomness generator in each vertex.},
  archive      = {J_JPDC},
  author       = {Leonid Barenboim and Harel Levin},
  doi          = {10.1016/j.jpdc.2022.09.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {130-140},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Secured distributed algorithms without hardness assumptions},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-saving optimization of application server clusters
based on mixed integer linear programming. <em>JPDC</em>, <em>171</em>,
111–129. (<a href="https://doi.org/10.1016/j.jpdc.2022.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of how to dynamically optimize the deployment of an application server cluster according to the changing load to reduce energy consumption is an important problem that must be urgently solved. In this paper, we propose an energy-saving optimization strategy for application server clusters, whose optimization content includes the on/off state, CPU frequency, and load size of each server. Compared with existing research, our strategy is not only more accurate in power and load models but also considers the switching cost of servers to avoid server switching jitter. The strategy includes two schemes, which both formulate the cluster energy-saving optimization as a mixed integer linear programming (MILP) problem and then adopt a toolkit to solve the problem. One scheme defines variables for each server, and the resulting programming problem is called the MILP4PH problem. The other scheme defines variables for each server type, resulting in a programming problem called the MILP4GH problem. The experimental results reveal that for clusters with poor homogeneity, the MILP4PH problem has fewer variables and can be solved in real time, while for clusters with good homogeneity, the MILP4GH problem has fewer variables and can be solved in real time.},
  archive      = {J_JPDC},
  author       = {Zhi Xiong and Min Zhao and Ziyue Yuan and Jianlong Xu and Lingru Cai},
  doi          = {10.1016/j.jpdc.2022.09.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {111-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-saving optimization of application server clusters based on mixed integer linear programming},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local node feature modeling for edge computing based on
network embedding in dynamic networks. <em>JPDC</em>, <em>171</em>,
98–110. (<a href="https://doi.org/10.1016/j.jpdc.2022.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a dynamic network, the characteristics of local nodes include first and higher-order proximity among the nodes as well as different attributes attached to each node. This complexity impose significant challenge for dynamic network modeling . As a result, few dynamic network studies have considered high-order proximity among local nodes. In this paper, we adopt the network embedding method to map high-order proximity of local nodes into low-dimensional, dense and real-valued vectors. Morevoer, we incorporate it into a model-based evolutionary clustering method through regularity conditions . Such a unified framework can increase the effectiveness and robustness of dynamic community detection while pertaining a good explanatory and visualization ability. Experiments based on synthetic and real world data sets show that our model can produce better community detection results than other popular models such as DECS and Genlouvain in dense networks. This result is consistent with the advantage of network embedding method in dense networks.},
  archive      = {J_JPDC},
  author       = {Xiaoming Li and Yi Luo and Neal Xiong and Wei Yu and Guangquan Xu and Changzheng Liu and Xiaoping Yang},
  doi          = {10.1016/j.jpdc.2022.09.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {98-110},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Local node feature modeling for edge computing based on network embedding in dynamic networks},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A job scheduling algorithm based on parallel workload
prediction on computational grid. <em>JPDC</em>, <em>171</em>, 88–97.
(<a href="https://doi.org/10.1016/j.jpdc.2022.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, the computational grid consists of a large number of computing nodes, some of them are idle due to the uneven geographical distribution of computing requirements. This may cause workload unbalancing problems, which affect the performance of large-scale computational grids. In order to balance the computing requirements and computing nodes, we propose a job scheduling algorithm based on the workload prediction of computing nodes. We first analyze the causes of workload imbalance and the feasibility of reallocating computing resources. Secondly, we design an application and workload-aware scheduling algorithm (AWAS) by combining the previously designed workload prediction model. To reduce the complexity of the AWAS algorithm, we propose a parallel job scheduling method based on computing node workload prediction. The experiments show that the AWAS algorithm can balance the workload among different computing nodes on the real-world dataset. In addition, we propose the parallelism of workload prediction model from the perspective of internal structure and data set to make AWAS apply to more computing nodes of the large-scale computing grids . Experimental results show that the combination of the two can achieve satisfactory acceleration efficiency.},
  archive      = {J_JPDC},
  author       = {Xiaoyong Tang and Yi Liu and Tan Deng and Zexin Zeng and Haowei Huang and Qiyu Wei and Xiaorong Li and Li Yang},
  doi          = {10.1016/j.jpdc.2022.09.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {88-97},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A job scheduling algorithm based on parallel workload prediction on computational grid},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-demand inference acceleration for directed acyclic graph
neural networks over edge-cloud collaboration. <em>JPDC</em>,
<em>171</em>, 79–87. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the development of deep neural network (DNN) tasks that feature a directed acyclic graph (DAG) surges in modern industries, simultaneously meeting the latency and accuracy requirements of these key DNN tasks has remained elusive. The general feasible solution is to find an optimal DNN partition that combines cloud with edge computing to implement edge–cloud collaboration for DNN inference. Nevertheless, the dynamicity of network conditions and the uncertain availability of cloud computing resources pose formidable obstacles to this collaborative inference. In this study, we formulate the optimization of DNN partition as a minimum cut problem in DAG derived from DNN and then propose an early-exit DAG-DNN inference (EDDI) framework that supports synergistically on-demand inference acceleration for DAG DNN. This framework introduces two novel components: (1) Evaluator that derives an approximately optimal solution for constructing DNN into appropriate DAG, assisting in solving the optimization problem; and (2) Optimizer that enables collaborative optimization of the early exit and DNN partition strategy at run time to improve performance while meeting user-defined latency requirements. Quantitative evaluations show that EDDI outperforms state-of-the-art schemes by 10.6\%, 3.3\%, and 3\%, on average, in terms of model accuracy, inference latency, and throughput under diverse latency constraints, respectively. Meanwhile, latency speedup ratio increases by an average of 8\% and 4\% under varying network conditions and cloud server capacities, respectively.},
  archive      = {J_JPDC},
  author       = {Lei Yang and Xiaoyuan Shen and Changyi Zhong and Yuwei Liao},
  doi          = {10.1016/j.jpdc.2022.09.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {79-87},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On-demand inference acceleration for directed acyclic graph neural networks over edge-cloud collaboration},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating execution time predictions on GPU kernels using
an analytical model and machine learning techniques. <em>JPDC</em>,
<em>171</em>, 66–78. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the performance of applications executed on GPUs is a great challenge and is essential for efficient job schedulers. There are different approaches to do this, namely analytical modeling and machine learning (ML) techniques. Machine learning requires large training sets and reliable features, nevertheless it can capture the interactions between architecture and software without manual intervention. In this paper, we compared a BSP-based analytical model to predict the time of execution of kernels executed over GPUs . The comparison was made using three different ML techniques. The analytical model is based on the number of computations and memory accesses of the GPU, with additional information on cache usage obtained from profiling. The ML techniques Linear Regression, Support Vector Machine , and Random Forest were evaluated over two scenarios: first, data input or features for ML techniques were the same as the analytical model and, second, using a process of feature extraction, which used correlation analysis and hierarchical clustering . Our experiments were conducted with 20 CUDA kernels, 11 of which belonged to 6 real-world applications of the Rodinia benchmark suite, and the other were classical matrix-vector applications commonly used for benchmarking. We collected data over 9 NVIDIA GPUs in different machines. We show that the analytical model performs better at predicting when applications scale regularly. For the analytical model a single parameter λ is capable of adjusting the predictions, minimizing the complex analysis in the applications. We show also that ML techniques obtained high accuracy when a process of feature extraction is implemented. Sets of 5 and 10 features were tested in two different ways, for unknown GPUs and for unknown Kernels. For ML experiments with a process of feature extractions, we got errors around 1.54\% and 2.71\%, for unknown GPUs and for unknown Kernels, respectively.},
  archive      = {J_JPDC},
  author       = {Marcos Amaris and Raphael Camargo and Daniel Cordeiro and Alfredo Goldman and Denis Trystram},
  doi          = {10.1016/j.jpdc.2022.09.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {66-78},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evaluating execution time predictions on GPU kernels using an analytical model and machine learning techniques},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subversion analyses of hierarchical networks based on (edge)
neighbor connectivity. <em>JPDC</em>, <em>171</em>, 54–65. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of neighbor connectivity was derived from the assessment of subversion in spy networks caused by the underground resistance movement. For a network G , the neighbor connectivity κ N B ( G ) κNB(G) (resp. edge neighbor connectivity λ N B ( G ) λNB(G) ) is defined as the least number of vertices (resp. edges) such that if we remove the closed neighborhoods of them, the network will become disconnected, empty, or complete (resp. trivial). The two connectivities can also provide more accurate measures regarding the reliability and fault-tolerance of networks. Star graphs S n Sn and hypercubes Q n Qn are the two most famous structures in the family of Cayley graphs . They are widely studied in the research of developing multiprocessor systems . In this paper, we investigate the neighbor connectivity and edge neighbor connectivity of two kinds of hierarchical networks, called hierarchical star network H S n HSn and complete cubic network C C n CCn , which take S n Sn and Q n Qn as building blocks , respectively. Specifically, we obtain the following results: κ N B ( H S n ) = n − 1 κNB(HSn)=n−1 and λ N B ( H S n ) = n λNB(HSn)=n for n ≥ 3 n≥3 , and κ N B ( C C n ) = ⌈ n 2 ⌉ + 1 κNB(CCn)=⌈n2⌉+1 and λ N B ( C C n ) = n + 1 λNB(CCn)=n+1 for n ≥ 2 n≥2 . In addition, to further determine the distribution of the number of subverted vertices and their occurrence status, we also carry out experiments to simulate the subversion at the vertices on these networks.},
  archive      = {J_JPDC},
  author       = {Mei-Mei Gu and Kung-Jui Pai and Jou-Ming Chang},
  doi          = {10.1016/j.jpdc.2022.09.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {54-65},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Subversion analyses of hierarchical networks based on (edge) neighbor connectivity},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grain data classification to filter token coherence
traffic. <em>JPDC</em>, <em>171</em>, 40–53. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snoop-based cache coherence protocols perform well in small-scale systems by enabling low latency cache-to-cache data transfers in just two-hop coherence transactions. However, they are not a scalable alternative as they require frequent broadcast of coherence requests. Token coherence protocols were proposed to improve the scalability of snoop-based protocols by removing a large amount of traffic due to broadcast responses. Still, broadcasting coherence requests on every cache miss represents a scalability issue for medium and large-scale systems. In this paper, we propose to reduce the number of broadcast operations in Token coherence protocols by performing an efficient fine-grain private-shared data classification and disabling broadcasts for misses to data classified as private. Our fine-grain classification is orchestrated and stored by the Translation Look-aside Buffers (TLBs), where entries are kept for a longer time than in local caches. We explore different classification granularity accounting for different storage overheads and their impact on filtering coherence traffic. We evaluate our proposals on a set of parallel benchmarks through full-system cycle-accurate simulation and show that a subpage-grain classification offers the best trade-off when accounting for storage, traffic, and performance. When running a 16-core configuration, our subpage-grain classification eliminates 40.1\% of broadcast operations compared to not performing any classification and 13.7\% of broadcast operations more than a page-grain data classification . This reduction translates into less network traffic (16.0\%), and finally, performance improvements of 12.0\% compared to not having a classification mechanism.},
  archive      = {J_JPDC},
  author       = {Bhargavi R. Upadhyay and Alberto Ros and Supriya M.},
  doi          = {10.1016/j.jpdc.2022.09.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {40-53},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fine-grain data classification to filter token coherence traffic},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online computation offloading with double reinforcement
learning algorithm in mobile edge computing. <em>JPDC</em>,
<em>171</em>, 28–39. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart mobile devices have recently emerged as a promising computing platform for computation tasks. However, the task performance is restricted by the computing power and battery capacity of mobile devices . Mobile edge computing , an extension of cloud computing , solves this problem well by providing computational support to mobile devices . In this paper, we discuss a mobile edge computing system with a server and multiple mobile devices that need to perform computation tasks with priorities. The limited resources of the mobile edge computing server and mobile device make it challenging to develop an offloading strategy to minimize both delay and energy consumption in the long term. To this end, an online algorithm is proposed, namely, the double reinforcement learning computation offloading (DRLCO) algorithm, which jointly decides the offloading decision, the CPU frequency, and transmit power for computation offloading. Concretely, we first formulate the power scheduling problem for mobile users to minimize energy consumption. Inspired by reinforcement learning, we solve the problem by presenting a power scheduling algorithm based on the deep deterministic policy gradient (DDPG). Then, we model the task offloading problem to minimize the delay of tasks and propose a double Deep Q-networks (DQN) based algorithm. In the decision-making process, we fully consider the influence of task queue information, channel state information , and task information. Moreover, we propose an adaptive prioritized experience replay algorithm to improve the model training efficiency. We conduct extensive simulations to verify the effectiveness of the scheme, and the simulation results show that compared with the conventional schemes, our method reduces the delay by 48\% and the energy consumption by 53\%.},
  archive      = {J_JPDC},
  author       = {Linbo Liao and Yongxuan Lai and Fan Yang and Wenhua Zeng},
  doi          = {10.1016/j.jpdc.2022.09.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {28-39},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Online computation offloading with double reinforcement learning algorithm in mobile edge computing},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulating structural plasticity of the brain more scalable
than expected. <em>JPDC</em>, <em>171</em>, 24–27. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural plasticity of the brain describes the creation of new and the deletion of old synapses over time. Rinke et al. (JPDC 2018) introduced a scalable algorithm that simulates structural plasticity for up to one billion neurons on current hardware using a variant of the Barnes–Hut algorithm. They demonstrate good scalability and prove a runtime complexity of O ( n log 2 ⁡ n ) O(nlog2⁡n) . In this comment paper, we show that with careful consideration of the algorithm and a rigorous proof , the theoretical runtime can even be classified as O ( n log ⁡ n ) O(nlog⁡n) .},
  archive      = {J_JPDC},
  author       = {Fabian Czappa and Alexander Geiß and Felix Wolf},
  doi          = {10.1016/j.jpdc.2022.09.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-27},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Simulating structural plasticity of the brain more scalable than expected},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forseti: Dynamic chunk-level reshaping for data processing
on heterogeneous clusters. <em>JPDC</em>, <em>171</em>, 14–23. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-intensive computing frameworks typically split job workload into fixed-size chunks, allowing them to be processed as parallel tasks on distributed machines. Ideally, when the machines are homogeneous and have identical speed, chunks of equal size would finish processing at the same time. However, such determinism in processing time cannot be guaranteed in practice. Diverging processing times can result from various sources such as system dynamics , machine heterogeneity, and variable network conditions. Such variation, together with dynamics and uncertainty during task processing, can lead to significant performance degradation at job level, due to long tails in job completion time resulted from residual chunk workload and stragglers. In this paper, we propose Forseti , a novel processing scheme that is able to reshape data chunk size on the fly with respect to heterogeneous machines and a dynamic execution environment. Forseti mitigates residual workload and stragglers to achieve significant improvement in performance. We note that Forseti is a fully online scheme and does not require any a priori knowledge of the machine configuration nor job statistics. Instead, it infers such information and adjusts data chunk sizes at runtime, making the solution robust even in environments with high volatility. In its implementation, Forseti also exploits a virtual machine reuse feature to avoid task start-up and initialization cost associated with launching new tasks. We prototype Forseti on a real-world cluster and evaluate its performance using several realistic benchmarks. The results show that Forseti outperforms a number of baselines, including default Hadoop by up to 68\% and SkewTune by up to 50\% in terms of average job completion time.},
  archive      = {J_JPDC},
  author       = {Sultan Alamro and Tian Lan and Suresh Subramaniam},
  doi          = {10.1016/j.jpdc.2022.09.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {14-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Forseti: Dynamic chunk-level reshaping for data processing on heterogeneous clusters},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent hybrid method: Multi-objective optimization
for MEC-enabled devices of IoE. <em>JPDC</em>, <em>171</em>, 1–13. (<a
href="https://doi.org/10.1016/j.jpdc.2022.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Internet-of-Everything (IoE) services require connected devices to respond instantly and operate for long durations. As smart mobile devices (SMDs) are often powered by batteries of limited capacity, offloading some computational tasks to nearby edge servers is a promising solution to reduce the latency and energy consumption of SMDs in operation. However, a challenge of computation offloading in the IoE system is the large number of SMDs that need to be handled. To address this problem, in this paper, we propose an intelligent two-stage computation offloading scheme with multiple optimization objectives . In the first stage, we categorize the computation tasks into classes (e.g., computation-intensive, data-intensive) and make early offloading decisions on some classes of tasks with offloading preferences. In the second stage, we make offloading decisions on the remaining tasks by solving a multi-objective optimization problem using the powerful Non-dominated Sorting Genetic Algorithm (NSGA-II). This two-stage design can help reduce the size of task instances in the second optimization stage , thus accelerating the convergence of the NSGA-II algorithm. The extensive simulation results show that, compared to the existing NSGA-II algorithm-based optimization methods, the proposed offloading scheme improves the performance by 10\% in terms of latency and energy consumption.},
  archive      = {J_JPDC},
  author       = {Kuanishbay Sadatdiynov and Laizhong Cui and Lei Zhang and Joshua Zhexue Huang and Neal N. Xiong and Chengwen Luo},
  doi          = {10.1016/j.jpdc.2022.09.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-13},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An intelligent hybrid method: Multi-objective optimization for MEC-enabled devices of IoE},
  volume       = {171},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
